{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from preprocessor import AgeRecognitionPreprocessor\n",
    "from dataset import AgeRecognitionDataset\n",
    "from models import vit_l_16_age_recognizer, vit_b_16_age_recognizer, resent101_age_recogniser\n",
    "from loss import AgeRecognitionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "\n",
    "IMAGE_DIR = './Cleaned/'\n",
    "TRAINING_PAIRINGS = './training_data.csv'\n",
    "BATCH_SIZE = 12\n",
    "EPOCHES = 1000\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resent101_age_recogniser().to(DEVICE)\n",
    "loss_function = AgeRecognitionLoss().to(DEVICE)\n",
    "# loss_function.importance.requires_grad = False\n",
    "preprocessor = AgeRecognitionPreprocessor()\n",
    "dataset = AgeRecognitionDataset(triplet_csv_path=TRAINING_PAIRINGS, image_dir=IMAGE_DIR, preprocessor=preprocessor, kfolds=1, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(list(model.parameters()) + list(loss_function.parameters()), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=EPOCHES, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./Checkpoint/best.pt'):\n",
    "    best_state = torch.load('./Checkpoint/best.pt')\n",
    "    model.load_state_dict(best_state['model_state_dict'])\n",
    "    loss_function.load_state_dict(best_state['loss_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for batch 0 : 1.217010259628296\n",
      "Training loss for batch 1 : 1.2170003652572632\n",
      "Training loss for batch 2 : 1.216990351676941\n",
      "Training loss for batch 3 : 1.2169803380966187\n",
      "Training loss for batch 4 : 1.2169703245162964\n",
      "Training loss for batch 5 : 1.2169603109359741\n",
      "Training loss for batch 6 : 1.2169502973556519\n",
      "Training loss for batch 7 : 1.2169402837753296\n",
      "Training loss for batch 8 : 1.2169303894042969\n",
      "Training loss for batch 9 : 1.2169203758239746\n",
      "Training loss for batch 10 : 1.2169103622436523\n",
      "Training loss for batch 11 : 1.21690034866333\n",
      "Training loss for batch 12 : 0.5728828310966492\n",
      "Training loss for batch 13 : 0.18566462397575378\n",
      "Training loss for batch 14 : 0.024652304127812386\n",
      "Training loss for batch 15 : 0.1435224413871765\n",
      "Training loss for batch 16 : 0.035693150013685226\n",
      "Training loss for batch 17 : 0.17657217383384705\n",
      "Training loss for batch 18 : 0.5170924067497253\n",
      "Training loss for batch 19 : 0.3168896734714508\n",
      "Training loss for batch 20 : 0.47924521565437317\n",
      "Training loss for batch 21 : 0.22018314898014069\n",
      "Training loss for batch 22 : 0.7151458859443665\n",
      "Training loss for batch 23 : 0.1593867987394333\n",
      "Training loss for batch 24 : 0.5395755767822266\n",
      "Training loss for batch 25 : 0.6297249794006348\n",
      "Training loss for batch 26 : 0.2410265952348709\n",
      "Training loss for batch 27 : 0.35532230138778687\n",
      "Training loss for batch 28 : 0.787112295627594\n",
      "Training loss for batch 29 : 0.5185861587524414\n",
      "Training loss for batch 30 : 0.2902422249317169\n",
      "Training loss for batch 31 : 0.2046411633491516\n",
      "Training loss for batch 32 : 0.5716461539268494\n",
      "Training loss for batch 33 : 0.26933062076568604\n",
      "Training loss for batch 34 : 0.16694962978363037\n",
      "Training loss for batch 35 : 0.26720550656318665\n",
      "Training loss for batch 36 : 0.31858518719673157\n",
      "Training loss for batch 37 : 0.4164193868637085\n",
      "Training loss for batch 38 : 0.5052711367607117\n",
      "Training loss for batch 39 : 0.0974765419960022\n",
      "Training loss for batch 40 : 0.2688855826854706\n",
      "Training loss for batch 41 : 0.00481588626280427\n",
      "Training loss for batch 42 : 0.22465740144252777\n",
      "Training loss for batch 43 : 0.37468400597572327\n",
      "Training loss for batch 44 : 0.2057037055492401\n",
      "Training loss for batch 45 : 0.434278666973114\n",
      "Training loss for batch 46 : 0.7412745356559753\n",
      "Training loss for batch 47 : 0.5302704572677612\n",
      "Training loss for batch 48 : 0.4189407527446747\n",
      "Training loss for batch 49 : 0.7984369397163391\n",
      "Training loss for batch 50 : 0.2983896732330322\n",
      "Training loss for batch 51 : 0.5179840922355652\n",
      "Training loss for batch 52 : 0.1774866282939911\n",
      "Training loss for batch 53 : 0.039109356701374054\n",
      "Training loss for batch 54 : 0.2464558482170105\n",
      "Training loss for batch 55 : 0.08493451029062271\n",
      "Training loss for batch 56 : 0.24778026342391968\n",
      "Training loss for batch 57 : 0.13731372356414795\n",
      "Training loss for batch 58 : 0.38723668456077576\n",
      "Training loss for batch 59 : 0.49597495794296265\n",
      "Training loss for batch 60 : 0.5451585650444031\n",
      "Training loss for batch 61 : 0.2830677330493927\n",
      "Training loss for batch 62 : 0.33186712861061096\n",
      "Training loss for batch 63 : 0.07191707193851471\n",
      "Training loss for batch 64 : 0.18409520387649536\n",
      "Training loss for batch 65 : 0.20633573830127716\n",
      "Training loss for batch 66 : 0.4035283625125885\n",
      "Training loss for batch 67 : 0.2496718317270279\n",
      "Training loss for batch 68 : 0.08786706626415253\n",
      "Training loss for batch 69 : 0.014984896406531334\n",
      "Training loss for batch 70 : 0.22948844730854034\n",
      "Training loss for batch 71 : 0.4982706904411316\n",
      "Training loss for batch 72 : 0.30090925097465515\n",
      "Training loss for batch 73 : 0.27345359325408936\n",
      "Training loss for batch 74 : 0.5711113810539246\n",
      "Training loss for batch 75 : 0.006851045414805412\n",
      "Training loss for batch 76 : 0.15949025750160217\n",
      "Training loss for batch 77 : 0.09651032090187073\n",
      "Training loss for batch 78 : 0.1288922280073166\n",
      "Training loss for batch 79 : 0.16244611144065857\n",
      "Training loss for batch 80 : 0.15806882083415985\n",
      "Training loss for batch 81 : 0.2601057291030884\n",
      "Training loss for batch 82 : 0.2506727874279022\n",
      "Training loss for batch 83 : 0.4516356885433197\n",
      "Training loss for batch 84 : 0.2479637861251831\n",
      "Training loss for batch 85 : 0.1976563185453415\n",
      "Training loss for batch 86 : 0.2700188159942627\n",
      "Training loss for batch 87 : 0.2534172534942627\n",
      "Training loss for batch 88 : 0.67594313621521\n",
      "Training loss for batch 89 : 0.8735820651054382\n",
      "Training loss for batch 90 : 0.6307855248451233\n",
      "Training loss for batch 91 : 0.21260938048362732\n",
      "Training loss for batch 92 : 0.7002174854278564\n",
      "Training loss for batch 93 : 0.07709959894418716\n",
      "Training loss for batch 94 : 0.6815814971923828\n",
      "Training loss for batch 95 : 0.12548168003559113\n",
      "Training loss for batch 96 : 0.21797077357769012\n",
      "Training loss for batch 97 : 0.6503533720970154\n",
      "Training loss for batch 98 : 0.643096387386322\n",
      "Training loss for batch 99 : 0.40589067339897156\n",
      "Training loss for batch 100 : 0.41287297010421753\n",
      "Training loss for batch 101 : 0.34601980447769165\n",
      "Training loss for batch 102 : 0.23081599175930023\n",
      "Training loss for batch 103 : 0.5502589344978333\n",
      "Training loss for batch 104 : 0.2024788111448288\n",
      "Training loss for batch 105 : 0.0\n",
      "Training loss for batch 106 : 0.06899813562631607\n",
      "Training loss for batch 107 : 0.4934665858745575\n",
      "Training loss for batch 108 : 0.12303532660007477\n",
      "Training loss for batch 109 : 0.20242080092430115\n",
      "Training loss for batch 110 : 0.36369675397872925\n",
      "Training loss for batch 111 : 0.11526328325271606\n",
      "Training loss for batch 112 : 0.5959718227386475\n",
      "Training loss for batch 113 : 0.4373736083507538\n",
      "Training loss for batch 114 : 0.2563806474208832\n",
      "Training loss for batch 115 : 0.2338407039642334\n",
      "Training loss for batch 116 : 0.3096468150615692\n",
      "Training loss for batch 117 : 0.24798862636089325\n",
      "Training loss for batch 118 : 0.5119160413742065\n",
      "Training loss for batch 119 : 0.48894718289375305\n",
      "Training loss for batch 120 : 0.9924129247665405\n",
      "Training loss for batch 121 : 0.5403224229812622\n",
      "Training loss for batch 122 : 0.5582960247993469\n",
      "Training loss for batch 123 : 0.368563175201416\n",
      "Training loss for batch 124 : 0.4199467599391937\n",
      "Training loss for batch 125 : 0.18228913843631744\n",
      "Training loss for batch 126 : 0.43697914481163025\n",
      "Training loss for batch 127 : 0.7167320251464844\n",
      "Training loss for batch 128 : 0.3117145597934723\n",
      "Training loss for batch 129 : 0.1966613084077835\n",
      "Training loss for batch 130 : 0.45052748918533325\n",
      "Training loss for batch 131 : 0.3917906582355499\n",
      "Training loss for batch 132 : 0.04961337894201279\n",
      "Training loss for batch 133 : 0.12614795565605164\n",
      "Training loss for batch 134 : 0.4062795341014862\n",
      "Training loss for batch 135 : 0.526337206363678\n",
      "Training loss for batch 136 : 0.3403799533843994\n",
      "Training loss for batch 137 : 0.716966450214386\n",
      "Training loss for batch 138 : 0.3341921269893646\n",
      "Training loss for batch 139 : 0.3526766896247864\n",
      "Training loss for batch 140 : 0.36761534214019775\n",
      "Training loss for batch 141 : 0.23061972856521606\n",
      "Training loss for batch 142 : 0.1483887881040573\n",
      "Training loss for batch 143 : 0.45268988609313965\n",
      "Training loss for batch 144 : 0.21914422512054443\n",
      "Training loss for batch 145 : 0.16250313818454742\n",
      "Training loss for batch 146 : 0.30629900097846985\n",
      "Training loss for batch 147 : 0.43865424394607544\n",
      "Training loss for batch 148 : 0.507792592048645\n",
      "Training loss for batch 149 : 0.5871472954750061\n",
      "Training loss for batch 150 : 0.34971126914024353\n",
      "Training loss for batch 151 : 0.2593403458595276\n",
      "Training loss for batch 152 : 0.21087680757045746\n",
      "Training loss for batch 153 : 0.3844316005706787\n",
      "Training loss for batch 154 : 0.18483534455299377\n",
      "Training loss for batch 155 : 0.28635725378990173\n",
      "Training loss for batch 156 : 0.040024079382419586\n",
      "Training loss for batch 157 : 0.47736454010009766\n",
      "Training loss for batch 158 : 0.2891699969768524\n",
      "Training loss for batch 159 : 0.2374725341796875\n",
      "Training loss for batch 160 : 0.4767204821109772\n",
      "Training loss for batch 161 : 0.19911015033721924\n",
      "Training loss for batch 162 : 0.4164799451828003\n",
      "Training loss for batch 163 : 0.23589986562728882\n",
      "Training loss for batch 164 : 0.1970084011554718\n",
      "Training loss for batch 165 : 0.6582785844802856\n",
      "Training loss for batch 166 : 0.43819597363471985\n",
      "Training loss for batch 167 : 0.1906794309616089\n",
      "Training loss for batch 168 : 0.35721510648727417\n",
      "Training loss for batch 169 : 0.23816035687923431\n",
      "Training loss for batch 170 : 0.424354612827301\n",
      "Training loss for batch 171 : 0.5122630000114441\n",
      "Training loss for batch 172 : 0.18267223238945007\n",
      "Training loss for batch 173 : 0.565929651260376\n",
      "Training loss for batch 174 : 0.6740962266921997\n",
      "Training loss for batch 175 : 0.7715357542037964\n",
      "Training loss for batch 176 : 0.76019287109375\n",
      "Training loss for batch 177 : 0.08411538600921631\n",
      "Training loss for batch 178 : 0.2390778809785843\n",
      "Training loss for batch 179 : 0.1625657081604004\n",
      "Training loss for batch 180 : 0.23202353715896606\n",
      "Training loss for batch 181 : 0.36923694610595703\n",
      "Training loss for batch 182 : 0.3945145606994629\n",
      "Training loss for batch 183 : 0.505527913570404\n",
      "Training loss for batch 184 : 0.2172165811061859\n",
      "Training loss for batch 185 : 0.34493574500083923\n",
      "Training loss for batch 186 : 0.45125672221183777\n",
      "Training loss for batch 187 : 0.5861310958862305\n",
      "Training loss for batch 188 : 0.4742894768714905\n",
      "Training loss for batch 189 : 0.253164142370224\n",
      "Training loss for batch 190 : 0.46145308017730713\n",
      "Training loss for batch 191 : 0.18330641090869904\n",
      "Training loss for batch 192 : 0.18474552035331726\n",
      "Training loss for batch 193 : 0.5371440052986145\n",
      "Training loss for batch 194 : 0.17122624814510345\n",
      "Training loss for batch 195 : 0.592365026473999\n",
      "Training loss for batch 196 : 0.35415858030319214\n",
      "Training loss for batch 197 : 0.3447221517562866\n",
      "Training loss for batch 198 : 0.12825137376785278\n",
      "Training loss for batch 199 : 0.671862006187439\n",
      "Training loss for batch 200 : 0.08439403027296066\n",
      "Training loss for batch 201 : 0.2875252962112427\n",
      "Training loss for batch 202 : 0.5701696276664734\n",
      "Training loss for batch 203 : 0.19568303227424622\n",
      "Training loss for batch 204 : 0.4132407009601593\n",
      "Training loss for batch 205 : 0.14298605918884277\n",
      "Training loss for batch 206 : 0.5907220244407654\n",
      "Training loss for batch 207 : 0.314881831407547\n",
      "Training loss for batch 208 : 0.29539361596107483\n",
      "Training loss for batch 209 : 0.4314049184322357\n",
      "Training loss for batch 210 : 0.1439853459596634\n",
      "Training loss for batch 211 : 0.2310776561498642\n",
      "Training loss for batch 212 : 0.47424817085266113\n",
      "Training loss for batch 213 : 0.4445742070674896\n",
      "Training loss for batch 214 : 0.18094894289970398\n",
      "Training loss for batch 215 : 0.252194881439209\n",
      "Training loss for batch 216 : 0.20252786576747894\n",
      "Training loss for batch 217 : 0.2870449125766754\n",
      "Training loss for batch 218 : 0.17536267638206482\n",
      "Training loss for batch 219 : 1.0005353689193726\n",
      "Training loss for batch 220 : 0.286760538816452\n",
      "Training loss for batch 221 : 0.16769936680793762\n",
      "Training loss for batch 222 : 0.5291491746902466\n",
      "Training loss for batch 223 : 0.5273600816726685\n",
      "Training loss for batch 224 : 0.1241549476981163\n",
      "Training loss for batch 225 : 0.511061429977417\n",
      "Training loss for batch 226 : 0.3479553461074829\n",
      "Training loss for batch 227 : 0.461655855178833\n",
      "Training loss for batch 228 : 0.2785572409629822\n",
      "Training loss for batch 229 : 0.17985500395298004\n",
      "Training loss for batch 230 : 0.3582805395126343\n",
      "Training loss for batch 231 : 0.18220646679401398\n",
      "Training loss for batch 232 : 0.17877916991710663\n",
      "Training loss for batch 233 : 0.3040074408054352\n",
      "Training loss for batch 234 : 0.23899507522583008\n",
      "Training loss for batch 235 : 0.609315812587738\n",
      "Training loss for batch 236 : 0.4712924361228943\n",
      "Training loss for batch 237 : 0.25738030672073364\n",
      "Training loss for batch 238 : 0.11942803859710693\n",
      "Training loss for batch 239 : 0.04279032349586487\n",
      "Training loss for batch 240 : 0.33577263355255127\n",
      "Training loss for batch 241 : 0.3108118772506714\n",
      "Training loss for batch 242 : 0.1607864499092102\n",
      "Training loss for batch 243 : 0.3778949975967407\n",
      "Training loss for batch 244 : 0.35524341464042664\n",
      "Training loss for batch 245 : 0.8236343264579773\n",
      "Training loss for batch 246 : 0.18139676749706268\n",
      "Training loss for batch 247 : 0.4932987689971924\n",
      "Training loss for batch 248 : 0.189854696393013\n",
      "Training loss for batch 249 : 0.36916401982307434\n",
      "Training loss for batch 250 : 0.37386852502822876\n",
      "Training loss for batch 251 : 0.3007965683937073\n",
      "Training loss for batch 252 : 0.7825112342834473\n",
      "Training loss for batch 253 : 0.30462783575057983\n",
      "Training loss for batch 254 : 0.0\n",
      "Training loss for batch 255 : 0.40605804324150085\n",
      "Training loss for batch 256 : 0.49216389656066895\n",
      "Training loss for batch 257 : 0.1116713285446167\n",
      "Training loss for batch 258 : 0.5209523439407349\n",
      "Training loss for batch 259 : 0.2726568579673767\n",
      "Training loss for batch 260 : 0.07464800029993057\n",
      "Training loss for batch 261 : 0.1499761939048767\n",
      "Training loss for batch 262 : 0.09670202434062958\n",
      "Training loss for batch 263 : 0.2970053553581238\n",
      "Training loss for batch 264 : 0.19534389674663544\n",
      "Training loss for batch 265 : 0.9281398057937622\n",
      "Training loss for batch 266 : 0.08268329501152039\n",
      "Training loss for batch 267 : 0.1721273809671402\n",
      "Training loss for batch 268 : 0.14908021688461304\n",
      "Training loss for batch 269 : 0.4436216950416565\n",
      "Training loss for batch 270 : 0.30080446600914\n",
      "Training loss for batch 271 : 0.21766579151153564\n",
      "Training loss for batch 272 : 0.8642009496688843\n",
      "Training loss for batch 273 : 0.44503769278526306\n",
      "Training loss for batch 274 : 0.2818458080291748\n",
      "Training loss for batch 275 : 0.3633180856704712\n",
      "Training loss for batch 276 : 0.3518576920032501\n",
      "Training loss for batch 277 : 0.2254076898097992\n",
      "Training loss for batch 278 : 0.2316058874130249\n",
      "Training loss for batch 279 : 0.27868059277534485\n",
      "Training loss for batch 280 : 0.11736861616373062\n",
      "Training loss for batch 281 : 0.5535497069358826\n",
      "Training loss for batch 282 : 0.5017558336257935\n",
      "Training loss for batch 283 : 0.4056134819984436\n",
      "Training loss for batch 284 : 0.10993027687072754\n",
      "Training loss for batch 285 : 0.16430553793907166\n",
      "Training loss for batch 286 : 0.28702110052108765\n",
      "Training loss for batch 287 : 0.8313747644424438\n",
      "Training loss for batch 288 : 0.18283036351203918\n",
      "Training loss for batch 289 : 0.2598290741443634\n",
      "Training loss for batch 290 : 0.2628559470176697\n",
      "Training loss for batch 291 : 0.2858894467353821\n",
      "Training loss for batch 292 : 0.22273197770118713\n",
      "Training loss for batch 293 : 0.296922892332077\n",
      "Training loss for batch 294 : 0.30769070982933044\n",
      "Training loss for batch 295 : 0.16760501265525818\n",
      "Training loss for batch 296 : 0.30354589223861694\n",
      "Training loss for batch 297 : 0.4944188594818115\n",
      "Training loss for batch 298 : 0.3366152048110962\n",
      "Training loss for batch 299 : 0.4014383852481842\n",
      "Training loss for batch 300 : 0.30047717690467834\n",
      "Training loss for batch 301 : 0.17132143676280975\n",
      "Training loss for batch 302 : 0.5702001452445984\n",
      "Training loss for batch 303 : 0.4366394281387329\n",
      "Training loss for batch 304 : 0.07863198220729828\n",
      "Training loss for batch 305 : 0.38233497738838196\n",
      "Training loss for batch 306 : 0.9227185249328613\n",
      "Training loss for batch 307 : 0.23390671610832214\n",
      "Training loss for batch 308 : 0.28181323409080505\n",
      "Training loss for batch 309 : 0.4828777015209198\n",
      "Training loss for batch 310 : 0.23961126804351807\n",
      "Training loss for batch 311 : 0.20124493539333344\n",
      "Training loss for batch 312 : 0.06747741252183914\n",
      "Training loss for batch 313 : 0.47775545716285706\n",
      "Training loss for batch 314 : 0.22090889513492584\n",
      "Training loss for batch 315 : 0.02457517944276333\n",
      "Training loss for batch 316 : 0.333040714263916\n",
      "Training loss for batch 317 : 0.46365469694137573\n",
      "Training loss for batch 318 : 0.11138515919446945\n",
      "Training loss for batch 319 : 0.4713696241378784\n",
      "Training loss for batch 320 : 0.26103195548057556\n",
      "Training loss for batch 321 : 0.4107847809791565\n",
      "Training loss for batch 322 : 0.33621180057525635\n",
      "Training loss for batch 323 : 0.03423687443137169\n",
      "Training loss for batch 324 : 0.6198886036872864\n",
      "Training loss for batch 325 : 0.5140479207038879\n",
      "Training loss for batch 326 : 0.22105418145656586\n",
      "Training loss for batch 327 : 0.03281247243285179\n",
      "Training loss for batch 328 : 0.49320557713508606\n",
      "Training loss for batch 329 : 0.2513427436351776\n",
      "Training loss for batch 330 : 0.36630499362945557\n",
      "Training loss for batch 331 : 0.02772773802280426\n",
      "Training loss for batch 332 : 0.6036940813064575\n",
      "Training loss for batch 333 : 0.7795392274856567\n",
      "Training loss for batch 334 : 0.536271333694458\n",
      "Training loss for batch 335 : 0.30782055854797363\n",
      "Training loss for batch 336 : 0.37819528579711914\n",
      "Training loss for batch 337 : 0.44040417671203613\n",
      "Training loss for batch 338 : 0.10134290158748627\n",
      "Training loss for batch 339 : 0.4853639304637909\n",
      "Training loss for batch 340 : 0.3935541808605194\n",
      "Training loss for batch 341 : 0.18043914437294006\n",
      "Training loss for batch 342 : 0.8684693574905396\n",
      "Training loss for batch 343 : 0.137284517288208\n",
      "Training loss for batch 344 : 0.2458992600440979\n",
      "Training loss for batch 345 : 0.331991583108902\n",
      "Training loss for batch 346 : 0.47618597745895386\n",
      "Training loss for batch 347 : 0.28724268078804016\n",
      "Training loss for batch 348 : 0.9116828441619873\n",
      "Training loss for batch 349 : 0.11096332967281342\n",
      "Training loss for batch 350 : 0.17360590398311615\n",
      "Training loss for batch 351 : 0.49311351776123047\n",
      "Training loss for batch 352 : 0.17294621467590332\n",
      "Training loss for batch 353 : 0.4800143241882324\n",
      "Training loss for batch 354 : 0.2983802855014801\n",
      "Training loss for batch 355 : 0.9155133962631226\n",
      "Training loss for batch 356 : 0.12085480242967606\n",
      "Training loss for batch 357 : 0.30243340134620667\n",
      "Training loss for batch 358 : 0.3002839684486389\n",
      "Training loss for batch 359 : 0.037646010518074036\n",
      "Training loss for batch 360 : 0.48780661821365356\n",
      "Training loss for batch 361 : 0.407538503408432\n",
      "Training loss for batch 362 : 0.37227290868759155\n",
      "Training loss for batch 363 : 0.4496786892414093\n",
      "Training loss for batch 364 : 0.654585063457489\n",
      "Training loss for batch 365 : 0.12979328632354736\n",
      "Training loss for batch 366 : 0.29088395833969116\n",
      "Training loss for batch 367 : 0.06829337775707245\n",
      "Training loss for batch 368 : 0.27185705304145813\n",
      "Training loss for batch 369 : 0.19247429072856903\n",
      "Training loss for batch 370 : 0.3824026584625244\n",
      "Training loss for batch 371 : 0.20713764429092407\n",
      "Training loss for batch 372 : 0.9244795441627502\n",
      "Training loss for batch 373 : 0.19951942563056946\n",
      "Training loss for batch 374 : 0.14888450503349304\n",
      "Training loss for batch 375 : 0.1401536762714386\n",
      "Training loss for batch 376 : 0.3866114616394043\n",
      "Training loss for batch 377 : 0.037166789174079895\n",
      "Training loss for batch 378 : 0.5617001056671143\n",
      "Training loss for batch 379 : 0.2936554551124573\n",
      "Training loss for batch 380 : 0.13360698521137238\n",
      "Training loss for batch 381 : 0.6263297200202942\n",
      "Training loss for batch 382 : 0.3786017596721649\n",
      "Training loss for batch 383 : 0.15343953669071198\n",
      "Training loss for batch 384 : 0.24580281972885132\n",
      "Training loss for batch 385 : 0.42774349451065063\n",
      "Training loss for batch 386 : 0.4053274691104889\n",
      "Training loss for batch 387 : 0.22278861701488495\n",
      "Training loss for batch 388 : 0.4859619736671448\n",
      "Training loss for batch 389 : 0.2657161056995392\n",
      "Training loss for batch 390 : 0.18575529754161835\n",
      "Training loss for batch 391 : 0.011874973773956299\n",
      "Training loss for batch 392 : 0.1228853166103363\n",
      "Training loss for batch 393 : 0.444172203540802\n",
      "Training loss for batch 394 : 0.5137160420417786\n",
      "Training loss for batch 395 : 0.2914569675922394\n",
      "Training loss for batch 396 : 0.1604795753955841\n",
      "Training loss for batch 397 : 0.21188540756702423\n",
      "Training loss for batch 398 : 0.3119714856147766\n",
      "Training loss for batch 399 : 0.3354928493499756\n",
      "Training loss for batch 400 : 0.21880337595939636\n",
      "Training loss for batch 401 : 0.4603254199028015\n",
      "Training loss for batch 402 : 0.2741338610649109\n",
      "Training loss for batch 403 : 0.25774654746055603\n",
      "Training loss for batch 404 : 0.5682612657546997\n",
      "Training loss for batch 405 : 0.6683251857757568\n",
      "Training loss for batch 406 : 0.12317865341901779\n",
      "Training loss for batch 407 : 0.3413318395614624\n",
      "Training loss for batch 408 : 0.2725592255592346\n",
      "Training loss for batch 409 : 0.6264265775680542\n",
      "Training loss for batch 410 : 0.5455678105354309\n",
      "Training loss for batch 411 : 0.30230340361595154\n",
      "Training loss for batch 412 : 0.43828266859054565\n",
      "Training loss for batch 413 : 0.09567084908485413\n",
      "Training loss for batch 414 : 0.2195623219013214\n",
      "Training loss for batch 415 : 0.14777816832065582\n",
      "Training loss for batch 416 : 0.5196611285209656\n",
      "Training loss for batch 417 : 0.4386221170425415\n",
      "Training loss for batch 418 : 0.10050556063652039\n",
      "Training loss for batch 419 : 0.5057902932167053\n",
      "Training loss for batch 420 : 0.5000043511390686\n",
      "Training loss for batch 421 : 0.49011939764022827\n",
      "Training loss for batch 422 : 0.548903226852417\n",
      "Training loss for batch 423 : 0.10063552856445312\n",
      "Training loss for batch 424 : 0.574049711227417\n",
      "Training loss for batch 425 : 0.17433494329452515\n",
      "Training loss for batch 426 : 0.5183018445968628\n",
      "Training loss for batch 427 : 0.14001180231571198\n",
      "Training loss for batch 428 : 0.37596556544303894\n",
      "Training loss for batch 429 : 0.5711288452148438\n",
      "Training loss for batch 430 : 0.19076703488826752\n",
      "Training loss for batch 431 : 0.20597060024738312\n",
      "Training loss for batch 432 : 0.1263256072998047\n",
      "Training loss for batch 433 : 0.5249935388565063\n",
      "Training loss for batch 434 : 0.21576888859272003\n",
      "Training loss for batch 435 : 0.17477641999721527\n",
      "Training loss for batch 436 : 0.6408327221870422\n",
      "Training loss for batch 437 : 0.15008899569511414\n",
      "Training loss for batch 438 : 0.27427738904953003\n",
      "Training loss for batch 439 : 0.5489741563796997\n",
      "Training loss for batch 440 : 0.18899308145046234\n",
      "Training loss for batch 441 : 0.27090683579444885\n",
      "Training loss for batch 442 : 0.5864500999450684\n",
      "Training loss for batch 443 : 0.30055177211761475\n",
      "Training loss for batch 444 : 0.5880956649780273\n",
      "Training loss for batch 445 : 0.3764375150203705\n",
      "Training loss for batch 446 : 0.47283393144607544\n",
      "Training loss for batch 447 : 0.265579491853714\n",
      "Training loss for batch 448 : 0.17818975448608398\n",
      "Training loss for batch 449 : 0.11008033901453018\n",
      "Training loss for batch 450 : 0.7638964056968689\n",
      "Training loss for batch 451 : 0.1477774977684021\n",
      "Training loss for batch 452 : 0.28908485174179077\n",
      "Training loss for batch 453 : 0.34705790877342224\n",
      "Training loss for batch 454 : 0.09535335004329681\n",
      "Training loss for batch 455 : 0.42412352561950684\n",
      "Training loss for batch 456 : 0.09092699736356735\n",
      "Training loss for batch 457 : 0.09491705149412155\n",
      "Training loss for batch 458 : 0.48177391290664673\n",
      "Training loss for batch 459 : 0.13849470019340515\n",
      "Training loss for batch 460 : 0.027796071022748947\n",
      "Training loss for batch 461 : 0.1523331254720688\n",
      "Training loss for batch 462 : 0.2888314723968506\n",
      "Training loss for batch 463 : 0.07918482273817062\n",
      "Training loss for batch 464 : 0.4909501075744629\n",
      "Training loss for batch 465 : 0.10253883898258209\n",
      "Training loss for batch 466 : 0.5420429110527039\n",
      "Training loss for batch 467 : 0.4598112404346466\n",
      "Training loss for batch 468 : 0.2650465965270996\n",
      "Training loss for batch 469 : 0.08360768854618073\n",
      "Training loss for batch 470 : 0.3785627484321594\n",
      "Training loss for batch 471 : 0.20331820845603943\n",
      "Training loss for batch 472 : 0.310369074344635\n",
      "Training loss for batch 473 : 0.4176619052886963\n",
      "Training loss for batch 474 : 0.3566269278526306\n",
      "Training loss for batch 475 : 0.219216451048851\n",
      "Training loss for batch 476 : 0.22735492885112762\n",
      "Training loss for batch 477 : 0.5790536999702454\n",
      "Training loss for batch 478 : 0.6544344425201416\n",
      "Training loss for batch 479 : 0.6563342809677124\n",
      "Training loss for batch 480 : 0.18146905303001404\n",
      "Training loss for batch 481 : 0.18665634095668793\n",
      "Training loss for batch 482 : 0.2195897251367569\n",
      "Training loss for batch 483 : 0.654823899269104\n",
      "Training loss for batch 484 : 0.6906222701072693\n",
      "Training loss for batch 485 : 0.11481499671936035\n",
      "Training loss for batch 486 : 0.5549073815345764\n",
      "Training loss for batch 487 : 0.11629219353199005\n",
      "Training loss for batch 488 : 0.13239839673042297\n",
      "Training loss for batch 489 : 0.0664953887462616\n",
      "Training loss for batch 490 : 0.1179942786693573\n",
      "Training loss for batch 491 : 0.4962137043476105\n",
      "Training loss for batch 492 : 0.12710125744342804\n",
      "Training loss for batch 493 : 0.13679324090480804\n",
      "Training loss for batch 494 : 0.14532707631587982\n",
      "Training loss for batch 495 : 0.028794944286346436\n",
      "Training loss for batch 496 : 0.16280511021614075\n",
      "Training loss for batch 497 : 0.3781608045101166\n",
      "Training loss for batch 498 : 0.032338909804821014\n",
      "Training loss for batch 499 : 0.4801512658596039\n",
      "Training loss for batch 500 : 0.17952367663383484\n",
      "Training loss for batch 501 : 0.3165248930454254\n",
      "Training loss for batch 502 : 0.2826392948627472\n",
      "Training loss for batch 503 : 0.1744055598974228\n",
      "Training loss for batch 504 : 0.6155350208282471\n",
      "Training loss for batch 505 : 0.6197333335876465\n",
      "Training loss for batch 506 : 0.0740136131644249\n",
      "Training loss for batch 507 : 0.049387361854314804\n",
      "Training loss for batch 508 : 0.6570852994918823\n",
      "Training loss for batch 509 : 0.23981235921382904\n",
      "Training loss for batch 510 : 0.056743819266557693\n",
      "Training loss for batch 511 : 0.23663346469402313\n",
      "Training loss for batch 512 : 0.11895471066236496\n",
      "Training loss for batch 513 : 0.4661352336406708\n",
      "Training loss for batch 514 : 0.24661079049110413\n",
      "Training loss for batch 515 : 0.18561625480651855\n",
      "Training loss for batch 516 : 0.3741086721420288\n",
      "Training loss for batch 517 : 0.4666004776954651\n",
      "Training loss for batch 518 : 0.06385192275047302\n",
      "Training loss for batch 519 : 0.5822578072547913\n",
      "Training loss for batch 520 : 0.38262635469436646\n",
      "Training loss for batch 521 : 0.04296375811100006\n",
      "Training loss for batch 522 : 0.0990639328956604\n",
      "Training loss for batch 523 : 0.12576010823249817\n",
      "Training loss for batch 524 : 0.20187769830226898\n",
      "Training loss for batch 525 : 0.005327239632606506\n",
      "Training loss for batch 526 : 0.05010299012064934\n",
      "Training loss for batch 527 : 0.24683599174022675\n",
      "Training loss for batch 528 : 0.1467423290014267\n",
      "Training loss for batch 529 : 0.07409776747226715\n",
      "Training loss for batch 530 : 0.5042787790298462\n",
      "Training loss for batch 531 : 0.5535931587219238\n",
      "Training loss for batch 532 : 0.5787179470062256\n",
      "Training loss for batch 533 : 0.21763825416564941\n",
      "Training loss for batch 534 : 0.19199976325035095\n",
      "Training loss for batch 535 : 0.25488799810409546\n",
      "Training loss for batch 536 : 0.20907558500766754\n",
      "Training loss for batch 537 : 0.19331546127796173\n",
      "Training loss for batch 538 : 0.1923782378435135\n",
      "Training loss for batch 539 : 0.2972528636455536\n",
      "Training loss for batch 540 : 0.08026093244552612\n",
      "Training loss for batch 541 : 0.05218613147735596\n",
      "Training loss for batch 542 : 0.48393091559410095\n",
      "Training loss for batch 543 : 0.2680805027484894\n",
      "Training loss for batch 544 : 0.25181907415390015\n",
      "Training loss for batch 545 : 0.8157362341880798\n",
      "Training loss for batch 546 : 0.34478309750556946\n",
      "Training loss for batch 547 : 0.32378432154655457\n",
      "Training loss for batch 548 : 0.08728602528572083\n",
      "Training loss for batch 549 : 0.4866725206375122\n",
      "Training loss for batch 550 : 0.4342518448829651\n",
      "Training loss for batch 551 : 0.4914320409297943\n",
      "Training loss for batch 552 : 0.32452502846717834\n",
      "Training loss for batch 553 : 0.05398077890276909\n",
      "Training loss for batch 554 : 0.3520742356777191\n",
      "Training loss for batch 555 : 0.11563041806221008\n",
      "Training loss for batch 556 : 0.27968841791152954\n",
      "Training loss for batch 557 : 0.3510335385799408\n",
      "Training loss for batch 558 : 0.4754793047904968\n",
      "Training loss for batch 559 : 0.032367900013923645\n",
      "Training loss for batch 560 : 0.1729741394519806\n",
      "Training loss for batch 561 : 0.104072704911232\n",
      "Training loss for batch 562 : 0.08731816709041595\n",
      "Training loss for batch 563 : 0.45948225259780884\n",
      "Training loss for batch 564 : 0.24656471610069275\n",
      "Training loss for batch 565 : 0.3268616497516632\n",
      "Training loss for batch 566 : 0.1366298943758011\n",
      "Training loss for batch 567 : 0.1362399458885193\n",
      "Training loss for batch 568 : 0.40571755170822144\n",
      "Training loss for batch 569 : 0.6250742673873901\n",
      "Training loss for batch 570 : 0.20603013038635254\n",
      "Training loss for batch 571 : 0.3708079159259796\n",
      "Training loss for batch 572 : 0.25529736280441284\n",
      "Training loss for batch 573 : 0.12818005681037903\n",
      "Training loss for batch 574 : 0.5972338318824768\n",
      "Training loss for batch 575 : 0.14081250131130219\n",
      "Training loss for batch 576 : 0.43897712230682373\n",
      "Training loss for batch 577 : 0.28799065947532654\n",
      "Training loss for batch 578 : 0.45289182662963867\n",
      "Training loss for batch 579 : 0.2812562882900238\n",
      "Training loss for batch 580 : 1.099777340888977\n",
      "Training loss for batch 581 : 0.4723066985607147\n",
      "Training loss for batch 582 : 0.14758659899234772\n",
      "Training loss for batch 583 : 0.4499921500682831\n",
      "Training loss for batch 584 : 0.1440049558877945\n",
      "Training loss for batch 585 : 0.4483594000339508\n",
      "Training loss for batch 586 : 0.15997816622257233\n",
      "Training loss for batch 587 : 0.09380783885717392\n",
      "Training loss for batch 588 : 0.5527749061584473\n",
      "Training loss for batch 589 : 0.6936477422714233\n",
      "Training loss for batch 590 : 0.20038212835788727\n",
      "Training loss for batch 591 : 0.16737638413906097\n",
      "Training loss for batch 592 : 0.12821905314922333\n",
      "Training loss for batch 593 : 0.23638740181922913\n",
      "Training loss for batch 594 : 0.6225482821464539\n",
      "Training loss for batch 595 : 0.3061392903327942\n",
      "Training loss for batch 596 : 0.8284594416618347\n",
      "Training loss for batch 597 : 0.1919787973165512\n",
      "Training loss for batch 598 : 0.3444655239582062\n",
      "Training loss for batch 599 : 0.275551438331604\n",
      "Training loss for batch 600 : 0.2690463364124298\n",
      "Training loss for batch 601 : 0.13913583755493164\n",
      "Training loss for batch 602 : 0.4446640610694885\n",
      "Training loss for batch 603 : 0.48536086082458496\n",
      "Training loss for batch 604 : 0.1468934565782547\n",
      "Training loss for batch 605 : 0.43753963708877563\n",
      "Training loss for batch 606 : 0.2708807587623596\n",
      "Training loss for batch 607 : 0.7024577260017395\n",
      "Training loss for batch 608 : 0.8560060858726501\n",
      "Training loss for batch 609 : 0.27752888202667236\n",
      "Training loss for batch 610 : 0.3426343500614166\n",
      "Training loss for batch 611 : 0.445523738861084\n",
      "Training loss for batch 612 : 0.19429805874824524\n",
      "Training loss for batch 613 : 0.583168089389801\n",
      "Training loss for batch 614 : 0.09151355177164078\n",
      "Training loss for batch 615 : 0.38961130380630493\n",
      "Training loss for batch 616 : 0.28985726833343506\n",
      "Training loss for batch 617 : 0.04759884253144264\n",
      "Training loss for batch 618 : 0.25397300720214844\n",
      "Training loss for batch 619 : 0.32842114567756653\n",
      "Training loss for batch 620 : 0.2726448178291321\n",
      "Training loss for batch 621 : 0.23317950963974\n",
      "Training loss for batch 622 : 0.3843209743499756\n",
      "Training loss for batch 623 : 0.44508659839630127\n",
      "Training loss for batch 624 : 0.4829724431037903\n",
      "Training loss for batch 625 : 0.4148801863193512\n",
      "Training loss for batch 626 : 0.03765173628926277\n",
      "Training loss for batch 627 : 0.10486653447151184\n",
      "Training loss for batch 628 : 0.33699437975883484\n",
      "Training loss for batch 629 : 0.39191770553588867\n",
      "Training loss for batch 630 : 0.38134992122650146\n",
      "Training loss for batch 631 : 0.3398785889148712\n",
      "Training loss for batch 632 : 0.2315884530544281\n",
      "Training loss for batch 633 : 0.48202604055404663\n",
      "Training loss for batch 634 : 0.1787356436252594\n",
      "Training loss for batch 635 : 0.15846140682697296\n",
      "Training loss for batch 636 : 0.5456509590148926\n",
      "Training loss for batch 637 : 0.07372947782278061\n",
      "Training loss for batch 638 : 0.3473726809024811\n",
      "Training loss for batch 639 : 0.22975844144821167\n",
      "Training loss for batch 640 : 0.3199523687362671\n",
      "Training loss for batch 641 : 0.22764530777931213\n",
      "Training loss for batch 642 : 0.347434937953949\n",
      "Training loss for batch 643 : 0.15090249478816986\n",
      "Training loss for batch 644 : 0.3962361812591553\n",
      "Training loss for batch 645 : 0.1986318826675415\n",
      "Training loss for batch 646 : 0.45820918679237366\n",
      "Training loss for batch 647 : 0.4297044575214386\n",
      "Training loss for batch 648 : 0.17593613266944885\n",
      "Training loss for batch 649 : 0.7519255876541138\n",
      "Training loss for batch 650 : 0.17964251339435577\n",
      "Training loss for batch 651 : 0.19769398868083954\n",
      "Training loss for batch 652 : 0.38030847907066345\n",
      "Training loss for batch 653 : 0.26888686418533325\n",
      "Training loss for batch 654 : 0.2630319595336914\n",
      "Training loss for batch 655 : 0.1150776594877243\n",
      "Training loss for batch 656 : 0.5362575054168701\n",
      "Training loss for batch 657 : 0.04045510292053223\n",
      "Training loss for batch 658 : 0.548259437084198\n",
      "Training loss for batch 659 : 0.4675886034965515\n",
      "Training loss for batch 660 : 0.32617902755737305\n",
      "Training loss for batch 661 : 0.3001534640789032\n",
      "Training loss for batch 662 : 0.07506787776947021\n",
      "Training loss for batch 663 : 0.3471478521823883\n",
      "Training loss for batch 664 : 0.36092880368232727\n",
      "Training loss for batch 665 : 0.1620904952287674\n",
      "Training loss for batch 666 : 0.16610245406627655\n",
      "Training loss for batch 667 : 0.5401762127876282\n",
      "Training loss for batch 668 : 0.2576219439506531\n",
      "Training loss for batch 669 : 0.16600629687309265\n",
      "Training loss for batch 670 : 0.30778029561042786\n",
      "Training loss for batch 671 : 0.2757223844528198\n",
      "Training loss for batch 672 : 0.1387827843427658\n",
      "Training loss for batch 673 : 0.5236269235610962\n",
      "Training loss for batch 674 : 0.2629823088645935\n",
      "Training loss for batch 675 : 0.3774280548095703\n",
      "Training loss for batch 676 : 0.24142737686634064\n",
      "Training loss for batch 677 : 0.1717126965522766\n",
      "Training loss for batch 678 : 0.4270922541618347\n",
      "Training loss for batch 679 : 0.4993721842765808\n",
      "Training loss for batch 680 : 0.3069423735141754\n",
      "Training loss for batch 681 : 0.09602650254964828\n",
      "Training loss for batch 682 : 0.41272789239883423\n",
      "Training loss for batch 683 : 0.5321841835975647\n",
      "Training loss for batch 684 : 0.23541301488876343\n",
      "Training loss for batch 685 : 0.27290433645248413\n",
      "Training loss for batch 686 : 0.28666675090789795\n",
      "Training loss for batch 687 : 0.7495957016944885\n",
      "Training loss for batch 688 : 0.15816190838813782\n",
      "Training loss for batch 689 : 0.45993199944496155\n",
      "Training loss for batch 690 : 0.18402886390686035\n",
      "Training loss for batch 691 : 0.4672905504703522\n",
      "Training loss for batch 692 : 0.624815821647644\n",
      "Training loss for batch 693 : 0.47255396842956543\n",
      "Training loss for batch 694 : 0.14674539864063263\n",
      "Training loss for batch 695 : 0.6386661529541016\n",
      "Training loss for batch 696 : 0.2774690091609955\n",
      "Training loss for batch 697 : 0.05279829353094101\n",
      "Training loss for batch 698 : 0.242649108171463\n",
      "Training loss for batch 699 : 0.15304654836654663\n",
      "Training loss for batch 700 : 0.059759799391031265\n",
      "Training loss for batch 701 : 0.41351139545440674\n",
      "Training loss for batch 702 : 0.2874627113342285\n",
      "Training loss for batch 703 : 0.4221365451812744\n",
      "Training loss for batch 704 : 0.05185170844197273\n",
      "Training loss for batch 705 : 0.17191337049007416\n",
      "Training loss for batch 706 : 0.1702600121498108\n",
      "Training loss for batch 707 : 0.12250255793333054\n",
      "Training loss for batch 708 : 0.3893020749092102\n",
      "Training loss for batch 709 : 0.209224671125412\n",
      "Training loss for batch 710 : 0.2445409744977951\n",
      "Training loss for batch 711 : 0.3162166178226471\n",
      "Training loss for batch 712 : 0.28748050332069397\n",
      "Training loss for batch 713 : 0.4477596879005432\n",
      "Training loss for batch 714 : 0.11126318573951721\n",
      "Training loss for batch 715 : 0.4964153468608856\n",
      "Training loss for batch 716 : 0.22590398788452148\n",
      "Training loss for batch 717 : 0.7113010287284851\n",
      "Training loss for batch 718 : 0.08029524236917496\n",
      "Training loss for batch 719 : 0.4170675575733185\n",
      "Training loss for batch 720 : 0.389606237411499\n",
      "Training loss for batch 721 : 0.13967344164848328\n",
      "Training loss for batch 722 : 0.5067429542541504\n",
      "Training loss for batch 723 : 0.2053254097700119\n",
      "Training loss for batch 724 : 0.13932150602340698\n",
      "Training loss for batch 725 : 0.5164793133735657\n",
      "Training loss for batch 726 : 0.3764110505580902\n",
      "Training loss for batch 727 : 0.2208341658115387\n",
      "Training loss for batch 728 : 0.1255803406238556\n",
      "Training loss for batch 729 : 0.5817697644233704\n",
      "Training loss for batch 730 : 0.28865498304367065\n",
      "Training loss for batch 731 : 0.30882230401039124\n",
      "Training loss for batch 732 : 0.40627580881118774\n",
      "Training loss for batch 733 : 0.19731324911117554\n",
      "Training loss for batch 734 : 0.22088707983493805\n",
      "Training loss for batch 735 : 0.47678521275520325\n",
      "Training loss for batch 736 : 0.22239449620246887\n",
      "Training loss for batch 737 : 0.20657803118228912\n",
      "Training loss for batch 738 : 0.08315755426883698\n",
      "Training loss for batch 739 : 0.14824903011322021\n",
      "Training loss for batch 740 : 0.3428984582424164\n",
      "Training loss for batch 741 : 0.05713411420583725\n",
      "Training loss for batch 742 : 0.132141575217247\n",
      "Training loss for batch 743 : 0.8027070164680481\n",
      "Training loss for batch 744 : 0.30419856309890747\n",
      "Training loss for batch 745 : 0.26487278938293457\n",
      "Training loss for batch 746 : 0.5999618172645569\n",
      "Training loss for batch 747 : 0.3152795732021332\n",
      "Training loss for batch 748 : 0.1483883112668991\n",
      "Training loss for batch 749 : 0.46324872970581055\n",
      "Training loss for batch 750 : 0.39486274123191833\n",
      "Training loss for batch 751 : 0.33578839898109436\n",
      "Training loss for batch 752 : 0.2773604393005371\n",
      "Training loss for batch 753 : 0.9402703046798706\n",
      "Training loss for batch 754 : 0.46339502930641174\n",
      "Training loss for batch 755 : 0.28878921270370483\n",
      "Training loss for batch 756 : 0.07131858170032501\n",
      "Training loss for batch 757 : 0.1288057565689087\n",
      "Training loss for batch 758 : 0.2878831624984741\n",
      "Training loss for batch 759 : 0.6859666705131531\n",
      "Training loss for batch 760 : 0.38546162843704224\n",
      "Training loss for batch 761 : 0.4726659059524536\n",
      "Training loss for batch 762 : 0.3163354694843292\n",
      "Training loss for batch 763 : 0.409127414226532\n",
      "Training loss for batch 764 : 0.30549436807632446\n",
      "Training loss for batch 765 : 0.33560895919799805\n",
      "Training loss for batch 766 : 0.5836093425750732\n",
      "Training loss for batch 767 : 0.1313411146402359\n",
      "Training loss for batch 768 : 0.35054901242256165\n",
      "Training loss for batch 769 : 0.44989970326423645\n",
      "Training loss for batch 770 : 0.5119081735610962\n",
      "Training loss for batch 771 : 0.2863110899925232\n",
      "Training loss for batch 772 : 0.14208433032035828\n",
      "Training loss for batch 773 : 0.06838986277580261\n",
      "Training loss for batch 774 : 0.5589966773986816\n",
      "Training loss for batch 775 : 0.24275636672973633\n",
      "Training loss for batch 776 : 0.47317442297935486\n",
      "Training loss for batch 777 : 0.30586671829223633\n",
      "Training loss for batch 778 : 0.024316370487213135\n",
      "Training loss for batch 779 : 0.20876897871494293\n",
      "Training loss for batch 780 : 0.2644176781177521\n",
      "Training loss for batch 781 : 0.5074172616004944\n",
      "Training loss for batch 782 : 0.3971071243286133\n",
      "Training loss for batch 783 : 0.6169564127922058\n",
      "Training loss for batch 784 : 0.20336313545703888\n",
      "Training loss for batch 785 : 0.5601611137390137\n",
      "Training loss for batch 786 : 0.21571505069732666\n",
      "Training loss for batch 787 : 0.3048979341983795\n",
      "Training loss for batch 788 : 0.3608372211456299\n",
      "Training loss for batch 789 : 0.1968376487493515\n",
      "Training loss for batch 790 : 0.39199960231781006\n",
      "Training loss for batch 791 : 0.48569536209106445\n",
      "Training loss for batch 792 : 0.2316875457763672\n",
      "Training loss for batch 793 : 0.27621251344680786\n",
      "Training loss for batch 794 : 0.18687668442726135\n",
      "Training loss for batch 795 : 0.145329087972641\n",
      "Training loss for batch 796 : 0.15632885694503784\n",
      "Training loss for batch 797 : 0.14134246110916138\n",
      "Training loss for batch 798 : 0.042495835572481155\n",
      "Training loss for batch 799 : 0.38432738184928894\n",
      "Training loss for batch 800 : 0.470544695854187\n",
      "Training loss for batch 801 : 0.19193023443222046\n",
      "Training loss for batch 802 : 0.4174095690250397\n",
      "Training loss for batch 803 : 0.0822780579328537\n",
      "Training loss for batch 804 : 0.33383724093437195\n",
      "Training loss for batch 805 : 0.41629892587661743\n",
      "Training loss for batch 806 : 0.7291737198829651\n",
      "Training loss for batch 807 : 0.4828319549560547\n",
      "Training loss for batch 808 : 0.12500549852848053\n",
      "Training loss for batch 809 : 0.20810531079769135\n",
      "Training loss for batch 810 : 0.5093520879745483\n",
      "Training loss for batch 811 : 0.6960551738739014\n",
      "Training loss for batch 812 : 0.23811709880828857\n",
      "Training loss for batch 813 : 0.08387057483196259\n",
      "Training loss for batch 814 : 0.6270070672035217\n",
      "Training loss for batch 815 : 0.28577643632888794\n",
      "Training loss for batch 816 : 0.2438543140888214\n",
      "Training loss for batch 817 : 0.34888941049575806\n",
      "Training loss for batch 818 : 0.3137143552303314\n",
      "Training loss for batch 819 : 0.47919049859046936\n",
      "Training loss for batch 820 : 0.2699008285999298\n",
      "Training loss for batch 821 : 0.31438061594963074\n",
      "Training loss for batch 822 : 0.2266552895307541\n",
      "Training loss for batch 823 : 0.3244422376155853\n",
      "Training loss for batch 824 : 0.10262972116470337\n",
      "Training loss for batch 825 : 0.2950356602668762\n",
      "Training loss for batch 826 : 0.43688708543777466\n",
      "Training loss for batch 827 : 0.09793046861886978\n",
      "Training loss for batch 828 : 0.2819688320159912\n",
      "Training loss for batch 829 : 0.0029141507111489773\n",
      "Training loss for batch 830 : 0.4799889922142029\n",
      "Training loss for batch 831 : 0.7383444309234619\n",
      "Training loss for batch 832 : 0.3493613004684448\n",
      "Training loss for batch 833 : 0.4699315130710602\n",
      "Training loss for batch 834 : 0.5305497646331787\n",
      "Training loss for batch 835 : 0.478255033493042\n",
      "Training loss for batch 836 : 1.3560759725805838e-05\n",
      "Training loss for batch 837 : 0.13834446668624878\n",
      "Training loss for batch 838 : 0.22509153187274933\n",
      "Training loss for batch 839 : 0.49653735756874084\n",
      "Training loss for batch 840 : 0.2252180278301239\n",
      "Training loss for batch 841 : 0.6414185762405396\n",
      "Training loss for batch 842 : 0.34173423051834106\n",
      "Training loss for batch 843 : 0.33397576212882996\n",
      "Training loss for batch 844 : 0.7219701409339905\n",
      "Training loss for batch 845 : 0.13390815258026123\n",
      "Training loss for batch 846 : 0.19145941734313965\n",
      "Training loss for batch 847 : 0.1320255547761917\n",
      "Training loss for batch 848 : 0.09150990843772888\n",
      "Training loss for batch 849 : 0.4808427393436432\n",
      "Training loss for batch 850 : 0.49270904064178467\n",
      "Training loss for batch 851 : 0.05463232845067978\n",
      "Training loss for batch 852 : 0.1695452183485031\n",
      "Training loss for batch 853 : 0.2040615677833557\n",
      "Training loss for batch 854 : 0.03391626477241516\n",
      "Training loss for batch 855 : 0.02659001015126705\n",
      "Training loss for batch 856 : 0.3855065703392029\n",
      "Training loss for batch 857 : 0.17655770480632782\n",
      "Training loss for batch 858 : 0.49468231201171875\n",
      "Training loss for batch 859 : 0.1320396363735199\n",
      "Training loss for batch 860 : 0.6180115342140198\n",
      "Training loss for batch 861 : 0.16777019202709198\n",
      "Training loss for batch 862 : 0.034689683467149734\n",
      "Training loss for batch 863 : 0.5661177039146423\n",
      "Training loss for batch 864 : 0.19972889125347137\n",
      "Training loss for batch 865 : 0.250283807516098\n",
      "Training loss for batch 866 : 0.4319269061088562\n",
      "Training loss for batch 867 : 0.6511526107788086\n",
      "Training loss for batch 868 : 0.577576220035553\n",
      "Training loss for batch 869 : 0.7367317080497742\n",
      "Training loss for batch 870 : 0.34687334299087524\n",
      "Training loss for batch 871 : 0.519716739654541\n",
      "Training loss for batch 872 : 0.3496955633163452\n",
      "Training loss for batch 873 : 0.02122640237212181\n",
      "Training loss for batch 874 : 0.36387723684310913\n",
      "Training loss for batch 875 : 0.26404765248298645\n",
      "Training loss for batch 876 : 0.2792905867099762\n",
      "Training loss for batch 877 : 0.4414918124675751\n",
      "Training loss for batch 878 : 0.26659074425697327\n",
      "Training loss for batch 879 : 0.32249724864959717\n",
      "Training loss for batch 880 : 0.4423288404941559\n",
      "Training loss for batch 881 : 0.10966726392507553\n",
      "Training loss for batch 882 : 0.587185263633728\n",
      "Training loss for batch 883 : 0.369351327419281\n",
      "Training loss for batch 884 : 0.1461038738489151\n",
      "Training loss for batch 885 : 0.1915251612663269\n",
      "Training loss for batch 886 : 0.20647142827510834\n",
      "Training loss for batch 887 : 0.20066362619400024\n",
      "Training loss for batch 888 : 0.07757174223661423\n",
      "Training loss for batch 889 : 0.28574466705322266\n",
      "Training loss for batch 890 : 0.18365032970905304\n",
      "Training loss for batch 891 : 0.2516343593597412\n",
      "Training loss for batch 892 : 0.14143463969230652\n",
      "Training loss for batch 893 : 0.8706814050674438\n",
      "Training loss for batch 894 : 0.024375902488827705\n",
      "Training loss for batch 895 : 0.33513736724853516\n",
      "Training loss for batch 896 : 0.6451446413993835\n",
      "Training loss for batch 897 : 0.5406927466392517\n",
      "Training loss for batch 898 : 0.437125563621521\n",
      "Training loss for batch 899 : 0.17382311820983887\n",
      "Training loss for batch 900 : 0.312778502702713\n",
      "Training loss for batch 901 : 0.5056283473968506\n",
      "Training loss for batch 902 : 0.26759102940559387\n",
      "Training loss for batch 903 : 0.0694349855184555\n",
      "Training loss for batch 904 : 0.3845122158527374\n",
      "Training loss for batch 905 : 0.45310521125793457\n",
      "Training loss for batch 906 : 0.24728882312774658\n",
      "Training loss for batch 907 : 0.3730904161930084\n",
      "Training loss for batch 908 : 0.3460956811904907\n",
      "Training loss for batch 909 : 0.15220193564891815\n",
      "Training loss for batch 910 : 0.3888172507286072\n",
      "Training loss for batch 911 : 0.4314984381198883\n",
      "Training loss for batch 912 : 0.03922279179096222\n",
      "Training loss for batch 913 : 0.33455100655555725\n",
      "Training loss for batch 914 : 0.0058023096062242985\n",
      "Training loss for batch 915 : 0.05551007762551308\n",
      "Training loss for batch 916 : 0.3854769468307495\n",
      "Training loss for batch 917 : 0.43287980556488037\n",
      "Training loss for batch 918 : 0.46026647090911865\n",
      "Training loss for batch 919 : 0.2709271311759949\n",
      "Training loss for batch 920 : 0.2492571473121643\n",
      "Training loss for batch 921 : 0.1813371628522873\n",
      "Training loss for batch 922 : 0.07577980309724808\n",
      "Training loss for batch 923 : 0.47735854983329773\n",
      "Training loss for batch 924 : 0.06846486032009125\n",
      "Training loss for batch 925 : 0.34233567118644714\n",
      "Training loss for batch 926 : 0.04024740681052208\n",
      "Training loss for batch 927 : 0.144376739859581\n",
      "Training loss for batch 928 : 0.5310022234916687\n",
      "Training loss for batch 929 : 0.3674384355545044\n",
      "Training loss for batch 930 : 0.3450416028499603\n",
      "Training loss for batch 931 : 0.9763097167015076\n",
      "Training loss for batch 932 : 0.527463436126709\n",
      "Training loss for batch 933 : 0.1396855115890503\n",
      "Training loss for batch 934 : 0.08638674020767212\n",
      "Training loss for batch 935 : 0.462721049785614\n",
      "Training loss for batch 936 : 0.8322535157203674\n",
      "Training loss for batch 937 : 0.46990299224853516\n",
      "Training loss for batch 938 : 0.04695199429988861\n",
      "Training loss for batch 939 : 0.4237358570098877\n",
      "Training loss for batch 940 : 0.09360922873020172\n",
      "Training loss for batch 941 : 0.2969638407230377\n",
      "Training loss for batch 942 : 0.36073169112205505\n",
      "Training loss for batch 943 : 0.6330846548080444\n",
      "Training loss for batch 944 : 0.3546585142612457\n",
      "Training loss for batch 945 : 0.2843955457210541\n",
      "Training loss for batch 946 : 0.2189333140850067\n",
      "Training loss for batch 947 : 0.13234186172485352\n",
      "Training loss for batch 948 : 0.27068257331848145\n",
      "Training loss for batch 949 : 0.16330711543560028\n",
      "Training loss for batch 950 : 0.2368023693561554\n",
      "Training loss for batch 951 : 0.04486815258860588\n",
      "Training loss for batch 952 : 0.1683352142572403\n",
      "Training loss for batch 953 : 0.6493783593177795\n",
      "Training loss for batch 954 : 0.24403074383735657\n",
      "Training loss for batch 955 : 0.2947522699832916\n",
      "Training loss for batch 956 : 0.2662299573421478\n",
      "Training loss for batch 957 : 0.1524668186903\n",
      "Training loss for batch 958 : 0.21371886134147644\n",
      "Training loss for batch 959 : 0.20471617579460144\n",
      "Training loss for batch 960 : 0.6079290509223938\n",
      "Training loss for batch 961 : 0.39080938696861267\n",
      "Training loss for batch 962 : 0.04984579235315323\n",
      "Training loss for batch 963 : 0.35763582587242126\n",
      "Training loss for batch 964 : 0.0750889927148819\n",
      "Training loss for batch 965 : 0.5452831387519836\n",
      "Training loss for batch 966 : 0.034960582852363586\n",
      "Training loss for batch 967 : 0.1447296291589737\n",
      "Training loss for batch 968 : 0.08229555934667587\n",
      "Training loss for batch 969 : 0.3882576525211334\n",
      "Training loss for batch 970 : 0.4098884165287018\n",
      "Training loss for batch 971 : 0.30501097440719604\n",
      "Training loss for batch 972 : 0.21771521866321564\n",
      "Training loss for batch 973 : 0.19304154813289642\n",
      "Training loss for batch 974 : 0.34879961609840393\n",
      "Training loss for batch 975 : 0.5106852054595947\n",
      "Training loss for batch 976 : 0.27443864941596985\n",
      "Training loss for batch 977 : 0.45978254079818726\n",
      "Training loss for batch 978 : 0.3912140130996704\n",
      "Training loss for batch 979 : 0.26472821831703186\n",
      "Training loss for batch 980 : 0.38162803649902344\n",
      "Training loss for batch 981 : 0.1548762172460556\n",
      "Training loss for batch 982 : 0.31279614567756653\n",
      "Training loss for batch 983 : 0.7267957925796509\n",
      "Training loss for batch 984 : 0.1441108137369156\n",
      "Training loss for batch 985 : 0.17052944004535675\n",
      "Training loss for batch 986 : 0.4857063293457031\n",
      "Training loss for batch 987 : 0.5721928477287292\n",
      "Training loss for batch 988 : 0.4833695888519287\n",
      "Training loss for batch 989 : 0.3077738583087921\n",
      "Training loss for batch 990 : 0.08366847783327103\n",
      "Training loss for batch 991 : 0.3536298871040344\n",
      "Training loss for batch 992 : 0.10042358189821243\n",
      "Training loss for batch 993 : 0.04556434601545334\n",
      "Training loss for batch 994 : 0.08301768451929092\n",
      "Training loss for batch 995 : 0.07845576107501984\n",
      "Training loss for batch 996 : 0.4673026502132416\n",
      "Training loss for batch 997 : 0.19411273300647736\n",
      "Training loss for batch 998 : 0.28911906480789185\n",
      "Training loss for batch 999 : 0.6019667387008667\n",
      "Training loss for batch 1000 : 0.19168801605701447\n",
      "Training loss for batch 1001 : 0.6075538992881775\n",
      "Training loss for batch 1002 : 0.15050163865089417\n",
      "Training loss for batch 1003 : 0.14030584692955017\n",
      "Training loss for batch 1004 : 0.4583929777145386\n",
      "Training loss for batch 1005 : 0.7886704206466675\n",
      "Training loss for batch 1006 : 0.08024224638938904\n",
      "Training loss for batch 1007 : 0.3072451651096344\n",
      "Training loss for batch 1008 : 0.6906177401542664\n",
      "Training loss for batch 1009 : 0.23368263244628906\n",
      "Training loss for batch 1010 : 0.020401624962687492\n",
      "Training loss for batch 1011 : 0.47362956404685974\n",
      "Training loss for batch 1012 : 0.7202802896499634\n",
      "Training loss for batch 1013 : 0.024129677563905716\n",
      "Training loss for batch 1014 : 0.10570105910301208\n",
      "Training loss for batch 1015 : 0.7855810523033142\n",
      "Training loss for batch 1016 : 0.005721107125282288\n",
      "Training loss for batch 1017 : 0.4732532501220703\n",
      "Training loss for batch 1018 : 0.15764999389648438\n",
      "Training loss for batch 1019 : 0.18825113773345947\n",
      "Training loss for batch 1020 : 0.8517428636550903\n",
      "Training loss for batch 1021 : 0.30726683139801025\n",
      "Training loss for batch 1022 : 0.10677269101142883\n",
      "Training loss for batch 1023 : 0.10789795964956284\n",
      "Training loss for batch 1024 : 0.007075670175254345\n",
      "Training loss for batch 1025 : 0.10456813871860504\n",
      "Training loss for batch 1026 : 0.3473706543445587\n",
      "Training loss for batch 1027 : 0.25078263878822327\n",
      "Training loss for batch 1028 : 0.26697659492492676\n",
      "Training loss for batch 1029 : 0.18640190362930298\n",
      "Training loss for batch 1030 : 0.24123142659664154\n",
      "Training loss for batch 1031 : 0.20122408866882324\n",
      "Training loss for batch 1032 : 0.21180428564548492\n",
      "Training loss for batch 1033 : 0.3760170638561249\n",
      "Training loss for batch 1034 : 0.5594367384910583\n",
      "Training loss for batch 1035 : 0.48209211230278015\n",
      "Training loss for batch 1036 : 0.4944306015968323\n",
      "Training loss for batch 1037 : 0.1803060621023178\n",
      "Training loss for batch 1038 : 0.12452573329210281\n",
      "Training loss for batch 1039 : 0.33616530895233154\n",
      "Training loss for batch 1040 : 0.22605004906654358\n",
      "Training loss for batch 1041 : 0.19658000767230988\n",
      "Training loss for batch 1042 : 0.1332119107246399\n",
      "Training loss for batch 1043 : 0.1901216208934784\n",
      "Training loss for batch 1044 : 0.10231047123670578\n",
      "Training loss for batch 1045 : 0.22837434709072113\n",
      "Training loss for batch 1046 : 0.5571051239967346\n",
      "Training loss for batch 1047 : 0.20027056336402893\n",
      "Training loss for batch 1048 : 0.1194109171628952\n",
      "Training loss for batch 1049 : 0.48038241267204285\n",
      "Training loss for batch 1050 : 0.8142841458320618\n",
      "Training loss for batch 1051 : 0.07476642727851868\n",
      "Training loss for batch 1052 : 0.19003191590309143\n",
      "Training loss for batch 1053 : 0.38967129588127136\n",
      "Training loss for batch 1054 : 0.09244275838136673\n",
      "Training loss for batch 1055 : 0.08300891518592834\n",
      "Training loss for batch 1056 : 0.4115077555179596\n",
      "Training loss for batch 1057 : 0.11145041882991791\n",
      "Training loss for batch 1058 : 0.18139895796775818\n",
      "Training loss for batch 1059 : 0.27372708916664124\n",
      "Training loss for batch 1060 : 0.19987989962100983\n",
      "Training loss for batch 1061 : 0.15171080827713013\n",
      "Training loss for batch 1062 : 0.4351002275943756\n",
      "Training loss for batch 1063 : 0.4197176396846771\n",
      "Training loss for batch 1064 : 0.3088587820529938\n",
      "Training loss for batch 1065 : 0.420402467250824\n",
      "Training loss for batch 1066 : 0.14683745801448822\n",
      "Training loss for batch 1067 : 0.12544971704483032\n",
      "Training loss for batch 1068 : 0.37079674005508423\n",
      "Training loss for batch 1069 : 0.23424041271209717\n",
      "Training loss for batch 1070 : 0.1478097140789032\n",
      "Training loss for batch 1071 : 0.35074764490127563\n",
      "Training loss for batch 1072 : 0.42420703172683716\n",
      "Training loss for batch 1073 : 0.17774122953414917\n",
      "Training loss for batch 1074 : 0.4782407283782959\n",
      "Training loss for batch 1075 : 0.0946747288107872\n",
      "Training loss for batch 1076 : 0.2968060374259949\n",
      "Training loss for batch 1077 : 0.5491814017295837\n",
      "Training loss for batch 1078 : 0.6329658031463623\n",
      "Training loss for batch 1079 : 0.4656520187854767\n",
      "Training loss for batch 1080 : 0.078243687748909\n",
      "Training loss for batch 1081 : 0.12623447179794312\n",
      "Training loss for batch 1082 : 0.09281209856271744\n",
      "Training loss for batch 1083 : 0.17042818665504456\n",
      "Training loss for batch 1084 : 0.4964790344238281\n",
      "Training loss for batch 1085 : 0.59075528383255\n",
      "Training loss for batch 1086 : 0.5176712870597839\n",
      "Training loss for batch 1087 : 0.19621916115283966\n",
      "Training loss for batch 1088 : 0.5653952956199646\n",
      "Training loss for batch 1089 : 0.26918333768844604\n",
      "Training loss for batch 1090 : 0.09662757813930511\n",
      "Training loss for batch 1091 : 0.5618612766265869\n",
      "Training loss for batch 1092 : 0.2977692484855652\n",
      "Training loss for batch 1093 : 0.2390107810497284\n",
      "Training loss for batch 1094 : 0.4832440912723541\n",
      "Training loss for batch 1095 : 0.4607730507850647\n",
      "Training loss for batch 1096 : 0.6015874147415161\n",
      "Training loss for batch 1097 : 0.16289043426513672\n",
      "Training loss for batch 1098 : 0.0382278747856617\n",
      "Training loss for batch 1099 : 0.6665980219841003\n",
      "Training loss for batch 1100 : 0.5878568291664124\n",
      "Training loss for batch 1101 : 0.15678671002388\n",
      "Training loss for batch 1102 : 0.19805839657783508\n",
      "Training loss for batch 1103 : 0.0921715721487999\n",
      "Training loss for batch 1104 : 0.42102766036987305\n",
      "Training loss for batch 1105 : 0.2154509276151657\n",
      "Training loss for batch 1106 : 0.714773952960968\n",
      "Training loss for batch 1107 : 0.5692180395126343\n",
      "Training loss for batch 1108 : 0.4151417315006256\n",
      "Training loss for batch 1109 : 0.7259548902511597\n",
      "Training loss for batch 1110 : 0.04985853657126427\n",
      "Training loss for batch 1111 : 0.06804532557725906\n",
      "Training loss for batch 1112 : 0.18220855295658112\n",
      "Training loss for batch 1113 : 0.30687379837036133\n",
      "Training loss for batch 1114 : 0.4485305845737457\n",
      "Training loss for batch 1115 : 0.30735620856285095\n",
      "Training loss for batch 1116 : 0.16010165214538574\n",
      "Training loss for batch 1117 : 0.5129989981651306\n",
      "Training loss for batch 1118 : 0.6414284706115723\n",
      "Training loss for batch 1119 : 0.2904379963874817\n",
      "Training loss for batch 1120 : 0.5072424411773682\n",
      "Training loss for batch 1121 : 0.21075986325740814\n",
      "Training loss for batch 1122 : 0.6177529096603394\n",
      "Training loss for batch 1123 : 0.1981504112482071\n",
      "Training loss for batch 1124 : 0.11329467594623566\n",
      "Training loss for batch 1125 : 0.088496632874012\n",
      "Training loss for batch 1126 : 0.1982133388519287\n",
      "Training loss for batch 1127 : 0.08934324234724045\n",
      "Training loss for batch 1128 : 0.20360687375068665\n",
      "Training loss for batch 1129 : 0.3003641366958618\n",
      "Training loss for batch 1130 : 0.2749944031238556\n",
      "Training loss for batch 1131 : 0.05898385867476463\n",
      "Training loss for batch 1132 : 0.2044273465871811\n",
      "Training loss for batch 1133 : 0.08449338376522064\n",
      "Training loss for batch 1134 : 0.14168420433998108\n",
      "Training loss for batch 1135 : 0.4381449520587921\n",
      "Training loss for batch 1136 : 0.47393837571144104\n",
      "Training loss for batch 1137 : 0.08595503866672516\n",
      "Training loss for batch 1138 : 0.1940012276172638\n",
      "Training loss for batch 1139 : 0.45289650559425354\n",
      "Training loss for batch 1140 : 0.45440471172332764\n",
      "Training loss for batch 1141 : 0.18165473639965057\n",
      "Training loss for batch 1142 : 0.06164389103651047\n",
      "Training loss for batch 1143 : 0.07161473482847214\n",
      "Training loss for batch 1144 : 0.4242400825023651\n",
      "Training loss for batch 1145 : 0.18291129171848297\n",
      "Training loss for batch 1146 : 0.05619535222649574\n",
      "Training loss for batch 1147 : 0.32268407940864563\n",
      "Training loss for batch 1148 : 0.2204263061285019\n",
      "Training loss for batch 1149 : 0.33605459332466125\n",
      "Training loss for batch 1150 : 0.13630451261997223\n",
      "Training loss for batch 1151 : 0.4917769730091095\n",
      "Training loss for batch 1152 : 0.39286553859710693\n",
      "Training loss for batch 1153 : 0.30298149585723877\n",
      "Training loss for batch 1154 : 0.537519097328186\n",
      "Training loss for batch 1155 : 0.26430967450141907\n",
      "Training loss for batch 1156 : 0.3199126124382019\n",
      "Training loss for batch 1157 : 0.17047080397605896\n",
      "Training loss for batch 1158 : 0.603423535823822\n",
      "Training loss for batch 1159 : 0.3790454566478729\n",
      "Training loss for batch 1160 : 0.10976794362068176\n",
      "Training loss for batch 1161 : 0.2773200273513794\n",
      "Training loss for batch 1162 : 0.20781897008419037\n",
      "Training loss for batch 1163 : 0.21423213183879852\n",
      "Training loss for batch 1164 : 0.06241833046078682\n",
      "Training loss for batch 1165 : 0.5586897134780884\n",
      "Training loss for batch 1166 : 0.04968094080686569\n",
      "Training loss for batch 1167 : 0.15012310445308685\n",
      "Training loss for batch 1168 : 0.050203852355480194\n",
      "Training loss for batch 1169 : 0.061850108206272125\n",
      "Training loss for batch 1170 : 0.4690272808074951\n",
      "Training loss for batch 1171 : 0.3174859285354614\n",
      "Training loss for batch 1172 : 0.1582126021385193\n",
      "Training loss for batch 1173 : 0.023692339658737183\n",
      "Training loss for batch 1174 : 0.2203933149576187\n",
      "Training loss for batch 1175 : 0.6462401747703552\n",
      "Training loss for batch 1176 : 0.2384960651397705\n",
      "Training loss for batch 1177 : 0.19679492712020874\n",
      "Training loss for batch 1178 : 0.38442784547805786\n",
      "Training loss for batch 1179 : 0.03928219899535179\n",
      "Training loss for batch 1180 : 0.35431742668151855\n",
      "Training loss for batch 1181 : 0.07220600545406342\n",
      "Training loss for batch 1182 : 0.287870317697525\n",
      "Training loss for batch 1183 : 0.2303583323955536\n",
      "Training loss for batch 1184 : 0.19164393842220306\n",
      "Training loss for batch 1185 : 0.062288764864206314\n",
      "Training loss for batch 1186 : 0.17187754809856415\n",
      "Training loss for batch 1187 : 0.05105675384402275\n",
      "Training loss for batch 1188 : 0.5529129505157471\n",
      "Training loss for batch 1189 : 0.3675176203250885\n",
      "Training loss for batch 1190 : 0.7233251333236694\n",
      "Training loss for batch 1191 : 0.2088969200849533\n",
      "Training loss for batch 1192 : 0.6043691635131836\n",
      "Training loss for batch 1193 : 0.4314172565937042\n",
      "Training loss for batch 1194 : 0.12656843662261963\n",
      "Training loss for batch 1195 : 0.13298003375530243\n",
      "Training loss for batch 1196 : 0.06975375860929489\n",
      "Training loss for batch 1197 : 0.4976198971271515\n",
      "Training loss for batch 1198 : 0.20093289017677307\n",
      "Training loss for batch 1199 : 0.1943332850933075\n",
      "Training loss for batch 1200 : 0.22967016696929932\n",
      "Training loss for batch 1201 : 0.5303151607513428\n",
      "Training loss for batch 1202 : 0.5241840481758118\n",
      "Training loss for batch 1203 : 0.18670274317264557\n",
      "Training loss for batch 1204 : 0.34003981947898865\n",
      "Training loss for batch 1205 : 0.15696729719638824\n",
      "Training loss for batch 1206 : 0.5765696167945862\n",
      "Training loss for batch 1207 : 0.6661555767059326\n",
      "Training loss for batch 1208 : 0.1432764232158661\n",
      "Training loss for batch 1209 : 0.2199123501777649\n",
      "Training loss for batch 1210 : 0.04909737408161163\n",
      "Training loss for batch 1211 : 0.26202791929244995\n",
      "Training loss for batch 1212 : 0.1857270449399948\n",
      "Training loss for batch 1213 : 0.20753218233585358\n",
      "Training loss for batch 1214 : 0.011028829962015152\n",
      "Training loss for batch 1215 : 0.7656177878379822\n",
      "Training loss for batch 1216 : 0.028392717242240906\n",
      "Training loss for batch 1217 : 0.09553194791078568\n",
      "Training loss for batch 1218 : 0.43968889117240906\n",
      "Training loss for batch 1219 : 0.22441627085208893\n",
      "Training loss for batch 1220 : 0.22322741150856018\n",
      "Training loss for batch 1221 : 0.6421685814857483\n",
      "Training loss for batch 1222 : 0.18411651253700256\n",
      "Training loss for batch 1223 : 0.6565914154052734\n",
      "Training loss for batch 1224 : 0.13455723226070404\n",
      "Training loss for batch 1225 : 0.3474690318107605\n",
      "Training loss for batch 1226 : 0.28433820605278015\n",
      "Training loss for batch 1227 : 0.004497180692851543\n",
      "Training loss for batch 1228 : 0.2867724895477295\n",
      "Training loss for batch 1229 : 0.4330972731113434\n",
      "Training loss for batch 1230 : 0.35375067591667175\n",
      "Training loss for batch 1231 : 0.2767299711704254\n",
      "Training loss for batch 1232 : 0.11400660872459412\n",
      "Training loss for batch 1233 : 0.28566065430641174\n",
      "Training loss for batch 1234 : 0.1651940643787384\n",
      "Training loss for batch 1235 : 0.1049574762582779\n",
      "Training loss for batch 1236 : 0.3092367947101593\n",
      "Training loss for batch 1237 : 0.31903260946273804\n",
      "Training loss for batch 1238 : 0.22492051124572754\n",
      "Training loss for batch 1239 : 0.5045809149742126\n",
      "Training loss for batch 1240 : 0.514659583568573\n",
      "Training loss for batch 1241 : 0.10089895129203796\n",
      "Training loss for batch 1242 : 0.6706552505493164\n",
      "Training loss for batch 1243 : 0.05486030876636505\n",
      "Training loss for batch 1244 : 0.22433894872665405\n",
      "Training loss for batch 1245 : 0.13399577140808105\n",
      "Training loss for batch 1246 : 0.5687860250473022\n",
      "Training loss for batch 1247 : 0.17346088588237762\n",
      "Training loss for batch 1248 : 0.11838024854660034\n",
      "Training loss for batch 1249 : 0.5744447112083435\n",
      "Training loss for batch 1250 : 0.5861757397651672\n",
      "Training loss for batch 1251 : 0.3905487060546875\n",
      "Training loss for batch 1252 : 0.27456262707710266\n",
      "Training loss for batch 1253 : 0.17153982818126678\n",
      "Training loss for batch 1254 : 0.6995454430580139\n",
      "Training loss for batch 1255 : 0.21444444358348846\n",
      "Training loss for batch 1256 : 0.04643905535340309\n",
      "Training loss for batch 1257 : 0.19245348870754242\n",
      "Training loss for batch 1258 : 0.5319141745567322\n",
      "Training loss for batch 1259 : 0.29989126324653625\n",
      "Training loss for batch 1260 : 0.4775325357913971\n",
      "Training loss for batch 1261 : 0.5737885236740112\n",
      "Training loss for batch 1262 : 0.33280256390571594\n",
      "Training loss for batch 1263 : 0.5895786285400391\n",
      "Training loss for batch 1264 : 0.5335207581520081\n",
      "Training loss for batch 1265 : 0.27133816480636597\n",
      "Training loss for batch 1266 : 0.24113038182258606\n",
      "Training loss for batch 1267 : 0.3396330177783966\n",
      "Training loss for batch 1268 : 0.5878678560256958\n",
      "Training loss for batch 1269 : 0.20829375088214874\n",
      "Training loss for batch 1270 : 0.40218040347099304\n",
      "Training loss for batch 1271 : 0.4958096444606781\n",
      "Training loss for batch 1272 : 0.2956332564353943\n",
      "Training loss for batch 1273 : 0.14753422141075134\n",
      "Training loss for batch 1274 : 0.15888859331607819\n",
      "Training loss for batch 1275 : 0.06863321363925934\n",
      "Training loss for batch 1276 : 0.16313020884990692\n",
      "Training loss for batch 1277 : 0.509520411491394\n",
      "Training loss for batch 1278 : 0.29334893822669983\n",
      "Training loss for batch 1279 : 0.3852766752243042\n",
      "Training loss for batch 1280 : 0.0742773786187172\n",
      "Training loss for batch 1281 : 0.457621306180954\n",
      "Training loss for batch 1282 : 0.3419407606124878\n",
      "Training loss for batch 1283 : 0.12716275453567505\n",
      "Training loss for batch 1284 : 0.43515318632125854\n",
      "Training loss for batch 1285 : 0.4109269678592682\n",
      "Training loss for batch 1286 : 0.49787959456443787\n",
      "Training loss for batch 1287 : 0.13659049570560455\n",
      "Training loss for batch 1288 : 0.39377617835998535\n",
      "Training loss for batch 1289 : 0.2750980257987976\n",
      "Training loss for batch 1290 : 0.474777489900589\n",
      "Training loss for batch 1291 : 0.1468234807252884\n",
      "Training loss for batch 1292 : 0.31313028931617737\n",
      "Training loss for batch 1293 : 0.2897797226905823\n",
      "Training loss for batch 1294 : 0.20772969722747803\n",
      "Training loss for batch 1295 : 0.3814021944999695\n",
      "Training loss for batch 1296 : 0.23325100541114807\n",
      "Training loss for batch 1297 : 0.05787912756204605\n",
      "Training loss for batch 1298 : 0.2939191460609436\n",
      "Training loss for batch 1299 : 0.2500583529472351\n",
      "Training loss for batch 1300 : 0.10927517712116241\n",
      "Training loss for batch 1301 : 0.34458228945732117\n",
      "Training loss for batch 1302 : 0.34858641028404236\n",
      "Training loss for batch 1303 : 0.12434443831443787\n",
      "Training loss for batch 1304 : 0.41549086570739746\n",
      "Training loss for batch 1305 : 0.07946300506591797\n",
      "Training loss for batch 1306 : 0.07836413383483887\n",
      "Training loss for batch 1307 : 0.09350650012493134\n",
      "Training loss for batch 1308 : 0.14572490751743317\n",
      "Training loss for batch 1309 : 0.2663860619068146\n",
      "Training loss for batch 1310 : 0.3975541293621063\n",
      "Training loss for batch 1311 : 0.04292892664670944\n",
      "Training loss for batch 1312 : 0.5301125049591064\n",
      "Training loss for batch 1313 : 0.1103767603635788\n",
      "Training loss for batch 1314 : 0.4405645728111267\n",
      "Training loss for batch 1315 : 0.31625688076019287\n",
      "Training loss for batch 1316 : 0.21062687039375305\n",
      "Training loss for batch 1317 : 0.301555335521698\n",
      "Training loss for batch 1318 : 0.37961217761039734\n",
      "Training loss for batch 1319 : 0.07739393413066864\n",
      "Training loss for batch 1320 : 0.23473918437957764\n",
      "Training loss for batch 1321 : 0.2543172538280487\n",
      "Training loss for batch 1322 : 0.5684815645217896\n",
      "Training loss for batch 1323 : 0.18630848824977875\n",
      "Training loss for batch 1324 : 0.14389543235301971\n",
      "Training loss for batch 1325 : 0.2947828471660614\n",
      "Training loss for batch 1326 : 0.0764036625623703\n",
      "Training loss for batch 1327 : 0.6306886076927185\n",
      "Training loss for batch 1328 : 0.1995300054550171\n",
      "Training loss for batch 1329 : 0.1041584312915802\n",
      "Training loss for batch 1330 : 0.08078476041555405\n",
      "Training loss for batch 1331 : 0.12124048173427582\n",
      "Training loss for batch 1332 : 0.26575398445129395\n",
      "Training loss for batch 1333 : 0.5112888813018799\n",
      "Training loss for batch 1334 : 0.6122128963470459\n",
      "Training loss for batch 1335 : 0.10209397226572037\n",
      "Training loss for batch 1336 : 0.19415375590324402\n",
      "Training loss for batch 1337 : 0.40565088391304016\n",
      "Training loss for batch 1338 : 0.5535919666290283\n",
      "Training loss for batch 1339 : 0.2382579892873764\n",
      "Training loss for batch 1340 : 0.5107870101928711\n",
      "Training loss for batch 1341 : 0.17304934561252594\n",
      "Training loss for batch 1342 : 0.012313341721892357\n",
      "Training loss for batch 1343 : 0.1614491045475006\n",
      "Training loss for batch 1344 : 0.17850923538208008\n",
      "Training loss for batch 1345 : 0.7314635515213013\n",
      "Training loss for batch 1346 : 0.07983182370662689\n",
      "Training loss for batch 1347 : 0.32028552889823914\n",
      "Training loss for batch 1348 : 0.0931519940495491\n",
      "Training loss for batch 1349 : 0.48299723863601685\n",
      "Training loss for batch 1350 : 0.0870649591088295\n",
      "Training loss for batch 1351 : 0.31784024834632874\n",
      "Training loss for batch 1352 : 0.522797703742981\n",
      "Training loss for batch 1353 : 0.4571075737476349\n",
      "Training loss for batch 1354 : 0.14578533172607422\n",
      "Training loss for batch 1355 : 0.1354467272758484\n",
      "Training loss for batch 1356 : 0.4109067916870117\n",
      "Training loss for batch 1357 : 0.4428950250148773\n",
      "Training loss for batch 1358 : 0.5743426084518433\n",
      "Training loss for batch 1359 : 0.5487911105155945\n",
      "Training loss for batch 1360 : 0.10473985970020294\n",
      "Training loss for batch 1361 : 0.6909667253494263\n",
      "Training loss for batch 1362 : 0.2721724510192871\n",
      "Training loss for batch 1363 : 0.056459829211235046\n",
      "Training loss for batch 1364 : 0.34617793560028076\n",
      "Training loss for batch 1365 : 1.1242704391479492\n",
      "Training loss for batch 1366 : 0.4791795611381531\n",
      "Training loss for batch 1367 : 0.013948296196758747\n",
      "Training loss for batch 1368 : 0.3195773661136627\n",
      "Training loss for batch 1369 : 0.15895624458789825\n",
      "Training loss for batch 1370 : 0.71543288230896\n",
      "Training loss for batch 1371 : 0.5289804935455322\n",
      "Training loss for batch 1372 : 0.27137625217437744\n",
      "Training loss for batch 1373 : 0.22605453431606293\n",
      "Training loss for batch 1374 : 0.06721675395965576\n",
      "Training loss for batch 1375 : 0.6679802536964417\n",
      "Training loss for batch 1376 : 0.187686949968338\n",
      "Training loss for batch 1377 : 0.14789006114006042\n",
      "Training loss for batch 1378 : 0.841727614402771\n",
      "Training loss for batch 1379 : 0.3550378680229187\n",
      "Training loss for batch 1380 : 0.021777933463454247\n",
      "Training loss for batch 1381 : 0.06782854348421097\n",
      "Training loss for batch 1382 : 0.1845468133687973\n",
      "Training loss for batch 1383 : 0.43745943903923035\n",
      "Training loss for batch 1384 : 0.7109841704368591\n",
      "Training loss for batch 1385 : 0.5449877977371216\n",
      "Training loss for batch 1386 : 0.34267738461494446\n",
      "Training loss for batch 1387 : 0.24739013612270355\n",
      "Training loss for batch 1388 : 0.860177755355835\n",
      "Training loss for batch 1389 : 0.24846380949020386\n",
      "Training loss for batch 1390 : 0.1250377744436264\n",
      "Training loss for batch 1391 : 0.37275704741477966\n",
      "Training loss for batch 1392 : 0.17171046137809753\n",
      "Training loss for batch 1393 : 0.3093489110469818\n",
      "Training loss for batch 1394 : 0.3496668338775635\n",
      "Training loss for batch 1395 : 0.44004955887794495\n",
      "Training loss for batch 1396 : 0.219823956489563\n",
      "Training loss for batch 1397 : 0.1588864028453827\n",
      "Training loss for batch 1398 : 0.3257710039615631\n",
      "Training loss for batch 1399 : 0.2442842572927475\n",
      "Training loss for batch 1400 : 0.35441744327545166\n",
      "Training loss for batch 1401 : 0.30654647946357727\n",
      "Training loss for batch 1402 : 0.05480072647333145\n",
      "Training loss for batch 1403 : 0.2111748307943344\n",
      "Training loss for batch 1404 : 0.33158156275749207\n",
      "Training loss for batch 1405 : 0.25879693031311035\n",
      "Training loss for batch 1406 : 0.18519751727581024\n",
      "Training loss for batch 1407 : 0.3474370241165161\n",
      "Training loss for batch 1408 : 0.25949159264564514\n",
      "Training loss for batch 1409 : 0.21410401165485382\n",
      "Training loss for batch 1410 : 0.09232308715581894\n",
      "Training loss for batch 1411 : 0.5068681240081787\n",
      "Training loss for batch 1412 : 0.2667498290538788\n",
      "Training loss for batch 1413 : 0.5084073543548584\n",
      "Training loss for batch 1414 : 0.4895934462547302\n",
      "Training loss for batch 1415 : 0.6874145269393921\n",
      "Training loss for batch 1416 : 0.18724149465560913\n",
      "Training loss for batch 1417 : 0.07607512176036835\n",
      "Training loss for batch 1418 : 0.1538931131362915\n",
      "Training loss for batch 1419 : 0.031465642154216766\n",
      "Training loss for batch 1420 : 0.23798176646232605\n",
      "Training loss for batch 1421 : 0.38014093041419983\n",
      "Training loss for batch 1422 : 0.7582146525382996\n",
      "Training loss for batch 1423 : 0.07203608006238937\n",
      "Training loss for batch 1424 : 0.3518446087837219\n",
      "Training loss for batch 1425 : 0.23599295318126678\n",
      "Training loss for batch 1426 : 0.41419267654418945\n",
      "Training loss for batch 1427 : 0.4786716103553772\n",
      "Training loss for batch 1428 : 0.5363363027572632\n",
      "Training loss for batch 1429 : 0.2517421245574951\n",
      "Training loss for batch 1430 : 0.15855658054351807\n",
      "Training loss for batch 1431 : 0.3971831798553467\n",
      "Training loss for batch 1432 : 0.3412078320980072\n",
      "Training loss for batch 1433 : 0.2790129780769348\n",
      "Training loss for batch 1434 : 0.23875659704208374\n",
      "Training loss for batch 1435 : 0.518515944480896\n",
      "Training loss for batch 1436 : 0.4856009781360626\n",
      "Training loss for batch 1437 : 0.7681972980499268\n",
      "Training loss for batch 1438 : 0.4066547751426697\n",
      "Training loss for batch 1439 : 0.28093013167381287\n",
      "Training loss for batch 1440 : 0.09242922812700272\n",
      "Training loss for batch 1441 : 0.1278003603219986\n",
      "Training loss for batch 1442 : 0.3529718816280365\n",
      "Training loss for batch 1443 : 0.13758379220962524\n",
      "Training loss for batch 1444 : 0.21527786552906036\n",
      "Training loss for batch 1445 : 0.1881999373435974\n",
      "Training loss for batch 1446 : 0.25542208552360535\n",
      "Training loss for batch 1447 : 0.6158180236816406\n",
      "Training loss for batch 1448 : 0.2600893974304199\n",
      "Training loss for batch 1449 : 0.5348979234695435\n",
      "Training loss for batch 1450 : 0.555385947227478\n",
      "Training loss for batch 1451 : 0.2194000780582428\n",
      "Training loss for batch 1452 : 0.0738745629787445\n",
      "Training loss for batch 1453 : 0.022398080676794052\n",
      "Training loss for batch 1454 : 0.16898491978645325\n",
      "Training loss for batch 1455 : 0.1594005972146988\n",
      "Training loss for batch 1456 : 0.5280717611312866\n",
      "Training loss for batch 1457 : 0.06817793846130371\n",
      "Training loss for batch 1458 : 0.4815135896205902\n",
      "Training loss for batch 1459 : 0.36293086409568787\n",
      "Training loss for batch 1460 : 0.3272603154182434\n",
      "Training loss for batch 1461 : 0.09350638091564178\n",
      "Training loss for batch 1462 : 0.354400634765625\n",
      "Training loss for batch 1463 : 0.3945416808128357\n",
      "Training loss for batch 1464 : 0.18318377435207367\n",
      "Training loss for batch 1465 : 0.03221621364355087\n",
      "Training loss for batch 1466 : 0.2733386158943176\n",
      "Training loss for batch 1467 : 0.14949968457221985\n",
      "Training loss for batch 1468 : 0.2530812919139862\n",
      "Training loss for batch 1469 : 0.36568692326545715\n",
      "Training loss for batch 1470 : 0.1069076657295227\n",
      "Training loss for batch 1471 : 0.23081029951572418\n",
      "Training loss for batch 1472 : 0.1978258490562439\n",
      "Training loss for batch 1473 : 0.18813693523406982\n",
      "Training loss for batch 1474 : 0.11408025771379471\n",
      "Training loss for batch 1475 : 0.14258714020252228\n",
      "Training loss for batch 1476 : 0.6456803679466248\n",
      "Training loss for batch 1477 : 0.5302997827529907\n",
      "Training loss for batch 1478 : 0.019271599128842354\n",
      "Training loss for batch 1479 : 0.6536986827850342\n",
      "Training loss for batch 1480 : 0.2825053334236145\n",
      "Training loss for batch 1481 : 0.9745675325393677\n",
      "Training loss for batch 1482 : 0.19271932542324066\n",
      "Training loss for batch 1483 : 1.145370364189148\n",
      "Training loss for batch 1484 : 0.5593624114990234\n",
      "Training loss for batch 1485 : 0.3430989384651184\n",
      "Training loss for batch 1486 : 0.3579959273338318\n",
      "Training loss for batch 1487 : 0.4576404094696045\n",
      "Training loss for batch 1488 : 0.3197217881679535\n",
      "Training loss for batch 1489 : 0.1717449575662613\n",
      "Training loss for batch 1490 : 0.4491269886493683\n",
      "Training loss for batch 1491 : 0.16435404121875763\n",
      "Training loss for batch 1492 : 0.10896773636341095\n",
      "Training loss for batch 1493 : 0.3896024227142334\n",
      "Training loss for batch 1494 : 0.10385052114725113\n",
      "Training loss for batch 1495 : 0.16997072100639343\n",
      "Training loss for batch 1496 : 0.8236448764801025\n",
      "Training loss for batch 1497 : 0.09346253424882889\n",
      "Training loss for batch 1498 : 0.19246964156627655\n",
      "Training loss for batch 1499 : 0.030053408816456795\n",
      "Training loss for batch 1500 : 0.09331312030553818\n",
      "Training loss for batch 1501 : 0.006522645242512226\n",
      "Training loss for batch 1502 : 0.3010134994983673\n",
      "Training loss for batch 1503 : 0.33184993267059326\n",
      "Training loss for batch 1504 : 0.6235673427581787\n",
      "Training loss for batch 1505 : 0.07899731397628784\n",
      "Training loss for batch 1506 : 0.3110644817352295\n",
      "Training loss for batch 1507 : 0.2715882360935211\n",
      "Training loss for batch 1508 : 0.2565237581729889\n",
      "Training loss for batch 1509 : 0.1260693371295929\n",
      "Training loss for batch 1510 : 0.09733826667070389\n",
      "Training loss for batch 1511 : 0.8577618598937988\n",
      "Training loss for batch 1512 : 0.44557783007621765\n",
      "Training loss for batch 1513 : 0.44622480869293213\n",
      "Training loss for batch 1514 : 0.2814687192440033\n",
      "Training loss for batch 1515 : 0.20035339891910553\n",
      "Training loss for batch 1516 : 0.2063332498073578\n",
      "Training loss for batch 1517 : 0.37221312522888184\n",
      "Training loss for batch 1518 : 0.42123842239379883\n",
      "Training loss for batch 1519 : 0.6967870593070984\n",
      "Training loss for batch 1520 : 0.2891567051410675\n",
      "Training loss for batch 1521 : 0.047733161598443985\n",
      "Training loss for batch 1522 : 0.23901021480560303\n",
      "Training loss for batch 1523 : 0.15034694969654083\n",
      "Training loss for batch 1524 : 0.12231049686670303\n",
      "Training loss for batch 1525 : 0.34930309653282166\n",
      "Training loss for batch 1526 : 0.2227591872215271\n",
      "Training loss for batch 1527 : 0.3347979784011841\n",
      "Training loss for batch 1528 : 0.11485594511032104\n",
      "Training loss for batch 1529 : 0.42992109060287476\n",
      "Training loss for batch 1530 : 0.05571170523762703\n",
      "Training loss for batch 1531 : 0.2645506262779236\n",
      "Training loss for batch 1532 : 0.1452745497226715\n",
      "Training loss for batch 1533 : 0.12037230283021927\n",
      "Training loss for batch 1534 : 0.47163137793540955\n",
      "Training loss for batch 1535 : 0.10073923319578171\n",
      "Training loss for batch 1536 : 0.43348270654678345\n",
      "Training loss for batch 1537 : 0.02642575278878212\n",
      "Training loss for batch 1538 : 0.45956695079803467\n",
      "Training loss for batch 1539 : 0.059783753007650375\n",
      "Training loss for batch 1540 : 0.38285258412361145\n",
      "Training loss for batch 1541 : 0.3391740024089813\n",
      "Training loss for batch 1542 : 0.08533091843128204\n",
      "Training loss for batch 1543 : 0.5260431170463562\n",
      "Training loss for batch 1544 : 0.005155566614121199\n",
      "Training loss for batch 1545 : 0.1536853015422821\n",
      "Training loss for batch 1546 : 0.2048751562833786\n",
      "Training loss for batch 1547 : 0.12964127957820892\n",
      "Training loss for batch 1548 : 0.6312674283981323\n",
      "Training loss for batch 1549 : 0.1670781672000885\n",
      "Training loss for batch 1550 : 0.3573743402957916\n",
      "Training loss for batch 1551 : 0.25940021872520447\n",
      "Training loss for batch 1552 : 0.47227251529693604\n",
      "Training loss for batch 1553 : 0.26212650537490845\n",
      "Training loss for batch 1554 : 0.2782894968986511\n",
      "Training loss for batch 1555 : 0.1654764860868454\n",
      "Training loss for batch 1556 : 0.653242826461792\n",
      "Training loss for batch 1557 : 0.7845926284790039\n",
      "Training loss for batch 1558 : 0.2358226627111435\n",
      "Training loss for batch 1559 : 0.15748535096645355\n",
      "Training loss for batch 1560 : 0.7438075542449951\n",
      "Training loss for batch 1561 : 0.6266114711761475\n",
      "Training loss for batch 1562 : 0.2673194706439972\n",
      "Training loss for batch 1563 : 0.3587442636489868\n",
      "Training loss for batch 1564 : 0.4573708772659302\n",
      "Training loss for batch 1565 : 0.09362252056598663\n",
      "Training loss for batch 1566 : 0.1467239409685135\n",
      "Training loss for batch 1567 : 0.5483773350715637\n",
      "Training loss for batch 1568 : 0.38360822200775146\n",
      "Training loss for batch 1569 : 0.27086880803108215\n",
      "Training loss for batch 1570 : 0.14260835945606232\n",
      "Training loss for batch 1571 : 0.7312533855438232\n",
      "Training loss for batch 1572 : 0.6176860928535461\n",
      "Training loss for batch 1573 : 0.1425381302833557\n",
      "Training loss for batch 1574 : 0.4434017837047577\n",
      "Training loss for batch 1575 : 0.301981657743454\n",
      "Training loss for batch 1576 : 0.21417909860610962\n",
      "Training loss for batch 1577 : 0.5490723252296448\n",
      "Training loss for batch 1578 : 0.30923783779144287\n",
      "Training loss for batch 1579 : 0.35059207677841187\n",
      "Training loss for batch 1580 : 0.09080737829208374\n",
      "Training loss for batch 1581 : 0.3261750042438507\n",
      "Training loss for batch 1582 : 0.45191752910614014\n",
      "Training loss for batch 1583 : 0.08794065564870834\n",
      "Training loss for batch 1584 : 0.37612929940223694\n",
      "Training loss for batch 1585 : 0.30959370732307434\n",
      "Training loss for batch 1586 : 0.3134893476963043\n",
      "Training loss for batch 1587 : 0.4290768504142761\n",
      "Training loss for batch 1588 : 0.06286391615867615\n",
      "Training loss for batch 1589 : 0.3604308068752289\n",
      "Training loss for batch 1590 : 0.2542758285999298\n",
      "Training loss for batch 1591 : 0.5074270367622375\n",
      "Training loss for batch 1592 : 0.37720203399658203\n",
      "Training loss for batch 1593 : 0.3034176230430603\n",
      "Training loss for batch 1594 : 0.2981342077255249\n",
      "Training loss for batch 1595 : 0.2651962339878082\n",
      "Training loss for batch 1596 : 0.2194996476173401\n",
      "Training loss for batch 1597 : 0.1235380470752716\n",
      "Training loss for batch 1598 : 0.19156944751739502\n",
      "Training loss for batch 1599 : 0.09752878546714783\n",
      "Training loss for batch 1600 : 0.030296524986624718\n",
      "Training loss for batch 1601 : 0.1202993392944336\n",
      "Training loss for batch 1602 : 0.1349918246269226\n",
      "Training loss for batch 1603 : 0.41258999705314636\n",
      "Training loss for batch 1604 : 0.37220239639282227\n",
      "Training loss for batch 1605 : 0.09155269712209702\n",
      "Training loss for batch 1606 : 0.28533804416656494\n",
      "Training loss for batch 1607 : 0.1808282434940338\n",
      "Training loss for batch 1608 : 0.5318523645401001\n",
      "Training loss for batch 1609 : 0.1863422691822052\n",
      "Training loss for batch 1610 : 0.5562729239463806\n",
      "Training loss for batch 1611 : 0.33265915513038635\n",
      "Training loss for batch 1612 : 0.06641346216201782\n",
      "Training loss for batch 1613 : 0.5576085448265076\n",
      "Training loss for batch 1614 : 0.6278902292251587\n",
      "Training loss for batch 1615 : 0.34564658999443054\n",
      "Training loss for batch 1616 : 0.5436553955078125\n",
      "Training loss for batch 1617 : 0.058858200907707214\n",
      "Training loss for batch 1618 : 0.15883052349090576\n",
      "Training loss for batch 1619 : 0.15141668915748596\n",
      "Training loss for batch 1620 : 0.1241784393787384\n",
      "Training loss for batch 1621 : 0.5527349710464478\n",
      "Training loss for batch 1622 : 0.19866226613521576\n",
      "Training loss for batch 1623 : 0.4476355314254761\n",
      "Training loss for batch 1624 : 0.190726637840271\n",
      "Training loss for batch 1625 : 0.23224030435085297\n",
      "Training loss for batch 1626 : 0.18303200602531433\n",
      "Training loss for batch 1627 : 0.41248732805252075\n",
      "Training loss for batch 1628 : 0.05180380120873451\n",
      "Training loss for batch 1629 : 0.026377415284514427\n",
      "Training loss for batch 1630 : 0.25044581294059753\n",
      "Training loss for batch 1631 : 0.13274875283241272\n",
      "Training loss for batch 1632 : 0.38653117418289185\n",
      "Training loss for batch 1633 : 0.3211610019207001\n",
      "Training loss for batch 1634 : 0.45352089405059814\n",
      "Training loss for batch 1635 : 0.5632269978523254\n",
      "Training loss for batch 1636 : 0.45244789123535156\n",
      "Training loss for batch 1637 : 0.2841077744960785\n",
      "Training loss for batch 1638 : 0.4059858024120331\n",
      "Training loss for batch 1639 : 0.1155143454670906\n",
      "Training loss for batch 1640 : 0.469686895608902\n",
      "Training loss for batch 1641 : 0.20875880122184753\n",
      "Training loss for batch 1642 : 0.305959016084671\n",
      "Training loss for batch 1643 : 0.4098830819129944\n",
      "Training loss for batch 1644 : 0.36441391706466675\n",
      "Training loss for batch 1645 : 0.3423733711242676\n",
      "Training loss for batch 1646 : 0.7223616242408752\n",
      "Training loss for batch 1647 : 0.24971358478069305\n",
      "Training loss for batch 1648 : 1.193814992904663\n",
      "Training loss for batch 1649 : 0.4031355082988739\n",
      "Training loss for batch 1650 : 0.28590720891952515\n",
      "Training loss for batch 1651 : 0.08393779397010803\n",
      "Training loss for batch 1652 : 0.09555904567241669\n",
      "Training loss for batch 1653 : 0.5217077136039734\n",
      "Training loss for batch 1654 : 0.17706820368766785\n",
      "Training loss for batch 1655 : 0.0633997991681099\n",
      "Training loss for batch 1656 : 0.14849776029586792\n",
      "Training loss for batch 1657 : 0.23141320049762726\n",
      "Training loss for batch 1658 : 0.1705094873905182\n",
      "Training loss for batch 1659 : 0.3553466796875\n",
      "Training loss for batch 1660 : 0.24979877471923828\n",
      "Training loss for batch 1661 : 0.520218014717102\n",
      "Training loss for batch 1662 : 0.2814212143421173\n",
      "Training loss for batch 1663 : 0.08957280963659286\n",
      "Training loss for batch 1664 : 0.18300464749336243\n",
      "Training loss for batch 1665 : 0.28012844920158386\n",
      "Training loss for batch 1666 : 0.32518959045410156\n",
      "Training loss for batch 1667 : 0.2071039229631424\n",
      "Training loss for batch 1668 : 0.15580298006534576\n",
      "Training loss for batch 1669 : 0.2327544242143631\n",
      "Training loss for batch 1670 : 0.5501347184181213\n",
      "Training loss for batch 1671 : 0.058328937739133835\n",
      "Training loss for batch 1672 : 0.569124162197113\n",
      "Training loss for batch 1673 : 0.34253671765327454\n",
      "Training loss for batch 1674 : 0.048095185309648514\n",
      "Training loss for batch 1675 : 0.13814108073711395\n",
      "Training loss for batch 1676 : 0.026184460148215294\n",
      "Training loss for batch 1677 : 0.1645209640264511\n",
      "Training loss for batch 1678 : 0.12774541974067688\n",
      "Training loss for batch 1679 : 0.30002766847610474\n",
      "Training loss for batch 1680 : 0.2211773544549942\n",
      "Training loss for batch 1681 : 0.37835654616355896\n",
      "Training loss for batch 1682 : 0.03761588782072067\n",
      "Training loss for batch 1683 : 0.3340587615966797\n",
      "Training loss for batch 1684 : 0.10402897000312805\n",
      "Training loss for batch 1685 : 0.26902106404304504\n",
      "Training loss for batch 1686 : 0.6535416841506958\n",
      "Training loss for batch 1687 : 0.38396918773651123\n",
      "Training loss for batch 1688 : 0.13987158238887787\n",
      "Training loss for batch 1689 : 0.4353049695491791\n",
      "Training loss for batch 1690 : 0.4508540630340576\n",
      "Training loss for batch 1691 : 0.04614050313830376\n",
      "Training loss for batch 1692 : 0.32209381461143494\n",
      "Training loss for batch 1693 : 0.09874972701072693\n",
      "Training loss for batch 1694 : 0.5165638327598572\n",
      "Training loss for batch 1695 : 0.0468025878071785\n",
      "Training loss for batch 1696 : 0.5226269960403442\n",
      "Training loss for batch 1697 : 0.2005794644355774\n",
      "Training loss for batch 1698 : 0.31578707695007324\n",
      "Training loss for batch 1699 : 0.510770857334137\n",
      "Training loss for batch 1700 : 0.5753993988037109\n",
      "Training loss for batch 1701 : 0.14948290586471558\n",
      "Training loss for batch 1702 : 0.5388860106468201\n",
      "Training loss for batch 1703 : 0.20808088779449463\n",
      "Training loss for batch 1704 : 0.10802251100540161\n",
      "Training loss for batch 1705 : 0.6709384322166443\n",
      "Training loss for batch 1706 : 0.11977524310350418\n",
      "Training loss for batch 1707 : 0.4375518560409546\n",
      "Training loss for batch 1708 : 0.40397942066192627\n",
      "Training loss for batch 1709 : 0.013337264768779278\n",
      "Training loss for batch 1710 : 0.12806741893291473\n",
      "Training loss for batch 1711 : 0.23779645562171936\n",
      "Training loss for batch 1712 : 0.4917948246002197\n",
      "Training loss for batch 1713 : 0.3119128346443176\n",
      "Training loss for batch 1714 : 0.0\n",
      "Training loss for batch 1715 : 0.3719375431537628\n",
      "Training loss for batch 1716 : 0.074432373046875\n",
      "Training loss for batch 1717 : 0.3699202537536621\n",
      "Training loss for batch 1718 : 0.2784624695777893\n",
      "Training loss for batch 1719 : 0.7037585377693176\n",
      "Training loss for batch 1720 : 0.20383581519126892\n",
      "Training loss for batch 1721 : 0.18271085619926453\n",
      "Training loss for batch 1722 : 0.21368525922298431\n",
      "Training loss for batch 1723 : 0.12012746930122375\n",
      "Training loss for batch 1724 : 0.3869163691997528\n",
      "Training loss for batch 1725 : 0.22787600755691528\n",
      "Training loss for batch 1726 : 0.5747987627983093\n",
      "Training loss for batch 1727 : 0.5778909921646118\n",
      "Training loss for batch 1728 : 0.5503062605857849\n",
      "Training loss for batch 1729 : 0.1932438462972641\n",
      "Training loss for batch 1730 : 0.34669166803359985\n",
      "Training loss for batch 1731 : 0.07361992448568344\n",
      "Training loss for batch 1732 : 0.18358732759952545\n",
      "Training loss for batch 1733 : 0.7742481231689453\n",
      "Training loss for batch 1734 : 0.3670947551727295\n",
      "Training loss for batch 1735 : 0.6214717626571655\n",
      "Training loss for batch 1736 : 0.5378831624984741\n",
      "Training loss for batch 1737 : 0.5412149429321289\n",
      "Training loss for batch 1738 : 0.7612849473953247\n",
      "Training loss for batch 1739 : 0.5590780973434448\n",
      "Training loss for batch 1740 : 0.3915395438671112\n",
      "Training loss for batch 1741 : 0.5523154139518738\n",
      "Training loss for batch 1742 : 0.16037029027938843\n",
      "Training loss for batch 1743 : 0.4767022728919983\n",
      "Training loss for batch 1744 : 0.1201898530125618\n",
      "Training loss for batch 1745 : 0.14628316462039948\n",
      "Training loss for batch 1746 : 0.2842119038105011\n",
      "Training loss for batch 1747 : 0.03975546360015869\n",
      "Training loss for batch 1748 : 0.39461368322372437\n",
      "Training loss for batch 1749 : 0.6399445533752441\n",
      "Training loss for batch 1750 : 0.4110545516014099\n",
      "Training loss for batch 1751 : 0.04666467010974884\n",
      "Training loss for batch 1752 : 0.2993834614753723\n",
      "Training loss for batch 1753 : 0.4710819721221924\n",
      "Training loss for batch 1754 : 0.06346996128559113\n",
      "Training loss for batch 1755 : 0.26098307967185974\n",
      "Training loss for batch 1756 : 0.07849017530679703\n",
      "Training loss for batch 1757 : 0.2882998585700989\n",
      "Training loss for batch 1758 : 0.14231480658054352\n",
      "Training loss for batch 1759 : 0.43317168951034546\n",
      "Training loss for batch 1760 : 0.058700043708086014\n",
      "Training loss for batch 1761 : 0.39113104343414307\n",
      "Training loss for batch 1762 : 0.12374809384346008\n",
      "Training loss for batch 1763 : 0.09300722926855087\n",
      "Training loss for batch 1764 : 0.5629811882972717\n",
      "Training loss for batch 1765 : 0.17134051024913788\n",
      "Training loss for batch 1766 : 0.32819250226020813\n",
      "Training loss for batch 1767 : 0.5512869358062744\n",
      "Training loss for batch 1768 : 0.5380616784095764\n",
      "Training loss for batch 1769 : 0.10978242754936218\n",
      "Training loss for batch 1770 : 0.5351071357727051\n",
      "Training loss for batch 1771 : 0.06418579816818237\n",
      "Training loss for batch 1772 : 0.2668168246746063\n",
      "Training loss for batch 1773 : 0.3722051978111267\n",
      "Training loss for batch 1774 : 0.16626222431659698\n",
      "Training loss for batch 1775 : 0.041453007608652115\n",
      "Training loss for batch 1776 : 0.0849405974149704\n",
      "Training loss for batch 1777 : 0.4614642262458801\n",
      "Training loss for batch 1778 : 0.33215203881263733\n",
      "Training loss for batch 1779 : 0.23954610526561737\n",
      "Training loss for batch 1780 : 0.13451503217220306\n",
      "Training loss for batch 1781 : 0.18547271192073822\n",
      "Training loss for batch 1782 : 0.38795408606529236\n",
      "Training loss for batch 1783 : 0.5878726840019226\n",
      "Training loss for batch 1784 : 0.2802162170410156\n",
      "Training loss for batch 1785 : 0.26851797103881836\n",
      "Training loss for batch 1786 : 0.23660680651664734\n",
      "Training loss for batch 1787 : 0.5258658528327942\n",
      "Training loss for batch 1788 : 0.15929357707500458\n",
      "Training loss for batch 1789 : 0.23286586999893188\n",
      "Training loss for batch 1790 : 0.3485853374004364\n",
      "Training loss for batch 1791 : 0.05406991392374039\n",
      "Training loss for batch 1792 : 0.21166108548641205\n",
      "Training loss for batch 1793 : 0.09893564879894257\n",
      "Training loss for batch 1794 : 0.6418536305427551\n",
      "Training loss for batch 1795 : 0.7814404368400574\n",
      "Training loss for batch 1796 : 0.37234562635421753\n",
      "Training loss for batch 1797 : 0.11596962809562683\n",
      "Training loss for batch 1798 : 0.615064263343811\n",
      "Training loss for batch 1799 : 0.5864119529724121\n",
      "Training loss for batch 1800 : 0.17356538772583008\n",
      "Training loss for batch 1801 : 0.15785524249076843\n",
      "Training loss for batch 1802 : 0.03433084115386009\n",
      "Training loss for batch 1803 : 0.2052132487297058\n",
      "Training loss for batch 1804 : 0.42617350816726685\n",
      "Training loss for batch 1805 : 0.026676403358578682\n",
      "Training loss for batch 1806 : 0.05219739302992821\n",
      "Training loss for batch 1807 : 0.7523472309112549\n",
      "Training loss for batch 1808 : 0.07899691164493561\n",
      "Training loss for batch 1809 : 0.5485163927078247\n",
      "Training loss for batch 1810 : 0.7922906875610352\n",
      "Training loss for batch 1811 : 0.08721672743558884\n",
      "Training loss for batch 1812 : 0.4528478682041168\n",
      "Training loss for batch 1813 : 1.4668241739273071\n",
      "Training loss for batch 1814 : 0.7951111197471619\n",
      "Training loss for batch 1815 : 0.2864418625831604\n",
      "Training loss for batch 1816 : 0.16361118853092194\n",
      "Training loss for batch 1817 : 0.4555685818195343\n",
      "Training loss for batch 1818 : 0.21325308084487915\n",
      "Training loss for batch 1819 : 0.12797097861766815\n",
      "Training loss for batch 1820 : 0.03489729017019272\n",
      "Training loss for batch 1821 : 0.3302235007286072\n",
      "Training loss for batch 1822 : 0.3069309592247009\n",
      "Training loss for batch 1823 : 0.1703857183456421\n",
      "Training loss for batch 1824 : 0.30669301748275757\n",
      "Training loss for batch 1825 : 0.19894257187843323\n",
      "Training loss for batch 1826 : 0.253688782453537\n",
      "Training loss for batch 1827 : 0.556236743927002\n",
      "Training loss for batch 1828 : 0.059544071555137634\n",
      "Training loss for batch 1829 : 0.19985167682170868\n",
      "Training loss for batch 1830 : 0.17333722114562988\n",
      "Training loss for batch 1831 : 0.24948981404304504\n",
      "Training loss for batch 1832 : 0.2648126482963562\n",
      "Training loss for batch 1833 : 0.372184693813324\n",
      "Training loss for batch 1834 : 0.5840930938720703\n",
      "Training loss for batch 1835 : 0.2521141469478607\n",
      "Training loss for batch 1836 : 0.14152652025222778\n",
      "Training loss for batch 1837 : 0.04963558167219162\n",
      "Training loss for batch 1838 : 0.22925758361816406\n",
      "Training loss for batch 1839 : 0.3911200165748596\n",
      "Training loss for batch 1840 : 0.480476051568985\n",
      "Training loss for batch 1841 : 0.12492341548204422\n",
      "Training loss for batch 1842 : 0.394502729177475\n",
      "Training loss for batch 1843 : 0.16427184641361237\n",
      "Training loss for batch 1844 : 0.10150489211082458\n",
      "Training loss for batch 1845 : 0.5388529300689697\n",
      "Training loss for batch 1846 : 0.2659769356250763\n",
      "Training loss for batch 1847 : 0.6063424348831177\n",
      "Training loss for batch 1848 : 0.8372849822044373\n",
      "Training loss for batch 1849 : 0.6790850162506104\n",
      "Training loss for batch 1850 : 0.2518244683742523\n",
      "Training loss for batch 1851 : 0.21956801414489746\n",
      "Training loss for batch 1852 : 0.10058838874101639\n",
      "Training loss for batch 1853 : 0.06504818052053452\n",
      "Training loss for batch 1854 : 0.06909432262182236\n",
      "Training loss for batch 1855 : 0.6546636819839478\n",
      "Training loss for batch 1856 : 0.13872553408145905\n",
      "Training loss for batch 1857 : 0.32634252309799194\n",
      "Training loss for batch 1858 : 0.2720942199230194\n",
      "Training loss for batch 1859 : 0.619500994682312\n",
      "Training loss for batch 1860 : 0.18993940949440002\n",
      "Training loss for batch 1861 : 0.3387484848499298\n",
      "Training loss for batch 1862 : 0.3142593801021576\n",
      "Training loss for batch 1863 : 0.4578768312931061\n",
      "Training loss for batch 1864 : 0.4667700231075287\n",
      "Training loss for batch 1865 : 0.33637821674346924\n",
      "Training loss for batch 1866 : 0.25060558319091797\n",
      "Training loss for batch 1867 : 0.2439216524362564\n",
      "Training loss for batch 1868 : 0.3132368326187134\n",
      "Training loss for batch 1869 : 0.06986083090305328\n",
      "Training loss for batch 1870 : 0.3723697364330292\n",
      "Training loss for batch 1871 : 0.19773457944393158\n",
      "Training loss for batch 1872 : 0.648990273475647\n",
      "Training loss for batch 1873 : 0.0892464891076088\n",
      "Training loss for batch 1874 : 0.28387418389320374\n",
      "Training loss for batch 1875 : 0.3513104319572449\n",
      "Training loss for batch 1876 : 0.22515924274921417\n",
      "Training loss for batch 1877 : 0.3405923843383789\n",
      "Training loss for batch 1878 : 0.4816076159477234\n",
      "Training loss for batch 1879 : 0.08821684122085571\n",
      "Training loss for batch 1880 : 0.5493589639663696\n",
      "Training loss for batch 1881 : 0.13620376586914062\n",
      "Training loss for batch 1882 : 0.18803440034389496\n",
      "Training loss for batch 1883 : 0.42034634947776794\n",
      "Training loss for batch 1884 : 0.277868390083313\n",
      "Training loss for batch 1885 : 0.4398929178714752\n",
      "Training loss for batch 1886 : 0.4564972221851349\n",
      "Training loss for batch 1887 : 0.2400227040052414\n",
      "Training loss for batch 1888 : 0.049997031688690186\n",
      "Training loss for batch 1889 : 0.16377973556518555\n",
      "Training loss for batch 1890 : 0.2527802288532257\n",
      "Training loss for batch 1891 : 0.1654706448316574\n",
      "Training loss for batch 1892 : 0.19131815433502197\n",
      "Training loss for batch 1893 : 0.12121868878602982\n",
      "Training loss for batch 1894 : 0.6780480742454529\n",
      "Training loss for batch 1895 : 0.6217281818389893\n",
      "Training loss for batch 1896 : 0.3698510527610779\n",
      "Training loss for batch 1897 : 0.6640006899833679\n",
      "Training loss for batch 1898 : 0.23335696756839752\n",
      "Training loss for batch 1899 : 0.4544256329536438\n",
      "Training loss for batch 1900 : 0.2987751364707947\n",
      "Training loss for batch 1901 : 0.6655797362327576\n",
      "Training loss for batch 1902 : 0.4516277313232422\n",
      "Training loss for batch 1903 : 0.18391649425029755\n",
      "Training loss for batch 1904 : 0.41131511330604553\n",
      "Training loss for batch 1905 : 0.38630160689353943\n",
      "Training loss for batch 1906 : 0.1976451724767685\n",
      "Training loss for batch 1907 : 0.32110795378685\n",
      "Training loss for batch 1908 : 0.4819207191467285\n",
      "Training loss for batch 1909 : 0.15602979063987732\n",
      "Training loss for batch 1910 : 0.2924012839794159\n",
      "Training loss for batch 1911 : 0.4234384298324585\n",
      "Training loss for batch 1912 : 0.19052451848983765\n",
      "Training loss for batch 1913 : 0.08582430332899094\n",
      "Training loss for batch 1914 : 0.22126220166683197\n",
      "Training loss for batch 1915 : 0.2491624355316162\n",
      "Training loss for batch 1916 : 0.5713830590248108\n",
      "Training loss for batch 1917 : 0.427928626537323\n",
      "Training loss for batch 1918 : 0.3427862524986267\n",
      "Training loss for batch 1919 : 0.418384850025177\n",
      "Training loss for batch 1920 : 0.43928930163383484\n",
      "Training loss for batch 1921 : 0.09839501231908798\n",
      "Training loss for batch 1922 : 0.523968517780304\n",
      "Training loss for batch 1923 : 0.5470622181892395\n",
      "Training loss for batch 1924 : 0.3974660038948059\n",
      "Training loss for batch 1925 : 0.06756868213415146\n",
      "Training loss for batch 1926 : 0.4002455174922943\n",
      "Training loss for batch 1927 : 0.08458083122968674\n",
      "Training loss for batch 1928 : 0.27572181820869446\n",
      "Training loss for batch 1929 : 0.09209894388914108\n",
      "Training loss for batch 1930 : 0.47697538137435913\n",
      "Training loss for batch 1931 : 0.21950317919254303\n",
      "Training loss for batch 1932 : 0.0609738752245903\n",
      "Training loss for batch 1933 : 0.18173547089099884\n",
      "Training loss for batch 1934 : 0.4980114996433258\n",
      "Training loss for batch 1935 : 0.4049251079559326\n",
      "Training loss for batch 1936 : 0.48008227348327637\n",
      "Training loss for batch 1937 : 0.3621681332588196\n",
      "Training loss for batch 1938 : 0.010942046530544758\n",
      "Training loss for batch 1939 : 0.3507585823535919\n",
      "Training loss for batch 1940 : 0.41460227966308594\n",
      "Training loss for batch 1941 : 0.7145406007766724\n",
      "Training loss for batch 1942 : 0.25872257351875305\n",
      "Training loss for batch 1943 : 0.05639611929655075\n",
      "Training loss for batch 1944 : 0.27940380573272705\n",
      "Training loss for batch 1945 : 0.0049460530281066895\n",
      "Training loss for batch 1946 : 0.12438657879829407\n",
      "Training loss for batch 1947 : 0.525788426399231\n",
      "Training loss for batch 1948 : 0.10501117259263992\n",
      "Training loss for batch 1949 : 0.2724519371986389\n",
      "Training loss for batch 1950 : 0.07704058289527893\n",
      "Training loss for batch 1951 : 0.34635335206985474\n",
      "Training loss for batch 1952 : 0.3196973204612732\n",
      "Training loss for batch 1953 : 0.1970667988061905\n",
      "Training loss for batch 1954 : 0.27526506781578064\n",
      "Training loss for batch 1955 : 0.2894109785556793\n",
      "Training loss for batch 1956 : 0.16044111549854279\n",
      "Training loss for batch 1957 : 0.23814961314201355\n",
      "Training loss for batch 1958 : 0.21895767748355865\n",
      "Training loss for batch 1959 : 0.03001261129975319\n",
      "Training loss for batch 1960 : 0.10376639664173126\n",
      "Training loss for batch 1961 : 0.18731921911239624\n",
      "Training loss for batch 1962 : 0.29562321305274963\n",
      "Training loss for batch 1963 : 0.4731192886829376\n",
      "Training loss for batch 1964 : 0.6477099657058716\n",
      "Training loss for batch 1965 : 0.4622933864593506\n",
      "Training loss for batch 1966 : 0.47401556372642517\n",
      "Training loss for batch 1967 : 0.14338815212249756\n",
      "Training loss for batch 1968 : 0.1882711797952652\n",
      "Training loss for batch 1969 : 0.47332653403282166\n",
      "Training loss for batch 1970 : 0.4859716594219208\n",
      "Training loss for batch 1971 : 0.6112162470817566\n",
      "Training loss for batch 1972 : 0.06260666996240616\n",
      "Training loss for batch 1973 : 0.27992701530456543\n",
      "Training loss for batch 1974 : 0.1997663825750351\n",
      "Training loss for batch 1975 : 0.5394697785377502\n",
      "Training loss for batch 1976 : 0.5380227565765381\n",
      "Training loss for batch 1977 : 0.46713730692863464\n",
      "Training loss for batch 1978 : 0.12722398340702057\n",
      "Training loss for batch 1979 : 0.5238194465637207\n",
      "Training loss for batch 1980 : 0.327518105506897\n",
      "Training loss for batch 1981 : 0.41209718585014343\n",
      "Training loss for batch 1982 : 0.06814120709896088\n",
      "Training loss for batch 1983 : 0.8699935674667358\n",
      "Training loss for batch 1984 : 0.407688707113266\n",
      "Training loss for batch 1985 : 0.36431166529655457\n",
      "Training loss for batch 1986 : 0.2692500352859497\n",
      "Training loss for batch 1987 : 0.6511045098304749\n",
      "Training loss for batch 1988 : 0.1390196532011032\n",
      "Training loss for batch 1989 : 0.3263653814792633\n",
      "Training loss for batch 1990 : 0.3518623411655426\n",
      "Training loss for batch 1991 : 0.34587785601615906\n",
      "Training loss for batch 1992 : 0.3113197982311249\n",
      "Training loss for batch 1993 : 1.019859790802002\n",
      "Training loss for batch 1994 : 0.708635687828064\n",
      "Training loss for batch 1995 : 0.14114506542682648\n",
      "Training loss for batch 1996 : 0.593107283115387\n",
      "Training loss for batch 1997 : 0.56297767162323\n",
      "Training loss for batch 1998 : 0.03825223445892334\n",
      "Training loss for batch 1999 : 0.0070129986852407455\n",
      "Training loss for batch 2000 : 0.12892349064350128\n",
      "Training loss for batch 2001 : 0.22828897833824158\n",
      "Training loss for batch 2002 : 0.3591364920139313\n",
      "Training loss for batch 2003 : 0.42915529012680054\n",
      "Training loss for batch 2004 : 0.7870417237281799\n",
      "Training loss for batch 2005 : 0.6699684858322144\n",
      "Training loss for batch 2006 : 0.8622186183929443\n",
      "Training loss for batch 2007 : 0.4112561345100403\n",
      "Training loss for batch 2008 : 0.0870121568441391\n",
      "Training loss for batch 2009 : 0.14637145400047302\n",
      "Training loss for batch 2010 : 0.18433411419391632\n",
      "Training loss for batch 2011 : 0.2984943389892578\n",
      "Training loss for batch 2012 : 0.08059120178222656\n",
      "Training loss for batch 2013 : 0.6999572515487671\n",
      "Training loss for batch 2014 : 0.20002399384975433\n",
      "Training loss for batch 2015 : 0.21696151793003082\n",
      "Training loss for batch 2016 : 0.034221451729536057\n",
      "Training loss for batch 2017 : 0.2517910301685333\n",
      "Training loss for batch 2018 : 0.24852758646011353\n",
      "Training loss for batch 2019 : 0.27950072288513184\n",
      "Training loss for batch 2020 : 0.32302287220954895\n",
      "Training loss for batch 2021 : 0.12754347920417786\n",
      "Training loss for batch 2022 : 0.5402592420578003\n",
      "Training loss for batch 2023 : 0.04130082204937935\n",
      "Training loss for batch 2024 : 0.47270286083221436\n",
      "Training loss for batch 2025 : 0.15337559580802917\n",
      "Training loss for batch 2026 : 0.17834709584712982\n",
      "Training loss for batch 2027 : 0.4234462082386017\n",
      "Training loss for batch 2028 : 0.5957416892051697\n",
      "Training loss for batch 2029 : 0.43599098920822144\n",
      "Training loss for batch 2030 : 0.06695500016212463\n",
      "Training loss for batch 2031 : 0.6409595012664795\n",
      "Training loss for batch 2032 : 0.23254430294036865\n",
      "Training loss for batch 2033 : 0.01124575361609459\n",
      "Training loss for batch 2034 : 0.07793325930833817\n",
      "Training loss for batch 2035 : 0.27011793851852417\n",
      "Training loss for batch 2036 : 0.1651415228843689\n",
      "Training loss for batch 2037 : 0.34667038917541504\n",
      "Training loss for batch 2038 : 0.09093776345252991\n",
      "Training loss for batch 2039 : 0.7331082224845886\n",
      "Training loss for batch 2040 : 0.6837839484214783\n",
      "Training loss for batch 2041 : 0.4184122681617737\n",
      "Training loss for batch 2042 : 0.3687739372253418\n",
      "Training loss for batch 2043 : 0.3742176592350006\n",
      "Training loss for batch 2044 : 0.1432798206806183\n",
      "Training loss for batch 2045 : 0.783585786819458\n",
      "Training loss for batch 2046 : 0.20948882400989532\n",
      "Training loss for batch 2047 : 0.0827564224600792\n",
      "Training loss for batch 2048 : 0.5677788853645325\n",
      "Training loss for batch 2049 : 0.30059412121772766\n",
      "Training loss for batch 2050 : 0.35975953936576843\n",
      "Training loss for batch 2051 : 0.302333265542984\n",
      "Training loss for batch 2052 : 0.32852569222450256\n",
      "Training loss for batch 2053 : 0.8072498440742493\n",
      "Training loss for batch 2054 : 0.3661028742790222\n",
      "Training loss for batch 2055 : 0.2071072906255722\n",
      "Training loss for batch 2056 : 0.25043314695358276\n",
      "Training loss for batch 2057 : 0.0812264084815979\n",
      "Training loss for batch 2058 : 0.29581186175346375\n",
      "Training loss for batch 2059 : 0.5898997783660889\n",
      "Training loss for batch 2060 : 0.2780969440937042\n",
      "Training loss for batch 2061 : 0.007159098982810974\n",
      "Training loss for batch 2062 : 0.40429064631462097\n",
      "Training loss for batch 2063 : 0.630815863609314\n",
      "Training loss for batch 2064 : 0.4116770923137665\n",
      "Training loss for batch 2065 : 0.6069453358650208\n",
      "Training loss for batch 2066 : 0.19021321833133698\n",
      "Training loss for batch 2067 : 0.11891723424196243\n",
      "Training loss for batch 2068 : 0.582371711730957\n",
      "Training loss for batch 2069 : 0.18461495637893677\n",
      "Training loss for batch 2070 : 0.10755368322134018\n",
      "Training loss for batch 2071 : 0.3080698847770691\n",
      "Training loss for batch 2072 : 0.24531684815883636\n",
      "Training loss for batch 2073 : 0.46347129344940186\n",
      "Training loss for batch 2074 : 0.5858481526374817\n",
      "Training loss for batch 2075 : 0.165012925863266\n",
      "Training loss for batch 2076 : 0.1914721429347992\n",
      "Training loss for batch 2077 : 0.2977723777294159\n",
      "Training loss for batch 2078 : 0.5083171725273132\n",
      "Training loss for batch 2079 : 0.4684407711029053\n",
      "Training loss for batch 2080 : 0.21331334114074707\n",
      "Training loss for batch 2081 : 0.10374334454536438\n",
      "Training loss for batch 2082 : 0.5065419673919678\n",
      "Training loss for batch 2083 : 0.6844767928123474\n",
      "Training loss for batch 2084 : 0.6103858351707458\n",
      "Training loss for batch 2085 : 0.0793873593211174\n",
      "Training loss for batch 2086 : 0.16247442364692688\n",
      "Training loss for batch 2087 : 0.24582307040691376\n",
      "Training loss for batch 2088 : 0.3982486128807068\n",
      "Training loss for batch 2089 : 0.03937853127717972\n",
      "Training loss for batch 2090 : 0.07438810169696808\n",
      "Training loss for batch 2091 : 0.09269023686647415\n",
      "Training loss for batch 2092 : 0.6334177851676941\n",
      "Training loss for batch 2093 : 0.481697678565979\n",
      "Training loss for batch 2094 : 0.2585652768611908\n",
      "Training loss for batch 2095 : 0.25187167525291443\n",
      "Training loss for batch 2096 : 0.10369396209716797\n",
      "Training loss for batch 2097 : 0.13542774319648743\n",
      "Training loss for batch 2098 : 0.4327548146247864\n",
      "Training loss for batch 2099 : 0.15139155089855194\n",
      "Training loss for batch 2100 : 0.6584915518760681\n",
      "Training loss for batch 2101 : 0.09998703747987747\n",
      "Training loss for batch 2102 : 0.15304577350616455\n",
      "Training loss for batch 2103 : 0.17022863030433655\n",
      "Training loss for batch 2104 : 0.5116108059883118\n",
      "Training loss for batch 2105 : 0.17260435223579407\n",
      "Training loss for batch 2106 : 0.7588760256767273\n",
      "Training loss for batch 2107 : 0.13123838603496552\n",
      "Training loss for batch 2108 : 0.03679543733596802\n",
      "Training loss for batch 2109 : 0.782456636428833\n",
      "Training loss for batch 2110 : 0.02113807201385498\n",
      "Training loss for batch 2111 : 0.20932993292808533\n",
      "Training loss for batch 2112 : 0.3936927616596222\n",
      "Training loss for batch 2113 : 0.20445801317691803\n",
      "Training loss for batch 2114 : 0.08653250336647034\n",
      "Training loss for batch 2115 : 0.3775540292263031\n",
      "Training loss for batch 2116 : 0.1737402379512787\n",
      "Training loss for batch 2117 : 0.37485766410827637\n",
      "Training loss for batch 2118 : 0.6299580335617065\n",
      "Training loss for batch 2119 : 0.1931208372116089\n",
      "Training loss for batch 2120 : 0.6775791645050049\n",
      "Training loss for batch 2121 : 0.505783200263977\n",
      "Training loss for batch 2122 : 0.2924754321575165\n",
      "Training loss for batch 2123 : 0.24081483483314514\n",
      "Training loss for batch 2124 : 0.5379448533058167\n",
      "Training loss for batch 2125 : 0.032512884587049484\n",
      "Training loss for batch 2126 : 0.23785655200481415\n",
      "Training loss for batch 2127 : 0.4034070670604706\n",
      "Training loss for batch 2128 : 0.783608078956604\n",
      "Training loss for batch 2129 : 0.568720817565918\n",
      "Training loss for batch 2130 : 0.09633190929889679\n",
      "Training loss for batch 2131 : 0.434666246175766\n",
      "Training loss for batch 2132 : 0.2147408127784729\n",
      "Training loss for batch 2133 : 0.2871797978878021\n",
      "Training loss for batch 2134 : 0.4607267379760742\n",
      "Training loss for batch 2135 : 0.37164631485939026\n",
      "Training loss for batch 2136 : 0.22747047245502472\n",
      "Training loss for batch 2137 : 0.25652584433555603\n",
      "Training loss for batch 2138 : 0.6411786675453186\n",
      "Training loss for batch 2139 : 0.23653070628643036\n",
      "Training loss for batch 2140 : 0.35579147934913635\n",
      "Training loss for batch 2141 : 0.4923945367336273\n",
      "Training loss for batch 2142 : 0.8058323264122009\n",
      "Training loss for batch 2143 : 0.5101785659790039\n",
      "Training loss for batch 2144 : 0.1818169802427292\n",
      "Training loss for batch 2145 : 0.17591418325901031\n",
      "Training loss for batch 2146 : 0.2372676581144333\n",
      "Training loss for batch 2147 : 0.5435270667076111\n",
      "Training loss for batch 2148 : 0.45230594277381897\n",
      "Training loss for batch 2149 : 0.08041275292634964\n",
      "Training loss for batch 2150 : 0.5046309232711792\n",
      "Training loss for batch 2151 : 0.497684121131897\n",
      "Training loss for batch 2152 : 0.14431895315647125\n",
      "Training loss for batch 2153 : 0.34202340245246887\n",
      "Training loss for batch 2154 : 0.22180764377117157\n",
      "Training loss for batch 2155 : 0.049592435359954834\n",
      "Training loss for batch 2156 : 0.7921095490455627\n",
      "Training loss for batch 2157 : 0.6853286027908325\n",
      "Training loss for batch 2158 : 0.17368946969509125\n",
      "Training loss for batch 2159 : 0.3315582275390625\n",
      "Training loss for batch 2160 : 0.905592143535614\n",
      "Training loss for batch 2161 : 0.32432928681373596\n",
      "Training loss for batch 2162 : 0.09610200673341751\n",
      "Training loss for batch 2163 : 0.2568523585796356\n",
      "Training loss for batch 2164 : 0.3786334991455078\n",
      "Training loss for batch 2165 : 0.24902720749378204\n",
      "Training loss for batch 2166 : 0.1049218401312828\n",
      "Training loss for batch 2167 : 0.06544720381498337\n",
      "Training loss for batch 2168 : 0.29207146167755127\n",
      "Training loss for batch 2169 : 0.4374690055847168\n",
      "Training loss for batch 2170 : 0.2591625154018402\n",
      "Training loss for batch 2171 : 0.38612040877342224\n",
      "Training loss for batch 2172 : 0.3955031633377075\n",
      "Training loss for batch 2173 : 0.6239508390426636\n",
      "Training loss for batch 2174 : 0.3244757652282715\n",
      "Training loss for batch 2175 : 0.09309996664524078\n",
      "Training loss for batch 2176 : 0.07230280339717865\n",
      "Training loss for batch 2177 : 0.4205629229545593\n",
      "Training loss for batch 2178 : 0.3845111131668091\n",
      "Training loss for batch 2179 : 0.19832199811935425\n",
      "Training loss for batch 2180 : 0.333905428647995\n",
      "Training loss for batch 2181 : 0.36916008591651917\n",
      "Training loss for batch 2182 : 0.33506473898887634\n",
      "Training loss for batch 2183 : 0.4351464509963989\n",
      "Training loss for batch 2184 : 0.44756144285202026\n",
      "Training loss for batch 2185 : 0.547233521938324\n",
      "Training loss for batch 2186 : 0.26049068570137024\n",
      "Training loss for batch 2187 : 0.0712859034538269\n",
      "Training loss for batch 2188 : 0.3867902159690857\n",
      "Training loss for batch 2189 : 0.2504108250141144\n",
      "Training loss for batch 2190 : 0.3177042603492737\n",
      "Training loss for batch 2191 : 0.21064528822898865\n",
      "Training loss for batch 2192 : 0.35620447993278503\n",
      "Training loss for batch 2193 : 0.43008655309677124\n",
      "Training loss for batch 2194 : 0.0038063586689531803\n",
      "Training loss for batch 2195 : 0.596319854259491\n",
      "Training loss for batch 2196 : 0.3950256407260895\n",
      "Training loss for batch 2197 : 0.38874515891075134\n",
      "Training loss for batch 2198 : 0.1305113583803177\n",
      "Training loss for batch 2199 : 0.10989760607481003\n",
      "Training loss for batch 2200 : 0.05595040321350098\n",
      "Training loss for batch 2201 : 0.28613191843032837\n",
      "Training loss for batch 2202 : 0.019644245505332947\n",
      "Training loss for batch 2203 : 0.9828751087188721\n",
      "Training loss for batch 2204 : 0.5846671462059021\n",
      "Training loss for batch 2205 : 0.4608028531074524\n",
      "Training loss for batch 2206 : 0.16314072906970978\n",
      "Training loss for batch 2207 : 0.1853228509426117\n",
      "Training loss for batch 2208 : 0.2201997935771942\n",
      "Training loss for batch 2209 : 0.806654155254364\n",
      "Training loss for batch 2210 : 0.09546328336000443\n",
      "Training loss for batch 2211 : 0.4109956622123718\n",
      "Training loss for batch 2212 : 0.7837647795677185\n",
      "Training loss for batch 2213 : 0.32839375734329224\n",
      "Training loss for batch 2214 : 0.5323194861412048\n",
      "Training loss for batch 2215 : 0.181039959192276\n",
      "Training loss for batch 2216 : 0.44466808438301086\n",
      "Training loss for batch 2217 : 0.09770198166370392\n",
      "Training loss for batch 2218 : 0.02657676301896572\n",
      "Training loss for batch 2219 : 0.8772932887077332\n",
      "Training loss for batch 2220 : 0.5631292462348938\n",
      "Training loss for batch 2221 : 0.2032943069934845\n",
      "Training loss for batch 2222 : 0.30205315351486206\n",
      "Training loss for batch 2223 : 0.2217022180557251\n",
      "Training loss for batch 2224 : 0.14925621449947357\n",
      "Training loss for batch 2225 : 0.16874781250953674\n",
      "Training loss for batch 2226 : 0.5576983094215393\n",
      "Training loss for batch 2227 : 0.14741109311580658\n",
      "Training loss for batch 2228 : 0.4111837148666382\n",
      "Training loss for batch 2229 : 0.20604655146598816\n",
      "Training loss for batch 2230 : 0.3103090524673462\n",
      "Training loss for batch 2231 : 0.5140786170959473\n",
      "Training loss for batch 2232 : 0.19865864515304565\n",
      "Training loss for batch 2233 : 0.46979111433029175\n",
      "Training loss for batch 2234 : 0.20291829109191895\n",
      "Training loss for batch 2235 : 0.12603391706943512\n",
      "Training loss for batch 2236 : 0.05509428307414055\n",
      "Training loss for batch 2237 : 0.2465597689151764\n",
      "Training loss for batch 2238 : 0.34869185090065\n",
      "Training loss for batch 2239 : 0.5267384648323059\n",
      "Training loss for batch 2240 : 0.16213691234588623\n",
      "Training loss for batch 2241 : 0.24482351541519165\n",
      "Training loss for batch 2242 : 0.5783179402351379\n",
      "Training loss for batch 2243 : 0.14241236448287964\n",
      "Training loss for batch 2244 : 0.4110521972179413\n",
      "Training loss for batch 2245 : 0.25632932782173157\n",
      "Training loss for batch 2246 : 0.03311052918434143\n",
      "Training loss for batch 2247 : 0.3140246570110321\n",
      "Training loss for batch 2248 : 0.11714015156030655\n",
      "Training loss for batch 2249 : 0.3159346282482147\n",
      "Training loss for batch 2250 : 0.047943197190761566\n",
      "Training loss for batch 2251 : 0.28390228748321533\n",
      "Training loss for batch 2252 : 0.3572292923927307\n",
      "Training loss for batch 2253 : 1.0952502489089966\n",
      "Training loss for batch 2254 : 0.2630038559436798\n",
      "Training loss for batch 2255 : 0.19777812063694\n",
      "Training loss for batch 2256 : 0.3288102149963379\n",
      "Training loss for batch 2257 : 0.2111068069934845\n",
      "Training loss for batch 2258 : 0.6433512568473816\n",
      "Training loss for batch 2259 : 0.40035444498062134\n",
      "Training loss for batch 2260 : 0.17507705092430115\n",
      "Training loss for batch 2261 : 0.05002062767744064\n",
      "Training loss for batch 2262 : 0.16535243391990662\n",
      "Training loss for batch 2263 : 0.32127344608306885\n",
      "Training loss for batch 2264 : 0.5495752096176147\n",
      "Training loss for batch 2265 : 0.20981425046920776\n",
      "Training loss for batch 2266 : 0.08239912986755371\n",
      "Training loss for batch 2267 : 0.09678148478269577\n",
      "Training loss for batch 2268 : 0.45592236518859863\n",
      "Training loss for batch 2269 : 0.26002928614616394\n",
      "Training loss for batch 2270 : 0.48739275336265564\n",
      "Training loss for batch 2271 : 0.05877650901675224\n",
      "Training loss for batch 2272 : 0.3390073776245117\n",
      "Training loss for batch 2273 : 0.3902145326137543\n",
      "Training loss for batch 2274 : 0.527976393699646\n",
      "Training loss for batch 2275 : 0.4031974971294403\n",
      "Training loss for batch 2276 : 0.6245158314704895\n",
      "Training loss for batch 2277 : 0.30011263489723206\n",
      "Training loss for batch 2278 : 0.07005728036165237\n",
      "Training loss for batch 2279 : 0.4181223213672638\n",
      "Training loss for batch 2280 : 0.6267005205154419\n",
      "Training loss for batch 2281 : 0.18703356385231018\n",
      "Training loss for batch 2282 : 0.24071866273880005\n",
      "Training loss for batch 2283 : 0.028288861736655235\n",
      "Training loss for batch 2284 : 0.30492591857910156\n",
      "Training loss for batch 2285 : 0.14188307523727417\n",
      "Training loss for batch 2286 : 0.39098548889160156\n",
      "Training loss for batch 2287 : 0.6276038289070129\n",
      "Training loss for batch 2288 : 0.9171491265296936\n",
      "Training loss for batch 2289 : 0.1536189168691635\n",
      "Training loss for batch 2290 : 0.48132866621017456\n",
      "Training loss for batch 2291 : 0.2156742662191391\n",
      "Training loss for batch 2292 : 0.28941115736961365\n",
      "Training loss for batch 2293 : 0.049140721559524536\n",
      "Training loss for batch 2294 : 0.7200304865837097\n",
      "Training loss for batch 2295 : 0.5999035239219666\n",
      "Training loss for batch 2296 : 0.15476110577583313\n",
      "Training loss for batch 2297 : 0.38259145617485046\n",
      "Training loss for batch 2298 : 0.18822787702083588\n",
      "Training loss for batch 2299 : 0.23429206013679504\n",
      "Training loss for batch 2300 : 0.22809721529483795\n",
      "Training loss for batch 2301 : 0.23435738682746887\n",
      "Training loss for batch 2302 : 0.027066126465797424\n",
      "Training loss for batch 2303 : 0.24468757212162018\n",
      "Training loss for batch 2304 : 0.1637575626373291\n",
      "Training loss for batch 2305 : 0.39045459032058716\n",
      "Training loss for batch 2306 : 0.3611479699611664\n",
      "Training loss for batch 2307 : 0.6634824275970459\n",
      "Training loss for batch 2308 : 0.3926355838775635\n",
      "Training loss for batch 2309 : 0.7546536922454834\n",
      "Training loss for batch 2310 : 0.2608030140399933\n",
      "Training loss for batch 2311 : 0.3641223609447479\n",
      "Training loss for batch 2312 : 0.1953498274087906\n",
      "Training loss for batch 2313 : 0.2975664734840393\n",
      "Training loss for batch 2314 : 0.5791590809822083\n",
      "Training loss for batch 2315 : 0.3265480399131775\n",
      "Training loss for batch 2316 : 0.2067165970802307\n",
      "Training loss for batch 2317 : 0.17706681787967682\n",
      "Training loss for batch 2318 : 0.23384369909763336\n",
      "Training loss for batch 2319 : 0.5167401432991028\n",
      "Training loss for batch 2320 : 0.07129189372062683\n",
      "Training loss for batch 2321 : 0.701865017414093\n",
      "Training loss for batch 2322 : 0.39352187514305115\n",
      "Training loss for batch 2323 : 0.2558705806732178\n",
      "Training loss for batch 2324 : 0.48204663395881653\n",
      "Training loss for batch 2325 : 0.03273072838783264\n",
      "Training loss for batch 2326 : 0.47154295444488525\n",
      "Training loss for batch 2327 : 0.3089306950569153\n",
      "Training loss for batch 2328 : 0.28789645433425903\n",
      "Training loss for batch 2329 : 0.08795413374900818\n",
      "Training loss for batch 2330 : 0.10545922815799713\n",
      "Training loss for batch 2331 : 0.5937542915344238\n",
      "Training loss for batch 2332 : 0.01945006474852562\n",
      "Training loss for batch 2333 : 0.5878478288650513\n",
      "Training loss for batch 2334 : 0.1727484166622162\n",
      "Training loss for batch 2335 : 0.15950575470924377\n",
      "Training loss for batch 2336 : 0.23765043914318085\n",
      "Training loss for batch 2337 : 0.09992298483848572\n",
      "Training loss for batch 2338 : 0.25970378518104553\n",
      "Training loss for batch 2339 : 0.19070746004581451\n",
      "Training loss for batch 2340 : 0.2519068121910095\n",
      "Training loss for batch 2341 : 0.32119515538215637\n",
      "Training loss for batch 2342 : 0.28668636083602905\n",
      "Training loss for batch 2343 : 0.16847555339336395\n",
      "Training loss for batch 2344 : 0.0660841315984726\n",
      "Training loss for batch 2345 : 0.5289609432220459\n",
      "Training loss for batch 2346 : 0.4337199032306671\n",
      "Training loss for batch 2347 : 0.3907031714916229\n",
      "Training loss for batch 2348 : 0.06440325826406479\n",
      "Training loss for batch 2349 : 0.6272384524345398\n",
      "Training loss for batch 2350 : 0.37216612696647644\n",
      "Training loss for batch 2351 : 0.5493253469467163\n",
      "Training loss for batch 2352 : 0.02835744246840477\n",
      "Training loss for batch 2353 : 0.16849054396152496\n",
      "Training loss for batch 2354 : 0.10699900984764099\n",
      "Training loss for batch 2355 : 0.24268318712711334\n",
      "Training loss for batch 2356 : 0.07164128869771957\n",
      "Training loss for batch 2357 : 0.7187005281448364\n",
      "Training loss for batch 2358 : 0.24383851885795593\n",
      "Training loss for batch 2359 : 0.39747920632362366\n",
      "Training loss for batch 2360 : 0.7218878269195557\n",
      "Training loss for batch 2361 : 0.5835623145103455\n",
      "Training loss for batch 2362 : 0.20404112339019775\n",
      "Training loss for batch 2363 : 0.14357835054397583\n",
      "Training loss for batch 2364 : 0.3031807839870453\n",
      "Training loss for batch 2365 : 0.438703328371048\n",
      "Training loss for batch 2366 : 0.17951039969921112\n",
      "Training loss for batch 2367 : 0.028747975826263428\n",
      "Training loss for batch 2368 : 0.21974676847457886\n",
      "Training loss for batch 2369 : 0.4364043176174164\n",
      "Training loss for batch 2370 : 0.35794568061828613\n",
      "Training loss for batch 2371 : 0.42715156078338623\n",
      "Training loss for batch 2372 : 0.2507808208465576\n",
      "Training loss for batch 2373 : 0.21360500156879425\n",
      "Training loss for batch 2374 : 0.38474008440971375\n",
      "Training loss for batch 2375 : 0.4295331537723541\n",
      "Training loss for batch 2376 : 0.31363433599472046\n",
      "Training loss for batch 2377 : 0.13541249930858612\n",
      "Training loss for batch 2378 : 0.13750268518924713\n",
      "Training loss for batch 2379 : 0.4379768967628479\n",
      "Training loss for batch 2380 : 0.11898791790008545\n",
      "Training loss for batch 2381 : 0.48696407675743103\n",
      "Training loss for batch 2382 : 0.4400767982006073\n",
      "Training loss for batch 2383 : 0.36987605690956116\n",
      "Training loss for batch 2384 : 0.6505810618400574\n",
      "Training loss for batch 2385 : 0.9832732677459717\n",
      "Training loss for batch 2386 : 0.17931097745895386\n",
      "Training loss for batch 2387 : 0.48994511365890503\n",
      "Training loss for batch 2388 : 0.17358526587486267\n",
      "Training loss for batch 2389 : 0.3480473756790161\n",
      "Training loss for batch 2390 : 0.3046993613243103\n",
      "Training loss for batch 2391 : 0.44440317153930664\n",
      "Training loss for batch 2392 : 0.2767261266708374\n",
      "Training loss for batch 2393 : 0.1949990689754486\n",
      "Training loss for batch 2394 : 0.583687961101532\n",
      "Training loss for batch 2395 : 0.006688719615340233\n",
      "Training loss for batch 2396 : 0.11196353286504745\n",
      "Training loss for batch 2397 : 0.09597121179103851\n",
      "Training loss for batch 2398 : 0.18373361229896545\n",
      "Training loss for batch 2399 : 0.6732219457626343\n",
      "Training loss for batch 2400 : 0.18298520147800446\n",
      "Training loss for batch 2401 : 0.7802897691726685\n",
      "Training loss for batch 2402 : 0.31609565019607544\n",
      "Training loss for batch 2403 : 0.16696099936962128\n",
      "Training loss for batch 2404 : 0.09891560673713684\n",
      "Training loss for batch 2405 : 0.5994346141815186\n",
      "Training loss for batch 2406 : 0.4919559955596924\n",
      "Training loss for batch 2407 : 0.2578671872615814\n",
      "Training loss for batch 2408 : 0.09536122530698776\n",
      "Training loss for batch 2409 : 0.07114332914352417\n",
      "Training loss for batch 2410 : 0.3522642254829407\n",
      "Training loss for batch 2411 : 0.4024163782596588\n",
      "Training loss for batch 2412 : 0.3931422829627991\n",
      "Training loss for batch 2413 : 0.16695934534072876\n",
      "Training loss for batch 2414 : 0.643665611743927\n",
      "Training loss for batch 2415 : 0.2629357576370239\n",
      "Training loss for batch 2416 : 0.3114025294780731\n",
      "Training loss for batch 2417 : 1.0042885541915894\n",
      "Training loss for batch 2418 : 0.7532909512519836\n",
      "Training loss for batch 2419 : 0.22732128202915192\n",
      "Training loss for batch 2420 : 0.9094494581222534\n",
      "Training loss for batch 2421 : 0.5765900611877441\n",
      "Training loss for batch 2422 : 0.4236341714859009\n",
      "Training loss for batch 2423 : 0.49581512808799744\n",
      "Training loss for batch 2424 : 0.12395520508289337\n",
      "Training loss for batch 2425 : 0.07386244833469391\n",
      "Training loss for batch 2426 : 0.36854445934295654\n",
      "Training loss for batch 2427 : 0.13496187329292297\n",
      "Training loss for batch 2428 : 0.09879246354103088\n",
      "Training loss for batch 2429 : 0.39479464292526245\n",
      "Training loss for batch 2430 : 0.3321179747581482\n",
      "Training loss for batch 2431 : 0.5494592785835266\n",
      "Training loss for batch 2432 : 0.06284601986408234\n",
      "Training loss for batch 2433 : 0.32830509543418884\n",
      "Training loss for batch 2434 : 0.1731007993221283\n",
      "Training loss for batch 2435 : 0.1443169116973877\n",
      "Training loss for batch 2436 : 0.4684563875198364\n",
      "Training loss for batch 2437 : 0.3375609815120697\n",
      "Training loss for batch 2438 : 0.5019313097000122\n",
      "Training loss for batch 2439 : 0.26781976222991943\n",
      "Training loss for batch 2440 : 0.446897029876709\n",
      "Training loss for batch 2441 : 0.7185566425323486\n",
      "Training loss for batch 2442 : 0.35081103444099426\n",
      "Training loss for batch 2443 : 0.19303791224956512\n",
      "Training loss for batch 2444 : 0.2159779667854309\n",
      "Training loss for batch 2445 : 0.18347761034965515\n",
      "Training loss for batch 2446 : 0.37557169795036316\n",
      "Training loss for batch 2447 : 0.2255643606185913\n",
      "Training loss for batch 2448 : 0.7485875487327576\n",
      "Training loss for batch 2449 : 0.5955273509025574\n",
      "Training loss for batch 2450 : 0.3308764398097992\n",
      "Training loss for batch 2451 : 0.2748788297176361\n",
      "Training loss for batch 2452 : 0.26247212290763855\n",
      "Training loss for batch 2453 : 0.45213982462882996\n",
      "Training loss for batch 2454 : 0.46104609966278076\n",
      "Training loss for batch 2455 : 0.0861915722489357\n",
      "Training loss for batch 2456 : 0.2103961706161499\n",
      "Training loss for batch 2457 : 0.2476394921541214\n",
      "Training loss for batch 2458 : 0.1513088196516037\n",
      "Training loss for batch 2459 : 0.6345755457878113\n",
      "Training loss for batch 2460 : 0.17880363762378693\n",
      "Training loss for batch 2461 : 0.33868294954299927\n",
      "Training loss for batch 2462 : 0.05535493791103363\n",
      "Training loss for batch 2463 : 0.12987923622131348\n",
      "Training loss for batch 2464 : 0.266171932220459\n",
      "Training loss for batch 2465 : 0.11298827826976776\n",
      "Training loss for batch 2466 : 0.3900132477283478\n",
      "Training loss for batch 2467 : 0.21956393122673035\n",
      "Training loss for batch 2468 : 0.4308519959449768\n",
      "Training loss for batch 2469 : 0.20954926311969757\n",
      "Training loss for batch 2470 : 0.1192920133471489\n",
      "Training loss for batch 2471 : 0.44584184885025024\n",
      "Training loss for batch 2472 : 0.2951456606388092\n",
      "Training loss for batch 2473 : 0.38041189312934875\n",
      "Training loss for batch 2474 : 0.36446455121040344\n",
      "Training loss for batch 2475 : 0.14335733652114868\n",
      "Training loss for batch 2476 : 0.19932720065116882\n",
      "Training loss for batch 2477 : 0.5932375192642212\n",
      "Training loss for batch 2478 : 0.15626755356788635\n",
      "Training loss for batch 2479 : 0.4577859044075012\n",
      "Training loss for batch 2480 : 0.2278514802455902\n",
      "Training loss for batch 2481 : 0.17285814881324768\n",
      "Training loss for batch 2482 : 0.4371351897716522\n",
      "Training loss for batch 2483 : 0.5791925191879272\n",
      "Training loss for batch 2484 : 0.36882856488227844\n",
      "Training loss for batch 2485 : 0.4103572368621826\n",
      "Training loss for batch 2486 : 0.0709250345826149\n",
      "Training loss for batch 2487 : 0.2676791548728943\n",
      "Training loss for batch 2488 : 0.4205133616924286\n",
      "Training loss for batch 2489 : 0.22308020293712616\n",
      "Training loss for batch 2490 : 0.5094467997550964\n",
      "Training loss for batch 2491 : 0.2954988479614258\n",
      "Training loss for batch 2492 : 0.30365341901779175\n",
      "Training loss for batch 2493 : 0.22510619461536407\n",
      "Training loss for batch 2494 : 0.31530946493148804\n",
      "Training loss for batch 2495 : 0.18972286581993103\n",
      "Training loss for batch 2496 : 0.06887039542198181\n",
      "Training loss for batch 2497 : 0.46356073021888733\n",
      "Training loss for batch 2498 : 0.5563625693321228\n",
      "Training loss for batch 2499 : 0.5329553484916687\n",
      "Training loss for batch 2500 : 0.873558521270752\n",
      "Training loss for batch 2501 : 0.11590220034122467\n",
      "Training loss for batch 2502 : 0.4225727319717407\n",
      "Training loss for batch 2503 : 0.15669599175453186\n",
      "Training loss for batch 2504 : 0.04768383875489235\n",
      "Training loss for batch 2505 : 0.20230042934417725\n",
      "Training loss for batch 2506 : 0.06766761839389801\n",
      "Training loss for batch 2507 : 0.31125253438949585\n",
      "Training loss for batch 2508 : 0.24817410111427307\n",
      "Training loss for batch 2509 : 0.217721626162529\n",
      "Training loss for batch 2510 : 0.37266504764556885\n",
      "Training loss for batch 2511 : 0.15708377957344055\n",
      "Training loss for batch 2512 : 0.6276621222496033\n",
      "Training loss for batch 2513 : 0.45460161566734314\n",
      "Training loss for batch 2514 : 0.0492834597826004\n",
      "Training loss for batch 2515 : 0.1280735582113266\n",
      "Training loss for batch 2516 : 0.42311999201774597\n",
      "Training loss for batch 2517 : 0.18299756944179535\n",
      "Training loss for batch 2518 : 0.3819890022277832\n",
      "Training loss for batch 2519 : 0.33849889039993286\n",
      "Training loss for batch 2520 : 0.2620692849159241\n",
      "Training loss for batch 2521 : 0.34113332629203796\n",
      "Training loss for batch 2522 : 0.12092254310846329\n",
      "Training loss for batch 2523 : 0.2730117738246918\n",
      "Training loss for batch 2524 : 0.09660815447568893\n",
      "Training loss for batch 2525 : 0.42509374022483826\n",
      "Training loss for batch 2526 : 0.14107608795166016\n",
      "Training loss for batch 2527 : 0.15397846698760986\n",
      "Training loss for batch 2528 : 0.3071630895137787\n",
      "Training loss for batch 2529 : 0.3593294322490692\n",
      "Training loss for batch 2530 : 0.5052775740623474\n",
      "Training loss for batch 2531 : 0.19415560364723206\n",
      "Training loss for batch 2532 : 0.4743027985095978\n",
      "Training loss for batch 2533 : 0.4956550598144531\n",
      "Training loss for batch 2534 : 0.3279825448989868\n",
      "Training loss for batch 2535 : 0.10071856528520584\n",
      "Training loss for batch 2536 : 0.20478302240371704\n",
      "Training loss for batch 2537 : 0.4229656457901001\n",
      "Training loss for batch 2538 : 0.5821996927261353\n",
      "Training loss for batch 2539 : 0.45801374316215515\n",
      "Training loss for batch 2540 : 0.45018869638442993\n",
      "Training loss for batch 2541 : 0.3428787589073181\n",
      "Training loss for batch 2542 : 0.21308888494968414\n",
      "Training loss for batch 2543 : 0.4225586950778961\n",
      "Training loss for batch 2544 : 0.06718561053276062\n",
      "Training loss for batch 2545 : 0.1359625905752182\n",
      "Training loss for batch 2546 : 0.2138802409172058\n",
      "Training loss for batch 2547 : 0.3550916016101837\n",
      "Training loss for batch 2548 : 0.48246684670448303\n",
      "Training loss for batch 2549 : 0.3228711187839508\n",
      "Training loss for batch 2550 : 0.6372733116149902\n",
      "Training loss for batch 2551 : 0.1303998827934265\n",
      "Training loss for batch 2552 : 0.3604470193386078\n",
      "Training loss for batch 2553 : 0.0972318947315216\n",
      "Training loss for batch 2554 : 0.4108515977859497\n",
      "Training loss for batch 2555 : 0.039965301752090454\n",
      "Training loss for batch 2556 : 0.05054337903857231\n",
      "Training loss for batch 2557 : 0.10081683099269867\n",
      "Training loss for batch 2558 : 0.4675042927265167\n",
      "Training loss for batch 2559 : 0.16266629099845886\n",
      "Training loss for batch 2560 : 0.7615309953689575\n",
      "Training loss for batch 2561 : 0.23664720356464386\n",
      "Training loss for batch 2562 : 0.016800981014966965\n",
      "Training loss for batch 2563 : 0.5208467245101929\n",
      "Training loss for batch 2564 : 0.11332513391971588\n",
      "Training loss for batch 2565 : 0.48826780915260315\n",
      "Training loss for batch 2566 : 0.34565237164497375\n",
      "Training loss for batch 2567 : 0.40320414304733276\n",
      "Training loss for batch 2568 : 0.40029484033584595\n",
      "Training loss for batch 2569 : 0.005402111914008856\n",
      "Training loss for batch 2570 : 0.20990104973316193\n",
      "Training loss for batch 2571 : 0.2620125412940979\n",
      "Training loss for batch 2572 : 0.22006726264953613\n",
      "Training loss for batch 2573 : 0.3263416588306427\n",
      "Training loss for batch 2574 : 0.11941134184598923\n",
      "Training loss for batch 2575 : 0.4438045620918274\n",
      "Training loss for batch 2576 : 0.5976748466491699\n",
      "Training loss for batch 2577 : 0.9093043804168701\n",
      "Training loss for batch 2578 : 0.2968880534172058\n",
      "Training loss for batch 2579 : 0.26632770895957947\n",
      "Training loss for batch 2580 : 0.3459649384021759\n",
      "Training loss for batch 2581 : 0.06929841637611389\n",
      "Training loss for batch 2582 : 0.1454678326845169\n",
      "Training loss for batch 2583 : 0.05810518562793732\n",
      "Training loss for batch 2584 : 0.5629807114601135\n",
      "Training loss for batch 2585 : 0.16677257418632507\n",
      "Training loss for batch 2586 : 0.15000306069850922\n",
      "Training loss for batch 2587 : 0.2195155769586563\n",
      "Training loss for batch 2588 : 0.08034621924161911\n",
      "Training loss for batch 2589 : 0.17990703880786896\n",
      "Training loss for batch 2590 : 0.6966208815574646\n",
      "Training loss for batch 2591 : 0.3227979838848114\n",
      "Training loss for batch 2592 : 0.39526796340942383\n",
      "Training loss for batch 2593 : 0.3473406136035919\n",
      "Training loss for batch 2594 : 0.6450578570365906\n",
      "Training loss for batch 2595 : 0.011258801445364952\n",
      "Training loss for batch 2596 : 0.47352489829063416\n",
      "Training loss for batch 2597 : 0.35503917932510376\n",
      "Training loss for batch 2598 : 0.3883654773235321\n",
      "Training loss for batch 2599 : 0.5433543920516968\n",
      "Training loss for batch 2600 : 0.5606045722961426\n",
      "Training loss for batch 2601 : 0.08929868787527084\n",
      "Training loss for batch 2602 : 0.5499063730239868\n",
      "Training loss for batch 2603 : 0.31626957654953003\n",
      "Training loss for batch 2604 : 0.8211715817451477\n",
      "Training loss for batch 2605 : 0.33674705028533936\n",
      "Training loss for batch 2606 : 0.38530731201171875\n",
      "Training loss for batch 2607 : 0.43435555696487427\n",
      "Training loss for batch 2608 : 0.18338921666145325\n",
      "Training loss for batch 2609 : 0.39749330282211304\n",
      "Training loss for batch 2610 : 0.1675720512866974\n",
      "Training loss for batch 2611 : 0.44297367334365845\n",
      "Training loss for batch 2612 : 0.049572501331567764\n",
      "Training loss for batch 2613 : 0.13003043830394745\n",
      "Training loss for batch 2614 : 0.17738546431064606\n",
      "Training loss for batch 2615 : 0.009146500378847122\n",
      "Training loss for batch 2616 : 0.130328968167305\n",
      "Training loss for batch 2617 : 0.2675260305404663\n",
      "Training loss for batch 2618 : 0.20753422379493713\n",
      "Training loss for batch 2619 : 0.47811374068260193\n",
      "Training loss for batch 2620 : 0.19968773424625397\n",
      "Training loss for batch 2621 : 0.24018241465091705\n",
      "Training loss for batch 2622 : 0.3352823853492737\n",
      "Training loss for batch 2623 : 0.5930886268615723\n",
      "Training loss for batch 2624 : 0.5154595375061035\n",
      "Training loss for batch 2625 : 0.7284737229347229\n",
      "Training loss for batch 2626 : 0.3840869069099426\n",
      "Training loss for batch 2627 : 0.376598984003067\n",
      "Training loss for batch 2628 : 0.4954390525817871\n",
      "Training loss for batch 2629 : 0.3770986795425415\n",
      "Training loss for batch 2630 : 0.2818075120449066\n",
      "Training loss for batch 2631 : 0.09671791642904282\n",
      "Training loss for batch 2632 : 0.23713046312332153\n",
      "Training loss for batch 2633 : 0.11104950308799744\n",
      "Training loss for batch 2634 : 0.444527804851532\n",
      "Training loss for batch 2635 : 0.32879874110221863\n",
      "Training loss for batch 2636 : 0.31010663509368896\n",
      "Training loss for batch 2637 : 0.2051081508398056\n",
      "Training loss for batch 2638 : 0.07206033170223236\n",
      "Training loss for batch 2639 : 0.28497573733329773\n",
      "Training loss for batch 2640 : 0.6517408490180969\n",
      "Training loss for batch 2641 : 0.34124821424484253\n",
      "Training loss for batch 2642 : 0.1712767481803894\n",
      "Training loss for batch 2643 : 0.6966913938522339\n",
      "Training loss for batch 2644 : 0.19477656483650208\n",
      "Training loss for batch 2645 : 0.4696696698665619\n",
      "Training loss for batch 2646 : 0.2157280147075653\n",
      "Training loss for batch 2647 : 0.9209370613098145\n",
      "Training loss for batch 2648 : 0.2706162631511688\n",
      "Training loss for batch 2649 : 0.33828094601631165\n",
      "Training loss for batch 2650 : 0.1622287631034851\n",
      "Training loss for batch 2651 : 0.4611232876777649\n",
      "Training loss for batch 2652 : 0.3195975720882416\n",
      "Training loss for batch 2653 : 0.5007078647613525\n",
      "Training loss for batch 2654 : 0.12071211636066437\n",
      "Training loss for batch 2655 : 0.09934618324041367\n",
      "Training loss for batch 2656 : 0.28100675344467163\n",
      "Training loss for batch 2657 : 0.2591682970523834\n",
      "Training loss for batch 2658 : 0.552748441696167\n",
      "Training loss for batch 2659 : 0.22507838904857635\n",
      "Training loss for batch 2660 : 0.27995380759239197\n",
      "Training loss for batch 2661 : 0.20792172849178314\n",
      "Training loss for batch 2662 : 0.05654420703649521\n",
      "Training loss for batch 2663 : 0.3237130343914032\n",
      "Training loss for batch 2664 : 0.135761559009552\n",
      "Training loss for batch 2665 : 0.40983492136001587\n",
      "Training loss for batch 2666 : 0.22253581881523132\n",
      "Training loss for batch 2667 : 0.7684377431869507\n",
      "Training loss for batch 2668 : 0.49024850130081177\n",
      "Training loss for batch 2669 : 0.45513156056404114\n",
      "Training loss for batch 2670 : 0.4716514050960541\n",
      "Training loss for batch 2671 : 0.026134705170989037\n",
      "Training loss for batch 2672 : 0.14116236567497253\n",
      "Training loss for batch 2673 : 0.48362383246421814\n",
      "Training loss for batch 2674 : 0.161362886428833\n",
      "Training loss for batch 2675 : 0.18308798968791962\n",
      "Training loss for batch 2676 : 0.5174686908721924\n",
      "Training loss for batch 2677 : 0.08293787389993668\n",
      "Training loss for batch 2678 : 0.33633556962013245\n",
      "Training loss for batch 2679 : 0.02085588127374649\n",
      "Training loss for batch 2680 : 0.05766080319881439\n",
      "Training loss for batch 2681 : 0.4915853440761566\n",
      "Training loss for batch 2682 : 0.3441183865070343\n",
      "Training loss for batch 2683 : 0.30195099115371704\n",
      "Training loss for batch 2684 : 0.24012626707553864\n",
      "Training loss for batch 2685 : 0.3259201645851135\n",
      "Training loss for batch 2686 : 0.21696673333644867\n",
      "Training loss for batch 2687 : 0.41751629114151\n",
      "Training loss for batch 2688 : 0.456401526927948\n",
      "Training loss for batch 2689 : 0.1741124540567398\n",
      "Training loss for batch 2690 : 0.2936903238296509\n",
      "Training loss for batch 2691 : 0.152048259973526\n",
      "Training loss for batch 2692 : 0.3404150605201721\n",
      "Training loss for batch 2693 : 0.04602393880486488\n",
      "Training loss for batch 2694 : 0.44031065702438354\n",
      "Training loss for batch 2695 : 0.5365133285522461\n",
      "Training loss for batch 2696 : 0.13218696415424347\n",
      "Training loss for batch 2697 : 0.14487037062644958\n",
      "Training loss for batch 2698 : 0.32046234607696533\n",
      "Training loss for batch 2699 : 0.14624521136283875\n",
      "Training loss for batch 2700 : 0.35060814023017883\n",
      "Training loss for batch 2701 : 0.03547336161136627\n",
      "Training loss for batch 2702 : 0.24844588339328766\n",
      "Training loss for batch 2703 : 0.28174319863319397\n",
      "Training loss for batch 2704 : 0.29930800199508667\n",
      "Training loss for batch 2705 : 0.33077260851860046\n",
      "Training loss for batch 2706 : 0.1337040513753891\n",
      "Training loss for batch 2707 : 0.05345417186617851\n",
      "Training loss for batch 2708 : 0.11276144534349442\n",
      "Training loss for batch 2709 : 0.6119175553321838\n",
      "Training loss for batch 2710 : 0.09789353609085083\n",
      "Training loss for batch 2711 : 0.16831998527050018\n",
      "Training loss for batch 2712 : 0.12328901886940002\n",
      "Training loss for batch 2713 : 0.4291510283946991\n",
      "Training loss for batch 2714 : 0.0834277793765068\n",
      "Training loss for batch 2715 : 0.1180112436413765\n",
      "Training loss for batch 2716 : 0.10011205077171326\n",
      "Training loss for batch 2717 : 0.06220218911767006\n",
      "Training loss for batch 2718 : 0.32735660672187805\n",
      "Training loss for batch 2719 : 0.34988531470298767\n",
      "Training loss for batch 2720 : 0.36356544494628906\n",
      "Training loss for batch 2721 : 0.15139122307300568\n",
      "Training loss for batch 2722 : 0.19220301508903503\n",
      "Training loss for batch 2723 : 0.4313506484031677\n",
      "Training loss for batch 2724 : 0.4546824097633362\n",
      "Training loss for batch 2725 : 0.2731988728046417\n",
      "Training loss for batch 2726 : 0.48686566948890686\n",
      "Training loss for batch 2727 : 0.19508817791938782\n",
      "Training loss for batch 2728 : 0.16536100208759308\n",
      "Training loss for batch 2729 : 0.5571187734603882\n",
      "Training loss for batch 2730 : 0.3708096742630005\n",
      "Training loss for batch 2731 : 0.15643322467803955\n",
      "Training loss for batch 2732 : 0.13877946138381958\n",
      "Training loss for batch 2733 : 0.19443011283874512\n",
      "Training loss for batch 2734 : 0.34936901926994324\n",
      "Training loss for batch 2735 : 0.2689671218395233\n",
      "Training loss for batch 2736 : 0.3569108247756958\n",
      "Training loss for batch 2737 : 0.35521015524864197\n",
      "Training loss for batch 2738 : 0.43176907300949097\n",
      "Training loss for batch 2739 : 0.6433005332946777\n",
      "Training loss for batch 2740 : 0.856489896774292\n",
      "Training loss for batch 2741 : 0.3880471885204315\n",
      "Training loss for batch 2742 : 0.2510663866996765\n",
      "Training loss for batch 2743 : 0.2956828773021698\n",
      "Training loss for batch 2744 : 0.1535862535238266\n",
      "Training loss for batch 2745 : 0.3281198740005493\n",
      "Training loss for batch 2746 : 0.3384043574333191\n",
      "Training loss for batch 2747 : 0.49878349900245667\n",
      "Training loss for batch 2748 : 0.25033727288246155\n",
      "Training loss for batch 2749 : 0.1783267706632614\n",
      "Training loss for batch 2750 : 0.1380200982093811\n",
      "Training loss for batch 2751 : 0.17538820207118988\n",
      "Training loss for batch 2752 : 0.49060672521591187\n",
      "Training loss for batch 2753 : 0.16048312187194824\n",
      "Training loss for batch 2754 : 0.623684287071228\n",
      "Training loss for batch 2755 : 0.5251185894012451\n",
      "Training loss for batch 2756 : 0.4839237630367279\n",
      "Training loss for batch 2757 : 0.41210752725601196\n",
      "Training loss for batch 2758 : 0.10014575719833374\n",
      "Training loss for batch 2759 : 0.09079296886920929\n",
      "Training loss for batch 2760 : 0.3084249198436737\n",
      "Training loss for batch 2761 : 0.048377856612205505\n",
      "Training loss for batch 2762 : 0.34517037868499756\n",
      "Training loss for batch 2763 : 0.3760283291339874\n",
      "Training loss for batch 2764 : 0.22781269252300262\n",
      "Training loss for batch 2765 : 0.35605207085609436\n",
      "Training loss for batch 2766 : 0.6100878119468689\n",
      "Training loss for batch 2767 : 0.04800073057413101\n",
      "Training loss for batch 2768 : 0.07244236767292023\n",
      "Training loss for batch 2769 : 0.25474274158477783\n",
      "Training loss for batch 2770 : 0.18429630994796753\n",
      "Training loss for batch 2771 : 0.2605913281440735\n",
      "Training loss for batch 2772 : 0.21031858026981354\n",
      "Training loss for batch 2773 : 0.567398190498352\n",
      "Training loss for batch 2774 : 0.5061860084533691\n",
      "Training loss for batch 2775 : 0.32448792457580566\n",
      "Training loss for batch 2776 : 0.47988471388816833\n",
      "Training loss for batch 2777 : 0.11427295953035355\n",
      "Training loss for batch 2778 : 0.2511945068836212\n",
      "Training loss for batch 2779 : 0.31974563002586365\n",
      "Training loss for batch 2780 : 0.6016494631767273\n",
      "Training loss for batch 2781 : 0.5941763520240784\n",
      "Training loss for batch 2782 : 0.27672895789146423\n",
      "Training loss for batch 2783 : 0.27607226371765137\n",
      "Training loss for batch 2784 : 0.5352437496185303\n",
      "Training loss for batch 2785 : 0.1635953187942505\n",
      "Training loss for batch 2786 : 0.13725772500038147\n",
      "Training loss for batch 2787 : 0.31690868735313416\n",
      "Training loss for batch 2788 : 0.4861406683921814\n",
      "Training loss for batch 2789 : 0.09418267756700516\n",
      "Training loss for batch 2790 : 0.4507659673690796\n",
      "Training loss for batch 2791 : 0.30124783515930176\n",
      "Training loss for batch 2792 : 0.1508508324623108\n",
      "Training loss for batch 2793 : 0.17090485990047455\n",
      "Training loss for batch 2794 : 0.6103855967521667\n",
      "Training loss for batch 2795 : 0.6740307807922363\n",
      "Training loss for batch 2796 : 0.1338406652212143\n",
      "Training loss for batch 2797 : 0.30876028537750244\n",
      "Training loss for batch 2798 : 0.47703835368156433\n",
      "Training loss for batch 2799 : 0.11775369942188263\n",
      "Training loss for batch 2800 : 0.0419258289039135\n",
      "Training loss for batch 2801 : 0.4576704204082489\n",
      "Training loss for batch 2802 : 0.2394041270017624\n",
      "Training loss for batch 2803 : 0.1806347519159317\n",
      "Training loss for batch 2804 : 0.26301607489585876\n",
      "Training loss for batch 2805 : 0.016345012933015823\n",
      "Training loss for batch 2806 : 0.28214675188064575\n",
      "Training loss for batch 2807 : 0.1700039505958557\n",
      "Training loss for batch 2808 : 0.3009466826915741\n",
      "Training loss for batch 2809 : 0.2774367332458496\n",
      "Training loss for batch 2810 : 0.2984500527381897\n",
      "Training loss for batch 2811 : 0.11522644758224487\n",
      "Training loss for batch 2812 : 0.4607302248477936\n",
      "Training loss for batch 2813 : 0.519603431224823\n",
      "Training loss for batch 2814 : 0.18358953297138214\n",
      "Training loss for batch 2815 : 0.10234393179416656\n",
      "Training loss for batch 2816 : 0.2008645087480545\n",
      "Training loss for batch 2817 : 0.1489078402519226\n",
      "Training loss for batch 2818 : 0.19006100296974182\n",
      "Training loss for batch 2819 : 0.34253397583961487\n",
      "Training loss for batch 2820 : 0.43873536586761475\n",
      "Training loss for batch 2821 : 0.24805288016796112\n",
      "Training loss for batch 2822 : 0.3961948752403259\n",
      "Training loss for batch 2823 : 0.05360035225749016\n",
      "Training loss for batch 2824 : 0.1921577751636505\n",
      "Training loss for batch 2825 : 0.04579230397939682\n",
      "Training loss for batch 2826 : 0.07351968437433243\n",
      "Training loss for batch 2827 : 0.10668954253196716\n",
      "Training loss for batch 2828 : 0.2570704221725464\n",
      "Training loss for batch 2829 : 0.39331483840942383\n",
      "Training loss for batch 2830 : 0.2154865711927414\n",
      "Training loss for batch 2831 : 0.6831130385398865\n",
      "Training loss for batch 2832 : 0.13011814653873444\n",
      "Training loss for batch 2833 : 0.23195718228816986\n",
      "Training loss for batch 2834 : 0.26757723093032837\n",
      "Training loss for batch 2835 : 0.04847434535622597\n",
      "Training loss for batch 2836 : 0.38636258244514465\n",
      "Training loss for batch 2837 : 0.27991124987602234\n",
      "Training loss for batch 2838 : 0.6019304394721985\n",
      "Training loss for batch 2839 : 0.703167736530304\n",
      "Training loss for batch 2840 : 0.2746792733669281\n",
      "Training loss for batch 2841 : 0.0334639847278595\n",
      "Training loss for batch 2842 : 0.46169188618659973\n",
      "Training loss for batch 2843 : 0.08575933426618576\n",
      "Training loss for batch 2844 : 0.010239144787192345\n",
      "Training loss for batch 2845 : 0.1336122453212738\n",
      "Training loss for batch 2846 : 0.23614458739757538\n",
      "Training loss for batch 2847 : 0.2403896152973175\n",
      "Training loss for batch 2848 : 0.1889422982931137\n",
      "Training loss for batch 2849 : 0.07340961694717407\n",
      "Training loss for batch 2850 : 0.04710128530859947\n",
      "Training loss for batch 2851 : 0.056039053946733475\n",
      "Training loss for batch 2852 : 0.33516135811805725\n",
      "Training loss for batch 2853 : 0.03984694182872772\n",
      "Training loss for batch 2854 : 0.460273802280426\n",
      "Training loss for batch 2855 : 0.11024662107229233\n",
      "Training loss for batch 2856 : 0.16114240884780884\n",
      "Training loss for batch 2857 : 0.698308527469635\n",
      "Training loss for batch 2858 : 0.08572795987129211\n",
      "Training loss for batch 2859 : 0.20305995643138885\n",
      "Training loss for batch 2860 : 0.4523639976978302\n",
      "Training loss for batch 2861 : 0.3721650540828705\n",
      "Training loss for batch 2862 : 0.18889765441417694\n",
      "Training loss for batch 2863 : 0.39243316650390625\n",
      "Training loss for batch 2864 : 0.2615564167499542\n",
      "Training loss for batch 2865 : 0.4226683974266052\n",
      "Training loss for batch 2866 : 0.11983799189329147\n",
      "Training loss for batch 2867 : 0.899864137172699\n",
      "Training loss for batch 2868 : 0.30836790800094604\n",
      "Training loss for batch 2869 : 0.1523805558681488\n",
      "Training loss for batch 2870 : 0.48094385862350464\n",
      "Training loss for batch 2871 : 0.08103244751691818\n",
      "Training loss for batch 2872 : 0.39642655849456787\n",
      "Training loss for batch 2873 : 0.5610741376876831\n",
      "Training loss for batch 2874 : 0.42109447717666626\n",
      "Training loss for batch 2875 : 0.2530628740787506\n",
      "Training loss for batch 2876 : 0.06446938961744308\n",
      "Training loss for batch 2877 : 0.10818353295326233\n",
      "Training loss for batch 2878 : 0.04119938239455223\n",
      "Training loss for batch 2879 : 0.472562313079834\n",
      "Training loss for batch 2880 : 0.11499002575874329\n",
      "Training loss for batch 2881 : 0.15411245822906494\n",
      "Training loss for batch 2882 : 0.3193395137786865\n",
      "Training loss for batch 2883 : 0.023467516526579857\n",
      "Training loss for batch 2884 : 0.31684911251068115\n",
      "Training loss for batch 2885 : 0.08399858325719833\n",
      "Training loss for batch 2886 : 0.21386708319187164\n",
      "Training loss for batch 2887 : 1.2579832077026367\n",
      "Training loss for batch 2888 : 0.16194294393062592\n",
      "Training loss for batch 2889 : 0.057105664163827896\n",
      "Training loss for batch 2890 : 0.5305197238922119\n",
      "Training loss for batch 2891 : 0.4549466073513031\n",
      "Training loss for batch 2892 : 0.22280192375183105\n",
      "Training loss for batch 2893 : 0.15477801859378815\n",
      "Training loss for batch 2894 : 0.12740646302700043\n",
      "Training loss for batch 2895 : 0.7568427920341492\n",
      "Training loss for batch 2896 : 0.36850398778915405\n",
      "Training loss for batch 2897 : 0.8325578570365906\n",
      "Training loss for batch 2898 : 0.4678466320037842\n",
      "Training loss for batch 2899 : 0.13629363477230072\n",
      "Training loss for batch 2900 : 0.10394304245710373\n",
      "Training loss for batch 2901 : 0.39314016699790955\n",
      "Training loss for batch 2902 : 0.5801394581794739\n",
      "Training loss for batch 2903 : 0.048533763736486435\n",
      "Training loss for batch 2904 : 0.3688904345035553\n",
      "Training loss for batch 2905 : 0.06516667455434799\n",
      "Training loss for batch 2906 : 0.08187601715326309\n",
      "Training loss for batch 2907 : 0.18360228836536407\n",
      "Training loss for batch 2908 : 0.2635240852832794\n",
      "Training loss for batch 2909 : 0.6863254904747009\n",
      "Training loss for batch 2910 : 0.411743700504303\n",
      "Training loss for batch 2911 : 0.032431378960609436\n",
      "Training loss for batch 2912 : 0.25716763734817505\n",
      "Training loss for batch 2913 : 0.527152419090271\n",
      "Training loss for batch 2914 : 0.24685198068618774\n",
      "Training loss for batch 2915 : 0.45885327458381653\n",
      "Training loss for batch 2916 : 0.7599287629127502\n",
      "Training loss for batch 2917 : 0.27282774448394775\n",
      "Training loss for batch 2918 : 0.17408069968223572\n",
      "Training loss for batch 2919 : 0.0825495570898056\n",
      "Training loss for batch 2920 : 0.2886231541633606\n",
      "Training loss for batch 2921 : 0.13918942213058472\n",
      "Training loss for batch 2922 : 0.16021165251731873\n",
      "Training loss for batch 2923 : 0.12114199250936508\n",
      "Training loss for batch 2924 : 0.4349927306175232\n",
      "Training loss for batch 2925 : 0.11569716036319733\n",
      "Training loss for batch 2926 : 0.6527997255325317\n",
      "Training loss for batch 2927 : 0.07233470678329468\n",
      "Training loss for batch 2928 : 0.11859133839607239\n",
      "Training loss for batch 2929 : 0.34317687153816223\n",
      "Training loss for batch 2930 : 0.11641989648342133\n",
      "Training loss for batch 2931 : 0.027852486819028854\n",
      "Training loss for batch 2932 : 0.012672975659370422\n",
      "Training loss for batch 2933 : 0.507879376411438\n",
      "Training loss for batch 2934 : 0.5930635333061218\n",
      "Training loss for batch 2935 : 0.24686579406261444\n",
      "Training loss for batch 2936 : 0.07858249545097351\n",
      "Training loss for batch 2937 : 0.23829486966133118\n",
      "Training loss for batch 2938 : 0.2180548906326294\n",
      "Training loss for batch 2939 : 0.12483331561088562\n",
      "Training loss for batch 2940 : 0.5253387093544006\n",
      "Training loss for batch 2941 : 0.22022601962089539\n",
      "Training loss for batch 2942 : 0.254457950592041\n",
      "Training loss for batch 2943 : 0.6077367663383484\n",
      "Training loss for batch 2944 : 0.2794467806816101\n",
      "Training loss for batch 2945 : 0.23108062148094177\n",
      "Training loss for batch 2946 : 0.5425094962120056\n",
      "Training loss for batch 2947 : 0.14244478940963745\n",
      "Training loss for batch 2948 : 0.17222455143928528\n",
      "Training loss for batch 2949 : 0.5347036719322205\n",
      "Training loss for batch 2950 : 0.47879257798194885\n",
      "Training loss for batch 2951 : 0.5772573947906494\n",
      "Training loss for batch 2952 : 0.18572059273719788\n",
      "Training loss for batch 2953 : 0.1897541880607605\n",
      "Training loss for batch 2954 : 0.2864694893360138\n",
      "Training loss for batch 2955 : 0.26276081800460815\n",
      "Training loss for batch 2956 : 0.26129475235939026\n",
      "Training loss for batch 2957 : 0.11818775534629822\n",
      "Training loss for batch 2958 : 0.27617910504341125\n",
      "Training loss for batch 2959 : 0.24623320996761322\n",
      "Training loss for batch 2960 : 0.23619185388088226\n",
      "Training loss for batch 2961 : 0.6537566781044006\n",
      "Training loss for batch 2962 : 0.40402138233184814\n",
      "Training loss for batch 2963 : 0.10095980763435364\n",
      "Training loss for batch 2964 : 0.32658007740974426\n",
      "Training loss for batch 2965 : 0.2300829440355301\n",
      "Training loss for batch 2966 : 0.1603473275899887\n",
      "Training loss for batch 2967 : 0.41090458631515503\n",
      "Training loss for batch 2968 : 0.29686444997787476\n",
      "Training loss for batch 2969 : 0.23353548347949982\n",
      "Training loss for batch 2970 : 0.5853263735771179\n",
      "Training loss for batch 2971 : 0.40695929527282715\n",
      "Training loss for batch 2972 : 0.27807608246803284\n",
      "Training loss for batch 2973 : 0.3793375492095947\n",
      "Training loss for batch 2974 : 0.10331684350967407\n",
      "Training loss for batch 2975 : 0.5957044363021851\n",
      "Training loss for batch 2976 : 0.28217655420303345\n",
      "Training loss for batch 2977 : 0.36623895168304443\n",
      "Training loss for batch 2978 : 0.34687140583992004\n",
      "Training loss for batch 2979 : 0.2888171970844269\n",
      "Training loss for batch 2980 : 0.41662099957466125\n",
      "Training loss for batch 2981 : 0.3514197766780853\n",
      "Training loss for batch 2982 : 0.2560429871082306\n",
      "Training loss for batch 2983 : 0.5231624841690063\n",
      "Training loss for batch 2984 : 0.3754126727581024\n",
      "Training loss for batch 2985 : 0.38770583271980286\n",
      "Training loss for batch 2986 : 0.32810866832733154\n",
      "Training loss for batch 2987 : 0.45495253801345825\n",
      "Training loss for batch 2988 : 0.5972832441329956\n",
      "Training loss for batch 2989 : 0.2811160385608673\n",
      "Training loss for batch 2990 : 0.41996896266937256\n",
      "Training loss for batch 2991 : 0.32560765743255615\n",
      "Training loss for batch 2992 : 0.24654556810855865\n",
      "Training loss for batch 2993 : 0.18759119510650635\n",
      "Training loss for batch 2994 : 0.2938554883003235\n",
      "Training loss for batch 2995 : 0.39181628823280334\n",
      "Training loss for batch 2996 : 0.026465952396392822\n",
      "Training loss for batch 2997 : 0.0955100804567337\n",
      "Training loss for batch 2998 : 0.21032944321632385\n",
      "Training loss for batch 2999 : 0.13648806512355804\n",
      "Training loss for batch 3000 : 0.30493050813674927\n",
      "Training loss for batch 3001 : 0.6920566558837891\n",
      "Training loss for batch 3002 : 0.25037121772766113\n",
      "Training loss for batch 3003 : 0.9596929550170898\n",
      "Training loss for batch 3004 : 0.21831707656383514\n",
      "Training loss for batch 3005 : 0.49819615483283997\n",
      "Training loss for batch 3006 : 0.4310510754585266\n",
      "Training loss for batch 3007 : 0.31623753905296326\n",
      "Training loss for batch 3008 : 0.4874870479106903\n",
      "Training loss for batch 3009 : 0.09313221275806427\n",
      "Training loss for batch 3010 : 0.3814961314201355\n",
      "Training loss for batch 3011 : 0.33483991026878357\n",
      "Training loss for batch 3012 : 0.13850264251232147\n",
      "Training loss for batch 3013 : 0.09273095428943634\n",
      "Training loss for batch 3014 : 0.179236501455307\n",
      "Training loss for batch 3015 : 0.6315183043479919\n",
      "Training loss for batch 3016 : 0.3927558660507202\n",
      "Training loss for batch 3017 : 0.2670886814594269\n",
      "Training loss for batch 3018 : 0.6126450896263123\n",
      "Training loss for batch 3019 : 0.3617081046104431\n",
      "Training loss for batch 3020 : 0.7793931365013123\n",
      "Training loss for batch 3021 : 0.26916825771331787\n",
      "Training loss for batch 3022 : 0.1423027664422989\n",
      "Training loss for batch 3023 : 0.4307875335216522\n",
      "Training loss for batch 3024 : 0.28679564595222473\n",
      "Training loss for batch 3025 : 0.2400084286928177\n",
      "Training loss for batch 3026 : 0.36827051639556885\n",
      "Training loss for batch 3027 : 0.02728973515331745\n",
      "Training loss for batch 3028 : 0.18762952089309692\n",
      "Training loss for batch 3029 : 0.49537572264671326\n",
      "Training loss for batch 3030 : 0.3769093155860901\n",
      "Training loss for batch 3031 : 0.9022106528282166\n",
      "Training loss for batch 3032 : 0.2263348251581192\n",
      "Training loss for batch 3033 : 0.14301720261573792\n",
      "Training loss for batch 3034 : 0.15797658264636993\n",
      "Training loss for batch 3035 : 0.21484191715717316\n",
      "Training loss for batch 3036 : 0.27004745602607727\n",
      "Training loss for batch 3037 : 0.16690096259117126\n",
      "Training loss for batch 3038 : 0.4156438410282135\n",
      "Training loss for batch 3039 : 0.19153818488121033\n",
      "Training loss for batch 3040 : 0.1591884344816208\n",
      "Training loss for batch 3041 : 0.5161892175674438\n",
      "Training loss for batch 3042 : 0.34504398703575134\n",
      "Training loss for batch 3043 : 0.2857919931411743\n",
      "Training loss for batch 3044 : 0.4334140717983246\n",
      "Training loss for batch 3045 : 0.06546244770288467\n",
      "Training loss for batch 3046 : 0.16344472765922546\n",
      "Training loss for batch 3047 : 0.2473507672548294\n",
      "Training loss for batch 3048 : 0.7179399728775024\n",
      "Training loss for batch 3049 : 0.18871037662029266\n",
      "Training loss for batch 3050 : 0.1404082030057907\n",
      "Training loss for batch 3051 : 0.03720619156956673\n",
      "Training loss for batch 3052 : 0.013849921524524689\n",
      "Training loss for batch 3053 : 0.14032761752605438\n",
      "Training loss for batch 3054 : 0.2924109101295471\n",
      "Training loss for batch 3055 : 0.37654632329940796\n",
      "Training loss for batch 3056 : 0.15554030239582062\n",
      "Training loss for batch 3057 : 0.5290274024009705\n",
      "Training loss for batch 3058 : 0.12180139124393463\n",
      "Training loss for batch 3059 : 0.02333301492035389\n",
      "Training loss for batch 3060 : 0.12464169412851334\n",
      "Training loss for batch 3061 : 0.03016330674290657\n",
      "Training loss for batch 3062 : 0.1038573607802391\n",
      "Training loss for batch 3063 : 0.46320265531539917\n",
      "Training loss for batch 3064 : 0.34786760807037354\n",
      "Training loss for batch 3065 : 0.41489550471305847\n",
      "Training loss for batch 3066 : 0.36627548933029175\n",
      "Training loss for batch 3067 : 0.6414164304733276\n",
      "Training loss for batch 3068 : 0.06551170349121094\n",
      "Training loss for batch 3069 : 0.20719818770885468\n",
      "Training loss for batch 3070 : 0.31308484077453613\n",
      "Training loss for batch 3071 : 0.2921569049358368\n",
      "Training loss for batch 3072 : 0.12458418309688568\n",
      "Training loss for batch 3073 : 0.42021483182907104\n",
      "Training loss for batch 3074 : 0.23489297926425934\n",
      "Training loss for batch 3075 : 0.45295873284339905\n",
      "Training loss for batch 3076 : 0.3522643446922302\n",
      "Training loss for batch 3077 : 0.11191926896572113\n",
      "Training loss for batch 3078 : 0.5436524152755737\n",
      "Training loss for batch 3079 : 0.2098703682422638\n",
      "Training loss for batch 3080 : 0.21006067097187042\n",
      "Training loss for batch 3081 : 0.2512814998626709\n",
      "Training loss for batch 3082 : 0.3570263981819153\n",
      "Training loss for batch 3083 : 0.5494222044944763\n",
      "Training loss for batch 3084 : 0.09905453771352768\n",
      "Training loss for batch 3085 : 0.21722504496574402\n",
      "Training loss for batch 3086 : 0.23772361874580383\n",
      "Training loss for batch 3087 : 0.25759807229042053\n",
      "Training loss for batch 3088 : 0.3118091821670532\n",
      "Training loss for batch 3089 : 0.22271767258644104\n",
      "Training loss for batch 3090 : 0.4082399904727936\n",
      "Training loss for batch 3091 : 0.31920647621154785\n",
      "Training loss for batch 3092 : 0.7722504138946533\n",
      "Training loss for batch 3093 : 0.5128383040428162\n",
      "Training loss for batch 3094 : 0.1919696182012558\n",
      "Training loss for batch 3095 : 0.22147753834724426\n",
      "Training loss for batch 3096 : 0.4428737461566925\n",
      "Training loss for batch 3097 : 0.13968153297901154\n",
      "Training loss for batch 3098 : 0.070709727704525\n",
      "Training loss for batch 3099 : 0.0933007225394249\n",
      "Training loss for batch 3100 : 0.12419156730175018\n",
      "Training loss for batch 3101 : 0.347902774810791\n",
      "Training loss for batch 3102 : 0.07321501523256302\n",
      "Training loss for batch 3103 : 0.22143040597438812\n",
      "Training loss for batch 3104 : 0.4494023323059082\n",
      "Training loss for batch 3105 : 0.039281364530324936\n",
      "Training loss for batch 3106 : 0.22610652446746826\n",
      "Training loss for batch 3107 : 0.33873772621154785\n",
      "Training loss for batch 3108 : 0.11137913167476654\n",
      "Training loss for batch 3109 : 0.5214897990226746\n",
      "Training loss for batch 3110 : 1.0840784311294556\n",
      "Training loss for batch 3111 : 0.3037429451942444\n",
      "Training loss for batch 3112 : 0.7891708016395569\n",
      "Training loss for batch 3113 : 0.7179217338562012\n",
      "Training loss for batch 3114 : 0.3829827904701233\n",
      "Training loss for batch 3115 : 0.3054359555244446\n",
      "Training loss for batch 3116 : 0.4189523160457611\n",
      "Training loss for batch 3117 : 0.17880970239639282\n",
      "Training loss for batch 3118 : 0.07802682369947433\n",
      "Training loss for batch 3119 : 0.5692456960678101\n",
      "Training loss for batch 3120 : 0.12701044976711273\n",
      "Training loss for batch 3121 : 0.5962045192718506\n",
      "Training loss for batch 3122 : 0.28441354632377625\n",
      "Training loss for batch 3123 : 0.9633930325508118\n",
      "Training loss for batch 3124 : 0.3966284990310669\n",
      "Training loss for batch 3125 : 0.3042239844799042\n",
      "Training loss for batch 3126 : 0.0\n",
      "Training loss for batch 3127 : 0.11304296553134918\n",
      "Training loss for batch 3128 : 0.1255246251821518\n",
      "Training loss for batch 3129 : 0.9931755065917969\n",
      "Training loss for batch 3130 : 0.08750203996896744\n",
      "Training loss for batch 3131 : 0.2850513756275177\n",
      "Training loss for batch 3132 : 0.7624291777610779\n",
      "Training loss for batch 3133 : 0.1706230342388153\n",
      "Training loss for batch 3134 : 0.4243350028991699\n",
      "Training loss for batch 3135 : 0.04034857079386711\n",
      "Training loss for batch 3136 : 0.3681853413581848\n",
      "Training loss for batch 3137 : 0.15500736236572266\n",
      "Training loss for batch 3138 : 0.3208216726779938\n",
      "Training loss for batch 3139 : 0.09682496637105942\n",
      "Training loss for batch 3140 : 0.11449794471263885\n",
      "Training loss for batch 3141 : 0.27832460403442383\n",
      "Training loss for batch 3142 : 0.2478812038898468\n",
      "Training loss for batch 3143 : 0.13934750854969025\n",
      "Training loss for batch 3144 : 0.08541309833526611\n",
      "Training loss for batch 3145 : 0.8990398645401001\n",
      "Training loss for batch 3146 : 0.3324781656265259\n",
      "Training loss for batch 3147 : 0.3655025064945221\n",
      "Training loss for batch 3148 : 0.3666636347770691\n",
      "Training loss for batch 3149 : 0.1264214962720871\n",
      "Training loss for batch 3150 : 0.29526621103286743\n",
      "Training loss for batch 3151 : 0.03464435413479805\n",
      "Training loss for batch 3152 : 0.26348602771759033\n",
      "Training loss for batch 3153 : 0.297263503074646\n",
      "Training loss for batch 3154 : 0.24010735750198364\n",
      "Training loss for batch 3155 : 0.6464047431945801\n",
      "Training loss for batch 3156 : 0.11489855498075485\n",
      "Training loss for batch 3157 : 0.137110635638237\n",
      "Training loss for batch 3158 : 0.01509388629347086\n",
      "Training loss for batch 3159 : 0.17418457567691803\n",
      "Training loss for batch 3160 : 0.2644209861755371\n",
      "Training loss for batch 3161 : 0.5829746127128601\n",
      "Training loss for batch 3162 : 0.5710252523422241\n",
      "Training loss for batch 3163 : 0.12252368032932281\n",
      "Training loss for batch 3164 : 0.10853523015975952\n",
      "Training loss for batch 3165 : 0.2551264762878418\n",
      "Training loss for batch 3166 : 0.13644470274448395\n",
      "Training loss for batch 3167 : 0.08712275326251984\n",
      "Training loss for batch 3168 : 0.10048703104257584\n",
      "Training loss for batch 3169 : 0.36579468846321106\n",
      "Training loss for batch 3170 : 0.5885734558105469\n",
      "Training loss for batch 3171 : 0.03908634930849075\n",
      "Training loss for batch 3172 : 0.18044504523277283\n",
      "Training loss for batch 3173 : 0.20463791489601135\n",
      "Training loss for batch 3174 : 0.3313347399234772\n",
      "Training loss for batch 3175 : 0.03723938390612602\n",
      "Training loss for batch 3176 : 0.1774754673242569\n",
      "Training loss for batch 3177 : 0.09880708158016205\n",
      "Training loss for batch 3178 : 0.3830198645591736\n",
      "Training loss for batch 3179 : 0.14442987740039825\n",
      "Training loss for batch 3180 : 0.5108128190040588\n",
      "Training loss for batch 3181 : 0.427815705537796\n",
      "Training loss for batch 3182 : 0.3616501986980438\n",
      "Training loss for batch 3183 : 0.6102554798126221\n",
      "Training loss for batch 3184 : 0.4273512065410614\n",
      "Training loss for batch 3185 : 0.05904097110033035\n",
      "Training loss for batch 3186 : 0.06797432154417038\n",
      "Training loss for batch 3187 : 0.14813821017742157\n",
      "Training loss for batch 3188 : 0.10995598137378693\n",
      "Training loss for batch 3189 : 0.38223204016685486\n",
      "Training loss for batch 3190 : 0.5483461618423462\n",
      "Training loss for batch 3191 : 0.23206722736358643\n",
      "Training loss for batch 3192 : 0.07246742397546768\n",
      "Training loss for batch 3193 : 0.022951096296310425\n",
      "Training loss for batch 3194 : 0.10263773798942566\n",
      "Training loss for batch 3195 : 0.09351449459791183\n",
      "Training loss for batch 3196 : 0.2965044379234314\n",
      "Training loss for batch 3197 : 0.29954802989959717\n",
      "Training loss for batch 3198 : 0.09808803349733353\n",
      "Training loss for batch 3199 : 0.3005201816558838\n",
      "Training loss for batch 3200 : 0.3057911992073059\n",
      "Training loss for batch 3201 : 0.6401363611221313\n",
      "Training loss for batch 3202 : 0.07072741538286209\n",
      "Training loss for batch 3203 : 0.6194425225257874\n",
      "Training loss for batch 3204 : 0.34877538681030273\n",
      "Training loss for batch 3205 : 0.08182211965322495\n",
      "Training loss for batch 3206 : 0.829460620880127\n",
      "Training loss for batch 3207 : 0.13861054182052612\n",
      "Training loss for batch 3208 : 0.42286446690559387\n",
      "Training loss for batch 3209 : 0.24780237674713135\n",
      "Training loss for batch 3210 : 0.006842831615358591\n",
      "Training loss for batch 3211 : 0.16167861223220825\n",
      "Training loss for batch 3212 : 0.049580179154872894\n",
      "Training loss for batch 3213 : 0.051642633974552155\n",
      "Training loss for batch 3214 : 0.10065542161464691\n",
      "Training loss for batch 3215 : 0.44429194927215576\n",
      "Training loss for batch 3216 : 0.32748711109161377\n",
      "Training loss for batch 3217 : 0.5296365022659302\n",
      "Training loss for batch 3218 : 0.3497074842453003\n",
      "Training loss for batch 3219 : 0.30724120140075684\n",
      "Training loss for batch 3220 : 0.11056004464626312\n",
      "Training loss for batch 3221 : 0.14032834768295288\n",
      "Training loss for batch 3222 : 0.6726756691932678\n",
      "Training loss for batch 3223 : 0.18856631219387054\n",
      "Training loss for batch 3224 : 0.017947068437933922\n",
      "Training loss for batch 3225 : 0.03931758925318718\n",
      "Training loss for batch 3226 : 0.024281274527311325\n",
      "Training loss for batch 3227 : 0.016495712101459503\n",
      "Training loss for batch 3228 : 0.345712810754776\n",
      "Training loss for batch 3229 : 0.24433985352516174\n",
      "Training loss for batch 3230 : 0.4706534743309021\n",
      "Training loss for batch 3231 : 0.27118682861328125\n",
      "Training loss for batch 3232 : 0.5221084356307983\n",
      "Training loss for batch 3233 : 0.09264078736305237\n",
      "Training loss for batch 3234 : 0.3439505696296692\n",
      "Training loss for batch 3235 : 0.2022499293088913\n",
      "Training loss for batch 3236 : 0.2553870379924774\n",
      "Training loss for batch 3237 : 0.1782209277153015\n",
      "Training loss for batch 3238 : 0.28881990909576416\n",
      "Training loss for batch 3239 : 0.06685887277126312\n",
      "Training loss for batch 3240 : 0.5856380462646484\n",
      "Training loss for batch 3241 : 0.2723138928413391\n",
      "Training loss for batch 3242 : 0.05207299813628197\n",
      "Training loss for batch 3243 : 0.4793664813041687\n",
      "Training loss for batch 3244 : 0.08995355665683746\n",
      "Training loss for batch 3245 : 0.03412570431828499\n",
      "Training loss for batch 3246 : 0.21800127625465393\n",
      "Training loss for batch 3247 : 0.46231383085250854\n",
      "Training loss for batch 3248 : 0.11762271821498871\n",
      "Training loss for batch 3249 : 0.4433443248271942\n",
      "Training loss for batch 3250 : 0.15680786967277527\n",
      "Training loss for batch 3251 : 0.30927032232284546\n",
      "Training loss for batch 3252 : 0.12714874744415283\n",
      "Training loss for batch 3253 : 0.28054678440093994\n",
      "Training loss for batch 3254 : 0.16636626422405243\n",
      "Training loss for batch 3255 : 0.24089749157428741\n",
      "Training loss for batch 3256 : 0.22177553176879883\n",
      "Training loss for batch 3257 : 0.3328787386417389\n",
      "Training loss for batch 3258 : 0.5094243288040161\n",
      "Training loss for batch 3259 : 0.06571967899799347\n",
      "Training loss for batch 3260 : 0.17040511965751648\n",
      "Training loss for batch 3261 : 0.8326594829559326\n",
      "Training loss for batch 3262 : 0.1499801129102707\n",
      "Training loss for batch 3263 : 0.36345964670181274\n",
      "Training loss for batch 3264 : 0.745427668094635\n",
      "Training loss for batch 3265 : 0.175890251994133\n",
      "Training loss for batch 3266 : 0.42413610219955444\n",
      "Training loss for batch 3267 : 0.44297707080841064\n",
      "Training loss for batch 3268 : 0.007857750169932842\n",
      "Training loss for batch 3269 : 0.23292839527130127\n",
      "Training loss for batch 3270 : 0.42936182022094727\n",
      "Training loss for batch 3271 : 0.2612870931625366\n",
      "Training loss for batch 3272 : 0.0781857818365097\n",
      "Training loss for batch 3273 : 0.14979234337806702\n",
      "Training loss for batch 3274 : 0.25250905752182007\n",
      "Training loss for batch 3275 : 0.1361653357744217\n",
      "Training loss for batch 3276 : 0.0323544442653656\n",
      "Training loss for batch 3277 : 0.2406175136566162\n",
      "Training loss for batch 3278 : 0.5727825164794922\n",
      "Training loss for batch 3279 : 1.0801364183425903\n",
      "Training loss for batch 3280 : 0.7414388656616211\n",
      "Training loss for batch 3281 : 0.3778049945831299\n",
      "Training loss for batch 3282 : 0.18872453272342682\n",
      "Training loss for batch 3283 : 0.1212841048836708\n",
      "Training loss for batch 3284 : 0.06706248968839645\n",
      "Training loss for batch 3285 : 0.5324146747589111\n",
      "Training loss for batch 3286 : 0.30637046694755554\n",
      "Training loss for batch 3287 : 0.1998719573020935\n",
      "Training loss for batch 3288 : 0.29804497957229614\n",
      "Training loss for batch 3289 : 0.094661645591259\n",
      "Training loss for batch 3290 : 0.6257809996604919\n",
      "Training loss for batch 3291 : 0.021349214017391205\n",
      "Training loss for batch 3292 : 0.29223954677581787\n",
      "Training loss for batch 3293 : 0.4117739498615265\n",
      "Training loss for batch 3294 : 0.17642773687839508\n",
      "Training loss for batch 3295 : 0.35611554980278015\n",
      "Training loss for batch 3296 : 0.7532188892364502\n",
      "Training loss for batch 3297 : 0.056168679147958755\n",
      "Training loss for batch 3298 : 0.00691269850358367\n",
      "Training loss for batch 3299 : 0.3397026062011719\n",
      "Training loss for batch 3300 : 0.48939207196235657\n",
      "Training loss for batch 3301 : 0.16737017035484314\n",
      "Training loss for batch 3302 : 0.18734334409236908\n",
      "Training loss for batch 3303 : 0.04692195728421211\n",
      "Training loss for batch 3304 : 0.17553453147411346\n",
      "Training loss for batch 3305 : 0.33303311467170715\n",
      "Training loss for batch 3306 : 0.5493651628494263\n",
      "Training loss for batch 3307 : 0.3477662205696106\n",
      "Training loss for batch 3308 : 0.37053030729293823\n",
      "Training loss for batch 3309 : 0.3768661916255951\n",
      "Training loss for batch 3310 : 0.34446558356285095\n",
      "Training loss for batch 3311 : 0.2602105736732483\n",
      "Training loss for batch 3312 : 0.798717200756073\n",
      "Training loss for batch 3313 : 0.3677026331424713\n",
      "Training loss for batch 3314 : 0.18488098680973053\n",
      "Training loss for batch 3315 : 0.1891515552997589\n",
      "Training loss for batch 3316 : 0.46857619285583496\n",
      "Training loss for batch 3317 : 0.5290241241455078\n",
      "Training loss for batch 3318 : 0.43022361397743225\n",
      "Training loss for batch 3319 : 0.3159354627132416\n",
      "Training loss for batch 3320 : 0.0\n",
      "Training loss for batch 3321 : 0.07997573912143707\n",
      "Training loss for batch 3322 : 0.326964408159256\n",
      "Training loss for batch 3323 : 0.33505570888519287\n",
      "Training loss for batch 3324 : 1.0181479454040527\n",
      "Training loss for batch 3325 : 0.05809541791677475\n",
      "Training loss for batch 3326 : 0.19060443341732025\n",
      "Training loss for batch 3327 : 0.3064042031764984\n",
      "Training loss for batch 3328 : 0.26605576276779175\n",
      "Training loss for batch 3329 : 0.15895144641399384\n",
      "Training loss for batch 3330 : 0.32551154494285583\n",
      "Training loss for batch 3331 : 0.4021674692630768\n",
      "Training loss for batch 3332 : 0.15849822759628296\n",
      "Training loss for batch 3333 : 0.5463789701461792\n",
      "Training loss for batch 3334 : 0.5507040619850159\n",
      "Training loss for batch 3335 : 0.3594638705253601\n",
      "Training loss for batch 3336 : 0.4180909991264343\n",
      "Training loss for batch 3337 : 0.33800652623176575\n",
      "Training loss for batch 3338 : 0.23833432793617249\n",
      "Training loss for batch 3339 : 0.5936753749847412\n",
      "Training loss for batch 3340 : 0.41906359791755676\n",
      "Training loss for batch 3341 : 0.38399767875671387\n",
      "Training loss for batch 3342 : 0.37410473823547363\n",
      "Training loss for batch 3343 : 0.31727343797683716\n",
      "Training loss for batch 3344 : 0.19829532504081726\n",
      "Training loss for batch 3345 : 0.550210177898407\n",
      "Training loss for batch 3346 : 0.23140889406204224\n",
      "Training loss for batch 3347 : 0.12094216048717499\n",
      "Training loss for batch 3348 : 0.5258998870849609\n",
      "Training loss for batch 3349 : 0.11626598238945007\n",
      "Training loss for batch 3350 : 0.21602413058280945\n",
      "Training loss for batch 3351 : 0.21065863966941833\n",
      "Training loss for batch 3352 : 0.5709453225135803\n",
      "Training loss for batch 3353 : 0.19941642880439758\n",
      "Training loss for batch 3354 : 0.0868871659040451\n",
      "Training loss for batch 3355 : 0.53971266746521\n",
      "Training loss for batch 3356 : 0.46062812209129333\n",
      "Training loss for batch 3357 : 0.24109557271003723\n",
      "Training loss for batch 3358 : 0.2848202586174011\n",
      "Training loss for batch 3359 : 0.4534970819950104\n",
      "Training loss for batch 3360 : 0.5532711148262024\n",
      "Training loss for batch 3361 : 0.09738278388977051\n",
      "Training loss for batch 3362 : 0.16817229986190796\n",
      "Training loss for batch 3363 : 0.12954984605312347\n",
      "Training loss for batch 3364 : 0.5704501271247864\n",
      "Training loss for batch 3365 : 0.14542272686958313\n",
      "Training loss for batch 3366 : 0.5228069424629211\n",
      "Training loss for batch 3367 : 0.6422674655914307\n",
      "Training loss for batch 3368 : 0.5490691065788269\n",
      "Training loss for batch 3369 : 0.5968874096870422\n",
      "Training loss for batch 3370 : 0.31173175573349\n",
      "Training loss for batch 3371 : 0.22007691860198975\n",
      "Training loss for batch 3372 : 0.14035087823867798\n",
      "Training loss for batch 3373 : 0.12981607019901276\n",
      "Training loss for batch 3374 : 0.20161128044128418\n",
      "Training loss for batch 3375 : 0.34940481185913086\n",
      "Training loss for batch 3376 : 0.27688542008399963\n",
      "Training loss for batch 3377 : 0.18443161249160767\n",
      "Training loss for batch 3378 : 0.11612290143966675\n",
      "Training loss for batch 3379 : 0.7149417996406555\n",
      "Training loss for batch 3380 : 0.234066903591156\n",
      "Training loss for batch 3381 : 0.3648497462272644\n",
      "Training loss for batch 3382 : 0.07147779315710068\n",
      "Training loss for batch 3383 : 0.3023894429206848\n",
      "Training loss for batch 3384 : 0.20175006985664368\n",
      "Training loss for batch 3385 : 0.4101438522338867\n",
      "Training loss for batch 3386 : 0.3159126937389374\n",
      "Training loss for batch 3387 : 0.3279152512550354\n",
      "Training loss for batch 3388 : 0.02765505760908127\n",
      "Training loss for batch 3389 : 0.087911456823349\n",
      "Training loss for batch 3390 : 0.27729591727256775\n",
      "Training loss for batch 3391 : 0.31679099798202515\n",
      "Training loss for batch 3392 : 0.4763137102127075\n",
      "Training loss for batch 3393 : 0.2784311771392822\n",
      "Training loss for batch 3394 : 0.3476588726043701\n",
      "Training loss for batch 3395 : 0.3966064751148224\n",
      "Training loss for batch 3396 : 0.3137308359146118\n",
      "Training loss for batch 3397 : 0.42137059569358826\n",
      "Training loss for batch 3398 : 0.3220466375350952\n",
      "Training loss for batch 3399 : 0.2776683270931244\n",
      "Training loss for batch 3400 : 0.4456852376461029\n",
      "Training loss for batch 3401 : 0.17184944450855255\n",
      "Training loss for batch 3402 : 0.35776934027671814\n",
      "Training loss for batch 3403 : 0.516873836517334\n",
      "Training loss for batch 3404 : 0.5402680039405823\n",
      "Training loss for batch 3405 : 0.35769903659820557\n",
      "Training loss for batch 3406 : 0.5128368735313416\n",
      "Training loss for batch 3407 : 0.15366561710834503\n",
      "Training loss for batch 3408 : 0.12245173007249832\n",
      "Training loss for batch 3409 : 0.2090768963098526\n",
      "Training loss for batch 3410 : 0.2395927906036377\n",
      "Training loss for batch 3411 : 0.38002705574035645\n",
      "Training loss for batch 3412 : 0.20428462326526642\n",
      "Training loss for batch 3413 : 0.4983612895011902\n",
      "Training loss for batch 3414 : 0.4816463887691498\n",
      "Training loss for batch 3415 : 0.24424055218696594\n",
      "Training loss for batch 3416 : 0.12948298454284668\n",
      "Training loss for batch 3417 : 0.20706665515899658\n",
      "Training loss for batch 3418 : 0.512602686882019\n",
      "Training loss for batch 3419 : 0.3237650394439697\n",
      "Training loss for batch 3420 : 0.415735125541687\n",
      "Training loss for batch 3421 : 0.6559221744537354\n",
      "Training loss for batch 3422 : 0.19131921231746674\n",
      "Training loss for batch 3423 : 0.44844821095466614\n",
      "Training loss for batch 3424 : 0.2244587540626526\n",
      "Training loss for batch 3425 : 0.6064532995223999\n",
      "Training loss for batch 3426 : 0.24497002363204956\n",
      "Training loss for batch 3427 : 0.2193242907524109\n",
      "Training loss for batch 3428 : 0.1530134528875351\n",
      "Training loss for batch 3429 : 0.27736058831214905\n",
      "Training loss for batch 3430 : 0.059430960565805435\n",
      "Training loss for batch 3431 : 0.09153583645820618\n",
      "Training loss for batch 3432 : 0.39089861512184143\n",
      "Training loss for batch 3433 : 0.5800285339355469\n",
      "Training loss for batch 3434 : 0.3042652904987335\n",
      "Training loss for batch 3435 : 0.18537218868732452\n",
      "Training loss for batch 3436 : 0.4023417830467224\n",
      "Training loss for batch 3437 : 0.5396275520324707\n",
      "Training loss for batch 3438 : 0.30192509293556213\n",
      "Training loss for batch 3439 : 0.4313828945159912\n",
      "Training loss for batch 3440 : 0.30738365650177\n",
      "Training loss for batch 3441 : 0.38433408737182617\n",
      "Training loss for batch 3442 : 0.6893799304962158\n",
      "Training loss for batch 3443 : 0.03562132641673088\n",
      "Training loss for batch 3444 : 0.08150968700647354\n",
      "Training loss for batch 3445 : 0.06717336922883987\n",
      "Training loss for batch 3446 : 0.2104242891073227\n",
      "Training loss for batch 3447 : 0.25077059864997864\n",
      "Training loss for batch 3448 : 0.48093581199645996\n",
      "Training loss for batch 3449 : 0.5969095826148987\n",
      "Training loss for batch 3450 : 0.3705863654613495\n",
      "Training loss for batch 3451 : 0.47568678855895996\n",
      "Training loss for batch 3452 : 0.3367963433265686\n",
      "Training loss for batch 3453 : 0.4863620400428772\n",
      "Training loss for batch 3454 : 0.14895711839199066\n",
      "Training loss for batch 3455 : 0.49796828627586365\n",
      "Training loss for batch 3456 : 0.34529149532318115\n",
      "Training loss for batch 3457 : 0.37020063400268555\n",
      "Training loss for batch 3458 : 0.2526441812515259\n",
      "Training loss for batch 3459 : 0.1944630891084671\n",
      "Training loss for batch 3460 : 0.4100276231765747\n",
      "Training loss for batch 3461 : 0.2613906264305115\n",
      "Training loss for batch 3462 : 0.21755549311637878\n",
      "Training loss for batch 3463 : 0.09867352247238159\n",
      "Training loss for batch 3464 : 0.3099445104598999\n",
      "Training loss for batch 3465 : 0.2562439441680908\n",
      "Training loss for batch 3466 : 0.17013531923294067\n",
      "Training loss for batch 3467 : 0.4226645231246948\n",
      "Training loss for batch 3468 : 0.2076965868473053\n",
      "Training loss for batch 3469 : 0.224299818277359\n",
      "Training loss for batch 3470 : 0.2799496650695801\n",
      "Training loss for batch 3471 : 0.3487758934497833\n",
      "Training loss for batch 3472 : 0.39730873703956604\n",
      "Training loss for batch 3473 : 0.42229360342025757\n",
      "Training loss for batch 3474 : 0.08283546566963196\n",
      "Training loss for batch 3475 : 0.16479593515396118\n",
      "Training loss for batch 3476 : 0.4298025965690613\n",
      "Training loss for batch 3477 : 0.10769016295671463\n",
      "Training loss for batch 3478 : 0.09603238850831985\n",
      "Training loss for batch 3479 : 0.13793706893920898\n",
      "Training loss for batch 3480 : 0.1960873156785965\n",
      "Training loss for batch 3481 : 0.46958836913108826\n",
      "Training loss for batch 3482 : 0.47638383507728577\n",
      "Training loss for batch 3483 : 0.3715733587741852\n",
      "Training loss for batch 3484 : 0.8009644746780396\n",
      "Training loss for batch 3485 : 0.4479125738143921\n",
      "Training loss for batch 3486 : 0.12404240667819977\n",
      "Training loss for batch 3487 : 0.39898011088371277\n",
      "Training loss for batch 3488 : 0.4364832639694214\n",
      "Training loss for batch 3489 : 0.08262008428573608\n",
      "Training loss for batch 3490 : 0.39239564538002014\n",
      "Training loss for batch 3491 : 0.08698353171348572\n",
      "Training loss for batch 3492 : 0.2774762511253357\n",
      "Training loss for batch 3493 : 0.6697558164596558\n",
      "Training loss for batch 3494 : 0.22705255448818207\n",
      "Training loss for batch 3495 : 0.10465291887521744\n",
      "Training loss for batch 3496 : 0.1713634729385376\n",
      "Training loss for batch 3497 : 0.31453999876976013\n",
      "Training loss for batch 3498 : 0.21996764838695526\n",
      "Training loss for batch 3499 : 0.4148668646812439\n",
      "Training loss for batch 3500 : 0.04229583591222763\n",
      "Training loss for batch 3501 : 0.3705455958843231\n",
      "Training loss for batch 3502 : 0.6147676706314087\n",
      "Training loss for batch 3503 : 0.10279542207717896\n",
      "Training loss for batch 3504 : 0.01922709122300148\n",
      "Training loss for batch 3505 : 0.4312134385108948\n",
      "Training loss for batch 3506 : 0.10500258952379227\n",
      "Training loss for batch 3507 : 0.37059563398361206\n",
      "Training loss for batch 3508 : 0.09883222728967667\n",
      "Training loss for batch 3509 : 0.17049409449100494\n",
      "Training loss for batch 3510 : 0.3846765160560608\n",
      "Training loss for batch 3511 : 0.1735612452030182\n",
      "Training loss for batch 3512 : 0.33411332964897156\n",
      "Training loss for batch 3513 : 0.47964680194854736\n",
      "Training loss for batch 3514 : 0.061439476907253265\n",
      "Training loss for batch 3515 : 0.025659378618001938\n",
      "Training loss for batch 3516 : 0.22743740677833557\n",
      "Training loss for batch 3517 : 0.35309863090515137\n",
      "Training loss for batch 3518 : 0.44077807664871216\n",
      "Training loss for batch 3519 : 0.47542229294776917\n",
      "Training loss for batch 3520 : 0.13631486892700195\n",
      "Training loss for batch 3521 : 0.6311882734298706\n",
      "Training loss for batch 3522 : 0.4794108271598816\n",
      "Training loss for batch 3523 : 0.2888735830783844\n",
      "Training loss for batch 3524 : 0.04156390205025673\n",
      "Training loss for batch 3525 : 0.09179668128490448\n",
      "Training loss for batch 3526 : 0.43748533725738525\n",
      "Training loss for batch 3527 : 0.30715200304985046\n",
      "Training loss for batch 3528 : 0.06698314100503922\n",
      "Training loss for batch 3529 : 0.6493571400642395\n",
      "Training loss for batch 3530 : 0.276308536529541\n",
      "Training loss for batch 3531 : 0.6702308654785156\n",
      "Training loss for batch 3532 : 0.11269877105951309\n",
      "Training loss for batch 3533 : 0.2713543176651001\n",
      "Training loss for batch 3534 : 0.2027120590209961\n",
      "Training loss for batch 3535 : 0.4000459313392639\n",
      "Training loss for batch 3536 : 0.0325239822268486\n",
      "Training loss for batch 3537 : 0.2274426519870758\n",
      "Training loss for batch 3538 : 0.1918952912092209\n",
      "Training loss for batch 3539 : 0.47910168766975403\n",
      "Training loss for batch 3540 : 0.7433916926383972\n",
      "Training loss for batch 3541 : 0.7076339721679688\n",
      "Training loss for batch 3542 : 0.13189752399921417\n",
      "Training loss for batch 3543 : 0.43972721695899963\n",
      "Training loss for batch 3544 : 0.40246933698654175\n",
      "Training loss for batch 3545 : 0.07062053680419922\n",
      "Training loss for batch 3546 : 0.007332841865718365\n",
      "Training loss for batch 3547 : 0.020122405141592026\n",
      "Training loss for batch 3548 : 0.3088786005973816\n",
      "Training loss for batch 3549 : 0.04612252116203308\n",
      "Training loss for batch 3550 : 0.15745916962623596\n",
      "Training loss for batch 3551 : 0.19263558089733124\n",
      "Training loss for batch 3552 : 0.18286868929862976\n",
      "Training loss for batch 3553 : 0.5708390474319458\n",
      "Training loss for batch 3554 : 0.5296316146850586\n",
      "Training loss for batch 3555 : 0.2819744348526001\n",
      "Training loss for batch 3556 : 0.5074399709701538\n",
      "Training loss for batch 3557 : 0.3841613829135895\n",
      "Training loss for batch 3558 : 0.34566551446914673\n",
      "Training loss for batch 3559 : 0.4295593500137329\n",
      "Training loss for batch 3560 : 0.6646431088447571\n",
      "Training loss for batch 3561 : 0.30056917667388916\n",
      "Training loss for batch 3562 : 0.9524891376495361\n",
      "Training loss for batch 3563 : 0.0\n",
      "Training loss for batch 3564 : 0.06943809986114502\n",
      "Training loss for batch 3565 : 0.3383650481700897\n",
      "Training loss for batch 3566 : 0.03542955219745636\n",
      "Training loss for batch 3567 : 0.5352388620376587\n",
      "Training loss for batch 3568 : 0.047991301864385605\n",
      "Training loss for batch 3569 : 0.2904738783836365\n",
      "Training loss for batch 3570 : 0.2860243618488312\n",
      "Training loss for batch 3571 : 0.23963607847690582\n",
      "Training loss for batch 3572 : 0.5465859770774841\n",
      "Training loss for batch 3573 : 0.3845340311527252\n",
      "Training loss for batch 3574 : 0.04106495529413223\n",
      "Training loss for batch 3575 : 0.3895861804485321\n",
      "Training loss for batch 3576 : 0.33798062801361084\n",
      "Training loss for batch 3577 : 0.6304727792739868\n",
      "Training loss for batch 3578 : 0.29398906230926514\n",
      "Training loss for batch 3579 : 0.20820605754852295\n",
      "Training loss for batch 3580 : 0.0\n",
      "Training loss for batch 3581 : 0.015013287775218487\n",
      "Training loss for batch 3582 : 0.2142118513584137\n",
      "Training loss for batch 3583 : 0.33339089155197144\n",
      "Training loss for batch 3584 : 0.05875205248594284\n",
      "Training loss for batch 3585 : 0.5446654558181763\n",
      "Training loss for batch 3586 : 0.23441283404827118\n",
      "Training loss for batch 3587 : 0.4943501949310303\n",
      "Training loss for batch 3588 : 0.4039370119571686\n",
      "Training loss for batch 3589 : 0.16791000962257385\n",
      "Training loss for batch 3590 : 0.3580748438835144\n",
      "Training loss for batch 3591 : 0.13356244564056396\n",
      "Training loss for batch 3592 : 0.2927664816379547\n",
      "Training loss for batch 3593 : 0.14672160148620605\n",
      "Training loss for batch 3594 : 0.15069594979286194\n",
      "Training loss for batch 3595 : 0.027982600033283234\n",
      "Training loss for batch 3596 : 0.45791003108024597\n",
      "Training loss for batch 3597 : 0.19564053416252136\n",
      "Training loss for batch 3598 : 0.33985206484794617\n",
      "Training loss for batch 3599 : 0.571553111076355\n",
      "Training loss for batch 3600 : 0.3021105229854584\n",
      "Training loss for batch 3601 : 0.3512272238731384\n",
      "Training loss for batch 3602 : 0.2711244225502014\n",
      "Training loss for batch 3603 : 0.13819409906864166\n",
      "Training loss for batch 3604 : 0.5091152787208557\n",
      "Training loss for batch 3605 : 0.37346988916397095\n",
      "Training loss for batch 3606 : 0.27691197395324707\n",
      "Training loss for batch 3607 : 0.8514044284820557\n",
      "Training loss for batch 3608 : 0.2765654921531677\n",
      "Training loss for batch 3609 : 0.6241195797920227\n",
      "Training loss for batch 3610 : 0.25395041704177856\n",
      "Training loss for batch 3611 : 0.6999406218528748\n",
      "Training loss for batch 3612 : 0.5205179452896118\n",
      "Training loss for batch 3613 : 0.0124034583568573\n",
      "Training loss for batch 3614 : 0.28451550006866455\n",
      "Training loss for batch 3615 : 0.24179542064666748\n",
      "Training loss for batch 3616 : 0.08098602294921875\n",
      "Training loss for batch 3617 : 0.20883776247501373\n",
      "Training loss for batch 3618 : 0.14138808846473694\n",
      "Training loss for batch 3619 : 0.28172558546066284\n",
      "Training loss for batch 3620 : 0.3095965087413788\n",
      "Training loss for batch 3621 : 0.34235772490501404\n",
      "Training loss for batch 3622 : 0.14973776042461395\n",
      "Training loss for batch 3623 : 0.22128629684448242\n",
      "Training loss for batch 3624 : 0.12750287353992462\n",
      "Training loss for batch 3625 : 0.18167182803153992\n",
      "Training loss for batch 3626 : 0.37521904706954956\n",
      "Training loss for batch 3627 : 0.118193618953228\n",
      "Training loss for batch 3628 : 0.44560307264328003\n",
      "Training loss for batch 3629 : 0.46724092960357666\n",
      "Training loss for batch 3630 : 0.42289185523986816\n",
      "Training loss for batch 3631 : 0.47482046484947205\n",
      "Training loss for batch 3632 : 0.9102687239646912\n",
      "Training loss for batch 3633 : 0.33402490615844727\n",
      "Training loss for batch 3634 : 0.09004189819097519\n",
      "Training loss for batch 3635 : 0.28816697001457214\n",
      "Training loss for batch 3636 : 0.36979132890701294\n",
      "Training loss for batch 3637 : 0.23485548794269562\n",
      "Training loss for batch 3638 : 0.06889690458774567\n",
      "Training loss for batch 3639 : 0.4240344762802124\n",
      "Training loss for batch 3640 : 0.4333512783050537\n",
      "Training loss for batch 3641 : 0.3715374171733856\n",
      "Training loss for batch 3642 : 0.2949203848838806\n",
      "Training loss for batch 3643 : 0.054546453058719635\n",
      "Training loss for batch 3644 : 0.7680094242095947\n",
      "Training loss for batch 3645 : 0.31968560814857483\n",
      "Training loss for batch 3646 : 0.1950049251317978\n",
      "Training loss for batch 3647 : 0.26291754841804504\n",
      "Training loss for batch 3648 : 0.20940575003623962\n",
      "Training loss for batch 3649 : 0.07655860483646393\n",
      "Training loss for batch 3650 : 0.23910364508628845\n",
      "Training loss for batch 3651 : 0.19720670580863953\n",
      "Training loss for batch 3652 : 0.08554109185934067\n",
      "Training loss for batch 3653 : 0.5138579607009888\n",
      "Training loss for batch 3654 : 0.11742788553237915\n",
      "Training loss for batch 3655 : 0.21251000463962555\n",
      "Training loss for batch 3656 : 0.5288764834403992\n",
      "Training loss for batch 3657 : 0.3646817207336426\n",
      "Training loss for batch 3658 : 0.2843739092350006\n",
      "Training loss for batch 3659 : 0.483997106552124\n",
      "Training loss for batch 3660 : 0.370322585105896\n",
      "Training loss for batch 3661 : 0.6138765215873718\n",
      "Training loss for batch 3662 : 0.527346670627594\n",
      "Training loss for batch 3663 : 0.2608902156352997\n",
      "Training loss for batch 3664 : 0.03190924972295761\n",
      "Training loss for batch 3665 : 0.1862843632698059\n",
      "Training loss for batch 3666 : 0.2746918797492981\n",
      "Training loss for batch 3667 : 0.5681517124176025\n",
      "Training loss for batch 3668 : 0.5489857196807861\n",
      "Training loss for batch 3669 : 0.6089407801628113\n",
      "Training loss for batch 3670 : 0.47434812784194946\n",
      "Training loss for batch 3671 : 0.1339113712310791\n",
      "Training loss for batch 3672 : 0.8297955393791199\n",
      "Training loss for batch 3673 : 0.10877460986375809\n",
      "Training loss for batch 3674 : 0.655014157295227\n",
      "Training loss for batch 3675 : 0.19195449352264404\n",
      "Training loss for batch 3676 : 0.5093699097633362\n",
      "Training loss for batch 3677 : 0.3520168364048004\n",
      "Training loss for batch 3678 : 0.4763714671134949\n",
      "Training loss for batch 3679 : 0.363627552986145\n",
      "Training loss for batch 3680 : 0.05126505717635155\n",
      "Training loss for batch 3681 : 0.06334073841571808\n",
      "Training loss for batch 3682 : 0.27066296339035034\n",
      "Training loss for batch 3683 : 0.47270622849464417\n",
      "Training loss for batch 3684 : 0.14575554430484772\n",
      "Training loss for batch 3685 : 0.2742612957954407\n",
      "Training loss for batch 3686 : 0.5591672658920288\n",
      "Training loss for batch 3687 : 0.3392675518989563\n",
      "Training loss for batch 3688 : 0.3829898238182068\n",
      "Training loss for batch 3689 : 0.18721097707748413\n",
      "Training loss for batch 3690 : 0.5769573450088501\n",
      "Training loss for batch 3691 : 0.1501292586326599\n",
      "Training loss for batch 3692 : 0.4853145480155945\n",
      "Training loss for batch 3693 : 0.25124016404151917\n",
      "Training loss for batch 3694 : 0.2653735876083374\n",
      "Training loss for batch 3695 : 0.24521739780902863\n",
      "Training loss for batch 3696 : 0.175668403506279\n",
      "Training loss for batch 3697 : 0.4261469841003418\n",
      "Training loss for batch 3698 : 0.144578754901886\n",
      "Training loss for batch 3699 : 0.36627140641212463\n",
      "Training loss for batch 3700 : 0.11465360969305038\n",
      "Training loss for batch 3701 : 0.167633518576622\n",
      "Training loss for batch 3702 : 0.36600711941719055\n",
      "Training loss for batch 3703 : 0.530188798904419\n",
      "Training loss for batch 3704 : 0.26972290873527527\n",
      "Training loss for batch 3705 : 0.15545961260795593\n",
      "Training loss for batch 3706 : 0.1611744463443756\n",
      "Training loss for batch 3707 : 0.19260451197624207\n",
      "Training loss for batch 3708 : 0.22368291020393372\n",
      "Training loss for batch 3709 : 0.422603964805603\n",
      "Training loss for batch 3710 : 0.12783995270729065\n",
      "Training loss for batch 3711 : 0.16960471868515015\n",
      "Training loss for batch 3712 : 0.3494689464569092\n",
      "Training loss for batch 3713 : 0.2769712507724762\n",
      "Training loss for batch 3714 : 0.43934112787246704\n",
      "Training loss for batch 3715 : 0.15075740218162537\n",
      "Training loss for batch 3716 : 0.4882369637489319\n",
      "Training loss for batch 3717 : 0.7199588418006897\n",
      "Training loss for batch 3718 : 0.7151401042938232\n",
      "Training loss for batch 3719 : 0.20324108004570007\n",
      "Training loss for batch 3720 : 0.2112065553665161\n",
      "Training loss for batch 3721 : 0.06817472726106644\n",
      "Training loss for batch 3722 : 0.527808666229248\n",
      "Training loss for batch 3723 : 0.30901646614074707\n",
      "Training loss for batch 3724 : 0.31373101472854614\n",
      "Training loss for batch 3725 : 0.2438560128211975\n",
      "Training loss for batch 3726 : 0.2531317174434662\n",
      "Training loss for batch 3727 : 0.5035898685455322\n",
      "Training loss for batch 3728 : 0.05890320986509323\n",
      "Training loss for batch 3729 : 0.14522235095500946\n",
      "Training loss for batch 3730 : 0.3957962989807129\n",
      "Training loss for batch 3731 : 0.6080775260925293\n",
      "Training loss for batch 3732 : 0.2699883282184601\n",
      "Training loss for batch 3733 : 0.5528947114944458\n",
      "Training loss for batch 3734 : 0.12369650602340698\n",
      "Training loss for batch 3735 : 0.2779556214809418\n",
      "Training loss for batch 3736 : 0.1615806519985199\n",
      "Training loss for batch 3737 : 0.23325088620185852\n",
      "Training loss for batch 3738 : 0.10078027844429016\n",
      "Training loss for batch 3739 : 0.19961003959178925\n",
      "Training loss for batch 3740 : 0.2655078172683716\n",
      "Training loss for batch 3741 : 0.6380319595336914\n",
      "Training loss for batch 3742 : 0.449329674243927\n",
      "Training loss for batch 3743 : 0.050394803285598755\n",
      "Training loss for batch 3744 : 0.2600255310535431\n",
      "Training loss for batch 3745 : 0.5771375894546509\n",
      "Training loss for batch 3746 : 0.19365619122982025\n",
      "Training loss for batch 3747 : 0.4466463625431061\n",
      "Training loss for batch 3748 : 0.3477891981601715\n",
      "Training loss for batch 3749 : 0.1548285186290741\n",
      "Training loss for batch 3750 : 0.3160780072212219\n",
      "Training loss for batch 3751 : 0.23865383863449097\n",
      "Training loss for batch 3752 : 0.14459636807441711\n",
      "Training loss for batch 3753 : 0.8807609677314758\n",
      "Training loss for batch 3754 : 0.46013274788856506\n",
      "Training loss for batch 3755 : 0.467763751745224\n",
      "Training loss for batch 3756 : 0.60292649269104\n",
      "Training loss for batch 3757 : 0.5916261672973633\n",
      "Training loss for batch 3758 : 0.266309916973114\n",
      "Training loss for batch 3759 : 0.05613404139876366\n",
      "Training loss for batch 3760 : 0.46620798110961914\n",
      "Training loss for batch 3761 : 0.20167303085327148\n",
      "Training loss for batch 3762 : 0.3207477927207947\n",
      "Training loss for batch 3763 : 0.34385374188423157\n",
      "Training loss for batch 3764 : 0.5593032240867615\n",
      "Training loss for batch 3765 : 0.028235360980033875\n",
      "Training loss for batch 3766 : 0.39937862753868103\n",
      "Training loss for batch 3767 : 0.10852646827697754\n",
      "Training loss for batch 3768 : 0.15114039182662964\n",
      "Training loss for batch 3769 : 0.4247608482837677\n",
      "Training loss for batch 3770 : 0.5890452265739441\n",
      "Training loss for batch 3771 : 0.15632084012031555\n",
      "Training loss for batch 3772 : 0.1836940348148346\n",
      "Training loss for batch 3773 : 0.057628732174634933\n",
      "Training loss for batch 3774 : 0.3211536109447479\n",
      "Training loss for batch 3775 : 0.037281107157468796\n",
      "Training loss for batch 3776 : 0.40486305952072144\n",
      "Training loss for batch 3777 : 0.6508581042289734\n",
      "Training loss for batch 3778 : 0.5736698508262634\n",
      "Training loss for batch 3779 : 0.6037577390670776\n",
      "Training loss for batch 3780 : 0.6095362305641174\n",
      "Training loss for batch 3781 : 0.4637523293495178\n",
      "Training loss for batch 3782 : 0.09642025083303452\n",
      "Training loss for batch 3783 : 0.22711366415023804\n",
      "Training loss for batch 3784 : 0.119137242436409\n",
      "Training loss for batch 3785 : 0.07419510930776596\n",
      "Training loss for batch 3786 : 0.1773444563150406\n",
      "Training loss for batch 3787 : 0.08092088997364044\n",
      "Training loss for batch 3788 : 0.11023661494255066\n",
      "Training loss for batch 3789 : 0.6028229594230652\n",
      "Training loss for batch 3790 : 0.12094257771968842\n",
      "Training loss for batch 3791 : 0.20345596969127655\n",
      "Training loss for batch 3792 : 0.28171107172966003\n",
      "Training loss for batch 3793 : 0.12706632912158966\n",
      "Training loss for batch 3794 : 0.112638920545578\n",
      "Training loss for batch 3795 : 0.5895529389381409\n",
      "Training loss for batch 3796 : 0.24784435331821442\n",
      "Training loss for batch 3797 : 0.25220638513565063\n",
      "Training loss for batch 3798 : 0.3357556462287903\n",
      "Training loss for batch 3799 : 0.3584297299385071\n",
      "Training loss for batch 3800 : 0.43681570887565613\n",
      "Training loss for batch 3801 : 0.5262079238891602\n",
      "Training loss for batch 3802 : 0.43485647439956665\n",
      "Training loss for batch 3803 : 0.5401259064674377\n",
      "Training loss for batch 3804 : 0.4726024866104126\n",
      "Training loss for batch 3805 : 0.48583436012268066\n",
      "Training loss for batch 3806 : 0.16369546949863434\n",
      "Training loss for batch 3807 : 0.37437132000923157\n",
      "Training loss for batch 3808 : 0.10295456647872925\n",
      "Training loss for batch 3809 : 0.020752303302288055\n",
      "Training loss for batch 3810 : 0.2142478972673416\n",
      "Training loss for batch 3811 : 0.11159337311983109\n",
      "Training loss for batch 3812 : 0.5279110074043274\n",
      "Training loss for batch 3813 : 0.12470141053199768\n",
      "Training loss for batch 3814 : 0.36471912264823914\n",
      "Training loss for batch 3815 : 0.21931438148021698\n",
      "Training loss for batch 3816 : 0.10012969374656677\n",
      "Training loss for batch 3817 : 0.49945762753486633\n",
      "Training loss for batch 3818 : 0.3927152156829834\n",
      "Training loss for batch 3819 : 0.03810131922364235\n",
      "Training loss for batch 3820 : 0.10752865672111511\n",
      "Training loss for batch 3821 : 0.5150667428970337\n",
      "Training loss for batch 3822 : 0.48813286423683167\n",
      "Training loss for batch 3823 : 0.09106380492448807\n",
      "Training loss for batch 3824 : 0.28223463892936707\n",
      "Training loss for batch 3825 : 0.21166837215423584\n",
      "Training loss for batch 3826 : 0.09773336350917816\n",
      "Training loss for batch 3827 : 0.09811589121818542\n",
      "Training loss for batch 3828 : 0.6273468732833862\n",
      "Training loss for batch 3829 : 0.32115527987480164\n",
      "Training loss for batch 3830 : 0.5644974112510681\n",
      "Training loss for batch 3831 : 0.6303988695144653\n",
      "Training loss for batch 3832 : 0.2792860269546509\n",
      "Training loss for batch 3833 : 0.15602043271064758\n",
      "Training loss for batch 3834 : 0.5107261538505554\n",
      "Training loss for batch 3835 : 0.14616049826145172\n",
      "Training loss for batch 3836 : 0.35716426372528076\n",
      "Training loss for batch 3837 : 0.00422397255897522\n",
      "Training loss for batch 3838 : 0.14967475831508636\n",
      "Training loss for batch 3839 : 0.25864991545677185\n",
      "Training loss for batch 3840 : 0.047596633434295654\n",
      "Training loss for batch 3841 : 0.12234456837177277\n",
      "Training loss for batch 3842 : 0.34161481261253357\n",
      "Training loss for batch 3843 : 0.2500748634338379\n",
      "Training loss for batch 3844 : 0.11377471685409546\n",
      "Training loss for batch 3845 : 0.30661606788635254\n",
      "Training loss for batch 3846 : 0.08951142430305481\n",
      "Training loss for batch 3847 : 0.07987399399280548\n",
      "Training loss for batch 3848 : 0.48668307065963745\n",
      "Training loss for batch 3849 : 0.7777754068374634\n",
      "Training loss for batch 3850 : 0.22380898892879486\n",
      "Training loss for batch 3851 : 0.4288818836212158\n",
      "Training loss for batch 3852 : 0.25290539860725403\n",
      "Training loss for batch 3853 : 0.9386207461357117\n",
      "Training loss for batch 3854 : 0.7157817482948303\n",
      "Training loss for batch 3855 : 0.2715919613838196\n",
      "Training loss for batch 3856 : 0.19731760025024414\n",
      "Training loss for batch 3857 : 0.15787436068058014\n",
      "Training loss for batch 3858 : 0.17201298475265503\n",
      "Training loss for batch 3859 : 0.3720814883708954\n",
      "Training loss for batch 3860 : 0.1645852029323578\n",
      "Training loss for batch 3861 : 0.23347565531730652\n",
      "Training loss for batch 3862 : 0.1311872899532318\n",
      "Training loss for batch 3863 : 0.07908112555742264\n",
      "Training loss for batch 3864 : 0.5308099389076233\n",
      "Training loss for batch 3865 : 0.16476638615131378\n",
      "Training loss for batch 3866 : 0.1423157900571823\n",
      "Training loss for batch 3867 : 0.2408464550971985\n",
      "Training loss for batch 3868 : 0.17339341342449188\n",
      "Training loss for batch 3869 : 0.4170413017272949\n",
      "Training loss for batch 3870 : 0.20544134080410004\n",
      "Training loss for batch 3871 : 0.4824601411819458\n",
      "Training loss for batch 3872 : 0.07380982488393784\n",
      "Training loss for batch 3873 : 0.17393353581428528\n",
      "Training loss for batch 3874 : 0.0677366703748703\n",
      "Training loss for batch 3875 : 0.35698387026786804\n",
      "Training loss for batch 3876 : 0.274884968996048\n",
      "Training loss for batch 3877 : 0.26545995473861694\n",
      "Training loss for batch 3878 : 0.6288435459136963\n",
      "Training loss for batch 3879 : 0.18789434432983398\n",
      "Training loss for batch 3880 : 0.29561105370521545\n",
      "Training loss for batch 3881 : 0.17954394221305847\n",
      "Training loss for batch 3882 : 0.26182499527931213\n",
      "Training loss for batch 3883 : 0.29471325874328613\n",
      "Training loss for batch 3884 : 0.920107364654541\n",
      "Training loss for batch 3885 : 0.17282985150814056\n",
      "Training loss for batch 3886 : 1.0575294494628906\n",
      "Training loss for batch 3887 : 0.09604582190513611\n",
      "Training loss for batch 3888 : 0.44786348938941956\n",
      "Training loss for batch 3889 : 0.2043847143650055\n",
      "Training loss for batch 3890 : 0.2916235029697418\n",
      "Training loss for batch 3891 : 0.6367250084877014\n",
      "Training loss for batch 3892 : 0.0041645667515695095\n",
      "Training loss for batch 3893 : 0.26634564995765686\n",
      "Training loss for batch 3894 : 0.3394950330257416\n",
      "Training loss for batch 3895 : 0.022935381159186363\n",
      "Training loss for batch 3896 : 0.12891030311584473\n",
      "Training loss for batch 3897 : 0.09587129950523376\n",
      "Training loss for batch 3898 : 0.25255751609802246\n",
      "Training loss for batch 3899 : 0.263274610042572\n",
      "Training loss for batch 3900 : 0.216096892952919\n",
      "Training loss for batch 3901 : 0.3368973731994629\n",
      "Training loss for batch 3902 : 0.7194419503211975\n",
      "Training loss for batch 3903 : 0.5453888773918152\n",
      "Training loss for batch 3904 : 0.1779344379901886\n",
      "Training loss for batch 3905 : 0.14479698240756989\n",
      "Training loss for batch 3906 : 0.08429662138223648\n",
      "Training loss for batch 3907 : 0.5022384524345398\n",
      "Training loss for batch 3908 : 0.25569576025009155\n",
      "Training loss for batch 3909 : 0.9715268015861511\n",
      "Training loss for batch 3910 : 0.1376257836818695\n",
      "Training loss for batch 3911 : 0.014769435860216618\n",
      "Training loss for batch 3912 : 0.8021993637084961\n",
      "Training loss for batch 3913 : 0.1980763077735901\n",
      "Training loss for batch 3914 : 0.029838981106877327\n",
      "Training loss for batch 3915 : 0.5572361946105957\n",
      "Training loss for batch 3916 : 0.12680135667324066\n",
      "Training loss for batch 3917 : 0.3495294451713562\n",
      "Training loss for batch 3918 : 0.3197995126247406\n",
      "Training loss for batch 3919 : 0.10994675755500793\n",
      "Training loss for batch 3920 : 0.35457324981689453\n",
      "Training loss for batch 3921 : 0.008810378611087799\n",
      "Training loss for batch 3922 : 0.4529556334018707\n",
      "Training loss for batch 3923 : 0.33072197437286377\n",
      "Training loss for batch 3924 : 0.540203332901001\n",
      "Training loss for batch 3925 : 0.29490432143211365\n",
      "Training loss for batch 3926 : 0.8017938733100891\n",
      "Training loss for batch 3927 : 0.3102995455265045\n",
      "Training loss for batch 3928 : 0.21810409426689148\n",
      "Training loss for batch 3929 : 0.2316424697637558\n",
      "Training loss for batch 3930 : 0.1776425838470459\n",
      "Training loss for batch 3931 : 0.15259870886802673\n",
      "Training loss for batch 3932 : 0.12082621455192566\n",
      "Training loss for batch 3933 : 0.24590176343917847\n",
      "Training loss for batch 3934 : 0.1753678321838379\n",
      "Training loss for batch 3935 : 0.48104503750801086\n",
      "Training loss for batch 3936 : 0.119778111577034\n",
      "Training loss for batch 3937 : 0.3450746536254883\n",
      "Training loss for batch 3938 : 0.08503493666648865\n",
      "Training loss for batch 3939 : 0.3326762318611145\n",
      "Training loss for batch 3940 : 0.4972296357154846\n",
      "Training loss for batch 3941 : 0.29416224360466003\n",
      "Training loss for batch 3942 : 0.3093253970146179\n",
      "Training loss for batch 3943 : 0.007876704446971416\n",
      "Training loss for batch 3944 : 0.725325882434845\n",
      "Training loss for batch 3945 : 0.29000434279441833\n",
      "Training loss for batch 3946 : 0.250823438167572\n",
      "Training loss for batch 3947 : 0.4174693822860718\n",
      "Training loss for batch 3948 : 0.5355151891708374\n",
      "Training loss for batch 3949 : 0.12244933843612671\n",
      "Training loss for batch 3950 : 0.0\n",
      "Training loss for batch 3951 : 0.45440083742141724\n",
      "Training loss for batch 3952 : 0.4778476059436798\n",
      "Training loss for batch 3953 : 0.555107057094574\n",
      "Training loss for batch 3954 : 0.141704261302948\n",
      "Training loss for batch 3955 : 0.28984957933425903\n",
      "Training loss for batch 3956 : 0.3447425961494446\n",
      "Training loss for batch 3957 : 0.7612919211387634\n",
      "Training loss for batch 3958 : 0.27371951937675476\n",
      "Training loss for batch 3959 : 0.2459699809551239\n",
      "Training loss for batch 3960 : 0.4055541753768921\n",
      "Training loss for batch 3961 : 0.24471063911914825\n",
      "Training loss for batch 3962 : 0.4105415344238281\n",
      "Training loss for batch 3963 : 0.6895284652709961\n",
      "Training loss for batch 3964 : 0.09002787619829178\n",
      "Training loss for batch 3965 : 0.10016866773366928\n",
      "Training loss for batch 3966 : 0.35932138562202454\n",
      "Training loss for batch 3967 : 0.16812139749526978\n",
      "Training loss for batch 3968 : 0.3966539800167084\n",
      "Training loss for batch 3969 : 0.15701758861541748\n",
      "Training loss for batch 3970 : 0.2823275029659271\n",
      "Training loss for batch 3971 : 0.1925630122423172\n",
      "Training loss for batch 3972 : 0.4562433063983917\n",
      "Training loss for batch 3973 : 0.68254154920578\n",
      "Training loss for batch 3974 : 0.04065823182463646\n",
      "Training loss for batch 3975 : 0.3233240246772766\n",
      "Training loss for batch 3976 : 0.2776448726654053\n",
      "Training loss for batch 3977 : 0.6483061909675598\n",
      "Training loss for batch 3978 : 0.22784125804901123\n",
      "Training loss for batch 3979 : 0.18863680958747864\n",
      "Training loss for batch 3980 : 0.15641483664512634\n",
      "Training loss for batch 3981 : 0.15970131754875183\n",
      "Training loss for batch 3982 : 0.12371153384447098\n",
      "Training loss for batch 3983 : 0.2838268280029297\n",
      "Training loss for batch 3984 : 0.45430988073349\n",
      "Training loss for batch 3985 : 0.3927891254425049\n",
      "Training loss for batch 3986 : 0.12061542272567749\n",
      "Training loss for batch 3987 : 0.277279794216156\n",
      "Training loss for batch 3988 : 0.3603823482990265\n",
      "Training loss for batch 3989 : 0.08310101926326752\n",
      "Training loss for batch 3990 : 0.18954813480377197\n",
      "Training loss for batch 3991 : 0.3540688157081604\n",
      "Training loss for batch 3992 : 0.21107544004917145\n",
      "Training loss for batch 3993 : 0.4262560307979584\n",
      "Training loss for batch 3994 : 0.34026023745536804\n",
      "Training loss for batch 3995 : 0.33688873052597046\n",
      "Training loss for batch 3996 : 0.44870835542678833\n",
      "Training loss for batch 3997 : 0.12996166944503784\n",
      "Training loss for batch 3998 : 0.6234663128852844\n",
      "Training loss for batch 3999 : 0.11062397062778473\n",
      "Training loss for batch 4000 : 0.38849931955337524\n",
      "Training loss for batch 4001 : 0.03895966708660126\n",
      "Training loss for batch 4002 : 0.5636849403381348\n",
      "Training loss for batch 4003 : 0.28997084498405457\n",
      "Training loss for batch 4004 : 0.47987431287765503\n",
      "Training loss for batch 4005 : 0.4084690809249878\n",
      "Training loss for batch 4006 : 0.1112910807132721\n",
      "Training loss for batch 4007 : 0.13008886575698853\n",
      "Training loss for batch 4008 : 0.2689990699291229\n",
      "Training loss for batch 4009 : 0.2602451741695404\n",
      "Training loss for batch 4010 : 0.3874590992927551\n",
      "Training loss for batch 4011 : 0.11867441982030869\n",
      "Training loss for batch 4012 : 0.49152371287345886\n",
      "Training loss for batch 4013 : 0.45244765281677246\n",
      "Training loss for batch 4014 : 0.13490189611911774\n",
      "Training loss for batch 4015 : 0.2517511248588562\n",
      "Training loss for batch 4016 : 0.5357800722122192\n",
      "Training loss for batch 4017 : 0.1528351753950119\n",
      "Training loss for batch 4018 : 0.7542823553085327\n",
      "Training loss for batch 4019 : 0.05113474279642105\n",
      "Training loss for batch 4020 : 0.27310290932655334\n",
      "Training loss for batch 4021 : 0.7010287046432495\n",
      "Training loss for batch 4022 : 0.5668589472770691\n",
      "Training loss for batch 4023 : 0.14264878630638123\n",
      "Training loss for batch 4024 : 0.2440171241760254\n",
      "Training loss for batch 4025 : 0.2573540508747101\n",
      "Training loss for batch 4026 : 0.13262037932872772\n",
      "Training loss for batch 4027 : 0.2563926875591278\n",
      "Training loss for batch 4028 : 0.15406374633312225\n",
      "Training loss for batch 4029 : 0.06993962079286575\n",
      "Training loss for batch 4030 : 0.49068301916122437\n",
      "Training loss for batch 4031 : 0.21044744551181793\n",
      "Training loss for batch 4032 : 0.15719114243984222\n",
      "Training loss for batch 4033 : 0.06875433027744293\n",
      "Training loss for batch 4034 : 0.38011762499809265\n",
      "Training loss for batch 4035 : 0.2951362729072571\n",
      "Training loss for batch 4036 : 0.08121229708194733\n",
      "Training loss for batch 4037 : 0.2953159511089325\n",
      "Training loss for batch 4038 : 0.15075966715812683\n",
      "Training loss for batch 4039 : 0.15796469151973724\n",
      "Training loss for batch 4040 : 0.18614697456359863\n",
      "Training loss for batch 4041 : 0.23392990231513977\n",
      "Training loss for batch 4042 : 0.4128979742527008\n",
      "Training loss for batch 4043 : 0.1538592129945755\n",
      "Training loss for batch 4044 : 0.5616233348846436\n",
      "Training loss for batch 4045 : 0.4595448672771454\n",
      "Training loss for batch 4046 : 0.1429879367351532\n",
      "Training loss for batch 4047 : 0.37063029408454895\n",
      "Training loss for batch 4048 : 0.7223671674728394\n",
      "Training loss for batch 4049 : 0.12302493304014206\n",
      "Training loss for batch 4050 : 0.163587287068367\n",
      "Training loss for batch 4051 : 0.06996773183345795\n",
      "Training loss for batch 4052 : 0.545979917049408\n",
      "Training loss for batch 4053 : 0.05062811076641083\n",
      "Training loss for batch 4054 : 0.1840054988861084\n",
      "Training loss for batch 4055 : 0.7002475261688232\n",
      "Training loss for batch 4056 : 0.2734987735748291\n",
      "Training loss for batch 4057 : 0.36084264516830444\n",
      "Training loss for batch 4058 : 0.2913353145122528\n",
      "Training loss for batch 4059 : 0.6938391327857971\n",
      "Training loss for batch 4060 : 0.263398140668869\n",
      "Training loss for batch 4061 : 0.025492042303085327\n",
      "Training loss for batch 4062 : 0.47686219215393066\n",
      "Training loss for batch 4063 : 0.31415441632270813\n",
      "Training loss for batch 4064 : 0.052550338208675385\n",
      "Training loss for batch 4065 : 0.06701040267944336\n",
      "Training loss for batch 4066 : 0.2883094847202301\n",
      "Training loss for batch 4067 : 0.26411527395248413\n",
      "Training loss for batch 4068 : 0.1685408651828766\n",
      "Training loss for batch 4069 : 0.06132156774401665\n",
      "Training loss for batch 4070 : 0.10291902720928192\n",
      "Training loss for batch 4071 : 0.12759524583816528\n",
      "Training loss for batch 4072 : 0.24846863746643066\n",
      "Training loss for batch 4073 : 0.09275595098733902\n",
      "Training loss for batch 4074 : 0.22742536664009094\n",
      "Training loss for batch 4075 : 0.3803499937057495\n",
      "Training loss for batch 4076 : 0.4718882441520691\n",
      "Training loss for batch 4077 : 0.3387322425842285\n",
      "Training loss for batch 4078 : 0.12848295271396637\n",
      "Training loss for batch 4079 : 0.31381744146347046\n",
      "Training loss for batch 4080 : 0.09032238274812698\n",
      "Training loss for batch 4081 : 0.10359390079975128\n",
      "Training loss for batch 4082 : 0.45810413360595703\n",
      "Training loss for batch 4083 : 0.41898298263549805\n",
      "Training loss for batch 4084 : 0.09573177993297577\n",
      "Training loss for batch 4085 : 0.14520299434661865\n",
      "Training loss for batch 4086 : 0.36003586649894714\n",
      "Training loss for batch 4087 : 0.1569068431854248\n",
      "Training loss for batch 4088 : 0.35113319754600525\n",
      "Training loss for batch 4089 : 0.5396780371665955\n",
      "Training loss for batch 4090 : 0.22002443671226501\n",
      "Training loss for batch 4091 : 0.13113445043563843\n",
      "Training loss for batch 4092 : 0.40979474782943726\n",
      "Training loss for batch 4093 : 0.11149442195892334\n",
      "Training loss for batch 4094 : 0.08979219198226929\n",
      "Training loss for batch 4095 : 0.22570478916168213\n",
      "Training loss for batch 4096 : 0.16125079989433289\n",
      "Training loss for batch 4097 : 0.19535945355892181\n",
      "Training loss for batch 4098 : 0.12547846138477325\n",
      "Training loss for batch 4099 : 0.031285833567380905\n",
      "Training loss for batch 4100 : 0.7012538313865662\n",
      "Training loss for batch 4101 : 0.35850346088409424\n",
      "Training loss for batch 4102 : 0.12263944745063782\n",
      "Training loss for batch 4103 : 0.23723426461219788\n",
      "Training loss for batch 4104 : 0.361039400100708\n",
      "Training loss for batch 4105 : 0.3803543746471405\n",
      "Training loss for batch 4106 : 0.1671784371137619\n",
      "Training loss for batch 4107 : 0.36469361186027527\n",
      "Training loss for batch 4108 : 0.0908191129565239\n",
      "Training loss for batch 4109 : 0.27888837456703186\n",
      "Training loss for batch 4110 : 0.011188887059688568\n",
      "Training loss for batch 4111 : 0.29635173082351685\n",
      "Training loss for batch 4112 : 0.1528019905090332\n",
      "Training loss for batch 4113 : 0.08760017156600952\n",
      "Training loss for batch 4114 : 0.10914888232946396\n",
      "Training loss for batch 4115 : 0.5284063816070557\n",
      "Training loss for batch 4116 : 0.5881927609443665\n",
      "Training loss for batch 4117 : 0.43200385570526123\n",
      "Training loss for batch 4118 : 0.14665010571479797\n",
      "Training loss for batch 4119 : 0.11165599524974823\n",
      "Training loss for batch 4120 : 0.4943930208683014\n",
      "Training loss for batch 4121 : 1.0623849630355835\n",
      "Training loss for batch 4122 : 0.4379224479198456\n",
      "Training loss for batch 4123 : 0.0803324431180954\n",
      "Training loss for batch 4124 : 0.06571079045534134\n",
      "Training loss for batch 4125 : 0.4638802111148834\n",
      "Training loss for batch 4126 : 0.6702262759208679\n",
      "Training loss for batch 4127 : 0.5869418382644653\n",
      "Training loss for batch 4128 : 0.4822959899902344\n",
      "Training loss for batch 4129 : 0.27019402384757996\n",
      "Training loss for batch 4130 : 0.11584143340587616\n",
      "Training loss for batch 4131 : 0.44647759199142456\n",
      "Training loss for batch 4132 : 0.13414481282234192\n",
      "Training loss for batch 4133 : 0.08996380865573883\n",
      "Training loss for batch 4134 : 0.01719704270362854\n",
      "Training loss for batch 4135 : 0.7624853849411011\n",
      "Training loss for batch 4136 : 0.1257595270872116\n",
      "Training loss for batch 4137 : 0.4736221134662628\n",
      "Training loss for batch 4138 : 0.19523809850215912\n",
      "Training loss for batch 4139 : 0.5019093155860901\n",
      "Training loss for batch 4140 : 0.3609277307987213\n",
      "Training loss for batch 4141 : 0.3752067983150482\n",
      "Training loss for batch 4142 : 0.4903220236301422\n",
      "Training loss for batch 4143 : 0.3049330413341522\n",
      "Training loss for batch 4144 : 0.2011716067790985\n",
      "Training loss for batch 4145 : 0.28770560026168823\n",
      "Training loss for batch 4146 : 0.32022982835769653\n",
      "Training loss for batch 4147 : 0.32232213020324707\n",
      "Training loss for batch 4148 : 0.41678550839424133\n",
      "Training loss for batch 4149 : 0.1897255927324295\n",
      "Training loss for batch 4150 : 0.04442243278026581\n",
      "Training loss for batch 4151 : 0.5889643430709839\n",
      "Training loss for batch 4152 : 0.1729932427406311\n",
      "Training loss for batch 4153 : 0.2586078345775604\n",
      "Training loss for batch 4154 : 0.3714773654937744\n",
      "Training loss for batch 4155 : 0.07517748326063156\n",
      "Training loss for batch 4156 : 0.022642239928245544\n",
      "Training loss for batch 4157 : 0.17637930810451508\n",
      "Training loss for batch 4158 : 0.6584884524345398\n",
      "Training loss for batch 4159 : 0.2536294758319855\n",
      "Training loss for batch 4160 : 0.5750676989555359\n",
      "Training loss for batch 4161 : 0.39596688747406006\n",
      "Training loss for batch 4162 : 0.13192063570022583\n",
      "Training loss for batch 4163 : 0.14281542599201202\n",
      "Training loss for batch 4164 : 0.36854466795921326\n",
      "Training loss for batch 4165 : 0.2898672819137573\n",
      "Training loss for batch 4166 : 0.3788106143474579\n",
      "Training loss for batch 4167 : 0.24374538660049438\n",
      "Training loss for batch 4168 : 0.2950190305709839\n",
      "Training loss for batch 4169 : 0.09529675543308258\n",
      "Training loss for batch 4170 : 0.11438703536987305\n",
      "Training loss for batch 4171 : 0.4563140571117401\n",
      "Training loss for batch 4172 : 0.4142673909664154\n",
      "Training loss for batch 4173 : 0.07650567591190338\n",
      "Training loss for batch 4174 : 0.1406947821378708\n",
      "Training loss for batch 4175 : 0.7706342935562134\n",
      "Training loss for batch 4176 : 0.08842424303293228\n",
      "Training loss for batch 4177 : 0.2090606540441513\n",
      "Training loss for batch 4178 : 0.29027581214904785\n",
      "Training loss for batch 4179 : 0.2813354730606079\n",
      "Training loss for batch 4180 : 0.4871392846107483\n",
      "Training loss for batch 4181 : 0.2179572433233261\n",
      "Training loss for batch 4182 : 0.1648276001214981\n",
      "Training loss for batch 4183 : 0.3063279688358307\n",
      "Training loss for batch 4184 : 0.49576520919799805\n",
      "Training loss for batch 4185 : 0.30778181552886963\n",
      "Training loss for batch 4186 : 0.5974125862121582\n",
      "Training loss for batch 4187 : 0.45651018619537354\n",
      "Training loss for batch 4188 : 0.05425816401839256\n",
      "Training loss for batch 4189 : 0.33224448561668396\n",
      "Training loss for batch 4190 : 0.5122023224830627\n",
      "Training loss for batch 4191 : 0.12396678328514099\n",
      "Training loss for batch 4192 : 0.363863080739975\n",
      "Training loss for batch 4193 : 0.14184153079986572\n",
      "Training loss for batch 4194 : 0.27357736229896545\n",
      "Training loss for batch 4195 : 0.2063487023115158\n",
      "Training loss for batch 4196 : 0.6046978235244751\n",
      "Training loss for batch 4197 : 0.4905812442302704\n",
      "Training loss for batch 4198 : 0.5513561367988586\n",
      "Training loss for batch 4199 : 0.6472290754318237\n",
      "Training loss for batch 4200 : 0.3867870569229126\n",
      "Training loss for batch 4201 : 0.30882588028907776\n",
      "Training loss for batch 4202 : 0.15649278461933136\n",
      "Training loss for batch 4203 : 0.45185622572898865\n",
      "Training loss for batch 4204 : 0.14425130188465118\n",
      "Training loss for batch 4205 : 0.2321726381778717\n",
      "Training loss for batch 4206 : 0.030686279758810997\n",
      "Training loss for batch 4207 : 0.3511938750743866\n",
      "Training loss for batch 4208 : 0.12770777940750122\n",
      "Training loss for batch 4209 : 0.26583394408226013\n",
      "Training loss for batch 4210 : 0.12573902308940887\n",
      "Training loss for batch 4211 : 0.2491825520992279\n",
      "Training loss for batch 4212 : 0.1773083209991455\n",
      "Training loss for batch 4213 : 0.4824621379375458\n",
      "Training loss for batch 4214 : 0.629036545753479\n",
      "Training loss for batch 4215 : 0.2447744607925415\n",
      "Training loss for batch 4216 : 0.32146137952804565\n",
      "Training loss for batch 4217 : 0.23644493520259857\n",
      "Training loss for batch 4218 : 0.5554524660110474\n",
      "Training loss for batch 4219 : 0.3257136344909668\n",
      "Training loss for batch 4220 : 0.20800606906414032\n",
      "Training loss for batch 4221 : 0.20641706883907318\n",
      "Training loss for batch 4222 : 0.1325286477804184\n",
      "Training loss for batch 4223 : 0.21312536299228668\n",
      "Training loss for batch 4224 : 0.15797142684459686\n",
      "Training loss for batch 4225 : 0.4671369791030884\n",
      "Training loss for batch 4226 : 0.5297856330871582\n",
      "Training loss for batch 4227 : 0.11972472816705704\n",
      "Training loss for batch 4228 : 0.3466236889362335\n",
      "Training loss for batch 4229 : 0.09570252895355225\n",
      "Training loss for batch 4230 : 0.35176950693130493\n",
      "Training loss for batch 4231 : 0.47255438566207886\n",
      "Training loss for batch 4232 : 0.35243716835975647\n",
      "Training loss for batch 4233 : 0.6844518184661865\n",
      "Training loss for batch 4234 : 0.2557961940765381\n",
      "Training loss for batch 4235 : 0.5258795022964478\n",
      "Training loss for batch 4236 : 0.42062559723854065\n",
      "Training loss for batch 4237 : 0.5609085559844971\n",
      "Training loss for batch 4238 : 0.27042505145072937\n",
      "Training loss for batch 4239 : 0.26611292362213135\n",
      "Training loss for batch 4240 : 0.2833155393600464\n",
      "Training loss for batch 4241 : 0.5917972326278687\n",
      "Training loss for batch 4242 : 0.2476712167263031\n",
      "Training loss for batch 4243 : 0.484535813331604\n",
      "Training loss for batch 4244 : 0.47903919219970703\n",
      "Training loss for batch 4245 : 0.3315942585468292\n",
      "Training loss for batch 4246 : 0.056370433419942856\n",
      "Training loss for batch 4247 : 0.21725675463676453\n",
      "Training loss for batch 4248 : 0.12714849412441254\n",
      "Training loss for batch 4249 : 0.17020300030708313\n",
      "Training loss for batch 4250 : 0.13525377213954926\n",
      "Training loss for batch 4251 : 0.21460457146167755\n",
      "Training loss for batch 4252 : 0.35512775182724\n",
      "Training loss for batch 4253 : 0.3361563980579376\n",
      "Training loss for batch 4254 : 0.44892674684524536\n",
      "Training loss for batch 4255 : 0.3038945198059082\n",
      "Training loss for batch 4256 : 0.14731474220752716\n",
      "Training loss for batch 4257 : 0.09886525571346283\n",
      "Training loss for batch 4258 : 0.10652199387550354\n",
      "Training loss for batch 4259 : 0.7849075198173523\n",
      "Training loss for batch 4260 : 0.40143951773643494\n",
      "Training loss for batch 4261 : 0.2142001837491989\n",
      "Training loss for batch 4262 : 0.3113056719303131\n",
      "Training loss for batch 4263 : 0.37913987040519714\n",
      "Training loss for batch 4264 : 0.3208434581756592\n",
      "Training loss for batch 4265 : 0.40350818634033203\n",
      "Training loss for batch 4266 : 0.18417464196681976\n",
      "Training loss for batch 4267 : 0.10056808590888977\n",
      "Training loss for batch 4268 : 0.23632362484931946\n",
      "Training loss for batch 4269 : 0.2561218738555908\n",
      "Training loss for batch 4270 : 0.4454909861087799\n",
      "Training loss for batch 4271 : 0.3151465654373169\n",
      "Training loss for batch 4272 : 0.49059340357780457\n",
      "Training loss for batch 4273 : 0.0973268672823906\n",
      "Training loss for batch 4274 : 0.14756140112876892\n",
      "Training loss for batch 4275 : 0.13617545366287231\n",
      "Training loss for batch 4276 : 0.511480987071991\n",
      "Training loss for batch 4277 : 0.1143331304192543\n",
      "Training loss for batch 4278 : 0.36738622188568115\n",
      "Training loss for batch 4279 : 0.2861279547214508\n",
      "Training loss for batch 4280 : 0.10192184150218964\n",
      "Training loss for batch 4281 : 0.21330241858959198\n",
      "Training loss for batch 4282 : 0.2843461036682129\n",
      "Training loss for batch 4283 : 0.0844852477312088\n",
      "Training loss for batch 4284 : 0.3878501057624817\n",
      "Training loss for batch 4285 : 0.2107561230659485\n",
      "Training loss for batch 4286 : 0.25431039929389954\n",
      "Training loss for batch 4287 : 0.29111000895500183\n",
      "Training loss for batch 4288 : 0.0\n",
      "Training loss for batch 4289 : 0.32928895950317383\n",
      "Training loss for batch 4290 : 0.4445844292640686\n",
      "Training loss for batch 4291 : 0.06731723248958588\n",
      "Training loss for batch 4292 : 0.07226332277059555\n",
      "Training loss for batch 4293 : 0.10331352055072784\n",
      "Training loss for batch 4294 : 0.3345935642719269\n",
      "Training loss for batch 4295 : 0.48835763335227966\n",
      "Training loss for batch 4296 : 0.1338684856891632\n",
      "Training loss for batch 4297 : 0.2669449746608734\n",
      "Training loss for batch 4298 : 0.31185612082481384\n",
      "Training loss for batch 4299 : 0.42589592933654785\n",
      "Training loss for batch 4300 : 0.42970097064971924\n",
      "Training loss for batch 4301 : 0.3979007303714752\n",
      "Training loss for batch 4302 : 0.3003261983394623\n",
      "Training loss for batch 4303 : 0.3574690818786621\n",
      "Training loss for batch 4304 : 0.38745078444480896\n",
      "Training loss for batch 4305 : 0.3959895968437195\n",
      "Training loss for batch 4306 : 0.4753318428993225\n",
      "Training loss for batch 4307 : 0.22528429329395294\n",
      "Training loss for batch 4308 : 0.42052268981933594\n",
      "Training loss for batch 4309 : 0.531851053237915\n",
      "Training loss for batch 4310 : 0.18986831605434418\n",
      "Training loss for batch 4311 : 0.14888896048069\n",
      "Training loss for batch 4312 : 0.1590251624584198\n",
      "Training loss for batch 4313 : 0.2521088719367981\n",
      "Training loss for batch 4314 : 0.38046616315841675\n",
      "Training loss for batch 4315 : 0.2598177194595337\n",
      "Training loss for batch 4316 : 0.31228137016296387\n",
      "Training loss for batch 4317 : 0.37210485339164734\n",
      "Training loss for batch 4318 : 0.13627474009990692\n",
      "Training loss for batch 4319 : 0.17259539663791656\n",
      "Training loss for batch 4320 : 0.21682263910770416\n",
      "Training loss for batch 4321 : 0.6058759689331055\n",
      "Training loss for batch 4322 : 0.6631366014480591\n",
      "Training loss for batch 4323 : 0.6625283360481262\n",
      "Training loss for batch 4324 : 0.24282369017601013\n",
      "Training loss for batch 4325 : 0.4872426390647888\n",
      "Training loss for batch 4326 : 0.10195516049861908\n",
      "Training loss for batch 4327 : 0.08528558164834976\n",
      "Training loss for batch 4328 : 0.5228906273841858\n",
      "Training loss for batch 4329 : 0.06527387350797653\n",
      "Training loss for batch 4330 : 0.05751892924308777\n",
      "Training loss for batch 4331 : 0.23935101926326752\n",
      "Training loss for batch 4332 : 0.45168453454971313\n",
      "Training loss for batch 4333 : 0.3283917009830475\n",
      "Training loss for batch 4334 : 0.3865610361099243\n",
      "Training loss for batch 4335 : 0.5011942386627197\n",
      "Training loss for batch 4336 : 0.4319230020046234\n",
      "Training loss for batch 4337 : 0.2922186255455017\n",
      "Training loss for batch 4338 : 0.5051568746566772\n",
      "Training loss for batch 4339 : 0.08180367201566696\n",
      "Training loss for batch 4340 : 0.09915518760681152\n",
      "Training loss for batch 4341 : 0.6957991123199463\n",
      "Training loss for batch 4342 : 0.6968696117401123\n",
      "Training loss for batch 4343 : 0.3614005148410797\n",
      "Training loss for batch 4344 : 0.6579998731613159\n",
      "Training loss for batch 4345 : 0.09377940744161606\n",
      "Training loss for batch 4346 : 0.5313226580619812\n",
      "Training loss for batch 4347 : 0.03126298263669014\n",
      "Training loss for batch 4348 : 0.26493942737579346\n",
      "Training loss for batch 4349 : 0.3522804379463196\n",
      "Training loss for batch 4350 : 0.22172455489635468\n",
      "Training loss for batch 4351 : 0.7315455675125122\n",
      "Training loss for batch 4352 : 0.0688399150967598\n",
      "Training loss for batch 4353 : 0.638396143913269\n",
      "Training loss for batch 4354 : 0.058672547340393066\n",
      "Training loss for batch 4355 : 0.25514790415763855\n",
      "Training loss for batch 4356 : 0.2387104481458664\n",
      "Training loss for batch 4357 : 0.1338222175836563\n",
      "Training loss for batch 4358 : 0.3893662691116333\n",
      "Training loss for batch 4359 : 0.08160647749900818\n",
      "Training loss for batch 4360 : 0.23473834991455078\n",
      "Training loss for batch 4361 : 0.23566675186157227\n",
      "Training loss for batch 4362 : 0.5061938166618347\n",
      "Training loss for batch 4363 : 0.1808415651321411\n",
      "Training loss for batch 4364 : 0.2250637412071228\n",
      "Training loss for batch 4365 : 0.21185271441936493\n",
      "Training loss for batch 4366 : 0.550620973110199\n",
      "Training loss for batch 4367 : 0.47765883803367615\n",
      "Training loss for batch 4368 : 0.17609050869941711\n",
      "Training loss for batch 4369 : 0.516812801361084\n",
      "Training loss for batch 4370 : 0.12280154228210449\n",
      "Training loss for batch 4371 : 0.14206136763095856\n",
      "Training loss for batch 4372 : 0.21576426923274994\n",
      "Training loss for batch 4373 : 0.6130827069282532\n",
      "Training loss for batch 4374 : 0.09392520785331726\n",
      "Training loss for batch 4375 : 0.1305333375930786\n",
      "Training loss for batch 4376 : 0.6118580102920532\n",
      "Training loss for batch 4377 : 0.5207921862602234\n",
      "Training loss for batch 4378 : 0.35679078102111816\n",
      "Training loss for batch 4379 : 0.40674352645874023\n",
      "Training loss for batch 4380 : 0.19413137435913086\n",
      "Training loss for batch 4381 : 0.6249013543128967\n",
      "Training loss for batch 4382 : 0.20042148232460022\n",
      "Training loss for batch 4383 : 0.48610997200012207\n",
      "Training loss for batch 4384 : 0.3619953691959381\n",
      "Training loss for batch 4385 : 0.27631500363349915\n",
      "Training loss for batch 4386 : 0.6661899089813232\n",
      "Training loss for batch 4387 : 0.605822741985321\n",
      "Training loss for batch 4388 : 0.32937443256378174\n",
      "Training loss for batch 4389 : 0.08664588630199432\n",
      "Training loss for batch 4390 : 0.3141734302043915\n",
      "Training loss for batch 4391 : 0.1457541435956955\n",
      "Training loss for batch 4392 : 0.6812359690666199\n",
      "Training loss for batch 4393 : 1.0871870517730713\n",
      "Training loss for batch 4394 : 0.38738709688186646\n",
      "Training loss for batch 4395 : 0.3586971163749695\n",
      "Training loss for batch 4396 : 0.7100980877876282\n",
      "Training loss for batch 4397 : 0.23805999755859375\n",
      "Training loss for batch 4398 : 0.5080541372299194\n",
      "Training loss for batch 4399 : 0.4683428108692169\n",
      "Training loss for batch 4400 : 0.28576624393463135\n",
      "Training loss for batch 4401 : 0.24634265899658203\n",
      "Training loss for batch 4402 : 0.3469667136669159\n",
      "Training loss for batch 4403 : 0.24938692152500153\n",
      "Training loss for batch 4404 : 0.0111318314447999\n",
      "Training loss for batch 4405 : 0.749515950679779\n",
      "Training loss for batch 4406 : 0.1596539318561554\n",
      "Training loss for batch 4407 : 0.4267420172691345\n",
      "Training loss for batch 4408 : 0.15957406163215637\n",
      "Training loss for batch 4409 : 0.49673962593078613\n",
      "Training loss for batch 4410 : 0.3360460102558136\n",
      "Training loss for batch 4411 : 0.07349099963903427\n",
      "Training loss for batch 4412 : 0.13266286253929138\n",
      "Training loss for batch 4413 : 0.5648698210716248\n",
      "Training loss for batch 4414 : 0.2755391299724579\n",
      "Training loss for batch 4415 : 0.2415970116853714\n",
      "Training loss for batch 4416 : 0.3137643039226532\n",
      "Training loss for batch 4417 : 0.141983762383461\n",
      "Training loss for batch 4418 : 0.0\n",
      "Training loss for batch 4419 : 0.09386089444160461\n",
      "Training loss for batch 4420 : 0.2962287962436676\n",
      "Training loss for batch 4421 : 0.34712329506874084\n",
      "Training loss for batch 4422 : 0.34194520115852356\n",
      "Training loss for batch 4423 : 0.44160276651382446\n",
      "Training loss for batch 4424 : 0.24846020340919495\n",
      "Training loss for batch 4425 : 0.34416481852531433\n",
      "Training loss for batch 4426 : 0.4637037515640259\n",
      "Training loss for batch 4427 : 0.6954224109649658\n",
      "Training loss for batch 4428 : 0.2649059593677521\n",
      "Training loss for batch 4429 : 0.4508436620235443\n",
      "Training loss for batch 4430 : 0.37177523970603943\n",
      "Training loss for batch 4431 : 0.0620885007083416\n",
      "Training loss for batch 4432 : 0.0021239188499748707\n",
      "Training loss for batch 4433 : 0.08732984960079193\n",
      "Training loss for batch 4434 : 0.21342019736766815\n",
      "Training loss for batch 4435 : 0.058776985853910446\n",
      "Training loss for batch 4436 : 0.48573070764541626\n",
      "Training loss for batch 4437 : 0.19104135036468506\n",
      "Training loss for batch 4438 : 0.4374246299266815\n",
      "Training loss for batch 4439 : 0.07399535179138184\n",
      "Training loss for batch 4440 : 0.12922760844230652\n",
      "Training loss for batch 4441 : 0.3476448357105255\n",
      "Training loss for batch 4442 : 0.05579793453216553\n",
      "Training loss for batch 4443 : 0.24599851667881012\n",
      "Training loss for batch 4444 : 0.38472336530685425\n",
      "Training loss for batch 4445 : 0.3372466564178467\n",
      "Training loss for batch 4446 : 0.39769288897514343\n",
      "Training loss for batch 4447 : 0.5369645357131958\n",
      "Training loss for batch 4448 : 0.6783370971679688\n",
      "Training loss for batch 4449 : 0.14220596849918365\n",
      "Training loss for batch 4450 : 0.1277870535850525\n",
      "Training loss for batch 4451 : 0.2973763942718506\n",
      "Training loss for batch 4452 : 0.2323775291442871\n",
      "Training loss for batch 4453 : 0.5582306385040283\n",
      "Training loss for batch 4454 : 0.2860580384731293\n",
      "Training loss for batch 4455 : 0.1571902185678482\n",
      "Training loss for batch 4456 : 0.0732346624135971\n",
      "Training loss for batch 4457 : 0.4021272361278534\n",
      "Training loss for batch 4458 : 0.27604854106903076\n",
      "Training loss for batch 4459 : 0.0848107859492302\n",
      "Training loss for batch 4460 : 0.5925803780555725\n",
      "Training loss for batch 4461 : 0.21501877903938293\n",
      "Training loss for batch 4462 : 0.07445098459720612\n",
      "Training loss for batch 4463 : 0.6098849177360535\n",
      "Training loss for batch 4464 : 0.12944740056991577\n",
      "Training loss for batch 4465 : 0.4933810532093048\n",
      "Training loss for batch 4466 : 0.10675612092018127\n",
      "Training loss for batch 4467 : 0.09555283933877945\n",
      "Training loss for batch 4468 : 0.010403633117675781\n",
      "Training loss for batch 4469 : 0.25959154963493347\n",
      "Training loss for batch 4470 : 0.7013964653015137\n",
      "Training loss for batch 4471 : 0.4817555844783783\n",
      "Training loss for batch 4472 : 0.6092131733894348\n",
      "Training loss for batch 4473 : 0.1928812861442566\n",
      "Training loss for batch 4474 : 0.15189574658870697\n",
      "Training loss for batch 4475 : 0.197633296251297\n",
      "Training loss for batch 4476 : 0.15111714601516724\n",
      "Training loss for batch 4477 : 0.4685728847980499\n",
      "Training loss for batch 4478 : 0.11857818812131882\n",
      "Training loss for batch 4479 : 0.2692328691482544\n",
      "Training loss for batch 4480 : 0.13285571336746216\n",
      "Training loss for batch 4481 : 0.32314878702163696\n",
      "Training loss for batch 4482 : 0.598312258720398\n",
      "Training loss for batch 4483 : 0.13941462337970734\n",
      "Training loss for batch 4484 : 0.3396708369255066\n",
      "Training loss for batch 4485 : 0.36732733249664307\n",
      "Training loss for batch 4486 : 0.365648090839386\n",
      "Training loss for batch 4487 : 0.09539248794317245\n",
      "Training loss for batch 4488 : 0.15510958433151245\n",
      "Training loss for batch 4489 : 0.40052300691604614\n",
      "Training loss for batch 4490 : 0.34274929761886597\n",
      "Training loss for batch 4491 : 0.42308902740478516\n",
      "Training loss for batch 4492 : 0.40497007966041565\n",
      "Training loss for batch 4493 : 0.49100297689437866\n",
      "Training loss for batch 4494 : 0.37651902437210083\n",
      "Training loss for batch 4495 : 0.3297255039215088\n",
      "Training loss for batch 4496 : 0.24364672601222992\n",
      "Training loss for batch 4497 : 0.2510141432285309\n",
      "Training loss for batch 4498 : 0.5603621602058411\n",
      "Training loss for batch 4499 : 0.10707780718803406\n",
      "Training loss for batch 4500 : 0.25411415100097656\n",
      "Training loss for batch 4501 : 0.5523107051849365\n",
      "Training loss for batch 4502 : 0.3740270435810089\n",
      "Training loss for batch 4503 : 0.39198485016822815\n",
      "Training loss for batch 4504 : 0.23508630692958832\n",
      "Training loss for batch 4505 : 0.13095776736736298\n",
      "Training loss for batch 4506 : 0.15569697320461273\n",
      "Training loss for batch 4507 : 0.1585005819797516\n",
      "Training loss for batch 4508 : 0.19543136656284332\n",
      "Training loss for batch 4509 : 0.13420593738555908\n",
      "Training loss for batch 4510 : 0.7857216596603394\n",
      "Training loss for batch 4511 : 0.43677186965942383\n",
      "Training loss for batch 4512 : 0.018503455445170403\n",
      "Training loss for batch 4513 : 0.22506241500377655\n",
      "Training loss for batch 4514 : 0.36166998744010925\n",
      "Training loss for batch 4515 : 0.9978758692741394\n",
      "Training loss for batch 4516 : 0.32664954662323\n",
      "Training loss for batch 4517 : 0.1308109313249588\n",
      "Training loss for batch 4518 : 0.26429495215415955\n",
      "Training loss for batch 4519 : 0.3238460421562195\n",
      "Training loss for batch 4520 : 0.27682480216026306\n",
      "Training loss for batch 4521 : 0.29755374789237976\n",
      "Training loss for batch 4522 : 1.0722626447677612\n",
      "Training loss for batch 4523 : 0.5035269856452942\n",
      "Training loss for batch 4524 : 0.183991476893425\n",
      "Training loss for batch 4525 : 0.24025192856788635\n",
      "Training loss for batch 4526 : 0.5520586371421814\n",
      "Training loss for batch 4527 : 0.300858736038208\n",
      "Training loss for batch 4528 : 0.1568126082420349\n",
      "Training loss for batch 4529 : 0.13117952644824982\n",
      "Training loss for batch 4530 : 0.3444879949092865\n",
      "Training loss for batch 4531 : 0.18675658106803894\n",
      "Training loss for batch 4532 : 0.4944993555545807\n",
      "Training loss for batch 4533 : 0.29589325189590454\n",
      "Training loss for batch 4534 : 0.09117713570594788\n",
      "Training loss for batch 4535 : 0.389453649520874\n",
      "Training loss for batch 4536 : 0.22146402299404144\n",
      "Training loss for batch 4537 : 0.346457302570343\n",
      "Training loss for batch 4538 : 0.8983112573623657\n",
      "Training loss for batch 4539 : 0.3444671034812927\n",
      "Training loss for batch 4540 : 0.18746741116046906\n",
      "Training loss for batch 4541 : 0.03947942703962326\n",
      "Training loss for batch 4542 : 0.11613956838846207\n",
      "Training loss for batch 4543 : 0.21123471856117249\n",
      "Training loss for batch 4544 : 0.22261600196361542\n",
      "Training loss for batch 4545 : 0.899012565612793\n",
      "Training loss for batch 4546 : 0.5984441637992859\n",
      "Training loss for batch 4547 : 0.13679341971874237\n",
      "Training loss for batch 4548 : 0.48708200454711914\n",
      "Training loss for batch 4549 : 0.14563524723052979\n",
      "Training loss for batch 4550 : 0.2418684959411621\n",
      "Training loss for batch 4551 : 0.05293641239404678\n",
      "Training loss for batch 4552 : 0.26992368698120117\n",
      "Training loss for batch 4553 : 0.44467952847480774\n",
      "Training loss for batch 4554 : 0.22105835378170013\n",
      "Training loss for batch 4555 : 0.5061570405960083\n",
      "Training loss for batch 4556 : 0.43377164006233215\n",
      "Training loss for batch 4557 : 0.21845509111881256\n",
      "Training loss for batch 4558 : 0.29083630442619324\n",
      "Training loss for batch 4559 : 0.40578851103782654\n",
      "Training loss for batch 4560 : 0.04654248431324959\n",
      "Training loss for batch 4561 : 0.1340291053056717\n",
      "Training loss for batch 4562 : 0.4570380449295044\n",
      "Training loss for batch 4563 : 0.6104500889778137\n",
      "Training loss for batch 4564 : 0.6251492500305176\n",
      "Training loss for batch 4565 : 0.20046518743038177\n",
      "Training loss for batch 4566 : 0.1044626384973526\n",
      "Training loss for batch 4567 : 0.6211587190628052\n",
      "Training loss for batch 4568 : 0.18779776990413666\n",
      "Training loss for batch 4569 : 0.2648466229438782\n",
      "Training loss for batch 4570 : 0.20319679379463196\n",
      "Training loss for batch 4571 : 0.24723942577838898\n",
      "Training loss for batch 4572 : 0.35959285497665405\n",
      "Training loss for batch 4573 : 0.5767452716827393\n",
      "Training loss for batch 4574 : 0.545198917388916\n",
      "Training loss for batch 4575 : 0.49113741517066956\n",
      "Training loss for batch 4576 : 0.32222601771354675\n",
      "Training loss for batch 4577 : 0.014319708570837975\n",
      "Training loss for batch 4578 : 0.21305660903453827\n",
      "Training loss for batch 4579 : 0.2358197420835495\n",
      "Training loss for batch 4580 : 0.08555416762828827\n",
      "Training loss for batch 4581 : 0.26840248703956604\n",
      "Training loss for batch 4582 : 0.3899051249027252\n",
      "Training loss for batch 4583 : 0.23745107650756836\n",
      "Training loss for batch 4584 : 0.43808096647262573\n",
      "Training loss for batch 4585 : 0.28666234016418457\n",
      "Training loss for batch 4586 : 0.1860651671886444\n",
      "Training loss for batch 4587 : 0.9323581457138062\n",
      "Training loss for batch 4588 : 0.22887025773525238\n",
      "Training loss for batch 4589 : 0.18238046765327454\n",
      "Training loss for batch 4590 : 0.11644367128610611\n",
      "Training loss for batch 4591 : 0.1527693271636963\n",
      "Training loss for batch 4592 : 0.20472602546215057\n",
      "Training loss for batch 4593 : 0.14076723158359528\n",
      "Training loss for batch 4594 : 0.6294697523117065\n",
      "Training loss for batch 4595 : 0.3382653295993805\n",
      "Training loss for batch 4596 : 0.05863137170672417\n",
      "Training loss for batch 4597 : 0.062018416821956635\n",
      "Training loss for batch 4598 : 0.06651363521814346\n",
      "Training loss for batch 4599 : 0.06605524569749832\n",
      "Training loss for batch 4600 : 0.1650456041097641\n",
      "Training loss for batch 4601 : 0.23850594460964203\n",
      "Training loss for batch 4602 : 0.8510310649871826\n",
      "Training loss for batch 4603 : 0.4136910140514374\n",
      "Training loss for batch 4604 : 0.37178754806518555\n",
      "Training loss for batch 4605 : 0.6066682934761047\n",
      "Training loss for batch 4606 : 0.460714191198349\n",
      "Training loss for batch 4607 : 0.02095319889485836\n",
      "Training loss for batch 4608 : 0.38190338015556335\n",
      "Training loss for batch 4609 : 0.1627352386713028\n",
      "Training loss for batch 4610 : 0.15971063077449799\n",
      "Training loss for batch 4611 : 0.4415375292301178\n",
      "Training loss for batch 4612 : 0.43741095066070557\n",
      "Training loss for batch 4613 : 0.4329955577850342\n",
      "Training loss for batch 4614 : 0.012377949431538582\n",
      "Training loss for batch 4615 : 0.612078845500946\n",
      "Training loss for batch 4616 : 0.054699283093214035\n",
      "Training loss for batch 4617 : 0.19808106124401093\n",
      "Training loss for batch 4618 : 0.4005040228366852\n",
      "Training loss for batch 4619 : 0.05614468455314636\n",
      "Training loss for batch 4620 : 0.08513160049915314\n",
      "Training loss for batch 4621 : 0.3211330473423004\n",
      "Training loss for batch 4622 : 0.22373409569263458\n",
      "Training loss for batch 4623 : 0.11493013799190521\n",
      "Training loss for batch 4624 : 0.4210716784000397\n",
      "Training loss for batch 4625 : 0.15818892419338226\n",
      "Training loss for batch 4626 : 0.315577894449234\n",
      "Training loss for batch 4627 : 0.24892538785934448\n",
      "Training loss for batch 4628 : 0.11949051916599274\n",
      "Training loss for batch 4629 : 0.6522717475891113\n",
      "Training loss for batch 4630 : 0.08619218319654465\n",
      "Training loss for batch 4631 : 0.13016697764396667\n",
      "Training loss for batch 4632 : 0.7350649237632751\n",
      "Training loss for batch 4633 : 0.5150022506713867\n",
      "Training loss for batch 4634 : 0.37375596165657043\n",
      "Training loss for batch 4635 : 0.0848672166466713\n",
      "Training loss for batch 4636 : 0.3118014335632324\n",
      "Training loss for batch 4637 : 0.2620020806789398\n",
      "Training loss for batch 4638 : 0.0798906609416008\n",
      "Training loss for batch 4639 : 0.42878615856170654\n",
      "Training loss for batch 4640 : 0.2490444779396057\n",
      "Training loss for batch 4641 : 0.3932637572288513\n",
      "Training loss for batch 4642 : 0.1383620798587799\n",
      "Training loss for batch 4643 : 0.10387645661830902\n",
      "Training loss for batch 4644 : 0.47669070959091187\n",
      "Training loss for batch 4645 : 0.18298283219337463\n",
      "Training loss for batch 4646 : 0.44714635610580444\n",
      "Training loss for batch 4647 : 0.35971304774284363\n",
      "Training loss for batch 4648 : 0.5700792074203491\n",
      "Training loss for batch 4649 : 0.6000303626060486\n",
      "Training loss for batch 4650 : 0.31971076130867004\n",
      "Training loss for batch 4651 : 0.41899675130844116\n",
      "Training loss for batch 4652 : 0.14073313772678375\n",
      "Training loss for batch 4653 : 0.20716938376426697\n",
      "Training loss for batch 4654 : 0.4896301031112671\n",
      "Training loss for batch 4655 : 0.1531393826007843\n",
      "Training loss for batch 4656 : 0.5254858136177063\n",
      "Training loss for batch 4657 : 0.3286817967891693\n",
      "Training loss for batch 4658 : 0.13752472400665283\n",
      "Training loss for batch 4659 : 0.07853243499994278\n",
      "Training loss for batch 4660 : 0.3064504861831665\n",
      "Training loss for batch 4661 : 0.06540238857269287\n",
      "Training loss for batch 4662 : 0.4053574800491333\n",
      "Training loss for batch 4663 : 0.2524932324886322\n",
      "Training loss for batch 4664 : 0.3165903389453888\n",
      "Training loss for batch 4665 : 0.41573360562324524\n",
      "Training loss for batch 4666 : 0.16713935136795044\n",
      "Training loss for batch 4667 : 0.17777909338474274\n",
      "Training loss for batch 4668 : 0.37311679124832153\n",
      "Training loss for batch 4669 : 0.3388863503932953\n",
      "Training loss for batch 4670 : 0.3267689645290375\n",
      "Training loss for batch 4671 : 0.34714141488075256\n",
      "Training loss for batch 4672 : 0.5506730675697327\n",
      "Training loss for batch 4673 : 0.20212838053703308\n",
      "Training loss for batch 4674 : 1.1463779211044312\n",
      "Training loss for batch 4675 : 0.6076762676239014\n",
      "Training loss for batch 4676 : 0.15639014542102814\n",
      "Training loss for batch 4677 : 0.398006409406662\n",
      "Training loss for batch 4678 : 0.010903029702603817\n",
      "Training loss for batch 4679 : 0.45691999793052673\n",
      "Training loss for batch 4680 : 0.34727686643600464\n",
      "Training loss for batch 4681 : 0.2279893308877945\n",
      "Training loss for batch 4682 : 0.25100287795066833\n",
      "Training loss for batch 4683 : 0.3137049376964569\n",
      "Training loss for batch 4684 : 0.619716465473175\n",
      "Training loss for batch 4685 : 0.36297109723091125\n",
      "Training loss for batch 4686 : 0.20353707671165466\n",
      "Training loss for batch 4687 : 0.2817988097667694\n",
      "Training loss for batch 4688 : 0.49896299839019775\n",
      "Training loss for batch 4689 : 0.6252611875534058\n",
      "Training loss for batch 4690 : 0.174497589468956\n",
      "Training loss for batch 4691 : 0.06919907033443451\n",
      "Training loss for batch 4692 : 0.35941943526268005\n",
      "Training loss for batch 4693 : 0.20277391374111176\n",
      "Training loss for batch 4694 : 0.29212668538093567\n",
      "Training loss for batch 4695 : 0.8176952004432678\n",
      "Training loss for batch 4696 : 0.14891894161701202\n",
      "Training loss for batch 4697 : 0.48087888956069946\n",
      "Training loss for batch 4698 : 0.6515031456947327\n",
      "Training loss for batch 4699 : 0.06099408492445946\n",
      "Training loss for batch 4700 : 0.20418933033943176\n",
      "Training loss for batch 4701 : 0.41705021262168884\n",
      "Training loss for batch 4702 : 0.18922704458236694\n",
      "Training loss for batch 4703 : 0.3899534344673157\n",
      "Training loss for batch 4704 : 0.18132580816745758\n",
      "Training loss for batch 4705 : 0.18475165963172913\n",
      "Training loss for batch 4706 : 0.1971830129623413\n",
      "Training loss for batch 4707 : 0.062088578939437866\n",
      "Training loss for batch 4708 : 0.14180850982666016\n",
      "Training loss for batch 4709 : 0.740139365196228\n",
      "Training loss for batch 4710 : 0.28513434529304504\n",
      "Training loss for batch 4711 : 0.41527658700942993\n",
      "Training loss for batch 4712 : 0.3448326289653778\n",
      "Training loss for batch 4713 : 0.42506152391433716\n",
      "Training loss for batch 4714 : 0.254661500453949\n",
      "Training loss for batch 4715 : 0.15093682706356049\n",
      "Training loss for batch 4716 : 0.004177896771579981\n",
      "Training loss for batch 4717 : 0.33636274933815\n",
      "Training loss for batch 4718 : 0.4322676658630371\n",
      "Training loss for batch 4719 : 0.2049253284931183\n",
      "Training loss for batch 4720 : 0.6887811422348022\n",
      "Training loss for batch 4721 : 0.04843378812074661\n",
      "Training loss for batch 4722 : 0.031914226710796356\n",
      "Training loss for batch 4723 : 0.23599660396575928\n",
      "Training loss for batch 4724 : 0.24934808909893036\n",
      "Training loss for batch 4725 : 0.7696685791015625\n",
      "Training loss for batch 4726 : 0.46802446246147156\n",
      "Training loss for batch 4727 : 0.08198701590299606\n",
      "Training loss for batch 4728 : 0.5762600302696228\n",
      "Training loss for batch 4729 : 0.1989230066537857\n",
      "Training loss for batch 4730 : 0.40152350068092346\n",
      "Training loss for batch 4731 : 0.2657889723777771\n",
      "Training loss for batch 4732 : 0.19759194552898407\n",
      "Training loss for batch 4733 : 0.1119522899389267\n",
      "Training loss for batch 4734 : 0.2168901115655899\n",
      "Training loss for batch 4735 : 0.17917026579380035\n",
      "Training loss for batch 4736 : 0.3010282516479492\n",
      "Training loss for batch 4737 : 0.48234304785728455\n",
      "Training loss for batch 4738 : 0.3398610055446625\n",
      "Training loss for batch 4739 : 0.3960137367248535\n",
      "Training loss for batch 4740 : 0.38717877864837646\n",
      "Training loss for batch 4741 : 0.943019688129425\n",
      "Training loss for batch 4742 : 0.37262389063835144\n",
      "Training loss for batch 4743 : 0.4116237163543701\n",
      "Training loss for batch 4744 : 0.07155656069517136\n",
      "Training loss for batch 4745 : 0.49592190980911255\n",
      "Training loss for batch 4746 : 0.4718587398529053\n",
      "Training loss for batch 4747 : 0.11555460840463638\n",
      "Training loss for batch 4748 : 0.33845028281211853\n",
      "Training loss for batch 4749 : 0.319662868976593\n",
      "Training loss for batch 4750 : 0.29599061608314514\n",
      "Training loss for batch 4751 : 0.18149855732917786\n",
      "Training loss for batch 4752 : 0.13829007744789124\n",
      "Training loss for batch 4753 : 0.3952825665473938\n",
      "Training loss for batch 4754 : 0.6902390122413635\n",
      "Training loss for batch 4755 : 0.1937153935432434\n",
      "Training loss for batch 4756 : 0.2082374393939972\n",
      "Training loss for batch 4757 : 0.3495239317417145\n",
      "Training loss for batch 4758 : 0.3246311843395233\n",
      "Training loss for batch 4759 : 0.4964293837547302\n",
      "Training loss for batch 4760 : 0.42021963000297546\n",
      "Training loss for batch 4761 : 0.662392258644104\n",
      "Training loss for batch 4762 : 0.23967033624649048\n",
      "Training loss for batch 4763 : 0.18442688882350922\n",
      "Training loss for batch 4764 : 0.12721669673919678\n",
      "Training loss for batch 4765 : 0.3877584934234619\n",
      "Training loss for batch 4766 : 0.2526334524154663\n",
      "Training loss for batch 4767 : 0.5955361723899841\n",
      "Training loss for batch 4768 : 0.15221188962459564\n",
      "Training loss for batch 4769 : 0.21730074286460876\n",
      "Training loss for batch 4770 : 0.2950699031352997\n",
      "Training loss for batch 4771 : 0.037142008543014526\n",
      "Training loss for batch 4772 : 0.10391303896903992\n",
      "Training loss for batch 4773 : 0.2494644671678543\n",
      "Training loss for batch 4774 : 0.37952494621276855\n",
      "Training loss for batch 4775 : 0.04285436496138573\n",
      "Training loss for batch 4776 : 0.3499455153942108\n",
      "Training loss for batch 4777 : 0.03304346650838852\n",
      "Training loss for batch 4778 : 0.33748021721839905\n",
      "Training loss for batch 4779 : 0.35340264439582825\n",
      "Training loss for batch 4780 : 0.15762819349765778\n",
      "Training loss for batch 4781 : 0.15147852897644043\n",
      "Training loss for batch 4782 : 0.157798632979393\n",
      "Training loss for batch 4783 : 0.274244099855423\n",
      "Training loss for batch 4784 : 0.3170549273490906\n",
      "Training loss for batch 4785 : 0.3384363353252411\n",
      "Training loss for batch 4786 : 0.09229292720556259\n",
      "Training loss for batch 4787 : 0.09266268461942673\n",
      "Training loss for batch 4788 : 0.2680697441101074\n",
      "Training loss for batch 4789 : 0.04510211944580078\n",
      "Training loss for batch 4790 : 0.28512799739837646\n",
      "Training loss for batch 4791 : 0.33190682530403137\n",
      "Training loss for batch 4792 : 0.26521366834640503\n",
      "Training loss for batch 4793 : 0.3395128846168518\n",
      "Training loss for batch 4794 : 0.23955032229423523\n",
      "Training loss for batch 4795 : 0.08962272852659225\n",
      "Training loss for batch 4796 : 0.490546315908432\n",
      "Training loss for batch 4797 : 0.28517013788223267\n",
      "Training loss for batch 4798 : 0.15193022787570953\n",
      "Training loss for batch 4799 : 0.15444016456604004\n",
      "Training loss for batch 4800 : 0.34193745255470276\n",
      "Training loss for batch 4801 : 0.2857511639595032\n",
      "Training loss for batch 4802 : 0.2741658091545105\n",
      "Training loss for batch 4803 : 0.20304684340953827\n",
      "Training loss for batch 4804 : 0.09459803253412247\n",
      "Training loss for batch 4805 : 0.5312310457229614\n",
      "Training loss for batch 4806 : 0.4706059694290161\n",
      "Training loss for batch 4807 : 0.1304675042629242\n",
      "Training loss for batch 4808 : 0.09190581738948822\n",
      "Training loss for batch 4809 : 0.4339539110660553\n",
      "Training loss for batch 4810 : 0.2471323162317276\n",
      "Training loss for batch 4811 : 0.12747153639793396\n",
      "Training loss for batch 4812 : 0.1719440221786499\n",
      "Training loss for batch 4813 : 0.05262301489710808\n",
      "Training loss for batch 4814 : 0.10767340660095215\n",
      "Training loss for batch 4815 : 0.2443617582321167\n",
      "Training loss for batch 4816 : 0.19404169917106628\n",
      "Training loss for batch 4817 : 0.2598285377025604\n",
      "Training loss for batch 4818 : 0.04191922768950462\n",
      "Training loss for batch 4819 : 0.3111381232738495\n",
      "Training loss for batch 4820 : 0.1920030266046524\n",
      "Training loss for batch 4821 : 0.2867543697357178\n",
      "Training loss for batch 4822 : 0.2930002808570862\n",
      "Training loss for batch 4823 : 0.35850033164024353\n",
      "Training loss for batch 4824 : 0.3739871084690094\n",
      "Training loss for batch 4825 : 0.11294512450695038\n",
      "Training loss for batch 4826 : 0.06380484998226166\n",
      "Training loss for batch 4827 : 0.5506601333618164\n",
      "Training loss for batch 4828 : 0.13446900248527527\n",
      "Training loss for batch 4829 : 0.2186441272497177\n",
      "Training loss for batch 4830 : 0.33481457829475403\n",
      "Training loss for batch 4831 : 0.13057631254196167\n",
      "Training loss for batch 4832 : 0.17919862270355225\n",
      "Training loss for batch 4833 : 0.25120407342910767\n",
      "Training loss for batch 4834 : 0.27616259455680847\n",
      "Training loss for batch 4835 : 0.09800080955028534\n",
      "Training loss for batch 4836 : 0.5853479504585266\n",
      "Training loss for batch 4837 : 0.33432450890541077\n",
      "Training loss for batch 4838 : 0.22547750174999237\n",
      "Training loss for batch 4839 : 0.2311263382434845\n",
      "Training loss for batch 4840 : 0.31839460134506226\n",
      "Training loss for batch 4841 : 0.07825173437595367\n",
      "Training loss for batch 4842 : 0.20004858076572418\n",
      "Training loss for batch 4843 : 0.16450613737106323\n",
      "Training loss for batch 4844 : 0.17755410075187683\n",
      "Training loss for batch 4845 : 0.3095029890537262\n",
      "Training loss for batch 4846 : 0.23724234104156494\n",
      "Training loss for batch 4847 : 0.373900830745697\n",
      "Training loss for batch 4848 : 0.10775632411241531\n",
      "Training loss for batch 4849 : 0.6156390309333801\n",
      "Training loss for batch 4850 : 0.21683581173419952\n",
      "Training loss for batch 4851 : 0.0582689568400383\n",
      "Training loss for batch 4852 : 0.5288754105567932\n",
      "Training loss for batch 4853 : 0.2517648935317993\n",
      "Training loss for batch 4854 : 0.34416133165359497\n",
      "Training loss for batch 4855 : 0.1537204533815384\n",
      "Training loss for batch 4856 : 0.37308579683303833\n",
      "Training loss for batch 4857 : 0.12643402814865112\n",
      "Training loss for batch 4858 : 0.7483859658241272\n",
      "Training loss for batch 4859 : 0.1328294426202774\n",
      "Training loss for batch 4860 : 0.37034228444099426\n",
      "Training loss for batch 4861 : 0.5562164783477783\n",
      "Training loss for batch 4862 : 0.18664275109767914\n",
      "Training loss for batch 4863 : 0.10631202161312103\n",
      "Training loss for batch 4864 : 0.490837037563324\n",
      "Training loss for batch 4865 : 0.3309551775455475\n",
      "Training loss for batch 4866 : 0.352593332529068\n",
      "Training loss for batch 4867 : 0.8008425831794739\n",
      "Training loss for batch 4868 : 0.2153424173593521\n",
      "Training loss for batch 4869 : 0.22021789848804474\n",
      "Training loss for batch 4870 : 0.36193907260894775\n",
      "Training loss for batch 4871 : 0.3273411989212036\n",
      "Training loss for batch 4872 : 0.12344197183847427\n",
      "Training loss for batch 4873 : 0.28827381134033203\n",
      "Training loss for batch 4874 : 0.45318803191185\n",
      "Training loss for batch 4875 : 0.6835446953773499\n",
      "Training loss for batch 4876 : 0.3578047752380371\n",
      "Training loss for batch 4877 : 0.18625573813915253\n",
      "Training loss for batch 4878 : 0.3658994734287262\n",
      "Training loss for batch 4879 : 0.2692200243473053\n",
      "Training loss for batch 4880 : 0.17694386839866638\n",
      "Training loss for batch 4881 : 0.26413220167160034\n",
      "Training loss for batch 4882 : 0.2777276635169983\n",
      "Training loss for batch 4883 : 0.12424861639738083\n",
      "Training loss for batch 4884 : 0.4175999164581299\n",
      "Training loss for batch 4885 : 0.17617960274219513\n",
      "Training loss for batch 4886 : 0.6099987626075745\n",
      "Training loss for batch 4887 : 0.0657915323972702\n",
      "Training loss for batch 4888 : 0.1337910294532776\n",
      "Training loss for batch 4889 : 0.27992093563079834\n",
      "Training loss for batch 4890 : 0.6433723568916321\n",
      "Training loss for batch 4891 : 0.774252712726593\n",
      "Training loss for batch 4892 : 0.04647298529744148\n",
      "Training loss for batch 4893 : 0.5894808173179626\n",
      "Training loss for batch 4894 : 0.2583349645137787\n",
      "Training loss for batch 4895 : 0.3227386474609375\n",
      "Training loss for batch 4896 : 0.14655140042304993\n",
      "Training loss for batch 4897 : 0.4403560161590576\n",
      "Training loss for batch 4898 : 1.057069182395935\n",
      "Training loss for batch 4899 : 0.5169768333435059\n",
      "Training loss for batch 4900 : 0.3818399906158447\n",
      "Training loss for batch 4901 : 0.33225423097610474\n",
      "Training loss for batch 4902 : 0.3488525152206421\n",
      "Training loss for batch 4903 : 0.2079934924840927\n",
      "Training loss for batch 4904 : 0.6541846990585327\n",
      "Training loss for batch 4905 : 0.5606287121772766\n",
      "Training loss for batch 4906 : 0.18693573772907257\n",
      "Training loss for batch 4907 : 0.5480308532714844\n",
      "Training loss for batch 4908 : 0.21983852982521057\n",
      "Training loss for batch 4909 : 0.43856000900268555\n",
      "Training loss for batch 4910 : 0.217734694480896\n",
      "Training loss for batch 4911 : 0.2995050251483917\n",
      "Training loss for batch 4912 : 0.43297895789146423\n",
      "Training loss for batch 4913 : 0.37032487988471985\n",
      "Training loss for batch 4914 : 0.12076862156391144\n",
      "Training loss for batch 4915 : 0.3071396052837372\n",
      "Training loss for batch 4916 : 0.5029200315475464\n",
      "Training loss for batch 4917 : 0.6235026717185974\n",
      "Training loss for batch 4918 : 0.24716632068157196\n",
      "Training loss for batch 4919 : 0.5930697917938232\n",
      "Training loss for batch 4920 : 0.18381348252296448\n",
      "Training loss for batch 4921 : 0.07772906869649887\n",
      "Training loss for batch 4922 : 0.09490843117237091\n",
      "Training loss for batch 4923 : 0.3003695011138916\n",
      "Training loss for batch 4924 : 0.16213130950927734\n",
      "Training loss for batch 4925 : 0.25231391191482544\n",
      "Training loss for batch 4926 : 0.3685619533061981\n",
      "Training loss for batch 4927 : 0.41160523891448975\n",
      "Training loss for batch 4928 : 0.46596983075141907\n",
      "Training loss for batch 4929 : 0.4989142119884491\n",
      "Training loss for batch 4930 : 0.21062307059764862\n",
      "Training loss for batch 4931 : 0.4626237154006958\n",
      "Training loss for batch 4932 : 0.23518943786621094\n",
      "Training loss for batch 4933 : 0.20940335094928741\n",
      "Training loss for batch 4934 : 0.48950350284576416\n",
      "Training loss for batch 4935 : 0.30329999327659607\n",
      "Training loss for batch 4936 : 0.08630210906267166\n",
      "Training loss for batch 4937 : 0.31227731704711914\n",
      "Training loss for batch 4938 : 0.4828587770462036\n",
      "Training loss for batch 4939 : 0.055770739912986755\n",
      "Training loss for batch 4940 : 0.04518933966755867\n",
      "Training loss for batch 4941 : 0.19402623176574707\n",
      "Training loss for batch 4942 : 0.14594845473766327\n",
      "Training loss for batch 4943 : 0.0828939750790596\n",
      "Training loss for batch 4944 : 0.7009557485580444\n",
      "Training loss for batch 4945 : 0.2839926779270172\n",
      "Training loss for batch 4946 : 0.4391792416572571\n",
      "Training loss for batch 4947 : 0.2918843924999237\n",
      "Training loss for batch 4948 : 0.3030529320240021\n",
      "Training loss for batch 4949 : 0.039494700729846954\n",
      "Training loss for batch 4950 : 0.43655407428741455\n",
      "Training loss for batch 4951 : 0.5812925100326538\n",
      "Training loss for batch 4952 : 0.17536020278930664\n",
      "Training loss for batch 4953 : 0.4879600405693054\n",
      "Training loss for batch 4954 : 0.3839801549911499\n",
      "Training loss for batch 4955 : 0.13619764149188995\n",
      "Training loss for batch 4956 : 0.04289144650101662\n",
      "Training loss for batch 4957 : 0.28648841381073\n",
      "Training loss for batch 4958 : 0.016719304025173187\n",
      "Training loss for batch 4959 : 0.2869255542755127\n",
      "Training loss for batch 4960 : 0.16722111403942108\n",
      "Training loss for batch 4961 : 0.439689576625824\n",
      "Training loss for batch 4962 : 0.14280717074871063\n",
      "Training loss for batch 4963 : 0.4609810709953308\n",
      "Training loss for batch 4964 : 0.0012421708088368177\n",
      "Training loss for batch 4965 : 0.5688465237617493\n",
      "Training loss for batch 4966 : 0.12725476920604706\n",
      "Training loss for batch 4967 : 0.3099263608455658\n",
      "Training loss for batch 4968 : 0.5921774506568909\n",
      "Training loss for batch 4969 : 0.409749299287796\n",
      "Training loss for batch 4970 : 0.06924144178628922\n",
      "Training loss for batch 4971 : 0.3784043490886688\n",
      "Training loss for batch 4972 : 0.11051739752292633\n",
      "Training loss for batch 4973 : 0.02748611383140087\n",
      "Training loss for batch 4974 : 0.3954741060733795\n",
      "Training loss for batch 4975 : 0.08028382807970047\n",
      "Training loss for batch 4976 : 0.39981400966644287\n",
      "Training loss for batch 4977 : 0.4404653310775757\n",
      "Training loss for batch 4978 : 0.06671290844678879\n",
      "Training loss for batch 4979 : 0.06342446804046631\n",
      "Training loss for batch 4980 : 0.21795575320720673\n",
      "Training loss for batch 4981 : 0.25283512473106384\n",
      "Training loss for batch 4982 : 0.1350812464952469\n",
      "Training loss for batch 4983 : 0.09298263490200043\n",
      "Training loss for batch 4984 : 0.2152365744113922\n",
      "Training loss for batch 4985 : 0.5351728200912476\n",
      "Training loss for batch 4986 : 0.17573414742946625\n",
      "Training loss for batch 4987 : 0.6203168630599976\n",
      "Training loss for batch 4988 : 0.23431986570358276\n",
      "Training loss for batch 4989 : 0.16906461119651794\n",
      "Training loss for batch 4990 : 0.24013125896453857\n",
      "Training loss for batch 4991 : 0.4078719913959503\n",
      "Training loss for batch 4992 : 0.028222423046827316\n",
      "Training loss for batch 4993 : 0.20114371180534363\n",
      "Training loss for batch 4994 : 0.3588845133781433\n",
      "Training loss for batch 4995 : 0.42133572697639465\n",
      "Training loss for batch 4996 : 0.15562713146209717\n",
      "Training loss for batch 4997 : 0.5171205997467041\n",
      "Training loss for batch 4998 : 0.09026850759983063\n",
      "Training loss for batch 4999 : 0.6272109746932983\n",
      "Training loss for batch 5000 : 0.3393060266971588\n",
      "Training loss for batch 5001 : 0.33097484707832336\n",
      "Training loss for batch 5002 : 0.020823819562792778\n",
      "Training loss for batch 5003 : 0.2206883579492569\n",
      "Training loss for batch 5004 : 0.5218878984451294\n",
      "Training loss for batch 5005 : 0.5679540634155273\n",
      "Training loss for batch 5006 : 0.0456082746386528\n",
      "Training loss for batch 5007 : 0.0620589442551136\n",
      "Training loss for batch 5008 : 0.5007877349853516\n",
      "Training loss for batch 5009 : 0.6282925009727478\n",
      "Training loss for batch 5010 : 0.4967308044433594\n",
      "Training loss for batch 5011 : 0.16115374863147736\n",
      "Training loss for batch 5012 : 0.07602719217538834\n",
      "Training loss for batch 5013 : 0.13863985240459442\n",
      "Training loss for batch 5014 : 0.3882371485233307\n",
      "Training loss for batch 5015 : 0.5115194916725159\n",
      "Training loss for batch 5016 : 0.5052833557128906\n",
      "Training loss for batch 5017 : 0.5450761914253235\n",
      "Training loss for batch 5018 : 0.46162423491477966\n",
      "Training loss for batch 5019 : 0.1647305190563202\n",
      "Training loss for batch 5020 : 0.4185291826725006\n",
      "Training loss for batch 5021 : 0.2871955931186676\n",
      "Training loss for batch 5022 : 0.11509815603494644\n",
      "Training loss for batch 5023 : 0.007744540926069021\n",
      "Training loss for batch 5024 : 0.13055801391601562\n",
      "Training loss for batch 5025 : 0.26032325625419617\n",
      "Training loss for batch 5026 : 0.04029645025730133\n",
      "Training loss for batch 5027 : 0.04900325462222099\n",
      "Training loss for batch 5028 : 0.4892248809337616\n",
      "Training loss for batch 5029 : 0.10584205389022827\n",
      "Training loss for batch 5030 : 0.0013029873371124268\n",
      "Training loss for batch 5031 : 0.4006522595882416\n",
      "Training loss for batch 5032 : 0.21540814638137817\n",
      "Training loss for batch 5033 : 0.11065885424613953\n",
      "Training loss for batch 5034 : 0.13854296505451202\n",
      "Training loss for batch 5035 : 0.6648271083831787\n",
      "Training loss for batch 5036 : 0.398398220539093\n",
      "Training loss for batch 5037 : 0.444903165102005\n",
      "Training loss for batch 5038 : 0.2657681107521057\n",
      "Training loss for batch 5039 : 0.12081797420978546\n",
      "Training loss for batch 5040 : 0.29709652066230774\n",
      "Training loss for batch 5041 : 0.27238819003105164\n",
      "Training loss for batch 5042 : 0.12375039607286453\n",
      "Training loss for batch 5043 : 0.30535516142845154\n",
      "Training loss for batch 5044 : 0.15460573136806488\n",
      "Training loss for batch 5045 : 0.3122880458831787\n",
      "Training loss for batch 5046 : 0.6901432275772095\n",
      "Training loss for batch 5047 : 0.04097128286957741\n",
      "Training loss for batch 5048 : 0.08034112304449081\n",
      "Training loss for batch 5049 : 0.1225939616560936\n",
      "Training loss for batch 5050 : 0.2967647910118103\n",
      "Training loss for batch 5051 : 0.08613818138837814\n",
      "Training loss for batch 5052 : 0.166757732629776\n",
      "Training loss for batch 5053 : 0.4508395195007324\n",
      "Training loss for batch 5054 : 0.23314155638217926\n",
      "Training loss for batch 5055 : 0.502870500087738\n",
      "Training loss for batch 5056 : 0.41550615429878235\n",
      "Training loss for batch 5057 : 0.14668764173984528\n",
      "Training loss for batch 5058 : 0.3432328402996063\n",
      "Training loss for batch 5059 : 0.23273086547851562\n",
      "Training loss for batch 5060 : 0.253503680229187\n",
      "Training loss for batch 5061 : 0.3148197829723358\n",
      "Training loss for batch 5062 : 0.08877010643482208\n",
      "Training loss for batch 5063 : 0.5931754112243652\n",
      "Training loss for batch 5064 : 0.4571601450443268\n",
      "Training loss for batch 5065 : 0.044997721910476685\n",
      "Training loss for batch 5066 : 0.20589473843574524\n",
      "Training loss for batch 5067 : 0.19266949594020844\n",
      "Training loss for batch 5068 : 0.30425015091896057\n",
      "Training loss for batch 5069 : 0.6409583687782288\n",
      "Training loss for batch 5070 : 0.07190880924463272\n",
      "Training loss for batch 5071 : 0.34033310413360596\n",
      "Training loss for batch 5072 : 0.023581964895129204\n",
      "Training loss for batch 5073 : 0.12328517436981201\n",
      "Training loss for batch 5074 : 0.11248646676540375\n",
      "Training loss for batch 5075 : 0.07873190939426422\n",
      "Training loss for batch 5076 : 0.3181069791316986\n",
      "Training loss for batch 5077 : 0.34605810046195984\n",
      "Training loss for batch 5078 : 0.9730014204978943\n",
      "Training loss for batch 5079 : 0.1473233699798584\n",
      "Training loss for batch 5080 : 0.08615899831056595\n",
      "Training loss for batch 5081 : 0.5769468545913696\n",
      "Training loss for batch 5082 : 0.22379688918590546\n",
      "Training loss for batch 5083 : 0.1635863333940506\n",
      "Training loss for batch 5084 : 0.6939088106155396\n",
      "Training loss for batch 5085 : 0.09904607385396957\n",
      "Training loss for batch 5086 : 0.21116848289966583\n",
      "Training loss for batch 5087 : 0.21412865817546844\n",
      "Training loss for batch 5088 : 0.073501355946064\n",
      "Training loss for batch 5089 : 0.2694385349750519\n",
      "Training loss for batch 5090 : 0.20808668434619904\n",
      "Training loss for batch 5091 : 0.20448912680149078\n",
      "Training loss for batch 5092 : 0.701208770275116\n",
      "Training loss for batch 5093 : 0.4227425754070282\n",
      "Training loss for batch 5094 : 0.269715279340744\n",
      "Training loss for batch 5095 : 0.07435565441846848\n",
      "Training loss for batch 5096 : 0.6170771718025208\n",
      "Training loss for batch 5097 : 0.2837008833885193\n",
      "Training loss for batch 5098 : 0.15195225179195404\n",
      "Training loss for batch 5099 : 0.10589621216058731\n",
      "Training loss for batch 5100 : 0.11766653507947922\n",
      "Training loss for batch 5101 : 0.11743389070034027\n",
      "Training loss for batch 5102 : 0.5566805005073547\n",
      "Training loss for batch 5103 : 0.00844299141317606\n",
      "Training loss for batch 5104 : 0.7748809456825256\n",
      "Training loss for batch 5105 : 0.32390743494033813\n",
      "Training loss for batch 5106 : 0.062023039907217026\n",
      "Training loss for batch 5107 : 0.09707304835319519\n",
      "Training loss for batch 5108 : 0.3213177025318146\n",
      "Training loss for batch 5109 : 0.6714327335357666\n",
      "Training loss for batch 5110 : 0.40261074900627136\n",
      "Training loss for batch 5111 : 0.367719441652298\n",
      "Training loss for batch 5112 : 0.21574591100215912\n",
      "Training loss for batch 5113 : 0.432669460773468\n",
      "Training loss for batch 5114 : 0.9604002833366394\n",
      "Training loss for batch 5115 : 0.10383064299821854\n",
      "Training loss for batch 5116 : 0.13276313245296478\n",
      "Training loss for batch 5117 : 0.5798743963241577\n",
      "Training loss for batch 5118 : 0.1903885155916214\n",
      "Training loss for batch 5119 : 0.513606071472168\n",
      "Training loss for batch 5120 : 0.46083563566207886\n",
      "Training loss for batch 5121 : 0.13270527124404907\n",
      "Training loss for batch 5122 : 0.11121194064617157\n",
      "Training loss for batch 5123 : 0.6529650688171387\n",
      "Training loss for batch 5124 : 0.12744207680225372\n",
      "Training loss for batch 5125 : 0.23982477188110352\n",
      "Training loss for batch 5126 : 0.4861986041069031\n",
      "Training loss for batch 5127 : 0.6543164849281311\n",
      "Training loss for batch 5128 : 0.4068930149078369\n",
      "Training loss for batch 5129 : 0.2325255423784256\n",
      "Training loss for batch 5130 : 0.0755588486790657\n",
      "Training loss for batch 5131 : 0.19250284135341644\n",
      "Training loss for batch 5132 : 0.10569353401660919\n",
      "Training loss for batch 5133 : 0.21261754631996155\n",
      "Training loss for batch 5134 : 0.575751781463623\n",
      "Training loss for batch 5135 : 0.46063554286956787\n",
      "Training loss for batch 5136 : 0.30379268527030945\n",
      "Training loss for batch 5137 : 0.2602728307247162\n",
      "Training loss for batch 5138 : 0.2251889556646347\n",
      "Training loss for batch 5139 : 0.21129193902015686\n",
      "Training loss for batch 5140 : 0.47406312823295593\n",
      "Training loss for batch 5141 : 0.04877973347902298\n",
      "Training loss for batch 5142 : 0.36499613523483276\n",
      "Training loss for batch 5143 : 0.04512085020542145\n",
      "Training loss for batch 5144 : 0.439156174659729\n",
      "Training loss for batch 5145 : 0.19914571940898895\n",
      "Training loss for batch 5146 : 0.18291884660720825\n",
      "Training loss for batch 5147 : 0.30263397097587585\n",
      "Training loss for batch 5148 : 0.19276036322116852\n",
      "Training loss for batch 5149 : 0.1932811588048935\n",
      "Training loss for batch 5150 : 0.3095759451389313\n",
      "Training loss for batch 5151 : 0.4240576922893524\n",
      "Training loss for batch 5152 : 0.19260238111019135\n",
      "Training loss for batch 5153 : 0.09301355481147766\n",
      "Training loss for batch 5154 : 0.6863994002342224\n",
      "Training loss for batch 5155 : 0.2931557297706604\n",
      "Training loss for batch 5156 : 0.019877275452017784\n",
      "Training loss for batch 5157 : 0.0355091467499733\n",
      "Training loss for batch 5158 : 0.36031168699264526\n",
      "Training loss for batch 5159 : 0.2503524720668793\n",
      "Training loss for batch 5160 : 0.003262857673689723\n",
      "Training loss for batch 5161 : 0.22633901238441467\n",
      "Training loss for batch 5162 : 0.00415771221742034\n",
      "Training loss for batch 5163 : 0.43589961528778076\n",
      "Training loss for batch 5164 : 0.32290950417518616\n",
      "Training loss for batch 5165 : 0.15262454748153687\n",
      "Training loss for batch 5166 : 0.07449416816234589\n",
      "Training loss for batch 5167 : 0.13708238303661346\n",
      "Training loss for batch 5168 : 0.09691955894231796\n",
      "Training loss for batch 5169 : 0.18363577127456665\n",
      "Training loss for batch 5170 : 0.285536527633667\n",
      "Training loss for batch 5171 : 0.2526894509792328\n",
      "Training loss for batch 5172 : 0.34382349252700806\n",
      "Training loss for batch 5173 : 0.014166601002216339\n",
      "Training loss for batch 5174 : 0.35895463824272156\n",
      "Training loss for batch 5175 : 0.4492395520210266\n",
      "Training loss for batch 5176 : 0.030654286965727806\n",
      "Training loss for batch 5177 : 0.1726512610912323\n",
      "Training loss for batch 5178 : 0.4815996587276459\n",
      "Training loss for batch 5179 : 0.09870769083499908\n",
      "Training loss for batch 5180 : 0.10556075721979141\n",
      "Training loss for batch 5181 : 0.07370120286941528\n",
      "Training loss for batch 5182 : 0.38318371772766113\n",
      "Training loss for batch 5183 : 0.003674631007015705\n",
      "Training loss for batch 5184 : 0.3133518397808075\n",
      "Training loss for batch 5185 : 0.3756672739982605\n",
      "Training loss for batch 5186 : 0.08562805503606796\n",
      "Training loss for batch 5187 : 0.08581298589706421\n",
      "Training loss for batch 5188 : 0.2942865192890167\n",
      "Training loss for batch 5189 : 0.10214284062385559\n",
      "Training loss for batch 5190 : 0.053672391921281815\n",
      "Training loss for batch 5191 : 0.10033001750707626\n",
      "Training loss for batch 5192 : 0.22603276371955872\n",
      "Training loss for batch 5193 : 0.2141609936952591\n",
      "Training loss for batch 5194 : 0.32936516404151917\n",
      "Training loss for batch 5195 : 0.11388185620307922\n",
      "Training loss for batch 5196 : 0.17735211551189423\n",
      "Training loss for batch 5197 : 0.5961959958076477\n",
      "Training loss for batch 5198 : 0.4900737702846527\n",
      "Training loss for batch 5199 : 0.09362432360649109\n",
      "Training loss for batch 5200 : 0.007296472787857056\n",
      "Training loss for batch 5201 : 0.5505497455596924\n",
      "Training loss for batch 5202 : 0.6504238843917847\n",
      "Training loss for batch 5203 : 0.43294963240623474\n",
      "Training loss for batch 5204 : 0.8258993625640869\n",
      "Training loss for batch 5205 : 0.8575823307037354\n",
      "Training loss for batch 5206 : 0.515810489654541\n",
      "Training loss for batch 5207 : 0.015145626850426197\n",
      "Training loss for batch 5208 : 0.3524589240550995\n",
      "Training loss for batch 5209 : 0.08979707211256027\n",
      "Training loss for batch 5210 : 0.13127335906028748\n",
      "Training loss for batch 5211 : 0.321254700422287\n",
      "Training loss for batch 5212 : 0.0926862433552742\n",
      "Training loss for batch 5213 : 0.25195538997650146\n",
      "Training loss for batch 5214 : 0.008671830408275127\n",
      "Training loss for batch 5215 : 0.3555976450443268\n",
      "Training loss for batch 5216 : 0.16514497995376587\n",
      "Training loss for batch 5217 : 0.7132321000099182\n",
      "Training loss for batch 5218 : 0.18489588797092438\n",
      "Training loss for batch 5219 : 0.39950031042099\n",
      "Training loss for batch 5220 : 0.486568808555603\n",
      "Training loss for batch 5221 : 0.42519479990005493\n",
      "Training loss for batch 5222 : 0.23474574089050293\n",
      "Training loss for batch 5223 : 0.07202035188674927\n",
      "Training loss for batch 5224 : 0.00199626456014812\n",
      "Training loss for batch 5225 : 0.4856172800064087\n",
      "Training loss for batch 5226 : 0.27300024032592773\n",
      "Training loss for batch 5227 : 0.2507869601249695\n",
      "Training loss for batch 5228 : 0.42400211095809937\n",
      "Training loss for batch 5229 : 0.23091447353363037\n",
      "Training loss for batch 5230 : 0.38061612844467163\n",
      "Training loss for batch 5231 : 0.2835022509098053\n",
      "Training loss for batch 5232 : 0.2500796616077423\n",
      "Training loss for batch 5233 : 1.1174718141555786\n",
      "Training loss for batch 5234 : 0.2937260866165161\n",
      "Training loss for batch 5235 : 0.049194592982530594\n",
      "Training loss for batch 5236 : 0.40942245721817017\n",
      "Training loss for batch 5237 : 0.4848659038543701\n",
      "Training loss for batch 5238 : 0.09126798808574677\n",
      "Training loss for batch 5239 : 0.30680590867996216\n",
      "Training loss for batch 5240 : 0.08025919646024704\n",
      "Training loss for batch 5241 : 0.4018515646457672\n",
      "Training loss for batch 5242 : 0.19118058681488037\n",
      "Training loss for batch 5243 : 0.07937177270650864\n",
      "Training loss for batch 5244 : 0.05165211111307144\n",
      "Training loss for batch 5245 : 0.07966581732034683\n",
      "Training loss for batch 5246 : 0.4426548480987549\n",
      "Training loss for batch 5247 : 0.3805665373802185\n",
      "Training loss for batch 5248 : 0.2361876368522644\n",
      "Training loss for batch 5249 : 0.07453013211488724\n",
      "Training loss for batch 5250 : 0.21075715124607086\n",
      "Training loss for batch 5251 : 0.17108112573623657\n",
      "Training loss for batch 5252 : 0.1661192923784256\n",
      "Training loss for batch 5253 : 0.22554734349250793\n",
      "Training loss for batch 5254 : 0.25389862060546875\n",
      "Training loss for batch 5255 : 0.11762437224388123\n",
      "Training loss for batch 5256 : 0.16555695235729218\n",
      "Training loss for batch 5257 : 0.6639894247055054\n",
      "Training loss for batch 5258 : 0.3730291426181793\n",
      "Training loss for batch 5259 : 0.029835181310772896\n",
      "Training loss for batch 5260 : 0.006997942924499512\n",
      "Training loss for batch 5261 : 0.20861408114433289\n",
      "Training loss for batch 5262 : 0.6112126111984253\n",
      "Training loss for batch 5263 : 0.2218744158744812\n",
      "Training loss for batch 5264 : 0.4646167755126953\n",
      "Training loss for batch 5265 : 0.2948448061943054\n",
      "Training loss for batch 5266 : 0.008665730245411396\n",
      "Training loss for batch 5267 : 0.28753775358200073\n",
      "Training loss for batch 5268 : 0.24959807097911835\n",
      "Training loss for batch 5269 : 0.4071786403656006\n",
      "Training loss for batch 5270 : 0.4943041205406189\n",
      "Training loss for batch 5271 : 0.844002366065979\n",
      "Training loss for batch 5272 : 0.17694072425365448\n",
      "Training loss for batch 5273 : 0.15266524255275726\n",
      "Training loss for batch 5274 : 0.11638601124286652\n",
      "Training loss for batch 5275 : 0.012815008871257305\n",
      "Training loss for batch 5276 : 0.3957659602165222\n",
      "Training loss for batch 5277 : 0.0783737450838089\n",
      "Training loss for batch 5278 : 0.415884792804718\n",
      "Training loss for batch 5279 : 0.06978318840265274\n",
      "Training loss for batch 5280 : 0.10669873654842377\n",
      "Training loss for batch 5281 : 0.7798340320587158\n",
      "Training loss for batch 5282 : 0.5695513486862183\n",
      "Training loss for batch 5283 : 0.3115551769733429\n",
      "Training loss for batch 5284 : 0.4361560046672821\n",
      "Training loss for batch 5285 : 0.006253342144191265\n",
      "Training loss for batch 5286 : 0.16633747518062592\n",
      "Training loss for batch 5287 : 0.8889344930648804\n",
      "Training loss for batch 5288 : 0.3508371114730835\n",
      "Training loss for batch 5289 : 0.30515170097351074\n",
      "Training loss for batch 5290 : 0.21804575622081757\n",
      "Training loss for batch 5291 : 0.2402641773223877\n",
      "Training loss for batch 5292 : 0.23422829806804657\n",
      "Training loss for batch 5293 : 0.46976158022880554\n",
      "Training loss for batch 5294 : 0.3745979368686676\n",
      "Training loss for batch 5295 : 0.1397077888250351\n",
      "Training loss for batch 5296 : 0.4323980212211609\n",
      "Training loss for batch 5297 : 0.18100596964359283\n",
      "Training loss for batch 5298 : 0.3827264606952667\n",
      "Training loss for batch 5299 : 0.4325299859046936\n",
      "Training loss for batch 5300 : 0.07651199400424957\n",
      "Training loss for batch 5301 : 0.13309471309185028\n",
      "Training loss for batch 5302 : 0.17237745225429535\n",
      "Training loss for batch 5303 : 0.3833552598953247\n",
      "Training loss for batch 5304 : 0.3065665662288666\n",
      "Training loss for batch 5305 : 0.5517189502716064\n",
      "Training loss for batch 5306 : 0.24430610239505768\n",
      "Training loss for batch 5307 : 0.26513487100601196\n",
      "Training loss for batch 5308 : 0.4493480622768402\n",
      "Training loss for batch 5309 : 0.8502730131149292\n",
      "Training loss for batch 5310 : 0.3105006217956543\n",
      "Training loss for batch 5311 : 0.7747915983200073\n",
      "Training loss for batch 5312 : 0.009163767099380493\n",
      "Training loss for batch 5313 : 0.002270396100357175\n",
      "Training loss for batch 5314 : 0.046065133064985275\n",
      "Training loss for batch 5315 : 0.6285638213157654\n",
      "Training loss for batch 5316 : 0.5782032012939453\n",
      "Training loss for batch 5317 : 0.035999614745378494\n",
      "Training loss for batch 5318 : 0.11004004627466202\n",
      "Training loss for batch 5319 : 0.444107323884964\n",
      "Training loss for batch 5320 : 0.05887468159198761\n",
      "Training loss for batch 5321 : 0.0010382302571088076\n",
      "Training loss for batch 5322 : 0.31073352694511414\n",
      "Training loss for batch 5323 : 0.08121077716350555\n",
      "Training loss for batch 5324 : 0.3386080265045166\n",
      "Training loss for batch 5325 : 0.3979915380477905\n",
      "Training loss for batch 5326 : 0.3667365312576294\n",
      "Training loss for batch 5327 : 0.3613450527191162\n",
      "Training loss for batch 5328 : 0.16133952140808105\n",
      "Training loss for batch 5329 : 0.46506842970848083\n",
      "Training loss for batch 5330 : 0.4046626091003418\n",
      "Training loss for batch 5331 : 0.4210885167121887\n",
      "Training loss for batch 5332 : 0.6982627511024475\n",
      "Training loss for batch 5333 : 0.34225791692733765\n",
      "Training loss for batch 5334 : 0.42349913716316223\n",
      "Training loss for batch 5335 : 0.08529353886842728\n",
      "Training loss for batch 5336 : 0.17601025104522705\n",
      "Training loss for batch 5337 : 0.1751619279384613\n",
      "Training loss for batch 5338 : 0.19953212141990662\n",
      "Training loss for batch 5339 : 0.438745379447937\n",
      "Training loss for batch 5340 : 0.3967176079750061\n",
      "Training loss for batch 5341 : 0.28848347067832947\n",
      "Training loss for batch 5342 : 0.03989385813474655\n",
      "Training loss for batch 5343 : 0.11423256993293762\n",
      "Training loss for batch 5344 : 0.08371718972921371\n",
      "Training loss for batch 5345 : 0.541180431842804\n",
      "Training loss for batch 5346 : 0.09712441265583038\n",
      "Training loss for batch 5347 : 0.20136931538581848\n",
      "Training loss for batch 5348 : 0.0835212767124176\n",
      "Training loss for batch 5349 : 0.7504248023033142\n",
      "Training loss for batch 5350 : 0.3393803834915161\n",
      "Training loss for batch 5351 : 0.7152705788612366\n",
      "Training loss for batch 5352 : 0.018611907958984375\n",
      "Training loss for batch 5353 : 0.5530849695205688\n",
      "Training loss for batch 5354 : 0.41185030341148376\n",
      "Training loss for batch 5355 : 0.29666298627853394\n",
      "Training loss for batch 5356 : 0.4390314817428589\n",
      "Training loss for batch 5357 : 0.492855966091156\n",
      "Training loss for batch 5358 : 0.2654879689216614\n",
      "Training loss for batch 5359 : 0.6802343130111694\n",
      "Training loss for batch 5360 : 0.321836918592453\n",
      "Training loss for batch 5361 : 0.48187455534935\n",
      "Training loss for batch 5362 : 0.3127221167087555\n",
      "Training loss for batch 5363 : 0.029636980965733528\n",
      "Training loss for batch 5364 : 0.43953192234039307\n",
      "Training loss for batch 5365 : 0.3962116539478302\n",
      "Training loss for batch 5366 : 0.4210679531097412\n",
      "Training loss for batch 5367 : 0.27852699160575867\n",
      "Training loss for batch 5368 : 0.25921595096588135\n",
      "Training loss for batch 5369 : 0.21444030106067657\n",
      "Training loss for batch 5370 : 0.31681644916534424\n",
      "Training loss for batch 5371 : 0.0705535039305687\n",
      "Training loss for batch 5372 : 0.5273016095161438\n",
      "Training loss for batch 5373 : 0.1348189413547516\n",
      "Training loss for batch 5374 : 0.3665409982204437\n",
      "Training loss for batch 5375 : 0.1864652782678604\n",
      "Training loss for batch 5376 : 0.16855493187904358\n",
      "Training loss for batch 5377 : 0.2871910333633423\n",
      "Training loss for batch 5378 : 0.28779011964797974\n",
      "Training loss for batch 5379 : 0.34599289298057556\n",
      "Training loss for batch 5380 : 0.3689715266227722\n",
      "Training loss for batch 5381 : 0.21373790502548218\n",
      "Training loss for batch 5382 : 0.09101907908916473\n",
      "Training loss for batch 5383 : 0.4685247540473938\n",
      "Training loss for batch 5384 : 0.1356152594089508\n",
      "Training loss for batch 5385 : 0.38501492142677307\n",
      "Training loss for batch 5386 : 0.5387243032455444\n",
      "Training loss for batch 5387 : 0.1746746152639389\n",
      "Training loss for batch 5388 : 0.11149255931377411\n",
      "Training loss for batch 5389 : 0.4088432788848877\n",
      "Training loss for batch 5390 : 0.028125103563070297\n",
      "Training loss for batch 5391 : 0.45978376269340515\n",
      "Training loss for batch 5392 : 0.5301687717437744\n",
      "Training loss for batch 5393 : 0.25814592838287354\n",
      "Training loss for batch 5394 : 0.2523829936981201\n",
      "Training loss for batch 5395 : 0.31810101866722107\n",
      "Training loss for batch 5396 : 0.2585170269012451\n",
      "Training loss for batch 5397 : 0.11144568026065826\n",
      "Training loss for batch 5398 : 0.1519172191619873\n",
      "Training loss for batch 5399 : 0.30456459522247314\n",
      "Training loss for batch 5400 : 0.40484684705734253\n",
      "Training loss for batch 5401 : 0.3212657570838928\n",
      "Training loss for batch 5402 : 0.5178731679916382\n",
      "Training loss for batch 5403 : 0.5493043661117554\n",
      "Training loss for batch 5404 : 0.1154169887304306\n",
      "Training loss for batch 5405 : 0.5851498246192932\n",
      "Training loss for batch 5406 : 0.09354639798402786\n",
      "Training loss for batch 5407 : 0.4937483072280884\n",
      "Training loss for batch 5408 : 0.22134457528591156\n",
      "Training loss for batch 5409 : 0.11183209717273712\n",
      "Training loss for batch 5410 : 0.2832319736480713\n",
      "Training loss for batch 5411 : 0.24300143122673035\n",
      "Training loss for batch 5412 : 0.40141594409942627\n",
      "Training loss for batch 5413 : 0.3279101848602295\n",
      "Training loss for batch 5414 : 0.27407926321029663\n",
      "Training loss for batch 5415 : 0.27433329820632935\n",
      "Training loss for batch 5416 : 0.1360238939523697\n",
      "Training loss for batch 5417 : 0.4610588848590851\n",
      "Training loss for batch 5418 : 0.24904681742191315\n",
      "Training loss for batch 5419 : 0.5458436608314514\n",
      "Training loss for batch 5420 : 0.33134323358535767\n",
      "Training loss for batch 5421 : 0.4896096885204315\n",
      "Training loss for batch 5422 : 0.3975062668323517\n",
      "Training loss for batch 5423 : 0.4237608313560486\n",
      "Training loss for batch 5424 : 0.26328808069229126\n",
      "Training loss for batch 5425 : 0.3040439784526825\n",
      "Training loss for batch 5426 : 0.7071391344070435\n",
      "Training loss for batch 5427 : 0.10843583196401596\n",
      "Training loss for batch 5428 : 0.2524483799934387\n",
      "Training loss for batch 5429 : 0.33578771352767944\n",
      "Training loss for batch 5430 : 0.6448765993118286\n",
      "Training loss for batch 5431 : 0.5383110642433167\n",
      "Training loss for batch 5432 : 0.19469907879829407\n",
      "Training loss for batch 5433 : 0.05290254205465317\n",
      "Training loss for batch 5434 : 0.7715824246406555\n",
      "Training loss for batch 5435 : 0.35178786516189575\n",
      "Training loss for batch 5436 : 0.42879530787467957\n",
      "Training loss for batch 5437 : 0.05369936674833298\n",
      "Training loss for batch 5438 : 0.29004037380218506\n",
      "Training loss for batch 5439 : 0.1724701225757599\n",
      "Training loss for batch 5440 : 0.3669792711734772\n",
      "Training loss for batch 5441 : 0.2560325860977173\n",
      "Training loss for batch 5442 : 0.07456684857606888\n",
      "Training loss for batch 5443 : 0.5730608105659485\n",
      "Training loss for batch 5444 : 0.34863245487213135\n",
      "Training loss for batch 5445 : 0.5749595165252686\n",
      "Training loss for batch 5446 : 0.4144795536994934\n",
      "Training loss for batch 5447 : 0.49330997467041016\n",
      "Training loss for batch 5448 : 0.18416132032871246\n",
      "Training loss for batch 5449 : 0.20590808987617493\n",
      "Training loss for batch 5450 : 0.11087364703416824\n",
      "Training loss for batch 5451 : 0.20755097270011902\n",
      "Training loss for batch 5452 : 0.37495720386505127\n",
      "Training loss for batch 5453 : 0.23797565698623657\n",
      "Training loss for batch 5454 : 0.14468471705913544\n",
      "Training loss for batch 5455 : 0.24329103529453278\n",
      "Training loss for batch 5456 : 0.4855963885784149\n",
      "Training loss for batch 5457 : 0.48612260818481445\n",
      "Training loss for batch 5458 : 0.15893560647964478\n",
      "Training loss for batch 5459 : 0.200781911611557\n",
      "Training loss for batch 5460 : 0.31842100620269775\n",
      "Training loss for batch 5461 : 0.2639520466327667\n",
      "Training loss for batch 5462 : 0.31679919362068176\n",
      "Training loss for batch 5463 : 0.3339526951313019\n",
      "Training loss for batch 5464 : 0.28467628359794617\n",
      "Training loss for batch 5465 : 0.4387670159339905\n",
      "Training loss for batch 5466 : 0.11482067406177521\n",
      "Training loss for batch 5467 : 0.21982398629188538\n",
      "Training loss for batch 5468 : 0.2847706377506256\n",
      "Training loss for batch 5469 : 0.45724087953567505\n",
      "Training loss for batch 5470 : 0.08133971691131592\n",
      "Training loss for batch 5471 : 0.17544911801815033\n",
      "Training loss for batch 5472 : 0.011961936950683594\n",
      "Training loss for batch 5473 : 0.07747786492109299\n",
      "Training loss for batch 5474 : 0.08187466859817505\n",
      "Training loss for batch 5475 : 0.16466286778450012\n",
      "Training loss for batch 5476 : 0.0773686096072197\n",
      "Training loss for batch 5477 : 0.010124096646904945\n",
      "Training loss for batch 5478 : 0.22495251893997192\n",
      "Training loss for batch 5479 : 0.30191048979759216\n",
      "Training loss for batch 5480 : 0.19580015540122986\n",
      "Training loss for batch 5481 : 0.024060267955064774\n",
      "Training loss for batch 5482 : 0.14785878360271454\n",
      "Training loss for batch 5483 : 0.0474570132791996\n",
      "Training loss for batch 5484 : 0.11184777319431305\n",
      "Training loss for batch 5485 : 0.16589929163455963\n",
      "Training loss for batch 5486 : 0.2289559543132782\n",
      "Training loss for batch 5487 : 0.16706515848636627\n",
      "Training loss for batch 5488 : 0.40606528520584106\n",
      "Training loss for batch 5489 : 0.0\n",
      "Training loss for batch 5490 : 0.58537757396698\n",
      "Training loss for batch 5491 : 0.17610885202884674\n",
      "Training loss for batch 5492 : 0.08988265693187714\n",
      "Training loss for batch 5493 : 0.7710434198379517\n",
      "Training loss for batch 5494 : 0.09283094853162766\n",
      "Training loss for batch 5495 : 0.37413489818573\n",
      "Training loss for batch 5496 : 0.07584551721811295\n",
      "Training loss for batch 5497 : 0.21874651312828064\n",
      "Training loss for batch 5498 : 0.20387697219848633\n",
      "Training loss for batch 5499 : 0.5789947509765625\n",
      "Training loss for batch 5500 : 0.5079277157783508\n",
      "Training loss for batch 5501 : 0.22777234017848969\n",
      "Training loss for batch 5502 : 0.3482520878314972\n",
      "Training loss for batch 5503 : 0.08375994861125946\n",
      "Training loss for batch 5504 : 0.3060304522514343\n",
      "Training loss for batch 5505 : 0.3017667233943939\n",
      "Training loss for batch 5506 : 0.12652859091758728\n",
      "Training loss for batch 5507 : 0.13163451850414276\n",
      "Training loss for batch 5508 : 0.5339260101318359\n",
      "Training loss for batch 5509 : 0.12529096007347107\n",
      "Training loss for batch 5510 : 0.28943684697151184\n",
      "Training loss for batch 5511 : 0.0919341966509819\n",
      "Training loss for batch 5512 : 0.343639999628067\n",
      "Training loss for batch 5513 : 0.5508443117141724\n",
      "Training loss for batch 5514 : 0.6892446875572205\n",
      "Training loss for batch 5515 : 0.311273455619812\n",
      "Training loss for batch 5516 : 0.44303780794143677\n",
      "Training loss for batch 5517 : 0.40762951970100403\n",
      "Training loss for batch 5518 : 0.039079658687114716\n",
      "Training loss for batch 5519 : 0.03291812166571617\n",
      "Training loss for batch 5520 : 0.033783040940761566\n",
      "Training loss for batch 5521 : 0.26197072863578796\n",
      "Training loss for batch 5522 : 0.17875824868679047\n",
      "Training loss for batch 5523 : 0.057261332869529724\n",
      "Training loss for batch 5524 : 0.027646243572235107\n",
      "Training loss for batch 5525 : 0.17388318479061127\n",
      "Training loss for batch 5526 : 0.4066163897514343\n",
      "Training loss for batch 5527 : 0.3839477300643921\n",
      "Training loss for batch 5528 : 0.5777644515037537\n",
      "Training loss for batch 5529 : 0.18948981165885925\n",
      "Training loss for batch 5530 : 0.23114250600337982\n",
      "Training loss for batch 5531 : 0.18244755268096924\n",
      "Training loss for batch 5532 : 0.008205460384488106\n",
      "Training loss for batch 5533 : 0.1016768142580986\n",
      "Training loss for batch 5534 : 0.4393877685070038\n",
      "Training loss for batch 5535 : 0.5559751391410828\n",
      "Training loss for batch 5536 : 0.09948425740003586\n",
      "Training loss for batch 5537 : 0.09716537594795227\n",
      "Training loss for batch 5538 : 0.3605300188064575\n",
      "Training loss for batch 5539 : 0.076964370906353\n",
      "Training loss for batch 5540 : 0.06913923472166061\n",
      "Training loss for batch 5541 : 0.14946801960468292\n",
      "Training loss for batch 5542 : 0.08043426275253296\n",
      "Training loss for batch 5543 : 0.03900395333766937\n",
      "Training loss for batch 5544 : 0.20111168920993805\n",
      "Training loss for batch 5545 : 0.021703550592064857\n",
      "Training loss for batch 5546 : 0.02239714190363884\n",
      "Training loss for batch 5547 : 0.4960137605667114\n",
      "Training loss for batch 5548 : 0.1523577868938446\n",
      "Training loss for batch 5549 : 0.22286148369312286\n",
      "Training loss for batch 5550 : 0.049557991325855255\n",
      "Training loss for batch 5551 : 0.05104810371994972\n",
      "Training loss for batch 5552 : 0.3763462007045746\n",
      "Training loss for batch 5553 : 0.25945109128952026\n",
      "Training loss for batch 5554 : 0.5202385187149048\n",
      "Training loss for batch 5555 : 0.9320069551467896\n",
      "Training loss for batch 5556 : 0.21876868605613708\n",
      "Training loss for batch 5557 : 0.15622763335704803\n",
      "Training loss for batch 5558 : 0.15930208563804626\n",
      "Training loss for batch 5559 : 0.522600531578064\n",
      "Training loss for batch 5560 : 0.556768536567688\n",
      "Training loss for batch 5561 : 0.9325293898582458\n",
      "Training loss for batch 5562 : 0.0\n",
      "Training loss for batch 5563 : 0.011826574802398682\n",
      "Training loss for batch 5564 : 0.4003482758998871\n",
      "Training loss for batch 5565 : 0.2858075499534607\n",
      "Training loss for batch 5566 : 0.597721517086029\n",
      "Training loss for batch 5567 : 0.2487492859363556\n",
      "Training loss for batch 5568 : 0.5845891833305359\n",
      "Training loss for batch 5569 : 0.07869993895292282\n",
      "Training loss for batch 5570 : 0.30284667015075684\n",
      "Training loss for batch 5571 : 0.3781934976577759\n",
      "Training loss for batch 5572 : 0.1192089393734932\n",
      "Training loss for batch 5573 : 0.550912618637085\n",
      "Training loss for batch 5574 : 0.4868055582046509\n",
      "Training loss for batch 5575 : 0.20897206664085388\n",
      "Training loss for batch 5576 : 0.24005086719989777\n",
      "Training loss for batch 5577 : 0.055632274597883224\n",
      "Training loss for batch 5578 : 0.12461436539888382\n",
      "Training loss for batch 5579 : 0.2069936841726303\n",
      "Training loss for batch 5580 : 0.0960557609796524\n",
      "Training loss for batch 5581 : 0.4141521751880646\n",
      "Training loss for batch 5582 : 0.03111242689192295\n",
      "Training loss for batch 5583 : 0.09714526683092117\n",
      "Training loss for batch 5584 : 0.11337646096944809\n",
      "Training loss for batch 5585 : 0.6599445939064026\n",
      "Training loss for batch 5586 : 0.44138315320014954\n",
      "Training loss for batch 5587 : 0.2968142628669739\n",
      "Training loss for batch 5588 : 0.11215357482433319\n",
      "Training loss for batch 5589 : 0.344974547624588\n",
      "Training loss for batch 5590 : 0.2821443974971771\n",
      "Training loss for batch 5591 : 0.10526786744594574\n",
      "Training loss for batch 5592 : 0.23625102639198303\n",
      "Training loss for batch 5593 : 0.04033824801445007\n",
      "Training loss for batch 5594 : 0.11427643895149231\n",
      "Training loss for batch 5595 : 0.0013813178520649672\n",
      "Training loss for batch 5596 : 0.0\n",
      "Training loss for batch 5597 : 0.39097103476524353\n",
      "Training loss for batch 5598 : 0.40438312292099\n",
      "Training loss for batch 5599 : 0.4620286226272583\n",
      "Training loss for batch 5600 : 0.019789019599556923\n",
      "Training loss for batch 5601 : 0.44576430320739746\n",
      "Training loss for batch 5602 : 0.2609088718891144\n",
      "Training loss for batch 5603 : 0.6148158311843872\n",
      "Training loss for batch 5604 : 0.16215598583221436\n",
      "Training loss for batch 5605 : 0.429305762052536\n",
      "Training loss for batch 5606 : 0.04196770116686821\n",
      "Training loss for batch 5607 : 0.554490327835083\n",
      "Training loss for batch 5608 : 0.10480215400457382\n",
      "Training loss for batch 5609 : 0.6145199537277222\n",
      "Training loss for batch 5610 : 0.3848719894886017\n",
      "Training loss for batch 5611 : 0.39097142219543457\n",
      "Training loss for batch 5612 : 0.18146486580371857\n",
      "Training loss for batch 5613 : 0.23938292264938354\n",
      "Training loss for batch 5614 : 0.22405892610549927\n",
      "Training loss for batch 5615 : 0.1659666895866394\n",
      "Training loss for batch 5616 : 0.1566820591688156\n",
      "Training loss for batch 5617 : 0.11252152174711227\n",
      "Training loss for batch 5618 : 0.4075451195240021\n",
      "Training loss for batch 5619 : 0.3335680067539215\n",
      "Training loss for batch 5620 : 0.419939786195755\n",
      "Training loss for batch 5621 : 0.7494487762451172\n",
      "Training loss for batch 5622 : 0.3418258726596832\n",
      "Training loss for batch 5623 : 0.14836227893829346\n",
      "Training loss for batch 5624 : 0.5583298206329346\n",
      "Training loss for batch 5625 : 0.352556437253952\n",
      "Training loss for batch 5626 : 0.02835117280483246\n",
      "Training loss for batch 5627 : 0.6100764274597168\n",
      "Training loss for batch 5628 : 0.04693811014294624\n",
      "Training loss for batch 5629 : 0.5344423651695251\n",
      "Training loss for batch 5630 : 0.3488280773162842\n",
      "Training loss for batch 5631 : 0.25399473309516907\n",
      "Training loss for batch 5632 : 0.1830494999885559\n",
      "Training loss for batch 5633 : 0.2045673280954361\n",
      "Training loss for batch 5634 : 0.6178120970726013\n",
      "Training loss for batch 5635 : 0.4362739324569702\n",
      "Training loss for batch 5636 : 0.6833080649375916\n",
      "Training loss for batch 5637 : 0.21246594190597534\n",
      "Training loss for batch 5638 : 0.21475571393966675\n",
      "Training loss for batch 5639 : 0.02760712243616581\n",
      "Training loss for batch 5640 : 0.1782771348953247\n",
      "Training loss for batch 5641 : 0.4074544310569763\n",
      "Training loss for batch 5642 : 0.012924537062644958\n",
      "Training loss for batch 5643 : 0.18579471111297607\n",
      "Training loss for batch 5644 : 0.14424417912960052\n",
      "Training loss for batch 5645 : 0.3335930109024048\n",
      "Training loss for batch 5646 : 0.3103080093860626\n",
      "Training loss for batch 5647 : 0.6002495288848877\n",
      "Training loss for batch 5648 : 0.08164200186729431\n",
      "Training loss for batch 5649 : 0.3045109212398529\n",
      "Training loss for batch 5650 : 0.1428605020046234\n",
      "Training loss for batch 5651 : 0.14067332446575165\n",
      "Training loss for batch 5652 : 0.2895555794239044\n",
      "Training loss for batch 5653 : 0.6503031849861145\n",
      "Training loss for batch 5654 : 0.20771066844463348\n",
      "Training loss for batch 5655 : 0.1438024789094925\n",
      "Training loss for batch 5656 : 0.11822342127561569\n",
      "Training loss for batch 5657 : 0.07308343797922134\n",
      "Training loss for batch 5658 : 0.4513378441333771\n",
      "Training loss for batch 5659 : 0.3543546497821808\n",
      "Training loss for batch 5660 : 0.39885854721069336\n",
      "Training loss for batch 5661 : 0.15586748719215393\n",
      "Training loss for batch 5662 : 0.051595352590084076\n",
      "Training loss for batch 5663 : 0.17354029417037964\n",
      "Training loss for batch 5664 : 0.1389826536178589\n",
      "Training loss for batch 5665 : 0.23637841641902924\n",
      "Training loss for batch 5666 : 0.1155303418636322\n",
      "Training loss for batch 5667 : 0.15558719635009766\n",
      "Training loss for batch 5668 : 0.4190571904182434\n",
      "Training loss for batch 5669 : 0.09420768916606903\n",
      "Training loss for batch 5670 : 0.023478619754314423\n",
      "Training loss for batch 5671 : 0.09988480806350708\n",
      "Training loss for batch 5672 : 0.5807180404663086\n",
      "Training loss for batch 5673 : 0.296385794878006\n",
      "Training loss for batch 5674 : 0.4547593891620636\n",
      "Training loss for batch 5675 : 0.03879376873373985\n",
      "Training loss for batch 5676 : 1.0022542476654053\n",
      "Training loss for batch 5677 : 0.33236801624298096\n",
      "Training loss for batch 5678 : 0.21225599944591522\n",
      "Training loss for batch 5679 : 0.40367454290390015\n",
      "Training loss for batch 5680 : 0.2833678424358368\n",
      "Training loss for batch 5681 : 0.2439800649881363\n",
      "Training loss for batch 5682 : 0.650543212890625\n",
      "Training loss for batch 5683 : 0.40202948451042175\n",
      "Training loss for batch 5684 : 0.5426398515701294\n",
      "Training loss for batch 5685 : 0.27398601174354553\n",
      "Training loss for batch 5686 : 1.1070209741592407\n",
      "Training loss for batch 5687 : 0.2046392560005188\n",
      "Training loss for batch 5688 : 0.10138832777738571\n",
      "Training loss for batch 5689 : 0.4350000023841858\n",
      "Training loss for batch 5690 : 0.18067730963230133\n",
      "Training loss for batch 5691 : 0.07330016791820526\n",
      "Training loss for batch 5692 : 0.5118913054466248\n",
      "Training loss for batch 5693 : 0.2821134924888611\n",
      "Training loss for batch 5694 : 0.06959843635559082\n",
      "Training loss for batch 5695 : 0.30977386236190796\n",
      "Training loss for batch 5696 : 0.2979358732700348\n",
      "Training loss for batch 5697 : 0.32287833094596863\n",
      "Training loss for batch 5698 : 0.7200379371643066\n",
      "Training loss for batch 5699 : 0.3031132221221924\n",
      "Training loss for batch 5700 : 0.45833075046539307\n",
      "Training loss for batch 5701 : 0.2378431260585785\n",
      "Training loss for batch 5702 : 0.2521546483039856\n",
      "Training loss for batch 5703 : 0.5041828155517578\n",
      "Training loss for batch 5704 : 0.26557910442352295\n",
      "Training loss for batch 5705 : 0.2112175077199936\n",
      "Training loss for batch 5706 : 1.0081931352615356\n",
      "Training loss for batch 5707 : 0.07687386870384216\n",
      "Training loss for batch 5708 : 0.2766859829425812\n",
      "Training loss for batch 5709 : 0.2376292496919632\n",
      "Training loss for batch 5710 : 0.3357062339782715\n",
      "Training loss for batch 5711 : 0.3082118332386017\n",
      "Training loss for batch 5712 : 0.47216126322746277\n",
      "Training loss for batch 5713 : 0.3153817355632782\n",
      "Training loss for batch 5714 : 0.3206481337547302\n",
      "Training loss for batch 5715 : 0.6672747731208801\n",
      "Training loss for batch 5716 : 0.055147841572761536\n",
      "Training loss for batch 5717 : 0.713100790977478\n",
      "Training loss for batch 5718 : 0.30266913771629333\n",
      "Training loss for batch 5719 : 0.4300408363342285\n",
      "Training loss for batch 5720 : 0.5980060696601868\n",
      "Training loss for batch 5721 : 0.3096503019332886\n",
      "Training loss for batch 5722 : 0.08149871975183487\n",
      "Training loss for batch 5723 : 0.2939471900463104\n",
      "Training loss for batch 5724 : 0.21735619008541107\n",
      "Training loss for batch 5725 : 0.2634037733078003\n",
      "Training loss for batch 5726 : 0.7068604230880737\n",
      "Training loss for batch 5727 : 0.3048703670501709\n",
      "Training loss for batch 5728 : 0.22026246786117554\n",
      "Training loss for batch 5729 : 0.08768162131309509\n",
      "Training loss for batch 5730 : 0.2770991325378418\n",
      "Training loss for batch 5731 : 0.21981024742126465\n",
      "Training loss for batch 5732 : 0.5101253986358643\n",
      "Training loss for batch 5733 : 0.15244120359420776\n",
      "Training loss for batch 5734 : 0.09884814918041229\n",
      "Training loss for batch 5735 : 0.2401876002550125\n",
      "Training loss for batch 5736 : 0.15974847972393036\n",
      "Training loss for batch 5737 : 0.19488626718521118\n",
      "Training loss for batch 5738 : 0.018699761480093002\n",
      "Training loss for batch 5739 : 0.4369460642337799\n",
      "Training loss for batch 5740 : 0.19698654115200043\n",
      "Training loss for batch 5741 : 0.020977383479475975\n",
      "Training loss for batch 5742 : 0.14892394840717316\n",
      "Training loss for batch 5743 : 0.4102955758571625\n",
      "Training loss for batch 5744 : 0.30404597520828247\n",
      "Training loss for batch 5745 : 0.35150280594825745\n",
      "Training loss for batch 5746 : 0.4464673697948456\n",
      "Training loss for batch 5747 : 0.15539228916168213\n",
      "Training loss for batch 5748 : 0.15646807849407196\n",
      "Training loss for batch 5749 : 0.5273962020874023\n",
      "Training loss for batch 5750 : 0.12605169415473938\n",
      "Training loss for batch 5751 : 0.07893365621566772\n",
      "Training loss for batch 5752 : 0.1349412053823471\n",
      "Training loss for batch 5753 : 0.013947291299700737\n",
      "Training loss for batch 5754 : 0.1361197680234909\n",
      "Training loss for batch 5755 : 0.08053825795650482\n",
      "Training loss for batch 5756 : 0.19125200808048248\n",
      "Training loss for batch 5757 : 0.32053816318511963\n",
      "Training loss for batch 5758 : 0.3073180317878723\n",
      "Training loss for batch 5759 : 0.5364580750465393\n",
      "Training loss for batch 5760 : 0.34511426091194153\n",
      "Training loss for batch 5761 : 0.1289421021938324\n",
      "Training loss for batch 5762 : 0.9856528043746948\n",
      "Training loss for batch 5763 : 0.2739201784133911\n",
      "Training loss for batch 5764 : 0.5836521983146667\n",
      "Training loss for batch 5765 : 0.4854649305343628\n",
      "Training loss for batch 5766 : 0.4612646698951721\n",
      "Training loss for batch 5767 : 0.3569644093513489\n",
      "Training loss for batch 5768 : 0.23741714656352997\n",
      "Training loss for batch 5769 : 0.06745394319295883\n",
      "Training loss for batch 5770 : 0.47431668639183044\n",
      "Training loss for batch 5771 : 0.1444978415966034\n",
      "Training loss for batch 5772 : 0.35671496391296387\n",
      "Training loss for batch 5773 : 0.3926597535610199\n",
      "Training loss for batch 5774 : 0.5099591016769409\n",
      "Training loss for batch 5775 : 0.2472122609615326\n",
      "Training loss for batch 5776 : 0.5053579211235046\n",
      "Training loss for batch 5777 : 0.11358986794948578\n",
      "Training loss for batch 5778 : 0.31555643677711487\n",
      "Training loss for batch 5779 : 0.31178605556488037\n",
      "Training loss for batch 5780 : 0.07560085505247116\n",
      "Training loss for batch 5781 : 0.34965741634368896\n",
      "Training loss for batch 5782 : 0.3363679051399231\n",
      "Training loss for batch 5783 : 0.19303643703460693\n",
      "Training loss for batch 5784 : 0.3415638208389282\n",
      "Training loss for batch 5785 : 0.28311771154403687\n",
      "Training loss for batch 5786 : 0.23808524012565613\n",
      "Training loss for batch 5787 : 0.13070940971374512\n",
      "Training loss for batch 5788 : 0.3161114454269409\n",
      "Training loss for batch 5789 : 0.45898017287254333\n",
      "Training loss for batch 5790 : 0.20339913666248322\n",
      "Training loss for batch 5791 : 0.059581462293863297\n",
      "Training loss for batch 5792 : 0.18524383008480072\n",
      "Training loss for batch 5793 : 0.31216779351234436\n",
      "Training loss for batch 5794 : 0.14752580225467682\n",
      "Training loss for batch 5795 : 0.08023907989263535\n",
      "Training loss for batch 5796 : 0.16132813692092896\n",
      "Training loss for batch 5797 : 0.10420545935630798\n",
      "Training loss for batch 5798 : 0.4499087929725647\n",
      "Training loss for batch 5799 : 0.3506057858467102\n",
      "Training loss for batch 5800 : 0.6534715294837952\n",
      "Training loss for batch 5801 : 0.20422492921352386\n",
      "Training loss for batch 5802 : 0.3315138816833496\n",
      "Training loss for batch 5803 : 0.19165928661823273\n",
      "Training loss for batch 5804 : 0.29560062289237976\n",
      "Training loss for batch 5805 : 0.2877529263496399\n",
      "Training loss for batch 5806 : 0.37858131527900696\n",
      "Training loss for batch 5807 : 0.22160924971103668\n",
      "Training loss for batch 5808 : 0.28077542781829834\n",
      "Training loss for batch 5809 : 0.2837858200073242\n",
      "Training loss for batch 5810 : 0.19817905128002167\n",
      "Training loss for batch 5811 : 0.1881735473871231\n",
      "Training loss for batch 5812 : 0.3480605483055115\n",
      "Training loss for batch 5813 : 0.08923718333244324\n",
      "Training loss for batch 5814 : 0.39209887385368347\n",
      "Training loss for batch 5815 : 0.41435620188713074\n",
      "Training loss for batch 5816 : 0.24466818571090698\n",
      "Training loss for batch 5817 : 0.3097478747367859\n",
      "Training loss for batch 5818 : 0.12859943509101868\n",
      "Training loss for batch 5819 : 0.13958808779716492\n",
      "Training loss for batch 5820 : 0.5652398467063904\n",
      "Training loss for batch 5821 : 0.17463743686676025\n",
      "Training loss for batch 5822 : 0.20773963630199432\n",
      "Training loss for batch 5823 : 0.2260875552892685\n",
      "Training loss for batch 5824 : 0.49522286653518677\n",
      "Training loss for batch 5825 : 0.2652604877948761\n",
      "Training loss for batch 5826 : 0.055050499737262726\n",
      "Training loss for batch 5827 : 0.3067488372325897\n",
      "Training loss for batch 5828 : 0.7365733981132507\n",
      "Training loss for batch 5829 : 0.43990910053253174\n",
      "Training loss for batch 5830 : 0.34827423095703125\n",
      "Training loss for batch 5831 : 0.18521535396575928\n",
      "Training loss for batch 5832 : 0.5451470017433167\n",
      "Training loss for batch 5833 : 0.0\n",
      "Training loss for batch 5834 : 0.2625778615474701\n",
      "Training loss for batch 5835 : 0.27609744668006897\n",
      "Training loss for batch 5836 : 0.19682207703590393\n",
      "Training loss for batch 5837 : 0.05047569051384926\n",
      "Training loss for batch 5838 : 0.03684346005320549\n",
      "Training loss for batch 5839 : 0.2686642110347748\n",
      "Training loss for batch 5840 : 0.2233210653066635\n",
      "Training loss for batch 5841 : 0.4917941689491272\n",
      "Training loss for batch 5842 : 0.0858636274933815\n",
      "Training loss for batch 5843 : 0.2760756313800812\n",
      "Training loss for batch 5844 : 0.09468424320220947\n",
      "Training loss for batch 5845 : 0.05507035180926323\n",
      "Training loss for batch 5846 : 0.4376654028892517\n",
      "Training loss for batch 5847 : 0.20385490357875824\n",
      "Training loss for batch 5848 : 0.14373701810836792\n",
      "Training loss for batch 5849 : 0.6006523966789246\n",
      "Training loss for batch 5850 : 0.0\n",
      "Training loss for batch 5851 : 0.10678021609783173\n",
      "Training loss for batch 5852 : 0.16725671291351318\n",
      "Training loss for batch 5853 : 0.5476930737495422\n",
      "Training loss for batch 5854 : 0.28673309087753296\n",
      "Training loss for batch 5855 : 0.47630155086517334\n",
      "Training loss for batch 5856 : 0.09432440251111984\n",
      "Training loss for batch 5857 : 0.3392980992794037\n",
      "Training loss for batch 5858 : 0.285724401473999\n",
      "Training loss for batch 5859 : 0.05553923547267914\n",
      "Training loss for batch 5860 : 0.39308086037635803\n",
      "Training loss for batch 5861 : 0.05846377834677696\n",
      "Training loss for batch 5862 : 0.21747945249080658\n",
      "Training loss for batch 5863 : 0.018507955595850945\n",
      "Training loss for batch 5864 : 0.5689327716827393\n",
      "Training loss for batch 5865 : 0.2555448114871979\n",
      "Training loss for batch 5866 : 0.1049092710018158\n",
      "Training loss for batch 5867 : 0.18652915954589844\n",
      "Training loss for batch 5868 : 0.5124797224998474\n",
      "Training loss for batch 5869 : 0.09426754713058472\n",
      "Training loss for batch 5870 : 0.41343438625335693\n",
      "Training loss for batch 5871 : 0.31232741475105286\n",
      "Training loss for batch 5872 : 0.35984697937965393\n",
      "Training loss for batch 5873 : 0.31186443567276\n",
      "Training loss for batch 5874 : 0.022120393812656403\n",
      "Training loss for batch 5875 : 0.1850251406431198\n",
      "Training loss for batch 5876 : 0.025433393195271492\n",
      "Training loss for batch 5877 : 0.17039883136749268\n",
      "Training loss for batch 5878 : 0.5072261691093445\n",
      "Training loss for batch 5879 : 0.14123286306858063\n",
      "Training loss for batch 5880 : 0.16834822297096252\n",
      "Training loss for batch 5881 : 0.15168918669223785\n",
      "Training loss for batch 5882 : 0.11108475178480148\n",
      "Training loss for batch 5883 : 0.41457968950271606\n",
      "Training loss for batch 5884 : 0.42686745524406433\n",
      "Training loss for batch 5885 : 0.012385591864585876\n",
      "Training loss for batch 5886 : 0.08891650289297104\n",
      "Training loss for batch 5887 : 0.22785809636116028\n",
      "Training loss for batch 5888 : 0.4358466565608978\n",
      "Training loss for batch 5889 : 0.15549373626708984\n",
      "Training loss for batch 5890 : 0.2286492884159088\n",
      "Training loss for batch 5891 : 0.06901780515909195\n",
      "Training loss for batch 5892 : 0.04459681361913681\n",
      "Training loss for batch 5893 : 0.06957151740789413\n",
      "Training loss for batch 5894 : 0.431459903717041\n",
      "Training loss for batch 5895 : 0.08945094048976898\n",
      "Training loss for batch 5896 : 0.5586407780647278\n",
      "Training loss for batch 5897 : 0.2969266176223755\n",
      "Training loss for batch 5898 : 0.4040168821811676\n",
      "Training loss for batch 5899 : 0.7215759754180908\n",
      "Training loss for batch 5900 : 0.16148217022418976\n",
      "Training loss for batch 5901 : 0.2792929410934448\n",
      "Training loss for batch 5902 : 0.3851870596408844\n",
      "Training loss for batch 5903 : 0.08236739039421082\n",
      "Training loss for batch 5904 : 0.00020175767713226378\n",
      "Training loss for batch 5905 : 0.16102299094200134\n",
      "Training loss for batch 5906 : 0.22131167352199554\n",
      "Training loss for batch 5907 : 0.7483269572257996\n",
      "Training loss for batch 5908 : 0.30728113651275635\n",
      "Training loss for batch 5909 : 1.0308983325958252\n",
      "Training loss for batch 5910 : 0.29248881340026855\n",
      "Training loss for batch 5911 : 0.05077807977795601\n",
      "Training loss for batch 5912 : 0.09974551945924759\n",
      "Training loss for batch 5913 : 0.1202792152762413\n",
      "Training loss for batch 5914 : 0.4102155566215515\n",
      "Training loss for batch 5915 : 0.0627942606806755\n",
      "Training loss for batch 5916 : 0.3055088520050049\n",
      "Training loss for batch 5917 : 0.42181503772735596\n",
      "Training loss for batch 5918 : 0.29883551597595215\n",
      "Training loss for batch 5919 : 0.8056604266166687\n",
      "Training loss for batch 5920 : 0.6470486521720886\n",
      "Training loss for batch 5921 : 0.059575337916612625\n",
      "Training loss for batch 5922 : 0.08344636857509613\n",
      "Training loss for batch 5923 : 0.17406317591667175\n",
      "Training loss for batch 5924 : 0.4643523097038269\n",
      "Training loss for batch 5925 : 0.2691352963447571\n",
      "Training loss for batch 5926 : 0.2535350024700165\n",
      "Training loss for batch 5927 : 0.3717922866344452\n",
      "Training loss for batch 5928 : 0.2549135088920593\n",
      "Training loss for batch 5929 : 0.5232346057891846\n",
      "Training loss for batch 5930 : 0.3083972632884979\n",
      "Training loss for batch 5931 : 0.1678469479084015\n",
      "Training loss for batch 5932 : 0.3902107775211334\n",
      "Training loss for batch 5933 : 0.198272705078125\n",
      "Training loss for batch 5934 : 0.6161904335021973\n",
      "Training loss for batch 5935 : 0.2186085730791092\n",
      "Training loss for batch 5936 : 0.001845280872657895\n",
      "Training loss for batch 5937 : 0.2437209039926529\n",
      "Training loss for batch 5938 : 0.2125236988067627\n",
      "Training loss for batch 5939 : 0.6376615166664124\n",
      "Training loss for batch 5940 : 0.08347722887992859\n",
      "Training loss for batch 5941 : 0.27795305848121643\n",
      "Training loss for batch 5942 : 0.3521115481853485\n",
      "Training loss for batch 5943 : 0.3979179263114929\n",
      "Training loss for batch 5944 : 0.13208550214767456\n",
      "Training loss for batch 5945 : 0.09062781184911728\n",
      "Training loss for batch 5946 : 0.5007692575454712\n",
      "Training loss for batch 5947 : 0.46249938011169434\n",
      "Training loss for batch 5948 : 0.27980557084083557\n",
      "Training loss for batch 5949 : 0.2951756715774536\n",
      "Training loss for batch 5950 : 0.3278346061706543\n",
      "Training loss for batch 5951 : 0.15820525586605072\n",
      "Training loss for batch 5952 : 0.1283617466688156\n",
      "Training loss for batch 5953 : 0.5746278762817383\n",
      "Training loss for batch 5954 : 0.7522228956222534\n",
      "Training loss for batch 5955 : 0.27452540397644043\n",
      "Training loss for batch 5956 : 0.24469904601573944\n",
      "Training loss for batch 5957 : 0.17455871403217316\n",
      "Training loss for batch 5958 : 0.015214185230433941\n",
      "Training loss for batch 5959 : 0.0\n",
      "Training loss for batch 5960 : 0.14491328597068787\n",
      "Training loss for batch 5961 : 0.08471783250570297\n",
      "Training loss for batch 5962 : 0.08447781205177307\n",
      "Training loss for batch 5963 : 0.5101590752601624\n",
      "Training loss for batch 5964 : 0.23856742680072784\n",
      "Training loss for batch 5965 : 0.037783149629831314\n",
      "Training loss for batch 5966 : 0.06284575164318085\n",
      "Training loss for batch 5967 : 0.050021201372146606\n",
      "Training loss for batch 5968 : 0.4279903471469879\n",
      "Training loss for batch 5969 : 0.10526741296052933\n",
      "Training loss for batch 5970 : 0.3771745562553406\n",
      "Training loss for batch 5971 : 0.2410801351070404\n",
      "Training loss for batch 5972 : 0.03102290816605091\n",
      "Training loss for batch 5973 : 0.319235235452652\n",
      "Training loss for batch 5974 : 0.12633256614208221\n",
      "Training loss for batch 5975 : 0.29821377992630005\n",
      "Training loss for batch 5976 : 0.5193281769752502\n",
      "Training loss for batch 5977 : 0.029558580368757248\n",
      "Training loss for batch 5978 : 0.2140987068414688\n",
      "Training loss for batch 5979 : 0.33818086981773376\n",
      "Training loss for batch 5980 : 0.2558330297470093\n",
      "Training loss for batch 5981 : 0.4728029668331146\n",
      "Training loss for batch 5982 : 0.11371354013681412\n",
      "Training loss for batch 5983 : 0.5059570074081421\n",
      "Training loss for batch 5984 : 0.45629027485847473\n",
      "Training loss for batch 5985 : 0.11358760297298431\n",
      "Training loss for batch 5986 : 0.16100271046161652\n",
      "Training loss for batch 5987 : 0.6304643750190735\n",
      "Training loss for batch 5988 : 0.26655399799346924\n",
      "Training loss for batch 5989 : 0.4467315971851349\n",
      "Training loss for batch 5990 : 0.007954501546919346\n",
      "Training loss for batch 5991 : 0.7700942158699036\n",
      "Training loss for batch 5992 : 0.46831202507019043\n",
      "Training loss for batch 5993 : 0.4456790089607239\n",
      "Training loss for batch 5994 : 0.9970554709434509\n",
      "Training loss for batch 5995 : 0.11225095391273499\n",
      "Training loss for batch 5996 : 0.044353097677230835\n",
      "Training loss for batch 5997 : 0.2766321301460266\n",
      "Training loss for batch 5998 : 0.38514137268066406\n",
      "Training loss for batch 5999 : 0.2524378299713135\n",
      "Training loss for batch 6000 : 0.4223802387714386\n",
      "Training loss for batch 6001 : 0.17375902831554413\n",
      "Training loss for batch 6002 : 0.6031711101531982\n",
      "Training loss for batch 6003 : 0.13898102939128876\n",
      "Training loss for batch 6004 : 0.3865729570388794\n",
      "Training loss for batch 6005 : 0.51100754737854\n",
      "Training loss for batch 6006 : 0.30983999371528625\n",
      "Training loss for batch 6007 : 0.10243268311023712\n",
      "Training loss for batch 6008 : 0.6391125321388245\n",
      "Training loss for batch 6009 : 0.3387341797351837\n",
      "Training loss for batch 6010 : 0.21187277138233185\n",
      "Training loss for batch 6011 : 0.015129623003304005\n",
      "Training loss for batch 6012 : 0.2009224146604538\n",
      "Training loss for batch 6013 : 0.02007742039859295\n",
      "Training loss for batch 6014 : 0.08812493085861206\n",
      "Training loss for batch 6015 : 0.4166834056377411\n",
      "Training loss for batch 6016 : 0.592275083065033\n",
      "Training loss for batch 6017 : 0.29627272486686707\n",
      "Training loss for batch 6018 : 0.23085825145244598\n",
      "Training loss for batch 6019 : 0.06376773864030838\n",
      "Training loss for batch 6020 : 0.16347776353359222\n",
      "Training loss for batch 6021 : 0.27543124556541443\n",
      "Training loss for batch 6022 : 0.18733790516853333\n",
      "Training loss for batch 6023 : 0.5721263885498047\n",
      "Training loss for batch 6024 : 0.18871751427650452\n",
      "Training loss for batch 6025 : 0.06357773393392563\n",
      "Training loss for batch 6026 : 0.10676918923854828\n",
      "Training loss for batch 6027 : 0.3730369210243225\n",
      "Training loss for batch 6028 : 0.2632455825805664\n",
      "Training loss for batch 6029 : 0.08250505477190018\n",
      "Training loss for batch 6030 : 0.3201602101325989\n",
      "Training loss for batch 6031 : 0.005543490871787071\n",
      "Training loss for batch 6032 : 0.1518336534500122\n",
      "Training loss for batch 6033 : 0.1014971137046814\n",
      "Training loss for batch 6034 : 0.3185245394706726\n",
      "Training loss for batch 6035 : 0.38113945722579956\n",
      "Training loss for batch 6036 : 0.395214319229126\n",
      "Training loss for batch 6037 : 0.10439261794090271\n",
      "Training loss for batch 6038 : 0.22237396240234375\n",
      "Training loss for batch 6039 : 0.5031924247741699\n",
      "Training loss for batch 6040 : 0.2834866940975189\n",
      "Training loss for batch 6041 : 0.6115666031837463\n",
      "Training loss for batch 6042 : 0.353376567363739\n",
      "Training loss for batch 6043 : 0.08072923868894577\n",
      "Training loss for batch 6044 : 0.3338991403579712\n",
      "Training loss for batch 6045 : 0.37345802783966064\n",
      "Training loss for batch 6046 : 0.3374945819377899\n",
      "Training loss for batch 6047 : 0.13927024602890015\n",
      "Training loss for batch 6048 : 0.7055663466453552\n",
      "Training loss for batch 6049 : 0.15961378812789917\n",
      "Training loss for batch 6050 : 0.3373015224933624\n",
      "Training loss for batch 6051 : 0.35798636078834534\n",
      "Training loss for batch 6052 : 0.47152385115623474\n",
      "Training loss for batch 6053 : 0.5213358998298645\n",
      "Training loss for batch 6054 : 0.30489394068717957\n",
      "Training loss for batch 6055 : 0.7537955045700073\n",
      "Training loss for batch 6056 : 0.11343823373317719\n",
      "Training loss for batch 6057 : 0.12039085477590561\n",
      "Training loss for batch 6058 : 0.36490580439567566\n",
      "Training loss for batch 6059 : 0.07286972552537918\n",
      "Training loss for batch 6060 : 0.2214859575033188\n",
      "Training loss for batch 6061 : 0.36096832156181335\n",
      "Training loss for batch 6062 : 0.13743796944618225\n",
      "Training loss for batch 6063 : 0.5107972621917725\n",
      "Training loss for batch 6064 : 0.2950375974178314\n",
      "Training loss for batch 6065 : 0.08914731442928314\n",
      "Training loss for batch 6066 : 0.21041962504386902\n",
      "Training loss for batch 6067 : 0.4598348140716553\n",
      "Training loss for batch 6068 : 0.47192972898483276\n",
      "Training loss for batch 6069 : 0.16618989408016205\n",
      "Training loss for batch 6070 : 0.14138613641262054\n",
      "Training loss for batch 6071 : 0.403964102268219\n",
      "Training loss for batch 6072 : 0.2782125473022461\n",
      "Training loss for batch 6073 : 0.12057510763406754\n",
      "Training loss for batch 6074 : 0.2587498426437378\n",
      "Training loss for batch 6075 : 0.24250444769859314\n",
      "Training loss for batch 6076 : 0.409563809633255\n",
      "Training loss for batch 6077 : 0.38630205392837524\n",
      "Training loss for batch 6078 : 0.6839797496795654\n",
      "Training loss for batch 6079 : 0.30629563331604004\n",
      "Training loss for batch 6080 : 0.4124443829059601\n",
      "Training loss for batch 6081 : 0.41569775342941284\n",
      "Training loss for batch 6082 : 0.6272158622741699\n",
      "Training loss for batch 6083 : 0.181996688246727\n",
      "Training loss for batch 6084 : 0.43811485171318054\n",
      "Training loss for batch 6085 : 0.6973645687103271\n",
      "Training loss for batch 6086 : 0.015478599816560745\n",
      "Training loss for batch 6087 : 0.5691896080970764\n",
      "Training loss for batch 6088 : 0.30579400062561035\n",
      "Training loss for batch 6089 : 0.2534797191619873\n",
      "Training loss for batch 6090 : 0.27121540904045105\n",
      "Training loss for batch 6091 : 0.5147235989570618\n",
      "Training loss for batch 6092 : 0.14411525428295135\n",
      "Training loss for batch 6093 : 0.45415109395980835\n",
      "Training loss for batch 6094 : 0.13147211074829102\n",
      "Training loss for batch 6095 : 0.31310755014419556\n",
      "Training loss for batch 6096 : 0.2694149315357208\n",
      "Training loss for batch 6097 : 0.1144719049334526\n",
      "Training loss for batch 6098 : 0.10390879213809967\n",
      "Training loss for batch 6099 : 0.08955967426300049\n",
      "Training loss for batch 6100 : 0.22814534604549408\n",
      "Training loss for batch 6101 : 0.2825509011745453\n",
      "Training loss for batch 6102 : 0.2626677453517914\n",
      "Training loss for batch 6103 : 0.6060807704925537\n",
      "Training loss for batch 6104 : 0.7128898501396179\n",
      "Training loss for batch 6105 : 0.35870450735092163\n",
      "Training loss for batch 6106 : 0.14442960917949677\n",
      "Training loss for batch 6107 : 0.24310484528541565\n",
      "Training loss for batch 6108 : 0.2909173369407654\n",
      "Training loss for batch 6109 : 0.3983630836009979\n",
      "Training loss for batch 6110 : 0.10048773884773254\n",
      "Training loss for batch 6111 : 0.2675462067127228\n",
      "Training loss for batch 6112 : 0.08530660718679428\n",
      "Training loss for batch 6113 : 0.3855896294116974\n",
      "Training loss for batch 6114 : 0.07305347174406052\n",
      "Training loss for batch 6115 : 0.3447206914424896\n",
      "Training loss for batch 6116 : 0.08537538349628448\n",
      "Training loss for batch 6117 : 0.3051823377609253\n",
      "Training loss for batch 6118 : 0.14140284061431885\n",
      "Training loss for batch 6119 : 0.24244838953018188\n",
      "Training loss for batch 6120 : 0.13785989582538605\n",
      "Training loss for batch 6121 : 0.22300028800964355\n",
      "Training loss for batch 6122 : 0.23042480647563934\n",
      "Training loss for batch 6123 : 0.1548720896244049\n",
      "Training loss for batch 6124 : 0.17658013105392456\n",
      "Training loss for batch 6125 : 0.358486533164978\n",
      "Training loss for batch 6126 : 0.26155897974967957\n",
      "Training loss for batch 6127 : 0.14289313554763794\n",
      "Training loss for batch 6128 : 0.31683114171028137\n",
      "Training loss for batch 6129 : 0.23573431372642517\n",
      "Training loss for batch 6130 : 0.32490798830986023\n",
      "Training loss for batch 6131 : 0.11619846522808075\n",
      "Training loss for batch 6132 : 0.06044788286089897\n",
      "Training loss for batch 6133 : 0.17542549967765808\n",
      "Training loss for batch 6134 : 0.21787674725055695\n",
      "Training loss for batch 6135 : 0.5081921219825745\n",
      "Training loss for batch 6136 : 0.3755275011062622\n",
      "Training loss for batch 6137 : 0.15723448991775513\n",
      "Training loss for batch 6138 : 0.3505808711051941\n",
      "Training loss for batch 6139 : 0.5536425709724426\n",
      "Training loss for batch 6140 : 0.4414995610713959\n",
      "Training loss for batch 6141 : 0.23064152896404266\n",
      "Training loss for batch 6142 : 0.5694335699081421\n",
      "Training loss for batch 6143 : 0.23355276882648468\n",
      "Training loss for batch 6144 : 0.2741076648235321\n",
      "Training loss for batch 6145 : 0.44297391176223755\n",
      "Training loss for batch 6146 : 0.15160244703292847\n",
      "Training loss for batch 6147 : 0.17481234669685364\n",
      "Training loss for batch 6148 : 0.1575399935245514\n",
      "Training loss for batch 6149 : 0.4424755275249481\n",
      "Training loss for batch 6150 : 0.18302200734615326\n",
      "Training loss for batch 6151 : 0.18505941331386566\n",
      "Training loss for batch 6152 : 0.4775988757610321\n",
      "Training loss for batch 6153 : 0.06152907758951187\n",
      "Training loss for batch 6154 : 0.25110867619514465\n",
      "Training loss for batch 6155 : 0.44981247186660767\n",
      "Training loss for batch 6156 : 0.42446279525756836\n",
      "Training loss for batch 6157 : 0.15883097052574158\n",
      "Training loss for batch 6158 : 0.15694722533226013\n",
      "Training loss for batch 6159 : 0.08964698016643524\n",
      "Training loss for batch 6160 : 0.1539778858423233\n",
      "Training loss for batch 6161 : 0.07181600481271744\n",
      "Training loss for batch 6162 : 0.13529044389724731\n",
      "Training loss for batch 6163 : 0.32694876194000244\n",
      "Training loss for batch 6164 : 0.014103776775300503\n",
      "Training loss for batch 6165 : 0.4450565278530121\n",
      "Training loss for batch 6166 : 0.48216167092323303\n",
      "Training loss for batch 6167 : 0.7070986032485962\n",
      "Training loss for batch 6168 : 0.19284410774707794\n",
      "Training loss for batch 6169 : 0.7117525935173035\n",
      "Training loss for batch 6170 : 0.5052411556243896\n",
      "Training loss for batch 6171 : 0.10902777314186096\n",
      "Training loss for batch 6172 : 0.4124700427055359\n",
      "Training loss for batch 6173 : 0.19636286795139313\n",
      "Training loss for batch 6174 : 0.12107984721660614\n",
      "Training loss for batch 6175 : 0.6583456993103027\n",
      "Training loss for batch 6176 : 0.8156427145004272\n",
      "Training loss for batch 6177 : 0.2830868065357208\n",
      "Training loss for batch 6178 : 0.0\n",
      "Training loss for batch 6179 : 0.638420820236206\n",
      "Training loss for batch 6180 : 0.426783949136734\n",
      "Training loss for batch 6181 : 0.548726499080658\n",
      "Training loss for batch 6182 : 0.4380445182323456\n",
      "Training loss for batch 6183 : 0.09843796491622925\n",
      "Training loss for batch 6184 : 0.4295090138912201\n",
      "Training loss for batch 6185 : 0.5021980404853821\n",
      "Training loss for batch 6186 : 0.4210130572319031\n",
      "Training loss for batch 6187 : 0.3816309869289398\n",
      "Training loss for batch 6188 : 0.31937283277511597\n",
      "Training loss for batch 6189 : 0.1288420408964157\n",
      "Training loss for batch 6190 : 0.6242905855178833\n",
      "Training loss for batch 6191 : 0.5031167268753052\n",
      "Training loss for batch 6192 : 0.5804815888404846\n",
      "Training loss for batch 6193 : 0.03464245796203613\n",
      "Training loss for batch 6194 : 0.3491421639919281\n",
      "Training loss for batch 6195 : 0.06951431185007095\n",
      "Training loss for batch 6196 : 0.05918218567967415\n",
      "Training loss for batch 6197 : 0.4116629362106323\n",
      "Training loss for batch 6198 : 0.3435662090778351\n",
      "Training loss for batch 6199 : 0.452339768409729\n",
      "Training loss for batch 6200 : 0.22583122551441193\n",
      "Training loss for batch 6201 : 0.2601717412471771\n",
      "Training loss for batch 6202 : 0.25241661071777344\n",
      "Training loss for batch 6203 : 0.23204369843006134\n",
      "Training loss for batch 6204 : 0.6471508145332336\n",
      "Training loss for batch 6205 : 0.30000191926956177\n",
      "Training loss for batch 6206 : 0.2786227762699127\n",
      "Training loss for batch 6207 : 0.42240455746650696\n",
      "Training loss for batch 6208 : 0.07726754248142242\n",
      "Training loss for batch 6209 : 0.7921427488327026\n",
      "Training loss for batch 6210 : 0.19332201778888702\n",
      "Training loss for batch 6211 : 0.1542045772075653\n",
      "Training loss for batch 6212 : 0.23066820204257965\n",
      "Training loss for batch 6213 : 0.5225168466567993\n",
      "Training loss for batch 6214 : 0.15210437774658203\n",
      "Training loss for batch 6215 : 0.05166810750961304\n",
      "Training loss for batch 6216 : 0.10334023088216782\n",
      "Training loss for batch 6217 : 0.05329049751162529\n",
      "Training loss for batch 6218 : 0.04135117307305336\n",
      "Training loss for batch 6219 : 0.24593335390090942\n",
      "Training loss for batch 6220 : 0.5103018283843994\n",
      "Training loss for batch 6221 : 0.2752509117126465\n",
      "Training loss for batch 6222 : 0.17440062761306763\n",
      "Training loss for batch 6223 : 0.40388762950897217\n",
      "Training loss for batch 6224 : 0.49549323320388794\n",
      "Training loss for batch 6225 : 0.4179127514362335\n",
      "Training loss for batch 6226 : 0.4220484495162964\n",
      "Training loss for batch 6227 : 0.3261958956718445\n",
      "Training loss for batch 6228 : 0.3287302851676941\n",
      "Training loss for batch 6229 : 0.13515812158584595\n",
      "Training loss for batch 6230 : 0.2333616018295288\n",
      "Training loss for batch 6231 : 0.18583199381828308\n",
      "Training loss for batch 6232 : 0.3462435305118561\n",
      "Training loss for batch 6233 : 0.5745936632156372\n",
      "Training loss for batch 6234 : 0.11107311397790909\n",
      "Training loss for batch 6235 : 0.024047642946243286\n",
      "Training loss for batch 6236 : 0.5333830714225769\n",
      "Training loss for batch 6237 : 0.18227079510688782\n",
      "Training loss for batch 6238 : 0.4668397903442383\n",
      "Training loss for batch 6239 : 0.14658837020397186\n",
      "Training loss for batch 6240 : 0.1663527935743332\n",
      "Training loss for batch 6241 : 0.12577055394649506\n",
      "Training loss for batch 6242 : 0.1850299835205078\n",
      "Training loss for batch 6243 : 0.48224589228630066\n",
      "Training loss for batch 6244 : 0.1857781708240509\n",
      "Training loss for batch 6245 : 0.5227717161178589\n",
      "Training loss for batch 6246 : 0.29215309023857117\n",
      "Training loss for batch 6247 : 0.2315957397222519\n",
      "Training loss for batch 6248 : 0.21773700416088104\n",
      "Training loss for batch 6249 : 0.40097710490226746\n",
      "Training loss for batch 6250 : 0.18635918200016022\n",
      "Training loss for batch 6251 : 0.44394123554229736\n",
      "Training loss for batch 6252 : 0.30563679337501526\n",
      "Training loss for batch 6253 : 0.16332879662513733\n",
      "Training loss for batch 6254 : 0.27608543634414673\n",
      "Training loss for batch 6255 : 0.11439107358455658\n",
      "Training loss for batch 6256 : 0.37521326541900635\n",
      "Training loss for batch 6257 : 0.35734158754348755\n",
      "Training loss for batch 6258 : 0.11377228051424026\n",
      "Training loss for batch 6259 : 0.4321310818195343\n",
      "Training loss for batch 6260 : 0.4161381125450134\n",
      "Training loss for batch 6261 : 0.10221689194440842\n",
      "Training loss for batch 6262 : 0.1842067986726761\n",
      "Training loss for batch 6263 : 0.0937720313668251\n",
      "Training loss for batch 6264 : 0.32786375284194946\n",
      "Training loss for batch 6265 : 0.5841193795204163\n",
      "Training loss for batch 6266 : 0.4561879634857178\n",
      "Training loss for batch 6267 : 0.27438414096832275\n",
      "Training loss for batch 6268 : 0.7344330549240112\n",
      "Training loss for batch 6269 : 0.17394854128360748\n",
      "Training loss for batch 6270 : 0.2049095630645752\n",
      "Training loss for batch 6271 : 0.32079190015792847\n",
      "Training loss for batch 6272 : 0.7216864824295044\n",
      "Training loss for batch 6273 : 0.3435267210006714\n",
      "Training loss for batch 6274 : 0.0038426718674600124\n",
      "Training loss for batch 6275 : 0.09021519869565964\n",
      "Training loss for batch 6276 : 0.4121282994747162\n",
      "Training loss for batch 6277 : 0.15015800297260284\n",
      "Training loss for batch 6278 : 0.15214858949184418\n",
      "Training loss for batch 6279 : 0.5215023756027222\n",
      "Training loss for batch 6280 : 0.06968207657337189\n",
      "Training loss for batch 6281 : 0.015594704076647758\n",
      "Training loss for batch 6282 : 0.04014807567000389\n",
      "Training loss for batch 6283 : 0.350355863571167\n",
      "Training loss for batch 6284 : 0.22960618138313293\n",
      "Training loss for batch 6285 : 0.42682960629463196\n",
      "Training loss for batch 6286 : 0.41246554255485535\n",
      "Training loss for batch 6287 : 0.777083694934845\n",
      "Training loss for batch 6288 : 0.23110386729240417\n",
      "Training loss for batch 6289 : 0.3937016427516937\n",
      "Training loss for batch 6290 : 0.3341701030731201\n",
      "Training loss for batch 6291 : 0.13432377576828003\n",
      "Training loss for batch 6292 : 0.14830555021762848\n",
      "Training loss for batch 6293 : 0.07343489676713943\n",
      "Training loss for batch 6294 : 0.4702298045158386\n",
      "Training loss for batch 6295 : 0.2640266418457031\n",
      "Training loss for batch 6296 : 0.32672393321990967\n",
      "Training loss for batch 6297 : 0.06640402227640152\n",
      "Training loss for batch 6298 : 0.08948136866092682\n",
      "Training loss for batch 6299 : 0.257376492023468\n",
      "Training loss for batch 6300 : 0.18417982757091522\n",
      "Training loss for batch 6301 : 0.24801215529441833\n",
      "Training loss for batch 6302 : 0.5804215669631958\n",
      "Training loss for batch 6303 : 0.353071004152298\n",
      "Training loss for batch 6304 : 0.11279311776161194\n",
      "Training loss for batch 6305 : 0.6380216479301453\n",
      "Training loss for batch 6306 : 0.21965007483959198\n",
      "Training loss for batch 6307 : 0.24590736627578735\n",
      "Training loss for batch 6308 : 0.4969615936279297\n",
      "Training loss for batch 6309 : 0.7197569608688354\n",
      "Training loss for batch 6310 : 0.5162744522094727\n",
      "Training loss for batch 6311 : 0.09558342397212982\n",
      "Training loss for batch 6312 : 0.1062111034989357\n",
      "Training loss for batch 6313 : 0.4316350519657135\n",
      "Training loss for batch 6314 : 0.3205670416355133\n",
      "Training loss for batch 6315 : 0.27028888463974\n",
      "Training loss for batch 6316 : 0.13855956494808197\n",
      "Training loss for batch 6317 : 0.11947514116764069\n",
      "Training loss for batch 6318 : 0.28196051716804504\n",
      "Training loss for batch 6319 : 0.4633018374443054\n",
      "Training loss for batch 6320 : 0.4036061465740204\n",
      "Training loss for batch 6321 : 0.5316005349159241\n",
      "Training loss for batch 6322 : 0.23792815208435059\n",
      "Training loss for batch 6323 : 0.15827947854995728\n",
      "Training loss for batch 6324 : 0.34173694252967834\n",
      "Training loss for batch 6325 : 0.21494844555854797\n",
      "Training loss for batch 6326 : 0.4899410903453827\n",
      "Training loss for batch 6327 : 0.17503900825977325\n",
      "Training loss for batch 6328 : 0.06399593502283096\n",
      "Training loss for batch 6329 : 0.28370237350463867\n",
      "Training loss for batch 6330 : 0.24273689091205597\n",
      "Training loss for batch 6331 : 0.217967689037323\n",
      "Training loss for batch 6332 : 0.4407079815864563\n",
      "Training loss for batch 6333 : 0.16454121470451355\n",
      "Training loss for batch 6334 : 0.33865031599998474\n",
      "Training loss for batch 6335 : 0.27109813690185547\n",
      "Training loss for batch 6336 : 0.4595329165458679\n",
      "Training loss for batch 6337 : 0.01897714100778103\n",
      "Training loss for batch 6338 : 0.5177081823348999\n",
      "Training loss for batch 6339 : 0.5501569509506226\n",
      "Training loss for batch 6340 : 0.3994431793689728\n",
      "Training loss for batch 6341 : 0.21998369693756104\n",
      "Training loss for batch 6342 : 0.12938474118709564\n",
      "Training loss for batch 6343 : 0.22640281915664673\n",
      "Training loss for batch 6344 : 0.35225164890289307\n",
      "Training loss for batch 6345 : 0.3218105733394623\n",
      "Training loss for batch 6346 : 0.37066569924354553\n",
      "Training loss for batch 6347 : 0.3083234131336212\n",
      "Training loss for batch 6348 : 0.11883484572172165\n",
      "Training loss for batch 6349 : 0.20614522695541382\n",
      "Training loss for batch 6350 : 0.5868276953697205\n",
      "Training loss for batch 6351 : 0.38069701194763184\n",
      "Training loss for batch 6352 : 0.12670305371284485\n",
      "Training loss for batch 6353 : 0.11039766669273376\n",
      "Training loss for batch 6354 : 0.10480853170156479\n",
      "Training loss for batch 6355 : 0.49111390113830566\n",
      "Training loss for batch 6356 : 0.33827856183052063\n",
      "Training loss for batch 6357 : 0.0897727757692337\n",
      "Training loss for batch 6358 : 0.22360865771770477\n",
      "Training loss for batch 6359 : 0.4428851902484894\n",
      "Training loss for batch 6360 : 0.3479551374912262\n",
      "Training loss for batch 6361 : 0.3789338171482086\n",
      "Training loss for batch 6362 : 0.5608871579170227\n",
      "Training loss for batch 6363 : 0.33512425422668457\n",
      "Training loss for batch 6364 : 0.158229261636734\n",
      "Training loss for batch 6365 : 0.2593650817871094\n",
      "Training loss for batch 6366 : 0.5872763395309448\n",
      "Training loss for batch 6367 : 0.5155127048492432\n",
      "Training loss for batch 6368 : 0.28631430864334106\n",
      "Training loss for batch 6369 : 0.08778157830238342\n",
      "Training loss for batch 6370 : 0.3050300180912018\n",
      "Training loss for batch 6371 : 0.3222721815109253\n",
      "Training loss for batch 6372 : 0.3808594346046448\n",
      "Training loss for batch 6373 : 0.10536761581897736\n",
      "Training loss for batch 6374 : 0.05758906528353691\n",
      "Training loss for batch 6375 : 0.19971777498722076\n",
      "Training loss for batch 6376 : 0.18894948065280914\n",
      "Training loss for batch 6377 : 0.20828919112682343\n",
      "Training loss for batch 6378 : 0.4755248725414276\n",
      "Training loss for batch 6379 : 0.36646243929862976\n",
      "Training loss for batch 6380 : 0.12172175198793411\n",
      "Training loss for batch 6381 : 0.535220205783844\n",
      "Training loss for batch 6382 : 0.5167180299758911\n",
      "Training loss for batch 6383 : 0.17965787649154663\n",
      "Training loss for batch 6384 : 0.3790309727191925\n",
      "Training loss for batch 6385 : 0.4779239296913147\n",
      "Training loss for batch 6386 : 0.2939997911453247\n",
      "Training loss for batch 6387 : 0.434216171503067\n",
      "Training loss for batch 6388 : 0.4408380389213562\n",
      "Training loss for batch 6389 : 0.13440607488155365\n",
      "Training loss for batch 6390 : 0.27156227827072144\n",
      "Training loss for batch 6391 : 0.07268338650465012\n",
      "Training loss for batch 6392 : 0.07353225350379944\n",
      "Training loss for batch 6393 : 0.06244390457868576\n",
      "Training loss for batch 6394 : 0.4541873633861542\n",
      "Training loss for batch 6395 : 0.4129268527030945\n",
      "Training loss for batch 6396 : 0.2902263104915619\n",
      "Training loss for batch 6397 : 0.10144252330064774\n",
      "Training loss for batch 6398 : 0.7951869368553162\n",
      "Training loss for batch 6399 : 0.13776330649852753\n",
      "Training loss for batch 6400 : 0.07457146793603897\n",
      "Training loss for batch 6401 : 0.3270457983016968\n",
      "Training loss for batch 6402 : 0.4607236981391907\n",
      "Training loss for batch 6403 : 0.43323901295661926\n",
      "Training loss for batch 6404 : 0.275797039270401\n",
      "Training loss for batch 6405 : 0.26538240909576416\n",
      "Training loss for batch 6406 : 0.4330945909023285\n",
      "Training loss for batch 6407 : 0.6135439276695251\n",
      "Training loss for batch 6408 : 0.638081967830658\n",
      "Training loss for batch 6409 : 0.16134388744831085\n",
      "Training loss for batch 6410 : 0.1693589836359024\n",
      "Training loss for batch 6411 : 0.3790169656276703\n",
      "Training loss for batch 6412 : 0.16938495635986328\n",
      "Training loss for batch 6413 : 0.4931398928165436\n",
      "Training loss for batch 6414 : 0.3216606676578522\n",
      "Training loss for batch 6415 : 0.34402865171432495\n",
      "Training loss for batch 6416 : 0.5975961685180664\n",
      "Training loss for batch 6417 : 0.26351606845855713\n",
      "Training loss for batch 6418 : 0.5419726371765137\n",
      "Training loss for batch 6419 : 0.05318846181035042\n",
      "Training loss for batch 6420 : 0.1694350689649582\n",
      "Training loss for batch 6421 : 0.02941947989165783\n",
      "Training loss for batch 6422 : 0.1585647463798523\n",
      "Training loss for batch 6423 : 0.17440290749073029\n",
      "Training loss for batch 6424 : 0.5127642154693604\n",
      "Training loss for batch 6425 : 0.29731905460357666\n",
      "Training loss for batch 6426 : 0.2353324443101883\n",
      "Training loss for batch 6427 : 0.021113015711307526\n",
      "Training loss for batch 6428 : 0.26338252425193787\n",
      "Training loss for batch 6429 : 0.4093901813030243\n",
      "Training loss for batch 6430 : 0.30006521940231323\n",
      "Training loss for batch 6431 : 0.27440038323402405\n",
      "Training loss for batch 6432 : 0.11782614886760712\n",
      "Training loss for batch 6433 : 0.07680495828390121\n",
      "Training loss for batch 6434 : 0.5150104761123657\n",
      "Training loss for batch 6435 : 0.858245849609375\n",
      "Training loss for batch 6436 : 0.4647004008293152\n",
      "Training loss for batch 6437 : 0.46753427386283875\n",
      "Training loss for batch 6438 : 0.41460859775543213\n",
      "Training loss for batch 6439 : 0.24511639773845673\n",
      "Training loss for batch 6440 : 0.2946845293045044\n",
      "Training loss for batch 6441 : 0.21682055294513702\n",
      "Training loss for batch 6442 : 0.6326764822006226\n",
      "Training loss for batch 6443 : 0.4177909791469574\n",
      "Training loss for batch 6444 : 0.054002415388822556\n",
      "Training loss for batch 6445 : 0.5932286381721497\n",
      "Training loss for batch 6446 : 0.5383047461509705\n",
      "Training loss for batch 6447 : 0.5026984214782715\n",
      "Training loss for batch 6448 : 0.17980723083019257\n",
      "Training loss for batch 6449 : 0.12419010698795319\n",
      "Training loss for batch 6450 : 0.47392645478248596\n",
      "Training loss for batch 6451 : 0.3406875431537628\n",
      "Training loss for batch 6452 : 0.5620660185813904\n",
      "Training loss for batch 6453 : 0.2512139678001404\n",
      "Training loss for batch 6454 : 0.2653656005859375\n",
      "Training loss for batch 6455 : 0.13745813071727753\n",
      "Training loss for batch 6456 : 0.2616170644760132\n",
      "Training loss for batch 6457 : 0.7652508020401001\n",
      "Training loss for batch 6458 : 0.09257149696350098\n",
      "Training loss for batch 6459 : 0.5769100189208984\n",
      "Training loss for batch 6460 : 0.33583328127861023\n",
      "Training loss for batch 6461 : 0.39561745524406433\n",
      "Training loss for batch 6462 : 0.3240044414997101\n",
      "Training loss for batch 6463 : 0.48639845848083496\n",
      "Training loss for batch 6464 : 0.4107106328010559\n",
      "Training loss for batch 6465 : 0.21753372251987457\n",
      "Training loss for batch 6466 : 0.04244734346866608\n",
      "Training loss for batch 6467 : 0.16791082918643951\n",
      "Training loss for batch 6468 : 0.28851255774497986\n",
      "Training loss for batch 6469 : 0.34883493185043335\n",
      "Training loss for batch 6470 : 0.12787313759326935\n",
      "Training loss for batch 6471 : 0.17368675768375397\n",
      "Training loss for batch 6472 : 0.14676371216773987\n",
      "Training loss for batch 6473 : 0.4984171390533447\n",
      "Training loss for batch 6474 : 0.2686461806297302\n",
      "Training loss for batch 6475 : 0.5420284271240234\n",
      "Training loss for batch 6476 : 0.027538742870092392\n",
      "Training loss for batch 6477 : 0.012897335924208164\n",
      "Training loss for batch 6478 : 0.3304438889026642\n",
      "Training loss for batch 6479 : 0.33636632561683655\n",
      "Training loss for batch 6480 : 0.20070497691631317\n",
      "Training loss for batch 6481 : 0.14763957262039185\n",
      "Training loss for batch 6482 : 0.25710272789001465\n",
      "Training loss for batch 6483 : 0.3913695216178894\n",
      "Training loss for batch 6484 : 0.15126922726631165\n",
      "Training loss for batch 6485 : 0.15068508684635162\n",
      "Training loss for batch 6486 : 0.4169372618198395\n",
      "Training loss for batch 6487 : 0.23801279067993164\n",
      "Training loss for batch 6488 : 0.22226804494857788\n",
      "Training loss for batch 6489 : 0.513348400592804\n",
      "Training loss for batch 6490 : 0.3261664807796478\n",
      "Training loss for batch 6491 : 0.27137431502342224\n",
      "Training loss for batch 6492 : 0.14479075372219086\n",
      "Training loss for batch 6493 : 0.21311144530773163\n",
      "Training loss for batch 6494 : 0.697160005569458\n",
      "Training loss for batch 6495 : 0.37148842215538025\n",
      "Training loss for batch 6496 : 0.5870832800865173\n",
      "Training loss for batch 6497 : 0.02679528295993805\n",
      "Training loss for batch 6498 : 0.05900916829705238\n",
      "Training loss for batch 6499 : 0.2796752452850342\n",
      "Training loss for batch 6500 : 0.3057708442211151\n",
      "Training loss for batch 6501 : 0.2630394995212555\n",
      "Training loss for batch 6502 : 0.7023020386695862\n",
      "Training loss for batch 6503 : 0.04963768646121025\n",
      "Training loss for batch 6504 : 0.30871227383613586\n",
      "Training loss for batch 6505 : 0.4298859238624573\n",
      "Training loss for batch 6506 : 0.6506084203720093\n",
      "Training loss for batch 6507 : 0.07807847112417221\n",
      "Training loss for batch 6508 : 0.07058413326740265\n",
      "Training loss for batch 6509 : 0.11337928473949432\n",
      "Training loss for batch 6510 : 0.17793843150138855\n",
      "Training loss for batch 6511 : 0.4260703921318054\n",
      "Training loss for batch 6512 : 0.4646661579608917\n",
      "Training loss for batch 6513 : 0.28796571493148804\n",
      "Training loss for batch 6514 : 0.13001176714897156\n",
      "Training loss for batch 6515 : 0.4323270916938782\n",
      "Training loss for batch 6516 : 0.22087320685386658\n",
      "Training loss for batch 6517 : 0.10450687259435654\n",
      "Training loss for batch 6518 : 0.31705182790756226\n",
      "Training loss for batch 6519 : 0.30446311831474304\n",
      "Training loss for batch 6520 : 0.12889981269836426\n",
      "Training loss for batch 6521 : 0.3497455418109894\n",
      "Training loss for batch 6522 : 0.4623391330242157\n",
      "Training loss for batch 6523 : 0.5316509008407593\n",
      "Training loss for batch 6524 : 0.23436014354228973\n",
      "Training loss for batch 6525 : 0.45277291536331177\n",
      "Training loss for batch 6526 : 0.141361266374588\n",
      "Training loss for batch 6527 : 0.2417503446340561\n",
      "Training loss for batch 6528 : 0.20750543475151062\n",
      "Training loss for batch 6529 : 0.15911312401294708\n",
      "Training loss for batch 6530 : 0.10674016922712326\n",
      "Training loss for batch 6531 : 0.8264465928077698\n",
      "Training loss for batch 6532 : 0.3494518995285034\n",
      "Training loss for batch 6533 : 0.15196742117404938\n",
      "Training loss for batch 6534 : 0.26314130425453186\n",
      "Training loss for batch 6535 : 0.45044195652008057\n",
      "Training loss for batch 6536 : 0.16149646043777466\n",
      "Training loss for batch 6537 : 0.06483697891235352\n",
      "Training loss for batch 6538 : 0.4117348790168762\n",
      "Training loss for batch 6539 : 0.08208610117435455\n",
      "Training loss for batch 6540 : 0.3907775580883026\n",
      "Training loss for batch 6541 : 0.048530589789152145\n",
      "Training loss for batch 6542 : 0.4259668290615082\n",
      "Training loss for batch 6543 : 0.16403412818908691\n",
      "Training loss for batch 6544 : 0.07979480922222137\n",
      "Training loss for batch 6545 : 0.038779258728027344\n",
      "Training loss for batch 6546 : 0.16632422804832458\n",
      "Training loss for batch 6547 : 0.5972086191177368\n",
      "Training loss for batch 6548 : 0.7459161877632141\n",
      "Training loss for batch 6549 : 0.38271355628967285\n",
      "Training loss for batch 6550 : 0.06108173355460167\n",
      "Training loss for batch 6551 : 0.13632036745548248\n",
      "Training loss for batch 6552 : 0.1343531459569931\n",
      "Training loss for batch 6553 : 0.3707655072212219\n",
      "Training loss for batch 6554 : 0.14160725474357605\n",
      "Training loss for batch 6555 : 0.13870488107204437\n",
      "Training loss for batch 6556 : 0.3423548638820648\n",
      "Training loss for batch 6557 : 0.9876425862312317\n",
      "Training loss for batch 6558 : 0.4379771947860718\n",
      "Training loss for batch 6559 : 0.7114275693893433\n",
      "Training loss for batch 6560 : 0.01658475585281849\n",
      "Training loss for batch 6561 : 0.15521545708179474\n",
      "Training loss for batch 6562 : 0.19131633639335632\n",
      "Training loss for batch 6563 : 0.3408116102218628\n",
      "Training loss for batch 6564 : 0.059697896242141724\n",
      "Training loss for batch 6565 : 0.6034448146820068\n",
      "Training loss for batch 6566 : 0.14652468264102936\n",
      "Training loss for batch 6567 : 0.08586011826992035\n",
      "Training loss for batch 6568 : 0.6274974346160889\n",
      "Training loss for batch 6569 : 0.12888941168785095\n",
      "Training loss for batch 6570 : 0.12585663795471191\n",
      "Training loss for batch 6571 : 0.3538196384906769\n",
      "Training loss for batch 6572 : 0.06306242197751999\n",
      "Training loss for batch 6573 : 0.4244002401828766\n",
      "Training loss for batch 6574 : 0.3334676921367645\n",
      "Training loss for batch 6575 : 0.033479589968919754\n",
      "Training loss for batch 6576 : 0.5249791145324707\n",
      "Training loss for batch 6577 : 0.1762574017047882\n",
      "Training loss for batch 6578 : 0.33519285917282104\n",
      "Training loss for batch 6579 : 0.0979953482747078\n",
      "Training loss for batch 6580 : 0.44726380705833435\n",
      "Training loss for batch 6581 : 0.392935186624527\n",
      "Training loss for batch 6582 : 0.5428592562675476\n",
      "Training loss for batch 6583 : 0.6855606436729431\n",
      "Training loss for batch 6584 : 0.21947522461414337\n",
      "Training loss for batch 6585 : 0.22126534581184387\n",
      "Training loss for batch 6586 : 0.22091439366340637\n",
      "Training loss for batch 6587 : 0.49842002987861633\n",
      "Training loss for batch 6588 : 0.2466754913330078\n",
      "Training loss for batch 6589 : 0.540852427482605\n",
      "Training loss for batch 6590 : 0.2871982157230377\n",
      "Training loss for batch 6591 : 0.08535753190517426\n",
      "Training loss for batch 6592 : 0.40536975860595703\n",
      "Training loss for batch 6593 : 0.1410570740699768\n",
      "Training loss for batch 6594 : 0.09641242772340775\n",
      "Training loss for batch 6595 : 0.09786512702703476\n",
      "Training loss for batch 6596 : 0.18037039041519165\n",
      "Training loss for batch 6597 : 0.3168683648109436\n",
      "Training loss for batch 6598 : 0.278704971075058\n",
      "Training loss for batch 6599 : 0.33136996626853943\n",
      "Training loss for batch 6600 : 0.2978021204471588\n",
      "Training loss for batch 6601 : 0.34144720435142517\n",
      "Training loss for batch 6602 : 0.29398468136787415\n",
      "Training loss for batch 6603 : 0.36504316329956055\n",
      "Training loss for batch 6604 : 0.35229039192199707\n",
      "Training loss for batch 6605 : 0.20471881330013275\n",
      "Training loss for batch 6606 : 0.2571796774864197\n",
      "Training loss for batch 6607 : 0.037499815225601196\n",
      "Training loss for batch 6608 : 0.3745691776275635\n",
      "Training loss for batch 6609 : 0.1389877051115036\n",
      "Training loss for batch 6610 : 0.1927666962146759\n",
      "Training loss for batch 6611 : 0.5710569620132446\n",
      "Training loss for batch 6612 : 0.585202157497406\n",
      "Training loss for batch 6613 : 0.22427481412887573\n",
      "Training loss for batch 6614 : 0.1497988998889923\n",
      "Training loss for batch 6615 : 0.5987439751625061\n",
      "Training loss for batch 6616 : 0.10492875427007675\n",
      "Training loss for batch 6617 : 0.31744062900543213\n",
      "Training loss for batch 6618 : 0.031034622341394424\n",
      "Training loss for batch 6619 : 0.12637780606746674\n",
      "Training loss for batch 6620 : 0.3565910756587982\n",
      "Training loss for batch 6621 : 0.25376883149147034\n",
      "Training loss for batch 6622 : 0.3470820188522339\n",
      "Training loss for batch 6623 : 0.17656272649765015\n",
      "Training loss for batch 6624 : 0.14303354918956757\n",
      "Training loss for batch 6625 : 0.8103128671646118\n",
      "Training loss for batch 6626 : 0.2045275866985321\n",
      "Training loss for batch 6627 : 0.30753543972969055\n",
      "Training loss for batch 6628 : 0.4091308116912842\n",
      "Training loss for batch 6629 : 0.03135725110769272\n",
      "Training loss for batch 6630 : 0.2620280683040619\n",
      "Training loss for batch 6631 : 0.11574463546276093\n",
      "Training loss for batch 6632 : 0.1904977709054947\n",
      "Training loss for batch 6633 : 0.12957975268363953\n",
      "Training loss for batch 6634 : 0.17148864269256592\n",
      "Training loss for batch 6635 : 0.5246220231056213\n",
      "Training loss for batch 6636 : 0.6903862357139587\n",
      "Training loss for batch 6637 : 0.08631966263055801\n",
      "Training loss for batch 6638 : 0.08944561332464218\n",
      "Training loss for batch 6639 : 0.11705414950847626\n",
      "Training loss for batch 6640 : 0.0205711480230093\n",
      "Training loss for batch 6641 : 0.03511372208595276\n",
      "Training loss for batch 6642 : 0.3305942416191101\n",
      "Training loss for batch 6643 : 0.023618360981345177\n",
      "Training loss for batch 6644 : 0.03182436153292656\n",
      "Training loss for batch 6645 : 0.6339706778526306\n",
      "Training loss for batch 6646 : 0.5893726348876953\n",
      "Training loss for batch 6647 : 0.06143658980727196\n",
      "Training loss for batch 6648 : 0.38866570591926575\n",
      "Training loss for batch 6649 : 0.22556863725185394\n",
      "Training loss for batch 6650 : 0.05413222312927246\n",
      "Training loss for batch 6651 : 0.45551955699920654\n",
      "Training loss for batch 6652 : 0.07756341993808746\n",
      "Training loss for batch 6653 : 0.11260607093572617\n",
      "Training loss for batch 6654 : 0.20382720232009888\n",
      "Training loss for batch 6655 : 0.18069274723529816\n",
      "Training loss for batch 6656 : 0.46064990758895874\n",
      "Training loss for batch 6657 : 0.2487359642982483\n",
      "Training loss for batch 6658 : 0.06551527231931686\n",
      "Training loss for batch 6659 : 0.4064323306083679\n",
      "Training loss for batch 6660 : 0.671524703502655\n",
      "Training loss for batch 6661 : 0.13771748542785645\n",
      "Training loss for batch 6662 : 0.33413565158843994\n",
      "Training loss for batch 6663 : 0.1637592911720276\n",
      "Training loss for batch 6664 : 0.6125481128692627\n",
      "Training loss for batch 6665 : 0.1367851197719574\n",
      "Training loss for batch 6666 : 0.12481727451086044\n",
      "Training loss for batch 6667 : 0.5407035946846008\n",
      "Training loss for batch 6668 : 0.3030616343021393\n",
      "Training loss for batch 6669 : 0.7153148055076599\n",
      "Training loss for batch 6670 : 0.1817910373210907\n",
      "Training loss for batch 6671 : 0.11337665468454361\n",
      "Training loss for batch 6672 : 0.7733951210975647\n",
      "Training loss for batch 6673 : 0.5831294059753418\n",
      "Training loss for batch 6674 : 0.2730880677700043\n",
      "Training loss for batch 6675 : 0.5277746319770813\n",
      "Training loss for batch 6676 : 0.08523597568273544\n",
      "Training loss for batch 6677 : 0.6184518337249756\n",
      "Training loss for batch 6678 : 0.04940544441342354\n",
      "Training loss for batch 6679 : 0.3807879090309143\n",
      "Training loss for batch 6680 : 0.1772952526807785\n",
      "Training loss for batch 6681 : 0.0\n",
      "Training loss for batch 6682 : 0.19943957030773163\n",
      "Training loss for batch 6683 : 0.4374636709690094\n",
      "Training loss for batch 6684 : 0.2987426221370697\n",
      "Training loss for batch 6685 : 0.253431499004364\n",
      "Training loss for batch 6686 : 0.28901782631874084\n",
      "Training loss for batch 6687 : 0.32850581407546997\n",
      "Training loss for batch 6688 : 0.2460872381925583\n",
      "Training loss for batch 6689 : 0.1403404176235199\n",
      "Training loss for batch 6690 : 0.21374298632144928\n",
      "Training loss for batch 6691 : 0.2560019791126251\n",
      "Training loss for batch 6692 : 0.6363884210586548\n",
      "Training loss for batch 6693 : 0.5410144925117493\n",
      "Training loss for batch 6694 : 0.9728186726570129\n",
      "Training loss for batch 6695 : 0.25797003507614136\n",
      "Training loss for batch 6696 : 0.20318001508712769\n",
      "Training loss for batch 6697 : 0.823512613773346\n",
      "Training loss for batch 6698 : 0.23773716390132904\n",
      "Training loss for batch 6699 : 0.5256391763687134\n",
      "Training loss for batch 6700 : 0.6575813889503479\n",
      "Training loss for batch 6701 : 0.22682850062847137\n",
      "Training loss for batch 6702 : 0.39854374527931213\n",
      "Training loss for batch 6703 : 0.26817435026168823\n",
      "Training loss for batch 6704 : 0.13585177063941956\n",
      "Training loss for batch 6705 : 0.45643484592437744\n",
      "Training loss for batch 6706 : 0.3119388818740845\n",
      "Training loss for batch 6707 : 0.2105509340763092\n",
      "Training loss for batch 6708 : 0.035464126616716385\n",
      "Training loss for batch 6709 : 0.2682250738143921\n",
      "Training loss for batch 6710 : 0.6201726794242859\n",
      "Training loss for batch 6711 : 0.1101672500371933\n",
      "Training loss for batch 6712 : 0.12886492908000946\n",
      "Training loss for batch 6713 : 0.6478529572486877\n",
      "Training loss for batch 6714 : 0.2545638084411621\n",
      "Training loss for batch 6715 : 0.6568347215652466\n",
      "Training loss for batch 6716 : 0.22379468381404877\n",
      "Training loss for batch 6717 : 0.19915182888507843\n",
      "Training loss for batch 6718 : 0.2765612304210663\n",
      "Training loss for batch 6719 : 0.05618366226553917\n",
      "Training loss for batch 6720 : 0.24053585529327393\n",
      "Training loss for batch 6721 : 0.2548733949661255\n",
      "Training loss for batch 6722 : 0.5974084138870239\n",
      "Training loss for batch 6723 : 0.17597684264183044\n",
      "Training loss for batch 6724 : 0.004120226949453354\n",
      "Training loss for batch 6725 : 0.2675819993019104\n",
      "Training loss for batch 6726 : 0.40090832114219666\n",
      "Training loss for batch 6727 : 0.34085002541542053\n",
      "Training loss for batch 6728 : 0.2972164452075958\n",
      "Training loss for batch 6729 : 0.3487280309200287\n",
      "Training loss for batch 6730 : 0.8691878914833069\n",
      "Training loss for batch 6731 : 0.06897970288991928\n",
      "Training loss for batch 6732 : 0.1649017632007599\n",
      "Training loss for batch 6733 : 8.134047675412148e-05\n",
      "Training loss for batch 6734 : 0.439582884311676\n",
      "Training loss for batch 6735 : 0.3848054111003876\n",
      "Training loss for batch 6736 : 0.3914983868598938\n",
      "Training loss for batch 6737 : 0.3115358054637909\n",
      "Training loss for batch 6738 : 0.08189679682254791\n",
      "Training loss for batch 6739 : 0.5092930793762207\n",
      "Training loss for batch 6740 : 0.10863815248012543\n",
      "Training loss for batch 6741 : 0.06996523588895798\n",
      "Training loss for batch 6742 : 0.03581841289997101\n",
      "Training loss for batch 6743 : 0.5097992420196533\n",
      "Training loss for batch 6744 : 0.23860082030296326\n",
      "Training loss for batch 6745 : 0.48058491945266724\n",
      "Training loss for batch 6746 : 0.17212672531604767\n",
      "Training loss for batch 6747 : 0.12427547574043274\n",
      "Training loss for batch 6748 : 0.3513300120830536\n",
      "Training loss for batch 6749 : 0.16866250336170197\n",
      "Training loss for batch 6750 : 0.05160937458276749\n",
      "Training loss for batch 6751 : 0.1373537927865982\n",
      "Training loss for batch 6752 : 0.5462912917137146\n",
      "Training loss for batch 6753 : 0.8619199395179749\n",
      "Training loss for batch 6754 : 0.20060375332832336\n",
      "Training loss for batch 6755 : 0.468769371509552\n",
      "Training loss for batch 6756 : 0.2104349434375763\n",
      "Training loss for batch 6757 : 0.35807013511657715\n",
      "Training loss for batch 6758 : 0.4264591634273529\n",
      "Training loss for batch 6759 : 0.165558859705925\n",
      "Training loss for batch 6760 : 0.4496823251247406\n",
      "Training loss for batch 6761 : 0.1540016233921051\n",
      "Training loss for batch 6762 : 0.11929960548877716\n",
      "Training loss for batch 6763 : 0.12426549196243286\n",
      "Training loss for batch 6764 : 0.23588737845420837\n",
      "Training loss for batch 6765 : 0.15338318049907684\n",
      "Training loss for batch 6766 : 0.24357320368289948\n",
      "Training loss for batch 6767 : 0.19561639428138733\n",
      "Training loss for batch 6768 : 0.8574841022491455\n",
      "Training loss for batch 6769 : 0.1673046499490738\n",
      "Training loss for batch 6770 : 0.18633614480495453\n",
      "Training loss for batch 6771 : 0.18509402871131897\n",
      "Training loss for batch 6772 : 0.06676993519067764\n",
      "Training loss for batch 6773 : 0.42507418990135193\n",
      "Training loss for batch 6774 : 0.5109302401542664\n",
      "Training loss for batch 6775 : 0.1365366280078888\n",
      "Training loss for batch 6776 : 0.052621036767959595\n",
      "Training loss for batch 6777 : 0.0575619712471962\n",
      "Training loss for batch 6778 : 0.8641999363899231\n",
      "Training loss for batch 6779 : 0.12267687916755676\n",
      "Training loss for batch 6780 : 0.5008734464645386\n",
      "Training loss for batch 6781 : 0.19713632762432098\n",
      "Training loss for batch 6782 : 0.2611650228500366\n",
      "Training loss for batch 6783 : 0.02457144483923912\n",
      "Training loss for batch 6784 : 0.19621270895004272\n",
      "Training loss for batch 6785 : 0.19500821828842163\n",
      "Training loss for batch 6786 : 0.3226343095302582\n",
      "Training loss for batch 6787 : 0.26685604453086853\n",
      "Training loss for batch 6788 : 0.2341081202030182\n",
      "Training loss for batch 6789 : 0.13593512773513794\n",
      "Training loss for batch 6790 : 0.5792837142944336\n",
      "Training loss for batch 6791 : 0.5445936918258667\n",
      "Training loss for batch 6792 : 0.6425392031669617\n",
      "Training loss for batch 6793 : 0.2467736154794693\n",
      "Training loss for batch 6794 : 0.5694068670272827\n",
      "Training loss for batch 6795 : 0.15324486792087555\n",
      "Training loss for batch 6796 : 0.08394362032413483\n",
      "Training loss for batch 6797 : 0.19604964554309845\n",
      "Training loss for batch 6798 : 0.4446465075016022\n",
      "Training loss for batch 6799 : 0.061859309673309326\n",
      "Training loss for batch 6800 : 0.12273642420768738\n",
      "Training loss for batch 6801 : 0.3019421398639679\n",
      "Training loss for batch 6802 : 0.11772582679986954\n",
      "Training loss for batch 6803 : 0.15113581717014313\n",
      "Training loss for batch 6804 : 0.28215929865837097\n",
      "Training loss for batch 6805 : 0.2361694872379303\n",
      "Training loss for batch 6806 : 0.17794081568717957\n",
      "Training loss for batch 6807 : 0.3489266037940979\n",
      "Training loss for batch 6808 : 0.445919394493103\n",
      "Training loss for batch 6809 : 0.4670458436012268\n",
      "Training loss for batch 6810 : 0.14409050345420837\n",
      "Training loss for batch 6811 : 0.07582608610391617\n",
      "Training loss for batch 6812 : 0.356488436460495\n",
      "Training loss for batch 6813 : 0.22343535721302032\n",
      "Training loss for batch 6814 : 0.34982678294181824\n",
      "Training loss for batch 6815 : 0.4666668772697449\n",
      "Training loss for batch 6816 : 0.38852205872535706\n",
      "Training loss for batch 6817 : 0.16684752702713013\n",
      "Training loss for batch 6818 : 0.25543031096458435\n",
      "Training loss for batch 6819 : 0.0\n",
      "Training loss for batch 6820 : 0.0812607854604721\n",
      "Training loss for batch 6821 : 0.12167494744062424\n",
      "Training loss for batch 6822 : 0.564868152141571\n",
      "Training loss for batch 6823 : 0.5818811058998108\n",
      "Training loss for batch 6824 : 0.18908461928367615\n",
      "Training loss for batch 6825 : 0.5813488364219666\n",
      "Training loss for batch 6826 : 0.41691553592681885\n",
      "Training loss for batch 6827 : 0.12208443880081177\n",
      "Training loss for batch 6828 : 0.13463512063026428\n",
      "Training loss for batch 6829 : 0.4485778212547302\n",
      "Training loss for batch 6830 : 0.09637001156806946\n",
      "Training loss for batch 6831 : 0.5317937135696411\n",
      "Training loss for batch 6832 : 0.35717472434043884\n",
      "Training loss for batch 6833 : 0.10776130110025406\n",
      "Training loss for batch 6834 : 0.1861630529165268\n",
      "Training loss for batch 6835 : 0.6348145008087158\n",
      "Training loss for batch 6836 : 0.06255359202623367\n",
      "Training loss for batch 6837 : 0.7949432134628296\n",
      "Training loss for batch 6838 : 0.0\n",
      "Training loss for batch 6839 : 0.36985716223716736\n",
      "Training loss for batch 6840 : 0.05958273634314537\n",
      "Training loss for batch 6841 : 0.14280009269714355\n",
      "Training loss for batch 6842 : 0.3270241916179657\n",
      "Training loss for batch 6843 : 0.25763943791389465\n",
      "Training loss for batch 6844 : 0.10184994339942932\n",
      "Training loss for batch 6845 : 0.23069746792316437\n",
      "Training loss for batch 6846 : 0.3762352168560028\n",
      "Training loss for batch 6847 : 0.5630964040756226\n",
      "Training loss for batch 6848 : 0.656624436378479\n",
      "Training loss for batch 6849 : 0.6759310364723206\n",
      "Training loss for batch 6850 : 0.34333932399749756\n",
      "Training loss for batch 6851 : 0.0704592689871788\n",
      "Training loss for batch 6852 : 0.07386728376150131\n",
      "Training loss for batch 6853 : 0.1348426640033722\n",
      "Training loss for batch 6854 : 0.12293713539838791\n",
      "Training loss for batch 6855 : 0.22741080820560455\n",
      "Training loss for batch 6856 : 0.3048657178878784\n",
      "Training loss for batch 6857 : 0.17930364608764648\n",
      "Training loss for batch 6858 : 0.4989427626132965\n",
      "Training loss for batch 6859 : 0.2931823432445526\n",
      "Training loss for batch 6860 : 0.19328078627586365\n",
      "Training loss for batch 6861 : 0.23343303799629211\n",
      "Training loss for batch 6862 : 0.08385765552520752\n",
      "Training loss for batch 6863 : 0.5503284335136414\n",
      "Training loss for batch 6864 : 0.117283396422863\n",
      "Training loss for batch 6865 : 0.14986303448677063\n",
      "Training loss for batch 6866 : 0.13398826122283936\n",
      "Training loss for batch 6867 : 0.22338703274726868\n",
      "Training loss for batch 6868 : 0.15503492951393127\n",
      "Training loss for batch 6869 : 0.2772864103317261\n",
      "Training loss for batch 6870 : 0.5638971924781799\n",
      "Training loss for batch 6871 : 0.38316309452056885\n",
      "Training loss for batch 6872 : 0.15356603264808655\n",
      "Training loss for batch 6873 : 0.4898008704185486\n",
      "Training loss for batch 6874 : 0.4470633864402771\n",
      "Training loss for batch 6875 : 0.3501051366329193\n",
      "Training loss for batch 6876 : 0.41917067766189575\n",
      "Training loss for batch 6877 : 0.33995991945266724\n",
      "Training loss for batch 6878 : 0.4869484007358551\n",
      "Training loss for batch 6879 : 0.3290732800960541\n",
      "Training loss for batch 6880 : 0.024719538167119026\n",
      "Training loss for batch 6881 : 0.39786189794540405\n",
      "Training loss for batch 6882 : 0.1550111025571823\n",
      "Training loss for batch 6883 : 0.23011472821235657\n",
      "Training loss for batch 6884 : 0.5141178369522095\n",
      "Training loss for batch 6885 : 1.249798059463501\n",
      "Training loss for batch 6886 : 0.18560148775577545\n",
      "Training loss for batch 6887 : 0.13679778575897217\n",
      "Training loss for batch 6888 : 0.5261229276657104\n",
      "Training loss for batch 6889 : 0.24125126004219055\n",
      "Training loss for batch 6890 : 0.3299644887447357\n",
      "Training loss for batch 6891 : 0.4996468722820282\n",
      "Training loss for batch 6892 : 0.4035891592502594\n",
      "Training loss for batch 6893 : 0.12912967801094055\n",
      "Training loss for batch 6894 : 0.5840163230895996\n",
      "Training loss for batch 6895 : 0.5034937858581543\n",
      "Training loss for batch 6896 : 0.20836980640888214\n",
      "Training loss for batch 6897 : 0.07622232288122177\n",
      "Training loss for batch 6898 : 0.22188331186771393\n",
      "Training loss for batch 6899 : 0.2847764194011688\n",
      "Training loss for batch 6900 : 0.5043897032737732\n",
      "Training loss for batch 6901 : 0.2443169802427292\n",
      "Training loss for batch 6902 : 0.11111155152320862\n",
      "Training loss for batch 6903 : 0.2556252181529999\n",
      "Training loss for batch 6904 : 0.45505577325820923\n",
      "Training loss for batch 6905 : 0.39683449268341064\n",
      "Training loss for batch 6906 : 0.2914832830429077\n",
      "Training loss for batch 6907 : 0.4431549310684204\n",
      "Training loss for batch 6908 : 0.07206876575946808\n",
      "Training loss for batch 6909 : 0.41996845602989197\n",
      "Training loss for batch 6910 : 0.6625380516052246\n",
      "Training loss for batch 6911 : 0.1992437243461609\n",
      "Training loss for batch 6912 : 0.11713121831417084\n",
      "Training loss for batch 6913 : 0.2954491674900055\n",
      "Training loss for batch 6914 : 0.011401817202568054\n",
      "Training loss for batch 6915 : 0.28981515765190125\n",
      "Training loss for batch 6916 : 0.6927106380462646\n",
      "Training loss for batch 6917 : 0.5659107565879822\n",
      "Training loss for batch 6918 : 0.2994345724582672\n",
      "Training loss for batch 6919 : 0.07040128111839294\n",
      "Training loss for batch 6920 : 0.17760802805423737\n",
      "Training loss for batch 6921 : 0.24736681580543518\n",
      "Training loss for batch 6922 : 0.34211498498916626\n",
      "Training loss for batch 6923 : 0.1973724365234375\n",
      "Training loss for batch 6924 : 0.47976452112197876\n",
      "Training loss for batch 6925 : 0.14734399318695068\n",
      "Training loss for batch 6926 : 0.33713001012802124\n",
      "Training loss for batch 6927 : 0.10365558415651321\n",
      "Training loss for batch 6928 : 0.4568295478820801\n",
      "Training loss for batch 6929 : 0.5236796736717224\n",
      "Training loss for batch 6930 : 0.130961611866951\n",
      "Training loss for batch 6931 : 0.27298519015312195\n",
      "Training loss for batch 6932 : 0.044703055173158646\n",
      "Training loss for batch 6933 : 0.439072847366333\n",
      "Training loss for batch 6934 : 0.39446571469306946\n",
      "Training loss for batch 6935 : 0.058383818715810776\n",
      "Training loss for batch 6936 : 0.4858272075653076\n",
      "Training loss for batch 6937 : 0.1597905457019806\n",
      "Training loss for batch 6938 : 0.1603621542453766\n",
      "Training loss for batch 6939 : 0.49066388607025146\n",
      "Training loss for batch 6940 : 0.6397725939750671\n",
      "Training loss for batch 6941 : 0.42829006910324097\n",
      "Training loss for batch 6942 : 0.18887510895729065\n",
      "Training loss for batch 6943 : 0.35734817385673523\n",
      "Training loss for batch 6944 : 0.8275383710861206\n",
      "Training loss for batch 6945 : 0.4278823435306549\n",
      "Training loss for batch 6946 : 0.128000870347023\n",
      "Training loss for batch 6947 : 0.04543198645114899\n",
      "Training loss for batch 6948 : 0.27302372455596924\n",
      "Training loss for batch 6949 : 0.4092622995376587\n",
      "Training loss for batch 6950 : 0.39924371242523193\n",
      "Training loss for batch 6951 : 0.7427977919578552\n",
      "Training loss for batch 6952 : 0.24776282906532288\n",
      "Training loss for batch 6953 : 0.36398613452911377\n",
      "Training loss for batch 6954 : 0.7236394286155701\n",
      "Training loss for batch 6955 : 0.3619931638240814\n",
      "Training loss for batch 6956 : 0.5249137878417969\n",
      "Training loss for batch 6957 : 0.5627143383026123\n",
      "Training loss for batch 6958 : 0.6081485748291016\n",
      "Training loss for batch 6959 : 0.31542855501174927\n",
      "Training loss for batch 6960 : 0.10502634197473526\n",
      "Training loss for batch 6961 : 0.17147071659564972\n",
      "Training loss for batch 6962 : 0.429370641708374\n",
      "Training loss for batch 6963 : 0.3477698266506195\n",
      "Training loss for batch 6964 : 0.3042558431625366\n",
      "Training loss for batch 6965 : 0.4338805079460144\n",
      "Training loss for batch 6966 : 0.1626087725162506\n",
      "Training loss for batch 6967 : 0.3952483832836151\n",
      "Training loss for batch 6968 : 0.5102635622024536\n",
      "Training loss for batch 6969 : 0.21746313571929932\n",
      "Training loss for batch 6970 : 0.25337907671928406\n",
      "Training loss for batch 6971 : 0.3847562074661255\n",
      "Training loss for batch 6972 : 0.2375982701778412\n",
      "Training loss for batch 6973 : 0.2241632342338562\n",
      "Training loss for batch 6974 : 0.03430570289492607\n",
      "Training loss for batch 6975 : 0.17775270342826843\n",
      "Training loss for batch 6976 : 0.5558054447174072\n",
      "Training loss for batch 6977 : 0.07112418860197067\n",
      "Training loss for batch 6978 : 0.463446706533432\n",
      "Training loss for batch 6979 : 0.6490895748138428\n",
      "Training loss for batch 6980 : 0.42138198018074036\n",
      "Training loss for batch 6981 : 0.37680456042289734\n",
      "Training loss for batch 6982 : 0.6231251955032349\n",
      "Training loss for batch 6983 : 0.4615168273448944\n",
      "Training loss for batch 6984 : 0.5493354201316833\n",
      "Training loss for batch 6985 : 0.3250742554664612\n",
      "Training loss for batch 6986 : 0.3517015278339386\n",
      "Training loss for batch 6987 : 0.07375527918338776\n",
      "Training loss for batch 6988 : 0.02621212601661682\n",
      "Training loss for batch 6989 : 0.6577370166778564\n",
      "Training loss for batch 6990 : 0.11675073951482773\n",
      "Training loss for batch 6991 : 0.3521372675895691\n",
      "Training loss for batch 6992 : 0.2056683897972107\n",
      "Training loss for batch 6993 : 0.1104825884103775\n",
      "Training loss for batch 6994 : 0.1911059319972992\n",
      "Training loss for batch 6995 : 0.3852623999118805\n",
      "Training loss for batch 6996 : 0.4602264165878296\n",
      "Training loss for batch 6997 : 0.26100534200668335\n",
      "Training loss for batch 6998 : 0.13258783519268036\n",
      "Training loss for batch 6999 : 0.3291648328304291\n",
      "Training loss for batch 7000 : 0.162758931517601\n",
      "Training loss for batch 7001 : 0.06330223381519318\n",
      "Training loss for batch 7002 : 0.16016624867916107\n",
      "Training loss for batch 7003 : 0.16557225584983826\n",
      "Training loss for batch 7004 : 0.3093319237232208\n",
      "Training loss for batch 7005 : 0.37475574016571045\n",
      "Training loss for batch 7006 : 0.22283333539962769\n",
      "Training loss for batch 7007 : 0.39235931634902954\n",
      "Training loss for batch 7008 : 0.32345977425575256\n",
      "Training loss for batch 7009 : 0.36044198274612427\n",
      "Training loss for batch 7010 : 0.3985536992549896\n",
      "Training loss for batch 7011 : 0.40496039390563965\n",
      "Training loss for batch 7012 : 0.020096031948924065\n",
      "Training loss for batch 7013 : 0.12732219696044922\n",
      "Training loss for batch 7014 : 0.27211132645606995\n",
      "Training loss for batch 7015 : 0.09202924370765686\n",
      "Training loss for batch 7016 : 0.09448938071727753\n",
      "Training loss for batch 7017 : 0.19099290668964386\n",
      "Training loss for batch 7018 : 0.155552476644516\n",
      "Training loss for batch 7019 : 0.38481250405311584\n",
      "Training loss for batch 7020 : 0.10335845500230789\n",
      "Training loss for batch 7021 : 0.2706294059753418\n",
      "Training loss for batch 7022 : 0.19171206653118134\n",
      "Training loss for batch 7023 : 0.4592200517654419\n",
      "Training loss for batch 7024 : 0.06752315163612366\n",
      "Training loss for batch 7025 : 0.5756058692932129\n",
      "Training loss for batch 7026 : 0.0031504840590059757\n",
      "Training loss for batch 7027 : 0.3641616404056549\n",
      "Training loss for batch 7028 : 0.2286873757839203\n",
      "Training loss for batch 7029 : 0.0\n",
      "Training loss for batch 7030 : 0.07624088227748871\n",
      "Training loss for batch 7031 : 0.5150219798088074\n",
      "Training loss for batch 7032 : 0.6016097068786621\n",
      "Training loss for batch 7033 : 0.11520069092512131\n",
      "Training loss for batch 7034 : 0.1131204217672348\n",
      "Training loss for batch 7035 : 0.004498978611081839\n",
      "Training loss for batch 7036 : 0.5815269351005554\n",
      "Training loss for batch 7037 : 0.14550845324993134\n",
      "Training loss for batch 7038 : 0.15641608834266663\n",
      "Training loss for batch 7039 : 0.5470941066741943\n",
      "Training loss for batch 7040 : 0.30559632182121277\n",
      "Training loss for batch 7041 : 0.15701138973236084\n",
      "Training loss for batch 7042 : 0.2954690158367157\n",
      "Training loss for batch 7043 : 0.8107010722160339\n",
      "Training loss for batch 7044 : 0.2459011822938919\n",
      "Training loss for batch 7045 : 0.6642971038818359\n",
      "Training loss for batch 7046 : 0.3358924388885498\n",
      "Training loss for batch 7047 : 0.10981360822916031\n",
      "Training loss for batch 7048 : 0.15776069462299347\n",
      "Training loss for batch 7049 : 0.19360408186912537\n",
      "Training loss for batch 7050 : 0.42325976490974426\n",
      "Training loss for batch 7051 : 0.028799008578062057\n",
      "Training loss for batch 7052 : 0.36221984028816223\n",
      "Training loss for batch 7053 : 0.20421156287193298\n",
      "Training loss for batch 7054 : 0.1352950632572174\n",
      "Training loss for batch 7055 : 0.16657792031764984\n",
      "Training loss for batch 7056 : 0.09262847900390625\n",
      "Training loss for batch 7057 : 0.0280341487377882\n",
      "Training loss for batch 7058 : 0.32658132910728455\n",
      "Training loss for batch 7059 : 0.0379423126578331\n",
      "Training loss for batch 7060 : 0.7135870456695557\n",
      "Training loss for batch 7061 : 0.10067883133888245\n",
      "Training loss for batch 7062 : 0.1716756522655487\n",
      "Training loss for batch 7063 : 0.11307718604803085\n",
      "Training loss for batch 7064 : 0.5649160146713257\n",
      "Training loss for batch 7065 : 0.37252533435821533\n",
      "Training loss for batch 7066 : 0.07412078976631165\n",
      "Training loss for batch 7067 : 0.2606886923313141\n",
      "Training loss for batch 7068 : 0.18592488765716553\n",
      "Training loss for batch 7069 : 0.19437454640865326\n",
      "Training loss for batch 7070 : 0.41269445419311523\n",
      "Training loss for batch 7071 : 0.1801074743270874\n",
      "Training loss for batch 7072 : 0.17910243570804596\n",
      "Training loss for batch 7073 : 0.1922164410352707\n",
      "Training loss for batch 7074 : 0.08123321086168289\n",
      "Training loss for batch 7075 : 0.35404253005981445\n",
      "Training loss for batch 7076 : 0.529076874256134\n",
      "Training loss for batch 7077 : 0.051726073026657104\n",
      "Training loss for batch 7078 : 0.19028818607330322\n",
      "Training loss for batch 7079 : 0.06093114987015724\n",
      "Training loss for batch 7080 : 0.0946805328130722\n",
      "Training loss for batch 7081 : 0.28265050053596497\n",
      "Training loss for batch 7082 : 0.061886582523584366\n",
      "Training loss for batch 7083 : 0.44070786237716675\n",
      "Training loss for batch 7084 : 0.3224116265773773\n",
      "Training loss for batch 7085 : 0.06311166286468506\n",
      "Training loss for batch 7086 : 0.15451429784297943\n",
      "Training loss for batch 7087 : 0.3803592622280121\n",
      "Training loss for batch 7088 : 0.3654961884021759\n",
      "Training loss for batch 7089 : 0.7092416882514954\n",
      "Training loss for batch 7090 : 0.22340264916419983\n",
      "Training loss for batch 7091 : 0.33827289938926697\n",
      "Training loss for batch 7092 : 0.13495558500289917\n",
      "Training loss for batch 7093 : 0.011885315179824829\n",
      "Training loss for batch 7094 : 0.5060784816741943\n",
      "Training loss for batch 7095 : 0.4457660913467407\n",
      "Training loss for batch 7096 : 0.3332483172416687\n",
      "Training loss for batch 7097 : 0.4618862271308899\n",
      "Training loss for batch 7098 : 0.1710520088672638\n",
      "Training loss for batch 7099 : 0.41810521483421326\n",
      "Training loss for batch 7100 : 0.39767393469810486\n",
      "Training loss for batch 7101 : 0.2546195387840271\n",
      "Training loss for batch 7102 : 0.3227154314517975\n",
      "Training loss for batch 7103 : 0.34047192335128784\n",
      "Training loss for batch 7104 : 0.46517592668533325\n",
      "Training loss for batch 7105 : 0.12894974648952484\n",
      "Training loss for batch 7106 : 0.4134141504764557\n",
      "Training loss for batch 7107 : 0.08099676668643951\n",
      "Training loss for batch 7108 : 0.015279432758688927\n",
      "Training loss for batch 7109 : 0.5589913725852966\n",
      "Training loss for batch 7110 : 0.7568495273590088\n",
      "Training loss for batch 7111 : 0.21798308193683624\n",
      "Training loss for batch 7112 : 0.3186478912830353\n",
      "Training loss for batch 7113 : 0.09270701557397842\n",
      "Training loss for batch 7114 : 0.2917405366897583\n",
      "Training loss for batch 7115 : 0.15566927194595337\n",
      "Training loss for batch 7116 : 0.060591284185647964\n",
      "Training loss for batch 7117 : 0.2548027038574219\n",
      "Training loss for batch 7118 : 0.48793748021125793\n",
      "Training loss for batch 7119 : 0.14491544663906097\n",
      "Training loss for batch 7120 : 0.04825759679079056\n",
      "Training loss for batch 7121 : 0.4532054662704468\n",
      "Training loss for batch 7122 : 0.5648307800292969\n",
      "Training loss for batch 7123 : 0.47877222299575806\n",
      "Training loss for batch 7124 : 0.38120877742767334\n",
      "Training loss for batch 7125 : 0.2402380257844925\n",
      "Training loss for batch 7126 : 0.3225421905517578\n",
      "Training loss for batch 7127 : 0.12878461182117462\n",
      "Training loss for batch 7128 : 0.2706528306007385\n",
      "Training loss for batch 7129 : 0.3507508933544159\n",
      "Training loss for batch 7130 : 0.5820069313049316\n",
      "Training loss for batch 7131 : 0.013674654066562653\n",
      "Training loss for batch 7132 : 0.31153759360313416\n",
      "Training loss for batch 7133 : 0.0956733301281929\n",
      "Training loss for batch 7134 : 0.04673875868320465\n",
      "Training loss for batch 7135 : 0.11343453079462051\n",
      "Training loss for batch 7136 : 0.5031761527061462\n",
      "Training loss for batch 7137 : 0.6263575553894043\n",
      "Training loss for batch 7138 : 0.5681822299957275\n",
      "Training loss for batch 7139 : 0.3788118362426758\n",
      "Training loss for batch 7140 : 0.22957132756710052\n",
      "Training loss for batch 7141 : 0.24856272339820862\n",
      "Training loss for batch 7142 : 0.11538869142532349\n",
      "Training loss for batch 7143 : 0.23161086440086365\n",
      "Training loss for batch 7144 : 0.1922713816165924\n",
      "Training loss for batch 7145 : 0.2851622998714447\n",
      "Training loss for batch 7146 : 0.6135432720184326\n",
      "Training loss for batch 7147 : 0.49935802817344666\n",
      "Training loss for batch 7148 : 0.09037323296070099\n",
      "Training loss for batch 7149 : 0.2617267072200775\n",
      "Training loss for batch 7150 : 0.4912666082382202\n",
      "Training loss for batch 7151 : 0.06952977180480957\n",
      "Training loss for batch 7152 : 0.6742544174194336\n",
      "Training loss for batch 7153 : 0.015421992167830467\n",
      "Training loss for batch 7154 : 0.3683716356754303\n",
      "Training loss for batch 7155 : 0.6434562802314758\n",
      "Training loss for batch 7156 : 0.6262252926826477\n",
      "Training loss for batch 7157 : 0.09703253209590912\n",
      "Training loss for batch 7158 : 0.3601028621196747\n",
      "Training loss for batch 7159 : 0.7891034483909607\n",
      "Training loss for batch 7160 : 0.17231746017932892\n",
      "Training loss for batch 7161 : 0.22728639841079712\n",
      "Training loss for batch 7162 : 0.4889647960662842\n",
      "Training loss for batch 7163 : 0.38027331233024597\n",
      "Training loss for batch 7164 : 0.07943367958068848\n",
      "Training loss for batch 7165 : 0.7176679372787476\n",
      "Training loss for batch 7166 : 0.07718539983034134\n",
      "Training loss for batch 7167 : 0.3681592047214508\n",
      "Training loss for batch 7168 : 0.006082435604184866\n",
      "Training loss for batch 7169 : 0.47353997826576233\n",
      "Training loss for batch 7170 : 0.23020882904529572\n",
      "Training loss for batch 7171 : 0.14758358895778656\n",
      "Training loss for batch 7172 : 0.4357259273529053\n",
      "Training loss for batch 7173 : 0.035490263253450394\n",
      "Training loss for batch 7174 : 0.32124078273773193\n",
      "Training loss for batch 7175 : 0.1692749261856079\n",
      "Training loss for batch 7176 : 0.4481847286224365\n",
      "Training loss for batch 7177 : 0.24939504265785217\n",
      "Training loss for batch 7178 : 0.4451703131198883\n",
      "Training loss for batch 7179 : 0.5482038259506226\n",
      "Training loss for batch 7180 : 0.10521569103002548\n",
      "Training loss for batch 7181 : 0.4636283218860626\n",
      "Training loss for batch 7182 : 0.37185490131378174\n",
      "Training loss for batch 7183 : 0.2867187559604645\n",
      "Training loss for batch 7184 : 0.3112271726131439\n",
      "Training loss for batch 7185 : 0.10042339563369751\n",
      "Training loss for batch 7186 : 0.28763049840927124\n",
      "Training loss for batch 7187 : 0.4027193784713745\n",
      "Training loss for batch 7188 : 0.5352472066879272\n",
      "Training loss for batch 7189 : 0.23305630683898926\n",
      "Training loss for batch 7190 : 0.31988123059272766\n",
      "Training loss for batch 7191 : 0.3779461681842804\n",
      "Training loss for batch 7192 : 0.13847778737545013\n",
      "Training loss for batch 7193 : 0.33025771379470825\n",
      "Training loss for batch 7194 : 0.2278072088956833\n",
      "Training loss for batch 7195 : 0.11949733644723892\n",
      "Training loss for batch 7196 : 0.29658713936805725\n",
      "Training loss for batch 7197 : 0.14315669238567352\n",
      "Training loss for batch 7198 : 0.11317319422960281\n",
      "Training loss for batch 7199 : 0.9379831552505493\n",
      "Training loss for batch 7200 : 0.15072426199913025\n",
      "Training loss for batch 7201 : 0.047200966626405716\n",
      "Training loss for batch 7202 : 0.07125977426767349\n",
      "Training loss for batch 7203 : 0.23913738131523132\n",
      "Training loss for batch 7204 : 0.4372945725917816\n",
      "Training loss for batch 7205 : 0.1759435087442398\n",
      "Training loss for batch 7206 : 0.5351325869560242\n",
      "Training loss for batch 7207 : 0.257514625787735\n",
      "Training loss for batch 7208 : 0.3129274547100067\n",
      "Training loss for batch 7209 : 0.39504164457321167\n",
      "Training loss for batch 7210 : 0.049159564077854156\n",
      "Training loss for batch 7211 : 0.4134978950023651\n",
      "Training loss for batch 7212 : 0.4444790184497833\n",
      "Training loss for batch 7213 : 0.16702446341514587\n",
      "Training loss for batch 7214 : 0.3757670521736145\n",
      "Training loss for batch 7215 : 0.20535166561603546\n",
      "Training loss for batch 7216 : 0.16007813811302185\n",
      "Training loss for batch 7217 : 0.48608601093292236\n",
      "Training loss for batch 7218 : 0.13696688413619995\n",
      "Training loss for batch 7219 : 0.21685180068016052\n",
      "Training loss for batch 7220 : 0.555743932723999\n",
      "Training loss for batch 7221 : 0.616373598575592\n",
      "Training loss for batch 7222 : 0.38003838062286377\n",
      "Training loss for batch 7223 : 0.49830350279808044\n",
      "Training loss for batch 7224 : 0.11913160234689713\n",
      "Training loss for batch 7225 : 0.410449743270874\n",
      "Training loss for batch 7226 : 0.09858958423137665\n",
      "Training loss for batch 7227 : 0.6562071442604065\n",
      "Training loss for batch 7228 : 0.14378434419631958\n",
      "Training loss for batch 7229 : 0.36501967906951904\n",
      "Training loss for batch 7230 : 0.2709808051586151\n",
      "Training loss for batch 7231 : 0.05399201065301895\n",
      "Training loss for batch 7232 : 0.5030467510223389\n",
      "Training loss for batch 7233 : 0.18364426493644714\n",
      "Training loss for batch 7234 : 0.7093743085861206\n",
      "Training loss for batch 7235 : 0.2886076867580414\n",
      "Training loss for batch 7236 : 0.36078718304634094\n",
      "Training loss for batch 7237 : 0.13332413136959076\n",
      "Training loss for batch 7238 : 0.29525983333587646\n",
      "Training loss for batch 7239 : 0.3484104871749878\n",
      "Training loss for batch 7240 : 0.24086903035640717\n",
      "Training loss for batch 7241 : 0.4298752546310425\n",
      "Training loss for batch 7242 : 0.22464513778686523\n",
      "Training loss for batch 7243 : 0.4816383123397827\n",
      "Training loss for batch 7244 : 0.23924605548381805\n",
      "Training loss for batch 7245 : 0.2550017535686493\n",
      "Training loss for batch 7246 : 0.16501064598560333\n",
      "Training loss for batch 7247 : 0.163103848695755\n",
      "Training loss for batch 7248 : 0.44121769070625305\n",
      "Training loss for batch 7249 : 0.2840111255645752\n",
      "Training loss for batch 7250 : 0.4948257803916931\n",
      "Training loss for batch 7251 : 0.6611176133155823\n",
      "Training loss for batch 7252 : 0.08452953398227692\n",
      "Training loss for batch 7253 : 0.19943983852863312\n",
      "Training loss for batch 7254 : 0.786125898361206\n",
      "Training loss for batch 7255 : 0.4696663022041321\n",
      "Training loss for batch 7256 : 0.3159390687942505\n",
      "Training loss for batch 7257 : 0.2549271881580353\n",
      "Training loss for batch 7258 : 0.22372373938560486\n",
      "Training loss for batch 7259 : 0.29499325156211853\n",
      "Training loss for batch 7260 : 0.16197265684604645\n",
      "Training loss for batch 7261 : 0.1638868898153305\n",
      "Training loss for batch 7262 : 0.1827150285243988\n",
      "Training loss for batch 7263 : 0.5082109570503235\n",
      "Training loss for batch 7264 : 0.24486525356769562\n",
      "Training loss for batch 7265 : 0.4643595516681671\n",
      "Training loss for batch 7266 : 0.2973918318748474\n",
      "Training loss for batch 7267 : 0.3323793113231659\n",
      "Training loss for batch 7268 : 0.20256982743740082\n",
      "Training loss for batch 7269 : 0.08795294910669327\n",
      "Training loss for batch 7270 : 0.2961139678955078\n",
      "Training loss for batch 7271 : 0.23220203816890717\n",
      "Training loss for batch 7272 : 0.3300621509552002\n",
      "Training loss for batch 7273 : 0.24518196284770966\n",
      "Training loss for batch 7274 : 0.21966426074504852\n",
      "Training loss for batch 7275 : 0.191708043217659\n",
      "Training loss for batch 7276 : 0.04125083610415459\n",
      "Training loss for batch 7277 : 0.17369472980499268\n",
      "Training loss for batch 7278 : 0.20167960226535797\n",
      "Training loss for batch 7279 : 0.047550465911626816\n",
      "Training loss for batch 7280 : 0.018415462225675583\n",
      "Training loss for batch 7281 : 0.33443596959114075\n",
      "Training loss for batch 7282 : 0.25435662269592285\n",
      "Training loss for batch 7283 : 0.458783894777298\n",
      "Training loss for batch 7284 : 0.33883628249168396\n",
      "Training loss for batch 7285 : 0.04320706054568291\n",
      "Training loss for batch 7286 : 0.23761843144893646\n",
      "Training loss for batch 7287 : 0.03608883172273636\n",
      "Training loss for batch 7288 : 0.19048988819122314\n",
      "Training loss for batch 7289 : 0.11327151209115982\n",
      "Training loss for batch 7290 : 0.17716914415359497\n",
      "Training loss for batch 7291 : 0.4526728689670563\n",
      "Training loss for batch 7292 : 0.20603954792022705\n",
      "Training loss for batch 7293 : 0.4231823682785034\n",
      "Training loss for batch 7294 : 0.2465219646692276\n",
      "Training loss for batch 7295 : 0.2762245535850525\n",
      "Training loss for batch 7296 : 0.22966070473194122\n",
      "Training loss for batch 7297 : 0.03987225517630577\n",
      "Training loss for batch 7298 : 0.27247995138168335\n",
      "Training loss for batch 7299 : 0.5265750885009766\n",
      "Training loss for batch 7300 : 0.16069534420967102\n",
      "Training loss for batch 7301 : 0.3419859707355499\n",
      "Training loss for batch 7302 : 0.4195743203163147\n",
      "Training loss for batch 7303 : 0.24824589490890503\n",
      "Training loss for batch 7304 : 0.28384929895401\n",
      "Training loss for batch 7305 : 0.030175019055604935\n",
      "Training loss for batch 7306 : 0.40541690587997437\n",
      "Training loss for batch 7307 : 0.21550358831882477\n",
      "Training loss for batch 7308 : 0.22836805880069733\n",
      "Training loss for batch 7309 : 0.48996788263320923\n",
      "Training loss for batch 7310 : 0.06453462690114975\n",
      "Training loss for batch 7311 : 0.07570301741361618\n",
      "Training loss for batch 7312 : 0.4684739112854004\n",
      "Training loss for batch 7313 : 0.02738851308822632\n",
      "Training loss for batch 7314 : 0.045753125101327896\n",
      "Training loss for batch 7315 : 0.34370890259742737\n",
      "Training loss for batch 7316 : 0.14433632791042328\n",
      "Training loss for batch 7317 : 0.534848153591156\n",
      "Training loss for batch 7318 : 0.3792434334754944\n",
      "Training loss for batch 7319 : 0.05691380798816681\n",
      "Training loss for batch 7320 : 0.17720773816108704\n",
      "Training loss for batch 7321 : 0.5960096716880798\n",
      "Training loss for batch 7322 : 0.18650448322296143\n",
      "Training loss for batch 7323 : 0.3568624258041382\n",
      "Training loss for batch 7324 : 0.15075941383838654\n",
      "Training loss for batch 7325 : 0.5798710584640503\n",
      "Training loss for batch 7326 : 0.6502072215080261\n",
      "Training loss for batch 7327 : 0.22064293920993805\n",
      "Training loss for batch 7328 : 0.10215292870998383\n",
      "Training loss for batch 7329 : 0.2882622480392456\n",
      "Training loss for batch 7330 : 0.11153216660022736\n",
      "Training loss for batch 7331 : 0.7096319198608398\n",
      "Training loss for batch 7332 : 0.15115530788898468\n",
      "Training loss for batch 7333 : 0.013913494534790516\n",
      "Training loss for batch 7334 : 0.32157522439956665\n",
      "Training loss for batch 7335 : 0.042898502200841904\n",
      "Training loss for batch 7336 : 0.5936078429222107\n",
      "Training loss for batch 7337 : 0.2527883052825928\n",
      "Training loss for batch 7338 : 0.09658623486757278\n",
      "Training loss for batch 7339 : 0.16090792417526245\n",
      "Training loss for batch 7340 : 0.3047618865966797\n",
      "Training loss for batch 7341 : 0.11980108916759491\n",
      "Training loss for batch 7342 : 0.5126677751541138\n",
      "Training loss for batch 7343 : 0.10395132750272751\n",
      "Training loss for batch 7344 : 0.2992948889732361\n",
      "Training loss for batch 7345 : 0.2999841272830963\n",
      "Training loss for batch 7346 : 0.06880827248096466\n",
      "Training loss for batch 7347 : 0.478752076625824\n",
      "Training loss for batch 7348 : 0.3073464334011078\n",
      "Training loss for batch 7349 : 0.8563749194145203\n",
      "Training loss for batch 7350 : 0.12924131751060486\n",
      "Training loss for batch 7351 : 0.416822224855423\n",
      "Training loss for batch 7352 : 0.28635576367378235\n",
      "Training loss for batch 7353 : 0.4336312413215637\n",
      "Training loss for batch 7354 : 0.40860044956207275\n",
      "Training loss for batch 7355 : 0.32286354899406433\n",
      "Training loss for batch 7356 : 0.18596187233924866\n",
      "Training loss for batch 7357 : 0.3908253610134125\n",
      "Training loss for batch 7358 : 0.394773006439209\n",
      "Training loss for batch 7359 : 0.1286313533782959\n",
      "Training loss for batch 7360 : 0.13817884027957916\n",
      "Training loss for batch 7361 : 0.0873061791062355\n",
      "Training loss for batch 7362 : 0.2561327815055847\n",
      "Training loss for batch 7363 : 0.25436604022979736\n",
      "Training loss for batch 7364 : 0.4304298758506775\n",
      "Training loss for batch 7365 : 0.09839598834514618\n",
      "Training loss for batch 7366 : 0.24879911541938782\n",
      "Training loss for batch 7367 : 0.130575031042099\n",
      "Training loss for batch 7368 : 0.2992354929447174\n",
      "Training loss for batch 7369 : 0.07723713666200638\n",
      "Training loss for batch 7370 : 0.3702894449234009\n",
      "Training loss for batch 7371 : 0.1743708997964859\n",
      "Training loss for batch 7372 : 0.03579038754105568\n",
      "Training loss for batch 7373 : 0.7044011354446411\n",
      "Training loss for batch 7374 : 0.005591413471847773\n",
      "Training loss for batch 7375 : 0.7932220101356506\n",
      "Training loss for batch 7376 : 0.3300122916698456\n",
      "Training loss for batch 7377 : 0.09339705109596252\n",
      "Training loss for batch 7378 : 0.04994681477546692\n",
      "Training loss for batch 7379 : 0.03820611163973808\n",
      "Training loss for batch 7380 : 0.5085117816925049\n",
      "Training loss for batch 7381 : 0.027455873787403107\n",
      "Training loss for batch 7382 : 0.09387831389904022\n",
      "Training loss for batch 7383 : 0.36965155601501465\n",
      "Training loss for batch 7384 : 0.1420125961303711\n",
      "Training loss for batch 7385 : 0.5506113171577454\n",
      "Training loss for batch 7386 : 0.500869631767273\n",
      "Training loss for batch 7387 : 0.14028199017047882\n",
      "Training loss for batch 7388 : 0.3957514464855194\n",
      "Training loss for batch 7389 : 0.423187792301178\n",
      "Training loss for batch 7390 : 0.24807590246200562\n",
      "Training loss for batch 7391 : 0.1639575958251953\n",
      "Training loss for batch 7392 : 0.12296057492494583\n",
      "Training loss for batch 7393 : 0.6457065343856812\n",
      "Training loss for batch 7394 : 0.11043998599052429\n",
      "Training loss for batch 7395 : 0.08967982977628708\n",
      "Training loss for batch 7396 : 0.565165102481842\n",
      "Training loss for batch 7397 : 0.28876349329948425\n",
      "Training loss for batch 7398 : 0.44253185391426086\n",
      "Training loss for batch 7399 : 0.15225821733474731\n",
      "Training loss for batch 7400 : 0.056224748492240906\n",
      "Training loss for batch 7401 : 0.05833122134208679\n",
      "Training loss for batch 7402 : 0.0\n",
      "Training loss for batch 7403 : 0.28246814012527466\n",
      "Training loss for batch 7404 : 0.4633643925189972\n",
      "Training loss for batch 7405 : 0.04310321807861328\n",
      "Training loss for batch 7406 : 0.37923377752304077\n",
      "Training loss for batch 7407 : 0.18027466535568237\n",
      "Training loss for batch 7408 : 0.08532789349555969\n",
      "Training loss for batch 7409 : 0.3241671919822693\n",
      "Training loss for batch 7410 : 0.3376566767692566\n",
      "Training loss for batch 7411 : 0.04971224069595337\n",
      "Training loss for batch 7412 : 0.32275980710983276\n",
      "Training loss for batch 7413 : 0.2784788906574249\n",
      "Training loss for batch 7414 : 0.561010479927063\n",
      "Training loss for batch 7415 : 0.04764577001333237\n",
      "Training loss for batch 7416 : 0.6820478439331055\n",
      "Training loss for batch 7417 : 0.031777556985616684\n",
      "Training loss for batch 7418 : 0.23985007405281067\n",
      "Training loss for batch 7419 : 0.30237266421318054\n",
      "Training loss for batch 7420 : 0.09243369847536087\n",
      "Training loss for batch 7421 : 0.17595049738883972\n",
      "Training loss for batch 7422 : 0.11927624046802521\n",
      "Training loss for batch 7423 : 0.2656027674674988\n",
      "Training loss for batch 7424 : 0.2457636594772339\n",
      "Training loss for batch 7425 : 0.13812454044818878\n",
      "Training loss for batch 7426 : 0.22508025169372559\n",
      "Training loss for batch 7427 : 0.16423483192920685\n",
      "Training loss for batch 7428 : 0.1344178169965744\n",
      "Training loss for batch 7429 : 0.1437983512878418\n",
      "Training loss for batch 7430 : 0.6297991871833801\n",
      "Training loss for batch 7431 : 0.2433541715145111\n",
      "Training loss for batch 7432 : 0.6176565289497375\n",
      "Training loss for batch 7433 : 0.19720524549484253\n",
      "Training loss for batch 7434 : 0.13946400582790375\n",
      "Training loss for batch 7435 : 0.27514854073524475\n",
      "Training loss for batch 7436 : 0.12609867751598358\n",
      "Training loss for batch 7437 : 0.062218550592660904\n",
      "Training loss for batch 7438 : 0.3397442102432251\n",
      "Training loss for batch 7439 : 0.8537951111793518\n",
      "Training loss for batch 7440 : 0.19425931572914124\n",
      "Training loss for batch 7441 : 0.04112546518445015\n",
      "Training loss for batch 7442 : 0.05622011423110962\n",
      "Training loss for batch 7443 : 0.6375730633735657\n",
      "Training loss for batch 7444 : 0.3704853951931\n",
      "Training loss for batch 7445 : 0.5708437561988831\n",
      "Training loss for batch 7446 : 0.40152400732040405\n",
      "Training loss for batch 7447 : 0.5261136293411255\n",
      "Training loss for batch 7448 : 0.19132301211357117\n",
      "Training loss for batch 7449 : 0.5199365615844727\n",
      "Training loss for batch 7450 : 0.20371964573860168\n",
      "Training loss for batch 7451 : 0.15596330165863037\n",
      "Training loss for batch 7452 : 0.09827723354101181\n",
      "Training loss for batch 7453 : 0.2606244385242462\n",
      "Training loss for batch 7454 : 0.20330260694026947\n",
      "Training loss for batch 7455 : 0.3229002356529236\n",
      "Training loss for batch 7456 : 0.34558627009391785\n",
      "Training loss for batch 7457 : 0.4547666311264038\n",
      "Training loss for batch 7458 : 0.31190353631973267\n",
      "Training loss for batch 7459 : 0.17318664491176605\n",
      "Training loss for batch 7460 : 0.6393241286277771\n",
      "Training loss for batch 7461 : 0.6568710803985596\n",
      "Training loss for batch 7462 : 0.14394605159759521\n",
      "Training loss for batch 7463 : 0.1518539935350418\n",
      "Training loss for batch 7464 : 0.6352312564849854\n",
      "Training loss for batch 7465 : 0.4872673451900482\n",
      "Training loss for batch 7466 : 0.2974434792995453\n",
      "Training loss for batch 7467 : 0.20198297500610352\n",
      "Training loss for batch 7468 : 0.30690279603004456\n",
      "Training loss for batch 7469 : 0.47355228662490845\n",
      "Training loss for batch 7470 : 0.2916323244571686\n",
      "Training loss for batch 7471 : 0.32739701867103577\n",
      "Training loss for batch 7472 : 0.0799792930483818\n",
      "Training loss for batch 7473 : 0.5469745993614197\n",
      "Training loss for batch 7474 : 0.5159695148468018\n",
      "Training loss for batch 7475 : 0.02557418867945671\n",
      "Training loss for batch 7476 : 0.2986181676387787\n",
      "Training loss for batch 7477 : 0.2543226182460785\n",
      "Training loss for batch 7478 : 0.002840349916368723\n",
      "Training loss for batch 7479 : 0.12565839290618896\n",
      "Training loss for batch 7480 : 0.20385418832302094\n",
      "Training loss for batch 7481 : 0.17740489542484283\n",
      "Training loss for batch 7482 : 0.3254241645336151\n",
      "Training loss for batch 7483 : 0.18581430613994598\n",
      "Training loss for batch 7484 : 0.23488375544548035\n",
      "Training loss for batch 7485 : 0.12844067811965942\n",
      "Training loss for batch 7486 : 0.1860044151544571\n",
      "Training loss for batch 7487 : 0.6270148158073425\n",
      "Training loss for batch 7488 : 0.08135852962732315\n",
      "Training loss for batch 7489 : 0.5100059509277344\n",
      "Training loss for batch 7490 : 0.5150287747383118\n",
      "Training loss for batch 7491 : 0.11338705569505692\n",
      "Training loss for batch 7492 : 0.9192432761192322\n",
      "Training loss for batch 7493 : 0.2948552668094635\n",
      "Training loss for batch 7494 : 0.05341460928320885\n",
      "Training loss for batch 7495 : 0.20085890591144562\n",
      "Training loss for batch 7496 : 0.6513459086418152\n",
      "Training loss for batch 7497 : 0.49996647238731384\n",
      "Training loss for batch 7498 : 0.3914449214935303\n",
      "Training loss for batch 7499 : 0.22046983242034912\n",
      "Training loss for batch 7500 : 0.23369678854942322\n",
      "Training loss for batch 7501 : 0.3284650146961212\n",
      "Training loss for batch 7502 : 0.10489881783723831\n",
      "Training loss for batch 7503 : 0.26344725489616394\n",
      "Training loss for batch 7504 : 0.2161927968263626\n",
      "Training loss for batch 7505 : 0.5451878905296326\n",
      "Training loss for batch 7506 : 0.340644896030426\n",
      "Training loss for batch 7507 : 0.05979137122631073\n",
      "Training loss for batch 7508 : 0.07875283062458038\n",
      "Training loss for batch 7509 : 0.2563399374485016\n",
      "Training loss for batch 7510 : 0.16625289618968964\n",
      "Training loss for batch 7511 : 0.3161243796348572\n",
      "Training loss for batch 7512 : 0.09007478505373001\n",
      "Training loss for batch 7513 : 0.1949307769536972\n",
      "Training loss for batch 7514 : 0.2064986526966095\n",
      "Training loss for batch 7515 : 0.21151548624038696\n",
      "Training loss for batch 7516 : 0.14866860210895538\n",
      "Training loss for batch 7517 : 0.2630692720413208\n",
      "Training loss for batch 7518 : 0.30123913288116455\n",
      "Training loss for batch 7519 : 0.2810165286064148\n",
      "Training loss for batch 7520 : 0.07260475307703018\n",
      "Training loss for batch 7521 : 0.21234773099422455\n",
      "Training loss for batch 7522 : 0.29676997661590576\n",
      "Training loss for batch 7523 : 0.2872590124607086\n",
      "Training loss for batch 7524 : 0.4807455539703369\n",
      "Training loss for batch 7525 : 0.4713718891143799\n",
      "Training loss for batch 7526 : 0.3044239282608032\n",
      "Training loss for batch 7527 : 0.2823452055454254\n",
      "Training loss for batch 7528 : 0.009626226499676704\n",
      "Training loss for batch 7529 : 0.39137887954711914\n",
      "Training loss for batch 7530 : 0.5259492993354797\n",
      "Training loss for batch 7531 : 0.5889574289321899\n",
      "Training loss for batch 7532 : 0.1563817411661148\n",
      "Training loss for batch 7533 : 0.11446287482976913\n",
      "Training loss for batch 7534 : 0.2675516903400421\n",
      "Training loss for batch 7535 : 0.28727078437805176\n",
      "Training loss for batch 7536 : 0.18147240579128265\n",
      "Training loss for batch 7537 : 0.3989914357662201\n",
      "Training loss for batch 7538 : 0.5098326802253723\n",
      "Training loss for batch 7539 : 0.3256794810295105\n",
      "Training loss for batch 7540 : 0.17104260623455048\n",
      "Training loss for batch 7541 : 0.5703278183937073\n",
      "Training loss for batch 7542 : 0.1780431717634201\n",
      "Training loss for batch 7543 : 0.1709360033273697\n",
      "Training loss for batch 7544 : 0.43590617179870605\n",
      "Training loss for batch 7545 : 0.10665521025657654\n",
      "Training loss for batch 7546 : 0.5244420170783997\n",
      "Training loss for batch 7547 : 0.22472719848155975\n",
      "Training loss for batch 7548 : 0.23834849894046783\n",
      "Training loss for batch 7549 : 0.2593914568424225\n",
      "Training loss for batch 7550 : 0.11852754652500153\n",
      "Training loss for batch 7551 : 0.12922731041908264\n",
      "Training loss for batch 7552 : 0.19369076192378998\n",
      "Training loss for batch 7553 : 0.1316666603088379\n",
      "Training loss for batch 7554 : 0.9385829567909241\n",
      "Training loss for batch 7555 : 0.45998746156692505\n",
      "Training loss for batch 7556 : 0.24073444306850433\n",
      "Training loss for batch 7557 : 0.07151467353105545\n",
      "Training loss for batch 7558 : 0.05907413363456726\n",
      "Training loss for batch 7559 : 0.17281915247440338\n",
      "Training loss for batch 7560 : 0.10082750022411346\n",
      "Training loss for batch 7561 : 0.3435981273651123\n",
      "Training loss for batch 7562 : 0.3774506151676178\n",
      "Training loss for batch 7563 : 0.39441031217575073\n",
      "Training loss for batch 7564 : 0.2361544370651245\n",
      "Training loss for batch 7565 : 0.058669667690992355\n",
      "Training loss for batch 7566 : 0.3711661696434021\n",
      "Training loss for batch 7567 : 0.13966014981269836\n",
      "Training loss for batch 7568 : 0.46646544337272644\n",
      "Training loss for batch 7569 : 0.26855817437171936\n",
      "Training loss for batch 7570 : 0.33349937200546265\n",
      "Training loss for batch 7571 : 0.11111622303724289\n",
      "Training loss for batch 7572 : 0.3458840847015381\n",
      "Training loss for batch 7573 : 0.42506757378578186\n",
      "Training loss for batch 7574 : 0.07066415995359421\n",
      "Training loss for batch 7575 : 0.22843889892101288\n",
      "Training loss for batch 7576 : 0.4895220696926117\n",
      "Training loss for batch 7577 : 0.21583691239356995\n",
      "Training loss for batch 7578 : 0.4519076943397522\n",
      "Training loss for batch 7579 : 0.11919951438903809\n",
      "Training loss for batch 7580 : 0.31430116295814514\n",
      "Training loss for batch 7581 : 0.411870539188385\n",
      "Training loss for batch 7582 : 0.32221317291259766\n",
      "Training loss for batch 7583 : 0.08990737795829773\n",
      "Training loss for batch 7584 : 0.2611924707889557\n",
      "Training loss for batch 7585 : 0.39723649621009827\n",
      "Training loss for batch 7586 : 0.4156503975391388\n",
      "Training loss for batch 7587 : 0.20497626066207886\n",
      "Training loss for batch 7588 : 0.7168212532997131\n",
      "Training loss for batch 7589 : 0.42262744903564453\n",
      "Training loss for batch 7590 : 0.29503151774406433\n",
      "Training loss for batch 7591 : 0.6104962229728699\n",
      "Training loss for batch 7592 : 0.25883620977401733\n",
      "Training loss for batch 7593 : 0.4893818795681\n",
      "Training loss for batch 7594 : 0.21791383624076843\n",
      "Training loss for batch 7595 : 0.2756379246711731\n",
      "Training loss for batch 7596 : 0.34837934374809265\n",
      "Training loss for batch 7597 : 0.20058415830135345\n",
      "Training loss for batch 7598 : 0.0802237018942833\n",
      "Training loss for batch 7599 : 0.06968605518341064\n",
      "Training loss for batch 7600 : 0.5016927123069763\n",
      "Training loss for batch 7601 : 0.20777201652526855\n",
      "Training loss for batch 7602 : 0.23531374335289001\n",
      "Training loss for batch 7603 : 0.18908441066741943\n",
      "Training loss for batch 7604 : 0.5286202430725098\n",
      "Training loss for batch 7605 : 0.07455316185951233\n",
      "Training loss for batch 7606 : 0.05719805881381035\n",
      "Training loss for batch 7607 : 0.43733203411102295\n",
      "Training loss for batch 7608 : 0.612136721611023\n",
      "Training loss for batch 7609 : 0.3337794542312622\n",
      "Training loss for batch 7610 : 0.10295001417398453\n",
      "Training loss for batch 7611 : 0.5018248558044434\n",
      "Training loss for batch 7612 : 0.36796024441719055\n",
      "Training loss for batch 7613 : 0.04684397578239441\n",
      "Training loss for batch 7614 : 0.4161396920681\n",
      "Training loss for batch 7615 : 0.4538148045539856\n",
      "Training loss for batch 7616 : 0.362595796585083\n",
      "Training loss for batch 7617 : 0.11967238783836365\n",
      "Training loss for batch 7618 : 0.3445236086845398\n",
      "Training loss for batch 7619 : 0.5139244198799133\n",
      "Training loss for batch 7620 : 0.18460190296173096\n",
      "Training loss for batch 7621 : 0.4677680730819702\n",
      "Training loss for batch 7622 : 0.14810776710510254\n",
      "Training loss for batch 7623 : 0.37797683477401733\n",
      "Training loss for batch 7624 : 0.14032140374183655\n",
      "Training loss for batch 7625 : 0.48877421021461487\n",
      "Training loss for batch 7626 : 0.0819287821650505\n",
      "Training loss for batch 7627 : 0.33069518208503723\n",
      "Training loss for batch 7628 : 0.3042052388191223\n",
      "Training loss for batch 7629 : 0.3294672966003418\n",
      "Training loss for batch 7630 : 0.3563917875289917\n",
      "Training loss for batch 7631 : 0.394716739654541\n",
      "Training loss for batch 7632 : 0.24116359651088715\n",
      "Training loss for batch 7633 : 0.25916168093681335\n",
      "Training loss for batch 7634 : 0.5031927824020386\n",
      "Training loss for batch 7635 : 0.171455979347229\n",
      "Training loss for batch 7636 : 0.35308822989463806\n",
      "Training loss for batch 7637 : 0.5090326070785522\n",
      "Training loss for batch 7638 : 0.014711356721818447\n",
      "Training loss for batch 7639 : 0.09009268879890442\n",
      "Training loss for batch 7640 : 0.3661056458950043\n",
      "Training loss for batch 7641 : 0.4298497438430786\n",
      "Training loss for batch 7642 : 0.14999008178710938\n",
      "Training loss for batch 7643 : 0.3038335144519806\n",
      "Training loss for batch 7644 : 0.15623058378696442\n",
      "Training loss for batch 7645 : 0.3155018985271454\n",
      "Training loss for batch 7646 : 0.10846929997205734\n",
      "Training loss for batch 7647 : 0.43487536907196045\n",
      "Training loss for batch 7648 : 0.29353252053260803\n",
      "Training loss for batch 7649 : 0.5132479071617126\n",
      "Training loss for batch 7650 : 0.3343144357204437\n",
      "Training loss for batch 7651 : 0.051556751132011414\n",
      "Training loss for batch 7652 : 0.4283256530761719\n",
      "Training loss for batch 7653 : 0.45514631271362305\n",
      "Training loss for batch 7654 : 0.40158340334892273\n",
      "Training loss for batch 7655 : 0.062471065670251846\n",
      "Training loss for batch 7656 : 0.33283185958862305\n",
      "Training loss for batch 7657 : 0.5126811861991882\n",
      "Training loss for batch 7658 : 0.09462591260671616\n",
      "Training loss for batch 7659 : 0.38232898712158203\n",
      "Training loss for batch 7660 : 0.3809625506401062\n",
      "Training loss for batch 7661 : 0.22836914658546448\n",
      "Training loss for batch 7662 : 0.31285959482192993\n",
      "Training loss for batch 7663 : 0.13481493294239044\n",
      "Training loss for batch 7664 : 0.3842719793319702\n",
      "Training loss for batch 7665 : 0.4864581525325775\n",
      "Training loss for batch 7666 : 0.4020109176635742\n",
      "Training loss for batch 7667 : 0.2718214690685272\n",
      "Training loss for batch 7668 : 0.0658932700753212\n",
      "Training loss for batch 7669 : 0.4586353003978729\n",
      "Training loss for batch 7670 : 0.1327003389596939\n",
      "Training loss for batch 7671 : 0.2775401473045349\n",
      "Training loss for batch 7672 : 0.3124261498451233\n",
      "Training loss for batch 7673 : 0.22989457845687866\n",
      "Training loss for batch 7674 : 0.40001630783081055\n",
      "Training loss for batch 7675 : 0.03394303843379021\n",
      "Training loss for batch 7676 : 0.12643587589263916\n",
      "Training loss for batch 7677 : 0.40839239954948425\n",
      "Training loss for batch 7678 : 0.7208295464515686\n",
      "Training loss for batch 7679 : 0.16887636482715607\n",
      "Training loss for batch 7680 : 0.24344119429588318\n",
      "Training loss for batch 7681 : 0.08370351791381836\n",
      "Training loss for batch 7682 : 0.7838953137397766\n",
      "Training loss for batch 7683 : 0.8324811458587646\n",
      "Training loss for batch 7684 : 0.3442351818084717\n",
      "Training loss for batch 7685 : 0.011857455596327782\n",
      "Training loss for batch 7686 : 0.22619517147541046\n",
      "Training loss for batch 7687 : 0.774117648601532\n",
      "Training loss for batch 7688 : 0.1631525605916977\n",
      "Training loss for batch 7689 : 0.19919198751449585\n",
      "Training loss for batch 7690 : 0.1855466216802597\n",
      "Training loss for batch 7691 : 0.09458402544260025\n",
      "Training loss for batch 7692 : 0.12816424667835236\n",
      "Training loss for batch 7693 : 0.10173401981592178\n",
      "Training loss for batch 7694 : 0.10212764143943787\n",
      "Training loss for batch 7695 : 0.35019853711128235\n",
      "Training loss for batch 7696 : 0.0783945843577385\n",
      "Training loss for batch 7697 : 1.0897724628448486\n",
      "Training loss for batch 7698 : 0.4451175332069397\n",
      "Training loss for batch 7699 : 0.026749329641461372\n",
      "Training loss for batch 7700 : 0.1753568798303604\n",
      "Training loss for batch 7701 : 0.1868533343076706\n",
      "Training loss for batch 7702 : 0.6149808764457703\n",
      "Training loss for batch 7703 : 0.33276018500328064\n",
      "Training loss for batch 7704 : 0.45778632164001465\n",
      "Training loss for batch 7705 : 0.4131962060928345\n",
      "Training loss for batch 7706 : 0.07443476468324661\n",
      "Training loss for batch 7707 : 0.41717401146888733\n",
      "Training loss for batch 7708 : 0.04975410923361778\n",
      "Training loss for batch 7709 : 0.3548663556575775\n",
      "Training loss for batch 7710 : 0.2888883352279663\n",
      "Training loss for batch 7711 : 0.18060320615768433\n",
      "Training loss for batch 7712 : 0.7830573916435242\n",
      "Training loss for batch 7713 : 0.3719201385974884\n",
      "Training loss for batch 7714 : 0.15226471424102783\n",
      "Training loss for batch 7715 : 0.09081932157278061\n",
      "Training loss for batch 7716 : 0.5369612574577332\n",
      "Training loss for batch 7717 : 0.2080017775297165\n",
      "Training loss for batch 7718 : 0.1911804974079132\n",
      "Training loss for batch 7719 : 0.12305514514446259\n",
      "Training loss for batch 7720 : 0.20307393372058868\n",
      "Training loss for batch 7721 : 0.26364386081695557\n",
      "Training loss for batch 7722 : 0.2846719026565552\n",
      "Training loss for batch 7723 : 0.3261106610298157\n",
      "Training loss for batch 7724 : 0.46150657534599304\n",
      "Training loss for batch 7725 : 0.48215052485466003\n",
      "Training loss for batch 7726 : 0.5460343360900879\n",
      "Training loss for batch 7727 : 0.135941743850708\n",
      "Training loss for batch 7728 : 0.3540259599685669\n",
      "Training loss for batch 7729 : 0.07222403585910797\n",
      "Training loss for batch 7730 : 0.33027535676956177\n",
      "Training loss for batch 7731 : 0.14004895091056824\n",
      "Training loss for batch 7732 : 0.32385894656181335\n",
      "Training loss for batch 7733 : 0.19259990751743317\n",
      "Training loss for batch 7734 : 0.4340023994445801\n",
      "Training loss for batch 7735 : 0.21448832750320435\n",
      "Training loss for batch 7736 : 0.4200371503829956\n",
      "Training loss for batch 7737 : 0.1983582079410553\n",
      "Training loss for batch 7738 : 0.2271682471036911\n",
      "Training loss for batch 7739 : 0.05888064205646515\n",
      "Training loss for batch 7740 : 0.20026780664920807\n",
      "Training loss for batch 7741 : 0.41687828302383423\n",
      "Training loss for batch 7742 : 0.2609497606754303\n",
      "Training loss for batch 7743 : 0.2094336897134781\n",
      "Training loss for batch 7744 : 0.12310758233070374\n",
      "Training loss for batch 7745 : 0.18557776510715485\n",
      "Training loss for batch 7746 : 0.7043778300285339\n",
      "Training loss for batch 7747 : 0.15007244050502777\n",
      "Training loss for batch 7748 : 0.5056986808776855\n",
      "Training loss for batch 7749 : 0.29525211453437805\n",
      "Training loss for batch 7750 : 0.24553282558918\n",
      "Training loss for batch 7751 : 0.3653172552585602\n",
      "Training loss for batch 7752 : 0.28293970227241516\n",
      "Training loss for batch 7753 : 0.34246525168418884\n",
      "Training loss for batch 7754 : 0.19717741012573242\n",
      "Training loss for batch 7755 : 0.3090671896934509\n",
      "Training loss for batch 7756 : 0.22516487538814545\n",
      "Training loss for batch 7757 : 0.3490177094936371\n",
      "Training loss for batch 7758 : 0.29393458366394043\n",
      "Training loss for batch 7759 : 0.6543630957603455\n",
      "Training loss for batch 7760 : 0.13014473021030426\n",
      "Training loss for batch 7761 : 0.41499048471450806\n",
      "Training loss for batch 7762 : 0.020577775314450264\n",
      "Training loss for batch 7763 : 0.5307137966156006\n",
      "Training loss for batch 7764 : 0.24744606018066406\n",
      "Training loss for batch 7765 : 0.22726519405841827\n",
      "Training loss for batch 7766 : 0.2922621965408325\n",
      "Training loss for batch 7767 : 0.4134417772293091\n",
      "Training loss for batch 7768 : 0.07419662177562714\n",
      "Training loss for batch 7769 : 0.3281061351299286\n",
      "Training loss for batch 7770 : 0.15443849563598633\n",
      "Training loss for batch 7771 : 0.20621934533119202\n",
      "Training loss for batch 7772 : 0.19437852501869202\n",
      "Training loss for batch 7773 : 0.4037432074546814\n",
      "Training loss for batch 7774 : 0.03962327539920807\n",
      "Training loss for batch 7775 : 0.313918799161911\n",
      "Training loss for batch 7776 : 0.3938602805137634\n",
      "Training loss for batch 7777 : 0.2271946370601654\n",
      "Training loss for batch 7778 : 0.3478776216506958\n",
      "Training loss for batch 7779 : 0.4282524883747101\n",
      "Training loss for batch 7780 : 0.046070635318756104\n",
      "Training loss for batch 7781 : 0.017448704689741135\n",
      "Training loss for batch 7782 : 0.3299267292022705\n",
      "Training loss for batch 7783 : 0.5379250645637512\n",
      "Training loss for batch 7784 : 0.060425546020269394\n",
      "Training loss for batch 7785 : 0.2420806735754013\n",
      "Training loss for batch 7786 : 0.43596717715263367\n",
      "Training loss for batch 7787 : 0.17236663401126862\n",
      "Training loss for batch 7788 : 0.4325011074542999\n",
      "Training loss for batch 7789 : 0.32365912199020386\n",
      "Training loss for batch 7790 : 0.11608229577541351\n",
      "Training loss for batch 7791 : 0.2412358522415161\n",
      "Training loss for batch 7792 : 0.2848111093044281\n",
      "Training loss for batch 7793 : 0.21019455790519714\n",
      "Training loss for batch 7794 : 0.12980826199054718\n",
      "Training loss for batch 7795 : 0.27178576588630676\n",
      "Training loss for batch 7796 : 0.020043428987264633\n",
      "Training loss for batch 7797 : 0.5224573612213135\n",
      "Training loss for batch 7798 : 0.37291622161865234\n",
      "Training loss for batch 7799 : 0.16138312220573425\n",
      "Training loss for batch 7800 : 0.30381786823272705\n",
      "Training loss for batch 7801 : 0.16769836843013763\n",
      "Training loss for batch 7802 : 0.05630924552679062\n",
      "Training loss for batch 7803 : 0.3001177906990051\n",
      "Training loss for batch 7804 : 0.21664604544639587\n",
      "Training loss for batch 7805 : 0.16542106866836548\n",
      "Training loss for batch 7806 : 0.2526347041130066\n",
      "Training loss for batch 7807 : 0.47047358751296997\n",
      "Training loss for batch 7808 : 0.7588843107223511\n",
      "Training loss for batch 7809 : 0.678525447845459\n",
      "Training loss for batch 7810 : 0.654623806476593\n",
      "Training loss for batch 7811 : 0.2650955021381378\n",
      "Training loss for batch 7812 : 0.18524503707885742\n",
      "Training loss for batch 7813 : 0.19176211953163147\n",
      "Training loss for batch 7814 : 0.7772837281227112\n",
      "Training loss for batch 7815 : 0.21735365688800812\n",
      "Training loss for batch 7816 : 0.33751535415649414\n",
      "Training loss for batch 7817 : 0.596184253692627\n",
      "Training loss for batch 7818 : 0.4423602223396301\n",
      "Training loss for batch 7819 : 0.280315637588501\n",
      "Training loss for batch 7820 : 0.10020074248313904\n",
      "Training loss for batch 7821 : 0.2670621871948242\n",
      "Training loss for batch 7822 : 0.17917105555534363\n",
      "Training loss for batch 7823 : 0.30612272024154663\n",
      "Training loss for batch 7824 : 0.379864901304245\n",
      "Training loss for batch 7825 : 0.37060999870300293\n",
      "Training loss for batch 7826 : 0.5193133354187012\n",
      "Training loss for batch 7827 : 0.10457275062799454\n",
      "Training loss for batch 7828 : 0.30066654086112976\n",
      "Training loss for batch 7829 : 0.07148231565952301\n",
      "Training loss for batch 7830 : 0.8176685571670532\n",
      "Training loss for batch 7831 : 0.2455582618713379\n",
      "Training loss for batch 7832 : 0.21813219785690308\n",
      "Training loss for batch 7833 : 0.6754797101020813\n",
      "Training loss for batch 7834 : 0.17121060192584991\n",
      "Training loss for batch 7835 : 0.9594054222106934\n",
      "Training loss for batch 7836 : 0.5103353261947632\n",
      "Training loss for batch 7837 : 0.22319287061691284\n",
      "Training loss for batch 7838 : 0.2350999116897583\n",
      "Training loss for batch 7839 : 0.3782828748226166\n",
      "Training loss for batch 7840 : 0.313876748085022\n",
      "Training loss for batch 7841 : 0.44639310240745544\n",
      "Training loss for batch 7842 : 0.3371336758136749\n",
      "Training loss for batch 7843 : 0.48055344820022583\n",
      "Training loss for batch 7844 : 0.3190877139568329\n",
      "Training loss for batch 7845 : 0.07440618425607681\n",
      "Training loss for batch 7846 : 0.4183915853500366\n",
      "Training loss for batch 7847 : 0.6349332928657532\n",
      "Training loss for batch 7848 : 0.4765300154685974\n",
      "Training loss for batch 7849 : 0.37572094798088074\n",
      "Training loss for batch 7850 : 0.45074930787086487\n",
      "Training loss for batch 7851 : 0.3024323880672455\n",
      "Training loss for batch 7852 : 0.3580329716205597\n",
      "Training loss for batch 7853 : 0.5388308167457581\n",
      "Training loss for batch 7854 : 0.3682228922843933\n",
      "Training loss for batch 7855 : 0.29533547163009644\n",
      "Training loss for batch 7856 : 0.026590459048748016\n",
      "Training loss for batch 7857 : 0.35767027735710144\n",
      "Training loss for batch 7858 : 0.14823061227798462\n",
      "Training loss for batch 7859 : 0.51608806848526\n",
      "Training loss for batch 7860 : 0.38931161165237427\n",
      "Training loss for batch 7861 : 0.4737745225429535\n",
      "Training loss for batch 7862 : 0.43220365047454834\n",
      "Training loss for batch 7863 : 0.03607189282774925\n",
      "Training loss for batch 7864 : 0.20101547241210938\n",
      "Training loss for batch 7865 : 0.16882306337356567\n",
      "Training loss for batch 7866 : 0.6369838714599609\n",
      "Training loss for batch 7867 : 0.1463099718093872\n",
      "Training loss for batch 7868 : 0.06512968987226486\n",
      "Training loss for batch 7869 : 0.22559934854507446\n",
      "Training loss for batch 7870 : 0.43436869978904724\n",
      "Training loss for batch 7871 : 0.6353484988212585\n",
      "Training loss for batch 7872 : 0.1637573093175888\n",
      "Training loss for batch 7873 : 0.43509554862976074\n",
      "Training loss for batch 7874 : 0.3381805419921875\n",
      "Training loss for batch 7875 : 0.16911761462688446\n",
      "Training loss for batch 7876 : 0.5393713712692261\n",
      "Training loss for batch 7877 : 0.32864755392074585\n",
      "Training loss for batch 7878 : 0.4639037549495697\n",
      "Training loss for batch 7879 : 0.35311004519462585\n",
      "Training loss for batch 7880 : 0.22538253664970398\n",
      "Training loss for batch 7881 : 0.6917833089828491\n",
      "Training loss for batch 7882 : 0.2949223518371582\n",
      "Training loss for batch 7883 : 0.1960853785276413\n",
      "Training loss for batch 7884 : 0.2576327919960022\n",
      "Training loss for batch 7885 : 0.4867944121360779\n",
      "Training loss for batch 7886 : 0.0726209506392479\n",
      "Training loss for batch 7887 : 0.3539636731147766\n",
      "Training loss for batch 7888 : 0.23928488790988922\n",
      "Training loss for batch 7889 : 0.1614672839641571\n",
      "Training loss for batch 7890 : 0.08374563604593277\n",
      "Training loss for batch 7891 : 0.20654180645942688\n",
      "Training loss for batch 7892 : 0.38252344727516174\n",
      "Training loss for batch 7893 : 0.4127058982849121\n",
      "Training loss for batch 7894 : 0.3755815923213959\n",
      "Training loss for batch 7895 : 0.12411288172006607\n",
      "Training loss for batch 7896 : 0.11617964506149292\n",
      "Training loss for batch 7897 : 0.17349426448345184\n",
      "Training loss for batch 7898 : 0.28194811940193176\n",
      "Training loss for batch 7899 : 0.5213539600372314\n",
      "Training loss for batch 7900 : 0.24736621975898743\n",
      "Training loss for batch 7901 : 0.6664255261421204\n",
      "Training loss for batch 7902 : 0.41668176651000977\n",
      "Training loss for batch 7903 : 0.5239078402519226\n",
      "Training loss for batch 7904 : 0.4054495692253113\n",
      "Training loss for batch 7905 : 0.14420054852962494\n",
      "Training loss for batch 7906 : 0.31811290979385376\n",
      "Training loss for batch 7907 : 0.2974807322025299\n",
      "Training loss for batch 7908 : 0.43007171154022217\n",
      "Training loss for batch 7909 : 0.4483130872249603\n",
      "Training loss for batch 7910 : 0.4443909227848053\n",
      "Training loss for batch 7911 : 0.5629981160163879\n",
      "Training loss for batch 7912 : 0.5657932758331299\n",
      "Training loss for batch 7913 : 0.38641995191574097\n",
      "Training loss for batch 7914 : 0.31039372086524963\n",
      "Training loss for batch 7915 : 0.47545528411865234\n",
      "Training loss for batch 7916 : 0.15204167366027832\n",
      "Training loss for batch 7917 : 0.16251319646835327\n",
      "Training loss for batch 7918 : 0.10831503570079803\n",
      "Training loss for batch 7919 : 0.1894499659538269\n",
      "Training loss for batch 7920 : 0.2514153718948364\n",
      "Training loss for batch 7921 : 0.46560201048851013\n",
      "Training loss for batch 7922 : 0.28193238377571106\n",
      "Training loss for batch 7923 : 0.18870460987091064\n",
      "Training loss for batch 7924 : 0.10014388710260391\n",
      "Training loss for batch 7925 : 0.1834360957145691\n",
      "Training loss for batch 7926 : 0.3957400321960449\n",
      "Training loss for batch 7927 : 0.22906005382537842\n",
      "Training loss for batch 7928 : 0.4687638282775879\n",
      "Training loss for batch 7929 : 0.28421035408973694\n",
      "Training loss for batch 7930 : 0.052367210388183594\n",
      "Training loss for batch 7931 : 0.6139454245567322\n",
      "Training loss for batch 7932 : 0.29261186718940735\n",
      "Training loss for batch 7933 : 0.10094865411520004\n",
      "Training loss for batch 7934 : 0.15052537620067596\n",
      "Training loss for batch 7935 : 0.2414281964302063\n",
      "Training loss for batch 7936 : 0.22663027048110962\n",
      "Training loss for batch 7937 : 0.18991781771183014\n",
      "Training loss for batch 7938 : 0.5480751991271973\n",
      "Training loss for batch 7939 : 0.15577267110347748\n",
      "Training loss for batch 7940 : 0.2776291072368622\n",
      "Training loss for batch 7941 : 0.28079530596733093\n",
      "Training loss for batch 7942 : 0.04387016221880913\n",
      "Training loss for batch 7943 : 0.24033772945404053\n",
      "Training loss for batch 7944 : 0.14285258948802948\n",
      "Training loss for batch 7945 : 0.14842019975185394\n",
      "Training loss for batch 7946 : 0.19288575649261475\n",
      "Training loss for batch 7947 : 0.4637221395969391\n",
      "Training loss for batch 7948 : 0.46313273906707764\n",
      "Training loss for batch 7949 : 0.3462374806404114\n",
      "Training loss for batch 7950 : 0.05726467818021774\n",
      "Training loss for batch 7951 : 0.2555835545063019\n",
      "Training loss for batch 7952 : 0.22617857158184052\n",
      "Training loss for batch 7953 : 0.35009586811065674\n",
      "Training loss for batch 7954 : 0.012615006417036057\n",
      "Training loss for batch 7955 : 0.21860241889953613\n",
      "Training loss for batch 7956 : 0.38548675179481506\n",
      "Training loss for batch 7957 : 0.2390541285276413\n",
      "Training loss for batch 7958 : 0.0414474792778492\n",
      "Training loss for batch 7959 : 0.10308569669723511\n",
      "Training loss for batch 7960 : 0.13942943513393402\n",
      "Training loss for batch 7961 : 0.00953469704836607\n",
      "Training loss for batch 7962 : 0.19567960500717163\n",
      "Training loss for batch 7963 : 0.45716437697410583\n",
      "Training loss for batch 7964 : 0.38348597288131714\n",
      "Training loss for batch 7965 : 0.46790945529937744\n",
      "Training loss for batch 7966 : 0.06966245174407959\n",
      "Training loss for batch 7967 : 0.08550997078418732\n",
      "Training loss for batch 7968 : 0.21142829954624176\n",
      "Training loss for batch 7969 : 0.29432594776153564\n",
      "Training loss for batch 7970 : 0.13250887393951416\n",
      "Training loss for batch 7971 : 0.2890833020210266\n",
      "Training loss for batch 7972 : 0.5043748617172241\n",
      "Training loss for batch 7973 : 0.275404691696167\n",
      "Training loss for batch 7974 : 0.1368718445301056\n",
      "Training loss for batch 7975 : 0.4223015010356903\n",
      "Training loss for batch 7976 : 0.20283091068267822\n",
      "Training loss for batch 7977 : 0.09418298304080963\n",
      "Training loss for batch 7978 : 0.5654395222663879\n",
      "Training loss for batch 7979 : 0.2442154735326767\n",
      "Training loss for batch 7980 : 0.08971919119358063\n",
      "Training loss for batch 7981 : 0.571894109249115\n",
      "Training loss for batch 7982 : 0.19771738350391388\n",
      "Training loss for batch 7983 : 0.5712931156158447\n",
      "Training loss for batch 7984 : 0.43560925126075745\n",
      "Training loss for batch 7985 : 0.08884420990943909\n",
      "Training loss for batch 7986 : 0.13545604050159454\n",
      "Training loss for batch 7987 : 0.460502028465271\n",
      "Training loss for batch 7988 : 0.12002293020486832\n",
      "Training loss for batch 7989 : 0.40302783250808716\n",
      "Training loss for batch 7990 : 0.06864657253026962\n",
      "Training loss for batch 7991 : 0.12125325947999954\n",
      "Training loss for batch 7992 : 0.1945519745349884\n",
      "Training loss for batch 7993 : 0.3041626214981079\n",
      "Training loss for batch 7994 : 0.4236886203289032\n",
      "Training loss for batch 7995 : 0.20395995676517487\n",
      "Training loss for batch 7996 : 0.7492748498916626\n",
      "Training loss for batch 7997 : 0.31234851479530334\n",
      "Training loss for batch 7998 : 0.09704277664422989\n",
      "Training loss for batch 7999 : 0.06079835072159767\n",
      "Training loss for batch 8000 : 0.2187049388885498\n",
      "Training loss for batch 8001 : 0.33978909254074097\n",
      "Training loss for batch 8002 : 0.26106300950050354\n",
      "Training loss for batch 8003 : 0.02231249399483204\n",
      "Training loss for batch 8004 : 0.5129115581512451\n",
      "Training loss for batch 8005 : 0.18561483919620514\n",
      "Training loss for batch 8006 : 0.3403586149215698\n",
      "Training loss for batch 8007 : 0.7109912037849426\n",
      "Training loss for batch 8008 : 0.3227844536304474\n",
      "Training loss for batch 8009 : 0.3573863208293915\n",
      "Training loss for batch 8010 : 0.32820743322372437\n",
      "Training loss for batch 8011 : 0.12440383434295654\n",
      "Training loss for batch 8012 : 0.2647733986377716\n",
      "Training loss for batch 8013 : 0.3577417731285095\n",
      "Training loss for batch 8014 : 0.49800872802734375\n",
      "Training loss for batch 8015 : 0.1556212306022644\n",
      "Training loss for batch 8016 : 0.5350366234779358\n",
      "Training loss for batch 8017 : 0.2482995092868805\n",
      "Training loss for batch 8018 : 0.08546987175941467\n",
      "Training loss for batch 8019 : 0.5643934011459351\n",
      "Training loss for batch 8020 : 0.27535542845726013\n",
      "Training loss for batch 8021 : 0.26696473360061646\n",
      "Training loss for batch 8022 : 0.12489373236894608\n",
      "Training loss for batch 8023 : 0.5308933258056641\n",
      "Training loss for batch 8024 : 0.6948425769805908\n",
      "Training loss for batch 8025 : 0.1473383605480194\n",
      "Training loss for batch 8026 : 0.24355867505073547\n",
      "Training loss for batch 8027 : 0.09942933171987534\n",
      "Training loss for batch 8028 : 0.3640289604663849\n",
      "Training loss for batch 8029 : 0.5885626077651978\n",
      "Training loss for batch 8030 : 0.25475165247917175\n",
      "Training loss for batch 8031 : 0.16885793209075928\n",
      "Training loss for batch 8032 : 0.5057546496391296\n",
      "Training loss for batch 8033 : 0.0022487251553684473\n",
      "Training loss for batch 8034 : 0.8306727409362793\n",
      "Training loss for batch 8035 : 0.08995956182479858\n",
      "Training loss for batch 8036 : 0.39260193705558777\n",
      "Training loss for batch 8037 : 0.2223539501428604\n",
      "Training loss for batch 8038 : 0.9222605228424072\n",
      "Training loss for batch 8039 : 0.13154780864715576\n",
      "Training loss for batch 8040 : 0.29811331629753113\n",
      "Training loss for batch 8041 : 0.21585805714130402\n",
      "Training loss for batch 8042 : 0.22189107537269592\n",
      "Training loss for batch 8043 : 0.2426781952381134\n",
      "Training loss for batch 8044 : 0.3576885759830475\n",
      "Training loss for batch 8045 : 0.41440701484680176\n",
      "Training loss for batch 8046 : 0.07958175241947174\n",
      "Training loss for batch 8047 : 0.09095849096775055\n",
      "Training loss for batch 8048 : 0.20041827857494354\n",
      "Training loss for batch 8049 : 0.092630535364151\n",
      "Training loss for batch 8050 : 0.286093145608902\n",
      "Training loss for batch 8051 : 0.11833072453737259\n",
      "Training loss for batch 8052 : 0.2596784830093384\n",
      "Training loss for batch 8053 : 0.6093949675559998\n",
      "Training loss for batch 8054 : 0.13826924562454224\n",
      "Training loss for batch 8055 : 0.4536077380180359\n",
      "Training loss for batch 8056 : 0.14674066007137299\n",
      "Training loss for batch 8057 : 0.291697233915329\n",
      "Training loss for batch 8058 : 0.19973623752593994\n",
      "Training loss for batch 8059 : 0.02166152372956276\n",
      "Training loss for batch 8060 : 0.5124288201332092\n",
      "Training loss for batch 8061 : 0.554807186126709\n",
      "Training loss for batch 8062 : 0.15518836677074432\n",
      "Training loss for batch 8063 : 0.0928485095500946\n",
      "Training loss for batch 8064 : 0.2718397080898285\n",
      "Training loss for batch 8065 : 0.29919755458831787\n",
      "Training loss for batch 8066 : 0.3037702441215515\n",
      "Training loss for batch 8067 : 0.2410673201084137\n",
      "Training loss for batch 8068 : 0.5613080263137817\n",
      "Training loss for batch 8069 : 0.43645185232162476\n",
      "Training loss for batch 8070 : 0.3474678695201874\n",
      "Training loss for batch 8071 : 0.033857591450214386\n",
      "Training loss for batch 8072 : 0.2111552655696869\n",
      "Training loss for batch 8073 : 0.44089367985725403\n",
      "Training loss for batch 8074 : 1.0844531059265137\n",
      "Training loss for batch 8075 : 0.30691593885421753\n",
      "Training loss for batch 8076 : 0.11719170957803726\n",
      "Training loss for batch 8077 : 0.4315909147262573\n",
      "Training loss for batch 8078 : 0.07390670478343964\n",
      "Training loss for batch 8079 : 0.11394494771957397\n",
      "Training loss for batch 8080 : 0.1515190303325653\n",
      "Training loss for batch 8081 : 0.15074491500854492\n",
      "Training loss for batch 8082 : 0.4037817120552063\n",
      "Training loss for batch 8083 : 0.31938284635543823\n",
      "Training loss for batch 8084 : 0.3999566435813904\n",
      "Training loss for batch 8085 : 0.03339758515357971\n",
      "Training loss for batch 8086 : 0.6746429800987244\n",
      "Training loss for batch 8087 : 0.40682029724121094\n",
      "Training loss for batch 8088 : 0.3589133024215698\n",
      "Training loss for batch 8089 : 0.2537769675254822\n",
      "Training loss for batch 8090 : 0.08256784081459045\n",
      "Training loss for batch 8091 : 0.18763630092144012\n",
      "Training loss for batch 8092 : 0.324024498462677\n",
      "Training loss for batch 8093 : 0.011187892407178879\n",
      "Training loss for batch 8094 : 0.1498362421989441\n",
      "Training loss for batch 8095 : 0.3285510838031769\n",
      "Training loss for batch 8096 : 0.2786047160625458\n",
      "Training loss for batch 8097 : 0.4133141040802002\n",
      "Training loss for batch 8098 : 0.31003934144973755\n",
      "Training loss for batch 8099 : 0.22261998057365417\n",
      "Training loss for batch 8100 : 0.4704044759273529\n",
      "Training loss for batch 8101 : 0.457137793302536\n",
      "Training loss for batch 8102 : 0.3760204613208771\n",
      "Training loss for batch 8103 : 0.28461796045303345\n",
      "Training loss for batch 8104 : 0.22485807538032532\n",
      "Training loss for batch 8105 : 0.24342074990272522\n",
      "Training loss for batch 8106 : 0.29614120721817017\n",
      "Training loss for batch 8107 : 0.05676405131816864\n",
      "Training loss for batch 8108 : 0.6849200129508972\n",
      "Training loss for batch 8109 : 0.0\n",
      "Training loss for batch 8110 : 0.11305300891399384\n",
      "Training loss for batch 8111 : 0.7062211632728577\n",
      "Training loss for batch 8112 : 0.523461639881134\n",
      "Training loss for batch 8113 : 0.28374117612838745\n",
      "Training loss for batch 8114 : 0.23046162724494934\n",
      "Training loss for batch 8115 : 0.49308809638023376\n",
      "Training loss for batch 8116 : 0.1855449676513672\n",
      "Training loss for batch 8117 : 0.26704177260398865\n",
      "Training loss for batch 8118 : 0.6967921853065491\n",
      "Training loss for batch 8119 : 0.34442174434661865\n",
      "Training loss for batch 8120 : 0.5802997946739197\n",
      "Training loss for batch 8121 : 0.29254183173179626\n",
      "Training loss for batch 8122 : 0.2000538557767868\n",
      "Training loss for batch 8123 : 0.18902751803398132\n",
      "Training loss for batch 8124 : 0.16243544220924377\n",
      "Training loss for batch 8125 : 0.30099067091941833\n",
      "Training loss for batch 8126 : 0.39265719056129456\n",
      "Training loss for batch 8127 : 0.3230970799922943\n",
      "Training loss for batch 8128 : 0.3433551490306854\n",
      "Training loss for batch 8129 : 0.31847986578941345\n",
      "Training loss for batch 8130 : 0.19087852537631989\n",
      "Training loss for batch 8131 : 0.06866367161273956\n",
      "Training loss for batch 8132 : 0.09181243926286697\n",
      "Training loss for batch 8133 : 0.4151798188686371\n",
      "Training loss for batch 8134 : 0.009647024795413017\n",
      "Training loss for batch 8135 : 0.024588385596871376\n",
      "Training loss for batch 8136 : 0.48750370740890503\n",
      "Training loss for batch 8137 : 0.5866143703460693\n",
      "Training loss for batch 8138 : 0.5218372344970703\n",
      "Training loss for batch 8139 : 0.2457209676504135\n",
      "Training loss for batch 8140 : 0.05097101256251335\n",
      "Training loss for batch 8141 : 0.22163844108581543\n",
      "Training loss for batch 8142 : 0.43282583355903625\n",
      "Training loss for batch 8143 : 0.10159115493297577\n",
      "Training loss for batch 8144 : 0.18930526077747345\n",
      "Training loss for batch 8145 : 0.16034767031669617\n",
      "Training loss for batch 8146 : 0.5697097182273865\n",
      "Training loss for batch 8147 : 0.2618381977081299\n",
      "Training loss for batch 8148 : 0.30587729811668396\n",
      "Training loss for batch 8149 : 0.13710251450538635\n",
      "Training loss for batch 8150 : 0.3883446455001831\n",
      "Training loss for batch 8151 : 0.33480575680732727\n",
      "Training loss for batch 8152 : 0.3139354884624481\n",
      "Training loss for batch 8153 : 0.1376742273569107\n",
      "Training loss for batch 8154 : 0.07118713855743408\n",
      "Training loss for batch 8155 : 0.10871026664972305\n",
      "Training loss for batch 8156 : 0.5682714581489563\n",
      "Training loss for batch 8157 : 0.27389246225357056\n",
      "Training loss for batch 8158 : 0.1441013216972351\n",
      "Training loss for batch 8159 : 0.48214131593704224\n",
      "Training loss for batch 8160 : 0.003388685639947653\n",
      "Training loss for batch 8161 : 0.1410205364227295\n",
      "Training loss for batch 8162 : 0.3335830867290497\n",
      "Training loss for batch 8163 : 0.26136475801467896\n",
      "Training loss for batch 8164 : 0.3382600247859955\n",
      "Training loss for batch 8165 : 0.4051319658756256\n",
      "Training loss for batch 8166 : 0.23639768362045288\n",
      "Training loss for batch 8167 : 0.1831064522266388\n",
      "Training loss for batch 8168 : 0.11192433536052704\n",
      "Training loss for batch 8169 : 0.2838479280471802\n",
      "Training loss for batch 8170 : 0.45729032158851624\n",
      "Training loss for batch 8171 : 0.539625346660614\n",
      "Training loss for batch 8172 : 0.4436017572879791\n",
      "Training loss for batch 8173 : 0.04516582563519478\n",
      "Training loss for batch 8174 : 0.4973987340927124\n",
      "Training loss for batch 8175 : 0.0532066635787487\n",
      "Training loss for batch 8176 : 0.024393131956458092\n",
      "Training loss for batch 8177 : 0.3044387698173523\n",
      "Training loss for batch 8178 : 0.19520382583141327\n",
      "Training loss for batch 8179 : 0.17476129531860352\n",
      "Training loss for batch 8180 : 0.10262994468212128\n",
      "Training loss for batch 8181 : 0.30756622552871704\n",
      "Training loss for batch 8182 : 0.1258278340101242\n",
      "Training loss for batch 8183 : 0.10823023319244385\n",
      "Training loss for batch 8184 : 0.27373233437538147\n",
      "Training loss for batch 8185 : 0.2964918613433838\n",
      "Training loss for batch 8186 : 0.2605227530002594\n",
      "Training loss for batch 8187 : 0.6094852685928345\n",
      "Training loss for batch 8188 : 0.2251625806093216\n",
      "Training loss for batch 8189 : 0.6257339119911194\n",
      "Training loss for batch 8190 : 0.40281057357788086\n",
      "Training loss for batch 8191 : 0.35728394985198975\n",
      "Training loss for batch 8192 : 0.18311041593551636\n",
      "Training loss for batch 8193 : 0.27383989095687866\n",
      "Training loss for batch 8194 : 0.25158506631851196\n",
      "Training loss for batch 8195 : 0.06616373360157013\n",
      "Training loss for batch 8196 : 0.5789876580238342\n",
      "Training loss for batch 8197 : 0.29682475328445435\n",
      "Training loss for batch 8198 : 0.3828858435153961\n",
      "Training loss for batch 8199 : 0.31868043541908264\n",
      "Training loss for batch 8200 : 0.11194886267185211\n",
      "Training loss for batch 8201 : 0.026789916679263115\n",
      "Training loss for batch 8202 : 0.2923296093940735\n",
      "Training loss for batch 8203 : 0.12103442847728729\n",
      "Training loss for batch 8204 : 0.2063097208738327\n",
      "Training loss for batch 8205 : 0.20437262952327728\n",
      "Training loss for batch 8206 : 0.15330447256565094\n",
      "Training loss for batch 8207 : 0.3023037612438202\n",
      "Training loss for batch 8208 : 0.13096646964550018\n",
      "Training loss for batch 8209 : 0.6676737070083618\n",
      "Training loss for batch 8210 : 0.38282257318496704\n",
      "Training loss for batch 8211 : 0.07377339899539948\n",
      "Training loss for batch 8212 : 0.3003658652305603\n",
      "Training loss for batch 8213 : 0.1627054214477539\n",
      "Training loss for batch 8214 : 0.12384577840566635\n",
      "Training loss for batch 8215 : 0.5225610136985779\n",
      "Training loss for batch 8216 : 0.03735577315092087\n",
      "Training loss for batch 8217 : 0.11042915284633636\n",
      "Training loss for batch 8218 : 0.06471855938434601\n",
      "Training loss for batch 8219 : 0.10226708650588989\n",
      "Training loss for batch 8220 : 0.2606737017631531\n",
      "Training loss for batch 8221 : 0.4900842010974884\n",
      "Training loss for batch 8222 : 0.3537016212940216\n",
      "Training loss for batch 8223 : 0.28745678067207336\n",
      "Training loss for batch 8224 : 0.1542503535747528\n",
      "Training loss for batch 8225 : 0.1450500190258026\n",
      "Training loss for batch 8226 : 0.3254384398460388\n",
      "Training loss for batch 8227 : 0.22736325860023499\n",
      "Training loss for batch 8228 : 0.2712261974811554\n",
      "Training loss for batch 8229 : 0.19156140089035034\n",
      "Training loss for batch 8230 : 0.1898738443851471\n",
      "Training loss for batch 8231 : 0.18913164734840393\n",
      "Training loss for batch 8232 : 0.2743091881275177\n",
      "Training loss for batch 8233 : 0.08738449215888977\n",
      "Training loss for batch 8234 : 0.5113251209259033\n",
      "Training loss for batch 8235 : 0.10304397344589233\n",
      "Training loss for batch 8236 : 0.03131111338734627\n",
      "Training loss for batch 8237 : 0.2596234083175659\n",
      "Training loss for batch 8238 : 0.30589500069618225\n",
      "Training loss for batch 8239 : 0.06481242924928665\n",
      "Training loss for batch 8240 : 0.006454785820096731\n",
      "Training loss for batch 8241 : 0.6038690209388733\n",
      "Training loss for batch 8242 : 0.08146568387746811\n",
      "Training loss for batch 8243 : 0.37057483196258545\n",
      "Training loss for batch 8244 : 0.232310950756073\n",
      "Training loss for batch 8245 : 0.1624898612499237\n",
      "Training loss for batch 8246 : 0.16885514557361603\n",
      "Training loss for batch 8247 : 0.33834946155548096\n",
      "Training loss for batch 8248 : 0.3467632532119751\n",
      "Training loss for batch 8249 : 0.2677857577800751\n",
      "Training loss for batch 8250 : 0.16496983170509338\n",
      "Training loss for batch 8251 : 0.403022438287735\n",
      "Training loss for batch 8252 : 0.057578686624765396\n",
      "Training loss for batch 8253 : 0.2469223141670227\n",
      "Training loss for batch 8254 : 0.45635712146759033\n",
      "Training loss for batch 8255 : 0.47775864601135254\n",
      "Training loss for batch 8256 : 0.5068114399909973\n",
      "Training loss for batch 8257 : 0.05468723177909851\n",
      "Training loss for batch 8258 : 0.10198918730020523\n",
      "Training loss for batch 8259 : 0.11858368664979935\n",
      "Training loss for batch 8260 : 0.7234779596328735\n",
      "Training loss for batch 8261 : 0.25701239705085754\n",
      "Training loss for batch 8262 : 0.9858095049858093\n",
      "Training loss for batch 8263 : 0.22087660431861877\n",
      "Training loss for batch 8264 : 0.41863924264907837\n",
      "Training loss for batch 8265 : 0.019209682941436768\n",
      "Training loss for batch 8266 : 0.5831451416015625\n",
      "Training loss for batch 8267 : 0.44254764914512634\n",
      "Training loss for batch 8268 : 0.008864011615514755\n",
      "Training loss for batch 8269 : 0.17533159255981445\n",
      "Training loss for batch 8270 : 0.26771512627601624\n",
      "Training loss for batch 8271 : 0.5527896881103516\n",
      "Training loss for batch 8272 : 0.22598807513713837\n",
      "Training loss for batch 8273 : 0.27726322412490845\n",
      "Training loss for batch 8274 : 0.11611742526292801\n",
      "Training loss for batch 8275 : 0.5389864444732666\n",
      "Training loss for batch 8276 : 0.27143394947052\n",
      "Training loss for batch 8277 : 0.49454784393310547\n",
      "Training loss for batch 8278 : 0.3929971158504486\n",
      "Training loss for batch 8279 : 0.5262417793273926\n",
      "Training loss for batch 8280 : 0.08159483969211578\n",
      "Training loss for batch 8281 : 0.1151188313961029\n",
      "Training loss for batch 8282 : 0.40552666783332825\n",
      "Training loss for batch 8283 : 0.06219480186700821\n",
      "Training loss for batch 8284 : 0.4298548996448517\n",
      "Training loss for batch 8285 : 0.20219342410564423\n",
      "Training loss for batch 8286 : 0.4349397122859955\n",
      "Training loss for batch 8287 : 0.8938867449760437\n",
      "Training loss for batch 8288 : 0.0927376076579094\n",
      "Training loss for batch 8289 : 0.32301294803619385\n",
      "Training loss for batch 8290 : 0.21370719373226166\n",
      "Training loss for batch 8291 : 0.28741514682769775\n",
      "Training loss for batch 8292 : 0.0837523564696312\n",
      "Training loss for batch 8293 : 0.3720160126686096\n",
      "Training loss for batch 8294 : 0.341989666223526\n",
      "Training loss for batch 8295 : 0.1926996111869812\n",
      "Training loss for batch 8296 : 0.4059431552886963\n",
      "Training loss for batch 8297 : 0.08875701576471329\n",
      "Training loss for batch 8298 : 0.1663898378610611\n",
      "Training loss for batch 8299 : 0.019680224359035492\n",
      "Training loss for batch 8300 : 0.4168807566165924\n",
      "Training loss for batch 8301 : 0.305053174495697\n",
      "Training loss for batch 8302 : 0.6455390453338623\n",
      "Training loss for batch 8303 : 0.38671860098838806\n",
      "Training loss for batch 8304 : 0.5823679566383362\n",
      "Training loss for batch 8305 : 0.368317574262619\n",
      "Training loss for batch 8306 : 0.35591191053390503\n",
      "Training loss for batch 8307 : 1.1562762260437012\n",
      "Parameter containing:\n",
      "tensor(0.1563, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.1562623977661133\n",
      "Training loss for batch 1 : 1.1562442779541016\n",
      "Training loss for batch 2 : 1.1562225818634033\n",
      "Training loss for batch 3 : 1.1561975479125977\n",
      "Training loss for batch 4 : 1.1561698913574219\n",
      "Training loss for batch 5 : 1.156139850616455\n",
      "Training loss for batch 6 : 1.1561076641082764\n",
      "Training loss for batch 7 : 1.1560739278793335\n",
      "Training loss for batch 8 : 1.1560386419296265\n",
      "Training loss for batch 9 : 1.156002163887024\n",
      "Training loss for batch 10 : 1.1559644937515259\n",
      "Training loss for batch 11 : 1.155925989151001\n",
      "Training loss for batch 12 : 0.07576487213373184\n",
      "Training loss for batch 13 : 0.18471482396125793\n",
      "Training loss for batch 14 : 0.3075769543647766\n",
      "Training loss for batch 15 : 0.07633046805858612\n",
      "Training loss for batch 16 : 0.3184385597705841\n",
      "Training loss for batch 17 : 0.06284112483263016\n",
      "Training loss for batch 18 : 0.05820511281490326\n",
      "Training loss for batch 19 : 0.22672778367996216\n",
      "Training loss for batch 20 : 0.22232680022716522\n",
      "Training loss for batch 21 : 0.09289129823446274\n",
      "Training loss for batch 22 : 0.421699196100235\n",
      "Training loss for batch 23 : 0.12422611564397812\n",
      "Training loss for batch 24 : 0.17365910112857819\n",
      "Training loss for batch 25 : 0.11849766224622726\n",
      "Training loss for batch 26 : 0.0978320762515068\n",
      "Training loss for batch 27 : 0.3933507204055786\n",
      "Training loss for batch 28 : 0.36845624446868896\n",
      "Training loss for batch 29 : 0.07293153554201126\n",
      "Training loss for batch 30 : 0.23195242881774902\n",
      "Training loss for batch 31 : 0.10661406815052032\n",
      "Training loss for batch 32 : 0.4211844503879547\n",
      "Training loss for batch 33 : 0.10807555168867111\n",
      "Training loss for batch 34 : 0.16350233554840088\n",
      "Training loss for batch 35 : 0.021836407482624054\n",
      "Training loss for batch 36 : 0.39122864603996277\n",
      "Training loss for batch 37 : 0.09613247215747833\n",
      "Training loss for batch 38 : 0.21140307188034058\n",
      "Training loss for batch 39 : 0.21889661252498627\n",
      "Training loss for batch 40 : 0.4907948076725006\n",
      "Training loss for batch 41 : 0.11883941292762756\n",
      "Training loss for batch 42 : 0.2370215207338333\n",
      "Training loss for batch 43 : 0.016570810228586197\n",
      "Training loss for batch 44 : 0.008991703391075134\n",
      "Training loss for batch 45 : 0.27680709958076477\n",
      "Training loss for batch 46 : 0.5106821060180664\n",
      "Training loss for batch 47 : 0.22550290822982788\n",
      "Training loss for batch 48 : 0.1814766526222229\n",
      "Training loss for batch 49 : 0.37691083550453186\n",
      "Training loss for batch 50 : 0.11520593613386154\n",
      "Training loss for batch 51 : 0.3088306784629822\n",
      "Training loss for batch 52 : 0.10581288486719131\n",
      "Training loss for batch 53 : 0.6888288855552673\n",
      "Training loss for batch 54 : 0.4267256259918213\n",
      "Training loss for batch 55 : 0.10931742936372757\n",
      "Training loss for batch 56 : 0.06414483487606049\n",
      "Training loss for batch 57 : 0.5013163089752197\n",
      "Training loss for batch 58 : 0.691405713558197\n",
      "Training loss for batch 59 : 0.015095110051333904\n",
      "Training loss for batch 60 : 0.22759883105754852\n",
      "Training loss for batch 61 : 0.3686353862285614\n",
      "Training loss for batch 62 : 0.16590392589569092\n",
      "Training loss for batch 63 : 0.21245600283145905\n",
      "Training loss for batch 64 : 0.19882416725158691\n",
      "Training loss for batch 65 : 0.8733668327331543\n",
      "Training loss for batch 66 : 0.21371474862098694\n",
      "Training loss for batch 67 : 0.25376275181770325\n",
      "Training loss for batch 68 : 0.3413439095020294\n",
      "Training loss for batch 69 : 0.316679984331131\n",
      "Training loss for batch 70 : 0.4894651770591736\n",
      "Training loss for batch 71 : 0.06588219851255417\n",
      "Training loss for batch 72 : 0.3512948751449585\n",
      "Training loss for batch 73 : 0.2865797281265259\n",
      "Training loss for batch 74 : 0.28492510318756104\n",
      "Training loss for batch 75 : 0.3522084355354309\n",
      "Training loss for batch 76 : 0.2014404982328415\n",
      "Training loss for batch 77 : 0.453570693731308\n",
      "Training loss for batch 78 : 0.15109361708164215\n",
      "Training loss for batch 79 : 0.24285751581192017\n",
      "Training loss for batch 80 : 0.49102500081062317\n",
      "Training loss for batch 81 : 0.1460653841495514\n",
      "Training loss for batch 82 : 0.07792456448078156\n",
      "Training loss for batch 83 : 0.2176988422870636\n",
      "Training loss for batch 84 : 0.3484432101249695\n",
      "Training loss for batch 85 : 0.12767226994037628\n",
      "Training loss for batch 86 : 0.11059140413999557\n",
      "Training loss for batch 87 : 0.4553045928478241\n",
      "Training loss for batch 88 : 0.3307541608810425\n",
      "Training loss for batch 89 : 0.2706894874572754\n",
      "Training loss for batch 90 : 0.20930148661136627\n",
      "Training loss for batch 91 : 0.18212221562862396\n",
      "Training loss for batch 92 : 0.2690701484680176\n",
      "Training loss for batch 93 : 0.16738659143447876\n",
      "Training loss for batch 94 : 0.2901054918766022\n",
      "Training loss for batch 95 : 0.24386155605316162\n",
      "Training loss for batch 96 : 0.20058417320251465\n",
      "Training loss for batch 97 : 0.35117727518081665\n",
      "Training loss for batch 98 : 0.41950076818466187\n",
      "Training loss for batch 99 : 0.07354853302240372\n",
      "Training loss for batch 100 : 0.1334516853094101\n",
      "Training loss for batch 101 : 0.3783864974975586\n",
      "Training loss for batch 102 : 0.2386481761932373\n",
      "Training loss for batch 103 : 0.048505350947380066\n",
      "Training loss for batch 104 : 0.8127379417419434\n",
      "Training loss for batch 105 : 0.047774605453014374\n",
      "Training loss for batch 106 : 0.05035848915576935\n",
      "Training loss for batch 107 : 0.16438642144203186\n",
      "Training loss for batch 108 : 0.011998703703284264\n",
      "Training loss for batch 109 : 0.48283955454826355\n",
      "Training loss for batch 110 : 0.04073686897754669\n",
      "Training loss for batch 111 : 0.08959365636110306\n",
      "Training loss for batch 112 : 0.21742361783981323\n",
      "Training loss for batch 113 : 0.017617881298065186\n",
      "Training loss for batch 114 : 0.01016146969050169\n",
      "Training loss for batch 115 : 0.6402761936187744\n",
      "Training loss for batch 116 : 0.2949962317943573\n",
      "Training loss for batch 117 : 0.07714854925870895\n",
      "Training loss for batch 118 : 0.05285034328699112\n",
      "Training loss for batch 119 : 0.10433652997016907\n",
      "Training loss for batch 120 : 0.0014182329177856445\n",
      "Training loss for batch 121 : 0.30454742908477783\n",
      "Training loss for batch 122 : 0.5973485112190247\n",
      "Training loss for batch 123 : 0.13900570571422577\n",
      "Training loss for batch 124 : 0.1377972513437271\n",
      "Training loss for batch 125 : 0.11555397510528564\n",
      "Training loss for batch 126 : 0.14948205649852753\n",
      "Training loss for batch 127 : 0.662445068359375\n",
      "Training loss for batch 128 : 0.4791979491710663\n",
      "Training loss for batch 129 : 0.3579789996147156\n",
      "Training loss for batch 130 : 0.709888756275177\n",
      "Training loss for batch 131 : 0.04078897833824158\n",
      "Training loss for batch 132 : 0.10732871294021606\n",
      "Training loss for batch 133 : 0.30635714530944824\n",
      "Training loss for batch 134 : 0.38524335622787476\n",
      "Training loss for batch 135 : 0.034147609025239944\n",
      "Training loss for batch 136 : 0.38708221912384033\n",
      "Training loss for batch 137 : 0.17811033129692078\n",
      "Training loss for batch 138 : 0.6807628273963928\n",
      "Training loss for batch 139 : 0.6095554232597351\n",
      "Training loss for batch 140 : 0.14448969066143036\n",
      "Training loss for batch 141 : 0.08349373191595078\n",
      "Training loss for batch 142 : 0.18132023513317108\n",
      "Training loss for batch 143 : 0.04864642769098282\n",
      "Training loss for batch 144 : 0.0057927570305764675\n",
      "Training loss for batch 145 : 0.39099088311195374\n",
      "Training loss for batch 146 : 0.4088962972164154\n",
      "Training loss for batch 147 : 0.11631990969181061\n",
      "Training loss for batch 148 : 0.38833314180374146\n",
      "Training loss for batch 149 : 0.11255837976932526\n",
      "Training loss for batch 150 : 0.03377804905176163\n",
      "Training loss for batch 151 : 0.4341828227043152\n",
      "Training loss for batch 152 : 0.4650213122367859\n",
      "Training loss for batch 153 : 0.07874878495931625\n",
      "Training loss for batch 154 : 0.48439642786979675\n",
      "Training loss for batch 155 : 0.4466119110584259\n",
      "Training loss for batch 156 : 0.11258389055728912\n",
      "Training loss for batch 157 : 0.05584879219532013\n",
      "Training loss for batch 158 : 0.17367851734161377\n",
      "Training loss for batch 159 : 0.4347272515296936\n",
      "Training loss for batch 160 : 0.3159884214401245\n",
      "Training loss for batch 161 : 0.37907591462135315\n",
      "Training loss for batch 162 : 0.23628835380077362\n",
      "Training loss for batch 163 : 0.28017187118530273\n",
      "Training loss for batch 164 : 0.429071307182312\n",
      "Training loss for batch 165 : 0.25515973567962646\n",
      "Training loss for batch 166 : 0.48704901337623596\n",
      "Training loss for batch 167 : 0.1353605091571808\n",
      "Training loss for batch 168 : 0.15671740472316742\n",
      "Training loss for batch 169 : 0.60693359375\n",
      "Training loss for batch 170 : 0.36248403787612915\n",
      "Training loss for batch 171 : 0.21109221875667572\n",
      "Training loss for batch 172 : 0.04626935347914696\n",
      "Training loss for batch 173 : 0.2532951831817627\n",
      "Training loss for batch 174 : 0.41837114095687866\n",
      "Training loss for batch 175 : 0.31740522384643555\n",
      "Training loss for batch 176 : 0.13752810657024384\n",
      "Training loss for batch 177 : 0.560287594795227\n",
      "Training loss for batch 178 : 0.08226878196001053\n",
      "Training loss for batch 179 : 0.5743567943572998\n",
      "Training loss for batch 180 : 0.3814833462238312\n",
      "Training loss for batch 181 : 0.34147191047668457\n",
      "Training loss for batch 182 : 0.11785375326871872\n",
      "Training loss for batch 183 : 0.2804860770702362\n",
      "Training loss for batch 184 : 0.14044170081615448\n",
      "Training loss for batch 185 : 0.09268413484096527\n",
      "Training loss for batch 186 : 0.35951805114746094\n",
      "Training loss for batch 187 : 0.1743420958518982\n",
      "Training loss for batch 188 : 0.19508226215839386\n",
      "Training loss for batch 189 : 0.22185631096363068\n",
      "Training loss for batch 190 : 0.05807353928685188\n",
      "Training loss for batch 191 : 0.3910595178604126\n",
      "Training loss for batch 192 : 0.21687738597393036\n",
      "Training loss for batch 193 : 0.09959051758050919\n",
      "Training loss for batch 194 : 0.11847876012325287\n",
      "Training loss for batch 195 : 0.1718945950269699\n",
      "Training loss for batch 196 : 0.34695059061050415\n",
      "Training loss for batch 197 : 0.44593870639801025\n",
      "Training loss for batch 198 : 0.30765730142593384\n",
      "Training loss for batch 199 : 0.26024219393730164\n",
      "Training loss for batch 200 : 0.5931854844093323\n",
      "Training loss for batch 201 : 0.1656762659549713\n",
      "Training loss for batch 202 : 0.04129750654101372\n",
      "Training loss for batch 203 : 0.04849548637866974\n",
      "Training loss for batch 204 : 0.3210349977016449\n",
      "Training loss for batch 205 : 0.16332602500915527\n",
      "Training loss for batch 206 : 0.17152388393878937\n",
      "Training loss for batch 207 : 0.24668516218662262\n",
      "Training loss for batch 208 : 0.255710631608963\n",
      "Training loss for batch 209 : 0.17114432156085968\n",
      "Training loss for batch 210 : 0.01914537511765957\n",
      "Training loss for batch 211 : 0.39240431785583496\n",
      "Training loss for batch 212 : 0.3931734263896942\n",
      "Training loss for batch 213 : 0.1216425970196724\n",
      "Training loss for batch 214 : 0.25128307938575745\n",
      "Training loss for batch 215 : 0.22219130396842957\n",
      "Training loss for batch 216 : 0.2967127561569214\n",
      "Training loss for batch 217 : 0.4600631296634674\n",
      "Training loss for batch 218 : 0.41584524512290955\n",
      "Training loss for batch 219 : 0.2916240394115448\n",
      "Training loss for batch 220 : 0.2044365257024765\n",
      "Training loss for batch 221 : 0.23212064802646637\n",
      "Training loss for batch 222 : 0.16172894835472107\n",
      "Training loss for batch 223 : 0.22273509204387665\n",
      "Training loss for batch 224 : 0.20966759324073792\n",
      "Training loss for batch 225 : 0.3802244961261749\n",
      "Training loss for batch 226 : 0.047038763761520386\n",
      "Training loss for batch 227 : 0.301803320646286\n",
      "Training loss for batch 228 : 0.18640032410621643\n",
      "Training loss for batch 229 : 0.30086788535118103\n",
      "Training loss for batch 230 : 0.6670318245887756\n",
      "Training loss for batch 231 : 0.3079635202884674\n",
      "Training loss for batch 232 : 0.551234245300293\n",
      "Training loss for batch 233 : 0.550524115562439\n",
      "Training loss for batch 234 : 0.1431092619895935\n",
      "Training loss for batch 235 : 0.24189306795597076\n",
      "Training loss for batch 236 : 0.4121437072753906\n",
      "Training loss for batch 237 : 0.405623197555542\n",
      "Training loss for batch 238 : 0.3027438223361969\n",
      "Training loss for batch 239 : 0.3140653371810913\n",
      "Training loss for batch 240 : 0.0643855407834053\n",
      "Training loss for batch 241 : 0.4036768078804016\n",
      "Training loss for batch 242 : 0.3208228051662445\n",
      "Training loss for batch 243 : 0.8710646629333496\n",
      "Training loss for batch 244 : 0.14924356341362\n",
      "Training loss for batch 245 : 0.07995833456516266\n",
      "Training loss for batch 246 : 0.31262364983558655\n",
      "Training loss for batch 247 : 0.32386595010757446\n",
      "Training loss for batch 248 : 0.3898875415325165\n",
      "Training loss for batch 249 : 0.20256716012954712\n",
      "Training loss for batch 250 : 0.1475609540939331\n",
      "Training loss for batch 251 : 0.1800776720046997\n",
      "Training loss for batch 252 : 0.10329584032297134\n",
      "Training loss for batch 253 : 0.40250298380851746\n",
      "Training loss for batch 254 : 0.1709994226694107\n",
      "Training loss for batch 255 : 0.16656172275543213\n",
      "Training loss for batch 256 : 0.027700796723365784\n",
      "Training loss for batch 257 : 0.0775381475687027\n",
      "Training loss for batch 258 : 0.2854115664958954\n",
      "Training loss for batch 259 : 0.022097215056419373\n",
      "Training loss for batch 260 : 0.13482923805713654\n",
      "Training loss for batch 261 : 0.4842650294303894\n",
      "Training loss for batch 262 : 0.3991232216358185\n",
      "Training loss for batch 263 : 0.3039209246635437\n",
      "Training loss for batch 264 : 0.2207210808992386\n",
      "Training loss for batch 265 : 0.3422187566757202\n",
      "Training loss for batch 266 : 0.4689166247844696\n",
      "Training loss for batch 267 : 0.030135052278637886\n",
      "Training loss for batch 268 : 0.007225575391203165\n",
      "Training loss for batch 269 : 0.28917229175567627\n",
      "Training loss for batch 270 : 0.4353256821632385\n",
      "Training loss for batch 271 : 0.10344552248716354\n",
      "Training loss for batch 272 : 0.11671596765518188\n",
      "Training loss for batch 273 : 0.5527734160423279\n",
      "Training loss for batch 274 : 0.03503841534256935\n",
      "Training loss for batch 275 : 0.031740911304950714\n",
      "Training loss for batch 276 : 0.13060054183006287\n",
      "Training loss for batch 277 : 0.048984792083501816\n",
      "Training loss for batch 278 : 0.1431395262479782\n",
      "Training loss for batch 279 : 0.09231525659561157\n",
      "Training loss for batch 280 : 0.019489338621497154\n",
      "Training loss for batch 281 : 0.3731636703014374\n",
      "Training loss for batch 282 : 0.31623411178588867\n",
      "Training loss for batch 283 : 0.08702637255191803\n",
      "Training loss for batch 284 : 0.8175604343414307\n",
      "Training loss for batch 285 : 0.5318979620933533\n",
      "Training loss for batch 286 : 0.018823888152837753\n",
      "Training loss for batch 287 : 0.3141249716281891\n",
      "Training loss for batch 288 : 0.20100249350070953\n",
      "Training loss for batch 289 : 0.11918755620718002\n",
      "Training loss for batch 290 : 0.24292927980422974\n",
      "Training loss for batch 291 : 0.14035692811012268\n",
      "Training loss for batch 292 : 0.29498887062072754\n",
      "Training loss for batch 293 : 0.11714836210012436\n",
      "Training loss for batch 294 : 0.29970648884773254\n",
      "Training loss for batch 295 : 0.0\n",
      "Training loss for batch 296 : 0.892955482006073\n",
      "Training loss for batch 297 : 0.7592717409133911\n",
      "Training loss for batch 298 : 0.28100767731666565\n",
      "Training loss for batch 299 : 0.11226115375757217\n",
      "Training loss for batch 300 : 0.1791144609451294\n",
      "Training loss for batch 301 : 0.4347269535064697\n",
      "Training loss for batch 302 : 0.0002618691651150584\n",
      "Training loss for batch 303 : 0.6193057298660278\n",
      "Training loss for batch 304 : 0.09249775111675262\n",
      "Training loss for batch 305 : 0.25301671028137207\n",
      "Training loss for batch 306 : 0.10069263726472855\n",
      "Training loss for batch 307 : 0.5410832762718201\n",
      "Training loss for batch 308 : 0.08357467502355576\n",
      "Training loss for batch 309 : 0.28332623839378357\n",
      "Training loss for batch 310 : 0.1652461737394333\n",
      "Training loss for batch 311 : 0.4390687644481659\n",
      "Training loss for batch 312 : 0.31309831142425537\n",
      "Training loss for batch 313 : 0.46745944023132324\n",
      "Training loss for batch 314 : 0.2710639536380768\n",
      "Training loss for batch 315 : 0.09291690587997437\n",
      "Training loss for batch 316 : 0.06734031438827515\n",
      "Training loss for batch 317 : 0.18757595121860504\n",
      "Training loss for batch 318 : 0.12364815175533295\n",
      "Training loss for batch 319 : 0.43807098269462585\n",
      "Training loss for batch 320 : 0.4888741075992584\n",
      "Training loss for batch 321 : 0.2707693874835968\n",
      "Training loss for batch 322 : 0.033194903284311295\n",
      "Training loss for batch 323 : 0.1479254513978958\n",
      "Training loss for batch 324 : 0.15913401544094086\n",
      "Training loss for batch 325 : 0.17483672499656677\n",
      "Training loss for batch 326 : 0.0031304010190069675\n",
      "Training loss for batch 327 : 0.06470426917076111\n",
      "Training loss for batch 328 : 0.16035489737987518\n",
      "Training loss for batch 329 : 0.15836071968078613\n",
      "Training loss for batch 330 : 0.10101030766963959\n",
      "Training loss for batch 331 : 0.3827020227909088\n",
      "Training loss for batch 332 : 0.35444310307502747\n",
      "Training loss for batch 333 : 0.2270146757364273\n",
      "Training loss for batch 334 : 0.29564040899276733\n",
      "Training loss for batch 335 : 0.40981152653694153\n",
      "Training loss for batch 336 : 0.5623247623443604\n",
      "Training loss for batch 337 : 0.15797655284404755\n",
      "Training loss for batch 338 : 0.6306021809577942\n",
      "Training loss for batch 339 : 0.27772003412246704\n",
      "Training loss for batch 340 : 0.4961138665676117\n",
      "Training loss for batch 341 : 0.5571926832199097\n",
      "Training loss for batch 342 : 0.21175867319107056\n",
      "Training loss for batch 343 : 0.1976121962070465\n",
      "Training loss for batch 344 : 0.5560715198516846\n",
      "Training loss for batch 345 : 0.21614080667495728\n",
      "Training loss for batch 346 : 0.20165106654167175\n",
      "Training loss for batch 347 : 0.16519753634929657\n",
      "Training loss for batch 348 : 0.23745103180408478\n",
      "Training loss for batch 349 : 0.6644330620765686\n",
      "Training loss for batch 350 : 0.32652410864830017\n",
      "Training loss for batch 351 : 0.4513927400112152\n",
      "Training loss for batch 352 : 0.2418535351753235\n",
      "Training loss for batch 353 : 0.5199890732765198\n",
      "Training loss for batch 354 : 0.20697839558124542\n",
      "Training loss for batch 355 : 0.37854841351509094\n",
      "Training loss for batch 356 : 0.2997687757015228\n",
      "Training loss for batch 357 : 0.1336362659931183\n",
      "Training loss for batch 358 : 0.5336737632751465\n",
      "Training loss for batch 359 : 0.3749289810657501\n",
      "Training loss for batch 360 : 0.5390178561210632\n",
      "Training loss for batch 361 : 0.4365065097808838\n",
      "Training loss for batch 362 : 0.26390376687049866\n",
      "Training loss for batch 363 : 0.4711132049560547\n",
      "Training loss for batch 364 : 0.2506014108657837\n",
      "Training loss for batch 365 : 0.21896463632583618\n",
      "Training loss for batch 366 : 0.23581205308437347\n",
      "Training loss for batch 367 : 0.14867772161960602\n",
      "Training loss for batch 368 : 0.1766594648361206\n",
      "Training loss for batch 369 : 0.08203816413879395\n",
      "Training loss for batch 370 : 0.385994553565979\n",
      "Training loss for batch 371 : 0.8199962973594666\n",
      "Training loss for batch 372 : 0.15182307362556458\n",
      "Training loss for batch 373 : 0.4197458326816559\n",
      "Training loss for batch 374 : 0.5510290861129761\n",
      "Training loss for batch 375 : 0.1931212693452835\n",
      "Training loss for batch 376 : 0.30910664796829224\n",
      "Training loss for batch 377 : 0.3374249339103699\n",
      "Training loss for batch 378 : 0.42510896921157837\n",
      "Training loss for batch 379 : 0.29954835772514343\n",
      "Training loss for batch 380 : 0.47463351488113403\n",
      "Training loss for batch 381 : 0.0926961675286293\n",
      "Training loss for batch 382 : 0.43229639530181885\n",
      "Training loss for batch 383 : 0.5486646294593811\n",
      "Training loss for batch 384 : 0.3348402678966522\n",
      "Training loss for batch 385 : 0.6526395678520203\n",
      "Training loss for batch 386 : 0.19664263725280762\n",
      "Training loss for batch 387 : 0.4257763624191284\n",
      "Training loss for batch 388 : 0.00832344125956297\n",
      "Training loss for batch 389 : 0.09476199746131897\n",
      "Training loss for batch 390 : 0.054951176047325134\n",
      "Training loss for batch 391 : 0.23083949089050293\n",
      "Training loss for batch 392 : 0.13162566721439362\n",
      "Training loss for batch 393 : 0.15447932481765747\n",
      "Training loss for batch 394 : 0.2517276704311371\n",
      "Training loss for batch 395 : 0.2447785884141922\n",
      "Training loss for batch 396 : 0.13126128911972046\n",
      "Training loss for batch 397 : 0.14618580043315887\n",
      "Training loss for batch 398 : 0.05646003782749176\n",
      "Training loss for batch 399 : 0.10348081588745117\n",
      "Training loss for batch 400 : 0.16354238986968994\n",
      "Training loss for batch 401 : 0.5426234006881714\n",
      "Training loss for batch 402 : 0.3129849135875702\n",
      "Training loss for batch 403 : 0.0708528384566307\n",
      "Training loss for batch 404 : 0.08610501140356064\n",
      "Training loss for batch 405 : 0.38219135999679565\n",
      "Training loss for batch 406 : 0.2829877734184265\n",
      "Training loss for batch 407 : 0.15716814994812012\n",
      "Training loss for batch 408 : 0.3445637822151184\n",
      "Training loss for batch 409 : 0.1612740010023117\n",
      "Training loss for batch 410 : 0.18081560730934143\n",
      "Training loss for batch 411 : 0.2891518175601959\n",
      "Training loss for batch 412 : 0.032595276832580566\n",
      "Training loss for batch 413 : 0.25061824917793274\n",
      "Training loss for batch 414 : 0.5832082629203796\n",
      "Training loss for batch 415 : 0.28468942642211914\n",
      "Training loss for batch 416 : 0.18485288321971893\n",
      "Training loss for batch 417 : 0.41632649302482605\n",
      "Training loss for batch 418 : 0.5056319832801819\n",
      "Training loss for batch 419 : 0.44612908363342285\n",
      "Training loss for batch 420 : 0.11299864202737808\n",
      "Training loss for batch 421 : 0.5110020041465759\n",
      "Training loss for batch 422 : 0.005243271589279175\n",
      "Training loss for batch 423 : 0.41957709193229675\n",
      "Training loss for batch 424 : 0.25658807158470154\n",
      "Training loss for batch 425 : 0.3776983320713043\n",
      "Training loss for batch 426 : 0.01749994419515133\n",
      "Training loss for batch 427 : 0.3370847702026367\n",
      "Training loss for batch 428 : 0.46984612941741943\n",
      "Training loss for batch 429 : 0.2523956596851349\n",
      "Training loss for batch 430 : 0.15684694051742554\n",
      "Training loss for batch 431 : 0.8563708662986755\n",
      "Training loss for batch 432 : 0.32766520977020264\n",
      "Training loss for batch 433 : 0.33225101232528687\n",
      "Training loss for batch 434 : 0.23658648133277893\n",
      "Training loss for batch 435 : 0.12759467959403992\n",
      "Training loss for batch 436 : 0.26541435718536377\n",
      "Training loss for batch 437 : 0.31508970260620117\n",
      "Training loss for batch 438 : 0.401313841342926\n",
      "Training loss for batch 439 : 0.2928375005722046\n",
      "Training loss for batch 440 : 0.04036077857017517\n",
      "Training loss for batch 441 : 0.13066579401493073\n",
      "Training loss for batch 442 : 0.19644685089588165\n",
      "Training loss for batch 443 : 0.581825315952301\n",
      "Training loss for batch 444 : 0.1977444440126419\n",
      "Training loss for batch 445 : 0.07167839258909225\n",
      "Training loss for batch 446 : 0.6183930039405823\n",
      "Training loss for batch 447 : 0.0375065878033638\n",
      "Training loss for batch 448 : 0.23036473989486694\n",
      "Training loss for batch 449 : 0.2925291359424591\n",
      "Training loss for batch 450 : 0.39676257967948914\n",
      "Training loss for batch 451 : 0.2881182134151459\n",
      "Training loss for batch 452 : 0.25782400369644165\n",
      "Training loss for batch 453 : 0.19585378468036652\n",
      "Training loss for batch 454 : 0.15229515731334686\n",
      "Training loss for batch 455 : 0.09226315468549728\n",
      "Training loss for batch 456 : 0.11806070804595947\n",
      "Training loss for batch 457 : 0.39366400241851807\n",
      "Training loss for batch 458 : 0.18297424912452698\n",
      "Training loss for batch 459 : 0.2416067272424698\n",
      "Training loss for batch 460 : 0.08610397577285767\n",
      "Training loss for batch 461 : 0.5544309616088867\n",
      "Training loss for batch 462 : 0.5081283450126648\n",
      "Training loss for batch 463 : 0.19666597247123718\n",
      "Training loss for batch 464 : 0.4944961667060852\n",
      "Training loss for batch 465 : 0.3701646625995636\n",
      "Training loss for batch 466 : 0.3959428369998932\n",
      "Training loss for batch 467 : 0.11869646608829498\n",
      "Training loss for batch 468 : 0.01236104965209961\n",
      "Training loss for batch 469 : 0.5134992599487305\n",
      "Training loss for batch 470 : 0.4100019335746765\n",
      "Training loss for batch 471 : 0.33159294724464417\n",
      "Training loss for batch 472 : 0.1520557552576065\n",
      "Training loss for batch 473 : 0.25627967715263367\n",
      "Training loss for batch 474 : 0.11620281636714935\n",
      "Training loss for batch 475 : 0.27394556999206543\n",
      "Training loss for batch 476 : 0.18415620923042297\n",
      "Training loss for batch 477 : 0.20416387915611267\n",
      "Training loss for batch 478 : 0.20661666989326477\n",
      "Training loss for batch 479 : 0.18804071843624115\n",
      "Training loss for batch 480 : 0.16775821149349213\n",
      "Training loss for batch 481 : 0.1907424032688141\n",
      "Training loss for batch 482 : 0.30591344833374023\n",
      "Training loss for batch 483 : 0.38255447149276733\n",
      "Training loss for batch 484 : 0.29031381011009216\n",
      "Training loss for batch 485 : 0.15558643639087677\n",
      "Training loss for batch 486 : 0.041842617094516754\n",
      "Training loss for batch 487 : 0.26049792766571045\n",
      "Training loss for batch 488 : 0.16869112849235535\n",
      "Training loss for batch 489 : 0.06258728355169296\n",
      "Training loss for batch 490 : 0.4989648163318634\n",
      "Training loss for batch 491 : 0.3561014235019684\n",
      "Training loss for batch 492 : 0.3663645386695862\n",
      "Training loss for batch 493 : 0.09950435906648636\n",
      "Training loss for batch 494 : 0.5800337791442871\n",
      "Training loss for batch 495 : 0.11704535782337189\n",
      "Training loss for batch 496 : 0.07155638933181763\n",
      "Training loss for batch 497 : 0.30816254019737244\n",
      "Training loss for batch 498 : 0.38784870505332947\n",
      "Training loss for batch 499 : 0.1900469958782196\n",
      "Training loss for batch 500 : 0.20459359884262085\n",
      "Training loss for batch 501 : 0.14250749349594116\n",
      "Training loss for batch 502 : 0.11276882886886597\n",
      "Training loss for batch 503 : 0.23375895619392395\n",
      "Training loss for batch 504 : 0.26871052384376526\n",
      "Training loss for batch 505 : 0.33268818259239197\n",
      "Training loss for batch 506 : 0.2598165273666382\n",
      "Training loss for batch 507 : 0.09466174244880676\n",
      "Training loss for batch 508 : 0.2791728973388672\n",
      "Training loss for batch 509 : 0.3471113443374634\n",
      "Training loss for batch 510 : 0.15441885590553284\n",
      "Training loss for batch 511 : 0.5375949740409851\n",
      "Training loss for batch 512 : 0.6070126891136169\n",
      "Training loss for batch 513 : 0.11517932265996933\n",
      "Training loss for batch 514 : 0.30774885416030884\n",
      "Training loss for batch 515 : 0.49317115545272827\n",
      "Training loss for batch 516 : 0.3654140532016754\n",
      "Training loss for batch 517 : 0.4081585705280304\n",
      "Training loss for batch 518 : 0.026547737419605255\n",
      "Training loss for batch 519 : 0.08371765911579132\n",
      "Training loss for batch 520 : 0.43041402101516724\n",
      "Training loss for batch 521 : 0.06672617793083191\n",
      "Training loss for batch 522 : 0.4354425370693207\n",
      "Training loss for batch 523 : 0.33440127968788147\n",
      "Training loss for batch 524 : 0.1801925152540207\n",
      "Training loss for batch 525 : 0.19377988576889038\n",
      "Training loss for batch 526 : 0.14384686946868896\n",
      "Training loss for batch 527 : 0.30471712350845337\n",
      "Training loss for batch 528 : 0.22060737013816833\n",
      "Training loss for batch 529 : 0.10616528987884521\n",
      "Training loss for batch 530 : 0.37516993284225464\n",
      "Training loss for batch 531 : 0.5354388952255249\n",
      "Training loss for batch 532 : 0.23958438634872437\n",
      "Training loss for batch 533 : 0.23298121988773346\n",
      "Training loss for batch 534 : 0.12921062111854553\n",
      "Training loss for batch 535 : 0.2872248888015747\n",
      "Training loss for batch 536 : 0.2378193587064743\n",
      "Training loss for batch 537 : 0.42774444818496704\n",
      "Training loss for batch 538 : 0.03015035390853882\n",
      "Training loss for batch 539 : 0.1785806119441986\n",
      "Training loss for batch 540 : 0.14862704277038574\n",
      "Training loss for batch 541 : 0.2651982605457306\n",
      "Training loss for batch 542 : 0.3887253999710083\n",
      "Training loss for batch 543 : 0.271506667137146\n",
      "Training loss for batch 544 : 0.1484019160270691\n",
      "Training loss for batch 545 : 0.22452443838119507\n",
      "Training loss for batch 546 : 0.4563618004322052\n",
      "Training loss for batch 547 : 0.08108635991811752\n",
      "Training loss for batch 548 : 0.38904890418052673\n",
      "Training loss for batch 549 : 0.25636664032936096\n",
      "Training loss for batch 550 : 0.13416458666324615\n",
      "Training loss for batch 551 : 0.1689683347940445\n",
      "Training loss for batch 552 : 0.009095285087823868\n",
      "Training loss for batch 553 : 0.21601645648479462\n",
      "Training loss for batch 554 : 0.04665501415729523\n",
      "Training loss for batch 555 : 0.6171674728393555\n",
      "Training loss for batch 556 : 0.10288020223379135\n",
      "Training loss for batch 557 : 0.3337559998035431\n",
      "Training loss for batch 558 : 0.29300063848495483\n",
      "Training loss for batch 559 : 0.04057411476969719\n",
      "Training loss for batch 560 : 0.0631813108921051\n",
      "Training loss for batch 561 : 0.32758644223213196\n",
      "Training loss for batch 562 : 0.11662455648183823\n",
      "Training loss for batch 563 : 0.05136125907301903\n",
      "Training loss for batch 564 : 0.25622645020484924\n",
      "Training loss for batch 565 : 0.38981425762176514\n",
      "Training loss for batch 566 : 0.5949411392211914\n",
      "Training loss for batch 567 : 0.3642658293247223\n",
      "Training loss for batch 568 : 0.20518173277378082\n",
      "Training loss for batch 569 : 0.23265139758586884\n",
      "Training loss for batch 570 : 0.3513143062591553\n",
      "Training loss for batch 571 : 0.5187304019927979\n",
      "Training loss for batch 572 : 0.0389990471303463\n",
      "Training loss for batch 573 : 0.40608251094818115\n",
      "Training loss for batch 574 : 0.4856070876121521\n",
      "Training loss for batch 575 : 0.5086724758148193\n",
      "Training loss for batch 576 : 0.21972322463989258\n",
      "Training loss for batch 577 : 0.7888144254684448\n",
      "Training loss for batch 578 : 0.1591387689113617\n",
      "Training loss for batch 579 : 0.1738625466823578\n",
      "Training loss for batch 580 : 0.06353423744440079\n",
      "Training loss for batch 581 : 0.27653607726097107\n",
      "Training loss for batch 582 : 0.13954955339431763\n",
      "Training loss for batch 583 : 0.2449084222316742\n",
      "Training loss for batch 584 : 0.16627244651317596\n",
      "Training loss for batch 585 : 0.2745550572872162\n",
      "Training loss for batch 586 : 0.16618812084197998\n",
      "Training loss for batch 587 : 0.36206650733947754\n",
      "Training loss for batch 588 : 0.5427117943763733\n",
      "Training loss for batch 589 : 0.31701844930648804\n",
      "Training loss for batch 590 : 0.0980573371052742\n",
      "Training loss for batch 591 : 0.2923837900161743\n",
      "Training loss for batch 592 : 0.1872958391904831\n",
      "Training loss for batch 593 : 0.2909204363822937\n",
      "Training loss for batch 594 : 0.059183649718761444\n",
      "Training loss for batch 595 : 0.24752818048000336\n",
      "Training loss for batch 596 : 0.23464295268058777\n",
      "Training loss for batch 597 : 0.1396331489086151\n",
      "Training loss for batch 598 : 0.09158932417631149\n",
      "Training loss for batch 599 : 0.3791993260383606\n",
      "Training loss for batch 600 : 0.23723503947257996\n",
      "Training loss for batch 601 : 0.41904887557029724\n",
      "Training loss for batch 602 : 0.04763001576066017\n",
      "Training loss for batch 603 : 0.15154054760932922\n",
      "Training loss for batch 604 : 0.2512364685535431\n",
      "Training loss for batch 605 : 0.39024606347084045\n",
      "Training loss for batch 606 : 0.3061457574367523\n",
      "Training loss for batch 607 : 0.2610693573951721\n",
      "Training loss for batch 608 : 0.17604413628578186\n",
      "Training loss for batch 609 : 0.0734555795788765\n",
      "Training loss for batch 610 : 0.31100594997406006\n",
      "Training loss for batch 611 : 0.2709271311759949\n",
      "Training loss for batch 612 : 0.29616430401802063\n",
      "Training loss for batch 613 : 0.10672874003648758\n",
      "Training loss for batch 614 : 0.4550461769104004\n",
      "Training loss for batch 615 : 0.08741350471973419\n",
      "Training loss for batch 616 : 0.3498019278049469\n",
      "Training loss for batch 617 : 0.26404523849487305\n",
      "Training loss for batch 618 : 0.5261449217796326\n",
      "Training loss for batch 619 : 0.4487012028694153\n",
      "Training loss for batch 620 : 0.4778606593608856\n",
      "Training loss for batch 621 : 0.17378398776054382\n",
      "Training loss for batch 622 : 0.21100811660289764\n",
      "Training loss for batch 623 : 0.3956526815891266\n",
      "Training loss for batch 624 : 0.1696775257587433\n",
      "Training loss for batch 625 : 0.22466090321540833\n",
      "Training loss for batch 626 : 0.10166196525096893\n",
      "Training loss for batch 627 : 0.1824464052915573\n",
      "Training loss for batch 628 : 0.5613166689872742\n",
      "Training loss for batch 629 : 0.23925155401229858\n",
      "Training loss for batch 630 : 0.42271751165390015\n",
      "Training loss for batch 631 : 0.16783170402050018\n",
      "Training loss for batch 632 : 0.19563011825084686\n",
      "Training loss for batch 633 : 0.3374878168106079\n",
      "Training loss for batch 634 : 0.47663018107414246\n",
      "Training loss for batch 635 : 0.3900087773799896\n",
      "Training loss for batch 636 : 0.15509766340255737\n",
      "Training loss for batch 637 : 0.5600460767745972\n",
      "Training loss for batch 638 : 0.4685121774673462\n",
      "Training loss for batch 639 : 0.02618780918419361\n",
      "Training loss for batch 640 : 0.46125006675720215\n",
      "Training loss for batch 641 : 0.1159900426864624\n",
      "Training loss for batch 642 : 0.19417299330234528\n",
      "Training loss for batch 643 : 0.3683701157569885\n",
      "Training loss for batch 644 : 0.24549520015716553\n",
      "Training loss for batch 645 : 0.22334036231040955\n",
      "Training loss for batch 646 : 0.21220634877681732\n",
      "Training loss for batch 647 : 0.22465446591377258\n",
      "Training loss for batch 648 : 0.28962820768356323\n",
      "Training loss for batch 649 : 0.031101111322641373\n",
      "Training loss for batch 650 : 0.11457772552967072\n",
      "Training loss for batch 651 : 0.2588635981082916\n",
      "Training loss for batch 652 : 0.1874590516090393\n",
      "Training loss for batch 653 : 0.019677918404340744\n",
      "Training loss for batch 654 : 0.4902605712413788\n",
      "Training loss for batch 655 : 0.21338503062725067\n",
      "Training loss for batch 656 : 0.06368210911750793\n",
      "Training loss for batch 657 : 0.8232637047767639\n",
      "Training loss for batch 658 : 0.47603839635849\n",
      "Training loss for batch 659 : 0.16280685365200043\n",
      "Training loss for batch 660 : 0.5380170345306396\n",
      "Training loss for batch 661 : 0.2430788278579712\n",
      "Training loss for batch 662 : 0.23535656929016113\n",
      "Training loss for batch 663 : 0.5209579467773438\n",
      "Training loss for batch 664 : 0.3787214159965515\n",
      "Training loss for batch 665 : 0.15910683572292328\n",
      "Training loss for batch 666 : 0.04720931127667427\n",
      "Training loss for batch 667 : 0.12442967295646667\n",
      "Training loss for batch 668 : 0.41771799325942993\n",
      "Training loss for batch 669 : 0.00961428415030241\n",
      "Training loss for batch 670 : 0.1173657476902008\n",
      "Training loss for batch 671 : 0.28645995259284973\n",
      "Training loss for batch 672 : 0.4698108434677124\n",
      "Training loss for batch 673 : 0.24961119890213013\n",
      "Training loss for batch 674 : 0.18345481157302856\n",
      "Training loss for batch 675 : 0.5267441868782043\n",
      "Training loss for batch 676 : 0.44108185172080994\n",
      "Training loss for batch 677 : 0.10875093191862106\n",
      "Training loss for batch 678 : 0.2933207154273987\n",
      "Training loss for batch 679 : 0.03651837632060051\n",
      "Training loss for batch 680 : 0.24175509810447693\n",
      "Training loss for batch 681 : 0.28309866786003113\n",
      "Training loss for batch 682 : 0.021721430122852325\n",
      "Training loss for batch 683 : 0.2168356329202652\n",
      "Training loss for batch 684 : 0.47007495164871216\n",
      "Training loss for batch 685 : 0.21809612214565277\n",
      "Training loss for batch 686 : 0.279509574174881\n",
      "Training loss for batch 687 : 0.29610711336135864\n",
      "Training loss for batch 688 : 0.09696856886148453\n",
      "Training loss for batch 689 : 0.30049481987953186\n",
      "Training loss for batch 690 : 0.0350409559905529\n",
      "Training loss for batch 691 : 0.10621019452810287\n",
      "Training loss for batch 692 : 0.2831076383590698\n",
      "Training loss for batch 693 : 0.006009827367961407\n",
      "Training loss for batch 694 : 0.1523861587047577\n",
      "Training loss for batch 695 : 0.25125810503959656\n",
      "Training loss for batch 696 : 0.6676393151283264\n",
      "Training loss for batch 697 : 0.33370548486709595\n",
      "Training loss for batch 698 : 0.0472203865647316\n",
      "Training loss for batch 699 : 0.303428053855896\n",
      "Training loss for batch 700 : 0.15483351051807404\n",
      "Training loss for batch 701 : 0.210012286901474\n",
      "Training loss for batch 702 : 0.37890946865081787\n",
      "Training loss for batch 703 : 0.40664994716644287\n",
      "Training loss for batch 704 : 0.019671987742185593\n",
      "Training loss for batch 705 : 0.34831708669662476\n",
      "Training loss for batch 706 : 0.16387714445590973\n",
      "Training loss for batch 707 : 0.23258456587791443\n",
      "Training loss for batch 708 : 0.11937540769577026\n",
      "Training loss for batch 709 : 0.5569989085197449\n",
      "Training loss for batch 710 : 0.2201942354440689\n",
      "Training loss for batch 711 : 0.21489450335502625\n",
      "Training loss for batch 712 : 0.6110968589782715\n",
      "Training loss for batch 713 : 0.11433703452348709\n",
      "Training loss for batch 714 : 0.23290616273880005\n",
      "Training loss for batch 715 : 0.5118199586868286\n",
      "Training loss for batch 716 : 0.3963596522808075\n",
      "Training loss for batch 717 : 0.13803234696388245\n",
      "Training loss for batch 718 : 0.2692134976387024\n",
      "Training loss for batch 719 : 0.5830724239349365\n",
      "Training loss for batch 720 : 0.7831783890724182\n",
      "Training loss for batch 721 : 0.0686330646276474\n",
      "Training loss for batch 722 : 0.25666993856430054\n",
      "Training loss for batch 723 : 0.13726484775543213\n",
      "Training loss for batch 724 : 0.19390854239463806\n",
      "Training loss for batch 725 : 0.5006913542747498\n",
      "Training loss for batch 726 : 0.5696280598640442\n",
      "Training loss for batch 727 : 0.2162749320268631\n",
      "Training loss for batch 728 : 0.49747127294540405\n",
      "Training loss for batch 729 : 0.17250408232212067\n",
      "Training loss for batch 730 : 0.32718250155448914\n",
      "Training loss for batch 731 : 0.33473989367485046\n",
      "Training loss for batch 732 : 0.6417367458343506\n",
      "Training loss for batch 733 : 0.13383179903030396\n",
      "Training loss for batch 734 : 0.2489173412322998\n",
      "Training loss for batch 735 : 0.17243441939353943\n",
      "Training loss for batch 736 : 0.21742594242095947\n",
      "Training loss for batch 737 : 0.32459038496017456\n",
      "Training loss for batch 738 : 0.08349677175283432\n",
      "Training loss for batch 739 : 0.22427907586097717\n",
      "Training loss for batch 740 : 0.16758093237876892\n",
      "Training loss for batch 741 : 0.02397189661860466\n",
      "Training loss for batch 742 : 0.0849456638097763\n",
      "Training loss for batch 743 : 0.1907701939344406\n",
      "Training loss for batch 744 : 0.2331363707780838\n",
      "Training loss for batch 745 : 0.2490054965019226\n",
      "Training loss for batch 746 : 0.026665490120649338\n",
      "Training loss for batch 747 : 0.2954024374485016\n",
      "Training loss for batch 748 : 0.03621339425444603\n",
      "Training loss for batch 749 : 0.027337949723005295\n",
      "Training loss for batch 750 : 0.4471036493778229\n",
      "Training loss for batch 751 : 0.2761482298374176\n",
      "Training loss for batch 752 : 0.39409250020980835\n",
      "Training loss for batch 753 : 0.3562465310096741\n",
      "Training loss for batch 754 : 0.2764585018157959\n",
      "Training loss for batch 755 : 0.4735867977142334\n",
      "Training loss for batch 756 : 0.285055935382843\n",
      "Training loss for batch 757 : 0.19652383029460907\n",
      "Training loss for batch 758 : 0.08022523671388626\n",
      "Training loss for batch 759 : 0.12632517516613007\n",
      "Training loss for batch 760 : 0.0928906500339508\n",
      "Training loss for batch 761 : 0.4297066628932953\n",
      "Training loss for batch 762 : 0.45801764726638794\n",
      "Training loss for batch 763 : 0.45126572251319885\n",
      "Training loss for batch 764 : 0.5063033699989319\n",
      "Training loss for batch 765 : 0.18769751489162445\n",
      "Training loss for batch 766 : 0.0\n",
      "Training loss for batch 767 : 0.03759235143661499\n",
      "Training loss for batch 768 : 0.11237458884716034\n",
      "Training loss for batch 769 : 0.10642092674970627\n",
      "Training loss for batch 770 : 0.7437046766281128\n",
      "Training loss for batch 771 : 0.26881226897239685\n",
      "Training loss for batch 772 : 0.2579568326473236\n",
      "Training loss for batch 773 : 0.2706173360347748\n",
      "Training loss for batch 774 : 0.09313520044088364\n",
      "Training loss for batch 775 : 0.21150648593902588\n",
      "Training loss for batch 776 : 0.39196035265922546\n",
      "Training loss for batch 777 : 0.30626624822616577\n",
      "Training loss for batch 778 : 0.25980308651924133\n",
      "Training loss for batch 779 : 0.05136214941740036\n",
      "Training loss for batch 780 : 0.16923624277114868\n",
      "Training loss for batch 781 : 0.19211407005786896\n",
      "Training loss for batch 782 : 0.24151106178760529\n",
      "Training loss for batch 783 : 0.31057071685791016\n",
      "Training loss for batch 784 : 0.2534604072570801\n",
      "Training loss for batch 785 : 0.22031298279762268\n",
      "Training loss for batch 786 : 0.18122416734695435\n",
      "Training loss for batch 787 : 0.14513303339481354\n",
      "Training loss for batch 788 : 0.18866199254989624\n",
      "Training loss for batch 789 : 0.02375340461730957\n",
      "Training loss for batch 790 : 0.030159343034029007\n",
      "Training loss for batch 791 : 0.35035941004753113\n",
      "Training loss for batch 792 : 0.2867773175239563\n",
      "Training loss for batch 793 : 0.012390474788844585\n",
      "Training loss for batch 794 : 0.19011415541172028\n",
      "Training loss for batch 795 : 0.13230706751346588\n",
      "Training loss for batch 796 : 0.21270573139190674\n",
      "Training loss for batch 797 : 0.0493951141834259\n",
      "Training loss for batch 798 : 0.04414687305688858\n",
      "Training loss for batch 799 : 0.5629271864891052\n",
      "Training loss for batch 800 : 0.08971089124679565\n",
      "Training loss for batch 801 : 0.32627159357070923\n",
      "Training loss for batch 802 : 0.07164984196424484\n",
      "Training loss for batch 803 : 0.08916900306940079\n",
      "Training loss for batch 804 : 0.30865150690078735\n",
      "Training loss for batch 805 : 0.06562767922878265\n",
      "Training loss for batch 806 : 0.42216867208480835\n",
      "Training loss for batch 807 : 0.46864980459213257\n",
      "Training loss for batch 808 : 0.0671255961060524\n",
      "Training loss for batch 809 : 0.8661729097366333\n",
      "Training loss for batch 810 : 0.04813464730978012\n",
      "Training loss for batch 811 : 0.1607527732849121\n",
      "Training loss for batch 812 : 0.439994752407074\n",
      "Training loss for batch 813 : 0.08948855847120285\n",
      "Training loss for batch 814 : 0.02960369549691677\n",
      "Training loss for batch 815 : 0.20333372056484222\n",
      "Training loss for batch 816 : 0.18893897533416748\n",
      "Training loss for batch 817 : 0.37908095121383667\n",
      "Training loss for batch 818 : 0.2312925159931183\n",
      "Training loss for batch 819 : 0.3122936189174652\n",
      "Training loss for batch 820 : 0.19868694245815277\n",
      "Training loss for batch 821 : 0.09505842626094818\n",
      "Training loss for batch 822 : 0.3044702708721161\n",
      "Training loss for batch 823 : 0.5337051153182983\n",
      "Training loss for batch 824 : 0.11850224435329437\n",
      "Training loss for batch 825 : 0.0\n",
      "Training loss for batch 826 : 0.22827282547950745\n",
      "Training loss for batch 827 : 0.37164968252182007\n",
      "Training loss for batch 828 : 0.21194052696228027\n",
      "Training loss for batch 829 : 0.14877429604530334\n",
      "Training loss for batch 830 : 0.09433583170175552\n",
      "Training loss for batch 831 : 0.42828449606895447\n",
      "Training loss for batch 832 : 0.23529328405857086\n",
      "Training loss for batch 833 : 0.06946586072444916\n",
      "Training loss for batch 834 : 0.05485009402036667\n",
      "Training loss for batch 835 : 0.40585842728614807\n",
      "Training loss for batch 836 : 0.17552094161510468\n",
      "Training loss for batch 837 : 0.08896195888519287\n",
      "Training loss for batch 838 : 0.032201457768678665\n",
      "Training loss for batch 839 : 0.21398812532424927\n",
      "Training loss for batch 840 : 0.10373138636350632\n",
      "Training loss for batch 841 : 0.2188797891139984\n",
      "Training loss for batch 842 : 0.2581423223018646\n",
      "Training loss for batch 843 : 0.39289239048957825\n",
      "Training loss for batch 844 : 0.42483845353126526\n",
      "Training loss for batch 845 : 0.21391911804676056\n",
      "Training loss for batch 846 : 0.4955042004585266\n",
      "Training loss for batch 847 : 0.3678564131259918\n",
      "Training loss for batch 848 : 0.043182045221328735\n",
      "Training loss for batch 849 : 0.16676482558250427\n",
      "Training loss for batch 850 : 0.2965342700481415\n",
      "Training loss for batch 851 : 0.025984011590480804\n",
      "Training loss for batch 852 : 0.12870433926582336\n",
      "Training loss for batch 853 : 0.14966422319412231\n",
      "Training loss for batch 854 : 0.10387558490037918\n",
      "Training loss for batch 855 : 0.3042981028556824\n",
      "Training loss for batch 856 : 0.09040138125419617\n",
      "Training loss for batch 857 : 0.04044235125184059\n",
      "Training loss for batch 858 : 0.3404090404510498\n",
      "Training loss for batch 859 : 0.11068323254585266\n",
      "Training loss for batch 860 : 0.13395234942436218\n",
      "Training loss for batch 861 : 0.179650217294693\n",
      "Training loss for batch 862 : 0.007291059009730816\n",
      "Training loss for batch 863 : 0.5313239097595215\n",
      "Training loss for batch 864 : 0.6287760734558105\n",
      "Training loss for batch 865 : 0.617821216583252\n",
      "Training loss for batch 866 : 0.06387332081794739\n",
      "Training loss for batch 867 : 0.018159370869398117\n",
      "Training loss for batch 868 : 0.7439358234405518\n",
      "Training loss for batch 869 : 0.4469279646873474\n",
      "Training loss for batch 870 : 0.5801093578338623\n",
      "Training loss for batch 871 : 0.1720566600561142\n",
      "Training loss for batch 872 : 0.05156561732292175\n",
      "Training loss for batch 873 : 0.6822977662086487\n",
      "Training loss for batch 874 : 0.40913334488868713\n",
      "Training loss for batch 875 : 0.2629503905773163\n",
      "Training loss for batch 876 : 0.18357674777507782\n",
      "Training loss for batch 877 : 0.7689816951751709\n",
      "Training loss for batch 878 : 0.32586362957954407\n",
      "Training loss for batch 879 : 0.33533650636672974\n",
      "Training loss for batch 880 : 0.36394473910331726\n",
      "Training loss for batch 881 : 0.05501733720302582\n",
      "Training loss for batch 882 : 0.0569344162940979\n",
      "Training loss for batch 883 : 0.20875532925128937\n",
      "Training loss for batch 884 : 0.1666894257068634\n",
      "Training loss for batch 885 : 0.0\n",
      "Training loss for batch 886 : 0.06786788254976273\n",
      "Training loss for batch 887 : 0.08476951718330383\n",
      "Training loss for batch 888 : 0.5137133598327637\n",
      "Training loss for batch 889 : 0.17345838248729706\n",
      "Training loss for batch 890 : 0.03773409128189087\n",
      "Training loss for batch 891 : 0.23724278807640076\n",
      "Training loss for batch 892 : 0.5584601163864136\n",
      "Training loss for batch 893 : 0.35475486516952515\n",
      "Training loss for batch 894 : 0.299955278635025\n",
      "Training loss for batch 895 : 0.2872771918773651\n",
      "Training loss for batch 896 : 0.08016549795866013\n",
      "Training loss for batch 897 : 0.5706663131713867\n",
      "Training loss for batch 898 : 0.08027051389217377\n",
      "Training loss for batch 899 : 0.16147038340568542\n",
      "Training loss for batch 900 : 0.146140456199646\n",
      "Training loss for batch 901 : 0.05365952104330063\n",
      "Training loss for batch 902 : 0.12973573803901672\n",
      "Training loss for batch 903 : 0.33864089846611023\n",
      "Training loss for batch 904 : 0.00159936910495162\n",
      "Training loss for batch 905 : 0.20224998891353607\n",
      "Training loss for batch 906 : 0.2926620543003082\n",
      "Training loss for batch 907 : 0.12887369096279144\n",
      "Training loss for batch 908 : 0.003617767244577408\n",
      "Training loss for batch 909 : 0.4019049108028412\n",
      "Training loss for batch 910 : 0.001401518122293055\n",
      "Training loss for batch 911 : 0.29347556829452515\n",
      "Training loss for batch 912 : 0.160758376121521\n",
      "Training loss for batch 913 : 0.435199111700058\n",
      "Training loss for batch 914 : 0.08470414578914642\n",
      "Training loss for batch 915 : 0.6246309280395508\n",
      "Training loss for batch 916 : 0.3944084346294403\n",
      "Training loss for batch 917 : 0.596960186958313\n",
      "Training loss for batch 918 : 0.19530978798866272\n",
      "Training loss for batch 919 : 0.2033659964799881\n",
      "Training loss for batch 920 : 0.42378517985343933\n",
      "Training loss for batch 921 : 0.018278004601597786\n",
      "Training loss for batch 922 : 0.0\n",
      "Training loss for batch 923 : 0.14670777320861816\n",
      "Training loss for batch 924 : 0.29178741574287415\n",
      "Training loss for batch 925 : 0.16512122750282288\n",
      "Training loss for batch 926 : 0.3860393762588501\n",
      "Training loss for batch 927 : 0.5001918077468872\n",
      "Training loss for batch 928 : 0.3827519118785858\n",
      "Training loss for batch 929 : 0.060161903500556946\n",
      "Training loss for batch 930 : 0.6789764761924744\n",
      "Training loss for batch 931 : 0.014410438016057014\n",
      "Training loss for batch 932 : 0.25280189514160156\n",
      "Training loss for batch 933 : 0.34136003255844116\n",
      "Training loss for batch 934 : 0.2301928550004959\n",
      "Training loss for batch 935 : 0.25815504789352417\n",
      "Training loss for batch 936 : 0.1057114452123642\n",
      "Training loss for batch 937 : 0.0859290212392807\n",
      "Training loss for batch 938 : 0.1691756546497345\n",
      "Training loss for batch 939 : 0.2629084885120392\n",
      "Training loss for batch 940 : 0.43615564703941345\n",
      "Training loss for batch 941 : 0.16697263717651367\n",
      "Training loss for batch 942 : 0.2763068675994873\n",
      "Training loss for batch 943 : 0.11868924647569656\n",
      "Training loss for batch 944 : 0.44847118854522705\n",
      "Training loss for batch 945 : 0.00273197703063488\n",
      "Training loss for batch 946 : 0.4299453794956207\n",
      "Training loss for batch 947 : 0.6017869114875793\n",
      "Training loss for batch 948 : 0.5895262956619263\n",
      "Training loss for batch 949 : 0.4119006097316742\n",
      "Training loss for batch 950 : 0.3472452163696289\n",
      "Training loss for batch 951 : 0.09475526958703995\n",
      "Training loss for batch 952 : 0.012661435641348362\n",
      "Training loss for batch 953 : 0.09091009944677353\n",
      "Training loss for batch 954 : 0.08690562844276428\n",
      "Training loss for batch 955 : 0.19287797808647156\n",
      "Training loss for batch 956 : 0.22452150285243988\n",
      "Training loss for batch 957 : 0.0790642499923706\n",
      "Training loss for batch 958 : 0.2567485570907593\n",
      "Training loss for batch 959 : 0.36339038610458374\n",
      "Training loss for batch 960 : 0.4157713055610657\n",
      "Training loss for batch 961 : 0.11928936094045639\n",
      "Training loss for batch 962 : 0.1280549168586731\n",
      "Training loss for batch 963 : 0.35297608375549316\n",
      "Training loss for batch 964 : 0.16685321927070618\n",
      "Training loss for batch 965 : 0.5179649591445923\n",
      "Training loss for batch 966 : 0.4459615647792816\n",
      "Training loss for batch 967 : 0.11022575944662094\n",
      "Training loss for batch 968 : 0.24169746041297913\n",
      "Training loss for batch 969 : 0.30788183212280273\n",
      "Training loss for batch 970 : 0.17297106981277466\n",
      "Training loss for batch 971 : 0.5358112454414368\n",
      "Training loss for batch 972 : 0.37103113532066345\n",
      "Training loss for batch 973 : 0.0834987610578537\n",
      "Training loss for batch 974 : 0.2427167445421219\n",
      "Training loss for batch 975 : 0.27887555956840515\n",
      "Training loss for batch 976 : 0.15330660343170166\n",
      "Training loss for batch 977 : 0.4550876319408417\n",
      "Training loss for batch 978 : 0.016186535358428955\n",
      "Training loss for batch 979 : 0.662432849407196\n",
      "Training loss for batch 980 : 0.32004159688949585\n",
      "Training loss for batch 981 : 0.894926130771637\n",
      "Training loss for batch 982 : 0.32205313444137573\n",
      "Training loss for batch 983 : 0.13171981275081635\n",
      "Training loss for batch 984 : 0.14169523119926453\n",
      "Training loss for batch 985 : 0.4961234927177429\n",
      "Training loss for batch 986 : 0.09791376441717148\n",
      "Training loss for batch 987 : 0.06462369859218597\n",
      "Training loss for batch 988 : 0.3068130612373352\n",
      "Training loss for batch 989 : 0.5538632869720459\n",
      "Training loss for batch 990 : 0.28415408730506897\n",
      "Training loss for batch 991 : 0.1341836154460907\n",
      "Training loss for batch 992 : 0.3134452700614929\n",
      "Training loss for batch 993 : 0.036549314856529236\n",
      "Training loss for batch 994 : 0.3187393546104431\n",
      "Training loss for batch 995 : 0.04558027163147926\n",
      "Training loss for batch 996 : 0.29506686329841614\n",
      "Training loss for batch 997 : 0.21722975373268127\n",
      "Training loss for batch 998 : 0.009927184320986271\n",
      "Training loss for batch 999 : 0.01759849116206169\n",
      "Training loss for batch 1000 : 0.20463715493679047\n",
      "Training loss for batch 1001 : 0.5777437090873718\n",
      "Training loss for batch 1002 : 0.0183500275015831\n",
      "Training loss for batch 1003 : 0.1580512970685959\n",
      "Training loss for batch 1004 : 0.07351372390985489\n",
      "Training loss for batch 1005 : 0.4059213697910309\n",
      "Training loss for batch 1006 : 0.058364447206258774\n",
      "Training loss for batch 1007 : 0.052096880972385406\n",
      "Training loss for batch 1008 : 0.08590057492256165\n",
      "Training loss for batch 1009 : 0.673416256904602\n",
      "Training loss for batch 1010 : 0.2735591530799866\n",
      "Training loss for batch 1011 : 0.2682259678840637\n",
      "Training loss for batch 1012 : 0.39166259765625\n",
      "Training loss for batch 1013 : 0.12765277922153473\n",
      "Training loss for batch 1014 : 0.19521987438201904\n",
      "Training loss for batch 1015 : 0.21904593706130981\n",
      "Training loss for batch 1016 : 0.28909844160079956\n",
      "Training loss for batch 1017 : 0.4068615734577179\n",
      "Training loss for batch 1018 : 0.42866581678390503\n",
      "Training loss for batch 1019 : 0.07129509001970291\n",
      "Training loss for batch 1020 : 0.04714149981737137\n",
      "Training loss for batch 1021 : 0.33738255500793457\n",
      "Training loss for batch 1022 : 0.020117996260523796\n",
      "Training loss for batch 1023 : 0.21449357271194458\n",
      "Training loss for batch 1024 : 0.016068240627646446\n",
      "Training loss for batch 1025 : 0.2620219588279724\n",
      "Training loss for batch 1026 : 0.173727884888649\n",
      "Training loss for batch 1027 : 0.5195194482803345\n",
      "Training loss for batch 1028 : 0.1362314224243164\n",
      "Training loss for batch 1029 : 0.21418032050132751\n",
      "Training loss for batch 1030 : 0.1568496972322464\n",
      "Training loss for batch 1031 : 0.3795348107814789\n",
      "Training loss for batch 1032 : 0.06706299632787704\n",
      "Training loss for batch 1033 : 0.20782555639743805\n",
      "Training loss for batch 1034 : 0.08594624698162079\n",
      "Training loss for batch 1035 : 0.3241884708404541\n",
      "Training loss for batch 1036 : 0.17049770057201385\n",
      "Training loss for batch 1037 : 0.2772500813007355\n",
      "Training loss for batch 1038 : 0.06493686884641647\n",
      "Training loss for batch 1039 : 0.25484535098075867\n",
      "Training loss for batch 1040 : 0.1966918557882309\n",
      "Training loss for batch 1041 : 0.3066694736480713\n",
      "Training loss for batch 1042 : 0.24410998821258545\n",
      "Training loss for batch 1043 : 0.0445309616625309\n",
      "Training loss for batch 1044 : 0.06213958561420441\n",
      "Training loss for batch 1045 : 0.18342065811157227\n",
      "Training loss for batch 1046 : 0.5080138444900513\n",
      "Training loss for batch 1047 : 0.17210908234119415\n",
      "Training loss for batch 1048 : 0.4355311095714569\n",
      "Training loss for batch 1049 : 0.17503474652767181\n",
      "Training loss for batch 1050 : 0.13233810663223267\n",
      "Training loss for batch 1051 : 0.06419496983289719\n",
      "Training loss for batch 1052 : 0.004612905438989401\n",
      "Training loss for batch 1053 : 0.7233028411865234\n",
      "Training loss for batch 1054 : 0.02669503167271614\n",
      "Training loss for batch 1055 : 0.13059425354003906\n",
      "Training loss for batch 1056 : 0.33387088775634766\n",
      "Training loss for batch 1057 : 0.5825141668319702\n",
      "Training loss for batch 1058 : 0.18788687884807587\n",
      "Training loss for batch 1059 : 0.0010931160068139434\n",
      "Training loss for batch 1060 : 0.08433596044778824\n",
      "Training loss for batch 1061 : 0.02513366937637329\n",
      "Training loss for batch 1062 : 0.137163907289505\n",
      "Training loss for batch 1063 : 0.19431547820568085\n",
      "Training loss for batch 1064 : 0.21831855177879333\n",
      "Training loss for batch 1065 : 0.6446264982223511\n",
      "Training loss for batch 1066 : 0.12886351346969604\n",
      "Training loss for batch 1067 : 0.2537134289741516\n",
      "Training loss for batch 1068 : 0.30576932430267334\n",
      "Training loss for batch 1069 : 0.11313256621360779\n",
      "Training loss for batch 1070 : 0.4332108199596405\n",
      "Training loss for batch 1071 : 0.03639000654220581\n",
      "Training loss for batch 1072 : 0.0647089034318924\n",
      "Training loss for batch 1073 : 0.5746832489967346\n",
      "Training loss for batch 1074 : 0.3578289747238159\n",
      "Training loss for batch 1075 : 0.10727684944868088\n",
      "Training loss for batch 1076 : 0.6261481046676636\n",
      "Training loss for batch 1077 : 0.1728914976119995\n",
      "Training loss for batch 1078 : 0.19950716197490692\n",
      "Training loss for batch 1079 : 0.3796241581439972\n",
      "Training loss for batch 1080 : 0.3994268476963043\n",
      "Training loss for batch 1081 : 0.25113368034362793\n",
      "Training loss for batch 1082 : 0.3835048973560333\n",
      "Training loss for batch 1083 : 0.17672887444496155\n",
      "Training loss for batch 1084 : 0.6291508078575134\n",
      "Training loss for batch 1085 : 0.14259488880634308\n",
      "Training loss for batch 1086 : 0.0922512337565422\n",
      "Training loss for batch 1087 : 0.05144328624010086\n",
      "Training loss for batch 1088 : 0.18688613176345825\n",
      "Training loss for batch 1089 : 0.1446518898010254\n",
      "Training loss for batch 1090 : 0.2543451189994812\n",
      "Training loss for batch 1091 : 0.11887416988611221\n",
      "Training loss for batch 1092 : 0.1007215678691864\n",
      "Training loss for batch 1093 : 0.7419422268867493\n",
      "Training loss for batch 1094 : 0.9102017879486084\n",
      "Training loss for batch 1095 : 0.2541870176792145\n",
      "Training loss for batch 1096 : 0.20745083689689636\n",
      "Training loss for batch 1097 : 0.019525140523910522\n",
      "Training loss for batch 1098 : 0.1271466761827469\n",
      "Training loss for batch 1099 : 0.39118364453315735\n",
      "Training loss for batch 1100 : 0.0\n",
      "Training loss for batch 1101 : 0.14207281172275543\n",
      "Training loss for batch 1102 : 0.4096032977104187\n",
      "Training loss for batch 1103 : 0.4471338987350464\n",
      "Training loss for batch 1104 : 0.24513256549835205\n",
      "Training loss for batch 1105 : 0.01021112035959959\n",
      "Training loss for batch 1106 : 0.13018587231636047\n",
      "Training loss for batch 1107 : 0.32080382108688354\n",
      "Training loss for batch 1108 : 0.6611862778663635\n",
      "Training loss for batch 1109 : 0.575008749961853\n",
      "Training loss for batch 1110 : 0.13730818033218384\n",
      "Training loss for batch 1111 : 0.4531013071537018\n",
      "Training loss for batch 1112 : 0.3713686764240265\n",
      "Training loss for batch 1113 : 0.3890853524208069\n",
      "Training loss for batch 1114 : 0.012929469347000122\n",
      "Training loss for batch 1115 : 0.10399314761161804\n",
      "Training loss for batch 1116 : 0.14434953033924103\n",
      "Training loss for batch 1117 : 0.16186372935771942\n",
      "Training loss for batch 1118 : 0.20685423910617828\n",
      "Training loss for batch 1119 : 0.5739153027534485\n",
      "Training loss for batch 1120 : 0.046336494386196136\n",
      "Training loss for batch 1121 : 0.5268527865409851\n",
      "Training loss for batch 1122 : 0.09713522344827652\n",
      "Training loss for batch 1123 : 0.22234445810317993\n",
      "Training loss for batch 1124 : 0.13047251105308533\n",
      "Training loss for batch 1125 : 0.39674633741378784\n",
      "Training loss for batch 1126 : 0.538572371006012\n",
      "Training loss for batch 1127 : 0.25371620059013367\n",
      "Training loss for batch 1128 : 0.10582570731639862\n",
      "Training loss for batch 1129 : 0.04957958683371544\n",
      "Training loss for batch 1130 : 0.10867906361818314\n",
      "Training loss for batch 1131 : 0.2608230412006378\n",
      "Training loss for batch 1132 : 0.29867833852767944\n",
      "Training loss for batch 1133 : 0.23953290283679962\n",
      "Training loss for batch 1134 : 0.47661134600639343\n",
      "Training loss for batch 1135 : 0.02122415602207184\n",
      "Training loss for batch 1136 : 0.055512018501758575\n",
      "Training loss for batch 1137 : 0.3736264407634735\n",
      "Training loss for batch 1138 : 0.2268071472644806\n",
      "Training loss for batch 1139 : 0.27092722058296204\n",
      "Training loss for batch 1140 : 0.09688419103622437\n",
      "Training loss for batch 1141 : 0.6687577962875366\n",
      "Training loss for batch 1142 : 0.12810195982456207\n",
      "Training loss for batch 1143 : 0.34555238485336304\n",
      "Training loss for batch 1144 : 0.24120526015758514\n",
      "Training loss for batch 1145 : 0.06685114651918411\n",
      "Training loss for batch 1146 : 0.09583374112844467\n",
      "Training loss for batch 1147 : 0.30123111605644226\n",
      "Training loss for batch 1148 : 0.16188965737819672\n",
      "Training loss for batch 1149 : 0.17157010734081268\n",
      "Training loss for batch 1150 : 0.3065144717693329\n",
      "Training loss for batch 1151 : 0.5606904029846191\n",
      "Training loss for batch 1152 : 0.287753701210022\n",
      "Training loss for batch 1153 : 0.49873846769332886\n",
      "Training loss for batch 1154 : 0.5834092497825623\n",
      "Training loss for batch 1155 : 0.22021155059337616\n",
      "Training loss for batch 1156 : 0.34068310260772705\n",
      "Training loss for batch 1157 : 0.12018218636512756\n",
      "Training loss for batch 1158 : 0.23466050624847412\n",
      "Training loss for batch 1159 : 0.17503061890602112\n",
      "Training loss for batch 1160 : 0.21497569978237152\n",
      "Training loss for batch 1161 : 0.0233210027217865\n",
      "Training loss for batch 1162 : 0.10334375500679016\n",
      "Training loss for batch 1163 : 0.1896299123764038\n",
      "Training loss for batch 1164 : 0.012001234106719494\n",
      "Training loss for batch 1165 : 0.10120569914579391\n",
      "Training loss for batch 1166 : 0.3907356262207031\n",
      "Training loss for batch 1167 : 0.0160260908305645\n",
      "Training loss for batch 1168 : 0.11574514955282211\n",
      "Training loss for batch 1169 : 0.2073681801557541\n",
      "Training loss for batch 1170 : 0.11160891503095627\n",
      "Training loss for batch 1171 : 0.13907390832901\n",
      "Training loss for batch 1172 : 0.25312960147857666\n",
      "Training loss for batch 1173 : 0.07590878009796143\n",
      "Training loss for batch 1174 : 0.21985898911952972\n",
      "Training loss for batch 1175 : 0.38361307978630066\n",
      "Training loss for batch 1176 : 0.27803468704223633\n",
      "Training loss for batch 1177 : 0.05090150237083435\n",
      "Training loss for batch 1178 : 0.26383599638938904\n",
      "Training loss for batch 1179 : 0.024170363321900368\n",
      "Training loss for batch 1180 : 0.06441114842891693\n",
      "Training loss for batch 1181 : 0.20414985716342926\n",
      "Training loss for batch 1182 : 0.08103229105472565\n",
      "Training loss for batch 1183 : 0.0635598823428154\n",
      "Training loss for batch 1184 : 0.021226614713668823\n",
      "Training loss for batch 1185 : 0.264604777097702\n",
      "Training loss for batch 1186 : 0.007168952841311693\n",
      "Training loss for batch 1187 : 0.669670581817627\n",
      "Training loss for batch 1188 : 0.46261054277420044\n",
      "Training loss for batch 1189 : 0.06412749737501144\n",
      "Training loss for batch 1190 : 0.3108842372894287\n",
      "Training loss for batch 1191 : 0.3071928024291992\n",
      "Training loss for batch 1192 : 0.18089686334133148\n",
      "Training loss for batch 1193 : 0.6086068749427795\n",
      "Training loss for batch 1194 : 0.12233220040798187\n",
      "Training loss for batch 1195 : 0.05542419105768204\n",
      "Training loss for batch 1196 : 0.12904469668865204\n",
      "Training loss for batch 1197 : 0.1284193992614746\n",
      "Training loss for batch 1198 : 0.6952305436134338\n",
      "Training loss for batch 1199 : 0.010748708620667458\n",
      "Training loss for batch 1200 : 0.5416722297668457\n",
      "Training loss for batch 1201 : 0.5141980648040771\n",
      "Training loss for batch 1202 : 0.128670334815979\n",
      "Training loss for batch 1203 : 0.21427395939826965\n",
      "Training loss for batch 1204 : 0.17608563601970673\n",
      "Training loss for batch 1205 : 0.0019024597713723779\n",
      "Training loss for batch 1206 : 0.46177342534065247\n",
      "Training loss for batch 1207 : 0.3088235855102539\n",
      "Training loss for batch 1208 : 0.5687189102172852\n",
      "Training loss for batch 1209 : 0.4156222641468048\n",
      "Training loss for batch 1210 : 0.2005203813314438\n",
      "Training loss for batch 1211 : 0.3488145172595978\n",
      "Training loss for batch 1212 : 0.3548789322376251\n",
      "Training loss for batch 1213 : 0.14756228029727936\n",
      "Training loss for batch 1214 : 0.34782281517982483\n",
      "Training loss for batch 1215 : 0.5906142592430115\n",
      "Training loss for batch 1216 : 0.11445149779319763\n",
      "Training loss for batch 1217 : 0.36253419518470764\n",
      "Training loss for batch 1218 : 0.35764333605766296\n",
      "Training loss for batch 1219 : 0.2092154175043106\n",
      "Training loss for batch 1220 : 0.02444775030016899\n",
      "Training loss for batch 1221 : 0.3344297707080841\n",
      "Training loss for batch 1222 : 0.08934080600738525\n",
      "Training loss for batch 1223 : 0.3186025321483612\n",
      "Training loss for batch 1224 : 0.44599753618240356\n",
      "Training loss for batch 1225 : 0.19546256959438324\n",
      "Training loss for batch 1226 : 0.18456850945949554\n",
      "Training loss for batch 1227 : 0.12086733430624008\n",
      "Training loss for batch 1228 : 0.18588320910930634\n",
      "Training loss for batch 1229 : 0.24083229899406433\n",
      "Training loss for batch 1230 : 0.35714149475097656\n",
      "Training loss for batch 1231 : 0.4643556773662567\n",
      "Training loss for batch 1232 : 0.15359947085380554\n",
      "Training loss for batch 1233 : 0.4020340144634247\n",
      "Training loss for batch 1234 : 0.09908738732337952\n",
      "Training loss for batch 1235 : 0.20423537492752075\n",
      "Training loss for batch 1236 : 0.05112861841917038\n",
      "Training loss for batch 1237 : 0.1709447205066681\n",
      "Training loss for batch 1238 : 0.38903510570526123\n",
      "Training loss for batch 1239 : 0.30193549394607544\n",
      "Training loss for batch 1240 : 0.20275342464447021\n",
      "Training loss for batch 1241 : 0.3040187358856201\n",
      "Training loss for batch 1242 : 0.36615151166915894\n",
      "Training loss for batch 1243 : 0.3981817960739136\n",
      "Training loss for batch 1244 : 0.503372311592102\n",
      "Training loss for batch 1245 : 0.003137961495667696\n",
      "Training loss for batch 1246 : 0.25449520349502563\n",
      "Training loss for batch 1247 : 0.7598890662193298\n",
      "Training loss for batch 1248 : 0.19625641405582428\n",
      "Training loss for batch 1249 : 0.032963015139102936\n",
      "Training loss for batch 1250 : 0.15113337337970734\n",
      "Training loss for batch 1251 : 0.5045447945594788\n",
      "Training loss for batch 1252 : 0.18549498915672302\n",
      "Training loss for batch 1253 : 0.30168142914772034\n",
      "Training loss for batch 1254 : 0.017751626670360565\n",
      "Training loss for batch 1255 : 0.40683528780937195\n",
      "Training loss for batch 1256 : 0.07431092113256454\n",
      "Training loss for batch 1257 : 0.5457382798194885\n",
      "Training loss for batch 1258 : 0.5626846551895142\n",
      "Training loss for batch 1259 : 0.25671669840812683\n",
      "Training loss for batch 1260 : 0.31796106696128845\n",
      "Training loss for batch 1261 : 0.25235795974731445\n",
      "Training loss for batch 1262 : 0.27259311079978943\n",
      "Training loss for batch 1263 : 0.3604307174682617\n",
      "Training loss for batch 1264 : 0.14248470962047577\n",
      "Training loss for batch 1265 : 0.7213128805160522\n",
      "Training loss for batch 1266 : 0.34328389167785645\n",
      "Training loss for batch 1267 : 0.005472138524055481\n",
      "Training loss for batch 1268 : 0.1801452934741974\n",
      "Training loss for batch 1269 : 0.21410638093948364\n",
      "Training loss for batch 1270 : 0.2890012860298157\n",
      "Training loss for batch 1271 : 0.14341352880001068\n",
      "Training loss for batch 1272 : 0.08887048810720444\n",
      "Training loss for batch 1273 : 0.10674837976694107\n",
      "Training loss for batch 1274 : 0.9473592638969421\n",
      "Training loss for batch 1275 : 0.3806554079055786\n",
      "Training loss for batch 1276 : 0.5123037695884705\n",
      "Training loss for batch 1277 : 0.2593991160392761\n",
      "Training loss for batch 1278 : 0.6575877070426941\n",
      "Training loss for batch 1279 : 0.3015684187412262\n",
      "Training loss for batch 1280 : 0.07890475541353226\n",
      "Training loss for batch 1281 : 0.17614421248435974\n",
      "Training loss for batch 1282 : 0.27116188406944275\n",
      "Training loss for batch 1283 : 0.24917033314704895\n",
      "Training loss for batch 1284 : 0.12221983075141907\n",
      "Training loss for batch 1285 : 0.07089944928884506\n",
      "Training loss for batch 1286 : 0.0003555913863237947\n",
      "Training loss for batch 1287 : 0.1982676237821579\n",
      "Training loss for batch 1288 : 0.17583149671554565\n",
      "Training loss for batch 1289 : 0.23868229985237122\n",
      "Training loss for batch 1290 : 0.3584114909172058\n",
      "Training loss for batch 1291 : 0.516941249370575\n",
      "Training loss for batch 1292 : 0.5162374973297119\n",
      "Training loss for batch 1293 : 0.12954546511173248\n",
      "Training loss for batch 1294 : 0.4627976417541504\n",
      "Training loss for batch 1295 : 0.21561697125434875\n",
      "Training loss for batch 1296 : 0.19295889139175415\n",
      "Training loss for batch 1297 : 0.3168799877166748\n",
      "Training loss for batch 1298 : 0.21655592322349548\n",
      "Training loss for batch 1299 : 0.5163084268569946\n",
      "Training loss for batch 1300 : 0.04673563688993454\n",
      "Training loss for batch 1301 : 0.22286108136177063\n",
      "Training loss for batch 1302 : 0.34083855152130127\n",
      "Training loss for batch 1303 : 0.32885414361953735\n",
      "Training loss for batch 1304 : 0.33888763189315796\n",
      "Training loss for batch 1305 : 0.4307716190814972\n",
      "Training loss for batch 1306 : 0.1758902668952942\n",
      "Training loss for batch 1307 : 0.30596497654914856\n",
      "Training loss for batch 1308 : 0.33257856965065\n",
      "Training loss for batch 1309 : 0.6873981952667236\n",
      "Training loss for batch 1310 : 0.13671021163463593\n",
      "Training loss for batch 1311 : 0.42313864827156067\n",
      "Training loss for batch 1312 : 0.17715975642204285\n",
      "Training loss for batch 1313 : 0.6316111087799072\n",
      "Training loss for batch 1314 : 0.19404476881027222\n",
      "Training loss for batch 1315 : 0.26412004232406616\n",
      "Training loss for batch 1316 : 0.20058673620224\n",
      "Training loss for batch 1317 : 0.37427133321762085\n",
      "Training loss for batch 1318 : 0.3823625147342682\n",
      "Training loss for batch 1319 : 0.2257058024406433\n",
      "Training loss for batch 1320 : 0.3775985538959503\n",
      "Training loss for batch 1321 : 0.07914256304502487\n",
      "Training loss for batch 1322 : 0.2906055152416229\n",
      "Training loss for batch 1323 : 0.15900635719299316\n",
      "Training loss for batch 1324 : 0.005806337110698223\n",
      "Training loss for batch 1325 : 0.01823687180876732\n",
      "Training loss for batch 1326 : 0.3075309693813324\n",
      "Training loss for batch 1327 : 0.27391698956489563\n",
      "Training loss for batch 1328 : 0.4085959196090698\n",
      "Training loss for batch 1329 : 0.39816170930862427\n",
      "Training loss for batch 1330 : 0.40580499172210693\n",
      "Training loss for batch 1331 : 0.2358720451593399\n",
      "Training loss for batch 1332 : 0.43003520369529724\n",
      "Training loss for batch 1333 : 0.31946441531181335\n",
      "Training loss for batch 1334 : 0.3495982587337494\n",
      "Training loss for batch 1335 : 0.3159726858139038\n",
      "Training loss for batch 1336 : 0.2017468512058258\n",
      "Training loss for batch 1337 : 0.22276458144187927\n",
      "Training loss for batch 1338 : 0.2655734419822693\n",
      "Training loss for batch 1339 : 0.2328454554080963\n",
      "Training loss for batch 1340 : 0.22488649189472198\n",
      "Training loss for batch 1341 : 0.11089727282524109\n",
      "Training loss for batch 1342 : 0.4075362980365753\n",
      "Training loss for batch 1343 : 0.4371531009674072\n",
      "Training loss for batch 1344 : 0.4112369418144226\n",
      "Training loss for batch 1345 : 0.24837249517440796\n",
      "Training loss for batch 1346 : 0.25162366032600403\n",
      "Training loss for batch 1347 : 0.2162405550479889\n",
      "Training loss for batch 1348 : 0.04443645477294922\n",
      "Training loss for batch 1349 : 0.004023830406367779\n",
      "Training loss for batch 1350 : 0.3094821870326996\n",
      "Training loss for batch 1351 : 0.21826110780239105\n",
      "Training loss for batch 1352 : 0.5551690459251404\n",
      "Training loss for batch 1353 : 0.3963327705860138\n",
      "Training loss for batch 1354 : 0.1065032035112381\n",
      "Training loss for batch 1355 : 0.19534868001937866\n",
      "Training loss for batch 1356 : 0.025689279660582542\n",
      "Training loss for batch 1357 : 0.6260448694229126\n",
      "Training loss for batch 1358 : 0.24359790980815887\n",
      "Training loss for batch 1359 : 0.20298664271831512\n",
      "Training loss for batch 1360 : 0.35193634033203125\n",
      "Training loss for batch 1361 : 0.03052951768040657\n",
      "Training loss for batch 1362 : 0.14493225514888763\n",
      "Training loss for batch 1363 : 0.49555474519729614\n",
      "Training loss for batch 1364 : 0.14284074306488037\n",
      "Training loss for batch 1365 : 0.118129201233387\n",
      "Training loss for batch 1366 : 0.34830012917518616\n",
      "Training loss for batch 1367 : 0.1275322139263153\n",
      "Training loss for batch 1368 : 0.33236977458000183\n",
      "Training loss for batch 1369 : 0.275640606880188\n",
      "Training loss for batch 1370 : 0.18875494599342346\n",
      "Training loss for batch 1371 : 0.40120986104011536\n",
      "Training loss for batch 1372 : 0.4080011546611786\n",
      "Training loss for batch 1373 : 0.32529401779174805\n",
      "Training loss for batch 1374 : 0.4634973108768463\n",
      "Training loss for batch 1375 : 0.28167465329170227\n",
      "Training loss for batch 1376 : 0.11401458084583282\n",
      "Training loss for batch 1377 : 0.22936880588531494\n",
      "Training loss for batch 1378 : 0.24121348559856415\n",
      "Training loss for batch 1379 : 0.23608729243278503\n",
      "Training loss for batch 1380 : 0.32524552941322327\n",
      "Training loss for batch 1381 : 0.207232266664505\n",
      "Training loss for batch 1382 : 0.24159887433052063\n",
      "Training loss for batch 1383 : 0.0\n",
      "Training loss for batch 1384 : 0.4169789254665375\n",
      "Training loss for batch 1385 : 0.07742059230804443\n",
      "Training loss for batch 1386 : 0.12444393336772919\n",
      "Training loss for batch 1387 : 0.5226393342018127\n",
      "Training loss for batch 1388 : 0.04985227435827255\n",
      "Training loss for batch 1389 : 0.1897689253091812\n",
      "Training loss for batch 1390 : 7.154154900490539e-06\n",
      "Training loss for batch 1391 : 0.022371040657162666\n",
      "Training loss for batch 1392 : 0.575425386428833\n",
      "Training loss for batch 1393 : 0.10976891964673996\n",
      "Training loss for batch 1394 : 0.34430450201034546\n",
      "Training loss for batch 1395 : 0.28673601150512695\n",
      "Training loss for batch 1396 : 0.14290636777877808\n",
      "Training loss for batch 1397 : 0.06716065853834152\n",
      "Training loss for batch 1398 : 0.887190580368042\n",
      "Training loss for batch 1399 : 0.6503732800483704\n",
      "Training loss for batch 1400 : 0.362853467464447\n",
      "Training loss for batch 1401 : 0.5785208940505981\n",
      "Training loss for batch 1402 : 0.20612981915473938\n",
      "Training loss for batch 1403 : 0.5434320569038391\n",
      "Training loss for batch 1404 : 0.5138393044471741\n",
      "Training loss for batch 1405 : 0.18247294425964355\n",
      "Training loss for batch 1406 : 0.08648066222667694\n",
      "Training loss for batch 1407 : 0.31902801990509033\n",
      "Training loss for batch 1408 : 0.1399042010307312\n",
      "Training loss for batch 1409 : 0.10176590085029602\n",
      "Training loss for batch 1410 : 0.2636208236217499\n",
      "Training loss for batch 1411 : 0.18275503814220428\n",
      "Training loss for batch 1412 : 0.21906301379203796\n",
      "Training loss for batch 1413 : 0.09162843227386475\n",
      "Training loss for batch 1414 : 0.45269060134887695\n",
      "Training loss for batch 1415 : 0.6316983103752136\n",
      "Training loss for batch 1416 : 0.3422859311103821\n",
      "Training loss for batch 1417 : 0.06379980593919754\n",
      "Training loss for batch 1418 : 0.15337136387825012\n",
      "Training loss for batch 1419 : 0.39303845167160034\n",
      "Training loss for batch 1420 : 0.10142383724451065\n",
      "Training loss for batch 1421 : 0.23581136763095856\n",
      "Training loss for batch 1422 : 0.3591848611831665\n",
      "Training loss for batch 1423 : 0.08534625172615051\n",
      "Training loss for batch 1424 : 0.37222301959991455\n",
      "Training loss for batch 1425 : 0.5505445599555969\n",
      "Training loss for batch 1426 : 0.2313244342803955\n",
      "Training loss for batch 1427 : 0.16554085910320282\n",
      "Training loss for batch 1428 : 0.5093337297439575\n",
      "Training loss for batch 1429 : 0.2664092779159546\n",
      "Training loss for batch 1430 : 0.2872430384159088\n",
      "Training loss for batch 1431 : 0.15132258832454681\n",
      "Training loss for batch 1432 : 0.16599225997924805\n",
      "Training loss for batch 1433 : 0.7130392789840698\n",
      "Training loss for batch 1434 : 0.19743888080120087\n",
      "Training loss for batch 1435 : 0.05286640673875809\n",
      "Training loss for batch 1436 : 0.336489737033844\n",
      "Training loss for batch 1437 : 0.19794157147407532\n",
      "Training loss for batch 1438 : 0.20347270369529724\n",
      "Training loss for batch 1439 : 0.3110002279281616\n",
      "Training loss for batch 1440 : 0.10138481855392456\n",
      "Training loss for batch 1441 : 0.20291510224342346\n",
      "Training loss for batch 1442 : 0.14639124274253845\n",
      "Training loss for batch 1443 : 0.3990694582462311\n",
      "Training loss for batch 1444 : 0.21933604776859283\n",
      "Training loss for batch 1445 : 0.21664051711559296\n",
      "Training loss for batch 1446 : 0.30131030082702637\n",
      "Training loss for batch 1447 : 0.10637497901916504\n",
      "Training loss for batch 1448 : 0.16974925994873047\n",
      "Training loss for batch 1449 : 0.33041101694107056\n",
      "Training loss for batch 1450 : 0.6309006810188293\n",
      "Training loss for batch 1451 : 0.2874458134174347\n",
      "Training loss for batch 1452 : 0.294740229845047\n",
      "Training loss for batch 1453 : 0.30677732825279236\n",
      "Training loss for batch 1454 : 0.2167767584323883\n",
      "Training loss for batch 1455 : 0.4794006645679474\n",
      "Training loss for batch 1456 : 0.4873224198818207\n",
      "Training loss for batch 1457 : 0.3792704939842224\n",
      "Training loss for batch 1458 : 0.35953110456466675\n",
      "Training loss for batch 1459 : 0.26279523968696594\n",
      "Training loss for batch 1460 : 0.056590449064970016\n",
      "Training loss for batch 1461 : 0.028573226183652878\n",
      "Training loss for batch 1462 : 0.05270211398601532\n",
      "Training loss for batch 1463 : 0.19651789963245392\n",
      "Training loss for batch 1464 : 0.13993455469608307\n",
      "Training loss for batch 1465 : 0.08041955530643463\n",
      "Training loss for batch 1466 : 0.31588196754455566\n",
      "Training loss for batch 1467 : 0.06941021978855133\n",
      "Training loss for batch 1468 : 0.08120301365852356\n",
      "Training loss for batch 1469 : 0.3583473563194275\n",
      "Training loss for batch 1470 : 0.27719470858573914\n",
      "Training loss for batch 1471 : 0.0635392814874649\n",
      "Training loss for batch 1472 : 0.7695975303649902\n",
      "Training loss for batch 1473 : 0.30774396657943726\n",
      "Training loss for batch 1474 : 0.18280233442783356\n",
      "Training loss for batch 1475 : 0.12239126861095428\n",
      "Training loss for batch 1476 : 0.13041742146015167\n",
      "Training loss for batch 1477 : 0.12452629208564758\n",
      "Training loss for batch 1478 : 0.45049798488616943\n",
      "Training loss for batch 1479 : 0.19055159389972687\n",
      "Training loss for batch 1480 : 0.3232806324958801\n",
      "Training loss for batch 1481 : 0.1037224754691124\n",
      "Training loss for batch 1482 : 0.4180237650871277\n",
      "Training loss for batch 1483 : 0.29449257254600525\n",
      "Training loss for batch 1484 : 0.13178953528404236\n",
      "Training loss for batch 1485 : 0.6626685261726379\n",
      "Training loss for batch 1486 : 0.1684703528881073\n",
      "Training loss for batch 1487 : 0.4343683123588562\n",
      "Training loss for batch 1488 : 0.14513124525547028\n",
      "Training loss for batch 1489 : 0.22070248425006866\n",
      "Training loss for batch 1490 : 0.2532045841217041\n",
      "Training loss for batch 1491 : 0.24458052217960358\n",
      "Training loss for batch 1492 : 0.1724504679441452\n",
      "Training loss for batch 1493 : 0.06516936421394348\n",
      "Training loss for batch 1494 : 0.2712693214416504\n",
      "Training loss for batch 1495 : 0.4134818911552429\n",
      "Training loss for batch 1496 : 0.4175823926925659\n",
      "Training loss for batch 1497 : 0.2820265591144562\n",
      "Training loss for batch 1498 : 0.3437146544456482\n",
      "Training loss for batch 1499 : 0.1757868230342865\n",
      "Training loss for batch 1500 : 0.06763982027769089\n",
      "Training loss for batch 1501 : 0.3018052875995636\n",
      "Training loss for batch 1502 : 0.15636226534843445\n",
      "Training loss for batch 1503 : 0.7002812027931213\n",
      "Training loss for batch 1504 : 0.06553018093109131\n",
      "Training loss for batch 1505 : 0.230096697807312\n",
      "Training loss for batch 1506 : 0.23441359400749207\n",
      "Training loss for batch 1507 : 0.37990087270736694\n",
      "Training loss for batch 1508 : 0.06619905680418015\n",
      "Training loss for batch 1509 : 0.036758702248334885\n",
      "Training loss for batch 1510 : 0.30696582794189453\n",
      "Training loss for batch 1511 : 0.20124952495098114\n",
      "Training loss for batch 1512 : 0.23307009041309357\n",
      "Training loss for batch 1513 : 0.3290744125843048\n",
      "Training loss for batch 1514 : 0.3351009488105774\n",
      "Training loss for batch 1515 : 0.29933860898017883\n",
      "Training loss for batch 1516 : 0.16463476419448853\n",
      "Training loss for batch 1517 : 0.44001448154449463\n",
      "Training loss for batch 1518 : 0.2665586471557617\n",
      "Training loss for batch 1519 : 0.11192037165164948\n",
      "Training loss for batch 1520 : 0.32613351941108704\n",
      "Training loss for batch 1521 : 0.08687007427215576\n",
      "Training loss for batch 1522 : 0.20025041699409485\n",
      "Training loss for batch 1523 : 0.2273402065038681\n",
      "Training loss for batch 1524 : 0.0976027250289917\n",
      "Training loss for batch 1525 : 0.019306836649775505\n",
      "Training loss for batch 1526 : 0.35859400033950806\n",
      "Training loss for batch 1527 : 0.27688464522361755\n",
      "Training loss for batch 1528 : 0.10955772548913956\n",
      "Training loss for batch 1529 : 0.146784245967865\n",
      "Training loss for batch 1530 : 0.18435606360435486\n",
      "Training loss for batch 1531 : 0.4308304488658905\n",
      "Training loss for batch 1532 : 0.2829861044883728\n",
      "Training loss for batch 1533 : 0.3144247531890869\n",
      "Training loss for batch 1534 : 0.17154307663440704\n",
      "Training loss for batch 1535 : 0.4407734274864197\n",
      "Training loss for batch 1536 : 0.4485750198364258\n",
      "Training loss for batch 1537 : 0.4120374321937561\n",
      "Training loss for batch 1538 : 0.08978183567523956\n",
      "Training loss for batch 1539 : 0.31856051087379456\n",
      "Training loss for batch 1540 : 0.8696914315223694\n",
      "Training loss for batch 1541 : 0.5120248198509216\n",
      "Training loss for batch 1542 : 0.26006290316581726\n",
      "Training loss for batch 1543 : 0.03018007054924965\n",
      "Training loss for batch 1544 : 0.06886259466409683\n",
      "Training loss for batch 1545 : 0.0961012914776802\n",
      "Training loss for batch 1546 : 0.26128846406936646\n",
      "Training loss for batch 1547 : 0.5621260404586792\n",
      "Training loss for batch 1548 : 0.23276251554489136\n",
      "Training loss for batch 1549 : 0.17802202701568604\n",
      "Training loss for batch 1550 : 0.2818465530872345\n",
      "Training loss for batch 1551 : 0.10316003859043121\n",
      "Training loss for batch 1552 : 0.23762619495391846\n",
      "Training loss for batch 1553 : 0.07485777884721756\n",
      "Training loss for batch 1554 : 0.03383512422442436\n",
      "Training loss for batch 1555 : 0.16070175170898438\n",
      "Training loss for batch 1556 : 0.46249818801879883\n",
      "Training loss for batch 1557 : 0.42402735352516174\n",
      "Training loss for batch 1558 : 0.30040600895881653\n",
      "Training loss for batch 1559 : 0.39241185784339905\n",
      "Training loss for batch 1560 : 0.4011155068874359\n",
      "Training loss for batch 1561 : 0.06372448056936264\n",
      "Training loss for batch 1562 : 0.19055727124214172\n",
      "Training loss for batch 1563 : 0.4958096444606781\n",
      "Training loss for batch 1564 : 0.3478820323944092\n",
      "Training loss for batch 1565 : 0.025200562551617622\n",
      "Training loss for batch 1566 : 0.1735396534204483\n",
      "Training loss for batch 1567 : 0.12275344133377075\n",
      "Training loss for batch 1568 : 0.32875996828079224\n",
      "Training loss for batch 1569 : 0.07894431054592133\n",
      "Training loss for batch 1570 : 0.48096764087677\n",
      "Training loss for batch 1571 : 0.1732596755027771\n",
      "Training loss for batch 1572 : 0.018299749121069908\n",
      "Training loss for batch 1573 : 0.04407219588756561\n",
      "Training loss for batch 1574 : 0.5910000801086426\n",
      "Training loss for batch 1575 : 0.041194695979356766\n",
      "Training loss for batch 1576 : 0.5337421894073486\n",
      "Training loss for batch 1577 : 0.19891709089279175\n",
      "Training loss for batch 1578 : 0.06655752658843994\n",
      "Training loss for batch 1579 : 0.3159099221229553\n",
      "Training loss for batch 1580 : 0.19079285860061646\n",
      "Training loss for batch 1581 : 0.24835363030433655\n",
      "Training loss for batch 1582 : 0.2580060362815857\n",
      "Training loss for batch 1583 : 0.22984713315963745\n",
      "Training loss for batch 1584 : 0.10088811069726944\n",
      "Training loss for batch 1585 : 0.4221149682998657\n",
      "Training loss for batch 1586 : 0.19504646956920624\n",
      "Training loss for batch 1587 : 0.07571051269769669\n",
      "Training loss for batch 1588 : 0.09411656856536865\n",
      "Training loss for batch 1589 : 0.26562416553497314\n",
      "Training loss for batch 1590 : 0.25570058822631836\n",
      "Training loss for batch 1591 : 0.010002069175243378\n",
      "Training loss for batch 1592 : 0.5284364223480225\n",
      "Training loss for batch 1593 : 0.6641790270805359\n",
      "Training loss for batch 1594 : 0.2438107132911682\n",
      "Training loss for batch 1595 : 0.06565360724925995\n",
      "Training loss for batch 1596 : 0.0755380243062973\n",
      "Training loss for batch 1597 : 0.18301913142204285\n",
      "Training loss for batch 1598 : 0.1354539841413498\n",
      "Training loss for batch 1599 : 0.3237398862838745\n",
      "Training loss for batch 1600 : 0.45020562410354614\n",
      "Training loss for batch 1601 : 0.37334370613098145\n",
      "Training loss for batch 1602 : 0.28965049982070923\n",
      "Training loss for batch 1603 : 0.25441426038742065\n",
      "Training loss for batch 1604 : 0.0669378712773323\n",
      "Training loss for batch 1605 : 0.04524994641542435\n",
      "Training loss for batch 1606 : 0.12375852465629578\n",
      "Training loss for batch 1607 : 0.14609801769256592\n",
      "Training loss for batch 1608 : 0.12876413762569427\n",
      "Training loss for batch 1609 : 0.27568304538726807\n",
      "Training loss for batch 1610 : 0.06767985969781876\n",
      "Training loss for batch 1611 : 0.41141200065612793\n",
      "Training loss for batch 1612 : 0.21094049513339996\n",
      "Training loss for batch 1613 : 0.22382752597332\n",
      "Training loss for batch 1614 : 0.4097321033477783\n",
      "Training loss for batch 1615 : 0.10986703634262085\n",
      "Training loss for batch 1616 : 0.1704370677471161\n",
      "Training loss for batch 1617 : 0.08668936789035797\n",
      "Training loss for batch 1618 : 0.17637814581394196\n",
      "Training loss for batch 1619 : 0.18687959015369415\n",
      "Training loss for batch 1620 : 0.10959431529045105\n",
      "Training loss for batch 1621 : 0.5069029927253723\n",
      "Training loss for batch 1622 : 0.12528204917907715\n",
      "Training loss for batch 1623 : 0.1408674418926239\n",
      "Training loss for batch 1624 : 0.1044018566608429\n",
      "Training loss for batch 1625 : 0.17214885354042053\n",
      "Training loss for batch 1626 : 0.3493851125240326\n",
      "Training loss for batch 1627 : 0.5199739336967468\n",
      "Training loss for batch 1628 : 0.09123555570840836\n",
      "Training loss for batch 1629 : 0.5426037311553955\n",
      "Training loss for batch 1630 : 0.24643367528915405\n",
      "Training loss for batch 1631 : 0.41312965750694275\n",
      "Training loss for batch 1632 : 0.18144384026527405\n",
      "Training loss for batch 1633 : 0.8222700953483582\n",
      "Training loss for batch 1634 : 0.1152975857257843\n",
      "Training loss for batch 1635 : 0.19471845030784607\n",
      "Training loss for batch 1636 : 0.17372222244739532\n",
      "Training loss for batch 1637 : 0.39919573068618774\n",
      "Training loss for batch 1638 : 0.16859717667102814\n",
      "Training loss for batch 1639 : 0.5163764953613281\n",
      "Training loss for batch 1640 : 0.16627837717533112\n",
      "Training loss for batch 1641 : 0.2842554748058319\n",
      "Training loss for batch 1642 : 0.1996980458498001\n",
      "Training loss for batch 1643 : 0.5265580415725708\n",
      "Training loss for batch 1644 : 0.372266948223114\n",
      "Training loss for batch 1645 : 0.1334712952375412\n",
      "Training loss for batch 1646 : 0.3570859730243683\n",
      "Training loss for batch 1647 : 0.11111561954021454\n",
      "Training loss for batch 1648 : 0.009313308633863926\n",
      "Training loss for batch 1649 : 0.3979639410972595\n",
      "Training loss for batch 1650 : 0.36921975016593933\n",
      "Training loss for batch 1651 : 0.8755996227264404\n",
      "Training loss for batch 1652 : 0.01721835322678089\n",
      "Training loss for batch 1653 : 0.40567678213119507\n",
      "Training loss for batch 1654 : 0.28537628054618835\n",
      "Training loss for batch 1655 : 0.229203999042511\n",
      "Training loss for batch 1656 : 0.382001668214798\n",
      "Training loss for batch 1657 : 0.25294309854507446\n",
      "Training loss for batch 1658 : 0.5942153334617615\n",
      "Training loss for batch 1659 : 0.05859487131237984\n",
      "Training loss for batch 1660 : 0.17841653525829315\n",
      "Training loss for batch 1661 : 0.035808809101581573\n",
      "Training loss for batch 1662 : 0.19557791948318481\n",
      "Training loss for batch 1663 : 0.14340290427207947\n",
      "Training loss for batch 1664 : 0.05086823180317879\n",
      "Training loss for batch 1665 : 0.2713105380535126\n",
      "Training loss for batch 1666 : 0.4123116731643677\n",
      "Training loss for batch 1667 : 0.2899830639362335\n",
      "Training loss for batch 1668 : 0.013247957453131676\n",
      "Training loss for batch 1669 : 0.3385053873062134\n",
      "Training loss for batch 1670 : 0.3209511637687683\n",
      "Training loss for batch 1671 : 0.3030491769313812\n",
      "Training loss for batch 1672 : 0.43946823477745056\n",
      "Training loss for batch 1673 : 0.3236056864261627\n",
      "Training loss for batch 1674 : 0.23838627338409424\n",
      "Training loss for batch 1675 : 0.2022586166858673\n",
      "Training loss for batch 1676 : 0.05530508607625961\n",
      "Training loss for batch 1677 : 0.3211013972759247\n",
      "Training loss for batch 1678 : 0.3529733121395111\n",
      "Training loss for batch 1679 : 0.34519243240356445\n",
      "Training loss for batch 1680 : 0.2282482385635376\n",
      "Training loss for batch 1681 : 0.07532414048910141\n",
      "Training loss for batch 1682 : 0.09640269726514816\n",
      "Training loss for batch 1683 : 0.39779984951019287\n",
      "Training loss for batch 1684 : 0.29378777742385864\n",
      "Training loss for batch 1685 : 0.34520071744918823\n",
      "Training loss for batch 1686 : 0.11479028314352036\n",
      "Training loss for batch 1687 : 0.08305727690458298\n",
      "Training loss for batch 1688 : 0.33582502603530884\n",
      "Training loss for batch 1689 : 0.13558077812194824\n",
      "Training loss for batch 1690 : 0.7113234400749207\n",
      "Training loss for batch 1691 : 0.45455384254455566\n",
      "Training loss for batch 1692 : 0.5208479166030884\n",
      "Training loss for batch 1693 : 0.549063503742218\n",
      "Training loss for batch 1694 : 0.3643885850906372\n",
      "Training loss for batch 1695 : 0.44930627942085266\n",
      "Training loss for batch 1696 : 0.14102119207382202\n",
      "Training loss for batch 1697 : 0.3003630042076111\n",
      "Training loss for batch 1698 : 0.024364477023482323\n",
      "Training loss for batch 1699 : 0.16888198256492615\n",
      "Training loss for batch 1700 : 0.34751567244529724\n",
      "Training loss for batch 1701 : 0.19977852702140808\n",
      "Training loss for batch 1702 : 0.1550406515598297\n",
      "Training loss for batch 1703 : 0.2171783149242401\n",
      "Training loss for batch 1704 : 0.04618702083826065\n",
      "Training loss for batch 1705 : 0.6834352016448975\n",
      "Training loss for batch 1706 : 0.0\n",
      "Training loss for batch 1707 : 0.29159411787986755\n",
      "Training loss for batch 1708 : 0.13572803139686584\n",
      "Training loss for batch 1709 : 0.5692046880722046\n",
      "Training loss for batch 1710 : 0.2561846077442169\n",
      "Training loss for batch 1711 : 0.1339637041091919\n",
      "Training loss for batch 1712 : 0.2146083563566208\n",
      "Training loss for batch 1713 : 0.6029535531997681\n",
      "Training loss for batch 1714 : 0.02707326039671898\n",
      "Training loss for batch 1715 : 0.23872067034244537\n",
      "Training loss for batch 1716 : 0.3017547130584717\n",
      "Training loss for batch 1717 : 0.5510362982749939\n",
      "Training loss for batch 1718 : 0.5224975347518921\n",
      "Training loss for batch 1719 : 0.30630967020988464\n",
      "Training loss for batch 1720 : 0.2433115392923355\n",
      "Training loss for batch 1721 : 0.03511672466993332\n",
      "Training loss for batch 1722 : 0.11002568900585175\n",
      "Training loss for batch 1723 : 0.3608338534832001\n",
      "Training loss for batch 1724 : 0.20330341160297394\n",
      "Training loss for batch 1725 : 0.16109631955623627\n",
      "Training loss for batch 1726 : 0.3783615529537201\n",
      "Training loss for batch 1727 : 0.41746121644973755\n",
      "Training loss for batch 1728 : 0.07619153708219528\n",
      "Training loss for batch 1729 : 0.3063128590583801\n",
      "Training loss for batch 1730 : 0.24543480575084686\n",
      "Training loss for batch 1731 : 0.3217371702194214\n",
      "Training loss for batch 1732 : 0.4424971342086792\n",
      "Training loss for batch 1733 : 0.1594296395778656\n",
      "Training loss for batch 1734 : 0.5139680504798889\n",
      "Training loss for batch 1735 : 0.31898319721221924\n",
      "Training loss for batch 1736 : 0.4458707869052887\n",
      "Training loss for batch 1737 : 0.23606827855110168\n",
      "Training loss for batch 1738 : 0.29421114921569824\n",
      "Training loss for batch 1739 : 0.2078883796930313\n",
      "Training loss for batch 1740 : 0.2659296989440918\n",
      "Training loss for batch 1741 : 0.194635272026062\n",
      "Training loss for batch 1742 : 0.34585610032081604\n",
      "Training loss for batch 1743 : 0.3763507306575775\n",
      "Training loss for batch 1744 : 0.3708563446998596\n",
      "Training loss for batch 1745 : 0.2290337085723877\n",
      "Training loss for batch 1746 : 0.1376141756772995\n",
      "Training loss for batch 1747 : 0.15383517742156982\n",
      "Training loss for batch 1748 : 0.11616896837949753\n",
      "Training loss for batch 1749 : 0.4936038553714752\n",
      "Training loss for batch 1750 : 0.41287940740585327\n",
      "Training loss for batch 1751 : 0.23122133314609528\n",
      "Training loss for batch 1752 : 0.1734926402568817\n",
      "Training loss for batch 1753 : 0.4752209484577179\n",
      "Training loss for batch 1754 : 0.09185433387756348\n",
      "Training loss for batch 1755 : 0.29936692118644714\n",
      "Training loss for batch 1756 : 0.2892593443393707\n",
      "Training loss for batch 1757 : 0.32435911893844604\n",
      "Training loss for batch 1758 : 0.20992469787597656\n",
      "Training loss for batch 1759 : 0.21693411469459534\n",
      "Training loss for batch 1760 : 0.42429324984550476\n",
      "Training loss for batch 1761 : 0.33444932103157043\n",
      "Training loss for batch 1762 : 0.023578891530632973\n",
      "Training loss for batch 1763 : 0.2740879952907562\n",
      "Training loss for batch 1764 : 0.35426318645477295\n",
      "Training loss for batch 1765 : 0.346817284822464\n",
      "Training loss for batch 1766 : 0.19238553941249847\n",
      "Training loss for batch 1767 : 0.08263356238603592\n",
      "Training loss for batch 1768 : 0.09530564397573471\n",
      "Training loss for batch 1769 : 0.27030208706855774\n",
      "Training loss for batch 1770 : 0.3590938448905945\n",
      "Training loss for batch 1771 : 0.25899267196655273\n",
      "Training loss for batch 1772 : 0.2909688353538513\n",
      "Training loss for batch 1773 : 0.18000927567481995\n",
      "Training loss for batch 1774 : 0.302335262298584\n",
      "Training loss for batch 1775 : 0.20011469721794128\n",
      "Training loss for batch 1776 : 0.11857061088085175\n",
      "Training loss for batch 1777 : 0.4217539131641388\n",
      "Training loss for batch 1778 : 0.35769370198249817\n",
      "Training loss for batch 1779 : 0.2049279510974884\n",
      "Training loss for batch 1780 : 0.12035664170980453\n",
      "Training loss for batch 1781 : 0.14599458873271942\n",
      "Training loss for batch 1782 : 0.09168936312198639\n",
      "Training loss for batch 1783 : 0.20820380747318268\n",
      "Training loss for batch 1784 : 0.18262213468551636\n",
      "Training loss for batch 1785 : 0.35016363859176636\n",
      "Training loss for batch 1786 : 0.056786924600601196\n",
      "Training loss for batch 1787 : 0.5617785453796387\n",
      "Training loss for batch 1788 : 0.01113134901970625\n",
      "Training loss for batch 1789 : 0.215494304895401\n",
      "Training loss for batch 1790 : 0.4145396053791046\n",
      "Training loss for batch 1791 : 0.25004464387893677\n",
      "Training loss for batch 1792 : 0.3067217469215393\n",
      "Training loss for batch 1793 : 0.12836295366287231\n",
      "Training loss for batch 1794 : 0.15602755546569824\n",
      "Training loss for batch 1795 : 0.11968441307544708\n",
      "Training loss for batch 1796 : 0.5627779364585876\n",
      "Training loss for batch 1797 : 0.4530552625656128\n",
      "Training loss for batch 1798 : 0.38087892532348633\n",
      "Training loss for batch 1799 : 0.17826464772224426\n",
      "Training loss for batch 1800 : 0.28601786494255066\n",
      "Training loss for batch 1801 : 0.83840012550354\n",
      "Training loss for batch 1802 : 0.18646830320358276\n",
      "Training loss for batch 1803 : 0.0572325699031353\n",
      "Training loss for batch 1804 : 0.18060508370399475\n",
      "Training loss for batch 1805 : 0.06525696814060211\n",
      "Training loss for batch 1806 : 0.2724921405315399\n",
      "Training loss for batch 1807 : 0.16511602699756622\n",
      "Training loss for batch 1808 : 0.2306312918663025\n",
      "Training loss for batch 1809 : 0.539456307888031\n",
      "Training loss for batch 1810 : 0.14526797831058502\n",
      "Training loss for batch 1811 : 0.3031172752380371\n",
      "Training loss for batch 1812 : 0.13223578035831451\n",
      "Training loss for batch 1813 : 0.1696365922689438\n",
      "Training loss for batch 1814 : 0.6927933096885681\n",
      "Training loss for batch 1815 : 0.26061883568763733\n",
      "Training loss for batch 1816 : 0.24610629677772522\n",
      "Training loss for batch 1817 : 0.2369793802499771\n",
      "Training loss for batch 1818 : 0.2769158184528351\n",
      "Training loss for batch 1819 : 0.11777519434690475\n",
      "Training loss for batch 1820 : 0.5321495532989502\n",
      "Training loss for batch 1821 : 0.2754812240600586\n",
      "Training loss for batch 1822 : 0.126799538731575\n",
      "Training loss for batch 1823 : 0.6082845330238342\n",
      "Training loss for batch 1824 : 0.5937555432319641\n",
      "Training loss for batch 1825 : 0.3251037299633026\n",
      "Training loss for batch 1826 : 0.6386463642120361\n",
      "Training loss for batch 1827 : 0.039649080485105515\n",
      "Training loss for batch 1828 : 0.09685101360082626\n",
      "Training loss for batch 1829 : 0.31334012746810913\n",
      "Training loss for batch 1830 : 0.019984446465969086\n",
      "Training loss for batch 1831 : 0.05768541246652603\n",
      "Training loss for batch 1832 : 0.18367858231067657\n",
      "Training loss for batch 1833 : 0.221258282661438\n",
      "Training loss for batch 1834 : 0.430439293384552\n",
      "Training loss for batch 1835 : 0.3254008889198303\n",
      "Training loss for batch 1836 : 0.33618953824043274\n",
      "Training loss for batch 1837 : 0.4951636791229248\n",
      "Training loss for batch 1838 : 0.03771248087286949\n",
      "Training loss for batch 1839 : 0.4221644103527069\n",
      "Training loss for batch 1840 : 0.20561011135578156\n",
      "Training loss for batch 1841 : 0.11263983696699142\n",
      "Training loss for batch 1842 : 0.414070188999176\n",
      "Training loss for batch 1843 : 0.11475080251693726\n",
      "Training loss for batch 1844 : 0.17679685354232788\n",
      "Training loss for batch 1845 : 0.014060056768357754\n",
      "Training loss for batch 1846 : 0.24061952531337738\n",
      "Training loss for batch 1847 : 0.09212902933359146\n",
      "Training loss for batch 1848 : 0.2604100704193115\n",
      "Training loss for batch 1849 : 0.2259991466999054\n",
      "Training loss for batch 1850 : 0.12586693465709686\n",
      "Training loss for batch 1851 : 0.1420293152332306\n",
      "Training loss for batch 1852 : 0.2226618528366089\n",
      "Training loss for batch 1853 : 0.09688253700733185\n",
      "Training loss for batch 1854 : 0.07087208330631256\n",
      "Training loss for batch 1855 : 0.2038525640964508\n",
      "Training loss for batch 1856 : 0.09143747389316559\n",
      "Training loss for batch 1857 : 0.43877503275871277\n",
      "Training loss for batch 1858 : 0.1466538906097412\n",
      "Training loss for batch 1859 : 0.19419674575328827\n",
      "Training loss for batch 1860 : 0.5193865895271301\n",
      "Training loss for batch 1861 : 0.30981963872909546\n",
      "Training loss for batch 1862 : 0.49420371651649475\n",
      "Training loss for batch 1863 : 0.3023684620857239\n",
      "Training loss for batch 1864 : 0.2995229661464691\n",
      "Training loss for batch 1865 : 0.10405371338129044\n",
      "Training loss for batch 1866 : 0.13267289102077484\n",
      "Training loss for batch 1867 : 0.022079626098275185\n",
      "Training loss for batch 1868 : 0.19481603801250458\n",
      "Training loss for batch 1869 : 0.3990561366081238\n",
      "Training loss for batch 1870 : 0.22024784982204437\n",
      "Training loss for batch 1871 : 0.47024425864219666\n",
      "Training loss for batch 1872 : 0.47986915707588196\n",
      "Training loss for batch 1873 : 0.31625455617904663\n",
      "Training loss for batch 1874 : 0.2911198139190674\n",
      "Training loss for batch 1875 : 0.40084800124168396\n",
      "Training loss for batch 1876 : 0.09002726525068283\n",
      "Training loss for batch 1877 : 0.4656696915626526\n",
      "Training loss for batch 1878 : 0.11617659777402878\n",
      "Training loss for batch 1879 : 0.6265609860420227\n",
      "Training loss for batch 1880 : 0.14503325521945953\n",
      "Training loss for batch 1881 : 0.0\n",
      "Training loss for batch 1882 : 0.1362028568983078\n",
      "Training loss for batch 1883 : 0.18838296830654144\n",
      "Training loss for batch 1884 : 0.3033962845802307\n",
      "Training loss for batch 1885 : 0.19448456168174744\n",
      "Training loss for batch 1886 : 0.41862431168556213\n",
      "Training loss for batch 1887 : 0.14718769490718842\n",
      "Training loss for batch 1888 : 0.09410089254379272\n",
      "Training loss for batch 1889 : 0.0849185660481453\n",
      "Training loss for batch 1890 : 0.06488589942455292\n",
      "Training loss for batch 1891 : 0.3941726088523865\n",
      "Training loss for batch 1892 : 0.31348222494125366\n",
      "Training loss for batch 1893 : 0.3153419494628906\n",
      "Training loss for batch 1894 : 0.12032933533191681\n",
      "Training loss for batch 1895 : 0.4215584397315979\n",
      "Training loss for batch 1896 : 0.35872650146484375\n",
      "Training loss for batch 1897 : 0.5227720141410828\n",
      "Training loss for batch 1898 : 0.1163811981678009\n",
      "Training loss for batch 1899 : 0.1348576545715332\n",
      "Training loss for batch 1900 : 0.05936966836452484\n",
      "Training loss for batch 1901 : 0.5787498354911804\n",
      "Training loss for batch 1902 : 0.14597366750240326\n",
      "Training loss for batch 1903 : 0.2544090151786804\n",
      "Training loss for batch 1904 : 0.1772395372390747\n",
      "Training loss for batch 1905 : 0.15695753693580627\n",
      "Training loss for batch 1906 : 0.031153148040175438\n",
      "Training loss for batch 1907 : 0.48949456214904785\n",
      "Training loss for batch 1908 : 0.11337848007678986\n",
      "Training loss for batch 1909 : 0.5832312107086182\n",
      "Training loss for batch 1910 : 0.37354132533073425\n",
      "Training loss for batch 1911 : 0.2911836504936218\n",
      "Training loss for batch 1912 : 0.1820056140422821\n",
      "Training loss for batch 1913 : 0.33945348858833313\n",
      "Training loss for batch 1914 : 0.2513969838619232\n",
      "Training loss for batch 1915 : 0.3314483165740967\n",
      "Training loss for batch 1916 : 0.5124638676643372\n",
      "Training loss for batch 1917 : 0.3489667475223541\n",
      "Training loss for batch 1918 : 0.15422646701335907\n",
      "Training loss for batch 1919 : 0.14960065484046936\n",
      "Training loss for batch 1920 : 0.16711604595184326\n",
      "Training loss for batch 1921 : 0.11907073110342026\n",
      "Training loss for batch 1922 : 0.22525417804718018\n",
      "Training loss for batch 1923 : 0.2067658007144928\n",
      "Training loss for batch 1924 : 0.25165197253227234\n",
      "Training loss for batch 1925 : 0.13209006190299988\n",
      "Training loss for batch 1926 : 0.8979777097702026\n",
      "Training loss for batch 1927 : 0.401815265417099\n",
      "Training loss for batch 1928 : 0.35869258642196655\n",
      "Training loss for batch 1929 : 0.39434367418289185\n",
      "Training loss for batch 1930 : 0.0293416865170002\n",
      "Training loss for batch 1931 : 0.17109811305999756\n",
      "Training loss for batch 1932 : 0.1731307953596115\n",
      "Training loss for batch 1933 : 0.16918443143367767\n",
      "Training loss for batch 1934 : 0.4911605715751648\n",
      "Training loss for batch 1935 : 0.07482481002807617\n",
      "Training loss for batch 1936 : 0.2925370931625366\n",
      "Training loss for batch 1937 : 0.11747030913829803\n",
      "Training loss for batch 1938 : 0.29926615953445435\n",
      "Training loss for batch 1939 : 0.6469204425811768\n",
      "Training loss for batch 1940 : 0.1489115059375763\n",
      "Training loss for batch 1941 : 0.1415351778268814\n",
      "Training loss for batch 1942 : 0.02644842490553856\n",
      "Training loss for batch 1943 : 0.24954593181610107\n",
      "Training loss for batch 1944 : 0.11320910602807999\n",
      "Training loss for batch 1945 : 0.11469835042953491\n",
      "Training loss for batch 1946 : 0.20728547871112823\n",
      "Training loss for batch 1947 : 0.3835105001926422\n",
      "Training loss for batch 1948 : 0.4791286587715149\n",
      "Training loss for batch 1949 : 0.11617317795753479\n",
      "Training loss for batch 1950 : 0.09439585357904434\n",
      "Training loss for batch 1951 : 0.15420469641685486\n",
      "Training loss for batch 1952 : 0.3347594141960144\n",
      "Training loss for batch 1953 : 0.08533453941345215\n",
      "Training loss for batch 1954 : 0.7171205282211304\n",
      "Training loss for batch 1955 : 0.16404758393764496\n",
      "Training loss for batch 1956 : 0.04634741693735123\n",
      "Training loss for batch 1957 : 0.2305002510547638\n",
      "Training loss for batch 1958 : 0.237023264169693\n",
      "Training loss for batch 1959 : 0.100762277841568\n",
      "Training loss for batch 1960 : 0.1451253890991211\n",
      "Training loss for batch 1961 : 0.2912973165512085\n",
      "Training loss for batch 1962 : 0.3226843476295471\n",
      "Training loss for batch 1963 : 0.3261089622974396\n",
      "Training loss for batch 1964 : 0.7054381370544434\n",
      "Training loss for batch 1965 : 0.02269565686583519\n",
      "Training loss for batch 1966 : 0.3407090902328491\n",
      "Training loss for batch 1967 : 0.10979188978672028\n",
      "Training loss for batch 1968 : 0.006103674881160259\n",
      "Training loss for batch 1969 : 0.008349079638719559\n",
      "Training loss for batch 1970 : 0.2638948857784271\n",
      "Training loss for batch 1971 : 0.24708566069602966\n",
      "Training loss for batch 1972 : 0.1501772105693817\n",
      "Training loss for batch 1973 : 0.015536585822701454\n",
      "Training loss for batch 1974 : 0.24633994698524475\n",
      "Training loss for batch 1975 : 0.3454037010669708\n",
      "Training loss for batch 1976 : 0.1950276792049408\n",
      "Training loss for batch 1977 : 0.17410454154014587\n",
      "Training loss for batch 1978 : 0.07415763288736343\n",
      "Training loss for batch 1979 : 0.21613863110542297\n",
      "Training loss for batch 1980 : 0.18499526381492615\n",
      "Training loss for batch 1981 : 0.22770996391773224\n",
      "Training loss for batch 1982 : 0.018951216712594032\n",
      "Training loss for batch 1983 : 0.04144890606403351\n",
      "Training loss for batch 1984 : 0.22419893741607666\n",
      "Training loss for batch 1985 : 0.22180399298667908\n",
      "Training loss for batch 1986 : 0.4670705199241638\n",
      "Training loss for batch 1987 : 0.29606884717941284\n",
      "Training loss for batch 1988 : 0.41709190607070923\n",
      "Training loss for batch 1989 : 0.7686889171600342\n",
      "Training loss for batch 1990 : 0.2653184235095978\n",
      "Training loss for batch 1991 : 0.13969141244888306\n",
      "Training loss for batch 1992 : 0.22174052894115448\n",
      "Training loss for batch 1993 : 0.3471662104129791\n",
      "Training loss for batch 1994 : 0.21897177398204803\n",
      "Training loss for batch 1995 : 0.08727708458900452\n",
      "Training loss for batch 1996 : 0.1622675657272339\n",
      "Training loss for batch 1997 : 0.3413264751434326\n",
      "Training loss for batch 1998 : 0.19315937161445618\n",
      "Training loss for batch 1999 : 0.3881773352622986\n",
      "Training loss for batch 2000 : 0.42383790016174316\n",
      "Training loss for batch 2001 : 0.13796409964561462\n",
      "Training loss for batch 2002 : 0.28731560707092285\n",
      "Training loss for batch 2003 : 0.22344474494457245\n",
      "Training loss for batch 2004 : 0.17358510196208954\n",
      "Training loss for batch 2005 : 0.1504422426223755\n",
      "Training loss for batch 2006 : 0.19830210506916046\n",
      "Training loss for batch 2007 : 0.06315742433071136\n",
      "Training loss for batch 2008 : 0.40165218710899353\n",
      "Training loss for batch 2009 : 0.631232500076294\n",
      "Training loss for batch 2010 : 0.22544896602630615\n",
      "Training loss for batch 2011 : 0.2491913139820099\n",
      "Training loss for batch 2012 : 0.36040908098220825\n",
      "Training loss for batch 2013 : 0.5007001757621765\n",
      "Training loss for batch 2014 : 0.15463386476039886\n",
      "Training loss for batch 2015 : 0.7983824014663696\n",
      "Training loss for batch 2016 : 0.1359030306339264\n",
      "Training loss for batch 2017 : 0.29018598794937134\n",
      "Training loss for batch 2018 : 0.165176123380661\n",
      "Training loss for batch 2019 : 0.40974655747413635\n",
      "Training loss for batch 2020 : 0.21827815473079681\n",
      "Training loss for batch 2021 : 0.5427210330963135\n",
      "Training loss for batch 2022 : 0.4081742763519287\n",
      "Training loss for batch 2023 : 0.3708711862564087\n",
      "Training loss for batch 2024 : 0.2417522668838501\n",
      "Training loss for batch 2025 : 0.3100179135799408\n",
      "Training loss for batch 2026 : 0.636970043182373\n",
      "Training loss for batch 2027 : 0.2600833475589752\n",
      "Training loss for batch 2028 : 0.3176787495613098\n",
      "Training loss for batch 2029 : 0.03999989107251167\n",
      "Training loss for batch 2030 : 0.3268006145954132\n",
      "Training loss for batch 2031 : 0.5778065323829651\n",
      "Training loss for batch 2032 : 0.404611736536026\n",
      "Training loss for batch 2033 : 0.5148638486862183\n",
      "Training loss for batch 2034 : 0.2319415956735611\n",
      "Training loss for batch 2035 : 0.1367415338754654\n",
      "Training loss for batch 2036 : 0.5246873497962952\n",
      "Training loss for batch 2037 : 0.37475118041038513\n",
      "Training loss for batch 2038 : 0.3532949388027191\n",
      "Training loss for batch 2039 : 0.12365929782390594\n",
      "Training loss for batch 2040 : 0.14860376715660095\n",
      "Training loss for batch 2041 : 0.2090374380350113\n",
      "Training loss for batch 2042 : 0.5359677076339722\n",
      "Training loss for batch 2043 : 0.06584085524082184\n",
      "Training loss for batch 2044 : 0.4430394768714905\n",
      "Training loss for batch 2045 : 0.14186491072177887\n",
      "Training loss for batch 2046 : 0.36585795879364014\n",
      "Training loss for batch 2047 : 0.03414875268936157\n",
      "Training loss for batch 2048 : 0.3373365104198456\n",
      "Training loss for batch 2049 : 0.07904579490423203\n",
      "Training loss for batch 2050 : 0.16463367640972137\n",
      "Training loss for batch 2051 : 0.4676722288131714\n",
      "Training loss for batch 2052 : 0.46026015281677246\n",
      "Training loss for batch 2053 : 0.527990460395813\n",
      "Training loss for batch 2054 : 0.0005548397894017398\n",
      "Training loss for batch 2055 : 0.18724225461483002\n",
      "Training loss for batch 2056 : 0.07268569618463516\n",
      "Training loss for batch 2057 : 0.2835846543312073\n",
      "Training loss for batch 2058 : 0.020087748765945435\n",
      "Training loss for batch 2059 : 0.3337690532207489\n",
      "Training loss for batch 2060 : 0.3544093370437622\n",
      "Training loss for batch 2061 : 0.6987239122390747\n",
      "Training loss for batch 2062 : 0.2883125841617584\n",
      "Training loss for batch 2063 : 0.5999903082847595\n",
      "Training loss for batch 2064 : 0.22888712584972382\n",
      "Training loss for batch 2065 : 0.3772987425327301\n",
      "Training loss for batch 2066 : 0.022186413407325745\n",
      "Training loss for batch 2067 : 0.39440062642097473\n",
      "Training loss for batch 2068 : 0.10693323612213135\n",
      "Training loss for batch 2069 : 0.37869951128959656\n",
      "Training loss for batch 2070 : 0.22863703966140747\n",
      "Training loss for batch 2071 : 0.4545063078403473\n",
      "Training loss for batch 2072 : 0.10743553936481476\n",
      "Training loss for batch 2073 : 0.02080628275871277\n",
      "Training loss for batch 2074 : 0.392749547958374\n",
      "Training loss for batch 2075 : 0.40518271923065186\n",
      "Training loss for batch 2076 : 0.4066585898399353\n",
      "Training loss for batch 2077 : 0.8896706104278564\n",
      "Training loss for batch 2078 : 0.15870848298072815\n",
      "Training loss for batch 2079 : 0.23108085989952087\n",
      "Training loss for batch 2080 : 0.2169288843870163\n",
      "Training loss for batch 2081 : 0.06430993974208832\n",
      "Training loss for batch 2082 : 0.17945407330989838\n",
      "Training loss for batch 2083 : 0.7045899629592896\n",
      "Training loss for batch 2084 : 0.36535122990608215\n",
      "Training loss for batch 2085 : 0.12493434548377991\n",
      "Training loss for batch 2086 : 0.26552674174308777\n",
      "Training loss for batch 2087 : 0.36152783036231995\n",
      "Training loss for batch 2088 : 0.0008784036035649478\n",
      "Training loss for batch 2089 : 0.32777148485183716\n",
      "Training loss for batch 2090 : 0.0889655128121376\n",
      "Training loss for batch 2091 : 0.2784096300601959\n",
      "Training loss for batch 2092 : 0.7367998361587524\n",
      "Training loss for batch 2093 : 0.11080895364284515\n",
      "Training loss for batch 2094 : 0.03990873321890831\n",
      "Training loss for batch 2095 : 0.1788996160030365\n",
      "Training loss for batch 2096 : 0.10196755826473236\n",
      "Training loss for batch 2097 : 0.32108092308044434\n",
      "Training loss for batch 2098 : 0.4268830418586731\n",
      "Training loss for batch 2099 : 0.2668401896953583\n",
      "Training loss for batch 2100 : 0.5256748795509338\n",
      "Training loss for batch 2101 : 0.09238045662641525\n",
      "Training loss for batch 2102 : 0.2174234539270401\n",
      "Training loss for batch 2103 : 0.8587974905967712\n",
      "Training loss for batch 2104 : 0.20537684857845306\n",
      "Training loss for batch 2105 : 0.2759755253791809\n",
      "Training loss for batch 2106 : 0.23541194200515747\n",
      "Training loss for batch 2107 : 0.36245352029800415\n",
      "Training loss for batch 2108 : 0.292492538690567\n",
      "Training loss for batch 2109 : 0.01694715954363346\n",
      "Training loss for batch 2110 : 0.187747061252594\n",
      "Training loss for batch 2111 : 0.25945937633514404\n",
      "Training loss for batch 2112 : 0.7229080200195312\n",
      "Training loss for batch 2113 : 0.6407434940338135\n",
      "Training loss for batch 2114 : 0.219564288854599\n",
      "Training loss for batch 2115 : 0.34888404607772827\n",
      "Training loss for batch 2116 : 0.016989711672067642\n",
      "Training loss for batch 2117 : 0.3216356933116913\n",
      "Training loss for batch 2118 : 0.19953486323356628\n",
      "Training loss for batch 2119 : 0.280954509973526\n",
      "Training loss for batch 2120 : 0.26215696334838867\n",
      "Training loss for batch 2121 : 0.28708335757255554\n",
      "Training loss for batch 2122 : 0.04681537672877312\n",
      "Training loss for batch 2123 : 0.48894375562667847\n",
      "Training loss for batch 2124 : 0.01734372228384018\n",
      "Training loss for batch 2125 : 0.32645729184150696\n",
      "Training loss for batch 2126 : 0.11239373683929443\n",
      "Training loss for batch 2127 : 0.3874273896217346\n",
      "Training loss for batch 2128 : 0.33884936571121216\n",
      "Training loss for batch 2129 : 0.0518040657043457\n",
      "Training loss for batch 2130 : 0.22434909641742706\n",
      "Training loss for batch 2131 : 0.2662595510482788\n",
      "Training loss for batch 2132 : 0.30909278988838196\n",
      "Training loss for batch 2133 : 0.7547680735588074\n",
      "Training loss for batch 2134 : 0.25248727202415466\n",
      "Training loss for batch 2135 : 0.3834037184715271\n",
      "Training loss for batch 2136 : 0.3753759264945984\n",
      "Training loss for batch 2137 : 0.04604574292898178\n",
      "Training loss for batch 2138 : 0.2227545529603958\n",
      "Training loss for batch 2139 : 0.07058534771203995\n",
      "Training loss for batch 2140 : 0.3868962228298187\n",
      "Training loss for batch 2141 : 0.6468060612678528\n",
      "Training loss for batch 2142 : 0.24920296669006348\n",
      "Training loss for batch 2143 : 0.392085462808609\n",
      "Training loss for batch 2144 : 0.09765622764825821\n",
      "Training loss for batch 2145 : 0.026026587933301926\n",
      "Training loss for batch 2146 : 0.6151920557022095\n",
      "Training loss for batch 2147 : 0.12200001627206802\n",
      "Training loss for batch 2148 : 0.15798266232013702\n",
      "Training loss for batch 2149 : 0.22225992381572723\n",
      "Training loss for batch 2150 : 0.17334550619125366\n",
      "Training loss for batch 2151 : 0.12628696858882904\n",
      "Training loss for batch 2152 : 0.379793643951416\n",
      "Training loss for batch 2153 : 0.10325275361537933\n",
      "Training loss for batch 2154 : 0.1937015801668167\n",
      "Training loss for batch 2155 : 0.2724214196205139\n",
      "Training loss for batch 2156 : 0.12130910158157349\n",
      "Training loss for batch 2157 : 0.2478301227092743\n",
      "Training loss for batch 2158 : 0.35966381430625916\n",
      "Training loss for batch 2159 : 0.3715773820877075\n",
      "Training loss for batch 2160 : 0.2371503710746765\n",
      "Training loss for batch 2161 : 0.28746455907821655\n",
      "Training loss for batch 2162 : 0.10298988223075867\n",
      "Training loss for batch 2163 : 0.3213762938976288\n",
      "Training loss for batch 2164 : 0.7855072021484375\n",
      "Training loss for batch 2165 : 0.27388593554496765\n",
      "Training loss for batch 2166 : 0.31409192085266113\n",
      "Training loss for batch 2167 : 0.4648834764957428\n",
      "Training loss for batch 2168 : 0.2142930030822754\n",
      "Training loss for batch 2169 : 0.05942678451538086\n",
      "Training loss for batch 2170 : 0.42082053422927856\n",
      "Training loss for batch 2171 : 0.17253988981246948\n",
      "Training loss for batch 2172 : 0.41588127613067627\n",
      "Training loss for batch 2173 : 0.08845599740743637\n",
      "Training loss for batch 2174 : 0.7488820552825928\n",
      "Training loss for batch 2175 : 0.44913148880004883\n",
      "Training loss for batch 2176 : 0.04957744851708412\n",
      "Training loss for batch 2177 : 0.15209342539310455\n",
      "Training loss for batch 2178 : 0.35135218501091003\n",
      "Training loss for batch 2179 : 0.252095490694046\n",
      "Training loss for batch 2180 : 0.32253679633140564\n",
      "Training loss for batch 2181 : 0.04689725488424301\n",
      "Training loss for batch 2182 : 0.35786059498786926\n",
      "Training loss for batch 2183 : 0.20583416521549225\n",
      "Training loss for batch 2184 : 0.2399853616952896\n",
      "Training loss for batch 2185 : 0.061403173953294754\n",
      "Training loss for batch 2186 : 0.4547533392906189\n",
      "Training loss for batch 2187 : 0.8478145003318787\n",
      "Training loss for batch 2188 : 0.030095519497990608\n",
      "Training loss for batch 2189 : 0.1931445449590683\n",
      "Training loss for batch 2190 : 0.07170357555150986\n",
      "Training loss for batch 2191 : 0.5440338253974915\n",
      "Training loss for batch 2192 : 0.27037566900253296\n",
      "Training loss for batch 2193 : 0.14102524518966675\n",
      "Training loss for batch 2194 : 0.36998656392097473\n",
      "Training loss for batch 2195 : 0.43383923172950745\n",
      "Training loss for batch 2196 : 0.4236343801021576\n",
      "Training loss for batch 2197 : 0.15082578361034393\n",
      "Training loss for batch 2198 : 0.40311551094055176\n",
      "Training loss for batch 2199 : 0.2903740406036377\n",
      "Training loss for batch 2200 : 0.6385923624038696\n",
      "Training loss for batch 2201 : 0.30093544721603394\n",
      "Training loss for batch 2202 : 0.17128930985927582\n",
      "Training loss for batch 2203 : 0.27660536766052246\n",
      "Training loss for batch 2204 : 0.004493264481425285\n",
      "Training loss for batch 2205 : 0.34934893250465393\n",
      "Training loss for batch 2206 : 0.01998305134475231\n",
      "Training loss for batch 2207 : 0.16863472759723663\n",
      "Training loss for batch 2208 : 0.13499724864959717\n",
      "Training loss for batch 2209 : 0.290855348110199\n",
      "Training loss for batch 2210 : 0.351597398519516\n",
      "Training loss for batch 2211 : 0.42307794094085693\n",
      "Training loss for batch 2212 : 0.39111918210983276\n",
      "Training loss for batch 2213 : 0.3751656413078308\n",
      "Training loss for batch 2214 : 0.27269119024276733\n",
      "Training loss for batch 2215 : 0.3609185814857483\n",
      "Training loss for batch 2216 : 0.6396942734718323\n",
      "Training loss for batch 2217 : 0.3587985038757324\n",
      "Training loss for batch 2218 : 0.1601160764694214\n",
      "Training loss for batch 2219 : 0.09902273863554001\n",
      "Training loss for batch 2220 : 0.2995540201663971\n",
      "Training loss for batch 2221 : 0.09964676201343536\n",
      "Training loss for batch 2222 : 0.12003432959318161\n",
      "Training loss for batch 2223 : 0.29905736446380615\n",
      "Training loss for batch 2224 : 0.08341933786869049\n",
      "Training loss for batch 2225 : 0.053831443190574646\n",
      "Training loss for batch 2226 : 0.005246691405773163\n",
      "Training loss for batch 2227 : 0.22608493268489838\n",
      "Training loss for batch 2228 : 0.23407533764839172\n",
      "Training loss for batch 2229 : 0.33503061532974243\n",
      "Training loss for batch 2230 : 0.3880006968975067\n",
      "Training loss for batch 2231 : 0.535262942314148\n",
      "Training loss for batch 2232 : 0.2059161365032196\n",
      "Training loss for batch 2233 : 0.09818479418754578\n",
      "Training loss for batch 2234 : 0.5930964946746826\n",
      "Training loss for batch 2235 : 0.520162045955658\n",
      "Training loss for batch 2236 : 0.19169792532920837\n",
      "Training loss for batch 2237 : 0.24843978881835938\n",
      "Training loss for batch 2238 : 0.09555519372224808\n",
      "Training loss for batch 2239 : 0.21657611429691315\n",
      "Training loss for batch 2240 : 0.29348763823509216\n",
      "Training loss for batch 2241 : 0.27675822377204895\n",
      "Training loss for batch 2242 : 0.7812623381614685\n",
      "Training loss for batch 2243 : 0.09615335613489151\n",
      "Training loss for batch 2244 : 0.05601697787642479\n",
      "Training loss for batch 2245 : 0.05517778545618057\n",
      "Training loss for batch 2246 : 0.09164636582136154\n",
      "Training loss for batch 2247 : 0.1271696388721466\n",
      "Training loss for batch 2248 : 0.06582348048686981\n",
      "Training loss for batch 2249 : 0.22367998957633972\n",
      "Training loss for batch 2250 : 0.27495473623275757\n",
      "Training loss for batch 2251 : 0.16389457881450653\n",
      "Training loss for batch 2252 : 0.09449756890535355\n",
      "Training loss for batch 2253 : 0.6158755421638489\n",
      "Training loss for batch 2254 : 0.5749478340148926\n",
      "Training loss for batch 2255 : 0.19462750852108002\n",
      "Training loss for batch 2256 : 0.19834044575691223\n",
      "Training loss for batch 2257 : 0.332675039768219\n",
      "Training loss for batch 2258 : 0.08886393904685974\n",
      "Training loss for batch 2259 : 0.1312750279903412\n",
      "Training loss for batch 2260 : 0.1452193558216095\n",
      "Training loss for batch 2261 : 0.10277877748012543\n",
      "Training loss for batch 2262 : 0.027322575449943542\n",
      "Training loss for batch 2263 : 0.20319277048110962\n",
      "Training loss for batch 2264 : 0.49409034848213196\n",
      "Training loss for batch 2265 : 0.16127818822860718\n",
      "Training loss for batch 2266 : 0.32460612058639526\n",
      "Training loss for batch 2267 : 0.1512613147497177\n",
      "Training loss for batch 2268 : 0.19743119180202484\n",
      "Training loss for batch 2269 : 0.2256719470024109\n",
      "Training loss for batch 2270 : 0.39143016934394836\n",
      "Training loss for batch 2271 : 0.15039284527301788\n",
      "Training loss for batch 2272 : 0.05701502785086632\n",
      "Training loss for batch 2273 : 0.17462435364723206\n",
      "Training loss for batch 2274 : 0.008804887533187866\n",
      "Training loss for batch 2275 : 0.06975574791431427\n",
      "Training loss for batch 2276 : 0.19632786512374878\n",
      "Training loss for batch 2277 : 0.10510335862636566\n",
      "Training loss for batch 2278 : 0.4384972155094147\n",
      "Training loss for batch 2279 : 0.5070132613182068\n",
      "Training loss for batch 2280 : 0.37286531925201416\n",
      "Training loss for batch 2281 : 0.08292757719755173\n",
      "Training loss for batch 2282 : 0.32714709639549255\n",
      "Training loss for batch 2283 : 0.06347537040710449\n",
      "Training loss for batch 2284 : 0.48326781392097473\n",
      "Training loss for batch 2285 : 0.1386648267507553\n",
      "Training loss for batch 2286 : 0.03175539895892143\n",
      "Training loss for batch 2287 : 0.3900395333766937\n",
      "Training loss for batch 2288 : 0.1860734224319458\n",
      "Training loss for batch 2289 : 0.6444017887115479\n",
      "Training loss for batch 2290 : 0.023039033636450768\n",
      "Training loss for batch 2291 : 0.43418818712234497\n",
      "Training loss for batch 2292 : 0.4947693645954132\n",
      "Training loss for batch 2293 : 0.2326945662498474\n",
      "Training loss for batch 2294 : 0.17030836641788483\n",
      "Training loss for batch 2295 : 0.23409809172153473\n",
      "Training loss for batch 2296 : 0.2820624113082886\n",
      "Training loss for batch 2297 : 0.034260496497154236\n",
      "Training loss for batch 2298 : 0.23832592368125916\n",
      "Training loss for batch 2299 : 0.4805745482444763\n",
      "Training loss for batch 2300 : 0.3796081244945526\n",
      "Training loss for batch 2301 : 0.2375432550907135\n",
      "Training loss for batch 2302 : 0.07601296156644821\n",
      "Training loss for batch 2303 : 0.11492180824279785\n",
      "Training loss for batch 2304 : 0.045708201825618744\n",
      "Training loss for batch 2305 : 0.12273860722780228\n",
      "Training loss for batch 2306 : 0.2340727001428604\n",
      "Training loss for batch 2307 : 0.3387567698955536\n",
      "Training loss for batch 2308 : 0.45653849840164185\n",
      "Training loss for batch 2309 : 0.08907242864370346\n",
      "Training loss for batch 2310 : 0.028746504336595535\n",
      "Training loss for batch 2311 : 1.0983320474624634\n",
      "Training loss for batch 2312 : 0.43219172954559326\n",
      "Training loss for batch 2313 : 0.35901033878326416\n",
      "Training loss for batch 2314 : 0.125242680311203\n",
      "Training loss for batch 2315 : 0.33938270807266235\n",
      "Training loss for batch 2316 : 0.4720498323440552\n",
      "Training loss for batch 2317 : 0.3006550967693329\n",
      "Training loss for batch 2318 : 0.262672483921051\n",
      "Training loss for batch 2319 : 0.1470247507095337\n",
      "Training loss for batch 2320 : 0.09448554366827011\n",
      "Training loss for batch 2321 : 0.23524820804595947\n",
      "Training loss for batch 2322 : 0.42238035798072815\n",
      "Training loss for batch 2323 : 0.3061088025569916\n",
      "Training loss for batch 2324 : 0.1486206352710724\n",
      "Training loss for batch 2325 : 0.2098066806793213\n",
      "Training loss for batch 2326 : 0.1523219645023346\n",
      "Training loss for batch 2327 : 0.31092438101768494\n",
      "Training loss for batch 2328 : 0.20349368453025818\n",
      "Training loss for batch 2329 : 0.1245201975107193\n",
      "Training loss for batch 2330 : 0.10838272422552109\n",
      "Training loss for batch 2331 : 0.12584705650806427\n",
      "Training loss for batch 2332 : 0.4315720796585083\n",
      "Training loss for batch 2333 : 0.23824329674243927\n",
      "Training loss for batch 2334 : 0.32044363021850586\n",
      "Training loss for batch 2335 : 0.5212799310684204\n",
      "Training loss for batch 2336 : 0.16843941807746887\n",
      "Training loss for batch 2337 : 0.2753782570362091\n",
      "Training loss for batch 2338 : 0.42592835426330566\n",
      "Training loss for batch 2339 : 0.30469587445259094\n",
      "Training loss for batch 2340 : 0.42156529426574707\n",
      "Training loss for batch 2341 : 0.0008443650440312922\n",
      "Training loss for batch 2342 : 0.027814358472824097\n",
      "Training loss for batch 2343 : 0.1810094565153122\n",
      "Training loss for batch 2344 : 0.007516433950513601\n",
      "Training loss for batch 2345 : 0.006538719870150089\n",
      "Training loss for batch 2346 : 0.3536841869354248\n",
      "Training loss for batch 2347 : 0.011912871152162552\n",
      "Training loss for batch 2348 : 0.12995463609695435\n",
      "Training loss for batch 2349 : 0.41709235310554504\n",
      "Training loss for batch 2350 : 0.1484094262123108\n",
      "Training loss for batch 2351 : 0.4025912582874298\n",
      "Training loss for batch 2352 : 0.25486043095588684\n",
      "Training loss for batch 2353 : 0.2268683910369873\n",
      "Training loss for batch 2354 : 0.6217216849327087\n",
      "Training loss for batch 2355 : 0.06939700990915298\n",
      "Training loss for batch 2356 : 0.5014821290969849\n",
      "Training loss for batch 2357 : 0.0708053931593895\n",
      "Training loss for batch 2358 : 0.051109373569488525\n",
      "Training loss for batch 2359 : 0.19921086728572845\n",
      "Training loss for batch 2360 : 0.25468897819519043\n",
      "Training loss for batch 2361 : 0.1041787639260292\n",
      "Training loss for batch 2362 : 0.3585870563983917\n",
      "Training loss for batch 2363 : 0.5333725810050964\n",
      "Training loss for batch 2364 : 0.22837021946907043\n",
      "Training loss for batch 2365 : 0.5646857619285583\n",
      "Training loss for batch 2366 : 0.5223358869552612\n",
      "Training loss for batch 2367 : 0.1000332310795784\n",
      "Training loss for batch 2368 : 0.1707509607076645\n",
      "Training loss for batch 2369 : 0.4624553620815277\n",
      "Training loss for batch 2370 : 0.21750499308109283\n",
      "Training loss for batch 2371 : 0.2354939728975296\n",
      "Training loss for batch 2372 : 0.21066297590732574\n",
      "Training loss for batch 2373 : 0.005907118320465088\n",
      "Training loss for batch 2374 : 0.1316191852092743\n",
      "Training loss for batch 2375 : 0.12868830561637878\n",
      "Training loss for batch 2376 : 0.32464510202407837\n",
      "Training loss for batch 2377 : 0.06006414815783501\n",
      "Training loss for batch 2378 : 0.4635775685310364\n",
      "Training loss for batch 2379 : 0.3446166515350342\n",
      "Training loss for batch 2380 : 0.0\n",
      "Training loss for batch 2381 : 0.15263968706130981\n",
      "Training loss for batch 2382 : 0.19511674344539642\n",
      "Training loss for batch 2383 : 0.11323685199022293\n",
      "Training loss for batch 2384 : 0.2841013967990875\n",
      "Training loss for batch 2385 : 0.6610981225967407\n",
      "Training loss for batch 2386 : 0.09965140372514725\n",
      "Training loss for batch 2387 : 0.04162266477942467\n",
      "Training loss for batch 2388 : 0.10950322449207306\n",
      "Training loss for batch 2389 : 0.14768624305725098\n",
      "Training loss for batch 2390 : 0.28962603211402893\n",
      "Training loss for batch 2391 : 0.02068694308400154\n",
      "Training loss for batch 2392 : 0.36018678545951843\n",
      "Training loss for batch 2393 : 0.48570767045021057\n",
      "Training loss for batch 2394 : 0.1713767796754837\n",
      "Training loss for batch 2395 : 0.7037145495414734\n",
      "Training loss for batch 2396 : 0.08940751105546951\n",
      "Training loss for batch 2397 : 0.11786762624979019\n",
      "Training loss for batch 2398 : 0.5071713924407959\n",
      "Training loss for batch 2399 : 0.3178858160972595\n",
      "Training loss for batch 2400 : 0.35005518794059753\n",
      "Training loss for batch 2401 : 0.16649594902992249\n",
      "Training loss for batch 2402 : 0.1212322786450386\n",
      "Training loss for batch 2403 : 0.1013709157705307\n",
      "Training loss for batch 2404 : 0.7856085896492004\n",
      "Training loss for batch 2405 : 0.0896138846874237\n",
      "Training loss for batch 2406 : 0.13312479853630066\n",
      "Training loss for batch 2407 : 0.5404582619667053\n",
      "Training loss for batch 2408 : 0.1911262720823288\n",
      "Training loss for batch 2409 : 0.018189938738942146\n",
      "Training loss for batch 2410 : 0.41419708728790283\n",
      "Training loss for batch 2411 : 0.5440996885299683\n",
      "Training loss for batch 2412 : 0.32059627771377563\n",
      "Training loss for batch 2413 : 0.5749998688697815\n",
      "Training loss for batch 2414 : 0.28704923391342163\n",
      "Training loss for batch 2415 : 0.10061264038085938\n",
      "Training loss for batch 2416 : 0.03570183366537094\n",
      "Training loss for batch 2417 : 0.3571985960006714\n",
      "Training loss for batch 2418 : 0.7280644774436951\n",
      "Training loss for batch 2419 : 0.5330146551132202\n",
      "Training loss for batch 2420 : 0.21089854836463928\n",
      "Training loss for batch 2421 : 0.3193897604942322\n",
      "Training loss for batch 2422 : 0.027087172493338585\n",
      "Training loss for batch 2423 : 0.34193646907806396\n",
      "Training loss for batch 2424 : 0.2035156488418579\n",
      "Training loss for batch 2425 : 0.21472126245498657\n",
      "Training loss for batch 2426 : 0.10146065801382065\n",
      "Training loss for batch 2427 : 0.030378159135580063\n",
      "Training loss for batch 2428 : 0.17003250122070312\n",
      "Training loss for batch 2429 : 0.24537931382656097\n",
      "Training loss for batch 2430 : 0.7159237861633301\n",
      "Training loss for batch 2431 : 0.21501336991786957\n",
      "Training loss for batch 2432 : 0.2215556800365448\n",
      "Training loss for batch 2433 : 0.2661437690258026\n",
      "Training loss for batch 2434 : 0.255435049533844\n",
      "Training loss for batch 2435 : 0.2666899859905243\n",
      "Training loss for batch 2436 : 0.17818236351013184\n",
      "Training loss for batch 2437 : 0.0\n",
      "Training loss for batch 2438 : 0.3539310693740845\n",
      "Training loss for batch 2439 : 0.06347548216581345\n",
      "Training loss for batch 2440 : 0.13703016936779022\n",
      "Training loss for batch 2441 : 0.5707904696464539\n",
      "Training loss for batch 2442 : 0.42787662148475647\n",
      "Training loss for batch 2443 : 0.2506764531135559\n",
      "Training loss for batch 2444 : 0.1387861967086792\n",
      "Training loss for batch 2445 : 0.13789373636245728\n",
      "Training loss for batch 2446 : 0.16529574990272522\n",
      "Training loss for batch 2447 : 0.23300959169864655\n",
      "Training loss for batch 2448 : 0.2971196174621582\n",
      "Training loss for batch 2449 : 0.01774427853524685\n",
      "Training loss for batch 2450 : 0.2334868609905243\n",
      "Training loss for batch 2451 : 0.17210176587104797\n",
      "Training loss for batch 2452 : 0.3560660183429718\n",
      "Training loss for batch 2453 : 0.015676727518439293\n",
      "Training loss for batch 2454 : 0.2218456119298935\n",
      "Training loss for batch 2455 : 0.1831391155719757\n",
      "Training loss for batch 2456 : 0.22683346271514893\n",
      "Training loss for batch 2457 : 0.054517101496458054\n",
      "Training loss for batch 2458 : 0.24003981053829193\n",
      "Training loss for batch 2459 : 0.5191535949707031\n",
      "Training loss for batch 2460 : 0.010171662084758282\n",
      "Training loss for batch 2461 : 0.26998016238212585\n",
      "Training loss for batch 2462 : 0.0018375515937805176\n",
      "Training loss for batch 2463 : 0.5988079905509949\n",
      "Training loss for batch 2464 : 0.3747280240058899\n",
      "Training loss for batch 2465 : 0.13425712287425995\n",
      "Training loss for batch 2466 : 0.2874569892883301\n",
      "Training loss for batch 2467 : 0.1495756357908249\n",
      "Training loss for batch 2468 : 0.22005464136600494\n",
      "Training loss for batch 2469 : 0.13288627564907074\n",
      "Training loss for batch 2470 : 0.21103671193122864\n",
      "Training loss for batch 2471 : 0.39223507046699524\n",
      "Training loss for batch 2472 : 0.28433957695961\n",
      "Training loss for batch 2473 : 0.0552515834569931\n",
      "Training loss for batch 2474 : 0.4059642553329468\n",
      "Training loss for batch 2475 : 0.06641288101673126\n",
      "Training loss for batch 2476 : 0.14137394726276398\n",
      "Training loss for batch 2477 : 0.039738498628139496\n",
      "Training loss for batch 2478 : 0.04732252657413483\n",
      "Training loss for batch 2479 : 0.10612945258617401\n",
      "Training loss for batch 2480 : 0.406745582818985\n",
      "Training loss for batch 2481 : 0.0908898115158081\n",
      "Training loss for batch 2482 : 0.3224293887615204\n",
      "Training loss for batch 2483 : 0.15332792699337006\n",
      "Training loss for batch 2484 : 0.20988619327545166\n",
      "Training loss for batch 2485 : 0.2437722384929657\n",
      "Training loss for batch 2486 : 0.18447716534137726\n",
      "Training loss for batch 2487 : 0.042636677622795105\n",
      "Training loss for batch 2488 : 0.3624560236930847\n",
      "Training loss for batch 2489 : 0.20647746324539185\n",
      "Training loss for batch 2490 : 0.1642242670059204\n",
      "Training loss for batch 2491 : 0.1196526363492012\n",
      "Training loss for batch 2492 : 0.11216288059949875\n",
      "Training loss for batch 2493 : 0.33807554841041565\n",
      "Training loss for batch 2494 : 0.3707529604434967\n",
      "Training loss for batch 2495 : 0.4814227223396301\n",
      "Training loss for batch 2496 : 0.044708479195833206\n",
      "Training loss for batch 2497 : 0.07491368800401688\n",
      "Training loss for batch 2498 : 0.16138318181037903\n",
      "Training loss for batch 2499 : 0.5519474148750305\n",
      "Training loss for batch 2500 : 0.04749737307429314\n",
      "Training loss for batch 2501 : 0.30767887830734253\n",
      "Training loss for batch 2502 : 0.34234797954559326\n",
      "Training loss for batch 2503 : 0.42593783140182495\n",
      "Training loss for batch 2504 : 0.08485876768827438\n",
      "Training loss for batch 2505 : 0.1767679750919342\n",
      "Training loss for batch 2506 : 0.26319974660873413\n",
      "Training loss for batch 2507 : 0.36427196860313416\n",
      "Training loss for batch 2508 : 0.06296290457248688\n",
      "Training loss for batch 2509 : 0.3377474546432495\n",
      "Training loss for batch 2510 : 0.1496555209159851\n",
      "Training loss for batch 2511 : 0.18774756789207458\n",
      "Training loss for batch 2512 : 0.038210127502679825\n",
      "Training loss for batch 2513 : 0.025389691814780235\n",
      "Training loss for batch 2514 : 0.2514183223247528\n",
      "Training loss for batch 2515 : 0.379305899143219\n",
      "Training loss for batch 2516 : 0.1692422777414322\n",
      "Training loss for batch 2517 : 0.34592360258102417\n",
      "Training loss for batch 2518 : 0.12239284068346024\n",
      "Training loss for batch 2519 : 0.3164348602294922\n",
      "Training loss for batch 2520 : 0.20648448169231415\n",
      "Training loss for batch 2521 : 0.3469681441783905\n",
      "Training loss for batch 2522 : 0.2791687846183777\n",
      "Training loss for batch 2523 : 0.44905853271484375\n",
      "Training loss for batch 2524 : 0.07120063900947571\n",
      "Training loss for batch 2525 : 0.0\n",
      "Training loss for batch 2526 : 0.3700496256351471\n",
      "Training loss for batch 2527 : 0.1805247962474823\n",
      "Training loss for batch 2528 : 0.05373426154255867\n",
      "Training loss for batch 2529 : 0.40965959429740906\n",
      "Training loss for batch 2530 : 0.23068425059318542\n",
      "Training loss for batch 2531 : 0.4481894075870514\n",
      "Training loss for batch 2532 : 0.4773552417755127\n",
      "Training loss for batch 2533 : 0.0789492279291153\n",
      "Training loss for batch 2534 : 0.06083826348185539\n",
      "Training loss for batch 2535 : 0.2994367480278015\n",
      "Training loss for batch 2536 : 0.5346003174781799\n",
      "Training loss for batch 2537 : 0.36366894841194153\n",
      "Training loss for batch 2538 : 0.2171144187450409\n",
      "Training loss for batch 2539 : 0.3374233543872833\n",
      "Training loss for batch 2540 : 0.23715049028396606\n",
      "Training loss for batch 2541 : 0.22806036472320557\n",
      "Training loss for batch 2542 : 0.26113659143447876\n",
      "Training loss for batch 2543 : 0.288115918636322\n",
      "Training loss for batch 2544 : 0.26552167534828186\n",
      "Training loss for batch 2545 : 0.4737771451473236\n",
      "Training loss for batch 2546 : 0.07639376819133759\n",
      "Training loss for batch 2547 : 0.20215675234794617\n",
      "Training loss for batch 2548 : 0.13127781450748444\n",
      "Training loss for batch 2549 : 0.39541909098625183\n",
      "Training loss for batch 2550 : 0.038965821266174316\n",
      "Training loss for batch 2551 : 0.17806941270828247\n",
      "Training loss for batch 2552 : 0.4423303008079529\n",
      "Training loss for batch 2553 : 0.030554411932826042\n",
      "Training loss for batch 2554 : 0.45265185832977295\n",
      "Training loss for batch 2555 : 0.4068053066730499\n",
      "Training loss for batch 2556 : 0.2200825959444046\n",
      "Training loss for batch 2557 : 0.6390340328216553\n",
      "Training loss for batch 2558 : 0.19207902252674103\n",
      "Training loss for batch 2559 : 0.179144024848938\n",
      "Training loss for batch 2560 : 0.0037233829498291016\n",
      "Training loss for batch 2561 : 0.011925682425498962\n",
      "Training loss for batch 2562 : 0.005511343479156494\n",
      "Training loss for batch 2563 : 0.41260433197021484\n",
      "Training loss for batch 2564 : 0.4738052189350128\n",
      "Training loss for batch 2565 : 0.15919676423072815\n",
      "Training loss for batch 2566 : 0.05460234731435776\n",
      "Training loss for batch 2567 : 0.1224265992641449\n",
      "Training loss for batch 2568 : 0.19654850661754608\n",
      "Training loss for batch 2569 : 0.3554382622241974\n",
      "Training loss for batch 2570 : 0.11362816393375397\n",
      "Training loss for batch 2571 : 0.3697623610496521\n",
      "Training loss for batch 2572 : 0.014352256432175636\n",
      "Training loss for batch 2573 : 0.15149764716625214\n",
      "Training loss for batch 2574 : 0.0923771932721138\n",
      "Training loss for batch 2575 : 0.223390594124794\n",
      "Training loss for batch 2576 : 0.060901008546352386\n",
      "Training loss for batch 2577 : 0.21437770128250122\n",
      "Training loss for batch 2578 : 0.38292166590690613\n",
      "Training loss for batch 2579 : 0.43057718873023987\n",
      "Training loss for batch 2580 : 0.08864571899175644\n",
      "Training loss for batch 2581 : 0.06718213856220245\n",
      "Training loss for batch 2582 : 0.2547474801540375\n",
      "Training loss for batch 2583 : 0.28227001428604126\n",
      "Training loss for batch 2584 : 0.05407533422112465\n",
      "Training loss for batch 2585 : 0.4458387792110443\n",
      "Training loss for batch 2586 : 0.5368866324424744\n",
      "Training loss for batch 2587 : 0.2520803213119507\n",
      "Training loss for batch 2588 : 0.1720396876335144\n",
      "Training loss for batch 2589 : 0.06783241778612137\n",
      "Training loss for batch 2590 : 0.24525047838687897\n",
      "Training loss for batch 2591 : 0.1503719687461853\n",
      "Training loss for batch 2592 : 0.3736223876476288\n",
      "Training loss for batch 2593 : 0.04253684729337692\n",
      "Training loss for batch 2594 : 0.06620778888463974\n",
      "Training loss for batch 2595 : 0.1514260172843933\n",
      "Training loss for batch 2596 : 0.11776430904865265\n",
      "Training loss for batch 2597 : 0.23347248136997223\n",
      "Training loss for batch 2598 : 0.7067589163780212\n",
      "Training loss for batch 2599 : 0.0010938511695712805\n",
      "Training loss for batch 2600 : 0.3112015426158905\n",
      "Training loss for batch 2601 : 0.023895537480711937\n",
      "Training loss for batch 2602 : 0.04407083988189697\n",
      "Training loss for batch 2603 : 0.08907809108495712\n",
      "Training loss for batch 2604 : 0.050900690257549286\n",
      "Training loss for batch 2605 : 0.043668877333402634\n",
      "Training loss for batch 2606 : 0.0891089141368866\n",
      "Training loss for batch 2607 : 0.05809973552823067\n",
      "Training loss for batch 2608 : 0.6562730073928833\n",
      "Training loss for batch 2609 : 0.21641114354133606\n",
      "Training loss for batch 2610 : 0.2100408971309662\n",
      "Training loss for batch 2611 : 0.05808320641517639\n",
      "Training loss for batch 2612 : 0.14460501074790955\n",
      "Training loss for batch 2613 : 0.20134009420871735\n",
      "Training loss for batch 2614 : 0.01391896978020668\n",
      "Training loss for batch 2615 : 0.013152558356523514\n",
      "Training loss for batch 2616 : 0.047872986644506454\n",
      "Training loss for batch 2617 : 0.364938348531723\n",
      "Training loss for batch 2618 : 0.10832516103982925\n",
      "Training loss for batch 2619 : 0.4501809775829315\n",
      "Training loss for batch 2620 : 0.17301586270332336\n",
      "Training loss for batch 2621 : 0.2006249874830246\n",
      "Training loss for batch 2622 : 9.744366252562031e-05\n",
      "Training loss for batch 2623 : 0.20900718867778778\n",
      "Training loss for batch 2624 : 0.06059979647397995\n",
      "Training loss for batch 2625 : 0.3499282896518707\n",
      "Training loss for batch 2626 : 0.502084493637085\n",
      "Training loss for batch 2627 : 0.3503938317298889\n",
      "Training loss for batch 2628 : 0.48710599541664124\n",
      "Training loss for batch 2629 : 0.17945566773414612\n",
      "Training loss for batch 2630 : 0.3577241003513336\n",
      "Training loss for batch 2631 : 0.0\n",
      "Training loss for batch 2632 : 0.09463168680667877\n",
      "Training loss for batch 2633 : 0.2550913989543915\n",
      "Training loss for batch 2634 : 0.31739360094070435\n",
      "Training loss for batch 2635 : 0.33778566122055054\n",
      "Training loss for batch 2636 : 0.39461007714271545\n",
      "Training loss for batch 2637 : 0.19240190088748932\n",
      "Training loss for batch 2638 : 0.016980677843093872\n",
      "Training loss for batch 2639 : 0.19004176557064056\n",
      "Training loss for batch 2640 : 0.5477639436721802\n",
      "Training loss for batch 2641 : 0.04295187443494797\n",
      "Training loss for batch 2642 : 0.11218387633562088\n",
      "Training loss for batch 2643 : 0.09746232628822327\n",
      "Training loss for batch 2644 : 0.01925763487815857\n",
      "Training loss for batch 2645 : 0.1882951557636261\n",
      "Training loss for batch 2646 : 0.10321761667728424\n",
      "Training loss for batch 2647 : 0.3411460816860199\n",
      "Training loss for batch 2648 : 0.10953757166862488\n",
      "Training loss for batch 2649 : 0.10471628606319427\n",
      "Training loss for batch 2650 : 0.3334128260612488\n",
      "Training loss for batch 2651 : 0.019486308097839355\n",
      "Training loss for batch 2652 : 0.030774857848882675\n",
      "Training loss for batch 2653 : 0.2958710193634033\n",
      "Training loss for batch 2654 : 0.14002077281475067\n",
      "Training loss for batch 2655 : 0.0030872425995767117\n",
      "Training loss for batch 2656 : 0.6120113134384155\n",
      "Training loss for batch 2657 : 0.351630300283432\n",
      "Training loss for batch 2658 : 0.28377261757850647\n",
      "Training loss for batch 2659 : 0.3312699496746063\n",
      "Training loss for batch 2660 : 0.09233244508504868\n",
      "Training loss for batch 2661 : 0.08356257528066635\n",
      "Training loss for batch 2662 : 0.29059964418411255\n",
      "Training loss for batch 2663 : 0.17026832699775696\n",
      "Training loss for batch 2664 : 0.07475856691598892\n",
      "Training loss for batch 2665 : 0.1585242599248886\n",
      "Training loss for batch 2666 : 0.38928601145744324\n",
      "Training loss for batch 2667 : 0.07955911755561829\n",
      "Training loss for batch 2668 : 0.27553585171699524\n",
      "Training loss for batch 2669 : 0.38633105158805847\n",
      "Training loss for batch 2670 : 0.2543758451938629\n",
      "Training loss for batch 2671 : 0.11267246305942535\n",
      "Training loss for batch 2672 : 0.09189920127391815\n",
      "Training loss for batch 2673 : 0.4634988605976105\n",
      "Training loss for batch 2674 : 0.10778424143791199\n",
      "Training loss for batch 2675 : 0.1862676590681076\n",
      "Training loss for batch 2676 : 0.09888182580471039\n",
      "Training loss for batch 2677 : 0.0556098073720932\n",
      "Training loss for batch 2678 : 0.3997005224227905\n",
      "Training loss for batch 2679 : 0.43657633662223816\n",
      "Training loss for batch 2680 : 0.524506688117981\n",
      "Training loss for batch 2681 : 0.21149688959121704\n",
      "Training loss for batch 2682 : 0.5748646855354309\n",
      "Training loss for batch 2683 : 0.2549434304237366\n",
      "Training loss for batch 2684 : 0.21577918529510498\n",
      "Training loss for batch 2685 : 0.1104888916015625\n",
      "Training loss for batch 2686 : 0.11467312276363373\n",
      "Training loss for batch 2687 : 0.20376741886138916\n",
      "Training loss for batch 2688 : 0.39678454399108887\n",
      "Training loss for batch 2689 : 0.1964493989944458\n",
      "Training loss for batch 2690 : 0.40949952602386475\n",
      "Training loss for batch 2691 : 0.14016589522361755\n",
      "Training loss for batch 2692 : 0.047566067427396774\n",
      "Training loss for batch 2693 : 0.579447865486145\n",
      "Training loss for batch 2694 : 0.41725918650627136\n",
      "Training loss for batch 2695 : 0.46502724289894104\n",
      "Training loss for batch 2696 : 0.31120264530181885\n",
      "Training loss for batch 2697 : 0.5651777982711792\n",
      "Training loss for batch 2698 : 0.2365802675485611\n",
      "Training loss for batch 2699 : 0.1892811805009842\n",
      "Training loss for batch 2700 : 0.130484938621521\n",
      "Training loss for batch 2701 : 0.06393694877624512\n",
      "Training loss for batch 2702 : 0.8250468969345093\n",
      "Training loss for batch 2703 : 0.3302486538887024\n",
      "Training loss for batch 2704 : 0.019277991726994514\n",
      "Training loss for batch 2705 : 0.17890769243240356\n",
      "Training loss for batch 2706 : 0.043672528117895126\n",
      "Training loss for batch 2707 : 0.17652881145477295\n",
      "Training loss for batch 2708 : 0.42595914006233215\n",
      "Training loss for batch 2709 : 0.04174454137682915\n",
      "Training loss for batch 2710 : 0.20903751254081726\n",
      "Training loss for batch 2711 : 0.19721980392932892\n",
      "Training loss for batch 2712 : 0.16294492781162262\n",
      "Training loss for batch 2713 : 0.3157104253768921\n",
      "Training loss for batch 2714 : 0.010551273822784424\n",
      "Training loss for batch 2715 : 0.07113399356603622\n",
      "Training loss for batch 2716 : 0.22733062505722046\n",
      "Training loss for batch 2717 : 0.011698615737259388\n",
      "Training loss for batch 2718 : 0.43533140420913696\n",
      "Training loss for batch 2719 : 0.032387733459472656\n",
      "Training loss for batch 2720 : 0.16626210510730743\n",
      "Training loss for batch 2721 : 0.3873564898967743\n",
      "Training loss for batch 2722 : 0.10168470442295074\n",
      "Training loss for batch 2723 : 0.17858947813510895\n",
      "Training loss for batch 2724 : 0.07060684263706207\n",
      "Training loss for batch 2725 : 0.12135879695415497\n",
      "Training loss for batch 2726 : 0.22650626301765442\n",
      "Training loss for batch 2727 : 0.3113124370574951\n",
      "Training loss for batch 2728 : 0.16100049018859863\n",
      "Training loss for batch 2729 : 0.22852836549282074\n",
      "Training loss for batch 2730 : 0.007614186964929104\n",
      "Training loss for batch 2731 : 0.13615451753139496\n",
      "Training loss for batch 2732 : 0.21515046060085297\n",
      "Training loss for batch 2733 : 0.2671146094799042\n",
      "Training loss for batch 2734 : 0.1700606346130371\n",
      "Training loss for batch 2735 : 0.16747431457042694\n",
      "Training loss for batch 2736 : 0.22980283200740814\n",
      "Training loss for batch 2737 : 0.1869366317987442\n",
      "Training loss for batch 2738 : 0.1994323432445526\n",
      "Training loss for batch 2739 : 0.08978410810232162\n",
      "Training loss for batch 2740 : 0.536380410194397\n",
      "Training loss for batch 2741 : 0.5266189575195312\n",
      "Training loss for batch 2742 : 0.05429406464099884\n",
      "Training loss for batch 2743 : 0.3520973324775696\n",
      "Training loss for batch 2744 : 0.6185998916625977\n",
      "Training loss for batch 2745 : 0.2623457908630371\n",
      "Training loss for batch 2746 : 0.632499635219574\n",
      "Training loss for batch 2747 : 0.30265241861343384\n",
      "Training loss for batch 2748 : 0.4615441858768463\n",
      "Training loss for batch 2749 : 0.5091508626937866\n",
      "Training loss for batch 2750 : 0.2271740734577179\n",
      "Training loss for batch 2751 : 0.6223772764205933\n",
      "Training loss for batch 2752 : 0.008764449506998062\n",
      "Training loss for batch 2753 : 0.3855690062046051\n",
      "Training loss for batch 2754 : 0.20821413397789001\n",
      "Training loss for batch 2755 : 0.15885601937770844\n",
      "Training loss for batch 2756 : 0.03029891476035118\n",
      "Training loss for batch 2757 : 0.11874417960643768\n",
      "Training loss for batch 2758 : 0.06059371307492256\n",
      "Training loss for batch 2759 : 0.12914970517158508\n",
      "Training loss for batch 2760 : 0.3782627284526825\n",
      "Training loss for batch 2761 : 0.6482293605804443\n",
      "Training loss for batch 2762 : 0.3508550524711609\n",
      "Training loss for batch 2763 : 0.12916521728038788\n",
      "Training loss for batch 2764 : 0.49141356348991394\n",
      "Training loss for batch 2765 : 0.683398962020874\n",
      "Training loss for batch 2766 : 0.32579004764556885\n",
      "Training loss for batch 2767 : 0.2430129051208496\n",
      "Training loss for batch 2768 : 0.12181513011455536\n",
      "Training loss for batch 2769 : 0.448077917098999\n",
      "Training loss for batch 2770 : 0.27425891160964966\n",
      "Training loss for batch 2771 : 0.020650915801525116\n",
      "Training loss for batch 2772 : 0.402771532535553\n",
      "Training loss for batch 2773 : 0.07450750470161438\n",
      "Training loss for batch 2774 : 0.011815548874437809\n",
      "Training loss for batch 2775 : 0.20648835599422455\n",
      "Training loss for batch 2776 : 0.12431376427412033\n",
      "Training loss for batch 2777 : 0.06752769649028778\n",
      "Training loss for batch 2778 : 0.05707891657948494\n",
      "Training loss for batch 2779 : 0.7295894622802734\n",
      "Training loss for batch 2780 : 0.027675971388816833\n",
      "Training loss for batch 2781 : 0.3901935815811157\n",
      "Training loss for batch 2782 : 0.26235389709472656\n",
      "Training loss for batch 2783 : 0.2467646449804306\n",
      "Training loss for batch 2784 : 1.1756020784378052\n",
      "Training loss for batch 2785 : 0.1187288910150528\n",
      "Training loss for batch 2786 : 0.048463884741067886\n",
      "Training loss for batch 2787 : 0.203207328915596\n",
      "Training loss for batch 2788 : 0.1923265904188156\n",
      "Training loss for batch 2789 : 0.06891629844903946\n",
      "Training loss for batch 2790 : 0.32733142375946045\n",
      "Training loss for batch 2791 : 0.17817756533622742\n",
      "Training loss for batch 2792 : 0.27015554904937744\n",
      "Training loss for batch 2793 : 0.4428596496582031\n",
      "Training loss for batch 2794 : 0.4059920310974121\n",
      "Training loss for batch 2795 : 0.15432317554950714\n",
      "Training loss for batch 2796 : 0.363233357667923\n",
      "Training loss for batch 2797 : 0.28909510374069214\n",
      "Training loss for batch 2798 : 0.27333512902259827\n",
      "Training loss for batch 2799 : 0.5072351098060608\n",
      "Training loss for batch 2800 : 0.2205273061990738\n",
      "Training loss for batch 2801 : 0.43110790848731995\n",
      "Training loss for batch 2802 : 0.18592290580272675\n",
      "Training loss for batch 2803 : 0.4109574258327484\n",
      "Training loss for batch 2804 : 0.21317262947559357\n",
      "Training loss for batch 2805 : 0.06993372738361359\n",
      "Training loss for batch 2806 : 0.31651178002357483\n",
      "Training loss for batch 2807 : 0.5472452640533447\n",
      "Training loss for batch 2808 : 0.05128627270460129\n",
      "Training loss for batch 2809 : 0.3667755424976349\n",
      "Training loss for batch 2810 : 0.351858526468277\n",
      "Training loss for batch 2811 : 0.5841138958930969\n",
      "Training loss for batch 2812 : 0.6868808269500732\n",
      "Training loss for batch 2813 : 0.00403188681229949\n",
      "Training loss for batch 2814 : 0.08423321694135666\n",
      "Training loss for batch 2815 : 0.2750950753688812\n",
      "Training loss for batch 2816 : 0.3686814606189728\n",
      "Training loss for batch 2817 : 0.044947098940610886\n",
      "Training loss for batch 2818 : 0.28849324584007263\n",
      "Training loss for batch 2819 : 0.10121852159500122\n",
      "Training loss for batch 2820 : 0.11236721277236938\n",
      "Training loss for batch 2821 : 0.05874596908688545\n",
      "Training loss for batch 2822 : 0.3172147870063782\n",
      "Training loss for batch 2823 : 0.3837781548500061\n",
      "Training loss for batch 2824 : 0.2945011556148529\n",
      "Training loss for batch 2825 : 0.2939169108867645\n",
      "Training loss for batch 2826 : 0.17283622920513153\n",
      "Training loss for batch 2827 : 5.185604095458984e-06\n",
      "Training loss for batch 2828 : 0.4037494361400604\n",
      "Training loss for batch 2829 : 0.21419529616832733\n",
      "Training loss for batch 2830 : 0.3354995846748352\n",
      "Training loss for batch 2831 : 0.5310176610946655\n",
      "Training loss for batch 2832 : 0.19202303886413574\n",
      "Training loss for batch 2833 : 0.32051366567611694\n",
      "Training loss for batch 2834 : 0.3303421139717102\n",
      "Training loss for batch 2835 : 0.14842630922794342\n",
      "Training loss for batch 2836 : 0.2297283262014389\n",
      "Training loss for batch 2837 : 0.5619983077049255\n",
      "Training loss for batch 2838 : 0.3887760639190674\n",
      "Training loss for batch 2839 : 0.048306696116924286\n",
      "Training loss for batch 2840 : 0.38451826572418213\n",
      "Training loss for batch 2841 : 0.6193269491195679\n",
      "Training loss for batch 2842 : 0.22312557697296143\n",
      "Training loss for batch 2843 : 0.3056972026824951\n",
      "Training loss for batch 2844 : 0.3652685582637787\n",
      "Training loss for batch 2845 : 0.10169604420661926\n",
      "Training loss for batch 2846 : 0.11195404082536697\n",
      "Training loss for batch 2847 : 0.4627320170402527\n",
      "Training loss for batch 2848 : 0.32919421792030334\n",
      "Training loss for batch 2849 : 0.2219642847776413\n",
      "Training loss for batch 2850 : 0.07459864765405655\n",
      "Training loss for batch 2851 : 0.712192952632904\n",
      "Training loss for batch 2852 : 0.2025078982114792\n",
      "Training loss for batch 2853 : 0.37508517503738403\n",
      "Training loss for batch 2854 : 0.2097252607345581\n",
      "Training loss for batch 2855 : 0.14677949249744415\n",
      "Training loss for batch 2856 : 0.00034610432339832187\n",
      "Training loss for batch 2857 : 0.24177195131778717\n",
      "Training loss for batch 2858 : 0.07207126170396805\n",
      "Training loss for batch 2859 : 0.03205082565546036\n",
      "Training loss for batch 2860 : 0.5643736124038696\n",
      "Training loss for batch 2861 : 0.43654507398605347\n",
      "Training loss for batch 2862 : 0.6525686383247375\n",
      "Training loss for batch 2863 : 0.06062038242816925\n",
      "Training loss for batch 2864 : 0.28533968329429626\n",
      "Training loss for batch 2865 : 0.09003447741270065\n",
      "Training loss for batch 2866 : 0.5307909846305847\n",
      "Training loss for batch 2867 : 0.1445043981075287\n",
      "Training loss for batch 2868 : 0.19435304403305054\n",
      "Training loss for batch 2869 : 0.24453605711460114\n",
      "Training loss for batch 2870 : 0.347345769405365\n",
      "Training loss for batch 2871 : 0.19873002171516418\n",
      "Training loss for batch 2872 : 0.3015271723270416\n",
      "Training loss for batch 2873 : 0.3416992127895355\n",
      "Training loss for batch 2874 : 0.20930306613445282\n",
      "Training loss for batch 2875 : 0.4766824543476105\n",
      "Training loss for batch 2876 : 0.07517434656620026\n",
      "Training loss for batch 2877 : 0.01332390308380127\n",
      "Training loss for batch 2878 : 0.3947976231575012\n",
      "Training loss for batch 2879 : 0.3913710117340088\n",
      "Training loss for batch 2880 : 0.13363392651081085\n",
      "Training loss for batch 2881 : 0.08196574449539185\n",
      "Training loss for batch 2882 : 0.052660420536994934\n",
      "Training loss for batch 2883 : 0.09220369160175323\n",
      "Training loss for batch 2884 : 0.05535319447517395\n",
      "Training loss for batch 2885 : 0.21755996346473694\n",
      "Training loss for batch 2886 : 0.2282841056585312\n",
      "Training loss for batch 2887 : 0.3101212680339813\n",
      "Training loss for batch 2888 : 0.14916947484016418\n",
      "Training loss for batch 2889 : 0.6359419822692871\n",
      "Training loss for batch 2890 : 0.148578479886055\n",
      "Training loss for batch 2891 : 0.16445204615592957\n",
      "Training loss for batch 2892 : 0.3777942955493927\n",
      "Training loss for batch 2893 : 0.20305249094963074\n",
      "Training loss for batch 2894 : 0.09329263120889664\n",
      "Training loss for batch 2895 : 0.05292084068059921\n",
      "Training loss for batch 2896 : 0.26831331849098206\n",
      "Training loss for batch 2897 : 0.19539688527584076\n",
      "Training loss for batch 2898 : 0.20663084089756012\n",
      "Training loss for batch 2899 : 0.6137009263038635\n",
      "Training loss for batch 2900 : 0.5298237800598145\n",
      "Training loss for batch 2901 : 0.255465030670166\n",
      "Training loss for batch 2902 : 0.060515448451042175\n",
      "Training loss for batch 2903 : 0.07381166517734528\n",
      "Training loss for batch 2904 : 0.155492901802063\n",
      "Training loss for batch 2905 : 0.38370850682258606\n",
      "Training loss for batch 2906 : 0.34119051694869995\n",
      "Training loss for batch 2907 : 0.7186546921730042\n",
      "Training loss for batch 2908 : 0.35038870573043823\n",
      "Training loss for batch 2909 : 0.34960758686065674\n",
      "Training loss for batch 2910 : 0.04966370761394501\n",
      "Training loss for batch 2911 : 0.31883504986763\n",
      "Training loss for batch 2912 : 0.20116406679153442\n",
      "Training loss for batch 2913 : 0.017764469608664513\n",
      "Training loss for batch 2914 : 0.6068477630615234\n",
      "Training loss for batch 2915 : 0.30362531542778015\n",
      "Training loss for batch 2916 : 0.06949316710233688\n",
      "Training loss for batch 2917 : 0.1565244346857071\n",
      "Training loss for batch 2918 : 0.114310123026371\n",
      "Training loss for batch 2919 : 0.4868471026420593\n",
      "Training loss for batch 2920 : 0.3844849765300751\n",
      "Training loss for batch 2921 : 0.3137265145778656\n",
      "Training loss for batch 2922 : 0.17245563864707947\n",
      "Training loss for batch 2923 : 0.4477006196975708\n",
      "Training loss for batch 2924 : 0.11879078298807144\n",
      "Training loss for batch 2925 : 0.6689909100532532\n",
      "Training loss for batch 2926 : 0.07172316312789917\n",
      "Training loss for batch 2927 : 0.0936012789607048\n",
      "Training loss for batch 2928 : 0.2087043970823288\n",
      "Training loss for batch 2929 : 0.08515893667936325\n",
      "Training loss for batch 2930 : 0.020378127694129944\n",
      "Training loss for batch 2931 : 0.1561293751001358\n",
      "Training loss for batch 2932 : 0.49965667724609375\n",
      "Training loss for batch 2933 : 0.4002333879470825\n",
      "Training loss for batch 2934 : 0.19622087478637695\n",
      "Training loss for batch 2935 : 0.5280353426933289\n",
      "Training loss for batch 2936 : 0.3312320113182068\n",
      "Training loss for batch 2937 : 0.5686827301979065\n",
      "Training loss for batch 2938 : 0.09528107196092606\n",
      "Training loss for batch 2939 : 0.029697507619857788\n",
      "Training loss for batch 2940 : 0.17609882354736328\n",
      "Training loss for batch 2941 : 0.11879128962755203\n",
      "Training loss for batch 2942 : 0.22637046873569489\n",
      "Training loss for batch 2943 : 0.04164786636829376\n",
      "Training loss for batch 2944 : 0.42131751775741577\n",
      "Training loss for batch 2945 : 0.057698339223861694\n",
      "Training loss for batch 2946 : 0.08312858641147614\n",
      "Training loss for batch 2947 : 0.005364664830267429\n",
      "Training loss for batch 2948 : 0.052463531494140625\n",
      "Training loss for batch 2949 : 0.27343642711639404\n",
      "Training loss for batch 2950 : 0.15472662448883057\n",
      "Training loss for batch 2951 : 0.4837878346443176\n",
      "Training loss for batch 2952 : 0.2813142240047455\n",
      "Training loss for batch 2953 : 0.45133092999458313\n",
      "Training loss for batch 2954 : 0.03765150532126427\n",
      "Training loss for batch 2955 : 0.10339140146970749\n",
      "Training loss for batch 2956 : 0.230238676071167\n",
      "Training loss for batch 2957 : 0.30723851919174194\n",
      "Training loss for batch 2958 : 0.0710296556353569\n",
      "Training loss for batch 2959 : 0.7446656823158264\n",
      "Training loss for batch 2960 : 0.21633495390415192\n",
      "Training loss for batch 2961 : 0.0021742270328104496\n",
      "Training loss for batch 2962 : 0.47041743993759155\n",
      "Training loss for batch 2963 : 0.1324276626110077\n",
      "Training loss for batch 2964 : 0.0\n",
      "Training loss for batch 2965 : 0.016664834693074226\n",
      "Training loss for batch 2966 : 0.5163732767105103\n",
      "Training loss for batch 2967 : 0.07122408598661423\n",
      "Training loss for batch 2968 : 0.041023146361112595\n",
      "Training loss for batch 2969 : 0.36278581619262695\n",
      "Training loss for batch 2970 : 0.04755043238401413\n",
      "Training loss for batch 2971 : 0.32424071431159973\n",
      "Training loss for batch 2972 : 0.26855698227882385\n",
      "Training loss for batch 2973 : 0.05696472153067589\n",
      "Training loss for batch 2974 : 0.08458983898162842\n",
      "Training loss for batch 2975 : 0.3055076003074646\n",
      "Training loss for batch 2976 : 0.31943559646606445\n",
      "Training loss for batch 2977 : 0.3178214728832245\n",
      "Training loss for batch 2978 : 0.511385977268219\n",
      "Training loss for batch 2979 : 0.017462333664298058\n",
      "Training loss for batch 2980 : 0.03661744296550751\n",
      "Training loss for batch 2981 : 0.10665366053581238\n",
      "Training loss for batch 2982 : 0.43099087476730347\n",
      "Training loss for batch 2983 : 0.4169979393482208\n",
      "Training loss for batch 2984 : 0.08263643085956573\n",
      "Training loss for batch 2985 : 0.19883732497692108\n",
      "Training loss for batch 2986 : 0.04005563259124756\n",
      "Training loss for batch 2987 : 0.2236378937959671\n",
      "Training loss for batch 2988 : 0.2594585418701172\n",
      "Training loss for batch 2989 : 0.49333417415618896\n",
      "Training loss for batch 2990 : 0.5606523156166077\n",
      "Training loss for batch 2991 : 0.11270588636398315\n",
      "Training loss for batch 2992 : 0.27717822790145874\n",
      "Training loss for batch 2993 : 0.20343869924545288\n",
      "Training loss for batch 2994 : 0.030771315097808838\n",
      "Training loss for batch 2995 : 0.316626638174057\n",
      "Training loss for batch 2996 : 0.05854586884379387\n",
      "Training loss for batch 2997 : 0.007826211862266064\n",
      "Training loss for batch 2998 : 0.25245732069015503\n",
      "Training loss for batch 2999 : 0.5019690990447998\n",
      "Training loss for batch 3000 : 0.022003261372447014\n",
      "Training loss for batch 3001 : 0.01643310859799385\n",
      "Training loss for batch 3002 : 0.030938737094402313\n",
      "Training loss for batch 3003 : 0.0891348123550415\n",
      "Training loss for batch 3004 : 0.04814552143216133\n",
      "Training loss for batch 3005 : 0.06653505563735962\n",
      "Training loss for batch 3006 : 0.1275900900363922\n",
      "Training loss for batch 3007 : 0.14424656331539154\n",
      "Training loss for batch 3008 : 0.3589521646499634\n",
      "Training loss for batch 3009 : 0.3037590980529785\n",
      "Training loss for batch 3010 : 0.013447284698486328\n",
      "Training loss for batch 3011 : 0.4245610535144806\n",
      "Training loss for batch 3012 : 0.43793153762817383\n",
      "Training loss for batch 3013 : 0.20274560153484344\n",
      "Training loss for batch 3014 : 0.06523457914590836\n",
      "Training loss for batch 3015 : 0.39723536372184753\n",
      "Training loss for batch 3016 : 0.0\n",
      "Training loss for batch 3017 : 0.3491579592227936\n",
      "Training loss for batch 3018 : 0.11033820360898972\n",
      "Training loss for batch 3019 : 0.010002355091273785\n",
      "Training loss for batch 3020 : 0.04573164880275726\n",
      "Training loss for batch 3021 : 0.17821715772151947\n",
      "Training loss for batch 3022 : 0.5185762643814087\n",
      "Training loss for batch 3023 : 0.5405458807945251\n",
      "Training loss for batch 3024 : 0.08017204701900482\n",
      "Training loss for batch 3025 : 0.5655719637870789\n",
      "Training loss for batch 3026 : 0.08360394090414047\n",
      "Training loss for batch 3027 : 0.046056509017944336\n",
      "Training loss for batch 3028 : 0.25918713212013245\n",
      "Training loss for batch 3029 : 0.16722655296325684\n",
      "Training loss for batch 3030 : 0.33652105927467346\n",
      "Training loss for batch 3031 : 0.7201146483421326\n",
      "Training loss for batch 3032 : 0.7033406496047974\n",
      "Training loss for batch 3033 : 0.03739655017852783\n",
      "Training loss for batch 3034 : 0.3338785469532013\n",
      "Training loss for batch 3035 : 0.18860574066638947\n",
      "Training loss for batch 3036 : 0.1494171917438507\n",
      "Training loss for batch 3037 : 0.18593887984752655\n",
      "Training loss for batch 3038 : 0.0681668072938919\n",
      "Training loss for batch 3039 : 0.05278284102678299\n",
      "Training loss for batch 3040 : 0.13035036623477936\n",
      "Training loss for batch 3041 : 0.020852239802479744\n",
      "Training loss for batch 3042 : 0.39921215176582336\n",
      "Training loss for batch 3043 : 0.24400338530540466\n",
      "Training loss for batch 3044 : 0.10595837235450745\n",
      "Training loss for batch 3045 : 0.5726728439331055\n",
      "Training loss for batch 3046 : 0.28819432854652405\n",
      "Training loss for batch 3047 : 0.21848759055137634\n",
      "Training loss for batch 3048 : 0.0758817195892334\n",
      "Training loss for batch 3049 : 0.10768887400627136\n",
      "Training loss for batch 3050 : 0.1716335117816925\n",
      "Training loss for batch 3051 : 0.41578513383865356\n",
      "Training loss for batch 3052 : 0.5999599695205688\n",
      "Training loss for batch 3053 : 0.046040553599596024\n",
      "Training loss for batch 3054 : 0.058109067380428314\n",
      "Training loss for batch 3055 : 0.019715528935194016\n",
      "Training loss for batch 3056 : 0.051829468458890915\n",
      "Training loss for batch 3057 : 0.04600707069039345\n",
      "Training loss for batch 3058 : 0.31251251697540283\n",
      "Training loss for batch 3059 : 0.00569219421595335\n",
      "Training loss for batch 3060 : 0.224000945687294\n",
      "Training loss for batch 3061 : 0.06786749511957169\n",
      "Training loss for batch 3062 : 0.05025660991668701\n",
      "Training loss for batch 3063 : 0.04030262306332588\n",
      "Training loss for batch 3064 : 0.09177947789430618\n",
      "Training loss for batch 3065 : 0.21337923407554626\n",
      "Training loss for batch 3066 : 0.09488251060247421\n",
      "Training loss for batch 3067 : 0.03287162631750107\n",
      "Training loss for batch 3068 : 0.3641040623188019\n",
      "Training loss for batch 3069 : 0.1210671216249466\n",
      "Training loss for batch 3070 : 0.45452383160591125\n",
      "Training loss for batch 3071 : 0.21057820320129395\n",
      "Training loss for batch 3072 : 0.034110426902770996\n",
      "Training loss for batch 3073 : 0.3472159504890442\n",
      "Training loss for batch 3074 : 0.3528846502304077\n",
      "Training loss for batch 3075 : 0.12614206969738007\n",
      "Training loss for batch 3076 : 0.053061991930007935\n",
      "Training loss for batch 3077 : 0.0\n",
      "Training loss for batch 3078 : 0.08788520097732544\n",
      "Training loss for batch 3079 : 0.4977268874645233\n",
      "Training loss for batch 3080 : 0.4100263714790344\n",
      "Training loss for batch 3081 : 0.5497336983680725\n",
      "Training loss for batch 3082 : 0.582140326499939\n",
      "Training loss for batch 3083 : 0.5128822922706604\n",
      "Training loss for batch 3084 : 0.010905584320425987\n",
      "Training loss for batch 3085 : 0.06938648968935013\n",
      "Training loss for batch 3086 : 0.17800696194171906\n",
      "Training loss for batch 3087 : 0.12998183071613312\n",
      "Training loss for batch 3088 : 1.0170254707336426\n",
      "Training loss for batch 3089 : 0.28529828786849976\n",
      "Training loss for batch 3090 : 0.36080920696258545\n",
      "Training loss for batch 3091 : 0.28408026695251465\n",
      "Training loss for batch 3092 : 0.004146104212850332\n",
      "Training loss for batch 3093 : 0.010109481401741505\n",
      "Training loss for batch 3094 : 0.01011867355555296\n",
      "Training loss for batch 3095 : 0.021511327475309372\n",
      "Training loss for batch 3096 : 0.07898839563131332\n",
      "Training loss for batch 3097 : 0.2632449269294739\n",
      "Training loss for batch 3098 : 0.17217516899108887\n",
      "Training loss for batch 3099 : 0.4838578999042511\n",
      "Training loss for batch 3100 : 0.1814943104982376\n",
      "Training loss for batch 3101 : 0.12931063771247864\n",
      "Training loss for batch 3102 : 0.2639152407646179\n",
      "Training loss for batch 3103 : 0.32761460542678833\n",
      "Training loss for batch 3104 : 0.6929841637611389\n",
      "Training loss for batch 3105 : 0.3586168885231018\n",
      "Training loss for batch 3106 : 0.09902995079755783\n",
      "Training loss for batch 3107 : 0.08460108935832977\n",
      "Training loss for batch 3108 : 0.10615156590938568\n",
      "Training loss for batch 3109 : 0.07953868061304092\n",
      "Training loss for batch 3110 : 0.4789062738418579\n",
      "Training loss for batch 3111 : 0.09179554134607315\n",
      "Training loss for batch 3112 : 0.4379896819591522\n",
      "Training loss for batch 3113 : 0.4099999666213989\n",
      "Training loss for batch 3114 : 0.3682177662849426\n",
      "Training loss for batch 3115 : 0.388005793094635\n",
      "Training loss for batch 3116 : 0.3701014518737793\n",
      "Training loss for batch 3117 : 0.015919135883450508\n",
      "Training loss for batch 3118 : 0.10062594711780548\n",
      "Training loss for batch 3119 : 0.32566002011299133\n",
      "Training loss for batch 3120 : 0.11027557402849197\n",
      "Training loss for batch 3121 : 0.42439165711402893\n",
      "Training loss for batch 3122 : 0.0\n",
      "Training loss for batch 3123 : 0.13228397071361542\n",
      "Training loss for batch 3124 : 0.3279838263988495\n",
      "Training loss for batch 3125 : 0.09736454486846924\n",
      "Training loss for batch 3126 : 0.2951950132846832\n",
      "Training loss for batch 3127 : 0.3667420744895935\n",
      "Training loss for batch 3128 : 0.2548186779022217\n",
      "Training loss for batch 3129 : 0.23032651841640472\n",
      "Training loss for batch 3130 : 0.3535885214805603\n",
      "Training loss for batch 3131 : 0.29842299222946167\n",
      "Training loss for batch 3132 : 0.3317328989505768\n",
      "Training loss for batch 3133 : 0.2802763283252716\n",
      "Training loss for batch 3134 : 0.18434983491897583\n",
      "Training loss for batch 3135 : 0.15050075948238373\n",
      "Training loss for batch 3136 : 0.42385104298591614\n",
      "Training loss for batch 3137 : 0.09888432919979095\n",
      "Training loss for batch 3138 : 0.2843128740787506\n",
      "Training loss for batch 3139 : 0.36202922463417053\n",
      "Training loss for batch 3140 : 0.12007296830415726\n",
      "Training loss for batch 3141 : 0.1826080083847046\n",
      "Training loss for batch 3142 : 0.47505250573158264\n",
      "Training loss for batch 3143 : 0.24350136518478394\n",
      "Training loss for batch 3144 : 0.11884942650794983\n",
      "Training loss for batch 3145 : 0.4007633626461029\n",
      "Training loss for batch 3146 : 0.6143638491630554\n",
      "Training loss for batch 3147 : 0.4633985161781311\n",
      "Training loss for batch 3148 : 0.5581385493278503\n",
      "Training loss for batch 3149 : 0.2817736864089966\n",
      "Training loss for batch 3150 : 0.036795713007450104\n",
      "Training loss for batch 3151 : 0.09708485752344131\n",
      "Training loss for batch 3152 : 0.22172203660011292\n",
      "Training loss for batch 3153 : 0.00670623779296875\n",
      "Training loss for batch 3154 : 0.6531985998153687\n",
      "Training loss for batch 3155 : 0.5693281292915344\n",
      "Training loss for batch 3156 : 0.0175054669380188\n",
      "Training loss for batch 3157 : 0.2554280459880829\n",
      "Training loss for batch 3158 : 0.3888416588306427\n",
      "Training loss for batch 3159 : 0.2744952440261841\n",
      "Training loss for batch 3160 : 0.027341559529304504\n",
      "Training loss for batch 3161 : 0.9175854921340942\n",
      "Training loss for batch 3162 : 0.09725558757781982\n",
      "Training loss for batch 3163 : 0.33089274168014526\n",
      "Training loss for batch 3164 : 0.1470949351787567\n",
      "Training loss for batch 3165 : 0.23178240656852722\n",
      "Training loss for batch 3166 : 0.06378912180662155\n",
      "Training loss for batch 3167 : 0.2819458246231079\n",
      "Training loss for batch 3168 : 0.21839869022369385\n",
      "Training loss for batch 3169 : 0.30256402492523193\n",
      "Training loss for batch 3170 : 0.4345857799053192\n",
      "Training loss for batch 3171 : 0.5579902529716492\n",
      "Training loss for batch 3172 : 0.4079141914844513\n",
      "Training loss for batch 3173 : 0.06274599581956863\n",
      "Training loss for batch 3174 : 0.5196132659912109\n",
      "Training loss for batch 3175 : 0.19469216465950012\n",
      "Training loss for batch 3176 : 0.2593752145767212\n",
      "Training loss for batch 3177 : 0.37043824791908264\n",
      "Training loss for batch 3178 : 0.502162754535675\n",
      "Training loss for batch 3179 : 0.1412207931280136\n",
      "Training loss for batch 3180 : 0.2664390206336975\n",
      "Training loss for batch 3181 : 0.13872361183166504\n",
      "Training loss for batch 3182 : 0.138518288731575\n",
      "Training loss for batch 3183 : 0.029423274099826813\n",
      "Training loss for batch 3184 : 0.137369304895401\n",
      "Training loss for batch 3185 : 0.3305850923061371\n",
      "Training loss for batch 3186 : 0.11316581070423126\n",
      "Training loss for batch 3187 : 0.17135988175868988\n",
      "Training loss for batch 3188 : 0.1114879623055458\n",
      "Training loss for batch 3189 : 0.20110675692558289\n",
      "Training loss for batch 3190 : 0.40508922934532166\n",
      "Training loss for batch 3191 : 0.28072354197502136\n",
      "Training loss for batch 3192 : 0.10490739345550537\n",
      "Training loss for batch 3193 : 0.21902646124362946\n",
      "Training loss for batch 3194 : 0.24672293663024902\n",
      "Training loss for batch 3195 : 0.08526727557182312\n",
      "Training loss for batch 3196 : 0.13876807689666748\n",
      "Training loss for batch 3197 : 0.04046203941106796\n",
      "Training loss for batch 3198 : 0.0\n",
      "Training loss for batch 3199 : 0.1775323748588562\n",
      "Training loss for batch 3200 : 0.0006695538759231567\n",
      "Training loss for batch 3201 : 0.01800895668566227\n",
      "Training loss for batch 3202 : 0.21620826423168182\n",
      "Training loss for batch 3203 : 0.5658843517303467\n",
      "Training loss for batch 3204 : 0.30966970324516296\n",
      "Training loss for batch 3205 : 0.20117312669754028\n",
      "Training loss for batch 3206 : 0.3239220678806305\n",
      "Training loss for batch 3207 : 0.28194570541381836\n",
      "Training loss for batch 3208 : 0.2769467830657959\n",
      "Training loss for batch 3209 : 0.07199189811944962\n",
      "Training loss for batch 3210 : 0.0011092297499999404\n",
      "Training loss for batch 3211 : 0.41054606437683105\n",
      "Training loss for batch 3212 : 0.17875778675079346\n",
      "Training loss for batch 3213 : 0.33418557047843933\n",
      "Training loss for batch 3214 : 0.005831678863614798\n",
      "Training loss for batch 3215 : 0.07944179326295853\n",
      "Training loss for batch 3216 : 0.35924580693244934\n",
      "Training loss for batch 3217 : 0.10301928222179413\n",
      "Training loss for batch 3218 : 0.2036847323179245\n",
      "Training loss for batch 3219 : 0.11540604382753372\n",
      "Training loss for batch 3220 : 0.17547845840454102\n",
      "Training loss for batch 3221 : 0.007233654614537954\n",
      "Training loss for batch 3222 : 0.2593730092048645\n",
      "Training loss for batch 3223 : 0.12730248272418976\n",
      "Training loss for batch 3224 : 0.5163211822509766\n",
      "Training loss for batch 3225 : 0.2125447392463684\n",
      "Training loss for batch 3226 : 0.1928601861000061\n",
      "Training loss for batch 3227 : 0.13666099309921265\n",
      "Training loss for batch 3228 : 0.31563660502433777\n",
      "Training loss for batch 3229 : 0.5277945399284363\n",
      "Training loss for batch 3230 : 0.19901259243488312\n",
      "Training loss for batch 3231 : 0.10200472176074982\n",
      "Training loss for batch 3232 : 0.33006951212882996\n",
      "Training loss for batch 3233 : 0.282137393951416\n",
      "Training loss for batch 3234 : 0.5194658637046814\n",
      "Training loss for batch 3235 : 1.1503241062164307\n",
      "Training loss for batch 3236 : 0.1831974983215332\n",
      "Training loss for batch 3237 : 0.10696810483932495\n",
      "Training loss for batch 3238 : 0.12124872952699661\n",
      "Training loss for batch 3239 : 0.18256497383117676\n",
      "Training loss for batch 3240 : 0.03967123478651047\n",
      "Training loss for batch 3241 : 0.18829625844955444\n",
      "Training loss for batch 3242 : 0.13952377438545227\n",
      "Training loss for batch 3243 : 0.14519663155078888\n",
      "Training loss for batch 3244 : 0.07252585142850876\n",
      "Training loss for batch 3245 : 0.19820529222488403\n",
      "Training loss for batch 3246 : 0.4831995368003845\n",
      "Training loss for batch 3247 : 0.0636260062456131\n",
      "Training loss for batch 3248 : 0.7270234823226929\n",
      "Training loss for batch 3249 : 0.3150990307331085\n",
      "Training loss for batch 3250 : 0.0517040491104126\n",
      "Training loss for batch 3251 : 0.28410425782203674\n",
      "Training loss for batch 3252 : 0.08153355866670609\n",
      "Training loss for batch 3253 : 0.27834242582321167\n",
      "Training loss for batch 3254 : 0.17320173978805542\n",
      "Training loss for batch 3255 : 0.29534029960632324\n",
      "Training loss for batch 3256 : 0.16008807718753815\n",
      "Training loss for batch 3257 : 0.07473032921552658\n",
      "Training loss for batch 3258 : 0.3518878221511841\n",
      "Training loss for batch 3259 : 0.0983627662062645\n",
      "Training loss for batch 3260 : 0.11206427961587906\n",
      "Training loss for batch 3261 : 0.0\n",
      "Training loss for batch 3262 : 0.1373271644115448\n",
      "Training loss for batch 3263 : 0.5319600105285645\n",
      "Training loss for batch 3264 : 0.34030404686927795\n",
      "Training loss for batch 3265 : 0.08555740118026733\n",
      "Training loss for batch 3266 : 0.6186346411705017\n",
      "Training loss for batch 3267 : 0.15916621685028076\n",
      "Training loss for batch 3268 : 0.2547488808631897\n",
      "Training loss for batch 3269 : 0.1283351480960846\n",
      "Training loss for batch 3270 : 0.1507571041584015\n",
      "Training loss for batch 3271 : 0.16688179969787598\n",
      "Training loss for batch 3272 : 0.07461807131767273\n",
      "Training loss for batch 3273 : 0.61622154712677\n",
      "Training loss for batch 3274 : 0.04806092754006386\n",
      "Training loss for batch 3275 : 0.12455595284700394\n",
      "Training loss for batch 3276 : 0.2525012493133545\n",
      "Training loss for batch 3277 : 0.32560133934020996\n",
      "Training loss for batch 3278 : 0.25457820296287537\n",
      "Training loss for batch 3279 : 0.1175803542137146\n",
      "Training loss for batch 3280 : 0.49305081367492676\n",
      "Training loss for batch 3281 : 0.8450856804847717\n",
      "Training loss for batch 3282 : 0.017491918057203293\n",
      "Training loss for batch 3283 : 0.3587731122970581\n",
      "Training loss for batch 3284 : 0.2957405149936676\n",
      "Training loss for batch 3285 : 0.07749190181493759\n",
      "Training loss for batch 3286 : 0.09658023715019226\n",
      "Training loss for batch 3287 : 0.2511443495750427\n",
      "Training loss for batch 3288 : 0.3890789747238159\n",
      "Training loss for batch 3289 : 0.6113013625144958\n",
      "Training loss for batch 3290 : 0.12220682948827744\n",
      "Training loss for batch 3291 : 0.18151529133319855\n",
      "Training loss for batch 3292 : 0.04854496568441391\n",
      "Training loss for batch 3293 : 0.044640060514211655\n",
      "Training loss for batch 3294 : 0.05478082224726677\n",
      "Training loss for batch 3295 : 0.25970470905303955\n",
      "Training loss for batch 3296 : 0.12019383907318115\n",
      "Training loss for batch 3297 : 0.5746715068817139\n",
      "Training loss for batch 3298 : 0.2787523567676544\n",
      "Training loss for batch 3299 : 0.12370668351650238\n",
      "Training loss for batch 3300 : 0.11783664673566818\n",
      "Training loss for batch 3301 : 0.2134973257780075\n",
      "Training loss for batch 3302 : 0.0028068427927792072\n",
      "Training loss for batch 3303 : 0.39632126688957214\n",
      "Training loss for batch 3304 : 0.21210744976997375\n",
      "Training loss for batch 3305 : 0.6609143018722534\n",
      "Training loss for batch 3306 : 0.45974040031433105\n",
      "Training loss for batch 3307 : 0.19144144654273987\n",
      "Training loss for batch 3308 : 0.18155105412006378\n",
      "Training loss for batch 3309 : 0.22803133726119995\n",
      "Training loss for batch 3310 : 0.26289522647857666\n",
      "Training loss for batch 3311 : 0.5011382699012756\n",
      "Training loss for batch 3312 : 0.23447757959365845\n",
      "Training loss for batch 3313 : 0.12744960188865662\n",
      "Training loss for batch 3314 : 0.026890939101576805\n",
      "Training loss for batch 3315 : 0.4186951518058777\n",
      "Training loss for batch 3316 : 0.35895881056785583\n",
      "Training loss for batch 3317 : 0.10227149724960327\n",
      "Training loss for batch 3318 : 0.37576261162757874\n",
      "Training loss for batch 3319 : 0.32032835483551025\n",
      "Training loss for batch 3320 : 0.21938320994377136\n",
      "Training loss for batch 3321 : 0.30861225724220276\n",
      "Training loss for batch 3322 : 0.03547879308462143\n",
      "Training loss for batch 3323 : 0.2143821269273758\n",
      "Training loss for batch 3324 : 0.30378785729408264\n",
      "Training loss for batch 3325 : 0.06468570232391357\n",
      "Training loss for batch 3326 : 0.316660612821579\n",
      "Training loss for batch 3327 : 0.11971796303987503\n",
      "Training loss for batch 3328 : 0.37221217155456543\n",
      "Training loss for batch 3329 : 0.5360503792762756\n",
      "Training loss for batch 3330 : 0.1901693046092987\n",
      "Training loss for batch 3331 : 0.15000523626804352\n",
      "Training loss for batch 3332 : 0.29328060150146484\n",
      "Training loss for batch 3333 : 0.08257712423801422\n",
      "Training loss for batch 3334 : 0.36168983578681946\n",
      "Training loss for batch 3335 : 0.2559818923473358\n",
      "Training loss for batch 3336 : 0.10629351437091827\n",
      "Training loss for batch 3337 : 0.10759691148996353\n",
      "Training loss for batch 3338 : 0.2676188051700592\n",
      "Training loss for batch 3339 : 0.06356750428676605\n",
      "Training loss for batch 3340 : 0.1758924275636673\n",
      "Training loss for batch 3341 : 0.0984594002366066\n",
      "Training loss for batch 3342 : 0.1326695829629898\n",
      "Training loss for batch 3343 : 0.2428959608078003\n",
      "Training loss for batch 3344 : 0.4679414629936218\n",
      "Training loss for batch 3345 : 0.2913309931755066\n",
      "Training loss for batch 3346 : 0.4183362126350403\n",
      "Training loss for batch 3347 : 0.31515416502952576\n",
      "Training loss for batch 3348 : 0.1122775599360466\n",
      "Training loss for batch 3349 : 0.27161097526550293\n",
      "Training loss for batch 3350 : 0.03497691452503204\n",
      "Training loss for batch 3351 : 0.1380477249622345\n",
      "Training loss for batch 3352 : 0.2916583716869354\n",
      "Training loss for batch 3353 : 0.2346542626619339\n",
      "Training loss for batch 3354 : 0.12269435822963715\n",
      "Training loss for batch 3355 : 0.7763605713844299\n",
      "Training loss for batch 3356 : 0.3650994598865509\n",
      "Training loss for batch 3357 : 0.06148989871144295\n",
      "Training loss for batch 3358 : 0.4557034373283386\n",
      "Training loss for batch 3359 : 0.33556702733039856\n",
      "Training loss for batch 3360 : 0.051751382648944855\n",
      "Training loss for batch 3361 : 0.17404451966285706\n",
      "Training loss for batch 3362 : 0.27071109414100647\n",
      "Training loss for batch 3363 : 0.038513585925102234\n",
      "Training loss for batch 3364 : 0.1597025990486145\n",
      "Training loss for batch 3365 : 0.1110612004995346\n",
      "Training loss for batch 3366 : 0.02427242323756218\n",
      "Training loss for batch 3367 : 0.5010352730751038\n",
      "Training loss for batch 3368 : 0.5548473596572876\n",
      "Training loss for batch 3369 : 0.24751295149326324\n",
      "Training loss for batch 3370 : 0.5158190727233887\n",
      "Training loss for batch 3371 : 0.3617919981479645\n",
      "Training loss for batch 3372 : 0.18094870448112488\n",
      "Training loss for batch 3373 : 0.30690744519233704\n",
      "Training loss for batch 3374 : 0.19757136702537537\n",
      "Training loss for batch 3375 : 0.07975377142429352\n",
      "Training loss for batch 3376 : 0.0529782734811306\n",
      "Training loss for batch 3377 : 0.2756107449531555\n",
      "Training loss for batch 3378 : 0.2531217038631439\n",
      "Training loss for batch 3379 : 0.3573732078075409\n",
      "Training loss for batch 3380 : 0.1626463234424591\n",
      "Training loss for batch 3381 : 0.4146564304828644\n",
      "Training loss for batch 3382 : 0.6885612607002258\n",
      "Training loss for batch 3383 : 0.3327982425689697\n",
      "Training loss for batch 3384 : 0.5557480454444885\n",
      "Training loss for batch 3385 : 0.28063008189201355\n",
      "Training loss for batch 3386 : 0.37752264738082886\n",
      "Training loss for batch 3387 : 0.4423842430114746\n",
      "Training loss for batch 3388 : 0.16838489472866058\n",
      "Training loss for batch 3389 : 0.5535621047019958\n",
      "Training loss for batch 3390 : 0.02312711626291275\n",
      "Training loss for batch 3391 : 0.021934976801276207\n",
      "Training loss for batch 3392 : 0.05546418949961662\n",
      "Training loss for batch 3393 : 0.029204120859503746\n",
      "Training loss for batch 3394 : 0.09171247482299805\n",
      "Training loss for batch 3395 : 0.15051120519638062\n",
      "Training loss for batch 3396 : 0.36503830552101135\n",
      "Training loss for batch 3397 : 0.04838388413190842\n",
      "Training loss for batch 3398 : 0.3667621612548828\n",
      "Training loss for batch 3399 : 0.4111267030239105\n",
      "Training loss for batch 3400 : 0.14147408306598663\n",
      "Training loss for batch 3401 : 0.8580797910690308\n",
      "Training loss for batch 3402 : 0.2027798444032669\n",
      "Training loss for batch 3403 : 0.04424716904759407\n",
      "Training loss for batch 3404 : 0.007711181882768869\n",
      "Training loss for batch 3405 : 0.2004884034395218\n",
      "Training loss for batch 3406 : 0.034496769309043884\n",
      "Training loss for batch 3407 : 0.27090075612068176\n",
      "Training loss for batch 3408 : 0.021454710513353348\n",
      "Training loss for batch 3409 : 0.25691187381744385\n",
      "Training loss for batch 3410 : 0.7479501366615295\n",
      "Training loss for batch 3411 : 0.4774836301803589\n",
      "Training loss for batch 3412 : 0.558361828327179\n",
      "Training loss for batch 3413 : 0.1927216649055481\n",
      "Training loss for batch 3414 : 0.5203993916511536\n",
      "Training loss for batch 3415 : 0.19292262196540833\n",
      "Training loss for batch 3416 : 0.9597219824790955\n",
      "Training loss for batch 3417 : 0.16253730654716492\n",
      "Training loss for batch 3418 : 0.12339688092470169\n",
      "Training loss for batch 3419 : 0.06227627396583557\n",
      "Training loss for batch 3420 : 0.13547839224338531\n",
      "Training loss for batch 3421 : 0.15044885873794556\n",
      "Training loss for batch 3422 : 0.8606277704238892\n",
      "Training loss for batch 3423 : 0.07810255885124207\n",
      "Training loss for batch 3424 : 0.38991203904151917\n",
      "Training loss for batch 3425 : 0.30839526653289795\n",
      "Training loss for batch 3426 : 0.29544395208358765\n",
      "Training loss for batch 3427 : 0.08970861881971359\n",
      "Training loss for batch 3428 : 0.22007639706134796\n",
      "Training loss for batch 3429 : 0.23998810350894928\n",
      "Training loss for batch 3430 : 0.6458367705345154\n",
      "Training loss for batch 3431 : 0.30139225721359253\n",
      "Training loss for batch 3432 : 0.46887558698654175\n",
      "Training loss for batch 3433 : 0.33783861994743347\n",
      "Training loss for batch 3434 : 0.2822403013706207\n",
      "Training loss for batch 3435 : 0.4591920077800751\n",
      "Training loss for batch 3436 : 0.33853915333747864\n",
      "Training loss for batch 3437 : 0.008005688898265362\n",
      "Training loss for batch 3438 : 0.5841030478477478\n",
      "Training loss for batch 3439 : 0.4741993248462677\n",
      "Training loss for batch 3440 : 0.03498359024524689\n",
      "Training loss for batch 3441 : 0.3749443292617798\n",
      "Training loss for batch 3442 : 0.0534980371594429\n",
      "Training loss for batch 3443 : 0.23355187475681305\n",
      "Training loss for batch 3444 : 0.29697197675704956\n",
      "Training loss for batch 3445 : 0.7536266446113586\n",
      "Training loss for batch 3446 : 0.17149604856967926\n",
      "Training loss for batch 3447 : 0.1650894731283188\n",
      "Training loss for batch 3448 : 0.16470970213413239\n",
      "Training loss for batch 3449 : 0.19266270101070404\n",
      "Training loss for batch 3450 : 0.0966373085975647\n",
      "Training loss for batch 3451 : 0.5206794142723083\n",
      "Training loss for batch 3452 : 0.20739176869392395\n",
      "Training loss for batch 3453 : 0.06239807605743408\n",
      "Training loss for batch 3454 : 0.25659075379371643\n",
      "Training loss for batch 3455 : 0.09998387098312378\n",
      "Training loss for batch 3456 : 0.0889187604188919\n",
      "Training loss for batch 3457 : 0.19160135090351105\n",
      "Training loss for batch 3458 : 0.3223365247249603\n",
      "Training loss for batch 3459 : 0.10075728595256805\n",
      "Training loss for batch 3460 : 0.0909402072429657\n",
      "Training loss for batch 3461 : 0.14403270184993744\n",
      "Training loss for batch 3462 : 0.3197510242462158\n",
      "Training loss for batch 3463 : 0.04565903916954994\n",
      "Training loss for batch 3464 : 0.3763435482978821\n",
      "Training loss for batch 3465 : 0.17836226522922516\n",
      "Training loss for batch 3466 : 0.03907553851604462\n",
      "Training loss for batch 3467 : 0.3136911988258362\n",
      "Training loss for batch 3468 : 0.08063357323408127\n",
      "Training loss for batch 3469 : 0.33155280351638794\n",
      "Training loss for batch 3470 : 0.2565048933029175\n",
      "Training loss for batch 3471 : 0.0007619529496878386\n",
      "Training loss for batch 3472 : 0.19421164691448212\n",
      "Training loss for batch 3473 : 0.02067941427230835\n",
      "Training loss for batch 3474 : 0.22163507342338562\n",
      "Training loss for batch 3475 : 0.4992621839046478\n",
      "Training loss for batch 3476 : 0.08846013247966766\n",
      "Training loss for batch 3477 : 0.1153760626912117\n",
      "Training loss for batch 3478 : 0.2882221043109894\n",
      "Training loss for batch 3479 : 0.30917322635650635\n",
      "Training loss for batch 3480 : 0.31048351526260376\n",
      "Training loss for batch 3481 : 0.495169997215271\n",
      "Training loss for batch 3482 : 0.1075938269495964\n",
      "Training loss for batch 3483 : 0.09601539373397827\n",
      "Training loss for batch 3484 : 0.4776765704154968\n",
      "Training loss for batch 3485 : 0.25946545600891113\n",
      "Training loss for batch 3486 : 0.2542174756526947\n",
      "Training loss for batch 3487 : 0.008793562650680542\n",
      "Training loss for batch 3488 : 0.12745755910873413\n",
      "Training loss for batch 3489 : 0.3484371602535248\n",
      "Training loss for batch 3490 : 0.6640459299087524\n",
      "Training loss for batch 3491 : 0.025899771600961685\n",
      "Training loss for batch 3492 : 0.08021973818540573\n",
      "Training loss for batch 3493 : 0.030496444553136826\n",
      "Training loss for batch 3494 : 0.2586078643798828\n",
      "Training loss for batch 3495 : 0.12195076793432236\n",
      "Training loss for batch 3496 : 0.13896459341049194\n",
      "Training loss for batch 3497 : 0.26604023575782776\n",
      "Training loss for batch 3498 : 0.33037179708480835\n",
      "Training loss for batch 3499 : 0.4454234540462494\n",
      "Training loss for batch 3500 : 0.32601451873779297\n",
      "Training loss for batch 3501 : 0.022033846005797386\n",
      "Training loss for batch 3502 : 0.1494835913181305\n",
      "Training loss for batch 3503 : 0.08689800649881363\n",
      "Training loss for batch 3504 : 0.05954306572675705\n",
      "Training loss for batch 3505 : 0.25312817096710205\n",
      "Training loss for batch 3506 : 0.1409720480442047\n",
      "Training loss for batch 3507 : 0.14335228502750397\n",
      "Training loss for batch 3508 : 0.40300223231315613\n",
      "Training loss for batch 3509 : 0.6314088106155396\n",
      "Training loss for batch 3510 : 0.2551771402359009\n",
      "Training loss for batch 3511 : 0.20951779186725616\n",
      "Training loss for batch 3512 : 0.5865283012390137\n",
      "Training loss for batch 3513 : 0.24610017240047455\n",
      "Training loss for batch 3514 : 0.013858919031918049\n",
      "Training loss for batch 3515 : 0.30438199639320374\n",
      "Training loss for batch 3516 : 0.11615116894245148\n",
      "Training loss for batch 3517 : 0.5691957473754883\n",
      "Training loss for batch 3518 : 0.40495216846466064\n",
      "Training loss for batch 3519 : 0.2867467403411865\n",
      "Training loss for batch 3520 : 0.12160873413085938\n",
      "Training loss for batch 3521 : 0.05551516264677048\n",
      "Training loss for batch 3522 : 0.11394084990024567\n",
      "Training loss for batch 3523 : 0.21862490475177765\n",
      "Training loss for batch 3524 : 0.13569480180740356\n",
      "Training loss for batch 3525 : 0.4244408905506134\n",
      "Training loss for batch 3526 : 0.6192072629928589\n",
      "Training loss for batch 3527 : 0.5606770515441895\n",
      "Training loss for batch 3528 : 0.323228657245636\n",
      "Training loss for batch 3529 : 0.35393792390823364\n",
      "Training loss for batch 3530 : 0.3053584396839142\n",
      "Training loss for batch 3531 : 0.16968581080436707\n",
      "Training loss for batch 3532 : 0.22786182165145874\n",
      "Training loss for batch 3533 : 0.3917904198169708\n",
      "Training loss for batch 3534 : 0.200111985206604\n",
      "Training loss for batch 3535 : 0.276093065738678\n",
      "Training loss for batch 3536 : 0.23461437225341797\n",
      "Training loss for batch 3537 : 0.3103139400482178\n",
      "Training loss for batch 3538 : 0.33465537428855896\n",
      "Training loss for batch 3539 : 0.14482493698596954\n",
      "Training loss for batch 3540 : 0.6154825687408447\n",
      "Training loss for batch 3541 : 0.4047601819038391\n",
      "Training loss for batch 3542 : 0.5188932418823242\n",
      "Training loss for batch 3543 : 0.34592559933662415\n",
      "Training loss for batch 3544 : 0.3751353919506073\n",
      "Training loss for batch 3545 : 0.05958379805088043\n",
      "Training loss for batch 3546 : 0.17191177606582642\n",
      "Training loss for batch 3547 : 0.04240446165204048\n",
      "Training loss for batch 3548 : 0.17602308094501495\n",
      "Training loss for batch 3549 : 0.2881425619125366\n",
      "Training loss for batch 3550 : 0.6508148312568665\n",
      "Training loss for batch 3551 : 0.009684337303042412\n",
      "Training loss for batch 3552 : 0.35682806372642517\n",
      "Training loss for batch 3553 : 0.5603814125061035\n",
      "Training loss for batch 3554 : 0.4303220808506012\n",
      "Training loss for batch 3555 : 0.17833513021469116\n",
      "Training loss for batch 3556 : 0.2669362723827362\n",
      "Training loss for batch 3557 : 0.0\n",
      "Training loss for batch 3558 : 0.27288925647735596\n",
      "Training loss for batch 3559 : 0.11778035759925842\n",
      "Training loss for batch 3560 : 0.38001197576522827\n",
      "Training loss for batch 3561 : 0.23532935976982117\n",
      "Training loss for batch 3562 : 0.44021081924438477\n",
      "Training loss for batch 3563 : 0.13311271369457245\n",
      "Training loss for batch 3564 : 0.16776137053966522\n",
      "Training loss for batch 3565 : 0.19955629110336304\n",
      "Training loss for batch 3566 : 0.20600974559783936\n",
      "Training loss for batch 3567 : 0.049515966325998306\n",
      "Training loss for batch 3568 : 0.3159388303756714\n",
      "Training loss for batch 3569 : 0.5518187284469604\n",
      "Training loss for batch 3570 : 0.16766737401485443\n",
      "Training loss for batch 3571 : 0.40413856506347656\n",
      "Training loss for batch 3572 : 0.29278045892715454\n",
      "Training loss for batch 3573 : 0.8468642830848694\n",
      "Training loss for batch 3574 : 0.13677796721458435\n",
      "Training loss for batch 3575 : 0.30049267411231995\n",
      "Training loss for batch 3576 : 0.2514423429965973\n",
      "Training loss for batch 3577 : 0.34008875489234924\n",
      "Training loss for batch 3578 : 0.0777275562286377\n",
      "Training loss for batch 3579 : 0.482117623090744\n",
      "Training loss for batch 3580 : 0.417045533657074\n",
      "Training loss for batch 3581 : 0.3167605400085449\n",
      "Training loss for batch 3582 : 0.17985789477825165\n",
      "Training loss for batch 3583 : 0.09322550892829895\n",
      "Training loss for batch 3584 : 0.6428231596946716\n",
      "Training loss for batch 3585 : 0.5126863718032837\n",
      "Training loss for batch 3586 : 0.44943150877952576\n",
      "Training loss for batch 3587 : 0.31660374999046326\n",
      "Training loss for batch 3588 : 0.3562941253185272\n",
      "Training loss for batch 3589 : 0.041475385427474976\n",
      "Training loss for batch 3590 : 0.18391932547092438\n",
      "Training loss for batch 3591 : 0.3240567743778229\n",
      "Training loss for batch 3592 : 0.2275959700345993\n",
      "Training loss for batch 3593 : 0.33069440722465515\n",
      "Training loss for batch 3594 : 0.35817551612854004\n",
      "Training loss for batch 3595 : 0.1406058371067047\n",
      "Training loss for batch 3596 : 0.2880646586418152\n",
      "Training loss for batch 3597 : 0.2648034989833832\n",
      "Training loss for batch 3598 : 0.2532617747783661\n",
      "Training loss for batch 3599 : 0.07611361891031265\n",
      "Training loss for batch 3600 : 0.16840539872646332\n",
      "Training loss for batch 3601 : 0.007896816357970238\n",
      "Training loss for batch 3602 : 0.30493876338005066\n",
      "Training loss for batch 3603 : 0.0683881863951683\n",
      "Training loss for batch 3604 : 0.09872987121343613\n",
      "Training loss for batch 3605 : 0.5308265686035156\n",
      "Training loss for batch 3606 : 0.3194955587387085\n",
      "Training loss for batch 3607 : 0.1698911339044571\n",
      "Training loss for batch 3608 : 0.4876187741756439\n",
      "Training loss for batch 3609 : 0.4355985224246979\n",
      "Training loss for batch 3610 : 0.20971128344535828\n",
      "Training loss for batch 3611 : 0.3057563602924347\n",
      "Training loss for batch 3612 : 0.5010547041893005\n",
      "Training loss for batch 3613 : 0.0945780947804451\n",
      "Training loss for batch 3614 : 0.08540899306535721\n",
      "Training loss for batch 3615 : 0.24669471383094788\n",
      "Training loss for batch 3616 : 0.243669793009758\n",
      "Training loss for batch 3617 : 0.3472876250743866\n",
      "Training loss for batch 3618 : 0.262578547000885\n",
      "Training loss for batch 3619 : 0.48165807127952576\n",
      "Training loss for batch 3620 : 0.08718927949666977\n",
      "Training loss for batch 3621 : 0.5864782333374023\n",
      "Training loss for batch 3622 : 0.3694564700126648\n",
      "Training loss for batch 3623 : 0.5108346939086914\n",
      "Training loss for batch 3624 : 0.5189812183380127\n",
      "Training loss for batch 3625 : 0.11849929392337799\n",
      "Training loss for batch 3626 : 0.6473073959350586\n",
      "Training loss for batch 3627 : 0.18878412246704102\n",
      "Training loss for batch 3628 : 0.6813677549362183\n",
      "Training loss for batch 3629 : 0.5205419659614563\n",
      "Training loss for batch 3630 : 0.36484402418136597\n",
      "Training loss for batch 3631 : 0.4443508982658386\n",
      "Training loss for batch 3632 : 0.00045623857295140624\n",
      "Training loss for batch 3633 : 0.583894670009613\n",
      "Training loss for batch 3634 : 0.22190698981285095\n",
      "Training loss for batch 3635 : 0.18656985461711884\n",
      "Training loss for batch 3636 : 0.9364249110221863\n",
      "Training loss for batch 3637 : 0.2436588555574417\n",
      "Training loss for batch 3638 : 0.3605217933654785\n",
      "Training loss for batch 3639 : 0.14679183065891266\n",
      "Training loss for batch 3640 : 0.17379416525363922\n",
      "Training loss for batch 3641 : 0.11588607728481293\n",
      "Training loss for batch 3642 : 0.2130473256111145\n",
      "Training loss for batch 3643 : 0.03195328637957573\n",
      "Training loss for batch 3644 : 0.0839831680059433\n",
      "Training loss for batch 3645 : 0.21573592722415924\n",
      "Training loss for batch 3646 : 0.31814029812812805\n",
      "Training loss for batch 3647 : 0.09099699556827545\n",
      "Training loss for batch 3648 : 0.10289940237998962\n",
      "Training loss for batch 3649 : 0.2750611901283264\n",
      "Training loss for batch 3650 : 0.12830668687820435\n",
      "Training loss for batch 3651 : 0.36271947622299194\n",
      "Training loss for batch 3652 : 0.33761581778526306\n",
      "Training loss for batch 3653 : 0.09458180516958237\n",
      "Training loss for batch 3654 : 0.1882210075855255\n",
      "Training loss for batch 3655 : 0.33227217197418213\n",
      "Training loss for batch 3656 : 0.3319784700870514\n",
      "Training loss for batch 3657 : 0.3592129647731781\n",
      "Training loss for batch 3658 : 0.08464774489402771\n",
      "Training loss for batch 3659 : 0.17833790183067322\n",
      "Training loss for batch 3660 : 0.00620762025937438\n",
      "Training loss for batch 3661 : 0.33817923069000244\n",
      "Training loss for batch 3662 : 0.16631394624710083\n",
      "Training loss for batch 3663 : 0.11470633745193481\n",
      "Training loss for batch 3664 : 0.4804481565952301\n",
      "Training loss for batch 3665 : 0.20655542612075806\n",
      "Training loss for batch 3666 : 0.10265766084194183\n",
      "Training loss for batch 3667 : 0.23102158308029175\n",
      "Training loss for batch 3668 : 0.09506770968437195\n",
      "Training loss for batch 3669 : 0.4558414816856384\n",
      "Training loss for batch 3670 : 0.1945810317993164\n",
      "Training loss for batch 3671 : 0.08310423046350479\n",
      "Training loss for batch 3672 : 0.06183670088648796\n",
      "Training loss for batch 3673 : 0.46587491035461426\n",
      "Training loss for batch 3674 : 0.0\n",
      "Training loss for batch 3675 : 0.3929494619369507\n",
      "Training loss for batch 3676 : 0.2471867948770523\n",
      "Training loss for batch 3677 : 0.29432418942451477\n",
      "Training loss for batch 3678 : 0.07080502808094025\n",
      "Training loss for batch 3679 : 0.05190010741353035\n",
      "Training loss for batch 3680 : 0.04135793447494507\n",
      "Training loss for batch 3681 : 0.16560232639312744\n",
      "Training loss for batch 3682 : 0.14118196070194244\n",
      "Training loss for batch 3683 : 0.35283762216567993\n",
      "Training loss for batch 3684 : 0.4371300935745239\n",
      "Training loss for batch 3685 : 0.12025030702352524\n",
      "Training loss for batch 3686 : 0.3981339633464813\n",
      "Training loss for batch 3687 : 0.06164451688528061\n",
      "Training loss for batch 3688 : 0.27901121973991394\n",
      "Training loss for batch 3689 : 0.42436447739601135\n",
      "Training loss for batch 3690 : 0.20944839715957642\n",
      "Training loss for batch 3691 : 0.0072703235782682896\n",
      "Training loss for batch 3692 : 0.021418295800685883\n",
      "Training loss for batch 3693 : 0.06854936480522156\n",
      "Training loss for batch 3694 : 0.12983417510986328\n",
      "Training loss for batch 3695 : 0.23806045949459076\n",
      "Training loss for batch 3696 : 0.03910202905535698\n",
      "Training loss for batch 3697 : 0.08266738802194595\n",
      "Training loss for batch 3698 : 0.5460634231567383\n",
      "Training loss for batch 3699 : 0.7049004435539246\n",
      "Training loss for batch 3700 : 0.17645658552646637\n",
      "Training loss for batch 3701 : 0.35074347257614136\n",
      "Training loss for batch 3702 : 0.1560145914554596\n",
      "Training loss for batch 3703 : 0.24868963658809662\n",
      "Training loss for batch 3704 : 0.10456196218729019\n",
      "Training loss for batch 3705 : 0.49839353561401367\n",
      "Training loss for batch 3706 : 0.27621302008628845\n",
      "Training loss for batch 3707 : 0.34640243649482727\n",
      "Training loss for batch 3708 : 0.10082311928272247\n",
      "Training loss for batch 3709 : 0.3608402609825134\n",
      "Training loss for batch 3710 : 0.5719112753868103\n",
      "Training loss for batch 3711 : 0.10021801292896271\n",
      "Training loss for batch 3712 : 0.4530399739742279\n",
      "Training loss for batch 3713 : 0.39136818051338196\n",
      "Training loss for batch 3714 : 0.23068629205226898\n",
      "Training loss for batch 3715 : 0.10296525061130524\n",
      "Training loss for batch 3716 : 0.30513957142829895\n",
      "Training loss for batch 3717 : 0.036844413727521896\n",
      "Training loss for batch 3718 : 0.04332064837217331\n",
      "Training loss for batch 3719 : 0.017258087173104286\n",
      "Training loss for batch 3720 : 0.09760087728500366\n",
      "Training loss for batch 3721 : 0.3313358426094055\n",
      "Training loss for batch 3722 : 0.09817475825548172\n",
      "Training loss for batch 3723 : 4.21901568188332e-05\n",
      "Training loss for batch 3724 : 0.2967025637626648\n",
      "Training loss for batch 3725 : 0.6499977707862854\n",
      "Training loss for batch 3726 : 0.14363135397434235\n",
      "Training loss for batch 3727 : 0.25108569860458374\n",
      "Training loss for batch 3728 : 0.05560297146439552\n",
      "Training loss for batch 3729 : 0.55448317527771\n",
      "Training loss for batch 3730 : 0.31590691208839417\n",
      "Training loss for batch 3731 : 0.04176886007189751\n",
      "Training loss for batch 3732 : 0.2004101574420929\n",
      "Training loss for batch 3733 : 0.11878947168588638\n",
      "Training loss for batch 3734 : 0.2595955729484558\n",
      "Training loss for batch 3735 : 0.21638520061969757\n",
      "Training loss for batch 3736 : 0.10693149268627167\n",
      "Training loss for batch 3737 : 0.16973696649074554\n",
      "Training loss for batch 3738 : 0.21128755807876587\n",
      "Training loss for batch 3739 : 0.12353850901126862\n",
      "Training loss for batch 3740 : 0.10520147532224655\n",
      "Training loss for batch 3741 : 0.4558112919330597\n",
      "Training loss for batch 3742 : 0.2349904477596283\n",
      "Training loss for batch 3743 : 0.11382471770048141\n",
      "Training loss for batch 3744 : 0.2807616889476776\n",
      "Training loss for batch 3745 : 0.03643959388136864\n",
      "Training loss for batch 3746 : 0.11907244473695755\n",
      "Training loss for batch 3747 : 0.35885241627693176\n",
      "Training loss for batch 3748 : 0.26941120624542236\n",
      "Training loss for batch 3749 : 0.07029523700475693\n",
      "Training loss for batch 3750 : 0.24067522585391998\n",
      "Training loss for batch 3751 : 0.10135279595851898\n",
      "Training loss for batch 3752 : 0.520717203617096\n",
      "Training loss for batch 3753 : 0.32939037680625916\n",
      "Training loss for batch 3754 : 0.5627341270446777\n",
      "Training loss for batch 3755 : 0.17602257430553436\n",
      "Training loss for batch 3756 : 0.3898251950740814\n",
      "Training loss for batch 3757 : 0.29530221223831177\n",
      "Training loss for batch 3758 : 0.2026796191930771\n",
      "Training loss for batch 3759 : 0.279472678899765\n",
      "Training loss for batch 3760 : 0.03991464897990227\n",
      "Training loss for batch 3761 : 0.5376880764961243\n",
      "Training loss for batch 3762 : 0.42543721199035645\n",
      "Training loss for batch 3763 : 0.7926867008209229\n",
      "Training loss for batch 3764 : 0.551851212978363\n",
      "Training loss for batch 3765 : 0.3126840591430664\n",
      "Training loss for batch 3766 : 0.032412659376859665\n",
      "Training loss for batch 3767 : 0.26155710220336914\n",
      "Training loss for batch 3768 : 0.18561804294586182\n",
      "Training loss for batch 3769 : 0.21980109810829163\n",
      "Training loss for batch 3770 : 0.6872384548187256\n",
      "Training loss for batch 3771 : 0.600922167301178\n",
      "Training loss for batch 3772 : 0.09051171690225601\n",
      "Training loss for batch 3773 : 0.5862879753112793\n",
      "Training loss for batch 3774 : 0.13305191695690155\n",
      "Training loss for batch 3775 : 0.6235569715499878\n",
      "Training loss for batch 3776 : 0.9177578091621399\n",
      "Training loss for batch 3777 : 0.552463948726654\n",
      "Training loss for batch 3778 : 0.19822987914085388\n",
      "Training loss for batch 3779 : 0.49782079458236694\n",
      "Training loss for batch 3780 : 0.4266287088394165\n",
      "Training loss for batch 3781 : 0.3333282768726349\n",
      "Training loss for batch 3782 : 0.5293950438499451\n",
      "Training loss for batch 3783 : 0.20547832548618317\n",
      "Training loss for batch 3784 : 0.30655544996261597\n",
      "Training loss for batch 3785 : 0.05432596057653427\n",
      "Training loss for batch 3786 : 0.20472446084022522\n",
      "Training loss for batch 3787 : 0.46755245327949524\n",
      "Training loss for batch 3788 : 0.25752580165863037\n",
      "Training loss for batch 3789 : 0.06863070279359818\n",
      "Training loss for batch 3790 : 0.3485637307167053\n",
      "Training loss for batch 3791 : 0.1454843431711197\n",
      "Training loss for batch 3792 : 0.44623860716819763\n",
      "Training loss for batch 3793 : 0.19948133826255798\n",
      "Training loss for batch 3794 : 0.09002735465765\n",
      "Training loss for batch 3795 : 0.21345855295658112\n",
      "Training loss for batch 3796 : 0.23896785080432892\n",
      "Training loss for batch 3797 : 0.177888423204422\n",
      "Training loss for batch 3798 : 0.24561601877212524\n",
      "Training loss for batch 3799 : 0.3421759307384491\n",
      "Training loss for batch 3800 : 0.053054507821798325\n",
      "Training loss for batch 3801 : 0.08802071213722229\n",
      "Training loss for batch 3802 : 0.10849449038505554\n",
      "Training loss for batch 3803 : 0.16425970196723938\n",
      "Training loss for batch 3804 : 0.23539277911186218\n",
      "Training loss for batch 3805 : 0.41314730048179626\n",
      "Training loss for batch 3806 : 0.36051201820373535\n",
      "Training loss for batch 3807 : 0.3550925552845001\n",
      "Training loss for batch 3808 : 0.22782176733016968\n",
      "Training loss for batch 3809 : 0.15939511358737946\n",
      "Training loss for batch 3810 : 0.07298299670219421\n",
      "Training loss for batch 3811 : 0.08366907387971878\n",
      "Training loss for batch 3812 : 0.1579776257276535\n",
      "Training loss for batch 3813 : 0.4048238694667816\n",
      "Training loss for batch 3814 : 0.017720812931656837\n",
      "Training loss for batch 3815 : 0.16933327913284302\n",
      "Training loss for batch 3816 : 0.32504943013191223\n",
      "Training loss for batch 3817 : 0.06486764550209045\n",
      "Training loss for batch 3818 : 0.6169797778129578\n",
      "Training loss for batch 3819 : 0.4395960867404938\n",
      "Training loss for batch 3820 : 0.14696358144283295\n",
      "Training loss for batch 3821 : 0.39886999130249023\n",
      "Training loss for batch 3822 : 0.22530697286128998\n",
      "Training loss for batch 3823 : 0.09477292001247406\n",
      "Training loss for batch 3824 : 0.22140151262283325\n",
      "Training loss for batch 3825 : 0.12224980443716049\n",
      "Training loss for batch 3826 : 0.10905097424983978\n",
      "Training loss for batch 3827 : 0.3053053915500641\n",
      "Training loss for batch 3828 : 0.11659090965986252\n",
      "Training loss for batch 3829 : 0.5264993906021118\n",
      "Training loss for batch 3830 : 0.2825809419155121\n",
      "Training loss for batch 3831 : 0.4011992812156677\n",
      "Training loss for batch 3832 : 0.08732787519693375\n",
      "Training loss for batch 3833 : 0.3648695647716522\n",
      "Training loss for batch 3834 : 0.3680840730667114\n",
      "Training loss for batch 3835 : 0.11691956222057343\n",
      "Training loss for batch 3836 : 0.06219269335269928\n",
      "Training loss for batch 3837 : 0.5038387179374695\n",
      "Training loss for batch 3838 : 0.2054128646850586\n",
      "Training loss for batch 3839 : 0.026679737493395805\n",
      "Training loss for batch 3840 : 0.43495893478393555\n",
      "Training loss for batch 3841 : 0.16782577335834503\n",
      "Training loss for batch 3842 : 0.08783292770385742\n",
      "Training loss for batch 3843 : 0.39750662446022034\n",
      "Training loss for batch 3844 : 0.1658027172088623\n",
      "Training loss for batch 3845 : 0.09930391609668732\n",
      "Training loss for batch 3846 : 0.3323931097984314\n",
      "Training loss for batch 3847 : 0.3533974587917328\n",
      "Training loss for batch 3848 : 0.1440763920545578\n",
      "Training loss for batch 3849 : 0.29945749044418335\n",
      "Training loss for batch 3850 : 0.5652928948402405\n",
      "Training loss for batch 3851 : 0.3712315559387207\n",
      "Training loss for batch 3852 : 0.17963163554668427\n",
      "Training loss for batch 3853 : 0.07114182412624359\n",
      "Training loss for batch 3854 : 0.28999456763267517\n",
      "Training loss for batch 3855 : 0.1709335744380951\n",
      "Training loss for batch 3856 : 0.640222430229187\n",
      "Training loss for batch 3857 : 0.11583401262760162\n",
      "Training loss for batch 3858 : 0.0696977749466896\n",
      "Training loss for batch 3859 : 0.7628912925720215\n",
      "Training loss for batch 3860 : 0.22998028993606567\n",
      "Training loss for batch 3861 : 0.15057140588760376\n",
      "Training loss for batch 3862 : 0.1676493138074875\n",
      "Training loss for batch 3863 : 0.1008385494351387\n",
      "Training loss for batch 3864 : 0.006285124458372593\n",
      "Training loss for batch 3865 : 0.15791095793247223\n",
      "Training loss for batch 3866 : 0.33203086256980896\n",
      "Training loss for batch 3867 : 0.06937924772500992\n",
      "Training loss for batch 3868 : 0.3002280294895172\n",
      "Training loss for batch 3869 : 0.6091434359550476\n",
      "Training loss for batch 3870 : 0.24873235821723938\n",
      "Training loss for batch 3871 : 0.033912062644958496\n",
      "Training loss for batch 3872 : 0.7222369313240051\n",
      "Training loss for batch 3873 : 0.2826470136642456\n",
      "Training loss for batch 3874 : 0.21599885821342468\n",
      "Training loss for batch 3875 : 0.4635653495788574\n",
      "Training loss for batch 3876 : 0.2681221663951874\n",
      "Training loss for batch 3877 : 0.06184772774577141\n",
      "Training loss for batch 3878 : 0.03931392356753349\n",
      "Training loss for batch 3879 : 0.23208564519882202\n",
      "Training loss for batch 3880 : 0.055463872849941254\n",
      "Training loss for batch 3881 : 0.38529300689697266\n",
      "Training loss for batch 3882 : 0.38270580768585205\n",
      "Training loss for batch 3883 : 0.05848723277449608\n",
      "Training loss for batch 3884 : 0.273034930229187\n",
      "Training loss for batch 3885 : 0.25685906410217285\n",
      "Training loss for batch 3886 : 0.5500629544258118\n",
      "Training loss for batch 3887 : 0.14229902625083923\n",
      "Training loss for batch 3888 : 0.39216792583465576\n",
      "Training loss for batch 3889 : 0.5016999840736389\n",
      "Training loss for batch 3890 : 0.2571766972541809\n",
      "Training loss for batch 3891 : 0.24910634756088257\n",
      "Training loss for batch 3892 : 0.08311167359352112\n",
      "Training loss for batch 3893 : 0.265699565410614\n",
      "Training loss for batch 3894 : 0.31524547934532166\n",
      "Training loss for batch 3895 : 0.06996233016252518\n",
      "Training loss for batch 3896 : 0.052325647324323654\n",
      "Training loss for batch 3897 : 0.23719853162765503\n",
      "Training loss for batch 3898 : 0.0053621986880898476\n",
      "Training loss for batch 3899 : 0.17103378474712372\n",
      "Training loss for batch 3900 : 0.2539464235305786\n",
      "Training loss for batch 3901 : 0.2630957365036011\n",
      "Training loss for batch 3902 : 0.10754051804542542\n",
      "Training loss for batch 3903 : 0.13697654008865356\n",
      "Training loss for batch 3904 : 0.3296698033809662\n",
      "Training loss for batch 3905 : 0.20348095893859863\n",
      "Training loss for batch 3906 : 0.1783531904220581\n",
      "Training loss for batch 3907 : 0.01056715939193964\n",
      "Training loss for batch 3908 : 0.21024242043495178\n",
      "Training loss for batch 3909 : 0.028142007067799568\n",
      "Training loss for batch 3910 : 0.4438517093658447\n",
      "Training loss for batch 3911 : 0.9146579504013062\n",
      "Training loss for batch 3912 : 0.2616128623485565\n",
      "Training loss for batch 3913 : 0.058044757694005966\n",
      "Training loss for batch 3914 : 0.3840477764606476\n",
      "Training loss for batch 3915 : 0.21545527875423431\n",
      "Training loss for batch 3916 : 0.47168296575546265\n",
      "Training loss for batch 3917 : 0.18766415119171143\n",
      "Training loss for batch 3918 : 0.19462651014328003\n",
      "Training loss for batch 3919 : 0.2783687114715576\n",
      "Training loss for batch 3920 : 0.3339737057685852\n",
      "Training loss for batch 3921 : 0.29962679743766785\n",
      "Training loss for batch 3922 : 0.3397223949432373\n",
      "Training loss for batch 3923 : 0.3372630178928375\n",
      "Training loss for batch 3924 : 0.22170959413051605\n",
      "Training loss for batch 3925 : 0.156303271651268\n",
      "Training loss for batch 3926 : 0.06675466150045395\n",
      "Training loss for batch 3927 : 0.3047018349170685\n",
      "Training loss for batch 3928 : 0.0915917158126831\n",
      "Training loss for batch 3929 : 0.3642321825027466\n",
      "Training loss for batch 3930 : 0.20177040994167328\n",
      "Training loss for batch 3931 : 0.04209892451763153\n",
      "Training loss for batch 3932 : 0.41560685634613037\n",
      "Training loss for batch 3933 : 0.09248814731836319\n",
      "Training loss for batch 3934 : 0.10023936629295349\n",
      "Training loss for batch 3935 : 0.17331287264823914\n",
      "Training loss for batch 3936 : 0.2831687331199646\n",
      "Training loss for batch 3937 : 0.40221738815307617\n",
      "Training loss for batch 3938 : 0.041731495410203934\n",
      "Training loss for batch 3939 : 0.6088606715202332\n",
      "Training loss for batch 3940 : 0.033729806542396545\n",
      "Training loss for batch 3941 : 0.14424854516983032\n",
      "Training loss for batch 3942 : 0.12252499163150787\n",
      "Training loss for batch 3943 : 0.14431671798229218\n",
      "Training loss for batch 3944 : 0.3367888927459717\n",
      "Training loss for batch 3945 : 0.39772143959999084\n",
      "Training loss for batch 3946 : 0.16498255729675293\n",
      "Training loss for batch 3947 : 0.08509621024131775\n",
      "Training loss for batch 3948 : 0.0871332436800003\n",
      "Training loss for batch 3949 : 0.26209789514541626\n",
      "Training loss for batch 3950 : 0.299309104681015\n",
      "Training loss for batch 3951 : 0.2581128180027008\n",
      "Training loss for batch 3952 : 0.38319873809814453\n",
      "Training loss for batch 3953 : 0.30473703145980835\n",
      "Training loss for batch 3954 : 0.4039880931377411\n",
      "Training loss for batch 3955 : 0.04028681665658951\n",
      "Training loss for batch 3956 : 0.29244524240493774\n",
      "Training loss for batch 3957 : 0.5942709445953369\n",
      "Training loss for batch 3958 : 0.0011896789073944092\n",
      "Training loss for batch 3959 : 0.16766613721847534\n",
      "Training loss for batch 3960 : 0.1373792588710785\n",
      "Training loss for batch 3961 : 0.23999309539794922\n",
      "Training loss for batch 3962 : 0.24116747081279755\n",
      "Training loss for batch 3963 : 0.10583873838186264\n",
      "Training loss for batch 3964 : 0.7233269810676575\n",
      "Training loss for batch 3965 : 0.1620202213525772\n",
      "Training loss for batch 3966 : 0.17785058915615082\n",
      "Training loss for batch 3967 : 0.42294707894325256\n",
      "Training loss for batch 3968 : 0.0\n",
      "Training loss for batch 3969 : 0.45957767963409424\n",
      "Training loss for batch 3970 : 0.15192921459674835\n",
      "Training loss for batch 3971 : 0.4102088212966919\n",
      "Training loss for batch 3972 : 0.2794872522354126\n",
      "Training loss for batch 3973 : 0.18599458038806915\n",
      "Training loss for batch 3974 : 0.336586058139801\n",
      "Training loss for batch 3975 : 0.1892106831073761\n",
      "Training loss for batch 3976 : 0.12809288501739502\n",
      "Training loss for batch 3977 : 0.4100605547428131\n",
      "Training loss for batch 3978 : 0.24422043561935425\n",
      "Training loss for batch 3979 : 0.43012890219688416\n",
      "Training loss for batch 3980 : 0.25419747829437256\n",
      "Training loss for batch 3981 : 0.08398792147636414\n",
      "Training loss for batch 3982 : 0.5148253440856934\n",
      "Training loss for batch 3983 : 0.18315532803535461\n",
      "Training loss for batch 3984 : 0.21160179376602173\n",
      "Training loss for batch 3985 : 0.4599458575248718\n",
      "Training loss for batch 3986 : 0.4159841537475586\n",
      "Training loss for batch 3987 : 0.21270953118801117\n",
      "Training loss for batch 3988 : 0.01887989044189453\n",
      "Training loss for batch 3989 : 0.18259406089782715\n",
      "Training loss for batch 3990 : 0.0858316570520401\n",
      "Training loss for batch 3991 : 0.34681278467178345\n",
      "Training loss for batch 3992 : 0.2135997712612152\n",
      "Training loss for batch 3993 : 0.356878399848938\n",
      "Training loss for batch 3994 : 0.15821555256843567\n",
      "Training loss for batch 3995 : 0.40657752752304077\n",
      "Training loss for batch 3996 : 0.08380930125713348\n",
      "Training loss for batch 3997 : 0.27870097756385803\n",
      "Training loss for batch 3998 : 0.11749929189682007\n",
      "Training loss for batch 3999 : 0.32123664021492004\n",
      "Training loss for batch 4000 : 0.3373580873012543\n",
      "Training loss for batch 4001 : 0.286215603351593\n",
      "Training loss for batch 4002 : 0.03729408234357834\n",
      "Training loss for batch 4003 : 0.23134838044643402\n",
      "Training loss for batch 4004 : 0.34741708636283875\n",
      "Training loss for batch 4005 : 0.035426247864961624\n",
      "Training loss for batch 4006 : 0.24735671281814575\n",
      "Training loss for batch 4007 : 0.09806978702545166\n",
      "Training loss for batch 4008 : 0.08140697330236435\n",
      "Training loss for batch 4009 : 0.19413141906261444\n",
      "Training loss for batch 4010 : 0.25090521574020386\n",
      "Training loss for batch 4011 : 0.2560358941555023\n",
      "Training loss for batch 4012 : 0.31890782713890076\n",
      "Training loss for batch 4013 : 0.20411184430122375\n",
      "Training loss for batch 4014 : 0.4301680028438568\n",
      "Training loss for batch 4015 : 0.27045491337776184\n",
      "Training loss for batch 4016 : 0.23708271980285645\n",
      "Training loss for batch 4017 : 0.1409374326467514\n",
      "Training loss for batch 4018 : 0.29686030745506287\n",
      "Training loss for batch 4019 : 0.137179896235466\n",
      "Training loss for batch 4020 : 0.31651151180267334\n",
      "Training loss for batch 4021 : 0.09352543950080872\n",
      "Training loss for batch 4022 : 0.35286256670951843\n",
      "Training loss for batch 4023 : 0.184976264834404\n",
      "Training loss for batch 4024 : 0.07326583564281464\n",
      "Training loss for batch 4025 : 0.001514355419203639\n",
      "Training loss for batch 4026 : 0.04867205023765564\n",
      "Training loss for batch 4027 : 0.21863144636154175\n",
      "Training loss for batch 4028 : 0.0974346324801445\n",
      "Training loss for batch 4029 : 0.2964290380477905\n",
      "Training loss for batch 4030 : 0.10608167946338654\n",
      "Training loss for batch 4031 : 0.6305745244026184\n",
      "Training loss for batch 4032 : 0.3120894134044647\n",
      "Training loss for batch 4033 : 0.8502433896064758\n",
      "Training loss for batch 4034 : 0.08942276984453201\n",
      "Training loss for batch 4035 : 0.4213164150714874\n",
      "Training loss for batch 4036 : 0.5539678335189819\n",
      "Training loss for batch 4037 : 0.011571981944143772\n",
      "Training loss for batch 4038 : 0.24341771006584167\n",
      "Training loss for batch 4039 : 0.11927124857902527\n",
      "Training loss for batch 4040 : 0.2006191462278366\n",
      "Training loss for batch 4041 : 0.22546321153640747\n",
      "Training loss for batch 4042 : 0.11260874569416046\n",
      "Training loss for batch 4043 : 0.09251479804515839\n",
      "Training loss for batch 4044 : 0.34180548787117004\n",
      "Training loss for batch 4045 : 0.12531401216983795\n",
      "Training loss for batch 4046 : 0.4706571102142334\n",
      "Training loss for batch 4047 : 0.4492151737213135\n",
      "Training loss for batch 4048 : 0.4399450123310089\n",
      "Training loss for batch 4049 : 0.12146494537591934\n",
      "Training loss for batch 4050 : 0.2265407294034958\n",
      "Training loss for batch 4051 : 0.3057854473590851\n",
      "Training loss for batch 4052 : 0.27969488501548767\n",
      "Training loss for batch 4053 : 0.14180314540863037\n",
      "Training loss for batch 4054 : 0.3763009309768677\n",
      "Training loss for batch 4055 : 0.11257650703191757\n",
      "Training loss for batch 4056 : 0.13842330873012543\n",
      "Training loss for batch 4057 : 1.0480836629867554\n",
      "Training loss for batch 4058 : 0.35130128264427185\n",
      "Training loss for batch 4059 : 0.2298673838376999\n",
      "Training loss for batch 4060 : 0.04357285797595978\n",
      "Training loss for batch 4061 : 0.372202068567276\n",
      "Training loss for batch 4062 : 0.26862961053848267\n",
      "Training loss for batch 4063 : 0.047617532312870026\n",
      "Training loss for batch 4064 : 0.08134948462247849\n",
      "Training loss for batch 4065 : 0.36866486072540283\n",
      "Training loss for batch 4066 : 0.2982982099056244\n",
      "Training loss for batch 4067 : 0.0\n",
      "Training loss for batch 4068 : 0.3617725074291229\n",
      "Training loss for batch 4069 : 0.239327073097229\n",
      "Training loss for batch 4070 : 0.018538648262619972\n",
      "Training loss for batch 4071 : 0.0\n",
      "Training loss for batch 4072 : 0.030179066583514214\n",
      "Training loss for batch 4073 : 0.5724480152130127\n",
      "Training loss for batch 4074 : 0.3506755232810974\n",
      "Training loss for batch 4075 : 0.39482995867729187\n",
      "Training loss for batch 4076 : 0.48917442560195923\n",
      "Training loss for batch 4077 : 0.0\n",
      "Training loss for batch 4078 : 0.07139809429645538\n",
      "Training loss for batch 4079 : 0.14977489411830902\n",
      "Training loss for batch 4080 : 0.10853926092386246\n",
      "Training loss for batch 4081 : 0.22177468240261078\n",
      "Training loss for batch 4082 : 0.38828983902931213\n",
      "Training loss for batch 4083 : 0.03936817869544029\n",
      "Training loss for batch 4084 : 0.15697868168354034\n",
      "Training loss for batch 4085 : 0.3020695745944977\n",
      "Training loss for batch 4086 : 0.0\n",
      "Training loss for batch 4087 : 0.024671588093042374\n",
      "Training loss for batch 4088 : 0.4078710973262787\n",
      "Training loss for batch 4089 : 0.2771812677383423\n",
      "Training loss for batch 4090 : 0.1764775812625885\n",
      "Training loss for batch 4091 : 0.011040257290005684\n",
      "Training loss for batch 4092 : 0.32234954833984375\n",
      "Training loss for batch 4093 : 0.34276795387268066\n",
      "Training loss for batch 4094 : 0.18382790684700012\n",
      "Training loss for batch 4095 : 0.21446600556373596\n",
      "Training loss for batch 4096 : 0.0\n",
      "Training loss for batch 4097 : 0.016565345227718353\n",
      "Training loss for batch 4098 : 0.35045015811920166\n",
      "Training loss for batch 4099 : 0.1796872615814209\n",
      "Training loss for batch 4100 : 0.07210693508386612\n",
      "Training loss for batch 4101 : 0.21283790469169617\n",
      "Training loss for batch 4102 : 0.06578870117664337\n",
      "Training loss for batch 4103 : 0.40030747652053833\n",
      "Training loss for batch 4104 : 0.3594081699848175\n",
      "Training loss for batch 4105 : 0.061332687735557556\n",
      "Training loss for batch 4106 : 0.3910447061061859\n",
      "Training loss for batch 4107 : 0.3986074924468994\n",
      "Training loss for batch 4108 : 0.05328790098428726\n",
      "Training loss for batch 4109 : 0.3178877830505371\n",
      "Training loss for batch 4110 : 0.26216837763786316\n",
      "Training loss for batch 4111 : 0.1496150642633438\n",
      "Training loss for batch 4112 : 0.009445502422749996\n",
      "Training loss for batch 4113 : 0.04621724784374237\n",
      "Training loss for batch 4114 : 0.20459893345832825\n",
      "Training loss for batch 4115 : 0.02907727286219597\n",
      "Training loss for batch 4116 : 0.2836374044418335\n",
      "Training loss for batch 4117 : 0.23919713497161865\n",
      "Training loss for batch 4118 : 0.3583645820617676\n",
      "Training loss for batch 4119 : 0.031121879816055298\n",
      "Training loss for batch 4120 : 0.3143659830093384\n",
      "Training loss for batch 4121 : 0.3985626697540283\n",
      "Training loss for batch 4122 : 0.13601036369800568\n",
      "Training loss for batch 4123 : 0.13532297313213348\n",
      "Training loss for batch 4124 : 0.11310674250125885\n",
      "Training loss for batch 4125 : 0.22363418340682983\n",
      "Training loss for batch 4126 : 0.13456040620803833\n",
      "Training loss for batch 4127 : 0.1392696350812912\n",
      "Training loss for batch 4128 : 0.37098339200019836\n",
      "Training loss for batch 4129 : 0.2238178551197052\n",
      "Training loss for batch 4130 : 0.10416651517152786\n",
      "Training loss for batch 4131 : 0.2660994827747345\n",
      "Training loss for batch 4132 : 0.2991151511669159\n",
      "Training loss for batch 4133 : 0.26819533109664917\n",
      "Training loss for batch 4134 : 0.09241248667240143\n",
      "Training loss for batch 4135 : 0.2709769308567047\n",
      "Training loss for batch 4136 : 0.006381094455718994\n",
      "Training loss for batch 4137 : 0.4226931035518646\n",
      "Training loss for batch 4138 : 0.2945874333381653\n",
      "Training loss for batch 4139 : 0.37724122405052185\n",
      "Training loss for batch 4140 : 0.09951520711183548\n",
      "Training loss for batch 4141 : 0.9428810477256775\n",
      "Training loss for batch 4142 : 0.4877619743347168\n",
      "Training loss for batch 4143 : 0.08348127454519272\n",
      "Training loss for batch 4144 : 0.3265640437602997\n",
      "Training loss for batch 4145 : 0.0977623462677002\n",
      "Training loss for batch 4146 : 0.17085187137126923\n",
      "Training loss for batch 4147 : 0.13630491495132446\n",
      "Training loss for batch 4148 : 0.3717556893825531\n",
      "Training loss for batch 4149 : 0.18027666211128235\n",
      "Training loss for batch 4150 : 0.19980083405971527\n",
      "Training loss for batch 4151 : 0.29877859354019165\n",
      "Training loss for batch 4152 : 0.14080168306827545\n",
      "Training loss for batch 4153 : 0.9391083121299744\n",
      "Training loss for batch 4154 : 0.33959317207336426\n",
      "Training loss for batch 4155 : 0.3035421073436737\n",
      "Training loss for batch 4156 : 0.1171458512544632\n",
      "Training loss for batch 4157 : 0.044106632471084595\n",
      "Training loss for batch 4158 : 0.3236084580421448\n",
      "Training loss for batch 4159 : 0.01986483857035637\n",
      "Training loss for batch 4160 : 0.33551493287086487\n",
      "Training loss for batch 4161 : 0.1396181434392929\n",
      "Training loss for batch 4162 : 0.23561608791351318\n",
      "Training loss for batch 4163 : 0.00251924991607666\n",
      "Training loss for batch 4164 : 0.4599617123603821\n",
      "Training loss for batch 4165 : 0.5138827562332153\n",
      "Training loss for batch 4166 : 0.011609305627644062\n",
      "Training loss for batch 4167 : 0.22482503950595856\n",
      "Training loss for batch 4168 : 0.10844691097736359\n",
      "Training loss for batch 4169 : 0.04643801972270012\n",
      "Training loss for batch 4170 : 0.33606892824172974\n",
      "Training loss for batch 4171 : 0.25241416692733765\n",
      "Training loss for batch 4172 : 0.27804866433143616\n",
      "Training loss for batch 4173 : 0.3691084384918213\n",
      "Training loss for batch 4174 : 0.08168432861566544\n",
      "Training loss for batch 4175 : 0.17623573541641235\n",
      "Training loss for batch 4176 : 0.29492005705833435\n",
      "Training loss for batch 4177 : 0.31551769375801086\n",
      "Training loss for batch 4178 : 0.404617577791214\n",
      "Training loss for batch 4179 : 0.47630274295806885\n",
      "Training loss for batch 4180 : 0.2648715078830719\n",
      "Training loss for batch 4181 : 0.23211492598056793\n",
      "Training loss for batch 4182 : 0.08226318657398224\n",
      "Training loss for batch 4183 : 0.4251444339752197\n",
      "Training loss for batch 4184 : 0.3335578143596649\n",
      "Training loss for batch 4185 : 0.3437304198741913\n",
      "Training loss for batch 4186 : 0.503305196762085\n",
      "Training loss for batch 4187 : 0.261258602142334\n",
      "Training loss for batch 4188 : 0.13462400436401367\n",
      "Training loss for batch 4189 : 0.15384705364704132\n",
      "Training loss for batch 4190 : 0.1864393651485443\n",
      "Training loss for batch 4191 : 0.2265169620513916\n",
      "Training loss for batch 4192 : 0.7987701296806335\n",
      "Training loss for batch 4193 : 0.06156638637185097\n",
      "Training loss for batch 4194 : 0.003436654806137085\n",
      "Training loss for batch 4195 : 0.32527050375938416\n",
      "Training loss for batch 4196 : 0.05408943071961403\n",
      "Training loss for batch 4197 : 0.3010464310646057\n",
      "Training loss for batch 4198 : 0.08912770450115204\n",
      "Training loss for batch 4199 : 0.6072412133216858\n",
      "Training loss for batch 4200 : 0.0612696036696434\n",
      "Training loss for batch 4201 : 0.055132023990154266\n",
      "Training loss for batch 4202 : 0.4027552604675293\n",
      "Training loss for batch 4203 : 0.7579855918884277\n",
      "Training loss for batch 4204 : 0.4696495234966278\n",
      "Training loss for batch 4205 : 0.1351328343153\n",
      "Training loss for batch 4206 : 0.07961887121200562\n",
      "Training loss for batch 4207 : 0.022302931174635887\n",
      "Training loss for batch 4208 : 0.6151130795478821\n",
      "Training loss for batch 4209 : 0.34627941250801086\n",
      "Training loss for batch 4210 : 0.28177371621131897\n",
      "Training loss for batch 4211 : 0.32557564973831177\n",
      "Training loss for batch 4212 : 0.6205754280090332\n",
      "Training loss for batch 4213 : 0.1893783062696457\n",
      "Training loss for batch 4214 : 0.17082209885120392\n",
      "Training loss for batch 4215 : 0.5579012036323547\n",
      "Training loss for batch 4216 : 0.547899603843689\n",
      "Training loss for batch 4217 : 0.28513386845588684\n",
      "Training loss for batch 4218 : 0.3263566493988037\n",
      "Training loss for batch 4219 : 0.13092991709709167\n",
      "Training loss for batch 4220 : 0.28974542021751404\n",
      "Training loss for batch 4221 : 0.2398293912410736\n",
      "Training loss for batch 4222 : 0.1079651489853859\n",
      "Training loss for batch 4223 : 0.3239820599555969\n",
      "Training loss for batch 4224 : 0.12902940809726715\n",
      "Training loss for batch 4225 : 0.14210331439971924\n",
      "Training loss for batch 4226 : 0.17984235286712646\n",
      "Training loss for batch 4227 : 0.3978479504585266\n",
      "Training loss for batch 4228 : 0.1657484918832779\n",
      "Training loss for batch 4229 : 0.1548919826745987\n",
      "Training loss for batch 4230 : 0.5177323222160339\n",
      "Training loss for batch 4231 : 0.2520073354244232\n",
      "Training loss for batch 4232 : 0.11689367890357971\n",
      "Training loss for batch 4233 : 0.19665877521038055\n",
      "Training loss for batch 4234 : 0.3440771996974945\n",
      "Training loss for batch 4235 : 0.09427642822265625\n",
      "Training loss for batch 4236 : 0.5470391511917114\n",
      "Training loss for batch 4237 : 0.058336593210697174\n",
      "Training loss for batch 4238 : 0.17489786446094513\n",
      "Training loss for batch 4239 : 0.2307184636592865\n",
      "Training loss for batch 4240 : 0.051448069512844086\n",
      "Training loss for batch 4241 : 0.27142420411109924\n",
      "Training loss for batch 4242 : 0.09168843924999237\n",
      "Training loss for batch 4243 : 0.2940845787525177\n",
      "Training loss for batch 4244 : 0.16859039664268494\n",
      "Training loss for batch 4245 : 0.040539104491472244\n",
      "Training loss for batch 4246 : 0.5236687660217285\n",
      "Training loss for batch 4247 : 0.10566984117031097\n",
      "Training loss for batch 4248 : 0.42819467186927795\n",
      "Training loss for batch 4249 : 0.3849118947982788\n",
      "Training loss for batch 4250 : 0.028231270611286163\n",
      "Training loss for batch 4251 : 0.0578850582242012\n",
      "Training loss for batch 4252 : 0.22979937493801117\n",
      "Training loss for batch 4253 : 0.3637820780277252\n",
      "Training loss for batch 4254 : 0.3075402081012726\n",
      "Training loss for batch 4255 : 0.5074015855789185\n",
      "Training loss for batch 4256 : 0.3493834137916565\n",
      "Training loss for batch 4257 : 0.46326684951782227\n",
      "Training loss for batch 4258 : 0.08860427886247635\n",
      "Training loss for batch 4259 : 0.5089972019195557\n",
      "Training loss for batch 4260 : 0.005168301053345203\n",
      "Training loss for batch 4261 : 0.05669254809617996\n",
      "Training loss for batch 4262 : 0.13438010215759277\n",
      "Training loss for batch 4263 : 0.2512185871601105\n",
      "Training loss for batch 4264 : 0.3754526674747467\n",
      "Training loss for batch 4265 : 0.20321576297283173\n",
      "Training loss for batch 4266 : 0.0005020698299631476\n",
      "Training loss for batch 4267 : 0.3357675075531006\n",
      "Training loss for batch 4268 : 0.048058658838272095\n",
      "Training loss for batch 4269 : 0.0921189934015274\n",
      "Training loss for batch 4270 : 0.4591023921966553\n",
      "Training loss for batch 4271 : 0.12114351987838745\n",
      "Training loss for batch 4272 : 0.09672020375728607\n",
      "Training loss for batch 4273 : 0.2222817838191986\n",
      "Training loss for batch 4274 : 0.04103228077292442\n",
      "Training loss for batch 4275 : 0.21724440157413483\n",
      "Training loss for batch 4276 : 0.3600303828716278\n",
      "Training loss for batch 4277 : 0.33982646465301514\n",
      "Training loss for batch 4278 : 0.07930542528629303\n",
      "Training loss for batch 4279 : 0.2618788480758667\n",
      "Training loss for batch 4280 : 0.20918917655944824\n",
      "Training loss for batch 4281 : 0.3040134608745575\n",
      "Training loss for batch 4282 : 0.14988894760608673\n",
      "Training loss for batch 4283 : 0.23252978920936584\n",
      "Training loss for batch 4284 : 0.45922625064849854\n",
      "Training loss for batch 4285 : 0.16123375296592712\n",
      "Training loss for batch 4286 : 0.29276350140571594\n",
      "Training loss for batch 4287 : 0.03226257860660553\n",
      "Training loss for batch 4288 : 0.1846098005771637\n",
      "Training loss for batch 4289 : 0.02174575999379158\n",
      "Training loss for batch 4290 : 0.5065463185310364\n",
      "Training loss for batch 4291 : 0.2166139781475067\n",
      "Training loss for batch 4292 : 0.5158021450042725\n",
      "Training loss for batch 4293 : 0.12955857813358307\n",
      "Training loss for batch 4294 : 0.08256988972425461\n",
      "Training loss for batch 4295 : 0.3137401044368744\n",
      "Training loss for batch 4296 : 0.4893997311592102\n",
      "Training loss for batch 4297 : 0.21595871448516846\n",
      "Training loss for batch 4298 : 0.21062971651554108\n",
      "Training loss for batch 4299 : 0.6131435036659241\n",
      "Training loss for batch 4300 : 0.16570569574832916\n",
      "Training loss for batch 4301 : 0.02890518307685852\n",
      "Training loss for batch 4302 : 0.03248710185289383\n",
      "Training loss for batch 4303 : 0.48028314113616943\n",
      "Training loss for batch 4304 : 0.5187462568283081\n",
      "Training loss for batch 4305 : 0.4505535364151001\n",
      "Training loss for batch 4306 : 0.41254231333732605\n",
      "Training loss for batch 4307 : 0.14682221412658691\n",
      "Training loss for batch 4308 : 0.24812005460262299\n",
      "Training loss for batch 4309 : 0.8600679039955139\n",
      "Training loss for batch 4310 : 0.4290182590484619\n",
      "Training loss for batch 4311 : 0.3550274968147278\n",
      "Training loss for batch 4312 : 0.10228028893470764\n",
      "Training loss for batch 4313 : 0.3774513602256775\n",
      "Training loss for batch 4314 : 0.24752932786941528\n",
      "Training loss for batch 4315 : 0.0639406070113182\n",
      "Training loss for batch 4316 : 0.35203173756599426\n",
      "Training loss for batch 4317 : 0.1301373541355133\n",
      "Training loss for batch 4318 : 0.05869508907198906\n",
      "Training loss for batch 4319 : 0.4212305545806885\n",
      "Training loss for batch 4320 : 0.0701310783624649\n",
      "Training loss for batch 4321 : 0.3537692129611969\n",
      "Training loss for batch 4322 : 0.16828790307044983\n",
      "Training loss for batch 4323 : 0.5238122344017029\n",
      "Training loss for batch 4324 : 0.11824344843626022\n",
      "Training loss for batch 4325 : 0.3176790177822113\n",
      "Training loss for batch 4326 : 0.17399239540100098\n",
      "Training loss for batch 4327 : 0.2989737391471863\n",
      "Training loss for batch 4328 : 0.16860757768154144\n",
      "Training loss for batch 4329 : 0.06142197176814079\n",
      "Training loss for batch 4330 : 0.06848880648612976\n",
      "Training loss for batch 4331 : 0.1854405701160431\n",
      "Training loss for batch 4332 : 0.18218249082565308\n",
      "Training loss for batch 4333 : 0.32978197932243347\n",
      "Training loss for batch 4334 : 0.23302021622657776\n",
      "Training loss for batch 4335 : 0.12972745299339294\n",
      "Training loss for batch 4336 : 0.029192686080932617\n",
      "Training loss for batch 4337 : 0.29405543208122253\n",
      "Training loss for batch 4338 : 0.3775773048400879\n",
      "Training loss for batch 4339 : 0.09827224165201187\n",
      "Training loss for batch 4340 : 0.0831044465303421\n",
      "Training loss for batch 4341 : 0.48205551505088806\n",
      "Training loss for batch 4342 : 0.4534393548965454\n",
      "Training loss for batch 4343 : 0.23226168751716614\n",
      "Training loss for batch 4344 : 0.20113424956798553\n",
      "Training loss for batch 4345 : 0.17292611300945282\n",
      "Training loss for batch 4346 : 0.484026700258255\n",
      "Training loss for batch 4347 : 0.5496698617935181\n",
      "Training loss for batch 4348 : 0.2336534857749939\n",
      "Training loss for batch 4349 : 0.053594477474689484\n",
      "Training loss for batch 4350 : 0.2937336266040802\n",
      "Training loss for batch 4351 : 0.34197747707366943\n",
      "Training loss for batch 4352 : 0.05741303041577339\n",
      "Training loss for batch 4353 : 0.44950205087661743\n",
      "Training loss for batch 4354 : 0.3561922013759613\n",
      "Training loss for batch 4355 : 0.38890475034713745\n",
      "Training loss for batch 4356 : 0.1685856133699417\n",
      "Training loss for batch 4357 : 0.28268247842788696\n",
      "Training loss for batch 4358 : 0.21925099194049835\n",
      "Training loss for batch 4359 : 0.1625930219888687\n",
      "Training loss for batch 4360 : 0.1741110235452652\n",
      "Training loss for batch 4361 : 0.07472947984933853\n",
      "Training loss for batch 4362 : 0.06814544647932053\n",
      "Training loss for batch 4363 : 0.07597943395376205\n",
      "Training loss for batch 4364 : 0.19929474592208862\n",
      "Training loss for batch 4365 : 0.06364724040031433\n",
      "Training loss for batch 4366 : 0.6480902433395386\n",
      "Training loss for batch 4367 : 0.234403595328331\n",
      "Training loss for batch 4368 : 0.010865532793104649\n",
      "Training loss for batch 4369 : 0.2622084319591522\n",
      "Training loss for batch 4370 : 0.2161864936351776\n",
      "Training loss for batch 4371 : 0.09618514031171799\n",
      "Training loss for batch 4372 : 0.06480325013399124\n",
      "Training loss for batch 4373 : 0.02238103561103344\n",
      "Training loss for batch 4374 : 0.0747600793838501\n",
      "Training loss for batch 4375 : 0.10814336687326431\n",
      "Training loss for batch 4376 : 0.5455443859100342\n",
      "Training loss for batch 4377 : 0.5315500497817993\n",
      "Training loss for batch 4378 : 0.021380558609962463\n",
      "Training loss for batch 4379 : 0.719075083732605\n",
      "Training loss for batch 4380 : 0.36355382204055786\n",
      "Training loss for batch 4381 : 0.03500761836767197\n",
      "Training loss for batch 4382 : 0.33348751068115234\n",
      "Training loss for batch 4383 : 0.09251106530427933\n",
      "Training loss for batch 4384 : 0.43085864186286926\n",
      "Training loss for batch 4385 : 0.044867996126413345\n",
      "Training loss for batch 4386 : 0.45048823952674866\n",
      "Training loss for batch 4387 : 0.3299100995063782\n",
      "Training loss for batch 4388 : 0.10616293549537659\n",
      "Training loss for batch 4389 : 0.7121452689170837\n",
      "Training loss for batch 4390 : 0.00793864019215107\n",
      "Training loss for batch 4391 : 0.3011088967323303\n",
      "Training loss for batch 4392 : 0.36636877059936523\n",
      "Training loss for batch 4393 : 0.1803538203239441\n",
      "Training loss for batch 4394 : 0.11712659150362015\n",
      "Training loss for batch 4395 : 0.08269055932760239\n",
      "Training loss for batch 4396 : 0.26886168122291565\n",
      "Training loss for batch 4397 : 0.581430971622467\n",
      "Training loss for batch 4398 : 0.0356028787791729\n",
      "Training loss for batch 4399 : 0.2542317807674408\n",
      "Training loss for batch 4400 : 0.39744722843170166\n",
      "Training loss for batch 4401 : 0.362521231174469\n",
      "Training loss for batch 4402 : 0.3019523620605469\n",
      "Training loss for batch 4403 : 0.2932402193546295\n",
      "Training loss for batch 4404 : 0.40221327543258667\n",
      "Training loss for batch 4405 : 0.238148033618927\n",
      "Training loss for batch 4406 : 0.40776872634887695\n",
      "Training loss for batch 4407 : 0.90740567445755\n",
      "Training loss for batch 4408 : 0.462585985660553\n",
      "Training loss for batch 4409 : 0.6016318202018738\n",
      "Training loss for batch 4410 : 0.2262112945318222\n",
      "Training loss for batch 4411 : 0.3852807581424713\n",
      "Training loss for batch 4412 : 0.1063983291387558\n",
      "Training loss for batch 4413 : 0.12026410549879074\n",
      "Training loss for batch 4414 : 0.4956684112548828\n",
      "Training loss for batch 4415 : 0.24822933971881866\n",
      "Training loss for batch 4416 : 0.25565269589424133\n",
      "Training loss for batch 4417 : 0.18779219686985016\n",
      "Training loss for batch 4418 : 0.07283035665750504\n",
      "Training loss for batch 4419 : 0.05782759562134743\n",
      "Training loss for batch 4420 : 0.33882713317871094\n",
      "Training loss for batch 4421 : 0.31890594959259033\n",
      "Training loss for batch 4422 : 0.25070467591285706\n",
      "Training loss for batch 4423 : 0.36790725588798523\n",
      "Training loss for batch 4424 : 0.14327959716320038\n",
      "Training loss for batch 4425 : 0.02558157406747341\n",
      "Training loss for batch 4426 : 0.3817461431026459\n",
      "Training loss for batch 4427 : 0.5388323664665222\n",
      "Training loss for batch 4428 : 0.29918956756591797\n",
      "Training loss for batch 4429 : 0.026983335614204407\n",
      "Training loss for batch 4430 : 0.3128982484340668\n",
      "Training loss for batch 4431 : 0.06907278299331665\n",
      "Training loss for batch 4432 : 0.4052959978580475\n",
      "Training loss for batch 4433 : 0.2870197296142578\n",
      "Training loss for batch 4434 : 0.6702285408973694\n",
      "Training loss for batch 4435 : 0.24020135402679443\n",
      "Training loss for batch 4436 : 0.2067977786064148\n",
      "Training loss for batch 4437 : 0.3273303806781769\n",
      "Training loss for batch 4438 : 0.7323607206344604\n",
      "Training loss for batch 4439 : 0.016148608177900314\n",
      "Training loss for batch 4440 : 0.4783346354961395\n",
      "Training loss for batch 4441 : 0.04672394320368767\n",
      "Training loss for batch 4442 : 0.042066849768161774\n",
      "Training loss for batch 4443 : 0.018744181841611862\n",
      "Training loss for batch 4444 : 0.3331630229949951\n",
      "Training loss for batch 4445 : 0.09561029076576233\n",
      "Training loss for batch 4446 : 0.3767661154270172\n",
      "Training loss for batch 4447 : 0.2010723352432251\n",
      "Training loss for batch 4448 : 0.12373633682727814\n",
      "Training loss for batch 4449 : 0.4492504894733429\n",
      "Training loss for batch 4450 : 0.19108186662197113\n",
      "Training loss for batch 4451 : 0.15422067046165466\n",
      "Training loss for batch 4452 : 0.0711631178855896\n",
      "Training loss for batch 4453 : 0.08076362311840057\n",
      "Training loss for batch 4454 : 0.1513545960187912\n",
      "Training loss for batch 4455 : 0.0658804401755333\n",
      "Training loss for batch 4456 : 0.22216640412807465\n",
      "Training loss for batch 4457 : 0.005018974654376507\n",
      "Training loss for batch 4458 : 0.08049199730157852\n",
      "Training loss for batch 4459 : 0.14196093380451202\n",
      "Training loss for batch 4460 : 0.08337828516960144\n",
      "Training loss for batch 4461 : 0.08106854557991028\n",
      "Training loss for batch 4462 : 4.889676347374916e-05\n",
      "Training loss for batch 4463 : 0.10912603884935379\n",
      "Training loss for batch 4464 : 0.3902783691883087\n",
      "Training loss for batch 4465 : 0.12561647593975067\n",
      "Training loss for batch 4466 : 0.4718146324157715\n",
      "Training loss for batch 4467 : 0.04358413442969322\n",
      "Training loss for batch 4468 : 0.10365863144397736\n",
      "Training loss for batch 4469 : 0.19557695090770721\n",
      "Training loss for batch 4470 : 0.2034769058227539\n",
      "Training loss for batch 4471 : 0.007167726755142212\n",
      "Training loss for batch 4472 : 0.31892621517181396\n",
      "Training loss for batch 4473 : 0.1331304907798767\n",
      "Training loss for batch 4474 : 0.5759157538414001\n",
      "Training loss for batch 4475 : 0.29776516556739807\n",
      "Training loss for batch 4476 : 0.06480032950639725\n",
      "Training loss for batch 4477 : 0.4852227568626404\n",
      "Training loss for batch 4478 : 0.027453944087028503\n",
      "Training loss for batch 4479 : 0.2803497016429901\n",
      "Training loss for batch 4480 : 0.03359716385602951\n",
      "Training loss for batch 4481 : 0.1431187093257904\n",
      "Training loss for batch 4482 : 0.3761521279811859\n",
      "Training loss for batch 4483 : 0.08282206952571869\n",
      "Training loss for batch 4484 : 0.22551293671131134\n",
      "Training loss for batch 4485 : 0.10166038572788239\n",
      "Training loss for batch 4486 : 0.1541193425655365\n",
      "Training loss for batch 4487 : 0.17324937880039215\n",
      "Training loss for batch 4488 : 0.2750979959964752\n",
      "Training loss for batch 4489 : 0.15630680322647095\n",
      "Training loss for batch 4490 : 0.42384058237075806\n",
      "Training loss for batch 4491 : 0.08057147264480591\n",
      "Training loss for batch 4492 : 0.5851321816444397\n",
      "Training loss for batch 4493 : 0.12956902384757996\n",
      "Training loss for batch 4494 : 0.47886401414871216\n",
      "Training loss for batch 4495 : 0.046609193086624146\n",
      "Training loss for batch 4496 : 0.061309874057769775\n",
      "Training loss for batch 4497 : 0.3183927536010742\n",
      "Training loss for batch 4498 : 0.17782680690288544\n",
      "Training loss for batch 4499 : 0.6345176696777344\n",
      "Training loss for batch 4500 : 0.18488910794258118\n",
      "Training loss for batch 4501 : 0.1908302903175354\n",
      "Training loss for batch 4502 : 0.42465683817863464\n",
      "Training loss for batch 4503 : 0.0964822918176651\n",
      "Training loss for batch 4504 : 0.42943280935287476\n",
      "Training loss for batch 4505 : 0.35562974214553833\n",
      "Training loss for batch 4506 : 0.20318730175495148\n",
      "Training loss for batch 4507 : 0.3213631510734558\n",
      "Training loss for batch 4508 : 0.6644576191902161\n",
      "Training loss for batch 4509 : 0.6573167443275452\n",
      "Training loss for batch 4510 : 0.3226962089538574\n",
      "Training loss for batch 4511 : 0.11970347166061401\n",
      "Training loss for batch 4512 : 0.09291825443506241\n",
      "Training loss for batch 4513 : 0.048193447291851044\n",
      "Training loss for batch 4514 : 0.31781747937202454\n",
      "Training loss for batch 4515 : 1.1019681692123413\n",
      "Training loss for batch 4516 : 0.3576304316520691\n",
      "Training loss for batch 4517 : 0.42235618829727173\n",
      "Training loss for batch 4518 : 0.011555890552699566\n",
      "Training loss for batch 4519 : 0.15748025476932526\n",
      "Training loss for batch 4520 : 0.6439314484596252\n",
      "Training loss for batch 4521 : 0.26884686946868896\n",
      "Training loss for batch 4522 : 0.6480286121368408\n",
      "Training loss for batch 4523 : 0.5991747975349426\n",
      "Training loss for batch 4524 : 0.7789640426635742\n",
      "Training loss for batch 4525 : 0.1525232195854187\n",
      "Training loss for batch 4526 : 0.5982993245124817\n",
      "Training loss for batch 4527 : 0.2639198303222656\n",
      "Training loss for batch 4528 : 0.005661770701408386\n",
      "Training loss for batch 4529 : 0.20217446982860565\n",
      "Training loss for batch 4530 : 0.11042923480272293\n",
      "Training loss for batch 4531 : 0.27771270275115967\n",
      "Training loss for batch 4532 : 0.06399166584014893\n",
      "Training loss for batch 4533 : 0.1878262609243393\n",
      "Training loss for batch 4534 : 0.2752421200275421\n",
      "Training loss for batch 4535 : 0.41450610756874084\n",
      "Training loss for batch 4536 : 0.25744184851646423\n",
      "Training loss for batch 4537 : 0.0\n",
      "Training loss for batch 4538 : 0.2574980556964874\n",
      "Training loss for batch 4539 : 0.4094197750091553\n",
      "Training loss for batch 4540 : 0.08766964077949524\n",
      "Training loss for batch 4541 : 0.3007296919822693\n",
      "Training loss for batch 4542 : 0.31909769773483276\n",
      "Training loss for batch 4543 : 0.3657603859901428\n",
      "Training loss for batch 4544 : 0.29913705587387085\n",
      "Training loss for batch 4545 : 0.28602516651153564\n",
      "Training loss for batch 4546 : 0.3849816620349884\n",
      "Training loss for batch 4547 : 0.17519111931324005\n",
      "Training loss for batch 4548 : 0.4798544943332672\n",
      "Training loss for batch 4549 : 0.11629892140626907\n",
      "Training loss for batch 4550 : 0.44578686356544495\n",
      "Training loss for batch 4551 : 0.033394817262887955\n",
      "Training loss for batch 4552 : 0.13065654039382935\n",
      "Training loss for batch 4553 : 0.15938660502433777\n",
      "Training loss for batch 4554 : 0.20281460881233215\n",
      "Training loss for batch 4555 : 0.09160332381725311\n",
      "Training loss for batch 4556 : 0.02766481228172779\n",
      "Training loss for batch 4557 : 0.141897514462471\n",
      "Training loss for batch 4558 : 0.33133426308631897\n",
      "Training loss for batch 4559 : 0.5548600554466248\n",
      "Training loss for batch 4560 : 0.5783308744430542\n",
      "Training loss for batch 4561 : 0.6548585891723633\n",
      "Training loss for batch 4562 : 0.05225156620144844\n",
      "Training loss for batch 4563 : 0.08404126018285751\n",
      "Training loss for batch 4564 : 0.2495192140340805\n",
      "Training loss for batch 4565 : 0.3043556809425354\n",
      "Training loss for batch 4566 : 0.041664496064186096\n",
      "Training loss for batch 4567 : 0.03716713935136795\n",
      "Training loss for batch 4568 : 0.029798859730362892\n",
      "Training loss for batch 4569 : 0.5292060971260071\n",
      "Training loss for batch 4570 : 0.14150698482990265\n",
      "Training loss for batch 4571 : 0.03191174194216728\n",
      "Training loss for batch 4572 : 0.2647869884967804\n",
      "Training loss for batch 4573 : 0.38769614696502686\n",
      "Training loss for batch 4574 : 0.0842491164803505\n",
      "Training loss for batch 4575 : 0.1169586330652237\n",
      "Training loss for batch 4576 : 0.11756320297718048\n",
      "Training loss for batch 4577 : 0.13575874269008636\n",
      "Training loss for batch 4578 : 0.5199507474899292\n",
      "Training loss for batch 4579 : 0.4592180848121643\n",
      "Training loss for batch 4580 : 0.2713753581047058\n",
      "Training loss for batch 4581 : 0.21075044572353363\n",
      "Training loss for batch 4582 : 0.02709822542965412\n",
      "Training loss for batch 4583 : 0.1470012217760086\n",
      "Training loss for batch 4584 : 0.22085176408290863\n",
      "Training loss for batch 4585 : 0.2605353593826294\n",
      "Training loss for batch 4586 : 0.181559756398201\n",
      "Training loss for batch 4587 : 0.004496768116950989\n",
      "Training loss for batch 4588 : 0.1475394070148468\n",
      "Training loss for batch 4589 : 0.023304572328925133\n",
      "Training loss for batch 4590 : 0.2315889447927475\n",
      "Training loss for batch 4591 : 0.21512800455093384\n",
      "Training loss for batch 4592 : 0.37489181756973267\n",
      "Training loss for batch 4593 : 0.5884326696395874\n",
      "Training loss for batch 4594 : 0.15152104198932648\n",
      "Training loss for batch 4595 : 0.07706430554389954\n",
      "Training loss for batch 4596 : 0.007144956849515438\n",
      "Training loss for batch 4597 : 0.3607080280780792\n",
      "Training loss for batch 4598 : 0.005833419039845467\n",
      "Training loss for batch 4599 : 0.04236606881022453\n",
      "Training loss for batch 4600 : 0.5141916275024414\n",
      "Training loss for batch 4601 : 0.17267559468746185\n",
      "Training loss for batch 4602 : 0.0\n",
      "Training loss for batch 4603 : 0.368154376745224\n",
      "Training loss for batch 4604 : 0.23021423816680908\n",
      "Training loss for batch 4605 : 0.24650907516479492\n",
      "Training loss for batch 4606 : 0.13764990866184235\n",
      "Training loss for batch 4607 : 0.0\n",
      "Training loss for batch 4608 : 0.5423385500907898\n",
      "Training loss for batch 4609 : 0.4360463321208954\n",
      "Training loss for batch 4610 : 0.05238454416394234\n",
      "Training loss for batch 4611 : 0.21905265748500824\n",
      "Training loss for batch 4612 : 0.44727393984794617\n",
      "Training loss for batch 4613 : 0.22027716040611267\n",
      "Training loss for batch 4614 : 0.3481752276420593\n",
      "Training loss for batch 4615 : 0.1174197569489479\n",
      "Training loss for batch 4616 : 0.32686519622802734\n",
      "Training loss for batch 4617 : 0.1743430197238922\n",
      "Training loss for batch 4618 : 0.36326509714126587\n",
      "Training loss for batch 4619 : 0.11745694279670715\n",
      "Training loss for batch 4620 : 0.24912136793136597\n",
      "Training loss for batch 4621 : 0.10284721106290817\n",
      "Training loss for batch 4622 : 0.17463646829128265\n",
      "Training loss for batch 4623 : 0.3485402762889862\n",
      "Training loss for batch 4624 : 0.03176712617278099\n",
      "Training loss for batch 4625 : 0.3983249068260193\n",
      "Training loss for batch 4626 : 0.22934997081756592\n",
      "Training loss for batch 4627 : 0.13885048031806946\n",
      "Training loss for batch 4628 : 0.11168408393859863\n",
      "Training loss for batch 4629 : 0.21048499643802643\n",
      "Training loss for batch 4630 : 0.5212777853012085\n",
      "Training loss for batch 4631 : 0.34831738471984863\n",
      "Training loss for batch 4632 : 0.3121947944164276\n",
      "Training loss for batch 4633 : 0.630721926689148\n",
      "Training loss for batch 4634 : 0.5145406126976013\n",
      "Training loss for batch 4635 : 0.06059585139155388\n",
      "Training loss for batch 4636 : 0.447223037481308\n",
      "Training loss for batch 4637 : 0.061715707182884216\n",
      "Training loss for batch 4638 : 0.6210994124412537\n",
      "Training loss for batch 4639 : 0.5873556137084961\n",
      "Training loss for batch 4640 : 0.04966982454061508\n",
      "Training loss for batch 4641 : 0.4967797100543976\n",
      "Training loss for batch 4642 : 0.193790003657341\n",
      "Training loss for batch 4643 : 0.22141623497009277\n",
      "Training loss for batch 4644 : 0.6642800569534302\n",
      "Training loss for batch 4645 : 0.025494037196040154\n",
      "Training loss for batch 4646 : 0.2456498146057129\n",
      "Training loss for batch 4647 : 0.14029726386070251\n",
      "Training loss for batch 4648 : 0.008128887042403221\n",
      "Training loss for batch 4649 : 0.6611821055412292\n",
      "Training loss for batch 4650 : 0.02369081974029541\n",
      "Training loss for batch 4651 : 0.017358889803290367\n",
      "Training loss for batch 4652 : 0.022836508229374886\n",
      "Training loss for batch 4653 : 0.17078079283237457\n",
      "Training loss for batch 4654 : 0.7309597134590149\n",
      "Training loss for batch 4655 : 0.14449399709701538\n",
      "Training loss for batch 4656 : 0.17559266090393066\n",
      "Training loss for batch 4657 : 0.18273374438285828\n",
      "Training loss for batch 4658 : 0.3250366449356079\n",
      "Training loss for batch 4659 : 0.09325844049453735\n",
      "Training loss for batch 4660 : 0.41427910327911377\n",
      "Training loss for batch 4661 : 0.10695242136716843\n",
      "Training loss for batch 4662 : 0.11343991756439209\n",
      "Training loss for batch 4663 : 0.20598088204860687\n",
      "Training loss for batch 4664 : 0.27676114439964294\n",
      "Training loss for batch 4665 : 0.24237877130508423\n",
      "Training loss for batch 4666 : 0.36616799235343933\n",
      "Training loss for batch 4667 : 0.5500011444091797\n",
      "Training loss for batch 4668 : 0.08610081672668457\n",
      "Training loss for batch 4669 : 0.29986771941185\n",
      "Training loss for batch 4670 : 0.3245338201522827\n",
      "Training loss for batch 4671 : 0.2692774534225464\n",
      "Training loss for batch 4672 : 0.13154315948486328\n",
      "Training loss for batch 4673 : 0.32308584451675415\n",
      "Training loss for batch 4674 : 0.3775603771209717\n",
      "Training loss for batch 4675 : 0.4520283043384552\n",
      "Training loss for batch 4676 : 0.05122789740562439\n",
      "Training loss for batch 4677 : 0.06749400496482849\n",
      "Training loss for batch 4678 : 0.045620452612638474\n",
      "Training loss for batch 4679 : 0.5016747713088989\n",
      "Training loss for batch 4680 : 0.34325897693634033\n",
      "Training loss for batch 4681 : 0.23073014616966248\n",
      "Training loss for batch 4682 : 0.2959898114204407\n",
      "Training loss for batch 4683 : 0.17886891961097717\n",
      "Training loss for batch 4684 : 0.08163192868232727\n",
      "Training loss for batch 4685 : 0.013552259653806686\n",
      "Training loss for batch 4686 : 0.16817350685596466\n",
      "Training loss for batch 4687 : 0.3129941523075104\n",
      "Training loss for batch 4688 : 0.04214154928922653\n",
      "Training loss for batch 4689 : 0.6221297979354858\n",
      "Training loss for batch 4690 : 0.06577886641025543\n",
      "Training loss for batch 4691 : 0.023358412086963654\n",
      "Training loss for batch 4692 : 0.34935253858566284\n",
      "Training loss for batch 4693 : 0.3677246570587158\n",
      "Training loss for batch 4694 : 0.6535477638244629\n",
      "Training loss for batch 4695 : 0.3663775622844696\n",
      "Training loss for batch 4696 : 0.38268643617630005\n",
      "Training loss for batch 4697 : 0.5367556810379028\n",
      "Training loss for batch 4698 : 0.38462239503860474\n",
      "Training loss for batch 4699 : 0.5417189598083496\n",
      "Training loss for batch 4700 : 0.10850759595632553\n",
      "Training loss for batch 4701 : 0.404442697763443\n",
      "Training loss for batch 4702 : 0.20566102862358093\n",
      "Training loss for batch 4703 : 0.008263945579528809\n",
      "Training loss for batch 4704 : 0.3379981517791748\n",
      "Training loss for batch 4705 : 0.16335298120975494\n",
      "Training loss for batch 4706 : 0.10378295183181763\n",
      "Training loss for batch 4707 : 0.4812016487121582\n",
      "Training loss for batch 4708 : 0.11422635614871979\n",
      "Training loss for batch 4709 : 0.04384642839431763\n",
      "Training loss for batch 4710 : 0.40008771419525146\n",
      "Training loss for batch 4711 : 0.26346948742866516\n",
      "Training loss for batch 4712 : 0.3752153515815735\n",
      "Training loss for batch 4713 : 0.36390793323516846\n",
      "Training loss for batch 4714 : 0.16341660916805267\n",
      "Training loss for batch 4715 : 0.307547003030777\n",
      "Training loss for batch 4716 : 0.057747986167669296\n",
      "Training loss for batch 4717 : 0.10795371234416962\n",
      "Training loss for batch 4718 : 0.4006052613258362\n",
      "Training loss for batch 4719 : 0.4695218503475189\n",
      "Training loss for batch 4720 : 0.3617299795150757\n",
      "Training loss for batch 4721 : 0.38626712560653687\n",
      "Training loss for batch 4722 : 0.0630224347114563\n",
      "Training loss for batch 4723 : 0.42472976446151733\n",
      "Training loss for batch 4724 : 0.49091455340385437\n",
      "Training loss for batch 4725 : 0.09314874559640884\n",
      "Training loss for batch 4726 : 0.4475778341293335\n",
      "Training loss for batch 4727 : 0.18242788314819336\n",
      "Training loss for batch 4728 : 0.06533071398735046\n",
      "Training loss for batch 4729 : 0.3972345292568207\n",
      "Training loss for batch 4730 : 0.22838883101940155\n",
      "Training loss for batch 4731 : 0.06828071177005768\n",
      "Training loss for batch 4732 : 0.23091639578342438\n",
      "Training loss for batch 4733 : 0.48883458971977234\n",
      "Training loss for batch 4734 : 0.16114263236522675\n",
      "Training loss for batch 4735 : 0.5877848863601685\n",
      "Training loss for batch 4736 : 0.3566928207874298\n",
      "Training loss for batch 4737 : 0.1052779108285904\n",
      "Training loss for batch 4738 : 0.3220426142215729\n",
      "Training loss for batch 4739 : 0.12246200442314148\n",
      "Training loss for batch 4740 : 0.7920013666152954\n",
      "Training loss for batch 4741 : 0.10791920125484467\n",
      "Training loss for batch 4742 : 0.0977087914943695\n",
      "Training loss for batch 4743 : 0.10796109586954117\n",
      "Training loss for batch 4744 : 0.15988227725028992\n",
      "Training loss for batch 4745 : 0.44528689980506897\n",
      "Training loss for batch 4746 : 0.09553758054971695\n",
      "Training loss for batch 4747 : 0.25685107707977295\n",
      "Training loss for batch 4748 : 0.1351996809244156\n",
      "Training loss for batch 4749 : 0.20149880647659302\n",
      "Training loss for batch 4750 : 0.4699334502220154\n",
      "Training loss for batch 4751 : 0.11381039023399353\n",
      "Training loss for batch 4752 : 0.13705816864967346\n",
      "Training loss for batch 4753 : 0.5644248127937317\n",
      "Training loss for batch 4754 : 0.4232809543609619\n",
      "Training loss for batch 4755 : 0.3987460136413574\n",
      "Training loss for batch 4756 : 0.33192408084869385\n",
      "Training loss for batch 4757 : 0.12421352416276932\n",
      "Training loss for batch 4758 : 0.058910537511110306\n",
      "Training loss for batch 4759 : 0.47348323464393616\n",
      "Training loss for batch 4760 : 0.3325488567352295\n",
      "Training loss for batch 4761 : 0.11858120560646057\n",
      "Training loss for batch 4762 : 0.1974465399980545\n",
      "Training loss for batch 4763 : 0.10202043503522873\n",
      "Training loss for batch 4764 : 0.14462608098983765\n",
      "Training loss for batch 4765 : 0.2514141798019409\n",
      "Training loss for batch 4766 : 0.18855704367160797\n",
      "Training loss for batch 4767 : 0.2682035267353058\n",
      "Training loss for batch 4768 : 0.13984991610050201\n",
      "Training loss for batch 4769 : 0.1919611394405365\n",
      "Training loss for batch 4770 : 0.04355979338288307\n",
      "Training loss for batch 4771 : 0.34838640689849854\n",
      "Training loss for batch 4772 : 0.5568611025810242\n",
      "Training loss for batch 4773 : 0.08234341442584991\n",
      "Training loss for batch 4774 : 0.2932177782058716\n",
      "Training loss for batch 4775 : 0.0634087547659874\n",
      "Training loss for batch 4776 : 0.09484153985977173\n",
      "Training loss for batch 4777 : 0.04090264067053795\n",
      "Training loss for batch 4778 : 0.1797679364681244\n",
      "Training loss for batch 4779 : 0.3835481107234955\n",
      "Training loss for batch 4780 : 0.1255403608083725\n",
      "Training loss for batch 4781 : 0.0854136124253273\n",
      "Training loss for batch 4782 : 3.8743019104003906e-05\n",
      "Training loss for batch 4783 : 0.1508987993001938\n",
      "Training loss for batch 4784 : 0.004067867994308472\n",
      "Training loss for batch 4785 : 0.25824227929115295\n",
      "Training loss for batch 4786 : 0.25475525856018066\n",
      "Training loss for batch 4787 : 0.5520309805870056\n",
      "Training loss for batch 4788 : 0.16323897242546082\n",
      "Training loss for batch 4789 : 0.06862668693065643\n",
      "Training loss for batch 4790 : 0.2684406042098999\n",
      "Training loss for batch 4791 : 0.2879074811935425\n",
      "Training loss for batch 4792 : 0.11315303295850754\n",
      "Training loss for batch 4793 : 0.09424582123756409\n",
      "Training loss for batch 4794 : 0.3256669044494629\n",
      "Training loss for batch 4795 : 0.3181936740875244\n",
      "Training loss for batch 4796 : 1.1238744258880615\n",
      "Training loss for batch 4797 : 0.6962296962738037\n",
      "Training loss for batch 4798 : 0.12729240953922272\n",
      "Training loss for batch 4799 : 0.12075210362672806\n",
      "Training loss for batch 4800 : 0.41051769256591797\n",
      "Training loss for batch 4801 : 0.30846330523490906\n",
      "Training loss for batch 4802 : 0.03368455171585083\n",
      "Training loss for batch 4803 : 0.8018093109130859\n",
      "Training loss for batch 4804 : 0.13029605150222778\n",
      "Training loss for batch 4805 : 0.6476149559020996\n",
      "Training loss for batch 4806 : 0.06479303538799286\n",
      "Training loss for batch 4807 : 0.02024368941783905\n",
      "Training loss for batch 4808 : 0.29754719138145447\n",
      "Training loss for batch 4809 : 0.34734031558036804\n",
      "Training loss for batch 4810 : 0.6220448613166809\n",
      "Training loss for batch 4811 : 0.8136570453643799\n",
      "Training loss for batch 4812 : 0.34345772862434387\n",
      "Training loss for batch 4813 : 0.10929539054632187\n",
      "Training loss for batch 4814 : 0.1743910312652588\n",
      "Training loss for batch 4815 : 0.17455485463142395\n",
      "Training loss for batch 4816 : 0.1911953240633011\n",
      "Training loss for batch 4817 : 0.055761754512786865\n",
      "Training loss for batch 4818 : 0.28352347016334534\n",
      "Training loss for batch 4819 : 0.47074005007743835\n",
      "Training loss for batch 4820 : 0.31117045879364014\n",
      "Training loss for batch 4821 : 0.18067939579486847\n",
      "Training loss for batch 4822 : 0.2871145009994507\n",
      "Training loss for batch 4823 : 0.5499445199966431\n",
      "Training loss for batch 4824 : 0.14328117668628693\n",
      "Training loss for batch 4825 : 0.3971741199493408\n",
      "Training loss for batch 4826 : 0.09008098393678665\n",
      "Training loss for batch 4827 : 0.34277182817459106\n",
      "Training loss for batch 4828 : 0.3862434923648834\n",
      "Training loss for batch 4829 : 0.25929272174835205\n",
      "Training loss for batch 4830 : 0.1703120768070221\n",
      "Training loss for batch 4831 : 0.38854125142097473\n",
      "Training loss for batch 4832 : 0.36620867252349854\n",
      "Training loss for batch 4833 : 0.13154225051403046\n",
      "Training loss for batch 4834 : 0.009068300947546959\n",
      "Training loss for batch 4835 : 0.25112539529800415\n",
      "Training loss for batch 4836 : 0.13224171102046967\n",
      "Training loss for batch 4837 : 0.0547560378909111\n",
      "Training loss for batch 4838 : 0.5961762070655823\n",
      "Training loss for batch 4839 : 0.13217559456825256\n",
      "Training loss for batch 4840 : 0.33653318881988525\n",
      "Training loss for batch 4841 : 0.061407096683979034\n",
      "Training loss for batch 4842 : 0.2253010869026184\n",
      "Training loss for batch 4843 : 0.42860841751098633\n",
      "Training loss for batch 4844 : 0.0797065794467926\n",
      "Training loss for batch 4845 : 0.5150666236877441\n",
      "Training loss for batch 4846 : 0.13853470981121063\n",
      "Training loss for batch 4847 : 0.7066633105278015\n",
      "Training loss for batch 4848 : 0.2427895963191986\n",
      "Training loss for batch 4849 : 0.1452840119600296\n",
      "Training loss for batch 4850 : 0.15032915771007538\n",
      "Training loss for batch 4851 : 0.22353801131248474\n",
      "Training loss for batch 4852 : 0.2160571813583374\n",
      "Training loss for batch 4853 : 0.07300595939159393\n",
      "Training loss for batch 4854 : 0.19208654761314392\n",
      "Training loss for batch 4855 : 0.41052088141441345\n",
      "Training loss for batch 4856 : 0.6522161960601807\n",
      "Training loss for batch 4857 : 0.08547763526439667\n",
      "Training loss for batch 4858 : 0.36098119616508484\n",
      "Training loss for batch 4859 : 0.5431097745895386\n",
      "Training loss for batch 4860 : 0.16485682129859924\n",
      "Training loss for batch 4861 : 0.27559036016464233\n",
      "Training loss for batch 4862 : 0.00019119036733172834\n",
      "Training loss for batch 4863 : 0.027312885969877243\n",
      "Training loss for batch 4864 : 0.21204368770122528\n",
      "Training loss for batch 4865 : 0.4821333885192871\n",
      "Training loss for batch 4866 : 0.335808664560318\n",
      "Training loss for batch 4867 : 0.4473777115345001\n",
      "Training loss for batch 4868 : 0.07103418558835983\n",
      "Training loss for batch 4869 : 0.008826998062431812\n",
      "Training loss for batch 4870 : 0.4563075602054596\n",
      "Training loss for batch 4871 : 0.0789378210902214\n",
      "Training loss for batch 4872 : 0.24372491240501404\n",
      "Training loss for batch 4873 : 0.5077260136604309\n",
      "Training loss for batch 4874 : 0.2600417137145996\n",
      "Training loss for batch 4875 : 0.35058069229125977\n",
      "Training loss for batch 4876 : 0.12081538140773773\n",
      "Training loss for batch 4877 : 0.14050139486789703\n",
      "Training loss for batch 4878 : 0.11591264605522156\n",
      "Training loss for batch 4879 : 0.23367349803447723\n",
      "Training loss for batch 4880 : 0.0966438427567482\n",
      "Training loss for batch 4881 : 0.06920652836561203\n",
      "Training loss for batch 4882 : 0.6303226947784424\n",
      "Training loss for batch 4883 : 0.1890987753868103\n",
      "Training loss for batch 4884 : 0.07566865533590317\n",
      "Training loss for batch 4885 : 0.00908611062914133\n",
      "Training loss for batch 4886 : 0.29741352796554565\n",
      "Training loss for batch 4887 : 0.1626153588294983\n",
      "Training loss for batch 4888 : 0.2554827630519867\n",
      "Training loss for batch 4889 : 0.18224118649959564\n",
      "Training loss for batch 4890 : 0.3799445927143097\n",
      "Training loss for batch 4891 : 0.2608231008052826\n",
      "Training loss for batch 4892 : 0.3256264925003052\n",
      "Training loss for batch 4893 : 0.17468249797821045\n",
      "Training loss for batch 4894 : 0.38126587867736816\n",
      "Training loss for batch 4895 : 0.3896111249923706\n",
      "Training loss for batch 4896 : 0.21914760768413544\n",
      "Training loss for batch 4897 : 0.2851860523223877\n",
      "Training loss for batch 4898 : 0.2980254888534546\n",
      "Training loss for batch 4899 : 0.1947951763868332\n",
      "Training loss for batch 4900 : 0.1496129333972931\n",
      "Training loss for batch 4901 : 0.06843075901269913\n",
      "Training loss for batch 4902 : 0.1746649295091629\n",
      "Training loss for batch 4903 : 0.6100811958312988\n",
      "Training loss for batch 4904 : 0.13554516434669495\n",
      "Training loss for batch 4905 : 0.2744452953338623\n",
      "Training loss for batch 4906 : 0.05567280575633049\n",
      "Training loss for batch 4907 : 0.06665018945932388\n",
      "Training loss for batch 4908 : 0.1138463094830513\n",
      "Training loss for batch 4909 : 0.2037670612335205\n",
      "Training loss for batch 4910 : 0.09731275588274002\n",
      "Training loss for batch 4911 : 0.344370573759079\n",
      "Training loss for batch 4912 : 0.03056582808494568\n",
      "Training loss for batch 4913 : 0.578641951084137\n",
      "Training loss for batch 4914 : 0.03682417795062065\n",
      "Training loss for batch 4915 : 0.34209978580474854\n",
      "Training loss for batch 4916 : 0.43698275089263916\n",
      "Training loss for batch 4917 : 0.4381304383277893\n",
      "Training loss for batch 4918 : 0.10022534430027008\n",
      "Training loss for batch 4919 : 0.21749787032604218\n",
      "Training loss for batch 4920 : 0.21317830681800842\n",
      "Training loss for batch 4921 : 0.48131635785102844\n",
      "Training loss for batch 4922 : 0.5473014712333679\n",
      "Training loss for batch 4923 : 0.08975794911384583\n",
      "Training loss for batch 4924 : 0.36800920963287354\n",
      "Training loss for batch 4925 : 0.14035233855247498\n",
      "Training loss for batch 4926 : 0.41882964968681335\n",
      "Training loss for batch 4927 : 0.17779704928398132\n",
      "Training loss for batch 4928 : 0.24211497604846954\n",
      "Training loss for batch 4929 : 0.015064222738146782\n",
      "Training loss for batch 4930 : 0.12492098659276962\n",
      "Training loss for batch 4931 : 0.1702156513929367\n",
      "Training loss for batch 4932 : 0.3045254647731781\n",
      "Training loss for batch 4933 : 0.16414454579353333\n",
      "Training loss for batch 4934 : 0.2487489879131317\n",
      "Training loss for batch 4935 : 0.3555499315261841\n",
      "Training loss for batch 4936 : 0.3558603823184967\n",
      "Training loss for batch 4937 : 0.4643736779689789\n",
      "Training loss for batch 4938 : 0.2505655586719513\n",
      "Training loss for batch 4939 : 0.21032695472240448\n",
      "Training loss for batch 4940 : 0.2835839092731476\n",
      "Training loss for batch 4941 : 0.6973831057548523\n",
      "Training loss for batch 4942 : 0.3139551281929016\n",
      "Training loss for batch 4943 : 0.335742324590683\n",
      "Training loss for batch 4944 : 0.39147964119911194\n",
      "Training loss for batch 4945 : 0.2448335736989975\n",
      "Training loss for batch 4946 : 0.2304944396018982\n",
      "Training loss for batch 4947 : 0.3500460386276245\n",
      "Training loss for batch 4948 : 0.3697417676448822\n",
      "Training loss for batch 4949 : 0.3286557197570801\n",
      "Training loss for batch 4950 : 0.27969780564308167\n",
      "Training loss for batch 4951 : 0.4125545918941498\n",
      "Training loss for batch 4952 : 0.07073027640581131\n",
      "Training loss for batch 4953 : 0.4563882350921631\n",
      "Training loss for batch 4954 : 0.07890255749225616\n",
      "Training loss for batch 4955 : 0.38302546739578247\n",
      "Training loss for batch 4956 : 0.05980585515499115\n",
      "Training loss for batch 4957 : 0.2288622409105301\n",
      "Training loss for batch 4958 : 0.09401792287826538\n",
      "Training loss for batch 4959 : 0.26527640223503113\n",
      "Training loss for batch 4960 : 0.26000702381134033\n",
      "Training loss for batch 4961 : 0.010349617339670658\n",
      "Training loss for batch 4962 : 0.1793186217546463\n",
      "Training loss for batch 4963 : 0.0257182028144598\n",
      "Training loss for batch 4964 : 0.13854454457759857\n",
      "Training loss for batch 4965 : 0.17565153539180756\n",
      "Training loss for batch 4966 : 0.3896709084510803\n",
      "Training loss for batch 4967 : 0.039269208908081055\n",
      "Training loss for batch 4968 : 0.29225754737854004\n",
      "Training loss for batch 4969 : 0.3960779905319214\n",
      "Training loss for batch 4970 : 0.05971045419573784\n",
      "Training loss for batch 4971 : 0.07813475281000137\n",
      "Training loss for batch 4972 : 0.05019132047891617\n",
      "Training loss for batch 4973 : 0.24564166367053986\n",
      "Training loss for batch 4974 : 0.3592327833175659\n",
      "Training loss for batch 4975 : 0.18246619403362274\n",
      "Training loss for batch 4976 : 0.5543341636657715\n",
      "Training loss for batch 4977 : 0.013057932257652283\n",
      "Training loss for batch 4978 : 0.04997917637228966\n",
      "Training loss for batch 4979 : 0.18183553218841553\n",
      "Training loss for batch 4980 : 0.020610317587852478\n",
      "Training loss for batch 4981 : 0.5761792063713074\n",
      "Training loss for batch 4982 : 0.1160091757774353\n",
      "Training loss for batch 4983 : 0.19203343987464905\n",
      "Training loss for batch 4984 : 0.2475387454032898\n",
      "Training loss for batch 4985 : 0.35873013734817505\n",
      "Training loss for batch 4986 : 0.6757681369781494\n",
      "Training loss for batch 4987 : 0.5380681753158569\n",
      "Training loss for batch 4988 : 0.16142231225967407\n",
      "Training loss for batch 4989 : 0.3756600022315979\n",
      "Training loss for batch 4990 : 0.23941797018051147\n",
      "Training loss for batch 4991 : 0.2102762907743454\n",
      "Training loss for batch 4992 : 0.7979844808578491\n",
      "Training loss for batch 4993 : 0.1988225132226944\n",
      "Training loss for batch 4994 : 0.2527132034301758\n",
      "Training loss for batch 4995 : 0.37670350074768066\n",
      "Training loss for batch 4996 : 0.026232989504933357\n",
      "Training loss for batch 4997 : 0.494901567697525\n",
      "Training loss for batch 4998 : 0.2820645272731781\n",
      "Training loss for batch 4999 : 0.5117629766464233\n",
      "Training loss for batch 5000 : 0.1917894035577774\n",
      "Training loss for batch 5001 : 0.305634081363678\n",
      "Training loss for batch 5002 : 0.1797911822795868\n",
      "Training loss for batch 5003 : 0.2599726617336273\n",
      "Training loss for batch 5004 : 0.4023818373680115\n",
      "Training loss for batch 5005 : 0.3926190733909607\n",
      "Training loss for batch 5006 : 0.16621319949626923\n",
      "Training loss for batch 5007 : 0.0005242051556706429\n",
      "Training loss for batch 5008 : 0.2489062249660492\n",
      "Training loss for batch 5009 : 0.1294793039560318\n",
      "Training loss for batch 5010 : 0.25492042303085327\n",
      "Training loss for batch 5011 : 0.33598628640174866\n",
      "Training loss for batch 5012 : 0.25362950563430786\n",
      "Training loss for batch 5013 : 0.15915392339229584\n",
      "Training loss for batch 5014 : 0.0\n",
      "Training loss for batch 5015 : 0.303083211183548\n",
      "Training loss for batch 5016 : 0.2021958976984024\n",
      "Training loss for batch 5017 : 0.1424488127231598\n",
      "Training loss for batch 5018 : 0.22905831038951874\n",
      "Training loss for batch 5019 : 0.07126905769109726\n",
      "Training loss for batch 5020 : 0.23152412474155426\n",
      "Training loss for batch 5021 : 0.02763073891401291\n",
      "Training loss for batch 5022 : 0.5013641119003296\n",
      "Training loss for batch 5023 : 0.21944351494312286\n",
      "Training loss for batch 5024 : 0.5952860116958618\n",
      "Training loss for batch 5025 : 0.3008289933204651\n",
      "Training loss for batch 5026 : 0.255686491727829\n",
      "Training loss for batch 5027 : 0.0001936741464305669\n",
      "Training loss for batch 5028 : 0.1804969161748886\n",
      "Training loss for batch 5029 : 0.5156421661376953\n",
      "Training loss for batch 5030 : 0.5297757983207703\n",
      "Training loss for batch 5031 : 0.18396663665771484\n",
      "Training loss for batch 5032 : 0.03707287088036537\n",
      "Training loss for batch 5033 : 0.10598047822713852\n",
      "Training loss for batch 5034 : 0.3008076846599579\n",
      "Training loss for batch 5035 : 0.9197171926498413\n",
      "Training loss for batch 5036 : 0.014940979890525341\n",
      "Training loss for batch 5037 : 0.23142772912979126\n",
      "Training loss for batch 5038 : 0.24782992899417877\n",
      "Training loss for batch 5039 : 0.15567554533481598\n",
      "Training loss for batch 5040 : 0.2611255645751953\n",
      "Training loss for batch 5041 : 0.4218439757823944\n",
      "Training loss for batch 5042 : 0.35539889335632324\n",
      "Training loss for batch 5043 : 0.382612943649292\n",
      "Training loss for batch 5044 : 0.1558893918991089\n",
      "Training loss for batch 5045 : 0.35291460156440735\n",
      "Training loss for batch 5046 : 0.425830215215683\n",
      "Training loss for batch 5047 : 0.05018520727753639\n",
      "Training loss for batch 5048 : 0.3984677195549011\n",
      "Training loss for batch 5049 : 0.5567219257354736\n",
      "Training loss for batch 5050 : 0.08115952461957932\n",
      "Training loss for batch 5051 : 0.2643575370311737\n",
      "Training loss for batch 5052 : 0.2694527506828308\n",
      "Training loss for batch 5053 : 0.6760696172714233\n",
      "Training loss for batch 5054 : 0.22743180394172668\n",
      "Training loss for batch 5055 : 0.05898299440741539\n",
      "Training loss for batch 5056 : 0.522155225276947\n",
      "Training loss for batch 5057 : 0.06908173114061356\n",
      "Training loss for batch 5058 : 0.14759942889213562\n",
      "Training loss for batch 5059 : 0.3663559854030609\n",
      "Training loss for batch 5060 : 0.0591544583439827\n",
      "Training loss for batch 5061 : 0.3041948080062866\n",
      "Training loss for batch 5062 : 0.046730488538742065\n",
      "Training loss for batch 5063 : 0.058789245784282684\n",
      "Training loss for batch 5064 : 0.6803176403045654\n",
      "Training loss for batch 5065 : 0.05357475206255913\n",
      "Training loss for batch 5066 : 0.025793304666876793\n",
      "Training loss for batch 5067 : 0.10187245905399323\n",
      "Training loss for batch 5068 : 0.37244337797164917\n",
      "Training loss for batch 5069 : 0.3152162432670593\n",
      "Training loss for batch 5070 : 0.11804318428039551\n",
      "Training loss for batch 5071 : 0.455300509929657\n",
      "Training loss for batch 5072 : 0.10660535097122192\n",
      "Training loss for batch 5073 : 0.756748616695404\n",
      "Training loss for batch 5074 : 0.5125311017036438\n",
      "Training loss for batch 5075 : 0.3100508153438568\n",
      "Training loss for batch 5076 : 0.007237707730382681\n",
      "Training loss for batch 5077 : 0.4307682514190674\n",
      "Training loss for batch 5078 : 0.1597709357738495\n",
      "Training loss for batch 5079 : 0.5045138597488403\n",
      "Training loss for batch 5080 : 0.4493757486343384\n",
      "Training loss for batch 5081 : 0.22959408164024353\n",
      "Training loss for batch 5082 : 0.494677871465683\n",
      "Training loss for batch 5083 : 0.43940529227256775\n",
      "Training loss for batch 5084 : 0.14391610026359558\n",
      "Training loss for batch 5085 : 0.19614756107330322\n",
      "Training loss for batch 5086 : 0.4050174355506897\n",
      "Training loss for batch 5087 : 0.0\n",
      "Training loss for batch 5088 : 0.7863173484802246\n",
      "Training loss for batch 5089 : 0.2554585039615631\n",
      "Training loss for batch 5090 : 0.09205199033021927\n",
      "Training loss for batch 5091 : 0.25130829215049744\n",
      "Training loss for batch 5092 : 0.5861827731132507\n",
      "Training loss for batch 5093 : 0.11447027325630188\n",
      "Training loss for batch 5094 : 0.07767125219106674\n",
      "Training loss for batch 5095 : 0.4833430349826813\n",
      "Training loss for batch 5096 : 0.054824307560920715\n",
      "Training loss for batch 5097 : 0.4327367842197418\n",
      "Training loss for batch 5098 : 0.1526547521352768\n",
      "Training loss for batch 5099 : 0.35245200991630554\n",
      "Training loss for batch 5100 : 0.42765355110168457\n",
      "Training loss for batch 5101 : 0.011260434985160828\n",
      "Training loss for batch 5102 : 0.46656087040901184\n",
      "Training loss for batch 5103 : 0.021950704976916313\n",
      "Training loss for batch 5104 : 0.11794836819171906\n",
      "Training loss for batch 5105 : 0.11149707436561584\n",
      "Training loss for batch 5106 : 0.4927157163619995\n",
      "Training loss for batch 5107 : 0.25746679306030273\n",
      "Training loss for batch 5108 : 0.5039734244346619\n",
      "Training loss for batch 5109 : 0.35071900486946106\n",
      "Training loss for batch 5110 : 0.10422453284263611\n",
      "Training loss for batch 5111 : 0.2246524542570114\n",
      "Training loss for batch 5112 : 0.04617396369576454\n",
      "Training loss for batch 5113 : 0.1576688289642334\n",
      "Training loss for batch 5114 : 0.0708540752530098\n",
      "Training loss for batch 5115 : 0.02655622735619545\n",
      "Training loss for batch 5116 : 0.14958809316158295\n",
      "Training loss for batch 5117 : 0.013790406286716461\n",
      "Training loss for batch 5118 : 0.11199510097503662\n",
      "Training loss for batch 5119 : 0.1194840669631958\n",
      "Training loss for batch 5120 : 0.34150275588035583\n",
      "Training loss for batch 5121 : 0.4544326663017273\n",
      "Training loss for batch 5122 : 0.17654113471508026\n",
      "Training loss for batch 5123 : 0.01226247102022171\n",
      "Training loss for batch 5124 : 0.5466206073760986\n",
      "Training loss for batch 5125 : 0.37557685375213623\n",
      "Training loss for batch 5126 : 0.36389976739883423\n",
      "Training loss for batch 5127 : 0.11996742337942123\n",
      "Training loss for batch 5128 : 0.1348298192024231\n",
      "Training loss for batch 5129 : 0.5504626035690308\n",
      "Training loss for batch 5130 : 0.4871600270271301\n",
      "Training loss for batch 5131 : 0.02189372107386589\n",
      "Training loss for batch 5132 : 0.18611925840377808\n",
      "Training loss for batch 5133 : 0.0339982807636261\n",
      "Training loss for batch 5134 : 0.21088692545890808\n",
      "Training loss for batch 5135 : 0.4280712604522705\n",
      "Training loss for batch 5136 : 0.055180225521326065\n",
      "Training loss for batch 5137 : 0.38754960894584656\n",
      "Training loss for batch 5138 : 0.13286495208740234\n",
      "Training loss for batch 5139 : 0.2601964771747589\n",
      "Training loss for batch 5140 : 0.06520156562328339\n",
      "Training loss for batch 5141 : 0.057404521852731705\n",
      "Training loss for batch 5142 : 0.2751234769821167\n",
      "Training loss for batch 5143 : 0.5151841640472412\n",
      "Training loss for batch 5144 : 0.02364795096218586\n",
      "Training loss for batch 5145 : 0.3334886431694031\n",
      "Training loss for batch 5146 : 0.3666665554046631\n",
      "Training loss for batch 5147 : 0.011814415454864502\n",
      "Training loss for batch 5148 : 0.10299945622682571\n",
      "Training loss for batch 5149 : 0.016966979950666428\n",
      "Training loss for batch 5150 : 0.22339726984500885\n",
      "Training loss for batch 5151 : 0.5076683759689331\n",
      "Training loss for batch 5152 : 0.15060560405254364\n",
      "Training loss for batch 5153 : 0.06873828172683716\n",
      "Training loss for batch 5154 : 0.269707590341568\n",
      "Training loss for batch 5155 : 0.20997977256774902\n",
      "Training loss for batch 5156 : 0.02656218409538269\n",
      "Training loss for batch 5157 : 0.12043388187885284\n",
      "Training loss for batch 5158 : 0.05417853593826294\n",
      "Training loss for batch 5159 : 0.21079209446907043\n",
      "Training loss for batch 5160 : 0.3101467192173004\n",
      "Training loss for batch 5161 : 0.5607433319091797\n",
      "Training loss for batch 5162 : 0.36903685331344604\n",
      "Training loss for batch 5163 : 0.39716750383377075\n",
      "Training loss for batch 5164 : 0.24757462739944458\n",
      "Training loss for batch 5165 : 0.015512840822339058\n",
      "Training loss for batch 5166 : 0.3213323950767517\n",
      "Training loss for batch 5167 : 0.30004364252090454\n",
      "Training loss for batch 5168 : 0.04369475319981575\n",
      "Training loss for batch 5169 : 0.17808377742767334\n",
      "Training loss for batch 5170 : 0.3171618580818176\n",
      "Training loss for batch 5171 : 0.2959141433238983\n",
      "Training loss for batch 5172 : 0.5858590006828308\n",
      "Training loss for batch 5173 : 0.22752664983272552\n",
      "Training loss for batch 5174 : 0.08988514542579651\n",
      "Training loss for batch 5175 : 0.03997977077960968\n",
      "Training loss for batch 5176 : 0.32162851095199585\n",
      "Training loss for batch 5177 : 0.12724849581718445\n",
      "Training loss for batch 5178 : 0.5540302991867065\n",
      "Training loss for batch 5179 : 0.09231409430503845\n",
      "Training loss for batch 5180 : 0.5392656326293945\n",
      "Training loss for batch 5181 : 0.23084759712219238\n",
      "Training loss for batch 5182 : 0.21832256019115448\n",
      "Training loss for batch 5183 : 0.04061729088425636\n",
      "Training loss for batch 5184 : 0.1259261816740036\n",
      "Training loss for batch 5185 : 0.5489720106124878\n",
      "Training loss for batch 5186 : 0.18396979570388794\n",
      "Training loss for batch 5187 : 0.1614968627691269\n",
      "Training loss for batch 5188 : 0.4800530970096588\n",
      "Training loss for batch 5189 : 0.08138565719127655\n",
      "Training loss for batch 5190 : 0.16278962790966034\n",
      "Training loss for batch 5191 : 0.0679415836930275\n",
      "Training loss for batch 5192 : 0.11189506947994232\n",
      "Training loss for batch 5193 : 0.0685943141579628\n",
      "Training loss for batch 5194 : 0.39851850271224976\n",
      "Training loss for batch 5195 : 0.030607562512159348\n",
      "Training loss for batch 5196 : 0.12396904826164246\n",
      "Training loss for batch 5197 : 0.09705426543951035\n",
      "Training loss for batch 5198 : 0.21415063738822937\n",
      "Training loss for batch 5199 : 0.16527587175369263\n",
      "Training loss for batch 5200 : 0.6265924572944641\n",
      "Training loss for batch 5201 : 0.4390857219696045\n",
      "Training loss for batch 5202 : 0.1484522819519043\n",
      "Training loss for batch 5203 : 0.19562695920467377\n",
      "Training loss for batch 5204 : 0.4027024209499359\n",
      "Training loss for batch 5205 : 0.4366866648197174\n",
      "Training loss for batch 5206 : 0.034939344972372055\n",
      "Training loss for batch 5207 : 1.1869654655456543\n",
      "Training loss for batch 5208 : 0.11501052975654602\n",
      "Training loss for batch 5209 : 0.022662121802568436\n",
      "Training loss for batch 5210 : 0.8239388465881348\n",
      "Training loss for batch 5211 : 0.5206952691078186\n",
      "Training loss for batch 5212 : 0.27227458357810974\n",
      "Training loss for batch 5213 : 0.029482215642929077\n",
      "Training loss for batch 5214 : 0.2881839871406555\n",
      "Training loss for batch 5215 : 0.5259065628051758\n",
      "Training loss for batch 5216 : 0.212059885263443\n",
      "Training loss for batch 5217 : 0.2058088332414627\n",
      "Training loss for batch 5218 : 0.16182266175746918\n",
      "Training loss for batch 5219 : 0.25200334191322327\n",
      "Training loss for batch 5220 : 0.26817482709884644\n",
      "Training loss for batch 5221 : 0.052084580063819885\n",
      "Training loss for batch 5222 : 0.04683816432952881\n",
      "Training loss for batch 5223 : 0.1084253340959549\n",
      "Training loss for batch 5224 : 0.39122074842453003\n",
      "Training loss for batch 5225 : 0.20188140869140625\n",
      "Training loss for batch 5226 : 0.4766964912414551\n",
      "Training loss for batch 5227 : 0.05018879473209381\n",
      "Training loss for batch 5228 : 0.30628105998039246\n",
      "Training loss for batch 5229 : 0.16875380277633667\n",
      "Training loss for batch 5230 : 0.3418176770210266\n",
      "Training loss for batch 5231 : 0.2429332286119461\n",
      "Training loss for batch 5232 : 0.13559654355049133\n",
      "Training loss for batch 5233 : 0.035589706152677536\n",
      "Training loss for batch 5234 : 0.004559020511806011\n",
      "Training loss for batch 5235 : 0.47816193103790283\n",
      "Training loss for batch 5236 : 0.09392722696065903\n",
      "Training loss for batch 5237 : 0.6443749666213989\n",
      "Training loss for batch 5238 : 0.3641582131385803\n",
      "Training loss for batch 5239 : 0.6324158310890198\n",
      "Training loss for batch 5240 : 0.07508375495672226\n",
      "Training loss for batch 5241 : 0.7764450311660767\n",
      "Training loss for batch 5242 : 0.8268492817878723\n",
      "Training loss for batch 5243 : 0.8018884658813477\n",
      "Training loss for batch 5244 : 0.3068688213825226\n",
      "Training loss for batch 5245 : 0.41281843185424805\n",
      "Training loss for batch 5246 : 0.49659866094589233\n",
      "Training loss for batch 5247 : 0.13611182570457458\n",
      "Training loss for batch 5248 : 0.06925299763679504\n",
      "Training loss for batch 5249 : 0.2846030592918396\n",
      "Training loss for batch 5250 : 0.5876729488372803\n",
      "Training loss for batch 5251 : 0.0\n",
      "Training loss for batch 5252 : 0.4188917577266693\n",
      "Training loss for batch 5253 : 0.1642390489578247\n",
      "Training loss for batch 5254 : 0.12540209293365479\n",
      "Training loss for batch 5255 : 0.6847313642501831\n",
      "Training loss for batch 5256 : 0.06030461937189102\n",
      "Training loss for batch 5257 : 0.3718564510345459\n",
      "Training loss for batch 5258 : 0.11114270985126495\n",
      "Training loss for batch 5259 : 0.2618290185928345\n",
      "Training loss for batch 5260 : 0.20657745003700256\n",
      "Training loss for batch 5261 : 0.003463665721938014\n",
      "Training loss for batch 5262 : 0.5831050872802734\n",
      "Training loss for batch 5263 : 0.3638104796409607\n",
      "Training loss for batch 5264 : 0.48325666785240173\n",
      "Training loss for batch 5265 : 0.08692733943462372\n",
      "Training loss for batch 5266 : 0.10043486952781677\n",
      "Training loss for batch 5267 : 0.04906148836016655\n",
      "Training loss for batch 5268 : 0.22328370809555054\n",
      "Training loss for batch 5269 : 0.077603779733181\n",
      "Training loss for batch 5270 : 0.4426478147506714\n",
      "Training loss for batch 5271 : 0.2182948738336563\n",
      "Training loss for batch 5272 : 0.34577715396881104\n",
      "Training loss for batch 5273 : 0.08062276989221573\n",
      "Training loss for batch 5274 : 0.19360607862472534\n",
      "Training loss for batch 5275 : 0.18970680236816406\n",
      "Training loss for batch 5276 : 0.4684664309024811\n",
      "Training loss for batch 5277 : 0.06715188920497894\n",
      "Training loss for batch 5278 : 0.08709543943405151\n",
      "Training loss for batch 5279 : 0.347217857837677\n",
      "Training loss for batch 5280 : 0.537280261516571\n",
      "Training loss for batch 5281 : 0.19263297319412231\n",
      "Training loss for batch 5282 : 0.5303003191947937\n",
      "Training loss for batch 5283 : 0.1626276969909668\n",
      "Training loss for batch 5284 : 0.9267152547836304\n",
      "Training loss for batch 5285 : 0.29800379276275635\n",
      "Training loss for batch 5286 : 0.21160465478897095\n",
      "Training loss for batch 5287 : 0.19432701170444489\n",
      "Training loss for batch 5288 : 0.4762946367263794\n",
      "Training loss for batch 5289 : 0.22830884158611298\n",
      "Training loss for batch 5290 : 0.14297106862068176\n",
      "Training loss for batch 5291 : 0.09052754193544388\n",
      "Training loss for batch 5292 : 0.02761027216911316\n",
      "Training loss for batch 5293 : 0.29838770627975464\n",
      "Training loss for batch 5294 : 0.09372837096452713\n",
      "Training loss for batch 5295 : 0.4125388562679291\n",
      "Training loss for batch 5296 : 0.0389963835477829\n",
      "Training loss for batch 5297 : 0.4905698895454407\n",
      "Training loss for batch 5298 : 0.42180371284484863\n",
      "Training loss for batch 5299 : 0.09586308151483536\n",
      "Training loss for batch 5300 : 0.330366849899292\n",
      "Training loss for batch 5301 : 0.15862178802490234\n",
      "Training loss for batch 5302 : 0.32240384817123413\n",
      "Training loss for batch 5303 : 0.21280276775360107\n",
      "Training loss for batch 5304 : 0.2245168834924698\n",
      "Training loss for batch 5305 : 0.04148019850254059\n",
      "Training loss for batch 5306 : 0.05801188200712204\n",
      "Training loss for batch 5307 : 0.06397753208875656\n",
      "Training loss for batch 5308 : 0.39709463715553284\n",
      "Training loss for batch 5309 : 0.33478352427482605\n",
      "Training loss for batch 5310 : 0.28184381127357483\n",
      "Training loss for batch 5311 : 0.38222289085388184\n",
      "Training loss for batch 5312 : 0.20221607387065887\n",
      "Training loss for batch 5313 : 0.2311425507068634\n",
      "Training loss for batch 5314 : 0.27834969758987427\n",
      "Training loss for batch 5315 : 0.1416826993227005\n",
      "Training loss for batch 5316 : 0.20852841436862946\n",
      "Training loss for batch 5317 : 0.48578396439552307\n",
      "Training loss for batch 5318 : 0.440386563539505\n",
      "Training loss for batch 5319 : 0.3265794515609741\n",
      "Training loss for batch 5320 : 0.23988181352615356\n",
      "Training loss for batch 5321 : 0.22740274667739868\n",
      "Training loss for batch 5322 : 0.28721532225608826\n",
      "Training loss for batch 5323 : 0.34864968061447144\n",
      "Training loss for batch 5324 : 0.24107959866523743\n",
      "Training loss for batch 5325 : 0.3331834673881531\n",
      "Training loss for batch 5326 : 0.4164033830165863\n",
      "Training loss for batch 5327 : 0.2889290452003479\n",
      "Training loss for batch 5328 : 0.4926398992538452\n",
      "Training loss for batch 5329 : 0.2661910355091095\n",
      "Training loss for batch 5330 : 0.13571757078170776\n",
      "Training loss for batch 5331 : 0.08952755481004715\n",
      "Training loss for batch 5332 : 0.1698877513408661\n",
      "Training loss for batch 5333 : 0.327045202255249\n",
      "Training loss for batch 5334 : 0.40804991126060486\n",
      "Training loss for batch 5335 : 0.22644975781440735\n",
      "Training loss for batch 5336 : 0.0\n",
      "Training loss for batch 5337 : 0.22083412110805511\n",
      "Training loss for batch 5338 : 0.3748328983783722\n",
      "Training loss for batch 5339 : 0.8382546305656433\n",
      "Training loss for batch 5340 : 0.007415453903377056\n",
      "Training loss for batch 5341 : 0.285428911447525\n",
      "Training loss for batch 5342 : 0.05416213721036911\n",
      "Training loss for batch 5343 : 0.09176664799451828\n",
      "Training loss for batch 5344 : 0.25381508469581604\n",
      "Training loss for batch 5345 : 0.17954552173614502\n",
      "Training loss for batch 5346 : 0.31234410405158997\n",
      "Training loss for batch 5347 : 0.10606769472360611\n",
      "Training loss for batch 5348 : 0.2572996914386749\n",
      "Training loss for batch 5349 : 0.49410831928253174\n",
      "Training loss for batch 5350 : 0.19175055623054504\n",
      "Training loss for batch 5351 : 0.02211458422243595\n",
      "Training loss for batch 5352 : 0.4714694619178772\n",
      "Training loss for batch 5353 : 0.17376579344272614\n",
      "Training loss for batch 5354 : 0.5903225541114807\n",
      "Training loss for batch 5355 : 0.26989510655403137\n",
      "Training loss for batch 5356 : 0.5203860402107239\n",
      "Training loss for batch 5357 : 0.08268120884895325\n",
      "Training loss for batch 5358 : 0.12035675346851349\n",
      "Training loss for batch 5359 : 0.21894612908363342\n",
      "Training loss for batch 5360 : 0.2760393023490906\n",
      "Training loss for batch 5361 : 0.08262117207050323\n",
      "Training loss for batch 5362 : 0.2997681498527527\n",
      "Training loss for batch 5363 : 0.27454936504364014\n",
      "Training loss for batch 5364 : 0.29624229669570923\n",
      "Training loss for batch 5365 : 0.3265094757080078\n",
      "Training loss for batch 5366 : 0.3357281982898712\n",
      "Training loss for batch 5367 : 0.05043245106935501\n",
      "Training loss for batch 5368 : 0.04091668128967285\n",
      "Training loss for batch 5369 : 0.4317694306373596\n",
      "Training loss for batch 5370 : 0.2144012153148651\n",
      "Training loss for batch 5371 : 0.060597632080316544\n",
      "Training loss for batch 5372 : 0.21290242671966553\n",
      "Training loss for batch 5373 : 0.031218141317367554\n",
      "Training loss for batch 5374 : 0.13444294035434723\n",
      "Training loss for batch 5375 : 0.05480591580271721\n",
      "Training loss for batch 5376 : 0.0596981905400753\n",
      "Training loss for batch 5377 : 0.2567194998264313\n",
      "Training loss for batch 5378 : 0.39769092202186584\n",
      "Training loss for batch 5379 : 0.41230809688568115\n",
      "Training loss for batch 5380 : 0.04861759394407272\n",
      "Training loss for batch 5381 : 0.748813807964325\n",
      "Training loss for batch 5382 : 0.0\n",
      "Training loss for batch 5383 : 0.393490195274353\n",
      "Training loss for batch 5384 : 0.07571111619472504\n",
      "Training loss for batch 5385 : 0.29391974210739136\n",
      "Training loss for batch 5386 : 0.52271968126297\n",
      "Training loss for batch 5387 : 0.060933809727430344\n",
      "Training loss for batch 5388 : 0.11855196952819824\n",
      "Training loss for batch 5389 : 0.07370615750551224\n",
      "Training loss for batch 5390 : 0.5405135154724121\n",
      "Training loss for batch 5391 : 0.3182310163974762\n",
      "Training loss for batch 5392 : 0.016365032643079758\n",
      "Training loss for batch 5393 : 0.012157721444964409\n",
      "Training loss for batch 5394 : 0.26164567470550537\n",
      "Training loss for batch 5395 : 0.1468256115913391\n",
      "Training loss for batch 5396 : 0.6724311113357544\n",
      "Training loss for batch 5397 : 0.6308580636978149\n",
      "Training loss for batch 5398 : 0.02524394914507866\n",
      "Training loss for batch 5399 : 0.07647605240345001\n",
      "Training loss for batch 5400 : 0.16806676983833313\n",
      "Training loss for batch 5401 : 0.04497017338871956\n",
      "Training loss for batch 5402 : 0.17087729275226593\n",
      "Training loss for batch 5403 : 0.32440871000289917\n",
      "Training loss for batch 5404 : 0.25614413619041443\n",
      "Training loss for batch 5405 : 0.6241788268089294\n",
      "Training loss for batch 5406 : 0.3066527843475342\n",
      "Training loss for batch 5407 : 0.3348921835422516\n",
      "Training loss for batch 5408 : 0.3234849274158478\n",
      "Training loss for batch 5409 : 0.08497349172830582\n",
      "Training loss for batch 5410 : 0.21426330506801605\n",
      "Training loss for batch 5411 : 0.096270352602005\n",
      "Training loss for batch 5412 : 0.17146235704421997\n",
      "Training loss for batch 5413 : 0.21522708237171173\n",
      "Training loss for batch 5414 : 0.415117472410202\n",
      "Training loss for batch 5415 : 0.42354580760002136\n",
      "Training loss for batch 5416 : 0.42626145482063293\n",
      "Training loss for batch 5417 : 0.12999658286571503\n",
      "Training loss for batch 5418 : 0.290785014629364\n",
      "Training loss for batch 5419 : 0.4687759280204773\n",
      "Training loss for batch 5420 : 0.12862904369831085\n",
      "Training loss for batch 5421 : 0.1334701031446457\n",
      "Training loss for batch 5422 : 0.2684038281440735\n",
      "Training loss for batch 5423 : 0.29887914657592773\n",
      "Training loss for batch 5424 : 0.3082607090473175\n",
      "Training loss for batch 5425 : 0.27384382486343384\n",
      "Training loss for batch 5426 : 0.4583192765712738\n",
      "Training loss for batch 5427 : 0.48958760499954224\n",
      "Training loss for batch 5428 : 0.887212336063385\n",
      "Training loss for batch 5429 : 0.2437027096748352\n",
      "Training loss for batch 5430 : 0.08675483614206314\n",
      "Training loss for batch 5431 : 0.20815932750701904\n",
      "Training loss for batch 5432 : 0.2313631922006607\n",
      "Training loss for batch 5433 : 0.37284430861473083\n",
      "Training loss for batch 5434 : 0.4801771342754364\n",
      "Training loss for batch 5435 : 0.15364116430282593\n",
      "Training loss for batch 5436 : 0.2666209638118744\n",
      "Training loss for batch 5437 : 0.2847830653190613\n",
      "Training loss for batch 5438 : 0.6853858828544617\n",
      "Training loss for batch 5439 : 0.37263327836990356\n",
      "Training loss for batch 5440 : 0.28862664103507996\n",
      "Training loss for batch 5441 : 0.4368632435798645\n",
      "Training loss for batch 5442 : 0.4092082381248474\n",
      "Training loss for batch 5443 : 0.2958160638809204\n",
      "Training loss for batch 5444 : 0.40042662620544434\n",
      "Training loss for batch 5445 : 0.29631903767585754\n",
      "Training loss for batch 5446 : 0.43198084831237793\n",
      "Training loss for batch 5447 : 0.5588312149047852\n",
      "Training loss for batch 5448 : 0.3435555398464203\n",
      "Training loss for batch 5449 : 0.14510224759578705\n",
      "Training loss for batch 5450 : 0.11598360538482666\n",
      "Training loss for batch 5451 : 0.16676972806453705\n",
      "Training loss for batch 5452 : 0.27959099411964417\n",
      "Training loss for batch 5453 : 0.42264217138290405\n",
      "Training loss for batch 5454 : 0.2054332196712494\n",
      "Training loss for batch 5455 : 0.1899658590555191\n",
      "Training loss for batch 5456 : 0.24828499555587769\n",
      "Training loss for batch 5457 : 0.3766621947288513\n",
      "Training loss for batch 5458 : 0.1680482178926468\n",
      "Training loss for batch 5459 : 0.22887565195560455\n",
      "Training loss for batch 5460 : 0.32588279247283936\n",
      "Training loss for batch 5461 : 0.182917520403862\n",
      "Training loss for batch 5462 : 0.3853784203529358\n",
      "Training loss for batch 5463 : 0.06764575839042664\n",
      "Training loss for batch 5464 : 0.0\n",
      "Training loss for batch 5465 : 0.2347370684146881\n",
      "Training loss for batch 5466 : 0.16451536118984222\n",
      "Training loss for batch 5467 : 0.3485374450683594\n",
      "Training loss for batch 5468 : 0.29797667264938354\n",
      "Training loss for batch 5469 : 0.23716796934604645\n",
      "Training loss for batch 5470 : 0.13166844844818115\n",
      "Training loss for batch 5471 : 0.30348220467567444\n",
      "Training loss for batch 5472 : 0.11698119342327118\n",
      "Training loss for batch 5473 : 0.12228148430585861\n",
      "Training loss for batch 5474 : 0.005416736006736755\n",
      "Training loss for batch 5475 : 0.17436593770980835\n",
      "Training loss for batch 5476 : 0.36437100172042847\n",
      "Training loss for batch 5477 : 0.36612507700920105\n",
      "Training loss for batch 5478 : 0.3889043927192688\n",
      "Training loss for batch 5479 : 0.28691381216049194\n",
      "Training loss for batch 5480 : 0.4852139353752136\n",
      "Training loss for batch 5481 : 0.25040870904922485\n",
      "Training loss for batch 5482 : 0.20598271489143372\n",
      "Training loss for batch 5483 : 0.10001371055841446\n",
      "Training loss for batch 5484 : 0.45561039447784424\n",
      "Training loss for batch 5485 : 0.2605593502521515\n",
      "Training loss for batch 5486 : 0.1867397427558899\n",
      "Training loss for batch 5487 : 0.03141676262021065\n",
      "Training loss for batch 5488 : 0.28647229075431824\n",
      "Training loss for batch 5489 : 0.47530463337898254\n",
      "Training loss for batch 5490 : 0.13390935957431793\n",
      "Training loss for batch 5491 : 0.10628785192966461\n",
      "Training loss for batch 5492 : 0.22803723812103271\n",
      "Training loss for batch 5493 : 0.15903355181217194\n",
      "Training loss for batch 5494 : 0.5464051961898804\n",
      "Training loss for batch 5495 : 0.0\n",
      "Training loss for batch 5496 : 0.4369986355304718\n",
      "Training loss for batch 5497 : 0.20327651500701904\n",
      "Training loss for batch 5498 : 0.20193220674991608\n",
      "Training loss for batch 5499 : 0.16187673807144165\n",
      "Training loss for batch 5500 : 0.19247502088546753\n",
      "Training loss for batch 5501 : 0.25494498014450073\n",
      "Training loss for batch 5502 : 0.26171553134918213\n",
      "Training loss for batch 5503 : 0.2378717064857483\n",
      "Training loss for batch 5504 : 0.19803093373775482\n",
      "Training loss for batch 5505 : 0.5653005242347717\n",
      "Training loss for batch 5506 : 0.5138030648231506\n",
      "Training loss for batch 5507 : 0.07163164764642715\n",
      "Training loss for batch 5508 : 0.5056670308113098\n",
      "Training loss for batch 5509 : 0.02580048143863678\n",
      "Training loss for batch 5510 : 0.22949740290641785\n",
      "Training loss for batch 5511 : 0.27572956681251526\n",
      "Training loss for batch 5512 : 0.038203056901693344\n",
      "Training loss for batch 5513 : 0.5269269943237305\n",
      "Training loss for batch 5514 : 0.37186571955680847\n",
      "Training loss for batch 5515 : 0.5449134111404419\n",
      "Training loss for batch 5516 : 0.3491605520248413\n",
      "Training loss for batch 5517 : 0.7428630590438843\n",
      "Training loss for batch 5518 : 0.19141334295272827\n",
      "Training loss for batch 5519 : 0.16750875115394592\n",
      "Training loss for batch 5520 : 0.6363687515258789\n",
      "Training loss for batch 5521 : 0.129817396402359\n",
      "Training loss for batch 5522 : 0.26091456413269043\n",
      "Training loss for batch 5523 : 0.09817319363355637\n",
      "Training loss for batch 5524 : 0.010333660058677197\n",
      "Training loss for batch 5525 : 0.2722809612751007\n",
      "Training loss for batch 5526 : 0.4258452355861664\n",
      "Training loss for batch 5527 : 0.7253196239471436\n",
      "Training loss for batch 5528 : 0.2824626863002777\n",
      "Training loss for batch 5529 : 0.16067884862422943\n",
      "Training loss for batch 5530 : 0.12660126388072968\n",
      "Training loss for batch 5531 : 0.5138915777206421\n",
      "Training loss for batch 5532 : 0.0493263304233551\n",
      "Training loss for batch 5533 : 0.29503390192985535\n",
      "Training loss for batch 5534 : 0.1773374378681183\n",
      "Training loss for batch 5535 : 0.6035699248313904\n",
      "Training loss for batch 5536 : 0.6961384415626526\n",
      "Training loss for batch 5537 : 0.26874297857284546\n",
      "Training loss for batch 5538 : 0.35096532106399536\n",
      "Training loss for batch 5539 : 0.014062421396374702\n",
      "Training loss for batch 5540 : 0.22694048285484314\n",
      "Training loss for batch 5541 : 0.22935469448566437\n",
      "Training loss for batch 5542 : 0.07810387015342712\n",
      "Training loss for batch 5543 : 0.1714196652173996\n",
      "Training loss for batch 5544 : 0.10292916744947433\n",
      "Training loss for batch 5545 : 0.0695478618144989\n",
      "Training loss for batch 5546 : 0.028724491596221924\n",
      "Training loss for batch 5547 : 0.31069260835647583\n",
      "Training loss for batch 5548 : 0.09898632764816284\n",
      "Training loss for batch 5549 : 0.5085458159446716\n",
      "Training loss for batch 5550 : 0.17757855355739594\n",
      "Training loss for batch 5551 : 0.12274004518985748\n",
      "Training loss for batch 5552 : 0.0662187710404396\n",
      "Training loss for batch 5553 : 0.008885960094630718\n",
      "Training loss for batch 5554 : 0.033768828958272934\n",
      "Training loss for batch 5555 : 0.28116661310195923\n",
      "Training loss for batch 5556 : 0.1763210892677307\n",
      "Training loss for batch 5557 : 0.5760210156440735\n",
      "Training loss for batch 5558 : 0.08868451416492462\n",
      "Training loss for batch 5559 : 0.23130223155021667\n",
      "Training loss for batch 5560 : 0.41890400648117065\n",
      "Training loss for batch 5561 : 0.1926266998052597\n",
      "Training loss for batch 5562 : 0.29876944422721863\n",
      "Training loss for batch 5563 : 0.00815728772431612\n",
      "Training loss for batch 5564 : 0.18913738429546356\n",
      "Training loss for batch 5565 : 0.057380665093660355\n",
      "Training loss for batch 5566 : 0.22528426349163055\n",
      "Training loss for batch 5567 : 0.5921370983123779\n",
      "Training loss for batch 5568 : 0.14272381365299225\n",
      "Training loss for batch 5569 : 0.34940430521965027\n",
      "Training loss for batch 5570 : 0.09000122547149658\n",
      "Training loss for batch 5571 : 0.11039189249277115\n",
      "Training loss for batch 5572 : 0.3174741268157959\n",
      "Training loss for batch 5573 : 0.12650710344314575\n",
      "Training loss for batch 5574 : 0.11891580373048782\n",
      "Training loss for batch 5575 : 0.7676379680633545\n",
      "Training loss for batch 5576 : 0.1142205074429512\n",
      "Training loss for batch 5577 : 0.1477946788072586\n",
      "Training loss for batch 5578 : 0.10854026675224304\n",
      "Training loss for batch 5579 : 0.05757167190313339\n",
      "Training loss for batch 5580 : 0.18217313289642334\n",
      "Training loss for batch 5581 : 0.6732233166694641\n",
      "Training loss for batch 5582 : 0.2086048722267151\n",
      "Training loss for batch 5583 : 0.07415686547756195\n",
      "Training loss for batch 5584 : 0.36286497116088867\n",
      "Training loss for batch 5585 : 0.0\n",
      "Training loss for batch 5586 : 0.0\n",
      "Training loss for batch 5587 : 0.6210017204284668\n",
      "Training loss for batch 5588 : 0.09953674674034119\n",
      "Training loss for batch 5589 : 0.5770699381828308\n",
      "Training loss for batch 5590 : 0.18062296509742737\n",
      "Training loss for batch 5591 : 0.17326340079307556\n",
      "Training loss for batch 5592 : 0.5372105836868286\n",
      "Training loss for batch 5593 : 0.3185326159000397\n",
      "Training loss for batch 5594 : 0.08286149054765701\n",
      "Training loss for batch 5595 : 0.034188635647296906\n",
      "Training loss for batch 5596 : 0.16526111960411072\n",
      "Training loss for batch 5597 : 0.12316006422042847\n",
      "Training loss for batch 5598 : 0.5598195791244507\n",
      "Training loss for batch 5599 : 0.4269220232963562\n",
      "Training loss for batch 5600 : 0.0\n",
      "Training loss for batch 5601 : 0.4462343156337738\n",
      "Training loss for batch 5602 : 0.16959017515182495\n",
      "Training loss for batch 5603 : 0.29426613450050354\n",
      "Training loss for batch 5604 : 0.11574990302324295\n",
      "Training loss for batch 5605 : 0.011267044581472874\n",
      "Training loss for batch 5606 : 0.5523892045021057\n",
      "Training loss for batch 5607 : 0.22760255634784698\n",
      "Training loss for batch 5608 : 0.15376438200473785\n",
      "Training loss for batch 5609 : 0.09857682883739471\n",
      "Training loss for batch 5610 : 0.1462748646736145\n",
      "Training loss for batch 5611 : 0.32248133420944214\n",
      "Training loss for batch 5612 : 0.021950040012598038\n",
      "Training loss for batch 5613 : 0.2762363851070404\n",
      "Training loss for batch 5614 : 0.27992480993270874\n",
      "Training loss for batch 5615 : 0.21667547523975372\n",
      "Training loss for batch 5616 : 0.10555379092693329\n",
      "Training loss for batch 5617 : 0.2081245630979538\n",
      "Training loss for batch 5618 : 0.3949066400527954\n",
      "Training loss for batch 5619 : 0.18022044003009796\n",
      "Training loss for batch 5620 : 0.6642922163009644\n",
      "Training loss for batch 5621 : 0.3812236487865448\n",
      "Training loss for batch 5622 : 0.12994404137134552\n",
      "Training loss for batch 5623 : 0.13605886697769165\n",
      "Training loss for batch 5624 : 0.0999654084444046\n",
      "Training loss for batch 5625 : 0.5338366627693176\n",
      "Training loss for batch 5626 : 0.3811779022216797\n",
      "Training loss for batch 5627 : 0.47932350635528564\n",
      "Training loss for batch 5628 : 0.3933597803115845\n",
      "Training loss for batch 5629 : 0.07129427790641785\n",
      "Training loss for batch 5630 : 0.2154501974582672\n",
      "Training loss for batch 5631 : 0.4295443892478943\n",
      "Training loss for batch 5632 : 0.4814491868019104\n",
      "Training loss for batch 5633 : 0.09918250143527985\n",
      "Training loss for batch 5634 : 0.4293097257614136\n",
      "Training loss for batch 5635 : 0.40121152997016907\n",
      "Training loss for batch 5636 : 0.300488144159317\n",
      "Training loss for batch 5637 : 0.04425085708498955\n",
      "Training loss for batch 5638 : 0.35689643025398254\n",
      "Training loss for batch 5639 : 0.04425572603940964\n",
      "Training loss for batch 5640 : 0.1219908744096756\n",
      "Training loss for batch 5641 : 0.18709510564804077\n",
      "Training loss for batch 5642 : 0.26407524943351746\n",
      "Training loss for batch 5643 : 0.29221099615097046\n",
      "Training loss for batch 5644 : 0.08174584060907364\n",
      "Training loss for batch 5645 : 0.4988628923892975\n",
      "Training loss for batch 5646 : 0.05615493655204773\n",
      "Training loss for batch 5647 : 0.5178849697113037\n",
      "Training loss for batch 5648 : 0.10321204364299774\n",
      "Training loss for batch 5649 : 0.04813997074961662\n",
      "Training loss for batch 5650 : 0.3133013844490051\n",
      "Training loss for batch 5651 : 0.2762039303779602\n",
      "Training loss for batch 5652 : 0.14132916927337646\n",
      "Training loss for batch 5653 : 0.5100285410881042\n",
      "Training loss for batch 5654 : 0.18901297450065613\n",
      "Training loss for batch 5655 : 0.24371618032455444\n",
      "Training loss for batch 5656 : 0.16592460870742798\n",
      "Training loss for batch 5657 : 0.48961764574050903\n",
      "Training loss for batch 5658 : 0.5292092561721802\n",
      "Training loss for batch 5659 : 0.5046280026435852\n",
      "Training loss for batch 5660 : 0.21272070705890656\n",
      "Training loss for batch 5661 : 0.4865400195121765\n",
      "Training loss for batch 5662 : 0.23973609507083893\n",
      "Training loss for batch 5663 : 0.3707984685897827\n",
      "Training loss for batch 5664 : 0.0\n",
      "Training loss for batch 5665 : 0.3397713899612427\n",
      "Training loss for batch 5666 : 0.18582594394683838\n",
      "Training loss for batch 5667 : 0.1430857926607132\n",
      "Training loss for batch 5668 : 0.33693116903305054\n",
      "Training loss for batch 5669 : 0.13872671127319336\n",
      "Training loss for batch 5670 : 0.31435275077819824\n",
      "Training loss for batch 5671 : 0.3669259548187256\n",
      "Training loss for batch 5672 : 0.4952709674835205\n",
      "Training loss for batch 5673 : 0.6838440299034119\n",
      "Training loss for batch 5674 : 0.23993083834648132\n",
      "Training loss for batch 5675 : 0.1175597682595253\n",
      "Training loss for batch 5676 : 0.16554023325443268\n",
      "Training loss for batch 5677 : 0.15016140043735504\n",
      "Training loss for batch 5678 : 0.1901235431432724\n",
      "Training loss for batch 5679 : 0.2607235610485077\n",
      "Training loss for batch 5680 : 0.25022009015083313\n",
      "Training loss for batch 5681 : 0.11186285316944122\n",
      "Training loss for batch 5682 : 0.19648590683937073\n",
      "Training loss for batch 5683 : 0.0870225727558136\n",
      "Training loss for batch 5684 : 0.08793295174837112\n",
      "Training loss for batch 5685 : 0.19769591093063354\n",
      "Training loss for batch 5686 : 0.0813707560300827\n",
      "Training loss for batch 5687 : 0.03883789852261543\n",
      "Training loss for batch 5688 : 0.010606484487652779\n",
      "Training loss for batch 5689 : 0.49667590856552124\n",
      "Training loss for batch 5690 : 0.43764156103134155\n",
      "Training loss for batch 5691 : 0.43648746609687805\n",
      "Training loss for batch 5692 : 0.12889999151229858\n",
      "Training loss for batch 5693 : 0.045674268156290054\n",
      "Training loss for batch 5694 : 0.04339324310421944\n",
      "Training loss for batch 5695 : 0.17010974884033203\n",
      "Training loss for batch 5696 : 0.3879665434360504\n",
      "Training loss for batch 5697 : 0.5042471289634705\n",
      "Training loss for batch 5698 : 0.06638941168785095\n",
      "Training loss for batch 5699 : 0.5237975716590881\n",
      "Training loss for batch 5700 : 0.7083206176757812\n",
      "Training loss for batch 5701 : 0.07834825664758682\n",
      "Training loss for batch 5702 : 0.33363109827041626\n",
      "Training loss for batch 5703 : 0.28433313965797424\n",
      "Training loss for batch 5704 : 0.2984216511249542\n",
      "Training loss for batch 5705 : 0.20628930628299713\n",
      "Training loss for batch 5706 : 0.2730364501476288\n",
      "Training loss for batch 5707 : 0.3206963837146759\n",
      "Training loss for batch 5708 : 0.582943320274353\n",
      "Training loss for batch 5709 : 0.34526121616363525\n",
      "Training loss for batch 5710 : 0.23895440995693207\n",
      "Training loss for batch 5711 : 0.16839520633220673\n",
      "Training loss for batch 5712 : 0.19610357284545898\n",
      "Training loss for batch 5713 : 0.5023228526115417\n",
      "Training loss for batch 5714 : 0.14444930851459503\n",
      "Training loss for batch 5715 : 0.5366000533103943\n",
      "Training loss for batch 5716 : 0.2925414741039276\n",
      "Training loss for batch 5717 : 0.13743004202842712\n",
      "Training loss for batch 5718 : 0.12294353544712067\n",
      "Training loss for batch 5719 : 0.364023894071579\n",
      "Training loss for batch 5720 : 0.20051811635494232\n",
      "Training loss for batch 5721 : 0.08287188410758972\n",
      "Training loss for batch 5722 : 0.03752375766634941\n",
      "Training loss for batch 5723 : 0.12522926926612854\n",
      "Training loss for batch 5724 : 0.09873848408460617\n",
      "Training loss for batch 5725 : 0.2844105362892151\n",
      "Training loss for batch 5726 : 0.6670218706130981\n",
      "Training loss for batch 5727 : 0.37893539667129517\n",
      "Training loss for batch 5728 : 0.19576571881771088\n",
      "Training loss for batch 5729 : 0.48078781366348267\n",
      "Training loss for batch 5730 : 0.10161705315113068\n",
      "Training loss for batch 5731 : 0.11227011680603027\n",
      "Training loss for batch 5732 : 0.0934179276227951\n",
      "Training loss for batch 5733 : 0.05874224379658699\n",
      "Training loss for batch 5734 : 0.3195168375968933\n",
      "Training loss for batch 5735 : 0.23156675696372986\n",
      "Training loss for batch 5736 : 0.11785893887281418\n",
      "Training loss for batch 5737 : 0.553158164024353\n",
      "Training loss for batch 5738 : 0.3360809087753296\n",
      "Training loss for batch 5739 : 0.07618428766727448\n",
      "Training loss for batch 5740 : 0.113059863448143\n",
      "Training loss for batch 5741 : 0.21540726721286774\n",
      "Training loss for batch 5742 : 0.22095726430416107\n",
      "Training loss for batch 5743 : 0.4580078721046448\n",
      "Training loss for batch 5744 : 0.22452831268310547\n",
      "Training loss for batch 5745 : 0.0930960401892662\n",
      "Training loss for batch 5746 : 0.2591632008552551\n",
      "Training loss for batch 5747 : 0.34074828028678894\n",
      "Training loss for batch 5748 : 0.11344272643327713\n",
      "Training loss for batch 5749 : 0.1422358751296997\n",
      "Training loss for batch 5750 : 0.08593112975358963\n",
      "Training loss for batch 5751 : 0.14506317675113678\n",
      "Training loss for batch 5752 : 0.5901051163673401\n",
      "Training loss for batch 5753 : 0.4051530659198761\n",
      "Training loss for batch 5754 : 0.08667817711830139\n",
      "Training loss for batch 5755 : 0.10377541929483414\n",
      "Training loss for batch 5756 : 0.08588876575231552\n",
      "Training loss for batch 5757 : 0.19376921653747559\n",
      "Training loss for batch 5758 : 0.4302016794681549\n",
      "Training loss for batch 5759 : 0.07724331319332123\n",
      "Training loss for batch 5760 : 0.23372900485992432\n",
      "Training loss for batch 5761 : 0.05121351405978203\n",
      "Training loss for batch 5762 : 0.6757078170776367\n",
      "Training loss for batch 5763 : 0.31427836418151855\n",
      "Training loss for batch 5764 : 0.20389792323112488\n",
      "Training loss for batch 5765 : 0.0795782208442688\n",
      "Training loss for batch 5766 : 0.21577565371990204\n",
      "Training loss for batch 5767 : 0.08426859974861145\n",
      "Training loss for batch 5768 : 0.16688911616802216\n",
      "Training loss for batch 5769 : 0.44590818881988525\n",
      "Training loss for batch 5770 : 0.268513023853302\n",
      "Training loss for batch 5771 : 0.1894902139902115\n",
      "Training loss for batch 5772 : 0.057306110858917236\n",
      "Training loss for batch 5773 : 0.4310816526412964\n",
      "Training loss for batch 5774 : 0.03712736815214157\n",
      "Training loss for batch 5775 : 0.22433049976825714\n",
      "Training loss for batch 5776 : 0.07923299819231033\n",
      "Training loss for batch 5777 : 0.41535019874572754\n",
      "Training loss for batch 5778 : 0.05494091659784317\n",
      "Training loss for batch 5779 : 0.4658973217010498\n",
      "Training loss for batch 5780 : 0.19084574282169342\n",
      "Training loss for batch 5781 : 0.22634924948215485\n",
      "Training loss for batch 5782 : 0.25704970955848694\n",
      "Training loss for batch 5783 : 0.2868745028972626\n",
      "Training loss for batch 5784 : 0.4101922810077667\n",
      "Training loss for batch 5785 : 0.8783205151557922\n",
      "Training loss for batch 5786 : 0.22757957875728607\n",
      "Training loss for batch 5787 : 0.22722959518432617\n",
      "Training loss for batch 5788 : 0.13731960952281952\n",
      "Training loss for batch 5789 : 0.11698757857084274\n",
      "Training loss for batch 5790 : 0.38856011629104614\n",
      "Training loss for batch 5791 : 0.09567123651504517\n",
      "Training loss for batch 5792 : 0.0\n",
      "Training loss for batch 5793 : 0.19213521480560303\n",
      "Training loss for batch 5794 : 0.4320216178894043\n",
      "Training loss for batch 5795 : 0.07039611786603928\n",
      "Training loss for batch 5796 : 0.31798991560935974\n",
      "Training loss for batch 5797 : 0.432752400636673\n",
      "Training loss for batch 5798 : 0.045041944831609726\n",
      "Training loss for batch 5799 : 0.023845018818974495\n",
      "Training loss for batch 5800 : 0.19043397903442383\n",
      "Training loss for batch 5801 : 0.5219182968139648\n",
      "Training loss for batch 5802 : 0.28861483931541443\n",
      "Training loss for batch 5803 : 0.1641106903553009\n",
      "Training loss for batch 5804 : 0.317626029253006\n",
      "Training loss for batch 5805 : 0.1519337147474289\n",
      "Training loss for batch 5806 : 0.23464763164520264\n",
      "Training loss for batch 5807 : 0.14686472713947296\n",
      "Training loss for batch 5808 : 0.08242087811231613\n",
      "Training loss for batch 5809 : 0.09619088470935822\n",
      "Training loss for batch 5810 : 0.019607599824666977\n",
      "Training loss for batch 5811 : 0.5570326447486877\n",
      "Training loss for batch 5812 : 0.025822054594755173\n",
      "Training loss for batch 5813 : 0.29932692646980286\n",
      "Training loss for batch 5814 : 0.23141855001449585\n",
      "Training loss for batch 5815 : 0.097136951982975\n",
      "Training loss for batch 5816 : 0.3533651530742645\n",
      "Training loss for batch 5817 : 0.04061655327677727\n",
      "Training loss for batch 5818 : 0.3900764286518097\n",
      "Training loss for batch 5819 : 0.38388046622276306\n",
      "Training loss for batch 5820 : 0.1751188039779663\n",
      "Training loss for batch 5821 : 0.4312392473220825\n",
      "Training loss for batch 5822 : 0.38269057869911194\n",
      "Training loss for batch 5823 : 0.33634963631629944\n",
      "Training loss for batch 5824 : 0.16522108018398285\n",
      "Training loss for batch 5825 : 0.0\n",
      "Training loss for batch 5826 : 0.44233155250549316\n",
      "Training loss for batch 5827 : 0.27995964884757996\n",
      "Training loss for batch 5828 : 0.023018820211291313\n",
      "Training loss for batch 5829 : 0.47880542278289795\n",
      "Training loss for batch 5830 : 0.11920031160116196\n",
      "Training loss for batch 5831 : 0.13736402988433838\n",
      "Training loss for batch 5832 : 0.3437139391899109\n",
      "Training loss for batch 5833 : 0.13062185049057007\n",
      "Training loss for batch 5834 : 0.012609015218913555\n",
      "Training loss for batch 5835 : 0.07937716692686081\n",
      "Training loss for batch 5836 : 0.09365442395210266\n",
      "Training loss for batch 5837 : 0.17585384845733643\n",
      "Training loss for batch 5838 : 0.1610504388809204\n",
      "Training loss for batch 5839 : 0.37603387236595154\n",
      "Training loss for batch 5840 : 0.6856552362442017\n",
      "Training loss for batch 5841 : 0.011570492759346962\n",
      "Training loss for batch 5842 : 0.031499214470386505\n",
      "Training loss for batch 5843 : 0.26606297492980957\n",
      "Training loss for batch 5844 : 0.09468572586774826\n",
      "Training loss for batch 5845 : 0.3750160038471222\n",
      "Training loss for batch 5846 : 0.11153046786785126\n",
      "Training loss for batch 5847 : 0.14961451292037964\n",
      "Training loss for batch 5848 : 0.27277424931526184\n",
      "Training loss for batch 5849 : 0.4335336983203888\n",
      "Training loss for batch 5850 : 0.07505704462528229\n",
      "Training loss for batch 5851 : 0.3988754451274872\n",
      "Training loss for batch 5852 : 0.045417651534080505\n",
      "Training loss for batch 5853 : 0.3257940411567688\n",
      "Training loss for batch 5854 : 0.6048907041549683\n",
      "Training loss for batch 5855 : 0.2761845588684082\n",
      "Training loss for batch 5856 : 0.5830655097961426\n",
      "Training loss for batch 5857 : 0.003380803158506751\n",
      "Training loss for batch 5858 : 0.10551077872514725\n",
      "Training loss for batch 5859 : 0.3154726028442383\n",
      "Training loss for batch 5860 : 0.005126903764903545\n",
      "Training loss for batch 5861 : 0.18360140919685364\n",
      "Training loss for batch 5862 : 0.006923736538738012\n",
      "Training loss for batch 5863 : 0.3283673822879791\n",
      "Training loss for batch 5864 : 0.0\n",
      "Training loss for batch 5865 : 0.2094627469778061\n",
      "Training loss for batch 5866 : 0.3341631293296814\n",
      "Training loss for batch 5867 : 0.48934149742126465\n",
      "Training loss for batch 5868 : 0.20730945467948914\n",
      "Training loss for batch 5869 : 0.129643052816391\n",
      "Training loss for batch 5870 : 0.3519473075866699\n",
      "Training loss for batch 5871 : 0.44926953315734863\n",
      "Training loss for batch 5872 : 0.42667147517204285\n",
      "Training loss for batch 5873 : 0.3682885766029358\n",
      "Training loss for batch 5874 : 0.5147496461868286\n",
      "Training loss for batch 5875 : 0.06943874061107635\n",
      "Training loss for batch 5876 : 0.031294792890548706\n",
      "Training loss for batch 5877 : 0.34994086623191833\n",
      "Training loss for batch 5878 : 0.20508132874965668\n",
      "Training loss for batch 5879 : 0.1891474723815918\n",
      "Training loss for batch 5880 : 0.3335818648338318\n",
      "Training loss for batch 5881 : 0.16158656775951385\n",
      "Training loss for batch 5882 : 0.2695172429084778\n",
      "Training loss for batch 5883 : 0.24612918496131897\n",
      "Training loss for batch 5884 : 0.294715940952301\n",
      "Training loss for batch 5885 : 0.5618507862091064\n",
      "Training loss for batch 5886 : 0.2840263545513153\n",
      "Training loss for batch 5887 : 0.3486607074737549\n",
      "Training loss for batch 5888 : 0.4813770353794098\n",
      "Training loss for batch 5889 : 0.20267198979854584\n",
      "Training loss for batch 5890 : 0.3629293739795685\n",
      "Training loss for batch 5891 : 0.15182332694530487\n",
      "Training loss for batch 5892 : 0.20139870047569275\n",
      "Training loss for batch 5893 : 0.07363643497228622\n",
      "Training loss for batch 5894 : 0.15046314895153046\n",
      "Training loss for batch 5895 : 0.09921342134475708\n",
      "Training loss for batch 5896 : 0.44389480352401733\n",
      "Training loss for batch 5897 : 0.7547398209571838\n",
      "Training loss for batch 5898 : 0.37960055470466614\n",
      "Training loss for batch 5899 : 0.11831121146678925\n",
      "Training loss for batch 5900 : 0.4753754734992981\n",
      "Training loss for batch 5901 : 0.1602514088153839\n",
      "Training loss for batch 5902 : 0.6724247336387634\n",
      "Training loss for batch 5903 : 0.09128150343894958\n",
      "Training loss for batch 5904 : 0.28642702102661133\n",
      "Training loss for batch 5905 : 0.04762902483344078\n",
      "Training loss for batch 5906 : 0.08461539447307587\n",
      "Training loss for batch 5907 : 0.36400315165519714\n",
      "Training loss for batch 5908 : 0.12808315455913544\n",
      "Training loss for batch 5909 : 0.29097095131874084\n",
      "Training loss for batch 5910 : 0.3594188094139099\n",
      "Training loss for batch 5911 : 0.4764816462993622\n",
      "Training loss for batch 5912 : 0.04375295341014862\n",
      "Training loss for batch 5913 : 0.12109588831663132\n",
      "Training loss for batch 5914 : 0.3736453354358673\n",
      "Training loss for batch 5915 : 0.08560745418071747\n",
      "Training loss for batch 5916 : 0.4145991802215576\n",
      "Training loss for batch 5917 : 0.01879837177693844\n",
      "Training loss for batch 5918 : 0.4175190031528473\n",
      "Training loss for batch 5919 : 0.16289815306663513\n",
      "Training loss for batch 5920 : 0.4365805685520172\n",
      "Training loss for batch 5921 : 0.1741834133863449\n",
      "Training loss for batch 5922 : 0.32311511039733887\n",
      "Training loss for batch 5923 : 0.4309319853782654\n",
      "Training loss for batch 5924 : 0.06901626288890839\n",
      "Training loss for batch 5925 : 0.27670615911483765\n",
      "Training loss for batch 5926 : 0.47872915863990784\n",
      "Training loss for batch 5927 : 0.41046223044395447\n",
      "Training loss for batch 5928 : 0.220331609249115\n",
      "Training loss for batch 5929 : 0.5907919406890869\n",
      "Training loss for batch 5930 : 0.11813309788703918\n",
      "Training loss for batch 5931 : 0.44125896692276\n",
      "Training loss for batch 5932 : 0.28822848200798035\n",
      "Training loss for batch 5933 : 0.15939100086688995\n",
      "Training loss for batch 5934 : 0.460867702960968\n",
      "Training loss for batch 5935 : 0.2682120203971863\n",
      "Training loss for batch 5936 : 0.6530886888504028\n",
      "Training loss for batch 5937 : 0.29476502537727356\n",
      "Training loss for batch 5938 : 0.05716728791594505\n",
      "Training loss for batch 5939 : 0.17226460576057434\n",
      "Training loss for batch 5940 : 0.17094948887825012\n",
      "Training loss for batch 5941 : 0.1041611060500145\n",
      "Training loss for batch 5942 : 0.4548764228820801\n",
      "Training loss for batch 5943 : 0.19599071145057678\n",
      "Training loss for batch 5944 : 0.12058396637439728\n",
      "Training loss for batch 5945 : 0.15704435110092163\n",
      "Training loss for batch 5946 : 0.2740554213523865\n",
      "Training loss for batch 5947 : 0.7225703597068787\n",
      "Training loss for batch 5948 : 0.19482943415641785\n",
      "Training loss for batch 5949 : 0.2200683206319809\n",
      "Training loss for batch 5950 : 0.48129454255104065\n",
      "Training loss for batch 5951 : 0.13546516001224518\n",
      "Training loss for batch 5952 : 0.28883597254753113\n",
      "Training loss for batch 5953 : 0.40605708956718445\n",
      "Training loss for batch 5954 : 0.2960742712020874\n",
      "Training loss for batch 5955 : 0.05830629914999008\n",
      "Training loss for batch 5956 : 0.46134403347969055\n",
      "Training loss for batch 5957 : 0.5979962944984436\n",
      "Training loss for batch 5958 : 0.08200161904096603\n",
      "Training loss for batch 5959 : 0.19204843044281006\n",
      "Training loss for batch 5960 : 0.4717727601528168\n",
      "Training loss for batch 5961 : 0.045164819806814194\n",
      "Training loss for batch 5962 : 0.33344918489456177\n",
      "Training loss for batch 5963 : 0.4494536817073822\n",
      "Training loss for batch 5964 : 0.5143774151802063\n",
      "Training loss for batch 5965 : 0.050967298448085785\n",
      "Training loss for batch 5966 : 0.306789368391037\n",
      "Training loss for batch 5967 : 0.27418339252471924\n",
      "Training loss for batch 5968 : 0.28774088621139526\n",
      "Training loss for batch 5969 : 0.2530808746814728\n",
      "Training loss for batch 5970 : 0.10396543145179749\n",
      "Training loss for batch 5971 : 0.08826626092195511\n",
      "Training loss for batch 5972 : 0.5103936195373535\n",
      "Training loss for batch 5973 : 0.49627068638801575\n",
      "Training loss for batch 5974 : 0.4262557625770569\n",
      "Training loss for batch 5975 : 0.24527518451213837\n",
      "Training loss for batch 5976 : 0.14521680772304535\n",
      "Training loss for batch 5977 : 0.20473012328147888\n",
      "Training loss for batch 5978 : 0.19017307460308075\n",
      "Training loss for batch 5979 : 0.2557559311389923\n",
      "Training loss for batch 5980 : 0.28628984093666077\n",
      "Training loss for batch 5981 : 0.41413256525993347\n",
      "Training loss for batch 5982 : 0.4108099043369293\n",
      "Training loss for batch 5983 : 0.1774541735649109\n",
      "Training loss for batch 5984 : 0.1586601436138153\n",
      "Training loss for batch 5985 : 0.7256163954734802\n",
      "Training loss for batch 5986 : 0.5521767139434814\n",
      "Training loss for batch 5987 : 0.42661362886428833\n",
      "Training loss for batch 5988 : 0.014906209893524647\n",
      "Training loss for batch 5989 : 0.6535316705703735\n",
      "Training loss for batch 5990 : 0.003389983903616667\n",
      "Training loss for batch 5991 : 0.4441882371902466\n",
      "Training loss for batch 5992 : 0.3815932273864746\n",
      "Training loss for batch 5993 : 0.40204399824142456\n",
      "Training loss for batch 5994 : 0.04334122687578201\n",
      "Training loss for batch 5995 : 0.18515737354755402\n",
      "Training loss for batch 5996 : 0.06273840367794037\n",
      "Training loss for batch 5997 : 0.26804620027542114\n",
      "Training loss for batch 5998 : 0.05287989228963852\n",
      "Training loss for batch 5999 : 0.11289995163679123\n",
      "Training loss for batch 6000 : 0.24623970687389374\n",
      "Training loss for batch 6001 : 0.20444969832897186\n",
      "Training loss for batch 6002 : 0.10261274874210358\n",
      "Training loss for batch 6003 : 0.42093297839164734\n",
      "Training loss for batch 6004 : 0.06449367851018906\n",
      "Training loss for batch 6005 : 0.20472797751426697\n",
      "Training loss for batch 6006 : 0.36918243765830994\n",
      "Training loss for batch 6007 : 0.07202223688364029\n",
      "Training loss for batch 6008 : 0.1149352565407753\n",
      "Training loss for batch 6009 : 0.4492352306842804\n",
      "Training loss for batch 6010 : 0.4322744309902191\n",
      "Training loss for batch 6011 : 0.10472336411476135\n",
      "Training loss for batch 6012 : 0.5663142204284668\n",
      "Training loss for batch 6013 : 0.34760501980781555\n",
      "Training loss for batch 6014 : 0.5075677633285522\n",
      "Training loss for batch 6015 : 0.2511700987815857\n",
      "Training loss for batch 6016 : 0.07352189719676971\n",
      "Training loss for batch 6017 : 0.1507907211780548\n",
      "Training loss for batch 6018 : 0.147819384932518\n",
      "Training loss for batch 6019 : 0.14727219939231873\n",
      "Training loss for batch 6020 : 0.29303234815597534\n",
      "Training loss for batch 6021 : 0.10574239492416382\n",
      "Training loss for batch 6022 : 0.2301248461008072\n",
      "Training loss for batch 6023 : 0.12446728348731995\n",
      "Training loss for batch 6024 : 0.039849091321229935\n",
      "Training loss for batch 6025 : 0.08209056407213211\n",
      "Training loss for batch 6026 : 0.45669424533843994\n",
      "Training loss for batch 6027 : 0.09492471069097519\n",
      "Training loss for batch 6028 : 0.27670204639434814\n",
      "Training loss for batch 6029 : 0.013002991676330566\n",
      "Training loss for batch 6030 : 0.23071132600307465\n",
      "Training loss for batch 6031 : 0.06084510684013367\n",
      "Training loss for batch 6032 : 0.15043231844902039\n",
      "Training loss for batch 6033 : 0.023039601743221283\n",
      "Training loss for batch 6034 : 0.10982820391654968\n",
      "Training loss for batch 6035 : 0.3142213523387909\n",
      "Training loss for batch 6036 : 0.3874373435974121\n",
      "Training loss for batch 6037 : 0.0019210379105061293\n",
      "Training loss for batch 6038 : 0.018950387835502625\n",
      "Training loss for batch 6039 : 0.2969677746295929\n",
      "Training loss for batch 6040 : 0.1891750544309616\n",
      "Training loss for batch 6041 : 0.5578233599662781\n",
      "Training loss for batch 6042 : 0.07579334825277328\n",
      "Training loss for batch 6043 : 0.343829482793808\n",
      "Training loss for batch 6044 : 0.2658348083496094\n",
      "Training loss for batch 6045 : 0.1651569902896881\n",
      "Training loss for batch 6046 : 0.2606496810913086\n",
      "Training loss for batch 6047 : 0.6696478724479675\n",
      "Training loss for batch 6048 : 0.44183266162872314\n",
      "Training loss for batch 6049 : 0.2993873953819275\n",
      "Training loss for batch 6050 : 0.3583092987537384\n",
      "Training loss for batch 6051 : 0.5668220520019531\n",
      "Training loss for batch 6052 : 0.18290504813194275\n",
      "Training loss for batch 6053 : 0.5361654758453369\n",
      "Training loss for batch 6054 : 0.2683466970920563\n",
      "Training loss for batch 6055 : 0.17303071916103363\n",
      "Training loss for batch 6056 : 0.5666342973709106\n",
      "Training loss for batch 6057 : 0.4329199492931366\n",
      "Training loss for batch 6058 : 0.5290310978889465\n",
      "Training loss for batch 6059 : 0.0727490484714508\n",
      "Training loss for batch 6060 : 0.10209891200065613\n",
      "Training loss for batch 6061 : 0.4330759644508362\n",
      "Training loss for batch 6062 : 0.09289656579494476\n",
      "Training loss for batch 6063 : 0.15369872748851776\n",
      "Training loss for batch 6064 : 0.06540057063102722\n",
      "Training loss for batch 6065 : 0.31407925486564636\n",
      "Training loss for batch 6066 : 0.15225021541118622\n",
      "Training loss for batch 6067 : 0.5346325039863586\n",
      "Training loss for batch 6068 : 0.17119814455509186\n",
      "Training loss for batch 6069 : 0.11551305651664734\n",
      "Training loss for batch 6070 : 0.5397818088531494\n",
      "Training loss for batch 6071 : 0.38736143708229065\n",
      "Training loss for batch 6072 : 0.07060739398002625\n",
      "Training loss for batch 6073 : 0.6550850868225098\n",
      "Training loss for batch 6074 : 0.3232778310775757\n",
      "Training loss for batch 6075 : 0.015547837130725384\n",
      "Training loss for batch 6076 : 0.01681159809231758\n",
      "Training loss for batch 6077 : 0.17655469477176666\n",
      "Training loss for batch 6078 : 0.4346802234649658\n",
      "Training loss for batch 6079 : 0.4667048156261444\n",
      "Training loss for batch 6080 : 0.20733287930488586\n",
      "Training loss for batch 6081 : 0.3180696666240692\n",
      "Training loss for batch 6082 : 0.49370840191841125\n",
      "Training loss for batch 6083 : 0.3117372691631317\n",
      "Training loss for batch 6084 : 0.3107748031616211\n",
      "Training loss for batch 6085 : 0.3500078618526459\n",
      "Training loss for batch 6086 : 0.1253943145275116\n",
      "Training loss for batch 6087 : 0.03459779918193817\n",
      "Training loss for batch 6088 : 0.2728287875652313\n",
      "Training loss for batch 6089 : 0.14151179790496826\n",
      "Training loss for batch 6090 : 0.3576429486274719\n",
      "Training loss for batch 6091 : 0.04044201225042343\n",
      "Training loss for batch 6092 : 0.3095472753047943\n",
      "Training loss for batch 6093 : 0.3862237334251404\n",
      "Training loss for batch 6094 : 0.12671120464801788\n",
      "Training loss for batch 6095 : 0.10382740944623947\n",
      "Training loss for batch 6096 : 0.3874865174293518\n",
      "Training loss for batch 6097 : 0.357740193605423\n",
      "Training loss for batch 6098 : 0.0\n",
      "Training loss for batch 6099 : 0.0080152852460742\n",
      "Training loss for batch 6100 : 0.7359152436256409\n",
      "Training loss for batch 6101 : 0.16850607097148895\n",
      "Training loss for batch 6102 : 0.013631529174745083\n",
      "Training loss for batch 6103 : 0.012877464294433594\n",
      "Training loss for batch 6104 : 0.5648122429847717\n",
      "Training loss for batch 6105 : 0.4085884094238281\n",
      "Training loss for batch 6106 : 0.27738240361213684\n",
      "Training loss for batch 6107 : 0.41417860984802246\n",
      "Training loss for batch 6108 : 0.24977684020996094\n",
      "Training loss for batch 6109 : 0.09077144414186478\n",
      "Training loss for batch 6110 : 0.19102782011032104\n",
      "Training loss for batch 6111 : 0.18144989013671875\n",
      "Training loss for batch 6112 : 0.17382477223873138\n",
      "Training loss for batch 6113 : 0.35306742787361145\n",
      "Training loss for batch 6114 : 0.33308640122413635\n",
      "Training loss for batch 6115 : 0.12627992033958435\n",
      "Training loss for batch 6116 : 0.33735784888267517\n",
      "Training loss for batch 6117 : 0.08989302814006805\n",
      "Training loss for batch 6118 : 0.20386016368865967\n",
      "Training loss for batch 6119 : 0.24856919050216675\n",
      "Training loss for batch 6120 : 0.16830557584762573\n",
      "Training loss for batch 6121 : 0.2978328466415405\n",
      "Training loss for batch 6122 : 0.31428053975105286\n",
      "Training loss for batch 6123 : 0.03841843456029892\n",
      "Training loss for batch 6124 : 0.5796522498130798\n",
      "Training loss for batch 6125 : 0.5456156134605408\n",
      "Training loss for batch 6126 : 0.07507266104221344\n",
      "Training loss for batch 6127 : 0.32170161604881287\n",
      "Training loss for batch 6128 : 0.4304805397987366\n",
      "Training loss for batch 6129 : 0.3833995759487152\n",
      "Training loss for batch 6130 : 0.22829586267471313\n",
      "Training loss for batch 6131 : 0.3766171336174011\n",
      "Training loss for batch 6132 : 0.09256421029567719\n",
      "Training loss for batch 6133 : 0.18757902085781097\n",
      "Training loss for batch 6134 : 0.15103641152381897\n",
      "Training loss for batch 6135 : 0.17685270309448242\n",
      "Training loss for batch 6136 : 0.21089670062065125\n",
      "Training loss for batch 6137 : 0.016301190480589867\n",
      "Training loss for batch 6138 : 0.26545730233192444\n",
      "Training loss for batch 6139 : 0.20755596458911896\n",
      "Training loss for batch 6140 : 0.07523531466722488\n",
      "Training loss for batch 6141 : 0.37387728691101074\n",
      "Training loss for batch 6142 : 0.13274206221103668\n",
      "Training loss for batch 6143 : 0.18345843255519867\n",
      "Training loss for batch 6144 : 0.5993947386741638\n",
      "Training loss for batch 6145 : 0.05374408885836601\n",
      "Training loss for batch 6146 : 0.4905652403831482\n",
      "Training loss for batch 6147 : 0.11493433266878128\n",
      "Training loss for batch 6148 : 0.21438026428222656\n",
      "Training loss for batch 6149 : 0.11866892129182816\n",
      "Training loss for batch 6150 : 0.029387684538960457\n",
      "Training loss for batch 6151 : 0.24872954189777374\n",
      "Training loss for batch 6152 : 0.736448347568512\n",
      "Training loss for batch 6153 : 0.14872956275939941\n",
      "Training loss for batch 6154 : 0.20939108729362488\n",
      "Training loss for batch 6155 : 0.1613083928823471\n",
      "Training loss for batch 6156 : 0.1993531733751297\n",
      "Training loss for batch 6157 : 0.18128252029418945\n",
      "Training loss for batch 6158 : 0.4144468605518341\n",
      "Training loss for batch 6159 : 0.5643744468688965\n",
      "Training loss for batch 6160 : 0.052290719002485275\n",
      "Training loss for batch 6161 : 0.2848530113697052\n",
      "Training loss for batch 6162 : 0.07520066946744919\n",
      "Training loss for batch 6163 : 0.4677581489086151\n",
      "Training loss for batch 6164 : 0.09398850798606873\n",
      "Training loss for batch 6165 : 0.3144369423389435\n",
      "Training loss for batch 6166 : 0.4082232117652893\n",
      "Training loss for batch 6167 : 0.0683310329914093\n",
      "Training loss for batch 6168 : 0.30267125368118286\n",
      "Training loss for batch 6169 : 0.7319329977035522\n",
      "Training loss for batch 6170 : 0.3691091239452362\n",
      "Training loss for batch 6171 : 0.34446394443511963\n",
      "Training loss for batch 6172 : 0.1219029426574707\n",
      "Training loss for batch 6173 : 0.39137518405914307\n",
      "Training loss for batch 6174 : 0.16987666487693787\n",
      "Training loss for batch 6175 : 0.1874697059392929\n",
      "Training loss for batch 6176 : 0.09713871777057648\n",
      "Training loss for batch 6177 : 0.14315317571163177\n",
      "Training loss for batch 6178 : 0.0911618247628212\n",
      "Training loss for batch 6179 : 0.10333119332790375\n",
      "Training loss for batch 6180 : 0.3408771753311157\n",
      "Training loss for batch 6181 : 0.13427281379699707\n",
      "Training loss for batch 6182 : 0.06529125571250916\n",
      "Training loss for batch 6183 : 0.052437327802181244\n",
      "Training loss for batch 6184 : 0.15588663518428802\n",
      "Training loss for batch 6185 : 0.2712317705154419\n",
      "Training loss for batch 6186 : 0.2586899697780609\n",
      "Training loss for batch 6187 : 0.22821103036403656\n",
      "Training loss for batch 6188 : 0.12191300839185715\n",
      "Training loss for batch 6189 : 0.2958378791809082\n",
      "Training loss for batch 6190 : 0.19949938356876373\n",
      "Training loss for batch 6191 : 0.04404699057340622\n",
      "Training loss for batch 6192 : 0.2590641677379608\n",
      "Training loss for batch 6193 : 0.19095800817012787\n",
      "Training loss for batch 6194 : 0.02523045241832733\n",
      "Training loss for batch 6195 : 0.15491709113121033\n",
      "Training loss for batch 6196 : 0.28126421570777893\n",
      "Training loss for batch 6197 : 0.007081568241119385\n",
      "Training loss for batch 6198 : 0.38323283195495605\n",
      "Training loss for batch 6199 : 0.11458554863929749\n",
      "Training loss for batch 6200 : 0.1764819175004959\n",
      "Training loss for batch 6201 : 0.07985569536685944\n",
      "Training loss for batch 6202 : 0.10489358752965927\n",
      "Training loss for batch 6203 : 0.11311140656471252\n",
      "Training loss for batch 6204 : 0.3340906500816345\n",
      "Training loss for batch 6205 : 0.059198424220085144\n",
      "Training loss for batch 6206 : 0.10184963792562485\n",
      "Training loss for batch 6207 : 0.4912494122982025\n",
      "Training loss for batch 6208 : 0.24051152169704437\n",
      "Training loss for batch 6209 : 0.42600083351135254\n",
      "Training loss for batch 6210 : 0.043551038950681686\n",
      "Training loss for batch 6211 : 0.06156779080629349\n",
      "Training loss for batch 6212 : 0.5202240347862244\n",
      "Training loss for batch 6213 : 0.24930588901042938\n",
      "Training loss for batch 6214 : 0.3261905908584595\n",
      "Training loss for batch 6215 : 0.17985686659812927\n",
      "Training loss for batch 6216 : 0.14126330614089966\n",
      "Training loss for batch 6217 : 0.26059505343437195\n",
      "Training loss for batch 6218 : 0.03408290445804596\n",
      "Training loss for batch 6219 : 0.11983662843704224\n",
      "Training loss for batch 6220 : 0.2442571371793747\n",
      "Training loss for batch 6221 : 0.18463940918445587\n",
      "Training loss for batch 6222 : 0.1891385167837143\n",
      "Training loss for batch 6223 : 0.27469131350517273\n",
      "Training loss for batch 6224 : 0.4767165780067444\n",
      "Training loss for batch 6225 : 0.010944878682494164\n",
      "Training loss for batch 6226 : 0.35035818815231323\n",
      "Training loss for batch 6227 : 0.17710475623607635\n",
      "Training loss for batch 6228 : 0.21174097061157227\n",
      "Training loss for batch 6229 : 0.011747002601623535\n",
      "Training loss for batch 6230 : 0.7814666628837585\n",
      "Training loss for batch 6231 : 0.30550554394721985\n",
      "Training loss for batch 6232 : 0.029739538207650185\n",
      "Training loss for batch 6233 : 0.4603097140789032\n",
      "Training loss for batch 6234 : 0.16964690387248993\n",
      "Training loss for batch 6235 : 0.2585306167602539\n",
      "Training loss for batch 6236 : 0.32215091586112976\n",
      "Training loss for batch 6237 : 0.26852840185165405\n",
      "Training loss for batch 6238 : 0.12350469082593918\n",
      "Training loss for batch 6239 : 0.6755139827728271\n",
      "Training loss for batch 6240 : 0.48661282658576965\n",
      "Training loss for batch 6241 : 0.180998757481575\n",
      "Training loss for batch 6242 : 0.15680313110351562\n",
      "Training loss for batch 6243 : 0.10308922082185745\n",
      "Training loss for batch 6244 : 0.3414759635925293\n",
      "Training loss for batch 6245 : 0.7058098912239075\n",
      "Training loss for batch 6246 : 0.20308475196361542\n",
      "Training loss for batch 6247 : 0.20526407659053802\n",
      "Training loss for batch 6248 : 0.3887053430080414\n",
      "Training loss for batch 6249 : 0.2440459132194519\n",
      "Training loss for batch 6250 : 0.21969324350357056\n",
      "Training loss for batch 6251 : 0.20800478756427765\n",
      "Training loss for batch 6252 : 0.07576945424079895\n",
      "Training loss for batch 6253 : 0.1050083115696907\n",
      "Training loss for batch 6254 : 0.1733524203300476\n",
      "Training loss for batch 6255 : 0.4724196195602417\n",
      "Training loss for batch 6256 : 0.638473391532898\n",
      "Training loss for batch 6257 : 0.12111659348011017\n",
      "Training loss for batch 6258 : 0.3016110360622406\n",
      "Training loss for batch 6259 : 0.06427831202745438\n",
      "Training loss for batch 6260 : 0.10009235143661499\n",
      "Training loss for batch 6261 : 0.18833516538143158\n",
      "Training loss for batch 6262 : 0.3654383718967438\n",
      "Training loss for batch 6263 : 0.08180104941129684\n",
      "Training loss for batch 6264 : 0.11371910572052002\n",
      "Training loss for batch 6265 : 0.41534101963043213\n",
      "Training loss for batch 6266 : 0.6708276867866516\n",
      "Training loss for batch 6267 : 0.2221539467573166\n",
      "Training loss for batch 6268 : 0.12284216284751892\n",
      "Training loss for batch 6269 : 0.030678486451506615\n",
      "Training loss for batch 6270 : 0.11319179832935333\n",
      "Training loss for batch 6271 : 0.4712878167629242\n",
      "Training loss for batch 6272 : 0.30353090167045593\n",
      "Training loss for batch 6273 : 0.5050989389419556\n",
      "Training loss for batch 6274 : 0.23324500024318695\n",
      "Training loss for batch 6275 : 0.06728509813547134\n",
      "Training loss for batch 6276 : 0.044664643704891205\n",
      "Training loss for batch 6277 : 0.2943369150161743\n",
      "Training loss for batch 6278 : 0.19203317165374756\n",
      "Training loss for batch 6279 : 0.08550059795379639\n",
      "Training loss for batch 6280 : 0.28115662932395935\n",
      "Training loss for batch 6281 : 0.3717285394668579\n",
      "Training loss for batch 6282 : 0.16226136684417725\n",
      "Training loss for batch 6283 : 0.07359450310468674\n",
      "Training loss for batch 6284 : 0.27292370796203613\n",
      "Training loss for batch 6285 : 0.3725988268852234\n",
      "Training loss for batch 6286 : 0.3116881847381592\n",
      "Training loss for batch 6287 : 0.11780020594596863\n",
      "Training loss for batch 6288 : 0.16215212643146515\n",
      "Training loss for batch 6289 : 0.33613157272338867\n",
      "Training loss for batch 6290 : 0.488174170255661\n",
      "Training loss for batch 6291 : 0.3934210240840912\n",
      "Training loss for batch 6292 : 0.3410314917564392\n",
      "Training loss for batch 6293 : 0.20373979210853577\n",
      "Training loss for batch 6294 : 0.41536131501197815\n",
      "Training loss for batch 6295 : 0.12145548313856125\n",
      "Training loss for batch 6296 : 0.09739626199007034\n",
      "Training loss for batch 6297 : 0.11096566915512085\n",
      "Training loss for batch 6298 : 0.34922993183135986\n",
      "Training loss for batch 6299 : 0.3459691107273102\n",
      "Training loss for batch 6300 : 0.22673149406909943\n",
      "Training loss for batch 6301 : 0.34424957633018494\n",
      "Training loss for batch 6302 : 0.4796108603477478\n",
      "Training loss for batch 6303 : 0.29881152510643005\n",
      "Training loss for batch 6304 : 0.048027217388153076\n",
      "Training loss for batch 6305 : 0.10126852989196777\n",
      "Training loss for batch 6306 : 0.2750720977783203\n",
      "Training loss for batch 6307 : 0.2103332132101059\n",
      "Training loss for batch 6308 : 0.04620025306940079\n",
      "Training loss for batch 6309 : 0.0984627977013588\n",
      "Training loss for batch 6310 : 0.5758835673332214\n",
      "Training loss for batch 6311 : 0.0343293771147728\n",
      "Training loss for batch 6312 : 0.24114848673343658\n",
      "Training loss for batch 6313 : 0.13674688339233398\n",
      "Training loss for batch 6314 : 0.04572242498397827\n",
      "Training loss for batch 6315 : 0.1574106067419052\n",
      "Training loss for batch 6316 : 0.8490320444107056\n",
      "Training loss for batch 6317 : 0.47696036100387573\n",
      "Training loss for batch 6318 : 0.776404619216919\n",
      "Training loss for batch 6319 : 0.05324998497962952\n",
      "Training loss for batch 6320 : 0.1101987361907959\n",
      "Training loss for batch 6321 : 0.05958782881498337\n",
      "Training loss for batch 6322 : 0.12930166721343994\n",
      "Training loss for batch 6323 : 0.5408543348312378\n",
      "Training loss for batch 6324 : 0.0\n",
      "Training loss for batch 6325 : 0.2588141858577728\n",
      "Training loss for batch 6326 : 0.09656476974487305\n",
      "Training loss for batch 6327 : 0.27786460518836975\n",
      "Training loss for batch 6328 : 0.43592873215675354\n",
      "Training loss for batch 6329 : 0.3994712829589844\n",
      "Training loss for batch 6330 : 0.44539257884025574\n",
      "Training loss for batch 6331 : 0.2108827531337738\n",
      "Training loss for batch 6332 : 0.41091495752334595\n",
      "Training loss for batch 6333 : 0.05117806792259216\n",
      "Training loss for batch 6334 : 0.26740360260009766\n",
      "Training loss for batch 6335 : 0.6293527483940125\n",
      "Training loss for batch 6336 : 0.04345162212848663\n",
      "Training loss for batch 6337 : 0.0771966353058815\n",
      "Training loss for batch 6338 : 0.3038063645362854\n",
      "Training loss for batch 6339 : 0.06643281131982803\n",
      "Training loss for batch 6340 : 0.1267482042312622\n",
      "Training loss for batch 6341 : 0.24563361704349518\n",
      "Training loss for batch 6342 : 0.43253234028816223\n",
      "Training loss for batch 6343 : 0.11678948253393173\n",
      "Training loss for batch 6344 : 0.2771022319793701\n",
      "Training loss for batch 6345 : 0.3638480007648468\n",
      "Training loss for batch 6346 : 0.34441062808036804\n",
      "Training loss for batch 6347 : 0.06359949707984924\n",
      "Training loss for batch 6348 : 0.22752071917057037\n",
      "Training loss for batch 6349 : 0.3060480058193207\n",
      "Training loss for batch 6350 : 0.5166184902191162\n",
      "Training loss for batch 6351 : 0.08992157876491547\n",
      "Training loss for batch 6352 : 0.3519076704978943\n",
      "Training loss for batch 6353 : 0.320068359375\n",
      "Training loss for batch 6354 : 0.08172547817230225\n",
      "Training loss for batch 6355 : 0.30026867985725403\n",
      "Training loss for batch 6356 : 0.34742873907089233\n",
      "Training loss for batch 6357 : 0.0679781585931778\n",
      "Training loss for batch 6358 : 0.5389501452445984\n",
      "Training loss for batch 6359 : 0.08924584090709686\n",
      "Training loss for batch 6360 : 0.29564279317855835\n",
      "Training loss for batch 6361 : 0.09501944482326508\n",
      "Training loss for batch 6362 : 0.23159681260585785\n",
      "Training loss for batch 6363 : 0.17365726828575134\n",
      "Training loss for batch 6364 : 0.011191463097929955\n",
      "Training loss for batch 6365 : 0.13340876996517181\n",
      "Training loss for batch 6366 : 0.12661993503570557\n",
      "Training loss for batch 6367 : 0.0\n",
      "Training loss for batch 6368 : 0.319447785615921\n",
      "Training loss for batch 6369 : 0.6402008533477783\n",
      "Training loss for batch 6370 : 0.3145065903663635\n",
      "Training loss for batch 6371 : 0.379062294960022\n",
      "Training loss for batch 6372 : 0.07442502677440643\n",
      "Training loss for batch 6373 : 0.0741988942027092\n",
      "Training loss for batch 6374 : 0.43538758158683777\n",
      "Training loss for batch 6375 : 0.02613169699907303\n",
      "Training loss for batch 6376 : 0.31703194975852966\n",
      "Training loss for batch 6377 : 0.06685560941696167\n",
      "Training loss for batch 6378 : 0.687299907207489\n",
      "Training loss for batch 6379 : 0.10192876309156418\n",
      "Training loss for batch 6380 : 0.19333817064762115\n",
      "Training loss for batch 6381 : 0.04466895014047623\n",
      "Training loss for batch 6382 : 0.31619420647621155\n",
      "Training loss for batch 6383 : 0.41412821412086487\n",
      "Training loss for batch 6384 : 0.026133574545383453\n",
      "Training loss for batch 6385 : 0.058062195777893066\n",
      "Training loss for batch 6386 : 0.21826769411563873\n",
      "Training loss for batch 6387 : 0.11647724360227585\n",
      "Training loss for batch 6388 : 0.21095234155654907\n",
      "Training loss for batch 6389 : 0.30816057324409485\n",
      "Training loss for batch 6390 : 0.18414394557476044\n",
      "Training loss for batch 6391 : 0.032494086772203445\n",
      "Training loss for batch 6392 : 0.42999839782714844\n",
      "Training loss for batch 6393 : 0.18508371710777283\n",
      "Training loss for batch 6394 : 0.06398972868919373\n",
      "Training loss for batch 6395 : 0.4793357849121094\n",
      "Training loss for batch 6396 : 0.44902220368385315\n",
      "Training loss for batch 6397 : 0.12542521953582764\n",
      "Training loss for batch 6398 : 0.2569367587566376\n",
      "Training loss for batch 6399 : 0.08043719828128815\n",
      "Training loss for batch 6400 : 0.10782139003276825\n",
      "Training loss for batch 6401 : 0.7688840627670288\n",
      "Training loss for batch 6402 : 0.29368165135383606\n",
      "Training loss for batch 6403 : 0.14719192683696747\n",
      "Training loss for batch 6404 : 0.5166537761688232\n",
      "Training loss for batch 6405 : 0.20640020072460175\n",
      "Training loss for batch 6406 : 0.880436360836029\n",
      "Training loss for batch 6407 : 0.20127879083156586\n",
      "Training loss for batch 6408 : 0.2991263270378113\n",
      "Training loss for batch 6409 : 0.2627086639404297\n",
      "Training loss for batch 6410 : 0.41002580523490906\n",
      "Training loss for batch 6411 : 0.3542584776878357\n",
      "Training loss for batch 6412 : 0.2960713803768158\n",
      "Training loss for batch 6413 : 0.12538179755210876\n",
      "Training loss for batch 6414 : 0.7515703439712524\n",
      "Training loss for batch 6415 : 0.1367260068655014\n",
      "Training loss for batch 6416 : 0.5140699148178101\n",
      "Training loss for batch 6417 : 0.3219408690929413\n",
      "Training loss for batch 6418 : 0.1284012347459793\n",
      "Training loss for batch 6419 : 0.6586474776268005\n",
      "Training loss for batch 6420 : 0.1869620531797409\n",
      "Training loss for batch 6421 : 0.1491411030292511\n",
      "Training loss for batch 6422 : 0.18634283542633057\n",
      "Training loss for batch 6423 : 0.19428949058055878\n",
      "Training loss for batch 6424 : 0.08466595411300659\n",
      "Training loss for batch 6425 : 0.09556964039802551\n",
      "Training loss for batch 6426 : 0.10667817294597626\n",
      "Training loss for batch 6427 : 0.5334591865539551\n",
      "Training loss for batch 6428 : 0.5069869160652161\n",
      "Training loss for batch 6429 : 0.39093559980392456\n",
      "Training loss for batch 6430 : 0.09727156162261963\n",
      "Training loss for batch 6431 : 0.30565038323402405\n",
      "Training loss for batch 6432 : 0.14724647998809814\n",
      "Training loss for batch 6433 : 0.3825042247772217\n",
      "Training loss for batch 6434 : 0.2863505780696869\n",
      "Training loss for batch 6435 : 0.11793079972267151\n",
      "Training loss for batch 6436 : 0.4430156350135803\n",
      "Training loss for batch 6437 : 0.4648799002170563\n",
      "Training loss for batch 6438 : 0.6474543809890747\n",
      "Training loss for batch 6439 : 0.04484544321894646\n",
      "Training loss for batch 6440 : 1.134137749671936\n",
      "Training loss for batch 6441 : 0.1716841757297516\n",
      "Training loss for batch 6442 : 0.3918881118297577\n",
      "Training loss for batch 6443 : 0.03596438094973564\n",
      "Training loss for batch 6444 : 0.43267643451690674\n",
      "Training loss for batch 6445 : 0.3807598650455475\n",
      "Training loss for batch 6446 : 0.2742471992969513\n",
      "Training loss for batch 6447 : 0.3905175030231476\n",
      "Training loss for batch 6448 : 0.15175095200538635\n",
      "Training loss for batch 6449 : 0.40880441665649414\n",
      "Training loss for batch 6450 : 0.03773476555943489\n",
      "Training loss for batch 6451 : 0.4217063784599304\n",
      "Training loss for batch 6452 : 0.14193220436573029\n",
      "Training loss for batch 6453 : 0.14356614649295807\n",
      "Training loss for batch 6454 : 0.07135863602161407\n",
      "Training loss for batch 6455 : 0.20635347068309784\n",
      "Training loss for batch 6456 : 0.08896508067846298\n",
      "Training loss for batch 6457 : 0.05491326004266739\n",
      "Training loss for batch 6458 : 0.2004319131374359\n",
      "Training loss for batch 6459 : 0.17064307630062103\n",
      "Training loss for batch 6460 : 0.09333616495132446\n",
      "Training loss for batch 6461 : 0.09980743378400803\n",
      "Training loss for batch 6462 : 0.3541455566883087\n",
      "Training loss for batch 6463 : 0.20367102324962616\n",
      "Training loss for batch 6464 : 0.23290687799453735\n",
      "Training loss for batch 6465 : 0.31831449270248413\n",
      "Training loss for batch 6466 : 0.4271588921546936\n",
      "Training loss for batch 6467 : 0.2421068400144577\n",
      "Training loss for batch 6468 : 0.12001863121986389\n",
      "Training loss for batch 6469 : 0.07887871563434601\n",
      "Training loss for batch 6470 : 0.2094009965658188\n",
      "Training loss for batch 6471 : 0.5326118469238281\n",
      "Training loss for batch 6472 : 0.16570551693439484\n",
      "Training loss for batch 6473 : 0.4293619990348816\n",
      "Training loss for batch 6474 : 0.39631858468055725\n",
      "Training loss for batch 6475 : 0.19640208780765533\n",
      "Training loss for batch 6476 : 0.5000852942466736\n",
      "Training loss for batch 6477 : 0.14989551901817322\n",
      "Training loss for batch 6478 : 0.04652776941657066\n",
      "Training loss for batch 6479 : 0.13134391605854034\n",
      "Training loss for batch 6480 : 0.0831645131111145\n",
      "Training loss for batch 6481 : 0.07806643098592758\n",
      "Training loss for batch 6482 : 0.14598338305950165\n",
      "Training loss for batch 6483 : 0.33166253566741943\n",
      "Training loss for batch 6484 : 0.7726007699966431\n",
      "Training loss for batch 6485 : 0.44542232155799866\n",
      "Training loss for batch 6486 : 0.25823530554771423\n",
      "Training loss for batch 6487 : 0.39393150806427\n",
      "Training loss for batch 6488 : 0.024466590955853462\n",
      "Training loss for batch 6489 : 0.2522680461406708\n",
      "Training loss for batch 6490 : 0.05818591266870499\n",
      "Training loss for batch 6491 : 0.1402534544467926\n",
      "Training loss for batch 6492 : 0.30558472871780396\n",
      "Training loss for batch 6493 : 0.44861528277397156\n",
      "Training loss for batch 6494 : 0.141001358628273\n",
      "Training loss for batch 6495 : 0.2546914219856262\n",
      "Training loss for batch 6496 : 0.2163897305727005\n",
      "Training loss for batch 6497 : 0.3058541715145111\n",
      "Training loss for batch 6498 : 0.23029203712940216\n",
      "Training loss for batch 6499 : 0.02552475407719612\n",
      "Training loss for batch 6500 : 0.14148446917533875\n",
      "Training loss for batch 6501 : 0.6614259481430054\n",
      "Training loss for batch 6502 : 0.2551177740097046\n",
      "Training loss for batch 6503 : 0.09195096790790558\n",
      "Training loss for batch 6504 : 0.10308738052845001\n",
      "Training loss for batch 6505 : 0.30459803342819214\n",
      "Training loss for batch 6506 : 0.013206696137785912\n",
      "Training loss for batch 6507 : 0.04680897295475006\n",
      "Training loss for batch 6508 : 0.05832461640238762\n",
      "Training loss for batch 6509 : 0.12445010989904404\n",
      "Training loss for batch 6510 : 0.13776837289333344\n",
      "Training loss for batch 6511 : 0.29651811718940735\n",
      "Training loss for batch 6512 : 0.1474207490682602\n",
      "Training loss for batch 6513 : 0.036235105246305466\n",
      "Training loss for batch 6514 : 0.10702269524335861\n",
      "Training loss for batch 6515 : 0.8228501677513123\n",
      "Training loss for batch 6516 : 0.488475501537323\n",
      "Training loss for batch 6517 : 0.07240506261587143\n",
      "Training loss for batch 6518 : 0.42596662044525146\n",
      "Training loss for batch 6519 : 0.3381904363632202\n",
      "Training loss for batch 6520 : 0.3248673379421234\n",
      "Training loss for batch 6521 : 0.4354283809661865\n",
      "Training loss for batch 6522 : 0.00020683805632870644\n",
      "Training loss for batch 6523 : 0.601139485836029\n",
      "Training loss for batch 6524 : 0.15768954157829285\n",
      "Training loss for batch 6525 : 0.0045210225507617\n",
      "Training loss for batch 6526 : 0.0726773664355278\n",
      "Training loss for batch 6527 : 0.18405689299106598\n",
      "Training loss for batch 6528 : 0.304523766040802\n",
      "Training loss for batch 6529 : 0.07570405304431915\n",
      "Training loss for batch 6530 : 0.4616086483001709\n",
      "Training loss for batch 6531 : 0.22341910004615784\n",
      "Training loss for batch 6532 : 0.3122495412826538\n",
      "Training loss for batch 6533 : 0.16491717100143433\n",
      "Training loss for batch 6534 : 0.2143341451883316\n",
      "Training loss for batch 6535 : 0.3686821758747101\n",
      "Training loss for batch 6536 : 0.30249834060668945\n",
      "Training loss for batch 6537 : 0.09958040714263916\n",
      "Training loss for batch 6538 : 0.23664669692516327\n",
      "Training loss for batch 6539 : 0.2536453604698181\n",
      "Training loss for batch 6540 : 0.19163978099822998\n",
      "Training loss for batch 6541 : 0.28635266423225403\n",
      "Training loss for batch 6542 : 0.315485417842865\n",
      "Training loss for batch 6543 : 0.399758905172348\n",
      "Training loss for batch 6544 : 0.1797211915254593\n",
      "Training loss for batch 6545 : 0.05080774053931236\n",
      "Training loss for batch 6546 : 0.08860708028078079\n",
      "Training loss for batch 6547 : 0.41333943605422974\n",
      "Training loss for batch 6548 : 0.27990707755088806\n",
      "Training loss for batch 6549 : 0.648034930229187\n",
      "Training loss for batch 6550 : 0.5095387101173401\n",
      "Training loss for batch 6551 : 0.16542236506938934\n",
      "Training loss for batch 6552 : 0.10305458307266235\n",
      "Training loss for batch 6553 : 0.3605920374393463\n",
      "Training loss for batch 6554 : 0.19576333463191986\n",
      "Training loss for batch 6555 : 0.15633706748485565\n",
      "Training loss for batch 6556 : 0.525642991065979\n",
      "Training loss for batch 6557 : 0.05250944942235947\n",
      "Training loss for batch 6558 : 0.21271853148937225\n",
      "Training loss for batch 6559 : 0.014418351463973522\n",
      "Training loss for batch 6560 : 0.03201337903738022\n",
      "Training loss for batch 6561 : 0.18806755542755127\n",
      "Training loss for batch 6562 : 0.4769318997859955\n",
      "Training loss for batch 6563 : 0.02710283175110817\n",
      "Training loss for batch 6564 : 0.06676594167947769\n",
      "Training loss for batch 6565 : 0.007276564836502075\n",
      "Training loss for batch 6566 : 0.1175873875617981\n",
      "Training loss for batch 6567 : 0.48240092396736145\n",
      "Training loss for batch 6568 : 0.26422497630119324\n",
      "Training loss for batch 6569 : 0.6316244602203369\n",
      "Training loss for batch 6570 : 0.09965217113494873\n",
      "Training loss for batch 6571 : 0.44313880801200867\n",
      "Training loss for batch 6572 : 0.12764577567577362\n",
      "Training loss for batch 6573 : 0.008991962298750877\n",
      "Training loss for batch 6574 : 0.7377117276191711\n",
      "Training loss for batch 6575 : 0.5146400928497314\n",
      "Training loss for batch 6576 : 0.1767224371433258\n",
      "Training loss for batch 6577 : 0.4388374388217926\n",
      "Training loss for batch 6578 : 0.34170854091644287\n",
      "Training loss for batch 6579 : 0.38877201080322266\n",
      "Training loss for batch 6580 : 0.1594042181968689\n",
      "Training loss for batch 6581 : 0.3885963261127472\n",
      "Training loss for batch 6582 : 0.09581713378429413\n",
      "Training loss for batch 6583 : 0.2815272808074951\n",
      "Training loss for batch 6584 : 0.31688857078552246\n",
      "Training loss for batch 6585 : 0.06405550986528397\n",
      "Training loss for batch 6586 : 0.08268789947032928\n",
      "Training loss for batch 6587 : 0.5818393230438232\n",
      "Training loss for batch 6588 : 0.29431337118148804\n",
      "Training loss for batch 6589 : 0.08735792338848114\n",
      "Training loss for batch 6590 : 0.28173378109931946\n",
      "Training loss for batch 6591 : 0.16124571859836578\n",
      "Training loss for batch 6592 : 0.1905614137649536\n",
      "Training loss for batch 6593 : 0.06517051160335541\n",
      "Training loss for batch 6594 : 0.2385753095149994\n",
      "Training loss for batch 6595 : 0.18030691146850586\n",
      "Training loss for batch 6596 : 0.10421976447105408\n",
      "Training loss for batch 6597 : 0.041824039071798325\n",
      "Training loss for batch 6598 : 0.023426705971360207\n",
      "Training loss for batch 6599 : 0.5036384463310242\n",
      "Training loss for batch 6600 : 0.15694427490234375\n",
      "Training loss for batch 6601 : 0.24618598818778992\n",
      "Training loss for batch 6602 : 0.07495281100273132\n",
      "Training loss for batch 6603 : 0.2184634953737259\n",
      "Training loss for batch 6604 : 0.17450571060180664\n",
      "Training loss for batch 6605 : 0.6733129620552063\n",
      "Training loss for batch 6606 : 0.3591919541358948\n",
      "Training loss for batch 6607 : 0.497445285320282\n",
      "Training loss for batch 6608 : 0.5595665574073792\n",
      "Training loss for batch 6609 : 0.1943598836660385\n",
      "Training loss for batch 6610 : 0.058826617896556854\n",
      "Training loss for batch 6611 : 0.6406090259552002\n",
      "Training loss for batch 6612 : 0.11371305584907532\n",
      "Training loss for batch 6613 : 0.17683063447475433\n",
      "Training loss for batch 6614 : 0.05070323124527931\n",
      "Training loss for batch 6615 : 0.12724673748016357\n",
      "Training loss for batch 6616 : 0.26385533809661865\n",
      "Training loss for batch 6617 : 0.14280861616134644\n",
      "Training loss for batch 6618 : 0.12873205542564392\n",
      "Training loss for batch 6619 : 0.16350241005420685\n",
      "Training loss for batch 6620 : 0.09647195786237717\n",
      "Training loss for batch 6621 : 0.6151343584060669\n",
      "Training loss for batch 6622 : 0.17378294467926025\n",
      "Training loss for batch 6623 : 0.4550861716270447\n",
      "Training loss for batch 6624 : 0.3783392906188965\n",
      "Training loss for batch 6625 : 0.2674163579940796\n",
      "Training loss for batch 6626 : 0.5407320857048035\n",
      "Training loss for batch 6627 : 0.18371796607971191\n",
      "Training loss for batch 6628 : 0.10991117358207703\n",
      "Training loss for batch 6629 : 0.19875213503837585\n",
      "Training loss for batch 6630 : 0.5940296053886414\n",
      "Training loss for batch 6631 : 0.41711223125457764\n",
      "Training loss for batch 6632 : 0.4453476369380951\n",
      "Training loss for batch 6633 : 0.5261226892471313\n",
      "Training loss for batch 6634 : 0.27540916204452515\n",
      "Training loss for batch 6635 : 0.28537869453430176\n",
      "Training loss for batch 6636 : 0.24559426307678223\n",
      "Training loss for batch 6637 : 0.04198203980922699\n",
      "Training loss for batch 6638 : 0.23364774882793427\n",
      "Training loss for batch 6639 : 0.1615465134382248\n",
      "Training loss for batch 6640 : 0.34310802817344666\n",
      "Training loss for batch 6641 : 0.0\n",
      "Training loss for batch 6642 : 0.4774012863636017\n",
      "Training loss for batch 6643 : 0.03321428224444389\n",
      "Training loss for batch 6644 : 0.7625248432159424\n",
      "Training loss for batch 6645 : 0.3050130307674408\n",
      "Training loss for batch 6646 : 0.6844720244407654\n",
      "Training loss for batch 6647 : 0.4503996968269348\n",
      "Training loss for batch 6648 : 0.07470052689313889\n",
      "Training loss for batch 6649 : 0.4270928204059601\n",
      "Training loss for batch 6650 : 0.1435641646385193\n",
      "Training loss for batch 6651 : 0.23588499426841736\n",
      "Training loss for batch 6652 : 0.2084227204322815\n",
      "Training loss for batch 6653 : 0.034238606691360474\n",
      "Training loss for batch 6654 : 0.0468631349503994\n",
      "Training loss for batch 6655 : 0.44680672883987427\n",
      "Training loss for batch 6656 : 0.3830741345882416\n",
      "Training loss for batch 6657 : 0.19316625595092773\n",
      "Training loss for batch 6658 : 0.3679824769496918\n",
      "Training loss for batch 6659 : 0.5412040948867798\n",
      "Training loss for batch 6660 : 0.28554317355155945\n",
      "Training loss for batch 6661 : 0.14780470728874207\n",
      "Training loss for batch 6662 : 0.1452029049396515\n",
      "Training loss for batch 6663 : 0.29900074005126953\n",
      "Training loss for batch 6664 : 0.22404055297374725\n",
      "Training loss for batch 6665 : 0.12737473845481873\n",
      "Training loss for batch 6666 : 0.21869924664497375\n",
      "Training loss for batch 6667 : 0.5413416028022766\n",
      "Training loss for batch 6668 : 0.36950522661209106\n",
      "Training loss for batch 6669 : 0.5715535879135132\n",
      "Training loss for batch 6670 : 0.10869275033473969\n",
      "Training loss for batch 6671 : 0.08803575485944748\n",
      "Training loss for batch 6672 : 0.15708035230636597\n",
      "Training loss for batch 6673 : 0.3112894892692566\n",
      "Training loss for batch 6674 : 0.11336766183376312\n",
      "Training loss for batch 6675 : 0.28366437554359436\n",
      "Training loss for batch 6676 : 0.13144512474536896\n",
      "Training loss for batch 6677 : 0.14305832982063293\n",
      "Training loss for batch 6678 : 0.3800637125968933\n",
      "Training loss for batch 6679 : 0.21876047551631927\n",
      "Training loss for batch 6680 : 0.10387621074914932\n",
      "Training loss for batch 6681 : 0.04355854168534279\n",
      "Training loss for batch 6682 : 0.3437722325325012\n",
      "Training loss for batch 6683 : 0.5381397008895874\n",
      "Training loss for batch 6684 : 0.27331939339637756\n",
      "Training loss for batch 6685 : 5.62015593459364e-05\n",
      "Training loss for batch 6686 : 0.711974561214447\n",
      "Training loss for batch 6687 : 0.32175326347351074\n",
      "Training loss for batch 6688 : 0.21389620006084442\n",
      "Training loss for batch 6689 : 0.3700154721736908\n",
      "Training loss for batch 6690 : 0.13817749917507172\n",
      "Training loss for batch 6691 : 0.217789888381958\n",
      "Training loss for batch 6692 : 0.05913599953055382\n",
      "Training loss for batch 6693 : 0.0767662450671196\n",
      "Training loss for batch 6694 : 0.26095908880233765\n",
      "Training loss for batch 6695 : 0.39455288648605347\n",
      "Training loss for batch 6696 : 0.07204797118902206\n",
      "Training loss for batch 6697 : 0.1482117772102356\n",
      "Training loss for batch 6698 : 0.13395372033119202\n",
      "Training loss for batch 6699 : 0.3256978690624237\n",
      "Training loss for batch 6700 : 0.18431417644023895\n",
      "Training loss for batch 6701 : 0.05230715498328209\n",
      "Training loss for batch 6702 : 0.2938413918018341\n",
      "Training loss for batch 6703 : 0.028348853811621666\n",
      "Training loss for batch 6704 : 0.05384540185332298\n",
      "Training loss for batch 6705 : 0.31650274991989136\n",
      "Training loss for batch 6706 : 0.0471794418990612\n",
      "Training loss for batch 6707 : 0.03467809408903122\n",
      "Training loss for batch 6708 : 0.20237451791763306\n",
      "Training loss for batch 6709 : 0.24981261789798737\n",
      "Training loss for batch 6710 : 0.4262929856777191\n",
      "Training loss for batch 6711 : 0.2233911156654358\n",
      "Training loss for batch 6712 : 0.0\n",
      "Training loss for batch 6713 : 0.18905532360076904\n",
      "Training loss for batch 6714 : 0.3235209584236145\n",
      "Training loss for batch 6715 : 0.016060177236795425\n",
      "Training loss for batch 6716 : 0.327045738697052\n",
      "Training loss for batch 6717 : 0.4607246518135071\n",
      "Training loss for batch 6718 : 0.013285182416439056\n",
      "Training loss for batch 6719 : 0.2497321218252182\n",
      "Training loss for batch 6720 : 0.19489668309688568\n",
      "Training loss for batch 6721 : 0.46747082471847534\n",
      "Training loss for batch 6722 : 0.2525886595249176\n",
      "Training loss for batch 6723 : 0.12552620470523834\n",
      "Training loss for batch 6724 : 0.3208044767379761\n",
      "Training loss for batch 6725 : 0.43281030654907227\n",
      "Training loss for batch 6726 : 0.34706738591194153\n",
      "Training loss for batch 6727 : 0.48339924216270447\n",
      "Training loss for batch 6728 : 0.012741145677864552\n",
      "Training loss for batch 6729 : 0.0688515454530716\n",
      "Training loss for batch 6730 : 0.4574200510978699\n",
      "Training loss for batch 6731 : 0.21673928201198578\n",
      "Training loss for batch 6732 : 0.15696848928928375\n",
      "Training loss for batch 6733 : 0.34891635179519653\n",
      "Training loss for batch 6734 : 0.39556530117988586\n",
      "Training loss for batch 6735 : 0.0946025475859642\n",
      "Training loss for batch 6736 : 0.4819028675556183\n",
      "Training loss for batch 6737 : 0.3566244840621948\n",
      "Training loss for batch 6738 : 0.0988980233669281\n",
      "Training loss for batch 6739 : 0.17102764546871185\n",
      "Training loss for batch 6740 : 0.4578343331813812\n",
      "Training loss for batch 6741 : 0.5009938478469849\n",
      "Training loss for batch 6742 : 0.35490933060646057\n",
      "Training loss for batch 6743 : 0.07200726866722107\n",
      "Training loss for batch 6744 : 0.5916664600372314\n",
      "Training loss for batch 6745 : 0.058510202914476395\n",
      "Training loss for batch 6746 : 0.3227408230304718\n",
      "Training loss for batch 6747 : 0.040755148977041245\n",
      "Training loss for batch 6748 : 0.9704042673110962\n",
      "Training loss for batch 6749 : 0.043179601430892944\n",
      "Training loss for batch 6750 : 0.10256023705005646\n",
      "Training loss for batch 6751 : 0.21076080203056335\n",
      "Training loss for batch 6752 : 0.4267825782299042\n",
      "Training loss for batch 6753 : 0.213680237531662\n",
      "Training loss for batch 6754 : 0.11010155826807022\n",
      "Training loss for batch 6755 : 0.2233947217464447\n",
      "Training loss for batch 6756 : 0.19418054819107056\n",
      "Training loss for batch 6757 : 0.4976626932621002\n",
      "Training loss for batch 6758 : 0.39459073543548584\n",
      "Training loss for batch 6759 : 0.3035525977611542\n",
      "Training loss for batch 6760 : 0.3393576741218567\n",
      "Training loss for batch 6761 : 0.19203396141529083\n",
      "Training loss for batch 6762 : 0.2313196361064911\n",
      "Training loss for batch 6763 : 0.33328554034233093\n",
      "Training loss for batch 6764 : 0.2823854684829712\n",
      "Training loss for batch 6765 : 0.3776552677154541\n",
      "Training loss for batch 6766 : 0.6904034614562988\n",
      "Training loss for batch 6767 : 0.14397256076335907\n",
      "Training loss for batch 6768 : 0.32104015350341797\n",
      "Training loss for batch 6769 : 0.15658994019031525\n",
      "Training loss for batch 6770 : 0.27404260635375977\n",
      "Training loss for batch 6771 : 0.35352668166160583\n",
      "Training loss for batch 6772 : 0.4811764657497406\n",
      "Training loss for batch 6773 : 0.4328274726867676\n",
      "Training loss for batch 6774 : 0.18261444568634033\n",
      "Training loss for batch 6775 : 0.3970964252948761\n",
      "Training loss for batch 6776 : 0.5773414373397827\n",
      "Training loss for batch 6777 : 0.1281735748052597\n",
      "Training loss for batch 6778 : 0.17080025374889374\n",
      "Training loss for batch 6779 : 0.10720629245042801\n",
      "Training loss for batch 6780 : 0.1820121705532074\n",
      "Training loss for batch 6781 : 0.29939547181129456\n",
      "Training loss for batch 6782 : 0.2006983906030655\n",
      "Training loss for batch 6783 : 0.15797027945518494\n",
      "Training loss for batch 6784 : 0.39053329825401306\n",
      "Training loss for batch 6785 : 0.10061550885438919\n",
      "Training loss for batch 6786 : 0.11936287581920624\n",
      "Training loss for batch 6787 : 0.08574272692203522\n",
      "Training loss for batch 6788 : 0.7043426036834717\n",
      "Training loss for batch 6789 : 0.3450968563556671\n",
      "Training loss for batch 6790 : 0.55174720287323\n",
      "Training loss for batch 6791 : 0.3959411382675171\n",
      "Training loss for batch 6792 : 0.0892447903752327\n",
      "Training loss for batch 6793 : 0.33055663108825684\n",
      "Training loss for batch 6794 : 0.3803664445877075\n",
      "Training loss for batch 6795 : 0.28785210847854614\n",
      "Training loss for batch 6796 : 0.10622906684875488\n",
      "Training loss for batch 6797 : 0.13242849707603455\n",
      "Training loss for batch 6798 : 0.011032572016119957\n",
      "Training loss for batch 6799 : 0.6293503642082214\n",
      "Training loss for batch 6800 : 0.03027334436774254\n",
      "Training loss for batch 6801 : 0.13375796377658844\n",
      "Training loss for batch 6802 : 0.14493878185749054\n",
      "Training loss for batch 6803 : 0.2432025969028473\n",
      "Training loss for batch 6804 : 0.27711308002471924\n",
      "Training loss for batch 6805 : 0.0785076916217804\n",
      "Training loss for batch 6806 : 0.06608089804649353\n",
      "Training loss for batch 6807 : 0.3799312710762024\n",
      "Training loss for batch 6808 : 0.5100698471069336\n",
      "Training loss for batch 6809 : 0.4452908933162689\n",
      "Training loss for batch 6810 : 0.37532728910446167\n",
      "Training loss for batch 6811 : 0.08851867914199829\n",
      "Training loss for batch 6812 : 0.14671722054481506\n",
      "Training loss for batch 6813 : 0.25970256328582764\n",
      "Training loss for batch 6814 : 0.2548123598098755\n",
      "Training loss for batch 6815 : 0.16709131002426147\n",
      "Training loss for batch 6816 : 0.2758343517780304\n",
      "Training loss for batch 6817 : 0.31059902906417847\n",
      "Training loss for batch 6818 : 0.24901226162910461\n",
      "Training loss for batch 6819 : 0.31573623418807983\n",
      "Training loss for batch 6820 : 0.15638190507888794\n",
      "Training loss for batch 6821 : 0.5528391003608704\n",
      "Training loss for batch 6822 : 0.5318737030029297\n",
      "Training loss for batch 6823 : 0.07331669330596924\n",
      "Training loss for batch 6824 : 0.08282116055488586\n",
      "Training loss for batch 6825 : 0.2628701329231262\n",
      "Training loss for batch 6826 : 0.5251569151878357\n",
      "Training loss for batch 6827 : 0.025242766365408897\n",
      "Training loss for batch 6828 : 0.13808996975421906\n",
      "Training loss for batch 6829 : 0.0\n",
      "Training loss for batch 6830 : 0.2774062156677246\n",
      "Training loss for batch 6831 : 0.1820196807384491\n",
      "Training loss for batch 6832 : 0.1522863358259201\n",
      "Training loss for batch 6833 : 0.23295623064041138\n",
      "Training loss for batch 6834 : 0.2048734426498413\n",
      "Training loss for batch 6835 : 0.15743854641914368\n",
      "Training loss for batch 6836 : 0.19260273873806\n",
      "Training loss for batch 6837 : 0.05860714986920357\n",
      "Training loss for batch 6838 : 0.3465748727321625\n",
      "Training loss for batch 6839 : 0.007941603660583496\n",
      "Training loss for batch 6840 : 0.6651066541671753\n",
      "Training loss for batch 6841 : 0.22810153663158417\n",
      "Training loss for batch 6842 : 0.3615969717502594\n",
      "Training loss for batch 6843 : 0.10817290097475052\n",
      "Training loss for batch 6844 : 0.27772146463394165\n",
      "Training loss for batch 6845 : 0.08943904936313629\n",
      "Training loss for batch 6846 : 0.31909215450286865\n",
      "Training loss for batch 6847 : 0.5239133834838867\n",
      "Training loss for batch 6848 : 0.2684633433818817\n",
      "Training loss for batch 6849 : 0.15844596922397614\n",
      "Training loss for batch 6850 : 0.1572667360305786\n",
      "Training loss for batch 6851 : 0.3505263328552246\n",
      "Training loss for batch 6852 : 0.38280802965164185\n",
      "Training loss for batch 6853 : 0.08193308115005493\n",
      "Training loss for batch 6854 : 0.05714849382638931\n",
      "Training loss for batch 6855 : 0.2636236548423767\n",
      "Training loss for batch 6856 : 0.13434329628944397\n",
      "Training loss for batch 6857 : 0.10089553892612457\n",
      "Training loss for batch 6858 : 0.5215023159980774\n",
      "Training loss for batch 6859 : 0.10561192780733109\n",
      "Training loss for batch 6860 : 0.3012489676475525\n",
      "Training loss for batch 6861 : 0.31104791164398193\n",
      "Training loss for batch 6862 : 0.3660023808479309\n",
      "Training loss for batch 6863 : 0.1779775172472\n",
      "Training loss for batch 6864 : 0.36716794967651367\n",
      "Training loss for batch 6865 : 0.5207877159118652\n",
      "Training loss for batch 6866 : 0.36308276653289795\n",
      "Training loss for batch 6867 : 0.11561119556427002\n",
      "Training loss for batch 6868 : 0.09249962866306305\n",
      "Training loss for batch 6869 : 0.0657055675983429\n",
      "Training loss for batch 6870 : 0.029532773420214653\n",
      "Training loss for batch 6871 : 0.26792973279953003\n",
      "Training loss for batch 6872 : 0.1349717080593109\n",
      "Training loss for batch 6873 : 0.29234036803245544\n",
      "Training loss for batch 6874 : 0.4199106693267822\n",
      "Training loss for batch 6875 : 0.21575552225112915\n",
      "Training loss for batch 6876 : 0.1431654393672943\n",
      "Training loss for batch 6877 : 0.4816489517688751\n",
      "Training loss for batch 6878 : 0.43154335021972656\n",
      "Training loss for batch 6879 : 0.28437331318855286\n",
      "Training loss for batch 6880 : 0.001591116189956665\n",
      "Training loss for batch 6881 : 0.11994467675685883\n",
      "Training loss for batch 6882 : 0.1570330113172531\n",
      "Training loss for batch 6883 : 0.3154968321323395\n",
      "Training loss for batch 6884 : 0.16914835572242737\n",
      "Training loss for batch 6885 : 0.5228191614151001\n",
      "Training loss for batch 6886 : 0.3036644458770752\n",
      "Training loss for batch 6887 : 0.2942920923233032\n",
      "Training loss for batch 6888 : 0.5567322373390198\n",
      "Training loss for batch 6889 : 0.22614334523677826\n",
      "Training loss for batch 6890 : 0.20634286105632782\n",
      "Training loss for batch 6891 : 0.36284083127975464\n",
      "Training loss for batch 6892 : 0.5859233140945435\n",
      "Training loss for batch 6893 : 0.254045695066452\n",
      "Training loss for batch 6894 : 0.27364403009414673\n",
      "Training loss for batch 6895 : 0.1588270515203476\n",
      "Training loss for batch 6896 : 0.226312056183815\n",
      "Training loss for batch 6897 : 0.07255591452121735\n",
      "Training loss for batch 6898 : 0.16267672181129456\n",
      "Training loss for batch 6899 : 0.07832670956850052\n",
      "Training loss for batch 6900 : 0.07175598293542862\n",
      "Training loss for batch 6901 : 0.16650238633155823\n",
      "Training loss for batch 6902 : 0.36806949973106384\n",
      "Training loss for batch 6903 : 0.3820395767688751\n",
      "Training loss for batch 6904 : 0.14765578508377075\n",
      "Training loss for batch 6905 : 0.11357397586107254\n",
      "Training loss for batch 6906 : 0.08715375512838364\n",
      "Training loss for batch 6907 : 0.37001484632492065\n",
      "Training loss for batch 6908 : 0.24771039187908173\n",
      "Training loss for batch 6909 : 0.3512561619281769\n",
      "Training loss for batch 6910 : 0.6210322380065918\n",
      "Training loss for batch 6911 : 0.3837163746356964\n",
      "Training loss for batch 6912 : 0.18887414038181305\n",
      "Training loss for batch 6913 : 0.14932823181152344\n",
      "Training loss for batch 6914 : 0.2282664030790329\n",
      "Training loss for batch 6915 : 0.3670908212661743\n",
      "Training loss for batch 6916 : 0.04471384733915329\n",
      "Training loss for batch 6917 : 0.0857895165681839\n",
      "Training loss for batch 6918 : 0.20355640351772308\n",
      "Training loss for batch 6919 : 0.10951212793588638\n",
      "Training loss for batch 6920 : 0.18295468389987946\n",
      "Training loss for batch 6921 : 0.2807222902774811\n",
      "Training loss for batch 6922 : 0.010270981118083\n",
      "Training loss for batch 6923 : 0.034884750843048096\n",
      "Training loss for batch 6924 : 0.15757186710834503\n",
      "Training loss for batch 6925 : 0.060994986444711685\n",
      "Training loss for batch 6926 : 0.5911910533905029\n",
      "Training loss for batch 6927 : 0.15430408716201782\n",
      "Training loss for batch 6928 : 0.01038313377648592\n",
      "Training loss for batch 6929 : 0.3186957836151123\n",
      "Training loss for batch 6930 : 0.3471021056175232\n",
      "Training loss for batch 6931 : 0.52391117811203\n",
      "Training loss for batch 6932 : 0.28857025504112244\n",
      "Training loss for batch 6933 : 0.31102824211120605\n",
      "Training loss for batch 6934 : 0.1867283433675766\n",
      "Training loss for batch 6935 : 0.5480915904045105\n",
      "Training loss for batch 6936 : 0.0667535662651062\n",
      "Training loss for batch 6937 : 0.3496839106082916\n",
      "Training loss for batch 6938 : 0.6449055671691895\n",
      "Training loss for batch 6939 : 0.10325250774621964\n",
      "Training loss for batch 6940 : 0.9333694577217102\n",
      "Training loss for batch 6941 : 0.01830984093248844\n",
      "Training loss for batch 6942 : 0.20754873752593994\n",
      "Training loss for batch 6943 : 0.49251264333724976\n",
      "Training loss for batch 6944 : 0.08892636001110077\n",
      "Training loss for batch 6945 : 0.09172628819942474\n",
      "Training loss for batch 6946 : 0.7677581906318665\n",
      "Training loss for batch 6947 : 0.30458304286003113\n",
      "Training loss for batch 6948 : 0.15512341260910034\n",
      "Training loss for batch 6949 : 0.14174756407737732\n",
      "Training loss for batch 6950 : 0.08439837396144867\n",
      "Training loss for batch 6951 : 0.1720159500837326\n",
      "Training loss for batch 6952 : 0.057840026915073395\n",
      "Training loss for batch 6953 : 0.16734571754932404\n",
      "Training loss for batch 6954 : 0.4254642128944397\n",
      "Training loss for batch 6955 : 0.5284950733184814\n",
      "Training loss for batch 6956 : 0.6866429448127747\n",
      "Training loss for batch 6957 : 0.018375065177679062\n",
      "Training loss for batch 6958 : 0.23247045278549194\n",
      "Training loss for batch 6959 : 0.31073033809661865\n",
      "Training loss for batch 6960 : 0.10790780931711197\n",
      "Training loss for batch 6961 : 0.3216131925582886\n",
      "Training loss for batch 6962 : 0.15420839190483093\n",
      "Training loss for batch 6963 : 0.4104451537132263\n",
      "Training loss for batch 6964 : 0.056916702538728714\n",
      "Training loss for batch 6965 : 0.27974221110343933\n",
      "Training loss for batch 6966 : 0.1823364645242691\n",
      "Training loss for batch 6967 : 0.26830530166625977\n",
      "Training loss for batch 6968 : 0.03786659240722656\n",
      "Training loss for batch 6969 : 0.28535550832748413\n",
      "Training loss for batch 6970 : 0.014294790104031563\n",
      "Training loss for batch 6971 : 0.27156034111976624\n",
      "Training loss for batch 6972 : 0.23764722049236298\n",
      "Training loss for batch 6973 : 0.008411518298089504\n",
      "Training loss for batch 6974 : 0.4016656279563904\n",
      "Training loss for batch 6975 : 0.11125533282756805\n",
      "Training loss for batch 6976 : 0.021940838545560837\n",
      "Training loss for batch 6977 : 0.2836199402809143\n",
      "Training loss for batch 6978 : 0.2141360193490982\n",
      "Training loss for batch 6979 : 0.1259293556213379\n",
      "Training loss for batch 6980 : 0.2926042079925537\n",
      "Training loss for batch 6981 : 0.16693079471588135\n",
      "Training loss for batch 6982 : 0.16915515065193176\n",
      "Training loss for batch 6983 : 0.062041498720645905\n",
      "Training loss for batch 6984 : 0.6252039074897766\n",
      "Training loss for batch 6985 : 0.08595878630876541\n",
      "Training loss for batch 6986 : 0.15856288373470306\n",
      "Training loss for batch 6987 : 0.010197202675044537\n",
      "Training loss for batch 6988 : 0.27425605058670044\n",
      "Training loss for batch 6989 : 0.10414403676986694\n",
      "Training loss for batch 6990 : 0.11342137306928635\n",
      "Training loss for batch 6991 : 0.31736719608306885\n",
      "Training loss for batch 6992 : 0.17463678121566772\n",
      "Training loss for batch 6993 : 0.38531020283699036\n",
      "Training loss for batch 6994 : 0.05361638590693474\n",
      "Training loss for batch 6995 : 0.40100347995758057\n",
      "Training loss for batch 6996 : 0.21187329292297363\n",
      "Training loss for batch 6997 : 0.12823635339736938\n",
      "Training loss for batch 6998 : 0.64937424659729\n",
      "Training loss for batch 6999 : 0.27458643913269043\n",
      "Training loss for batch 7000 : 0.5836461186408997\n",
      "Training loss for batch 7001 : 0.35446205735206604\n",
      "Training loss for batch 7002 : 0.31491005420684814\n",
      "Training loss for batch 7003 : 0.385055810213089\n",
      "Training loss for batch 7004 : 0.5115392208099365\n",
      "Training loss for batch 7005 : 0.5479210019111633\n",
      "Training loss for batch 7006 : 0.18959763646125793\n",
      "Training loss for batch 7007 : 0.16031672060489655\n",
      "Training loss for batch 7008 : 0.14686037600040436\n",
      "Training loss for batch 7009 : 0.3063265383243561\n",
      "Training loss for batch 7010 : 0.28380343317985535\n",
      "Training loss for batch 7011 : 0.2241799384355545\n",
      "Training loss for batch 7012 : 0.10376054048538208\n",
      "Training loss for batch 7013 : 0.286311537027359\n",
      "Training loss for batch 7014 : 0.12593123316764832\n",
      "Training loss for batch 7015 : 0.6096755862236023\n",
      "Training loss for batch 7016 : 0.06358836591243744\n",
      "Training loss for batch 7017 : 0.0\n",
      "Training loss for batch 7018 : 0.07812638580799103\n",
      "Training loss for batch 7019 : 0.0\n",
      "Training loss for batch 7020 : 0.20345117151737213\n",
      "Training loss for batch 7021 : 0.4205368161201477\n",
      "Training loss for batch 7022 : 0.14048761129379272\n",
      "Training loss for batch 7023 : 0.25422972440719604\n",
      "Training loss for batch 7024 : 0.4458801746368408\n",
      "Training loss for batch 7025 : 0.5100510716438293\n",
      "Training loss for batch 7026 : 0.5419650077819824\n",
      "Training loss for batch 7027 : 0.04342057555913925\n",
      "Training loss for batch 7028 : 0.16337868571281433\n",
      "Training loss for batch 7029 : 0.4500134289264679\n",
      "Training loss for batch 7030 : 0.025254257023334503\n",
      "Training loss for batch 7031 : 0.5568212270736694\n",
      "Training loss for batch 7032 : 0.24547994136810303\n",
      "Training loss for batch 7033 : 0.3418300151824951\n",
      "Training loss for batch 7034 : 0.206025630235672\n",
      "Training loss for batch 7035 : 0.052498798817396164\n",
      "Training loss for batch 7036 : 0.06884251534938812\n",
      "Training loss for batch 7037 : 0.21251754462718964\n",
      "Training loss for batch 7038 : 0.3644476532936096\n",
      "Training loss for batch 7039 : 0.612238347530365\n",
      "Training loss for batch 7040 : 0.6308189630508423\n",
      "Training loss for batch 7041 : 0.08967199921607971\n",
      "Training loss for batch 7042 : 0.17392639815807343\n",
      "Training loss for batch 7043 : 0.053329676389694214\n",
      "Training loss for batch 7044 : 0.0785299688577652\n",
      "Training loss for batch 7045 : 0.07805688679218292\n",
      "Training loss for batch 7046 : 0.028247278183698654\n",
      "Training loss for batch 7047 : 0.1391504406929016\n",
      "Training loss for batch 7048 : 0.052129387855529785\n",
      "Training loss for batch 7049 : 0.20661377906799316\n",
      "Training loss for batch 7050 : 0.3274115025997162\n",
      "Training loss for batch 7051 : 0.2479524463415146\n",
      "Training loss for batch 7052 : 0.06916705518960953\n",
      "Training loss for batch 7053 : 0.1664414256811142\n",
      "Training loss for batch 7054 : 0.1693667322397232\n",
      "Training loss for batch 7055 : 0.17656123638153076\n",
      "Training loss for batch 7056 : 0.16251510381698608\n",
      "Training loss for batch 7057 : 0.16529089212417603\n",
      "Training loss for batch 7058 : 0.16236534714698792\n",
      "Training loss for batch 7059 : 0.3427290618419647\n",
      "Training loss for batch 7060 : 0.008932169526815414\n",
      "Training loss for batch 7061 : 0.025891076773405075\n",
      "Training loss for batch 7062 : 0.2789190411567688\n",
      "Training loss for batch 7063 : 0.3978085219860077\n",
      "Training loss for batch 7064 : 0.04179941490292549\n",
      "Training loss for batch 7065 : 0.05294384807348251\n",
      "Training loss for batch 7066 : 0.040146004408597946\n",
      "Training loss for batch 7067 : 0.028537878766655922\n",
      "Training loss for batch 7068 : 0.04042398929595947\n",
      "Training loss for batch 7069 : 0.3335151672363281\n",
      "Training loss for batch 7070 : 0.3781355917453766\n",
      "Training loss for batch 7071 : 0.34196510910987854\n",
      "Training loss for batch 7072 : 0.6110044717788696\n",
      "Training loss for batch 7073 : 0.12972652912139893\n",
      "Training loss for batch 7074 : 0.11917772889137268\n",
      "Training loss for batch 7075 : 0.2906990349292755\n",
      "Training loss for batch 7076 : 0.3455914556980133\n",
      "Training loss for batch 7077 : 0.22670097649097443\n",
      "Training loss for batch 7078 : 0.5785112977027893\n",
      "Training loss for batch 7079 : 0.31735336780548096\n",
      "Training loss for batch 7080 : 0.281951904296875\n",
      "Training loss for batch 7081 : 0.3422401547431946\n",
      "Training loss for batch 7082 : 0.2969890236854553\n",
      "Training loss for batch 7083 : 0.0325416661798954\n",
      "Training loss for batch 7084 : 0.47563832998275757\n",
      "Training loss for batch 7085 : 0.2659376263618469\n",
      "Training loss for batch 7086 : 0.3599829375743866\n",
      "Training loss for batch 7087 : 0.02101597934961319\n",
      "Training loss for batch 7088 : 0.0\n",
      "Training loss for batch 7089 : 0.007328669540584087\n",
      "Training loss for batch 7090 : 0.18636858463287354\n",
      "Training loss for batch 7091 : 0.18716274201869965\n",
      "Training loss for batch 7092 : 0.013854335993528366\n",
      "Training loss for batch 7093 : 0.5143797993659973\n",
      "Training loss for batch 7094 : 0.05459822341799736\n",
      "Training loss for batch 7095 : 0.34147414565086365\n",
      "Training loss for batch 7096 : 0.6362819671630859\n",
      "Training loss for batch 7097 : 0.2288302779197693\n",
      "Training loss for batch 7098 : 0.12187846750020981\n",
      "Training loss for batch 7099 : 0.41216567158699036\n",
      "Training loss for batch 7100 : 0.47167742252349854\n",
      "Training loss for batch 7101 : 0.16731169819831848\n",
      "Training loss for batch 7102 : 0.0999387577176094\n",
      "Training loss for batch 7103 : 0.14796556532382965\n",
      "Training loss for batch 7104 : 0.5606706738471985\n",
      "Training loss for batch 7105 : 0.1522217094898224\n",
      "Training loss for batch 7106 : 0.06410437077283859\n",
      "Training loss for batch 7107 : 0.19671638309955597\n",
      "Training loss for batch 7108 : 0.4267967641353607\n",
      "Training loss for batch 7109 : 0.26515844464302063\n",
      "Training loss for batch 7110 : 0.07388285547494888\n",
      "Training loss for batch 7111 : 0.050283145159482956\n",
      "Training loss for batch 7112 : 0.004817396402359009\n",
      "Training loss for batch 7113 : 0.5748772621154785\n",
      "Training loss for batch 7114 : 0.08545471727848053\n",
      "Training loss for batch 7115 : 0.17025263607501984\n",
      "Training loss for batch 7116 : 0.3125460743904114\n",
      "Training loss for batch 7117 : 0.15194736421108246\n",
      "Training loss for batch 7118 : 0.2286256104707718\n",
      "Training loss for batch 7119 : 0.06836385279893875\n",
      "Training loss for batch 7120 : 0.12785457074642181\n",
      "Training loss for batch 7121 : 0.12297094613313675\n",
      "Training loss for batch 7122 : 0.33046889305114746\n",
      "Training loss for batch 7123 : 0.039130453020334244\n",
      "Training loss for batch 7124 : 0.04204963147640228\n",
      "Training loss for batch 7125 : 0.40045446157455444\n",
      "Training loss for batch 7126 : 0.09265439212322235\n",
      "Training loss for batch 7127 : 0.4667379856109619\n",
      "Training loss for batch 7128 : 0.02049018256366253\n",
      "Training loss for batch 7129 : 0.3418155312538147\n",
      "Training loss for batch 7130 : 0.11348374933004379\n",
      "Training loss for batch 7131 : 0.18900464475154877\n",
      "Training loss for batch 7132 : 0.2531100809574127\n",
      "Training loss for batch 7133 : 0.09707512706518173\n",
      "Training loss for batch 7134 : 0.20942285656929016\n",
      "Training loss for batch 7135 : 0.5330973863601685\n",
      "Training loss for batch 7136 : 0.04072773829102516\n",
      "Training loss for batch 7137 : 0.10367825627326965\n",
      "Training loss for batch 7138 : 0.40775132179260254\n",
      "Training loss for batch 7139 : 0.12324542552232742\n",
      "Training loss for batch 7140 : 0.0\n",
      "Training loss for batch 7141 : 0.21562398970127106\n",
      "Training loss for batch 7142 : 0.15901628136634827\n",
      "Training loss for batch 7143 : 1.0243076086044312\n",
      "Training loss for batch 7144 : 0.16474997997283936\n",
      "Training loss for batch 7145 : 0.5080061554908752\n",
      "Training loss for batch 7146 : 0.16805998980998993\n",
      "Training loss for batch 7147 : 0.01917322538793087\n",
      "Training loss for batch 7148 : 0.30929699540138245\n",
      "Training loss for batch 7149 : 0.2720305323600769\n",
      "Training loss for batch 7150 : 0.35508573055267334\n",
      "Training loss for batch 7151 : 0.07197783887386322\n",
      "Training loss for batch 7152 : 0.4597441554069519\n",
      "Training loss for batch 7153 : 0.04499173164367676\n",
      "Training loss for batch 7154 : 0.11098439991474152\n",
      "Training loss for batch 7155 : 0.17767322063446045\n",
      "Training loss for batch 7156 : 0.052946023643016815\n",
      "Training loss for batch 7157 : 0.1437728852033615\n",
      "Training loss for batch 7158 : 0.06022170931100845\n",
      "Training loss for batch 7159 : 0.12464796006679535\n",
      "Training loss for batch 7160 : 0.15922433137893677\n",
      "Training loss for batch 7161 : 0.11088477820158005\n",
      "Training loss for batch 7162 : 0.43523192405700684\n",
      "Training loss for batch 7163 : 0.3483201861381531\n",
      "Training loss for batch 7164 : 0.16649684309959412\n",
      "Training loss for batch 7165 : 0.488320916891098\n",
      "Training loss for batch 7166 : 0.2197391539812088\n",
      "Training loss for batch 7167 : 0.07924644649028778\n",
      "Training loss for batch 7168 : 0.23574019968509674\n",
      "Training loss for batch 7169 : 0.024555161595344543\n",
      "Training loss for batch 7170 : 0.012021585367619991\n",
      "Training loss for batch 7171 : 0.09725145995616913\n",
      "Training loss for batch 7172 : 0.26103803515434265\n",
      "Training loss for batch 7173 : 0.6174266338348389\n",
      "Training loss for batch 7174 : 0.19175946712493896\n",
      "Training loss for batch 7175 : 0.4496171474456787\n",
      "Training loss for batch 7176 : 0.596799910068512\n",
      "Training loss for batch 7177 : 0.2725813686847687\n",
      "Training loss for batch 7178 : 0.1757051646709442\n",
      "Training loss for batch 7179 : 0.015467236749827862\n",
      "Training loss for batch 7180 : 0.8916816115379333\n",
      "Training loss for batch 7181 : 0.06730380654335022\n",
      "Training loss for batch 7182 : 0.13000699877738953\n",
      "Training loss for batch 7183 : 0.3471533954143524\n",
      "Training loss for batch 7184 : 0.21565909683704376\n",
      "Training loss for batch 7185 : 0.04261363297700882\n",
      "Training loss for batch 7186 : 0.1177625060081482\n",
      "Training loss for batch 7187 : 0.6528151631355286\n",
      "Training loss for batch 7188 : 0.15861892700195312\n",
      "Training loss for batch 7189 : 0.05029013752937317\n",
      "Training loss for batch 7190 : 0.05731472373008728\n",
      "Training loss for batch 7191 : 0.3860950171947479\n",
      "Training loss for batch 7192 : 0.11911282688379288\n",
      "Training loss for batch 7193 : 0.1441769301891327\n",
      "Training loss for batch 7194 : 0.028706979006528854\n",
      "Training loss for batch 7195 : 0.46213701367378235\n",
      "Training loss for batch 7196 : 0.3048362731933594\n",
      "Training loss for batch 7197 : 0.17704954743385315\n",
      "Training loss for batch 7198 : 0.14378657937049866\n",
      "Training loss for batch 7199 : 0.10566620528697968\n",
      "Training loss for batch 7200 : 0.06715208292007446\n",
      "Training loss for batch 7201 : 0.7061547636985779\n",
      "Training loss for batch 7202 : 0.13101571798324585\n",
      "Training loss for batch 7203 : 0.15831755101680756\n",
      "Training loss for batch 7204 : 0.6504387855529785\n",
      "Training loss for batch 7205 : 0.0658387839794159\n",
      "Training loss for batch 7206 : 0.11405814439058304\n",
      "Training loss for batch 7207 : 0.4340295195579529\n",
      "Training loss for batch 7208 : 0.11349037289619446\n",
      "Training loss for batch 7209 : 0.3863011598587036\n",
      "Training loss for batch 7210 : 0.20119431614875793\n",
      "Training loss for batch 7211 : 0.29864874482154846\n",
      "Training loss for batch 7212 : 0.37571144104003906\n",
      "Training loss for batch 7213 : 0.06803514063358307\n",
      "Training loss for batch 7214 : 0.12306484580039978\n",
      "Training loss for batch 7215 : 0.046595118939876556\n",
      "Training loss for batch 7216 : 0.8766248822212219\n",
      "Training loss for batch 7217 : 0.0\n",
      "Training loss for batch 7218 : 0.101737841963768\n",
      "Training loss for batch 7219 : 0.19803109765052795\n",
      "Training loss for batch 7220 : 0.0\n",
      "Training loss for batch 7221 : 0.1283525675535202\n",
      "Training loss for batch 7222 : 0.7183380126953125\n",
      "Training loss for batch 7223 : 0.03758290410041809\n",
      "Training loss for batch 7224 : 0.0993587002158165\n",
      "Training loss for batch 7225 : 0.15769338607788086\n",
      "Training loss for batch 7226 : 0.6410808563232422\n",
      "Training loss for batch 7227 : 0.3395957350730896\n",
      "Training loss for batch 7228 : 0.2156369686126709\n",
      "Training loss for batch 7229 : 0.41075384616851807\n",
      "Training loss for batch 7230 : 0.08457313477993011\n",
      "Training loss for batch 7231 : 0.31566888093948364\n",
      "Training loss for batch 7232 : 0.3574243485927582\n",
      "Training loss for batch 7233 : 0.024291569367051125\n",
      "Training loss for batch 7234 : 0.04724084958434105\n",
      "Training loss for batch 7235 : 0.039733003824949265\n",
      "Training loss for batch 7236 : 0.0032378635369241238\n",
      "Training loss for batch 7237 : 0.6848892569541931\n",
      "Training loss for batch 7238 : 0.04361611232161522\n",
      "Training loss for batch 7239 : 0.16021452844142914\n",
      "Training loss for batch 7240 : 0.2703110873699188\n",
      "Training loss for batch 7241 : 0.3411718010902405\n",
      "Training loss for batch 7242 : 0.4168059229850769\n",
      "Training loss for batch 7243 : 0.6095420122146606\n",
      "Training loss for batch 7244 : 0.423786997795105\n",
      "Training loss for batch 7245 : 0.23574022948741913\n",
      "Training loss for batch 7246 : 0.3375193476676941\n",
      "Training loss for batch 7247 : 0.005183270666748285\n",
      "Training loss for batch 7248 : 0.08283589780330658\n",
      "Training loss for batch 7249 : 0.13590675592422485\n",
      "Training loss for batch 7250 : 0.14973647892475128\n",
      "Training loss for batch 7251 : 0.054968781769275665\n",
      "Training loss for batch 7252 : 0.4267404079437256\n",
      "Training loss for batch 7253 : 0.2792952358722687\n",
      "Training loss for batch 7254 : 0.08947570621967316\n",
      "Training loss for batch 7255 : 0.03496472164988518\n",
      "Training loss for batch 7256 : 0.4118099510669708\n",
      "Training loss for batch 7257 : 0.2961704432964325\n",
      "Training loss for batch 7258 : 0.31131988763809204\n",
      "Training loss for batch 7259 : 0.16621030867099762\n",
      "Training loss for batch 7260 : 0.1381116658449173\n",
      "Training loss for batch 7261 : 0.08599235117435455\n",
      "Training loss for batch 7262 : 0.7242953777313232\n",
      "Training loss for batch 7263 : 0.22670316696166992\n",
      "Training loss for batch 7264 : 0.011175434105098248\n",
      "Training loss for batch 7265 : 0.23269855976104736\n",
      "Training loss for batch 7266 : 0.01276966743171215\n",
      "Training loss for batch 7267 : 0.08765162527561188\n",
      "Training loss for batch 7268 : 0.3556554615497589\n",
      "Training loss for batch 7269 : 0.134360671043396\n",
      "Training loss for batch 7270 : 0.061292439699172974\n",
      "Training loss for batch 7271 : 0.44155076146125793\n",
      "Training loss for batch 7272 : 0.2482106238603592\n",
      "Training loss for batch 7273 : 0.46690210700035095\n",
      "Training loss for batch 7274 : 0.15697810053825378\n",
      "Training loss for batch 7275 : 0.29912269115448\n",
      "Training loss for batch 7276 : 0.08416362851858139\n",
      "Training loss for batch 7277 : 0.44962844252586365\n",
      "Training loss for batch 7278 : 0.2785053551197052\n",
      "Training loss for batch 7279 : 0.2166045606136322\n",
      "Training loss for batch 7280 : 0.25520235300064087\n",
      "Training loss for batch 7281 : 0.27430012822151184\n",
      "Training loss for batch 7282 : 0.19720517098903656\n",
      "Training loss for batch 7283 : 0.08296684175729752\n",
      "Training loss for batch 7284 : 0.5647208094596863\n",
      "Training loss for batch 7285 : 0.6610507369041443\n",
      "Training loss for batch 7286 : 0.01784813590347767\n",
      "Training loss for batch 7287 : 0.4624543786048889\n",
      "Training loss for batch 7288 : 0.1228741928935051\n",
      "Training loss for batch 7289 : 0.17737655341625214\n",
      "Training loss for batch 7290 : 0.49035507440567017\n",
      "Training loss for batch 7291 : 0.0872395932674408\n",
      "Training loss for batch 7292 : 0.4001438021659851\n",
      "Training loss for batch 7293 : 0.0\n",
      "Training loss for batch 7294 : 0.24248915910720825\n",
      "Training loss for batch 7295 : 0.5913006663322449\n",
      "Training loss for batch 7296 : 0.12746979296207428\n",
      "Training loss for batch 7297 : 0.08225023746490479\n",
      "Training loss for batch 7298 : 0.11535128951072693\n",
      "Training loss for batch 7299 : 0.6128713488578796\n",
      "Training loss for batch 7300 : 0.2747596204280853\n",
      "Training loss for batch 7301 : 0.2118299901485443\n",
      "Training loss for batch 7302 : 0.31723302602767944\n",
      "Training loss for batch 7303 : 0.3399001359939575\n",
      "Training loss for batch 7304 : 0.1104128286242485\n",
      "Training loss for batch 7305 : 0.3921210765838623\n",
      "Training loss for batch 7306 : 0.12470676749944687\n",
      "Training loss for batch 7307 : 0.13113375008106232\n",
      "Training loss for batch 7308 : 0.09707936644554138\n",
      "Training loss for batch 7309 : 0.33569300174713135\n",
      "Training loss for batch 7310 : 0.33408141136169434\n",
      "Training loss for batch 7311 : 0.042080607265233994\n",
      "Training loss for batch 7312 : 0.27166828513145447\n",
      "Training loss for batch 7313 : 0.0\n",
      "Training loss for batch 7314 : 0.315644770860672\n",
      "Training loss for batch 7315 : 0.1774599850177765\n",
      "Training loss for batch 7316 : 0.1815582811832428\n",
      "Training loss for batch 7317 : 0.11615212261676788\n",
      "Training loss for batch 7318 : 0.17733286321163177\n",
      "Training loss for batch 7319 : 0.14114952087402344\n",
      "Training loss for batch 7320 : 0.38028401136398315\n",
      "Training loss for batch 7321 : 0.49500828981399536\n",
      "Training loss for batch 7322 : 0.11478621512651443\n",
      "Training loss for batch 7323 : 0.5570632815361023\n",
      "Training loss for batch 7324 : 0.2804337739944458\n",
      "Training loss for batch 7325 : 0.3873984217643738\n",
      "Training loss for batch 7326 : 0.1640489250421524\n",
      "Training loss for batch 7327 : 0.14436160027980804\n",
      "Training loss for batch 7328 : 0.3983367681503296\n",
      "Training loss for batch 7329 : 0.09013142436742783\n",
      "Training loss for batch 7330 : 0.3595195710659027\n",
      "Training loss for batch 7331 : 0.04063569754362106\n",
      "Training loss for batch 7332 : 0.20853130519390106\n",
      "Training loss for batch 7333 : 0.08364077657461166\n",
      "Training loss for batch 7334 : 0.09955181181430817\n",
      "Training loss for batch 7335 : 0.006062030792236328\n",
      "Training loss for batch 7336 : 0.4864743947982788\n",
      "Training loss for batch 7337 : 0.21346253156661987\n",
      "Training loss for batch 7338 : 0.015754995867609978\n",
      "Training loss for batch 7339 : 0.011506538838148117\n",
      "Training loss for batch 7340 : 0.00021324021508917212\n",
      "Training loss for batch 7341 : 0.17303375899791718\n",
      "Training loss for batch 7342 : 0.041113123297691345\n",
      "Training loss for batch 7343 : 0.23076236248016357\n",
      "Training loss for batch 7344 : 0.06656631082296371\n",
      "Training loss for batch 7345 : 0.32127833366394043\n",
      "Training loss for batch 7346 : 0.6560984253883362\n",
      "Training loss for batch 7347 : 0.1897861510515213\n",
      "Training loss for batch 7348 : 0.11636044830083847\n",
      "Training loss for batch 7349 : 0.05813688039779663\n",
      "Training loss for batch 7350 : 0.003264695405960083\n",
      "Training loss for batch 7351 : 0.004289090633392334\n",
      "Training loss for batch 7352 : 0.254448801279068\n",
      "Training loss for batch 7353 : 0.03143371641635895\n",
      "Training loss for batch 7354 : 0.07565869390964508\n",
      "Training loss for batch 7355 : 0.1516936868429184\n",
      "Training loss for batch 7356 : 0.41586118936538696\n",
      "Training loss for batch 7357 : 0.061955440789461136\n",
      "Training loss for batch 7358 : 0.4866259694099426\n",
      "Training loss for batch 7359 : 0.41324421763420105\n",
      "Training loss for batch 7360 : 0.2861763834953308\n",
      "Training loss for batch 7361 : 0.2727113664150238\n",
      "Training loss for batch 7362 : 0.390248566865921\n",
      "Training loss for batch 7363 : 0.3335402309894562\n",
      "Training loss for batch 7364 : 0.26295921206474304\n",
      "Training loss for batch 7365 : 0.1876019835472107\n",
      "Training loss for batch 7366 : 0.12903454899787903\n",
      "Training loss for batch 7367 : 0.44182154536247253\n",
      "Training loss for batch 7368 : 0.019319167360663414\n",
      "Training loss for batch 7369 : 0.27478599548339844\n",
      "Training loss for batch 7370 : 0.3999527096748352\n",
      "Training loss for batch 7371 : 0.02017536200582981\n",
      "Training loss for batch 7372 : 0.12001920491456985\n",
      "Training loss for batch 7373 : 0.34139519929885864\n",
      "Training loss for batch 7374 : 0.44522297382354736\n",
      "Training loss for batch 7375 : 0.14277732372283936\n",
      "Training loss for batch 7376 : 0.21457451581954956\n",
      "Training loss for batch 7377 : 0.0342521071434021\n",
      "Training loss for batch 7378 : 0.14532549679279327\n",
      "Training loss for batch 7379 : 0.049517981708049774\n",
      "Training loss for batch 7380 : 0.082333505153656\n",
      "Training loss for batch 7381 : 0.13937385380268097\n",
      "Training loss for batch 7382 : 0.11319372057914734\n",
      "Training loss for batch 7383 : 0.3592747151851654\n",
      "Training loss for batch 7384 : 0.10374420136213303\n",
      "Training loss for batch 7385 : 0.36208078265190125\n",
      "Training loss for batch 7386 : 0.20385301113128662\n",
      "Training loss for batch 7387 : 0.11913907527923584\n",
      "Training loss for batch 7388 : 0.18688714504241943\n",
      "Training loss for batch 7389 : 0.0567576102912426\n",
      "Training loss for batch 7390 : 0.2853492200374603\n",
      "Training loss for batch 7391 : 0.04248623177409172\n",
      "Training loss for batch 7392 : 0.22577272355556488\n",
      "Training loss for batch 7393 : 0.22698740661144257\n",
      "Training loss for batch 7394 : 0.2071802020072937\n",
      "Training loss for batch 7395 : 0.43731024861335754\n",
      "Training loss for batch 7396 : 0.4085863530635834\n",
      "Training loss for batch 7397 : 0.40794262290000916\n",
      "Training loss for batch 7398 : 0.026505643501877785\n",
      "Training loss for batch 7399 : 0.10608211904764175\n",
      "Training loss for batch 7400 : 0.02057538740336895\n",
      "Training loss for batch 7401 : 0.1798938661813736\n",
      "Training loss for batch 7402 : 0.00046291088801808655\n",
      "Training loss for batch 7403 : 0.21751315891742706\n",
      "Training loss for batch 7404 : 0.1049606129527092\n",
      "Training loss for batch 7405 : 0.06359212845563889\n",
      "Training loss for batch 7406 : 0.03546864911913872\n",
      "Training loss for batch 7407 : 0.39852574467658997\n",
      "Training loss for batch 7408 : 0.2843620479106903\n",
      "Training loss for batch 7409 : 0.10846459865570068\n",
      "Training loss for batch 7410 : 0.35907313227653503\n",
      "Training loss for batch 7411 : 0.429697722196579\n",
      "Training loss for batch 7412 : 0.5029672980308533\n",
      "Training loss for batch 7413 : 0.6209883093833923\n",
      "Training loss for batch 7414 : 0.06934969872236252\n",
      "Training loss for batch 7415 : 0.097530297935009\n",
      "Training loss for batch 7416 : 0.3006192147731781\n",
      "Training loss for batch 7417 : 0.3798169195652008\n",
      "Training loss for batch 7418 : 0.08102380484342575\n",
      "Training loss for batch 7419 : 0.16364823281764984\n",
      "Training loss for batch 7420 : 0.05041560158133507\n",
      "Training loss for batch 7421 : 0.061756059527397156\n",
      "Training loss for batch 7422 : 0.052675310522317886\n",
      "Training loss for batch 7423 : 0.04909485951066017\n",
      "Training loss for batch 7424 : 0.0\n",
      "Training loss for batch 7425 : 0.043876610696315765\n",
      "Training loss for batch 7426 : 0.31739142537117004\n",
      "Training loss for batch 7427 : 0.0\n",
      "Training loss for batch 7428 : 0.0208535548299551\n",
      "Training loss for batch 7429 : 0.6607630252838135\n",
      "Training loss for batch 7430 : 0.5752332806587219\n",
      "Training loss for batch 7431 : 0.12084534019231796\n",
      "Training loss for batch 7432 : 0.08995906263589859\n",
      "Training loss for batch 7433 : 0.28319448232650757\n",
      "Training loss for batch 7434 : 0.3127167522907257\n",
      "Training loss for batch 7435 : 0.2662000358104706\n",
      "Training loss for batch 7436 : 0.29404571652412415\n",
      "Training loss for batch 7437 : 0.2863280475139618\n",
      "Training loss for batch 7438 : 0.13508959114551544\n",
      "Training loss for batch 7439 : 0.04836401715874672\n",
      "Training loss for batch 7440 : 0.2219356745481491\n",
      "Training loss for batch 7441 : 0.031078554689884186\n",
      "Training loss for batch 7442 : 0.35123538970947266\n",
      "Training loss for batch 7443 : 0.3529767692089081\n",
      "Training loss for batch 7444 : 0.034185755997896194\n",
      "Training loss for batch 7445 : 0.1323239952325821\n",
      "Training loss for batch 7446 : 0.2148800939321518\n",
      "Training loss for batch 7447 : 0.6582450270652771\n",
      "Training loss for batch 7448 : 0.024655506014823914\n",
      "Training loss for batch 7449 : 0.8016160726547241\n",
      "Training loss for batch 7450 : 0.3644144535064697\n",
      "Training loss for batch 7451 : 0.23459631204605103\n",
      "Training loss for batch 7452 : 0.4208582639694214\n",
      "Training loss for batch 7453 : 0.026981370523571968\n",
      "Training loss for batch 7454 : 0.38862502574920654\n",
      "Training loss for batch 7455 : 0.6370066404342651\n",
      "Training loss for batch 7456 : 0.34939682483673096\n",
      "Training loss for batch 7457 : 0.1367449313402176\n",
      "Training loss for batch 7458 : 0.18834887444972992\n",
      "Training loss for batch 7459 : 0.07470807433128357\n",
      "Training loss for batch 7460 : 0.24613124132156372\n",
      "Training loss for batch 7461 : 0.024467241019010544\n",
      "Training loss for batch 7462 : 0.2887829542160034\n",
      "Training loss for batch 7463 : 0.7966287732124329\n",
      "Training loss for batch 7464 : 0.24430178105831146\n",
      "Training loss for batch 7465 : 0.04437398165464401\n",
      "Training loss for batch 7466 : 0.30804339051246643\n",
      "Training loss for batch 7467 : 0.05187595263123512\n",
      "Training loss for batch 7468 : 0.36358052492141724\n",
      "Training loss for batch 7469 : 0.22861796617507935\n",
      "Training loss for batch 7470 : 0.4362775981426239\n",
      "Training loss for batch 7471 : 0.032054923474788666\n",
      "Training loss for batch 7472 : 0.09324655681848526\n",
      "Training loss for batch 7473 : 0.42229560017585754\n",
      "Training loss for batch 7474 : 0.12193960696458817\n",
      "Training loss for batch 7475 : 0.21574817597866058\n",
      "Training loss for batch 7476 : 0.23337705433368683\n",
      "Training loss for batch 7477 : 0.15185518562793732\n",
      "Training loss for batch 7478 : 0.003584484336897731\n",
      "Training loss for batch 7479 : 0.2804962992668152\n",
      "Training loss for batch 7480 : 0.28551578521728516\n",
      "Training loss for batch 7481 : 0.16250428557395935\n",
      "Training loss for batch 7482 : 0.09226486086845398\n",
      "Training loss for batch 7483 : 0.04383775219321251\n",
      "Training loss for batch 7484 : 0.12851187586784363\n",
      "Training loss for batch 7485 : 0.30999648571014404\n",
      "Training loss for batch 7486 : 0.39982181787490845\n",
      "Training loss for batch 7487 : 0.3098618984222412\n",
      "Training loss for batch 7488 : 0.3024190664291382\n",
      "Training loss for batch 7489 : 0.03850039467215538\n",
      "Training loss for batch 7490 : 0.4481825828552246\n",
      "Training loss for batch 7491 : 0.37279415130615234\n",
      "Training loss for batch 7492 : 0.02217797562479973\n",
      "Training loss for batch 7493 : 0.051184527575969696\n",
      "Training loss for batch 7494 : 0.19355660676956177\n",
      "Training loss for batch 7495 : 0.4342718720436096\n",
      "Training loss for batch 7496 : 0.12015292793512344\n",
      "Training loss for batch 7497 : 0.015439989976584911\n",
      "Training loss for batch 7498 : 0.01744340918958187\n",
      "Training loss for batch 7499 : 0.44446277618408203\n",
      "Training loss for batch 7500 : 0.30626365542411804\n",
      "Training loss for batch 7501 : 0.1354139894247055\n",
      "Training loss for batch 7502 : 0.521703839302063\n",
      "Training loss for batch 7503 : 0.29136529564857483\n",
      "Training loss for batch 7504 : 0.44706636667251587\n",
      "Training loss for batch 7505 : 0.12445312738418579\n",
      "Training loss for batch 7506 : 0.47056010365486145\n",
      "Training loss for batch 7507 : 0.43909701704978943\n",
      "Training loss for batch 7508 : 0.3255135118961334\n",
      "Training loss for batch 7509 : 0.21901528537273407\n",
      "Training loss for batch 7510 : 0.05810947343707085\n",
      "Training loss for batch 7511 : 0.11817657202482224\n",
      "Training loss for batch 7512 : 0.30293959379196167\n",
      "Training loss for batch 7513 : 0.21896465122699738\n",
      "Training loss for batch 7514 : 0.19206175208091736\n",
      "Training loss for batch 7515 : 0.39861324429512024\n",
      "Training loss for batch 7516 : 0.20766693353652954\n",
      "Training loss for batch 7517 : 0.041149869561195374\n",
      "Training loss for batch 7518 : 0.3092561960220337\n",
      "Training loss for batch 7519 : 0.10686760395765305\n",
      "Training loss for batch 7520 : 0.16088971495628357\n",
      "Training loss for batch 7521 : 0.18419450521469116\n",
      "Training loss for batch 7522 : 0.21276360750198364\n",
      "Training loss for batch 7523 : 0.2686727046966553\n",
      "Training loss for batch 7524 : 0.17549583315849304\n",
      "Training loss for batch 7525 : 0.37209367752075195\n",
      "Training loss for batch 7526 : 0.2877739667892456\n",
      "Training loss for batch 7527 : 0.36824533343315125\n",
      "Training loss for batch 7528 : 0.3709207773208618\n",
      "Training loss for batch 7529 : 0.4736207127571106\n",
      "Training loss for batch 7530 : 0.31687313318252563\n",
      "Training loss for batch 7531 : 0.03397161513566971\n",
      "Training loss for batch 7532 : 0.16141855716705322\n",
      "Training loss for batch 7533 : 0.1899484246969223\n",
      "Training loss for batch 7534 : 0.3854518234729767\n",
      "Training loss for batch 7535 : 0.4755125343799591\n",
      "Training loss for batch 7536 : 0.44401952624320984\n",
      "Training loss for batch 7537 : 0.33517903089523315\n",
      "Training loss for batch 7538 : 0.1267722249031067\n",
      "Training loss for batch 7539 : 0.2516553997993469\n",
      "Training loss for batch 7540 : 0.2531077265739441\n",
      "Training loss for batch 7541 : 0.6735708713531494\n",
      "Training loss for batch 7542 : 0.215642511844635\n",
      "Training loss for batch 7543 : 0.01209140196442604\n",
      "Training loss for batch 7544 : 0.056453607976436615\n",
      "Training loss for batch 7545 : 0.32183340191841125\n",
      "Training loss for batch 7546 : 0.5394154787063599\n",
      "Training loss for batch 7547 : 0.02650134637951851\n",
      "Training loss for batch 7548 : 0.22480285167694092\n",
      "Training loss for batch 7549 : 0.099609375\n",
      "Training loss for batch 7550 : 0.26331013441085815\n",
      "Training loss for batch 7551 : 0.10513701289892197\n",
      "Training loss for batch 7552 : 0.25467008352279663\n",
      "Training loss for batch 7553 : 0.04899505898356438\n",
      "Training loss for batch 7554 : 0.0\n",
      "Training loss for batch 7555 : 0.23106588423252106\n",
      "Training loss for batch 7556 : 0.0892631933093071\n",
      "Training loss for batch 7557 : 0.1702611893415451\n",
      "Training loss for batch 7558 : 0.06890838593244553\n",
      "Training loss for batch 7559 : 0.04796251282095909\n",
      "Training loss for batch 7560 : 0.20167353749275208\n",
      "Training loss for batch 7561 : 0.18253043293952942\n",
      "Training loss for batch 7562 : 0.26466691493988037\n",
      "Training loss for batch 7563 : 0.7025181651115417\n",
      "Training loss for batch 7564 : 0.15012826025485992\n",
      "Training loss for batch 7565 : 1.0683631896972656\n",
      "Training loss for batch 7566 : 0.42918169498443604\n",
      "Training loss for batch 7567 : 0.0535627044737339\n",
      "Training loss for batch 7568 : 0.1432773619890213\n",
      "Training loss for batch 7569 : 0.3788113594055176\n",
      "Training loss for batch 7570 : 0.08879504352807999\n",
      "Training loss for batch 7571 : 0.026491057127714157\n",
      "Training loss for batch 7572 : 0.07450685650110245\n",
      "Training loss for batch 7573 : 0.22184987366199493\n",
      "Training loss for batch 7574 : 0.04527213051915169\n",
      "Training loss for batch 7575 : 0.05887620896100998\n",
      "Training loss for batch 7576 : 0.5397160649299622\n",
      "Training loss for batch 7577 : 0.3572470247745514\n",
      "Training loss for batch 7578 : 0.8121921420097351\n",
      "Training loss for batch 7579 : 0.07226880639791489\n",
      "Training loss for batch 7580 : 0.14968647062778473\n",
      "Training loss for batch 7581 : 0.13749197125434875\n",
      "Training loss for batch 7582 : 0.2633439600467682\n",
      "Training loss for batch 7583 : 0.3872780501842499\n",
      "Training loss for batch 7584 : 0.31712523102760315\n",
      "Training loss for batch 7585 : 0.23425380885601044\n",
      "Training loss for batch 7586 : 0.41923442482948303\n",
      "Training loss for batch 7587 : 0.2832510471343994\n",
      "Training loss for batch 7588 : 0.24161089956760406\n",
      "Training loss for batch 7589 : 0.39066141843795776\n",
      "Training loss for batch 7590 : 0.23723751306533813\n",
      "Training loss for batch 7591 : 0.09400632977485657\n",
      "Training loss for batch 7592 : 0.31163695454597473\n",
      "Training loss for batch 7593 : 0.12054464221000671\n",
      "Training loss for batch 7594 : 0.033819399774074554\n",
      "Training loss for batch 7595 : 0.3609629273414612\n",
      "Training loss for batch 7596 : 0.15656566619873047\n",
      "Training loss for batch 7597 : 0.2222573161125183\n",
      "Training loss for batch 7598 : 0.44200003147125244\n",
      "Training loss for batch 7599 : 0.5133384466171265\n",
      "Training loss for batch 7600 : 0.2063412219285965\n",
      "Training loss for batch 7601 : 0.14471091330051422\n",
      "Training loss for batch 7602 : 0.3526327610015869\n",
      "Training loss for batch 7603 : 0.12351180613040924\n",
      "Training loss for batch 7604 : 0.026897188276052475\n",
      "Training loss for batch 7605 : 0.13305814564228058\n",
      "Training loss for batch 7606 : 0.3511100709438324\n",
      "Training loss for batch 7607 : 0.27774447202682495\n",
      "Training loss for batch 7608 : 0.028600428253412247\n",
      "Training loss for batch 7609 : 0.0651494711637497\n",
      "Training loss for batch 7610 : 0.24479633569717407\n",
      "Training loss for batch 7611 : 0.11254456639289856\n",
      "Training loss for batch 7612 : 0.5333791971206665\n",
      "Training loss for batch 7613 : 0.1810351461172104\n",
      "Training loss for batch 7614 : 0.045197974890470505\n",
      "Training loss for batch 7615 : 0.18659727275371552\n",
      "Training loss for batch 7616 : 0.5373020768165588\n",
      "Training loss for batch 7617 : 0.08287780731916428\n",
      "Training loss for batch 7618 : 0.3894583284854889\n",
      "Training loss for batch 7619 : 0.42455366253852844\n",
      "Training loss for batch 7620 : 0.4798637628555298\n",
      "Training loss for batch 7621 : 0.5288102626800537\n",
      "Training loss for batch 7622 : 0.29676955938339233\n",
      "Training loss for batch 7623 : 0.16215071082115173\n",
      "Training loss for batch 7624 : 0.2918599843978882\n",
      "Training loss for batch 7625 : 0.10955426096916199\n",
      "Training loss for batch 7626 : 0.009451746940612793\n",
      "Training loss for batch 7627 : 0.6599846482276917\n",
      "Training loss for batch 7628 : 0.07989440858364105\n",
      "Training loss for batch 7629 : 0.46409469842910767\n",
      "Training loss for batch 7630 : 0.21006886661052704\n",
      "Training loss for batch 7631 : 0.11589809507131577\n",
      "Training loss for batch 7632 : 0.23702372610569\n",
      "Training loss for batch 7633 : 0.06145182251930237\n",
      "Training loss for batch 7634 : 0.3205995559692383\n",
      "Training loss for batch 7635 : 0.29020261764526367\n",
      "Training loss for batch 7636 : 0.2852421700954437\n",
      "Training loss for batch 7637 : 0.3429797291755676\n",
      "Training loss for batch 7638 : 0.3539624810218811\n",
      "Training loss for batch 7639 : 0.20158007740974426\n",
      "Training loss for batch 7640 : 0.34903547167778015\n",
      "Training loss for batch 7641 : 0.2807791829109192\n",
      "Training loss for batch 7642 : 0.35424211621284485\n",
      "Training loss for batch 7643 : 0.05678772181272507\n",
      "Training loss for batch 7644 : 0.04302257299423218\n",
      "Training loss for batch 7645 : 0.2293352335691452\n",
      "Training loss for batch 7646 : 0.1387988030910492\n",
      "Training loss for batch 7647 : 0.1143849566578865\n",
      "Training loss for batch 7648 : 0.06363588571548462\n",
      "Training loss for batch 7649 : 0.3430211842060089\n",
      "Training loss for batch 7650 : 0.16655613481998444\n",
      "Training loss for batch 7651 : 0.16426865756511688\n",
      "Training loss for batch 7652 : 0.2685657739639282\n",
      "Training loss for batch 7653 : 0.08789371699094772\n",
      "Training loss for batch 7654 : 0.10816950350999832\n",
      "Training loss for batch 7655 : 0.22299975156784058\n",
      "Training loss for batch 7656 : 0.2654075026512146\n",
      "Training loss for batch 7657 : 0.48116335272789\n",
      "Training loss for batch 7658 : 0.3328077495098114\n",
      "Training loss for batch 7659 : 0.058017365634441376\n",
      "Training loss for batch 7660 : 0.02741720713675022\n",
      "Training loss for batch 7661 : 0.16233940422534943\n",
      "Training loss for batch 7662 : 0.041494220495224\n",
      "Training loss for batch 7663 : 0.3989843726158142\n",
      "Training loss for batch 7664 : 0.08824716508388519\n",
      "Training loss for batch 7665 : 0.3527948260307312\n",
      "Training loss for batch 7666 : 0.18491968512535095\n",
      "Training loss for batch 7667 : 0.04065351188182831\n",
      "Training loss for batch 7668 : 0.1416979432106018\n",
      "Training loss for batch 7669 : 0.4722577631473541\n",
      "Training loss for batch 7670 : 0.3763227164745331\n",
      "Training loss for batch 7671 : 0.23404629528522491\n",
      "Training loss for batch 7672 : 0.09723760187625885\n",
      "Training loss for batch 7673 : 0.04245268553495407\n",
      "Training loss for batch 7674 : 0.3506453037261963\n",
      "Training loss for batch 7675 : 0.06463868916034698\n",
      "Training loss for batch 7676 : 0.8287920355796814\n",
      "Training loss for batch 7677 : 0.40168488025665283\n",
      "Training loss for batch 7678 : 0.07817590981721878\n",
      "Training loss for batch 7679 : 0.023208629339933395\n",
      "Training loss for batch 7680 : 0.10155918449163437\n",
      "Training loss for batch 7681 : 0.2844444215297699\n",
      "Training loss for batch 7682 : 0.06761866807937622\n",
      "Training loss for batch 7683 : 0.23404282331466675\n",
      "Training loss for batch 7684 : 0.26054391264915466\n",
      "Training loss for batch 7685 : 0.15523651242256165\n",
      "Training loss for batch 7686 : 0.13440725207328796\n",
      "Training loss for batch 7687 : 0.042360126972198486\n",
      "Training loss for batch 7688 : 0.127082958817482\n",
      "Training loss for batch 7689 : 0.3794926106929779\n",
      "Training loss for batch 7690 : 0.3738764524459839\n",
      "Training loss for batch 7691 : 0.1543102115392685\n",
      "Training loss for batch 7692 : 0.12566061317920685\n",
      "Training loss for batch 7693 : 0.4130628705024719\n",
      "Training loss for batch 7694 : 0.12928231060504913\n",
      "Training loss for batch 7695 : 0.19257348775863647\n",
      "Training loss for batch 7696 : 0.3582676649093628\n",
      "Training loss for batch 7697 : 0.13779479265213013\n",
      "Training loss for batch 7698 : 0.3939327895641327\n",
      "Training loss for batch 7699 : 0.2696100175380707\n",
      "Training loss for batch 7700 : 0.5356255769729614\n",
      "Training loss for batch 7701 : 0.40019509196281433\n",
      "Training loss for batch 7702 : 0.19961343705654144\n",
      "Training loss for batch 7703 : 0.1218731552362442\n",
      "Training loss for batch 7704 : 0.06408480554819107\n",
      "Training loss for batch 7705 : 0.10230715572834015\n",
      "Training loss for batch 7706 : 0.3194941282272339\n",
      "Training loss for batch 7707 : 0.3935689926147461\n",
      "Training loss for batch 7708 : 0.003639118280261755\n",
      "Training loss for batch 7709 : 0.7890852093696594\n",
      "Training loss for batch 7710 : 0.2547988295555115\n",
      "Training loss for batch 7711 : 0.04684387519955635\n",
      "Training loss for batch 7712 : 0.3233337998390198\n",
      "Training loss for batch 7713 : 0.7454589605331421\n",
      "Training loss for batch 7714 : 0.00015732883184682578\n",
      "Training loss for batch 7715 : 0.2238544076681137\n",
      "Training loss for batch 7716 : 0.17859290540218353\n",
      "Training loss for batch 7717 : 0.12042567133903503\n",
      "Training loss for batch 7718 : 0.15213742852210999\n",
      "Training loss for batch 7719 : 0.4612795412540436\n",
      "Training loss for batch 7720 : 0.49613311886787415\n",
      "Training loss for batch 7721 : 0.07182802259922028\n",
      "Training loss for batch 7722 : 0.3374539315700531\n",
      "Training loss for batch 7723 : 0.13873004913330078\n",
      "Training loss for batch 7724 : 0.5025636553764343\n",
      "Training loss for batch 7725 : 0.1462889313697815\n",
      "Training loss for batch 7726 : 0.04994824528694153\n",
      "Training loss for batch 7727 : 0.28995946049690247\n",
      "Training loss for batch 7728 : 0.20322269201278687\n",
      "Training loss for batch 7729 : 0.37249842286109924\n",
      "Training loss for batch 7730 : 0.08478213846683502\n",
      "Training loss for batch 7731 : 0.27458345890045166\n",
      "Training loss for batch 7732 : 0.23522791266441345\n",
      "Training loss for batch 7733 : 0.38100534677505493\n",
      "Training loss for batch 7734 : 0.4355076551437378\n",
      "Training loss for batch 7735 : 0.0015980502357706428\n",
      "Training loss for batch 7736 : 0.10956661403179169\n",
      "Training loss for batch 7737 : 0.47582733631134033\n",
      "Training loss for batch 7738 : 0.36633700132369995\n",
      "Training loss for batch 7739 : 0.530202329158783\n",
      "Training loss for batch 7740 : 0.19388796389102936\n",
      "Training loss for batch 7741 : 0.08546479046344757\n",
      "Training loss for batch 7742 : 0.2779231667518616\n",
      "Training loss for batch 7743 : 0.3576229214668274\n",
      "Training loss for batch 7744 : 0.03400995582342148\n",
      "Training loss for batch 7745 : 0.04679581895470619\n",
      "Training loss for batch 7746 : 0.12150601297616959\n",
      "Training loss for batch 7747 : 0.10316837579011917\n",
      "Training loss for batch 7748 : 0.24746853113174438\n",
      "Training loss for batch 7749 : 0.2778127193450928\n",
      "Training loss for batch 7750 : 0.44636252522468567\n",
      "Training loss for batch 7751 : 0.08307069540023804\n",
      "Training loss for batch 7752 : 0.2848926782608032\n",
      "Training loss for batch 7753 : 0.35973238945007324\n",
      "Training loss for batch 7754 : 0.3398474156856537\n",
      "Training loss for batch 7755 : 0.3299512267112732\n",
      "Training loss for batch 7756 : 0.25433349609375\n",
      "Training loss for batch 7757 : 0.08438010513782501\n",
      "Training loss for batch 7758 : 0.30301082134246826\n",
      "Training loss for batch 7759 : 0.2990146279335022\n",
      "Training loss for batch 7760 : 0.24288347363471985\n",
      "Training loss for batch 7761 : 0.3447211980819702\n",
      "Training loss for batch 7762 : 0.6588351130485535\n",
      "Training loss for batch 7763 : 0.28568217158317566\n",
      "Training loss for batch 7764 : 0.3987899124622345\n",
      "Training loss for batch 7765 : 0.37824857234954834\n",
      "Training loss for batch 7766 : 0.12443838268518448\n",
      "Training loss for batch 7767 : 0.07272307574748993\n",
      "Training loss for batch 7768 : 0.030624235048890114\n",
      "Training loss for batch 7769 : 0.23180927336215973\n",
      "Training loss for batch 7770 : 0.11455654352903366\n",
      "Training loss for batch 7771 : 0.26558589935302734\n",
      "Training loss for batch 7772 : 0.2018267661333084\n",
      "Training loss for batch 7773 : 0.20351247489452362\n",
      "Training loss for batch 7774 : 0.05915060639381409\n",
      "Training loss for batch 7775 : 0.32815560698509216\n",
      "Training loss for batch 7776 : 0.08469527214765549\n",
      "Training loss for batch 7777 : 0.2574045658111572\n",
      "Training loss for batch 7778 : 0.13882960379123688\n",
      "Training loss for batch 7779 : 0.22782552242279053\n",
      "Training loss for batch 7780 : 0.041867345571517944\n",
      "Training loss for batch 7781 : 0.18941617012023926\n",
      "Training loss for batch 7782 : 0.044498223811388016\n",
      "Training loss for batch 7783 : 0.0638793483376503\n",
      "Training loss for batch 7784 : 0.0769418403506279\n",
      "Training loss for batch 7785 : 0.0736953541636467\n",
      "Training loss for batch 7786 : 0.29234471917152405\n",
      "Training loss for batch 7787 : 0.0\n",
      "Training loss for batch 7788 : 0.1601644903421402\n",
      "Training loss for batch 7789 : 0.14095133543014526\n",
      "Training loss for batch 7790 : 0.24174484610557556\n",
      "Training loss for batch 7791 : 0.16085979342460632\n",
      "Training loss for batch 7792 : 0.060107164084911346\n",
      "Training loss for batch 7793 : 0.5206927061080933\n",
      "Training loss for batch 7794 : 0.09621875733137131\n",
      "Training loss for batch 7795 : 0.21796484291553497\n",
      "Training loss for batch 7796 : 0.09324939548969269\n",
      "Training loss for batch 7797 : 0.09534773975610733\n",
      "Training loss for batch 7798 : 0.0037853061221539974\n",
      "Training loss for batch 7799 : 0.37720781564712524\n",
      "Training loss for batch 7800 : 0.5847359299659729\n",
      "Training loss for batch 7801 : 0.2584204375743866\n",
      "Training loss for batch 7802 : 0.5570135712623596\n",
      "Training loss for batch 7803 : 0.1068480834364891\n",
      "Training loss for batch 7804 : 0.6538862586021423\n",
      "Training loss for batch 7805 : 0.018205365166068077\n",
      "Training loss for batch 7806 : 0.23962220549583435\n",
      "Training loss for batch 7807 : 0.28447428345680237\n",
      "Training loss for batch 7808 : 0.06073926016688347\n",
      "Training loss for batch 7809 : 0.2341654747724533\n",
      "Training loss for batch 7810 : 0.3183337450027466\n",
      "Training loss for batch 7811 : 0.34453997015953064\n",
      "Training loss for batch 7812 : 0.56257563829422\n",
      "Training loss for batch 7813 : 0.2881350517272949\n",
      "Training loss for batch 7814 : 0.008075466379523277\n",
      "Training loss for batch 7815 : 0.1355321854352951\n",
      "Training loss for batch 7816 : 0.05898898094892502\n",
      "Training loss for batch 7817 : 0.0026084184646606445\n",
      "Training loss for batch 7818 : 0.00502325315028429\n",
      "Training loss for batch 7819 : 0.0\n",
      "Training loss for batch 7820 : 0.08606847375631332\n",
      "Training loss for batch 7821 : 0.0006211499567143619\n",
      "Training loss for batch 7822 : 0.5987410545349121\n",
      "Training loss for batch 7823 : 0.18554925918579102\n",
      "Training loss for batch 7824 : 0.467570960521698\n",
      "Training loss for batch 7825 : 0.0708329826593399\n",
      "Training loss for batch 7826 : 1.095025658607483\n",
      "Training loss for batch 7827 : 0.09711171686649323\n",
      "Training loss for batch 7828 : 0.2675178050994873\n",
      "Training loss for batch 7829 : 0.3738355338573456\n",
      "Training loss for batch 7830 : 0.09254831075668335\n",
      "Training loss for batch 7831 : 0.012600843794643879\n",
      "Training loss for batch 7832 : 0.06835611164569855\n",
      "Training loss for batch 7833 : 0.5382253527641296\n",
      "Training loss for batch 7834 : 0.24108213186264038\n",
      "Training loss for batch 7835 : 0.0023338794708251953\n",
      "Training loss for batch 7836 : 0.421488881111145\n",
      "Training loss for batch 7837 : 0.8217315673828125\n",
      "Training loss for batch 7838 : 0.47611576318740845\n",
      "Training loss for batch 7839 : 0.14917641878128052\n",
      "Training loss for batch 7840 : 0.10159556567668915\n",
      "Training loss for batch 7841 : 0.45489993691444397\n",
      "Training loss for batch 7842 : 0.20944437384605408\n",
      "Training loss for batch 7843 : 0.1615142524242401\n",
      "Training loss for batch 7844 : 0.6196305155754089\n",
      "Training loss for batch 7845 : 0.3152325451374054\n",
      "Training loss for batch 7846 : 0.467612087726593\n",
      "Training loss for batch 7847 : 0.24496030807495117\n",
      "Training loss for batch 7848 : 0.0230129212141037\n",
      "Training loss for batch 7849 : 0.45531967282295227\n",
      "Training loss for batch 7850 : 0.42638397216796875\n",
      "Training loss for batch 7851 : 0.30515098571777344\n",
      "Training loss for batch 7852 : 0.358040988445282\n",
      "Training loss for batch 7853 : 0.3524313271045685\n",
      "Training loss for batch 7854 : 0.41410017013549805\n",
      "Training loss for batch 7855 : 0.24838383495807648\n",
      "Training loss for batch 7856 : 0.2062312215566635\n",
      "Training loss for batch 7857 : 0.44862765073776245\n",
      "Training loss for batch 7858 : 0.17919927835464478\n",
      "Training loss for batch 7859 : 0.17476089298725128\n",
      "Training loss for batch 7860 : 0.7667421698570251\n",
      "Training loss for batch 7861 : 0.16488507390022278\n",
      "Training loss for batch 7862 : 0.30528518557548523\n",
      "Training loss for batch 7863 : 0.5618884563446045\n",
      "Training loss for batch 7864 : 0.42445239424705505\n",
      "Training loss for batch 7865 : 0.3247658312320709\n",
      "Training loss for batch 7866 : 0.18004558980464935\n",
      "Training loss for batch 7867 : 0.0955008864402771\n",
      "Training loss for batch 7868 : 0.07425352185964584\n",
      "Training loss for batch 7869 : 0.19317680597305298\n",
      "Training loss for batch 7870 : 0.24082621932029724\n",
      "Training loss for batch 7871 : 0.08152744174003601\n",
      "Training loss for batch 7872 : 0.24578271806240082\n",
      "Training loss for batch 7873 : 0.0890122503042221\n",
      "Training loss for batch 7874 : 0.2144063264131546\n",
      "Training loss for batch 7875 : 0.1327153593301773\n",
      "Training loss for batch 7876 : 0.002399156568571925\n",
      "Training loss for batch 7877 : 0.3107435703277588\n",
      "Training loss for batch 7878 : 0.2980567514896393\n",
      "Training loss for batch 7879 : 0.250095009803772\n",
      "Training loss for batch 7880 : 0.8239346742630005\n",
      "Training loss for batch 7881 : 0.6282068490982056\n",
      "Training loss for batch 7882 : 0.6054307818412781\n",
      "Training loss for batch 7883 : 0.07387108355760574\n",
      "Training loss for batch 7884 : 0.13985300064086914\n",
      "Training loss for batch 7885 : 0.08753391355276108\n",
      "Training loss for batch 7886 : 0.5269503593444824\n",
      "Training loss for batch 7887 : 0.021977568045258522\n",
      "Training loss for batch 7888 : 0.1303376406431198\n",
      "Training loss for batch 7889 : 0.047885265201330185\n",
      "Training loss for batch 7890 : 0.29522815346717834\n",
      "Training loss for batch 7891 : 0.42197903990745544\n",
      "Training loss for batch 7892 : 0.15243488550186157\n",
      "Training loss for batch 7893 : 0.03216294199228287\n",
      "Training loss for batch 7894 : 0.4119572043418884\n",
      "Training loss for batch 7895 : 0.08401910215616226\n",
      "Training loss for batch 7896 : 0.6178964376449585\n",
      "Training loss for batch 7897 : 0.30874526500701904\n",
      "Training loss for batch 7898 : 0.7853164076805115\n",
      "Training loss for batch 7899 : 0.0877588540315628\n",
      "Training loss for batch 7900 : 0.35483214259147644\n",
      "Training loss for batch 7901 : 0.4327205717563629\n",
      "Training loss for batch 7902 : 0.3270564079284668\n",
      "Training loss for batch 7903 : 0.11869923770427704\n",
      "Training loss for batch 7904 : 0.3197210729122162\n",
      "Training loss for batch 7905 : 0.19096989929676056\n",
      "Training loss for batch 7906 : 0.3335994780063629\n",
      "Training loss for batch 7907 : 0.12878157198429108\n",
      "Training loss for batch 7908 : 0.017548615112900734\n",
      "Training loss for batch 7909 : 0.3080574572086334\n",
      "Training loss for batch 7910 : 0.4813006818294525\n",
      "Training loss for batch 7911 : 0.05297219380736351\n",
      "Training loss for batch 7912 : 0.23787841200828552\n",
      "Training loss for batch 7913 : 0.17019610106945038\n",
      "Training loss for batch 7914 : 0.00029070687014609575\n",
      "Training loss for batch 7915 : 0.34144094586372375\n",
      "Training loss for batch 7916 : 0.1588648408651352\n",
      "Training loss for batch 7917 : 0.5498201251029968\n",
      "Training loss for batch 7918 : 0.08715725690126419\n",
      "Training loss for batch 7919 : 0.5648512244224548\n",
      "Training loss for batch 7920 : 0.1709911823272705\n",
      "Training loss for batch 7921 : 0.12662097811698914\n",
      "Training loss for batch 7922 : 0.20581857860088348\n",
      "Training loss for batch 7923 : 0.14492617547512054\n",
      "Training loss for batch 7924 : 0.24632343649864197\n",
      "Training loss for batch 7925 : 0.3770800232887268\n",
      "Training loss for batch 7926 : 0.1699678897857666\n",
      "Training loss for batch 7927 : 0.23097418248653412\n",
      "Training loss for batch 7928 : 0.33506080508232117\n",
      "Training loss for batch 7929 : 0.4571239650249481\n",
      "Training loss for batch 7930 : 0.15377925336360931\n",
      "Training loss for batch 7931 : 0.6128317713737488\n",
      "Training loss for batch 7932 : 0.27855730056762695\n",
      "Training loss for batch 7933 : 0.4044373631477356\n",
      "Training loss for batch 7934 : 0.22842343151569366\n",
      "Training loss for batch 7935 : 0.007515063043683767\n",
      "Training loss for batch 7936 : 0.25999873876571655\n",
      "Training loss for batch 7937 : 0.042953263968229294\n",
      "Training loss for batch 7938 : 0.01353425718843937\n",
      "Training loss for batch 7939 : 0.736941933631897\n",
      "Training loss for batch 7940 : 0.27179741859436035\n",
      "Training loss for batch 7941 : 0.4767252802848816\n",
      "Training loss for batch 7942 : 0.2222418487071991\n",
      "Training loss for batch 7943 : 0.11770422756671906\n",
      "Training loss for batch 7944 : 0.22354017198085785\n",
      "Training loss for batch 7945 : 0.49151095747947693\n",
      "Training loss for batch 7946 : 0.4682793617248535\n",
      "Training loss for batch 7947 : 0.0180926825851202\n",
      "Training loss for batch 7948 : 0.03568212687969208\n",
      "Training loss for batch 7949 : 0.21363994479179382\n",
      "Training loss for batch 7950 : 0.2803867757320404\n",
      "Training loss for batch 7951 : 0.27552536129951477\n",
      "Training loss for batch 7952 : 0.04734327271580696\n",
      "Training loss for batch 7953 : 0.27931925654411316\n",
      "Training loss for batch 7954 : 0.33894264698028564\n",
      "Training loss for batch 7955 : 0.5447415113449097\n",
      "Training loss for batch 7956 : 0.29259830713272095\n",
      "Training loss for batch 7957 : 0.22755558788776398\n",
      "Training loss for batch 7958 : 0.41929319500923157\n",
      "Training loss for batch 7959 : 0.3024287819862366\n",
      "Training loss for batch 7960 : 0.030131181702017784\n",
      "Training loss for batch 7961 : 0.2787606418132782\n",
      "Training loss for batch 7962 : 0.5171583890914917\n",
      "Training loss for batch 7963 : 0.8761595487594604\n",
      "Training loss for batch 7964 : 0.4013432264328003\n",
      "Training loss for batch 7965 : 0.2830020785331726\n",
      "Training loss for batch 7966 : 0.2546248435974121\n",
      "Training loss for batch 7967 : 0.3847010135650635\n",
      "Training loss for batch 7968 : 0.5645802617073059\n",
      "Training loss for batch 7969 : 0.014504415914416313\n",
      "Training loss for batch 7970 : 0.1498517245054245\n",
      "Training loss for batch 7971 : 0.292959988117218\n",
      "Training loss for batch 7972 : 0.2648938298225403\n",
      "Training loss for batch 7973 : 0.020210573449730873\n",
      "Training loss for batch 7974 : 0.2097610980272293\n",
      "Training loss for batch 7975 : 0.2608415484428406\n",
      "Training loss for batch 7976 : 0.282569020986557\n",
      "Training loss for batch 7977 : 0.039079051464796066\n",
      "Training loss for batch 7978 : 0.13973797857761383\n",
      "Training loss for batch 7979 : 0.4889802038669586\n",
      "Training loss for batch 7980 : 0.11588242650032043\n",
      "Training loss for batch 7981 : 0.0675564855337143\n",
      "Training loss for batch 7982 : 0.2316288948059082\n",
      "Training loss for batch 7983 : 0.13231369853019714\n",
      "Training loss for batch 7984 : 0.40926358103752136\n",
      "Training loss for batch 7985 : 0.16852301359176636\n",
      "Training loss for batch 7986 : 0.19849787652492523\n",
      "Training loss for batch 7987 : 0.505308985710144\n",
      "Training loss for batch 7988 : 0.472759872674942\n",
      "Training loss for batch 7989 : 0.2132350206375122\n",
      "Training loss for batch 7990 : 0.3165924549102783\n",
      "Training loss for batch 7991 : 0.49925994873046875\n",
      "Training loss for batch 7992 : 0.3369152545928955\n",
      "Training loss for batch 7993 : 0.07566528022289276\n",
      "Training loss for batch 7994 : 0.23022626340389252\n",
      "Training loss for batch 7995 : 0.118662990629673\n",
      "Training loss for batch 7996 : 0.12042886763811111\n",
      "Training loss for batch 7997 : 0.08450311422348022\n",
      "Training loss for batch 7998 : 0.04416154697537422\n",
      "Training loss for batch 7999 : 0.32605957984924316\n",
      "Training loss for batch 8000 : 0.4038669764995575\n",
      "Training loss for batch 8001 : 0.2035755217075348\n",
      "Training loss for batch 8002 : 0.00185642228461802\n",
      "Training loss for batch 8003 : 0.25953277945518494\n",
      "Training loss for batch 8004 : 0.3790522813796997\n",
      "Training loss for batch 8005 : 0.19692464172840118\n",
      "Training loss for batch 8006 : 0.047748863697052\n",
      "Training loss for batch 8007 : 0.09714086353778839\n",
      "Training loss for batch 8008 : 0.1368699073791504\n",
      "Training loss for batch 8009 : 0.4207459092140198\n",
      "Training loss for batch 8010 : 0.509925901889801\n",
      "Training loss for batch 8011 : 0.41797080636024475\n",
      "Training loss for batch 8012 : 0.281313419342041\n",
      "Training loss for batch 8013 : 0.08517341315746307\n",
      "Training loss for batch 8014 : 0.292280912399292\n",
      "Training loss for batch 8015 : 0.09678616374731064\n",
      "Training loss for batch 8016 : 0.25617486238479614\n",
      "Training loss for batch 8017 : 0.1524452567100525\n",
      "Training loss for batch 8018 : 0.05045001581311226\n",
      "Training loss for batch 8019 : 0.5643295049667358\n",
      "Training loss for batch 8020 : 0.4676385819911957\n",
      "Training loss for batch 8021 : 0.20569035410881042\n",
      "Training loss for batch 8022 : 0.3916504383087158\n",
      "Training loss for batch 8023 : 0.30732181668281555\n",
      "Training loss for batch 8024 : 0.07242496311664581\n",
      "Training loss for batch 8025 : 0.16885200142860413\n",
      "Training loss for batch 8026 : 0.640170156955719\n",
      "Training loss for batch 8027 : 0.5770721435546875\n",
      "Training loss for batch 8028 : 0.26253175735473633\n",
      "Training loss for batch 8029 : 0.05178498476743698\n",
      "Training loss for batch 8030 : 0.040253862738609314\n",
      "Training loss for batch 8031 : 0.28865763545036316\n",
      "Training loss for batch 8032 : 0.11610516905784607\n",
      "Training loss for batch 8033 : 0.10179465264081955\n",
      "Training loss for batch 8034 : 0.24078752100467682\n",
      "Training loss for batch 8035 : 0.053052809089422226\n",
      "Training loss for batch 8036 : 0.32940539717674255\n",
      "Training loss for batch 8037 : 0.12832581996917725\n",
      "Training loss for batch 8038 : 0.4546871781349182\n",
      "Training loss for batch 8039 : 0.1311834454536438\n",
      "Training loss for batch 8040 : 0.020782193168997765\n",
      "Training loss for batch 8041 : 0.39646682143211365\n",
      "Training loss for batch 8042 : 0.4402078092098236\n",
      "Training loss for batch 8043 : 0.1109936460852623\n",
      "Training loss for batch 8044 : 5.080208666186081e-06\n",
      "Training loss for batch 8045 : 0.2490655779838562\n",
      "Training loss for batch 8046 : 0.4714890420436859\n",
      "Training loss for batch 8047 : 0.06182452291250229\n",
      "Training loss for batch 8048 : 0.05398505926132202\n",
      "Training loss for batch 8049 : 0.31705808639526367\n",
      "Training loss for batch 8050 : 0.4621613025665283\n",
      "Training loss for batch 8051 : 0.24434734880924225\n",
      "Training loss for batch 8052 : 0.1673572063446045\n",
      "Training loss for batch 8053 : 0.07026715576648712\n",
      "Training loss for batch 8054 : 0.006743977777659893\n",
      "Training loss for batch 8055 : 0.30115094780921936\n",
      "Training loss for batch 8056 : 0.470265656709671\n",
      "Training loss for batch 8057 : 0.1501847356557846\n",
      "Training loss for batch 8058 : 0.28090253472328186\n",
      "Training loss for batch 8059 : 0.24688850343227386\n",
      "Training loss for batch 8060 : 0.05341325327754021\n",
      "Training loss for batch 8061 : 0.4165298044681549\n",
      "Training loss for batch 8062 : 0.10694791376590729\n",
      "Training loss for batch 8063 : 0.2935975193977356\n",
      "Training loss for batch 8064 : 0.48100733757019043\n",
      "Training loss for batch 8065 : 0.17912189662456512\n",
      "Training loss for batch 8066 : 0.3757965564727783\n",
      "Training loss for batch 8067 : 0.30266982316970825\n",
      "Training loss for batch 8068 : 0.2592361569404602\n",
      "Training loss for batch 8069 : 0.3263082802295685\n",
      "Training loss for batch 8070 : 0.2614896595478058\n",
      "Training loss for batch 8071 : 0.4957990348339081\n",
      "Training loss for batch 8072 : 0.3121616244316101\n",
      "Training loss for batch 8073 : 0.2948775291442871\n",
      "Training loss for batch 8074 : 0.11477857083082199\n",
      "Training loss for batch 8075 : 0.08421937376260757\n",
      "Training loss for batch 8076 : 0.25589412450790405\n",
      "Training loss for batch 8077 : 0.4335244596004486\n",
      "Training loss for batch 8078 : 0.10632851719856262\n",
      "Training loss for batch 8079 : 0.2427523285150528\n",
      "Training loss for batch 8080 : 0.0673179104924202\n",
      "Training loss for batch 8081 : 0.4536968469619751\n",
      "Training loss for batch 8082 : 0.3945886790752411\n",
      "Training loss for batch 8083 : 0.4175240993499756\n",
      "Training loss for batch 8084 : 0.4237392842769623\n",
      "Training loss for batch 8085 : 0.2901011109352112\n",
      "Training loss for batch 8086 : 0.32239922881126404\n",
      "Training loss for batch 8087 : 0.10889363288879395\n",
      "Training loss for batch 8088 : 0.3469102382659912\n",
      "Training loss for batch 8089 : 0.23902475833892822\n",
      "Training loss for batch 8090 : 0.09005638211965561\n",
      "Training loss for batch 8091 : 0.28158318996429443\n",
      "Training loss for batch 8092 : 0.39911985397338867\n",
      "Training loss for batch 8093 : 0.1721491813659668\n",
      "Training loss for batch 8094 : 0.6559261679649353\n",
      "Training loss for batch 8095 : 0.280967652797699\n",
      "Training loss for batch 8096 : 0.26835519075393677\n",
      "Training loss for batch 8097 : 0.05902310088276863\n",
      "Training loss for batch 8098 : 0.11773763597011566\n",
      "Training loss for batch 8099 : 0.4223453104496002\n",
      "Training loss for batch 8100 : 0.4819398820400238\n",
      "Training loss for batch 8101 : 0.25724032521247864\n",
      "Training loss for batch 8102 : 0.78278648853302\n",
      "Training loss for batch 8103 : 0.17389163374900818\n",
      "Training loss for batch 8104 : 0.4257887899875641\n",
      "Training loss for batch 8105 : 0.555248498916626\n",
      "Training loss for batch 8106 : 0.30097299814224243\n",
      "Training loss for batch 8107 : 0.18355324864387512\n",
      "Training loss for batch 8108 : 0.15555739402770996\n",
      "Training loss for batch 8109 : 0.2905622720718384\n",
      "Training loss for batch 8110 : 0.5405347347259521\n",
      "Training loss for batch 8111 : 0.27561601996421814\n",
      "Training loss for batch 8112 : 0.321711927652359\n",
      "Training loss for batch 8113 : 0.36448922753334045\n",
      "Training loss for batch 8114 : 0.3975357115268707\n",
      "Training loss for batch 8115 : 0.13674360513687134\n",
      "Training loss for batch 8116 : 0.35300111770629883\n",
      "Training loss for batch 8117 : 0.09802132844924927\n",
      "Training loss for batch 8118 : 0.28954437375068665\n",
      "Training loss for batch 8119 : 0.32516342401504517\n",
      "Training loss for batch 8120 : 0.4445309340953827\n",
      "Training loss for batch 8121 : 0.21752816438674927\n",
      "Training loss for batch 8122 : 0.37116724252700806\n",
      "Training loss for batch 8123 : 0.08419227600097656\n",
      "Training loss for batch 8124 : 0.41250768303871155\n",
      "Training loss for batch 8125 : 0.7289909720420837\n",
      "Training loss for batch 8126 : 0.2749076783657074\n",
      "Training loss for batch 8127 : 0.30790936946868896\n",
      "Training loss for batch 8128 : 0.2930969297885895\n",
      "Training loss for batch 8129 : 0.5366887450218201\n",
      "Training loss for batch 8130 : 0.5602852702140808\n",
      "Training loss for batch 8131 : 0.11177990585565567\n",
      "Training loss for batch 8132 : 0.1832897961139679\n",
      "Training loss for batch 8133 : 0.34205836057662964\n",
      "Training loss for batch 8134 : 0.10719747096300125\n",
      "Training loss for batch 8135 : 0.001846578437834978\n",
      "Training loss for batch 8136 : 0.15390248596668243\n",
      "Training loss for batch 8137 : 0.08775082230567932\n",
      "Training loss for batch 8138 : 0.4700673520565033\n",
      "Training loss for batch 8139 : 0.18471691012382507\n",
      "Training loss for batch 8140 : 0.23381349444389343\n",
      "Training loss for batch 8141 : 0.28795042634010315\n",
      "Training loss for batch 8142 : 0.2286352813243866\n",
      "Training loss for batch 8143 : 0.4506113529205322\n",
      "Training loss for batch 8144 : 0.3243069350719452\n",
      "Training loss for batch 8145 : 0.14616596698760986\n",
      "Training loss for batch 8146 : 0.06042379140853882\n",
      "Training loss for batch 8147 : 0.26377004384994507\n",
      "Training loss for batch 8148 : 0.04646681249141693\n",
      "Training loss for batch 8149 : 0.2831655740737915\n",
      "Training loss for batch 8150 : 0.1388106793165207\n",
      "Training loss for batch 8151 : 0.19637560844421387\n",
      "Training loss for batch 8152 : 0.36615312099456787\n",
      "Training loss for batch 8153 : 0.25798481702804565\n",
      "Training loss for batch 8154 : 0.14089223742485046\n",
      "Training loss for batch 8155 : 0.18351687490940094\n",
      "Training loss for batch 8156 : 0.1068553477525711\n",
      "Training loss for batch 8157 : 0.1999044418334961\n",
      "Training loss for batch 8158 : 0.09895576536655426\n",
      "Training loss for batch 8159 : 0.2949983477592468\n",
      "Training loss for batch 8160 : 0.11413736641407013\n",
      "Training loss for batch 8161 : 0.3694905936717987\n",
      "Training loss for batch 8162 : 0.171012282371521\n",
      "Training loss for batch 8163 : 0.1055850237607956\n",
      "Training loss for batch 8164 : 0.24053767323493958\n",
      "Training loss for batch 8165 : 0.19605916738510132\n",
      "Training loss for batch 8166 : 0.2589680552482605\n",
      "Training loss for batch 8167 : 0.08369455486536026\n",
      "Training loss for batch 8168 : 0.20342381298542023\n",
      "Training loss for batch 8169 : 0.09116677939891815\n",
      "Training loss for batch 8170 : 0.5737892389297485\n",
      "Training loss for batch 8171 : 0.11705533415079117\n",
      "Training loss for batch 8172 : 0.3896142542362213\n",
      "Training loss for batch 8173 : 0.26028701663017273\n",
      "Training loss for batch 8174 : 0.29764583706855774\n",
      "Training loss for batch 8175 : 0.6178691983222961\n",
      "Training loss for batch 8176 : 0.31316906213760376\n",
      "Training loss for batch 8177 : 0.05709259212017059\n",
      "Training loss for batch 8178 : 0.24557077884674072\n",
      "Training loss for batch 8179 : 0.03794450685381889\n",
      "Training loss for batch 8180 : 0.34041494131088257\n",
      "Training loss for batch 8181 : 0.007615725509822369\n",
      "Training loss for batch 8182 : 0.12725533545017242\n",
      "Training loss for batch 8183 : 0.5219549536705017\n",
      "Training loss for batch 8184 : 0.09761432558298111\n",
      "Training loss for batch 8185 : 0.4686533808708191\n",
      "Training loss for batch 8186 : 0.3937423825263977\n",
      "Training loss for batch 8187 : 0.08309097588062286\n",
      "Training loss for batch 8188 : 0.4252873361110687\n",
      "Training loss for batch 8189 : 0.18613921105861664\n",
      "Training loss for batch 8190 : 0.1923803687095642\n",
      "Training loss for batch 8191 : 0.11660167574882507\n",
      "Training loss for batch 8192 : 0.07346249371767044\n",
      "Training loss for batch 8193 : 0.17095831036567688\n",
      "Training loss for batch 8194 : 0.23058758676052094\n",
      "Training loss for batch 8195 : 0.04140549153089523\n",
      "Training loss for batch 8196 : 0.058928608894348145\n",
      "Training loss for batch 8197 : 0.5960159301757812\n",
      "Training loss for batch 8198 : 0.09813465178012848\n",
      "Training loss for batch 8199 : 0.236934632062912\n",
      "Training loss for batch 8200 : 0.1896519958972931\n",
      "Training loss for batch 8201 : 0.2518264651298523\n",
      "Training loss for batch 8202 : 0.570248544216156\n",
      "Training loss for batch 8203 : 0.47921207547187805\n",
      "Training loss for batch 8204 : 0.26284676790237427\n",
      "Training loss for batch 8205 : 0.3172479271888733\n",
      "Training loss for batch 8206 : 0.3559125065803528\n",
      "Training loss for batch 8207 : 0.19792193174362183\n",
      "Training loss for batch 8208 : 0.32188859581947327\n",
      "Training loss for batch 8209 : 0.31809911131858826\n",
      "Training loss for batch 8210 : 0.0264738816767931\n",
      "Training loss for batch 8211 : 0.2744370102882385\n",
      "Training loss for batch 8212 : 0.2621479034423828\n",
      "Training loss for batch 8213 : 0.47623780369758606\n",
      "Training loss for batch 8214 : 0.7286081910133362\n",
      "Training loss for batch 8215 : 0.02464090846478939\n",
      "Training loss for batch 8216 : 0.12606355547904968\n",
      "Training loss for batch 8217 : 0.32333195209503174\n",
      "Training loss for batch 8218 : 0.10811935365200043\n",
      "Training loss for batch 8219 : 0.29996538162231445\n",
      "Training loss for batch 8220 : 0.04919175058603287\n",
      "Training loss for batch 8221 : 0.26894739270210266\n",
      "Training loss for batch 8222 : 0.5135625004768372\n",
      "Training loss for batch 8223 : 0.16465461254119873\n",
      "Training loss for batch 8224 : 0.03610828146338463\n",
      "Training loss for batch 8225 : 0.10411585122346878\n",
      "Training loss for batch 8226 : 0.15219677984714508\n",
      "Training loss for batch 8227 : 0.27287033200263977\n",
      "Training loss for batch 8228 : 0.2263047993183136\n",
      "Training loss for batch 8229 : 0.2089853584766388\n",
      "Training loss for batch 8230 : 0.015411406755447388\n",
      "Training loss for batch 8231 : 0.42102259397506714\n",
      "Training loss for batch 8232 : 0.32693204283714294\n",
      "Training loss for batch 8233 : 0.39041048288345337\n",
      "Training loss for batch 8234 : 0.2919766306877136\n",
      "Training loss for batch 8235 : 0.1554265320301056\n",
      "Training loss for batch 8236 : 0.13955076038837433\n",
      "Training loss for batch 8237 : 0.4740881025791168\n",
      "Training loss for batch 8238 : 0.17041727900505066\n",
      "Training loss for batch 8239 : 0.5433173179626465\n",
      "Training loss for batch 8240 : 0.17534925043582916\n",
      "Training loss for batch 8241 : 0.3653990626335144\n",
      "Training loss for batch 8242 : 0.139749675989151\n",
      "Training loss for batch 8243 : 0.11015966534614563\n",
      "Training loss for batch 8244 : 0.20359405875205994\n",
      "Training loss for batch 8245 : 0.704502522945404\n",
      "Training loss for batch 8246 : 0.4080882668495178\n",
      "Training loss for batch 8247 : 0.3401557207107544\n",
      "Training loss for batch 8248 : 0.40641430020332336\n",
      "Training loss for batch 8249 : 0.11263056099414825\n",
      "Training loss for batch 8250 : 0.13021251559257507\n",
      "Training loss for batch 8251 : 0.2606908082962036\n",
      "Training loss for batch 8252 : 0.11216513812541962\n",
      "Training loss for batch 8253 : 0.3150886595249176\n",
      "Training loss for batch 8254 : 0.2109057903289795\n",
      "Training loss for batch 8255 : 0.2950875759124756\n",
      "Training loss for batch 8256 : 0.2624610662460327\n",
      "Training loss for batch 8257 : 0.2562822699546814\n",
      "Training loss for batch 8258 : 0.2800365388393402\n",
      "Training loss for batch 8259 : 0.20682382583618164\n",
      "Training loss for batch 8260 : 0.08263290673494339\n",
      "Training loss for batch 8261 : 0.3245601952075958\n",
      "Training loss for batch 8262 : 0.06088662147521973\n",
      "Training loss for batch 8263 : 0.031843774020671844\n",
      "Training loss for batch 8264 : 0.26109546422958374\n",
      "Training loss for batch 8265 : 0.5715797543525696\n",
      "Training loss for batch 8266 : 0.2300664484500885\n",
      "Training loss for batch 8267 : 0.047476887702941895\n",
      "Training loss for batch 8268 : 0.0\n",
      "Training loss for batch 8269 : 0.22256673872470856\n",
      "Training loss for batch 8270 : 0.06982441991567612\n",
      "Training loss for batch 8271 : 0.08707199990749359\n",
      "Training loss for batch 8272 : 0.36995264887809753\n",
      "Training loss for batch 8273 : 0.4609948694705963\n",
      "Training loss for batch 8274 : 0.31322136521339417\n",
      "Training loss for batch 8275 : 0.14863964915275574\n",
      "Training loss for batch 8276 : 0.1771790087223053\n",
      "Training loss for batch 8277 : 0.18639199435710907\n",
      "Training loss for batch 8278 : 0.0395878404378891\n",
      "Training loss for batch 8279 : 0.34905096888542175\n",
      "Training loss for batch 8280 : 0.13712193071842194\n",
      "Training loss for batch 8281 : 0.45642101764678955\n",
      "Training loss for batch 8282 : 0.08774427324533463\n",
      "Training loss for batch 8283 : 0.34251904487609863\n",
      "Training loss for batch 8284 : 0.30715468525886536\n",
      "Training loss for batch 8285 : 0.48274508118629456\n",
      "Training loss for batch 8286 : 0.14918960630893707\n",
      "Training loss for batch 8287 : 0.5415498614311218\n",
      "Training loss for batch 8288 : 0.45207035541534424\n",
      "Training loss for batch 8289 : 0.4906730353832245\n",
      "Training loss for batch 8290 : 0.399539053440094\n",
      "Training loss for batch 8291 : 0.8292948007583618\n",
      "Training loss for batch 8292 : 0.5562930703163147\n",
      "Training loss for batch 8293 : 0.25219157338142395\n",
      "Training loss for batch 8294 : 0.12058049440383911\n",
      "Training loss for batch 8295 : 0.29020148515701294\n",
      "Training loss for batch 8296 : 0.1337362378835678\n",
      "Training loss for batch 8297 : 0.013458512723445892\n",
      "Training loss for batch 8298 : 0.25423464179039\n",
      "Training loss for batch 8299 : 0.2810923755168915\n",
      "Training loss for batch 8300 : 0.2751777172088623\n",
      "Training loss for batch 8301 : 0.1461268961429596\n",
      "Training loss for batch 8302 : 0.16214561462402344\n",
      "Training loss for batch 8303 : 0.18170280754566193\n",
      "Training loss for batch 8304 : 0.10939574241638184\n",
      "Training loss for batch 8305 : 0.10495501011610031\n",
      "Training loss for batch 8306 : 0.0536629855632782\n",
      "Training loss for batch 8307 : 1.0962895154953003\n",
      "Parameter containing:\n",
      "tensor(0.0963, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.0962790250778198\n",
      "Training loss for batch 1 : 1.0962636470794678\n",
      "Training loss for batch 2 : 1.096243977546692\n",
      "Training loss for batch 3 : 1.0962207317352295\n",
      "Training loss for batch 4 : 1.0961942672729492\n",
      "Training loss for batch 5 : 1.0961650609970093\n",
      "Training loss for batch 6 : 1.0961337089538574\n",
      "Training loss for batch 7 : 1.0961002111434937\n",
      "Training loss for batch 8 : 1.0960651636123657\n",
      "Training loss for batch 9 : 1.0960285663604736\n",
      "Training loss for batch 10 : 1.095990777015686\n",
      "Training loss for batch 11 : 1.095952033996582\n",
      "Training loss for batch 12 : 0.27308908104896545\n",
      "Training loss for batch 13 : 0.048120949417352676\n",
      "Training loss for batch 14 : 0.19474227726459503\n",
      "Training loss for batch 15 : 0.10102258622646332\n",
      "Training loss for batch 16 : 0.44708988070487976\n",
      "Training loss for batch 17 : 0.07656148821115494\n",
      "Training loss for batch 18 : 0.12100653350353241\n",
      "Training loss for batch 19 : 0.26076754927635193\n",
      "Training loss for batch 20 : 0.11299686133861542\n",
      "Training loss for batch 21 : 0.2771618962287903\n",
      "Training loss for batch 22 : 0.05358261615037918\n",
      "Training loss for batch 23 : 0.26485639810562134\n",
      "Training loss for batch 24 : 0.20361939072608948\n",
      "Training loss for batch 25 : 0.17443151772022247\n",
      "Training loss for batch 26 : 0.03863473981618881\n",
      "Training loss for batch 27 : 0.26006293296813965\n",
      "Training loss for batch 28 : 0.0002766393590718508\n",
      "Training loss for batch 29 : 0.27656400203704834\n",
      "Training loss for batch 30 : 0.25821197032928467\n",
      "Training loss for batch 31 : 0.06304056942462921\n",
      "Training loss for batch 32 : 0.16457009315490723\n",
      "Training loss for batch 33 : 0.04468291625380516\n",
      "Training loss for batch 34 : 0.43995118141174316\n",
      "Training loss for batch 35 : 0.057214975357055664\n",
      "Training loss for batch 36 : 0.2534668445587158\n",
      "Training loss for batch 37 : 0.07384452223777771\n",
      "Training loss for batch 38 : 0.47099220752716064\n",
      "Training loss for batch 39 : 0.045811571180820465\n",
      "Training loss for batch 40 : 0.02015996351838112\n",
      "Training loss for batch 41 : 0.35523372888565063\n",
      "Training loss for batch 42 : 0.5483216047286987\n",
      "Training loss for batch 43 : 0.0385177880525589\n",
      "Training loss for batch 44 : 0.15830248594284058\n",
      "Training loss for batch 45 : 0.4649010896682739\n",
      "Training loss for batch 46 : 0.10121861100196838\n",
      "Training loss for batch 47 : 0.16441337764263153\n",
      "Training loss for batch 48 : 0.5798285007476807\n",
      "Training loss for batch 49 : 0.3999330997467041\n",
      "Training loss for batch 50 : 0.359235554933548\n",
      "Training loss for batch 51 : 8.631242963019758e-05\n",
      "Training loss for batch 52 : 0.44794943928718567\n",
      "Training loss for batch 53 : 0.42664316296577454\n",
      "Training loss for batch 54 : 0.1860334724187851\n",
      "Training loss for batch 55 : 0.21185711026191711\n",
      "Training loss for batch 56 : 0.13905712962150574\n",
      "Training loss for batch 57 : 0.184725821018219\n",
      "Training loss for batch 58 : 0.5842813849449158\n",
      "Training loss for batch 59 : 0.4348873496055603\n",
      "Training loss for batch 60 : 0.006539344787597656\n",
      "Training loss for batch 61 : 0.15527571737766266\n",
      "Training loss for batch 62 : 0.23473909497261047\n",
      "Training loss for batch 63 : 0.24892303347587585\n",
      "Training loss for batch 64 : 0.061375852674245834\n",
      "Training loss for batch 65 : 0.2782466411590576\n",
      "Training loss for batch 66 : 0.017356395721435547\n",
      "Training loss for batch 67 : 0.056321918964385986\n",
      "Training loss for batch 68 : 0.1343037486076355\n",
      "Training loss for batch 69 : 0.15126155316829681\n",
      "Training loss for batch 70 : 0.5509564876556396\n",
      "Training loss for batch 71 : 0.1616588681936264\n",
      "Training loss for batch 72 : 0.3796965479850769\n",
      "Training loss for batch 73 : 0.4834749698638916\n",
      "Training loss for batch 74 : 0.17054766416549683\n",
      "Training loss for batch 75 : 0.28985869884490967\n",
      "Training loss for batch 76 : 0.19488050043582916\n",
      "Training loss for batch 77 : 0.1400359570980072\n",
      "Training loss for batch 78 : 0.172611266374588\n",
      "Training loss for batch 79 : 0.23756961524486542\n",
      "Training loss for batch 80 : 0.09082797169685364\n",
      "Training loss for batch 81 : 0.5575912594795227\n",
      "Training loss for batch 82 : 0.12053005397319794\n",
      "Training loss for batch 83 : 0.013435428962111473\n",
      "Training loss for batch 84 : 0.19927233457565308\n",
      "Training loss for batch 85 : 0.3659909963607788\n",
      "Training loss for batch 86 : 0.25301992893218994\n",
      "Training loss for batch 87 : 0.12931230664253235\n",
      "Training loss for batch 88 : 0.4225060045719147\n",
      "Training loss for batch 89 : 0.03305555135011673\n",
      "Training loss for batch 90 : 0.11879754811525345\n",
      "Training loss for batch 91 : 0.2730599045753479\n",
      "Training loss for batch 92 : 0.08728134632110596\n",
      "Training loss for batch 93 : 0.1161462664604187\n",
      "Training loss for batch 94 : 0.2215327024459839\n",
      "Training loss for batch 95 : 0.11945150792598724\n",
      "Training loss for batch 96 : 0.0\n",
      "Training loss for batch 97 : 0.054012369364500046\n",
      "Training loss for batch 98 : 0.13914838433265686\n",
      "Training loss for batch 99 : 0.06740982085466385\n",
      "Training loss for batch 100 : 0.417538046836853\n",
      "Training loss for batch 101 : 0.128553107380867\n",
      "Training loss for batch 102 : 0.0\n",
      "Training loss for batch 103 : 0.1417384147644043\n",
      "Training loss for batch 104 : 0.12503786385059357\n",
      "Training loss for batch 105 : 0.14328832924365997\n",
      "Training loss for batch 106 : 0.06421144306659698\n",
      "Training loss for batch 107 : 0.12337001413106918\n",
      "Training loss for batch 108 : 0.07731005549430847\n",
      "Training loss for batch 109 : 0.21020060777664185\n",
      "Training loss for batch 110 : 0.23528413474559784\n",
      "Training loss for batch 111 : 0.10001734644174576\n",
      "Training loss for batch 112 : 0.01208651065826416\n",
      "Training loss for batch 113 : 0.41477522253990173\n",
      "Training loss for batch 114 : 0.3025873303413391\n",
      "Training loss for batch 115 : 0.33305442333221436\n",
      "Training loss for batch 116 : 0.2885031998157501\n",
      "Training loss for batch 117 : 0.056196730583906174\n",
      "Training loss for batch 118 : 0.0\n",
      "Training loss for batch 119 : 0.1664239913225174\n",
      "Training loss for batch 120 : 0.22729012370109558\n",
      "Training loss for batch 121 : 0.19188985228538513\n",
      "Training loss for batch 122 : 0.05187319219112396\n",
      "Training loss for batch 123 : 0.21901766955852509\n",
      "Training loss for batch 124 : 0.5403602719306946\n",
      "Training loss for batch 125 : 0.05663185194134712\n",
      "Training loss for batch 126 : 0.40175187587738037\n",
      "Training loss for batch 127 : 0.23621505498886108\n",
      "Training loss for batch 128 : 0.13333448767662048\n",
      "Training loss for batch 129 : 0.15044569969177246\n",
      "Training loss for batch 130 : 0.08634629845619202\n",
      "Training loss for batch 131 : 0.416181743144989\n",
      "Training loss for batch 132 : 0.38145947456359863\n",
      "Training loss for batch 133 : 0.0825500637292862\n",
      "Training loss for batch 134 : 0.08948831260204315\n",
      "Training loss for batch 135 : 0.5523903369903564\n",
      "Training loss for batch 136 : 0.39903339743614197\n",
      "Training loss for batch 137 : 0.04691792279481888\n",
      "Training loss for batch 138 : 0.0\n",
      "Training loss for batch 139 : 0.0777919590473175\n",
      "Training loss for batch 140 : 0.0\n",
      "Training loss for batch 141 : 0.3180190324783325\n",
      "Training loss for batch 142 : 0.2411840409040451\n",
      "Training loss for batch 143 : 0.17761491239070892\n",
      "Training loss for batch 144 : 0.839976966381073\n",
      "Training loss for batch 145 : 0.43501585721969604\n",
      "Training loss for batch 146 : 0.30963268876075745\n",
      "Training loss for batch 147 : 0.27609190344810486\n",
      "Training loss for batch 148 : 0.1266465038061142\n",
      "Training loss for batch 149 : 0.16936534643173218\n",
      "Training loss for batch 150 : 0.010462473146617413\n",
      "Training loss for batch 151 : 0.20509372651576996\n",
      "Training loss for batch 152 : 0.0636644959449768\n",
      "Training loss for batch 153 : 0.025543371215462685\n",
      "Training loss for batch 154 : 0.3780410885810852\n",
      "Training loss for batch 155 : 0.33782538771629333\n",
      "Training loss for batch 156 : 0.5355660319328308\n",
      "Training loss for batch 157 : 0.45216047763824463\n",
      "Training loss for batch 158 : 0.4315652847290039\n",
      "Training loss for batch 159 : 0.13574373722076416\n",
      "Training loss for batch 160 : 0.489173024892807\n",
      "Training loss for batch 161 : 0.11040602624416351\n",
      "Training loss for batch 162 : 0.4382132887840271\n",
      "Training loss for batch 163 : 0.4452589750289917\n",
      "Training loss for batch 164 : 0.16132645308971405\n",
      "Training loss for batch 165 : 0.15340271592140198\n",
      "Training loss for batch 166 : 0.2828200161457062\n",
      "Training loss for batch 167 : 0.24363598227500916\n",
      "Training loss for batch 168 : 0.28718942403793335\n",
      "Training loss for batch 169 : 0.02874843217432499\n",
      "Training loss for batch 170 : 0.6098425984382629\n",
      "Training loss for batch 171 : 0.5314546823501587\n",
      "Training loss for batch 172 : 0.13001222908496857\n",
      "Training loss for batch 173 : 0.1992933303117752\n",
      "Training loss for batch 174 : 0.2777792811393738\n",
      "Training loss for batch 175 : 0.2216213196516037\n",
      "Training loss for batch 176 : 0.19103647768497467\n",
      "Training loss for batch 177 : 0.1085633635520935\n",
      "Training loss for batch 178 : 0.11800777167081833\n",
      "Training loss for batch 179 : 0.25464922189712524\n",
      "Training loss for batch 180 : 0.1670849472284317\n",
      "Training loss for batch 181 : 0.32830655574798584\n",
      "Training loss for batch 182 : 0.3468184471130371\n",
      "Training loss for batch 183 : 2.2558384443982504e-05\n",
      "Training loss for batch 184 : 0.14897175133228302\n",
      "Training loss for batch 185 : 0.15829192101955414\n",
      "Training loss for batch 186 : 0.7156551480293274\n",
      "Training loss for batch 187 : 0.12125279009342194\n",
      "Training loss for batch 188 : 0.4662785232067108\n",
      "Training loss for batch 189 : 0.5019842386245728\n",
      "Training loss for batch 190 : 0.3407800495624542\n",
      "Training loss for batch 191 : 0.34025371074676514\n",
      "Training loss for batch 192 : 0.10901573300361633\n",
      "Training loss for batch 193 : 0.5901502370834351\n",
      "Training loss for batch 194 : 0.04890979453921318\n",
      "Training loss for batch 195 : 0.022020570933818817\n",
      "Training loss for batch 196 : 0.1319664865732193\n",
      "Training loss for batch 197 : 0.09359430521726608\n",
      "Training loss for batch 198 : 0.15291112661361694\n",
      "Training loss for batch 199 : 0.34029439091682434\n",
      "Training loss for batch 200 : 0.3112245500087738\n",
      "Training loss for batch 201 : 0.16208237409591675\n",
      "Training loss for batch 202 : 0.07545848190784454\n",
      "Training loss for batch 203 : 0.19349822402000427\n",
      "Training loss for batch 204 : 0.18967123329639435\n",
      "Training loss for batch 205 : 0.2264508455991745\n",
      "Training loss for batch 206 : 0.0012234277091920376\n",
      "Training loss for batch 207 : 0.11192616075277328\n",
      "Training loss for batch 208 : 0.27439117431640625\n",
      "Training loss for batch 209 : 0.08137083798646927\n",
      "Training loss for batch 210 : 0.10914729535579681\n",
      "Training loss for batch 211 : 0.11611540615558624\n",
      "Training loss for batch 212 : 0.22355298697948456\n",
      "Training loss for batch 213 : 0.12355733662843704\n",
      "Training loss for batch 214 : 0.11365652084350586\n",
      "Training loss for batch 215 : 0.5174497961997986\n",
      "Training loss for batch 216 : 0.1278822422027588\n",
      "Training loss for batch 217 : 0.2264236956834793\n",
      "Training loss for batch 218 : 0.02641329914331436\n",
      "Training loss for batch 219 : 0.45117563009262085\n",
      "Training loss for batch 220 : 0.22632285952568054\n",
      "Training loss for batch 221 : 0.04613497480750084\n",
      "Training loss for batch 222 : 0.5324152708053589\n",
      "Training loss for batch 223 : 0.0011734863510355353\n",
      "Training loss for batch 224 : 0.29286283254623413\n",
      "Training loss for batch 225 : 0.26416707038879395\n",
      "Training loss for batch 226 : 0.35744813084602356\n",
      "Training loss for batch 227 : 0.12087263911962509\n",
      "Training loss for batch 228 : 0.1884479522705078\n",
      "Training loss for batch 229 : 0.06332030892372131\n",
      "Training loss for batch 230 : 0.1866101622581482\n",
      "Training loss for batch 231 : 0.3412068784236908\n",
      "Training loss for batch 232 : 0.1047033742070198\n",
      "Training loss for batch 233 : 0.12997585535049438\n",
      "Training loss for batch 234 : 0.5630046129226685\n",
      "Training loss for batch 235 : 0.04000525176525116\n",
      "Training loss for batch 236 : 0.40948954224586487\n",
      "Training loss for batch 237 : 0.9198402762413025\n",
      "Training loss for batch 238 : 0.27176418900489807\n",
      "Training loss for batch 239 : 0.38579249382019043\n",
      "Training loss for batch 240 : 0.07549021393060684\n",
      "Training loss for batch 241 : 0.13139326870441437\n",
      "Training loss for batch 242 : 0.05901053547859192\n",
      "Training loss for batch 243 : 0.22113262116909027\n",
      "Training loss for batch 244 : 0.6415985822677612\n",
      "Training loss for batch 245 : 0.3596668839454651\n",
      "Training loss for batch 246 : 0.16061876714229584\n",
      "Training loss for batch 247 : 0.20235417783260345\n",
      "Training loss for batch 248 : 0.014019002206623554\n",
      "Training loss for batch 249 : 0.2547571361064911\n",
      "Training loss for batch 250 : 0.16425667703151703\n",
      "Training loss for batch 251 : 0.19452469050884247\n",
      "Training loss for batch 252 : 0.03018246218562126\n",
      "Training loss for batch 253 : 0.3659566044807434\n",
      "Training loss for batch 254 : 0.5291071534156799\n",
      "Training loss for batch 255 : 0.1758388727903366\n",
      "Training loss for batch 256 : 0.22827397286891937\n",
      "Training loss for batch 257 : 0.2358308881521225\n",
      "Training loss for batch 258 : 0.08108227699995041\n",
      "Training loss for batch 259 : 0.11169060319662094\n",
      "Training loss for batch 260 : 0.5576410889625549\n",
      "Training loss for batch 261 : 0.21419669687747955\n",
      "Training loss for batch 262 : 0.20650172233581543\n",
      "Training loss for batch 263 : 0.424972802400589\n",
      "Training loss for batch 264 : 0.30904319882392883\n",
      "Training loss for batch 265 : 0.5891912579536438\n",
      "Training loss for batch 266 : 0.10998217016458511\n",
      "Training loss for batch 267 : 0.26535502076148987\n",
      "Training loss for batch 268 : 0.9631255269050598\n",
      "Training loss for batch 269 : 0.14129742980003357\n",
      "Training loss for batch 270 : 0.21385088562965393\n",
      "Training loss for batch 271 : 0.28510963916778564\n",
      "Training loss for batch 272 : 0.3155541718006134\n",
      "Training loss for batch 273 : 0.054858844727277756\n",
      "Training loss for batch 274 : 0.5120863914489746\n",
      "Training loss for batch 275 : 0.21998728811740875\n",
      "Training loss for batch 276 : 0.0912148728966713\n",
      "Training loss for batch 277 : 0.02567911148071289\n",
      "Training loss for batch 278 : 0.060445740818977356\n",
      "Training loss for batch 279 : 0.4652835428714752\n",
      "Training loss for batch 280 : 0.5946322083473206\n",
      "Training loss for batch 281 : 0.14902907609939575\n",
      "Training loss for batch 282 : 0.1111285611987114\n",
      "Training loss for batch 283 : 0.36344388127326965\n",
      "Training loss for batch 284 : 0.3485317826271057\n",
      "Training loss for batch 285 : 0.16522341966629028\n",
      "Training loss for batch 286 : 0.5707232356071472\n",
      "Training loss for batch 287 : 0.08696896582841873\n",
      "Training loss for batch 288 : 0.1392299383878708\n",
      "Training loss for batch 289 : 0.3246051073074341\n",
      "Training loss for batch 290 : 0.4143129289150238\n",
      "Training loss for batch 291 : 0.15644647181034088\n",
      "Training loss for batch 292 : 0.1731555461883545\n",
      "Training loss for batch 293 : 0.08196132630109787\n",
      "Training loss for batch 294 : 0.31891968846321106\n",
      "Training loss for batch 295 : 0.11664620786905289\n",
      "Training loss for batch 296 : 0.24946707487106323\n",
      "Training loss for batch 297 : 0.1675586700439453\n",
      "Training loss for batch 298 : 0.22057098150253296\n",
      "Training loss for batch 299 : 0.23293626308441162\n",
      "Training loss for batch 300 : 0.25925540924072266\n",
      "Training loss for batch 301 : 0.07155999541282654\n",
      "Training loss for batch 302 : 0.021953225135803223\n",
      "Training loss for batch 303 : 0.24204327166080475\n",
      "Training loss for batch 304 : 0.31248822808265686\n",
      "Training loss for batch 305 : 0.16866812109947205\n",
      "Training loss for batch 306 : 0.2691058814525604\n",
      "Training loss for batch 307 : 0.07612060755491257\n",
      "Training loss for batch 308 : 0.1852003037929535\n",
      "Training loss for batch 309 : 0.3013390004634857\n",
      "Training loss for batch 310 : 0.01281118392944336\n",
      "Training loss for batch 311 : 0.19011598825454712\n",
      "Training loss for batch 312 : 0.10040085762739182\n",
      "Training loss for batch 313 : 0.0653751865029335\n",
      "Training loss for batch 314 : 0.05518006905913353\n",
      "Training loss for batch 315 : 0.02314368262887001\n",
      "Training loss for batch 316 : 0.12661336362361908\n",
      "Training loss for batch 317 : 0.07366077601909637\n",
      "Training loss for batch 318 : 0.3084124028682709\n",
      "Training loss for batch 319 : 0.2515644133090973\n",
      "Training loss for batch 320 : 0.19657786190509796\n",
      "Training loss for batch 321 : 0.019271250814199448\n",
      "Training loss for batch 322 : 0.27624252438545227\n",
      "Training loss for batch 323 : 0.02058848924934864\n",
      "Training loss for batch 324 : 0.22818471491336823\n",
      "Training loss for batch 325 : 0.28090235590934753\n",
      "Training loss for batch 326 : 0.10475564002990723\n",
      "Training loss for batch 327 : 0.3989904224872589\n",
      "Training loss for batch 328 : 0.3306439220905304\n",
      "Training loss for batch 329 : 0.11696527153253555\n",
      "Training loss for batch 330 : 0.028250647708773613\n",
      "Training loss for batch 331 : 0.0109989233314991\n",
      "Training loss for batch 332 : 0.1264270544052124\n",
      "Training loss for batch 333 : 0.16432948410511017\n",
      "Training loss for batch 334 : 0.22061432898044586\n",
      "Training loss for batch 335 : 0.11975152045488358\n",
      "Training loss for batch 336 : 0.4079072177410126\n",
      "Training loss for batch 337 : 0.5794782638549805\n",
      "Training loss for batch 338 : 0.3164355456829071\n",
      "Training loss for batch 339 : 0.11789041757583618\n",
      "Training loss for batch 340 : 0.02567528374493122\n",
      "Training loss for batch 341 : 0.04403966665267944\n",
      "Training loss for batch 342 : 0.5179606676101685\n",
      "Training loss for batch 343 : 0.15556059777736664\n",
      "Training loss for batch 344 : 0.05022718757390976\n",
      "Training loss for batch 345 : 0.19720086455345154\n",
      "Training loss for batch 346 : 0.18319612741470337\n",
      "Training loss for batch 347 : 0.2509673833847046\n",
      "Training loss for batch 348 : 0.2555941343307495\n",
      "Training loss for batch 349 : 0.21526551246643066\n",
      "Training loss for batch 350 : 0.2257530391216278\n",
      "Training loss for batch 351 : 0.24649758636951447\n",
      "Training loss for batch 352 : 0.00045140585280023515\n",
      "Training loss for batch 353 : 0.025763435289263725\n",
      "Training loss for batch 354 : 0.48694828152656555\n",
      "Training loss for batch 355 : 0.16094182431697845\n",
      "Training loss for batch 356 : 0.2219829261302948\n",
      "Training loss for batch 357 : 0.2566682994365692\n",
      "Training loss for batch 358 : 0.05773989111185074\n",
      "Training loss for batch 359 : 0.14924567937850952\n",
      "Training loss for batch 360 : 0.022463619709014893\n",
      "Training loss for batch 361 : 0.04543996974825859\n",
      "Training loss for batch 362 : 0.1880566030740738\n",
      "Training loss for batch 363 : 0.8679881691932678\n",
      "Training loss for batch 364 : 0.3027748465538025\n",
      "Training loss for batch 365 : 0.21554233133792877\n",
      "Training loss for batch 366 : 0.2978817820549011\n",
      "Training loss for batch 367 : 0.2533241808414459\n",
      "Training loss for batch 368 : 0.14308439195156097\n",
      "Training loss for batch 369 : 0.18694967031478882\n",
      "Training loss for batch 370 : 0.18808385729789734\n",
      "Training loss for batch 371 : 0.22555701434612274\n",
      "Training loss for batch 372 : 0.03546091541647911\n",
      "Training loss for batch 373 : 0.09337692707777023\n",
      "Training loss for batch 374 : 0.16312342882156372\n",
      "Training loss for batch 375 : 0.08856949955224991\n",
      "Training loss for batch 376 : 0.15563729405403137\n",
      "Training loss for batch 377 : 0.26671966910362244\n",
      "Training loss for batch 378 : 0.2193109393119812\n",
      "Training loss for batch 379 : 0.6890535950660706\n",
      "Training loss for batch 380 : 0.17663723230361938\n",
      "Training loss for batch 381 : 0.08265998214483261\n",
      "Training loss for batch 382 : 0.398770272731781\n",
      "Training loss for batch 383 : 0.529721200466156\n",
      "Training loss for batch 384 : 0.09550042450428009\n",
      "Training loss for batch 385 : 0.054514750838279724\n",
      "Training loss for batch 386 : 0.0655631497502327\n",
      "Training loss for batch 387 : 0.0940975546836853\n",
      "Training loss for batch 388 : 0.12326844781637192\n",
      "Training loss for batch 389 : 0.09379074722528458\n",
      "Training loss for batch 390 : 0.6031635403633118\n",
      "Training loss for batch 391 : 0.0009010237990878522\n",
      "Training loss for batch 392 : 0.2081088274717331\n",
      "Training loss for batch 393 : 0.2726598083972931\n",
      "Training loss for batch 394 : 0.28110888600349426\n",
      "Training loss for batch 395 : 0.3471056818962097\n",
      "Training loss for batch 396 : 0.028308352455496788\n",
      "Training loss for batch 397 : 0.7867166996002197\n",
      "Training loss for batch 398 : 0.10657180100679398\n",
      "Training loss for batch 399 : 0.36677637696266174\n",
      "Training loss for batch 400 : 0.10869038850069046\n",
      "Training loss for batch 401 : 0.14518757164478302\n",
      "Training loss for batch 402 : 0.12903524935245514\n",
      "Training loss for batch 403 : 0.11829791218042374\n",
      "Training loss for batch 404 : 0.07463771849870682\n",
      "Training loss for batch 405 : 0.3776654601097107\n",
      "Training loss for batch 406 : 0.16010242700576782\n",
      "Training loss for batch 407 : 0.4927234351634979\n",
      "Training loss for batch 408 : 0.17174813151359558\n",
      "Training loss for batch 409 : 0.24767233431339264\n",
      "Training loss for batch 410 : 0.28730276226997375\n",
      "Training loss for batch 411 : 0.08963456749916077\n",
      "Training loss for batch 412 : 0.15746349096298218\n",
      "Training loss for batch 413 : 0.6101604700088501\n",
      "Training loss for batch 414 : 0.1662738174200058\n",
      "Training loss for batch 415 : 0.3045056164264679\n",
      "Training loss for batch 416 : 0.009386958554387093\n",
      "Training loss for batch 417 : 0.1423899531364441\n",
      "Training loss for batch 418 : 0.3617481291294098\n",
      "Training loss for batch 419 : 0.09586293250322342\n",
      "Training loss for batch 420 : 0.6262248158454895\n",
      "Training loss for batch 421 : 0.27697205543518066\n",
      "Training loss for batch 422 : 0.16923995316028595\n",
      "Training loss for batch 423 : 0.09782706946134567\n",
      "Training loss for batch 424 : 0.15944203734397888\n",
      "Training loss for batch 425 : 0.10983064770698547\n",
      "Training loss for batch 426 : 0.2225891500711441\n",
      "Training loss for batch 427 : 0.10088244080543518\n",
      "Training loss for batch 428 : 0.2158212810754776\n",
      "Training loss for batch 429 : 0.6182959079742432\n",
      "Training loss for batch 430 : 0.05527254194021225\n",
      "Training loss for batch 431 : 0.43743330240249634\n",
      "Training loss for batch 432 : 0.08388111740350723\n",
      "Training loss for batch 433 : 0.2853190004825592\n",
      "Training loss for batch 434 : 0.12532252073287964\n",
      "Training loss for batch 435 : 0.16723693907260895\n",
      "Training loss for batch 436 : 0.23129625618457794\n",
      "Training loss for batch 437 : 0.010502565652132034\n",
      "Training loss for batch 438 : 0.125364288687706\n",
      "Training loss for batch 439 : 0.2878442406654358\n",
      "Training loss for batch 440 : 0.2509194612503052\n",
      "Training loss for batch 441 : 0.30942052602767944\n",
      "Training loss for batch 442 : 0.2039879858493805\n",
      "Training loss for batch 443 : 0.3273317515850067\n",
      "Training loss for batch 444 : 0.01966828852891922\n",
      "Training loss for batch 445 : 0.2254340499639511\n",
      "Training loss for batch 446 : 0.15310069918632507\n",
      "Training loss for batch 447 : 0.03029775246977806\n",
      "Training loss for batch 448 : 0.02562466636300087\n",
      "Training loss for batch 449 : 0.013253949582576752\n",
      "Training loss for batch 450 : 0.5033950805664062\n",
      "Training loss for batch 451 : 0.43924903869628906\n",
      "Training loss for batch 452 : 0.14643527567386627\n",
      "Training loss for batch 453 : 0.325578510761261\n",
      "Training loss for batch 454 : 0.16663189232349396\n",
      "Training loss for batch 455 : 0.37138786911964417\n",
      "Training loss for batch 456 : 0.46912774443626404\n",
      "Training loss for batch 457 : 0.23622769117355347\n",
      "Training loss for batch 458 : 0.44080817699432373\n",
      "Training loss for batch 459 : 0.023478083312511444\n",
      "Training loss for batch 460 : 0.0\n",
      "Training loss for batch 461 : 0.10093014687299728\n",
      "Training loss for batch 462 : 0.3984564244747162\n",
      "Training loss for batch 463 : 0.043247297406196594\n",
      "Training loss for batch 464 : 0.04598065838217735\n",
      "Training loss for batch 465 : 0.4414019286632538\n",
      "Training loss for batch 466 : 0.06932291388511658\n",
      "Training loss for batch 467 : 0.4241887629032135\n",
      "Training loss for batch 468 : 0.28530824184417725\n",
      "Training loss for batch 469 : 0.19488532841205597\n",
      "Training loss for batch 470 : 0.1847992092370987\n",
      "Training loss for batch 471 : 0.4152538478374481\n",
      "Training loss for batch 472 : 0.21629831194877625\n",
      "Training loss for batch 473 : 0.3407302796840668\n",
      "Training loss for batch 474 : 0.38579070568084717\n",
      "Training loss for batch 475 : 0.30572509765625\n",
      "Training loss for batch 476 : 0.4146535396575928\n",
      "Training loss for batch 477 : 0.24050606787204742\n",
      "Training loss for batch 478 : 0.13923139870166779\n",
      "Training loss for batch 479 : 0.1455361694097519\n",
      "Training loss for batch 480 : 0.022988220676779747\n",
      "Training loss for batch 481 : 0.13806256651878357\n",
      "Training loss for batch 482 : 0.2028646320104599\n",
      "Training loss for batch 483 : 0.24345587193965912\n",
      "Training loss for batch 484 : 0.39360857009887695\n",
      "Training loss for batch 485 : 0.07846343517303467\n",
      "Training loss for batch 486 : 0.09461310505867004\n",
      "Training loss for batch 487 : 0.14921240508556366\n",
      "Training loss for batch 488 : 0.20275990664958954\n",
      "Training loss for batch 489 : 0.30028215050697327\n",
      "Training loss for batch 490 : 0.2043236941099167\n",
      "Training loss for batch 491 : 0.22048217058181763\n",
      "Training loss for batch 492 : 0.1530895084142685\n",
      "Training loss for batch 493 : 0.24087992310523987\n",
      "Training loss for batch 494 : 0.21426443755626678\n",
      "Training loss for batch 495 : 0.04625290632247925\n",
      "Training loss for batch 496 : 0.3520672023296356\n",
      "Training loss for batch 497 : 0.0675046294927597\n",
      "Training loss for batch 498 : 0.5916259288787842\n",
      "Training loss for batch 499 : 0.053808800876140594\n",
      "Training loss for batch 500 : 0.3200758397579193\n",
      "Training loss for batch 501 : 0.1781444102525711\n",
      "Training loss for batch 502 : 0.057384468615055084\n",
      "Training loss for batch 503 : 0.17980773746967316\n",
      "Training loss for batch 504 : 0.07098080962896347\n",
      "Training loss for batch 505 : 0.26538196206092834\n",
      "Training loss for batch 506 : 0.23778361082077026\n",
      "Training loss for batch 507 : 0.15633173286914825\n",
      "Training loss for batch 508 : 0.5676265954971313\n",
      "Training loss for batch 509 : 0.08833838999271393\n",
      "Training loss for batch 510 : 0.16745339334011078\n",
      "Training loss for batch 511 : 0.2645779550075531\n",
      "Training loss for batch 512 : 0.24656857550144196\n",
      "Training loss for batch 513 : 0.49465179443359375\n",
      "Training loss for batch 514 : 0.18411271274089813\n",
      "Training loss for batch 515 : 0.04952418431639671\n",
      "Training loss for batch 516 : 0.01959003135561943\n",
      "Training loss for batch 517 : 0.13426095247268677\n",
      "Training loss for batch 518 : 0.07529032975435257\n",
      "Training loss for batch 519 : 0.05534464120864868\n",
      "Training loss for batch 520 : 0.18641191720962524\n",
      "Training loss for batch 521 : 0.23498590290546417\n",
      "Training loss for batch 522 : 0.325236052274704\n",
      "Training loss for batch 523 : 0.44380608201026917\n",
      "Training loss for batch 524 : 0.5830237865447998\n",
      "Training loss for batch 525 : 0.062124643474817276\n",
      "Training loss for batch 526 : 0.7278106808662415\n",
      "Training loss for batch 527 : 0.6698654294013977\n",
      "Training loss for batch 528 : 0.07900728285312653\n",
      "Training loss for batch 529 : 0.02866198495030403\n",
      "Training loss for batch 530 : 0.5578738451004028\n",
      "Training loss for batch 531 : 0.2467842996120453\n",
      "Training loss for batch 532 : 0.13385868072509766\n",
      "Training loss for batch 533 : 0.12749402225017548\n",
      "Training loss for batch 534 : 0.14123864471912384\n",
      "Training loss for batch 535 : 0.3098427355289459\n",
      "Training loss for batch 536 : 0.39340803027153015\n",
      "Training loss for batch 537 : 0.19225919246673584\n",
      "Training loss for batch 538 : 0.052199941128492355\n",
      "Training loss for batch 539 : 0.17446039617061615\n",
      "Training loss for batch 540 : 0.26665085554122925\n",
      "Training loss for batch 541 : 0.23019874095916748\n",
      "Training loss for batch 542 : 0.05294208601117134\n",
      "Training loss for batch 543 : 0.18319061398506165\n",
      "Training loss for batch 544 : 0.17838656902313232\n",
      "Training loss for batch 545 : 0.24281707406044006\n",
      "Training loss for batch 546 : 0.43188783526420593\n",
      "Training loss for batch 547 : 0.15215100347995758\n",
      "Training loss for batch 548 : 0.22507691383361816\n",
      "Training loss for batch 549 : 0.40295037627220154\n",
      "Training loss for batch 550 : 0.00827435590326786\n",
      "Training loss for batch 551 : 0.13606658577919006\n",
      "Training loss for batch 552 : 0.1618025302886963\n",
      "Training loss for batch 553 : 0.05467852205038071\n",
      "Training loss for batch 554 : 0.10943503677845001\n",
      "Training loss for batch 555 : 0.43443790078163147\n",
      "Training loss for batch 556 : 0.08509992063045502\n",
      "Training loss for batch 557 : 0.17063410580158234\n",
      "Training loss for batch 558 : 0.17137758433818817\n",
      "Training loss for batch 559 : 0.4093925356864929\n",
      "Training loss for batch 560 : 0.0770830288529396\n",
      "Training loss for batch 561 : 0.3098716735839844\n",
      "Training loss for batch 562 : 0.531255304813385\n",
      "Training loss for batch 563 : 0.05741618946194649\n",
      "Training loss for batch 564 : 0.1898743063211441\n",
      "Training loss for batch 565 : 0.21381548047065735\n",
      "Training loss for batch 566 : 0.33725622296333313\n",
      "Training loss for batch 567 : 0.16281752288341522\n",
      "Training loss for batch 568 : 0.3110264837741852\n",
      "Training loss for batch 569 : 0.7024399042129517\n",
      "Training loss for batch 570 : 0.37851595878601074\n",
      "Training loss for batch 571 : 0.21905992925167084\n",
      "Training loss for batch 572 : 0.06690600514411926\n",
      "Training loss for batch 573 : 0.028171151876449585\n",
      "Training loss for batch 574 : 0.36288759112358093\n",
      "Training loss for batch 575 : 0.13335038721561432\n",
      "Training loss for batch 576 : 0.24519307911396027\n",
      "Training loss for batch 577 : 0.13669157028198242\n",
      "Training loss for batch 578 : 0.2655639350414276\n",
      "Training loss for batch 579 : 0.11736089736223221\n",
      "Training loss for batch 580 : 0.31767839193344116\n",
      "Training loss for batch 581 : 0.027965039014816284\n",
      "Training loss for batch 582 : 0.13887648284435272\n",
      "Training loss for batch 583 : 0.24475394189357758\n",
      "Training loss for batch 584 : 0.05854915454983711\n",
      "Training loss for batch 585 : 0.34077996015548706\n",
      "Training loss for batch 586 : 0.030056431889533997\n",
      "Training loss for batch 587 : 0.43927353620529175\n",
      "Training loss for batch 588 : 0.19043657183647156\n",
      "Training loss for batch 589 : 0.6013845205307007\n",
      "Training loss for batch 590 : 0.30297210812568665\n",
      "Training loss for batch 591 : 0.5105912685394287\n",
      "Training loss for batch 592 : 0.17690664529800415\n",
      "Training loss for batch 593 : 0.1929030418395996\n",
      "Training loss for batch 594 : 0.1830863654613495\n",
      "Training loss for batch 595 : 0.24576011300086975\n",
      "Training loss for batch 596 : 0.16564716398715973\n",
      "Training loss for batch 597 : 0.20277920365333557\n",
      "Training loss for batch 598 : 0.19292545318603516\n",
      "Training loss for batch 599 : 0.21611881256103516\n",
      "Training loss for batch 600 : 0.14100605249404907\n",
      "Training loss for batch 601 : 0.01423216238617897\n",
      "Training loss for batch 602 : 0.03295239061117172\n",
      "Training loss for batch 603 : 0.17111064493656158\n",
      "Training loss for batch 604 : 0.37335067987442017\n",
      "Training loss for batch 605 : 0.2519341707229614\n",
      "Training loss for batch 606 : 0.2755579650402069\n",
      "Training loss for batch 607 : 0.21767748892307281\n",
      "Training loss for batch 608 : 0.2846001088619232\n",
      "Training loss for batch 609 : 0.3140893280506134\n",
      "Training loss for batch 610 : 0.2591511905193329\n",
      "Training loss for batch 611 : 0.25908583402633667\n",
      "Training loss for batch 612 : 0.07848752290010452\n",
      "Training loss for batch 613 : 0.06262637674808502\n",
      "Training loss for batch 614 : 0.2310272455215454\n",
      "Training loss for batch 615 : 0.14099057018756866\n",
      "Training loss for batch 616 : 0.06276571750640869\n",
      "Training loss for batch 617 : 0.11467407643795013\n",
      "Training loss for batch 618 : 0.4717848598957062\n",
      "Training loss for batch 619 : 0.2896721065044403\n",
      "Training loss for batch 620 : 0.10085056722164154\n",
      "Training loss for batch 621 : 0.35336509346961975\n",
      "Training loss for batch 622 : 0.28616857528686523\n",
      "Training loss for batch 623 : 0.18267007172107697\n",
      "Training loss for batch 624 : 0.03642811253666878\n",
      "Training loss for batch 625 : 0.36884260177612305\n",
      "Training loss for batch 626 : 0.3012378215789795\n",
      "Training loss for batch 627 : 0.33218395709991455\n",
      "Training loss for batch 628 : 0.07877513766288757\n",
      "Training loss for batch 629 : 0.44307297468185425\n",
      "Training loss for batch 630 : 0.15176869928836823\n",
      "Training loss for batch 631 : 0.34928271174430847\n",
      "Training loss for batch 632 : 0.060292184352874756\n",
      "Training loss for batch 633 : 0.24269482493400574\n",
      "Training loss for batch 634 : 0.00628643250092864\n",
      "Training loss for batch 635 : 0.08042749017477036\n",
      "Training loss for batch 636 : 0.12626685202121735\n",
      "Training loss for batch 637 : 0.24282050132751465\n",
      "Training loss for batch 638 : 0.18613111972808838\n",
      "Training loss for batch 639 : 0.23803701996803284\n",
      "Training loss for batch 640 : 0.1819145381450653\n",
      "Training loss for batch 641 : 0.1298135668039322\n",
      "Training loss for batch 642 : 0.04726661741733551\n",
      "Training loss for batch 643 : 0.24818724393844604\n",
      "Training loss for batch 644 : 0.3826219141483307\n",
      "Training loss for batch 645 : 0.38990822434425354\n",
      "Training loss for batch 646 : 0.19911910593509674\n",
      "Training loss for batch 647 : 0.07315660268068314\n",
      "Training loss for batch 648 : 0.19460317492485046\n",
      "Training loss for batch 649 : 0.2136095017194748\n",
      "Training loss for batch 650 : 0.05852701514959335\n",
      "Training loss for batch 651 : 0.39154887199401855\n",
      "Training loss for batch 652 : 0.0354343056678772\n",
      "Training loss for batch 653 : 0.13988256454467773\n",
      "Training loss for batch 654 : 0.22058111429214478\n",
      "Training loss for batch 655 : 0.336643248796463\n",
      "Training loss for batch 656 : 0.265739768743515\n",
      "Training loss for batch 657 : 0.1707952618598938\n",
      "Training loss for batch 658 : 0.4134507477283478\n",
      "Training loss for batch 659 : 0.246826633810997\n",
      "Training loss for batch 660 : 0.2754702568054199\n",
      "Training loss for batch 661 : 0.4413851201534271\n",
      "Training loss for batch 662 : 0.31189391016960144\n",
      "Training loss for batch 663 : 0.1286977231502533\n",
      "Training loss for batch 664 : 0.10342001169919968\n",
      "Training loss for batch 665 : 0.14944320917129517\n",
      "Training loss for batch 666 : 0.06981950998306274\n",
      "Training loss for batch 667 : 0.1984899938106537\n",
      "Training loss for batch 668 : 0.061923250555992126\n",
      "Training loss for batch 669 : 0.10697107017040253\n",
      "Training loss for batch 670 : 0.4418811798095703\n",
      "Training loss for batch 671 : 0.10426075756549835\n",
      "Training loss for batch 672 : 0.7030741572380066\n",
      "Training loss for batch 673 : 0.20730550587177277\n",
      "Training loss for batch 674 : 0.01746147871017456\n",
      "Training loss for batch 675 : 0.10279134660959244\n",
      "Training loss for batch 676 : 0.18642432987689972\n",
      "Training loss for batch 677 : 0.39970844984054565\n",
      "Training loss for batch 678 : 0.1735808104276657\n",
      "Training loss for batch 679 : 0.024233784526586533\n",
      "Training loss for batch 680 : 0.176473930478096\n",
      "Training loss for batch 681 : 0.0975288674235344\n",
      "Training loss for batch 682 : 0.4189179539680481\n",
      "Training loss for batch 683 : 0.12631765007972717\n",
      "Training loss for batch 684 : 0.44640523195266724\n",
      "Training loss for batch 685 : 0.11492758244276047\n",
      "Training loss for batch 686 : 0.24802684783935547\n",
      "Training loss for batch 687 : 0.014119388535618782\n",
      "Training loss for batch 688 : 0.15515606105327606\n",
      "Training loss for batch 689 : 0.11222698539495468\n",
      "Training loss for batch 690 : 0.37723657488822937\n",
      "Training loss for batch 691 : 0.14816460013389587\n",
      "Training loss for batch 692 : 0.05263591930270195\n",
      "Training loss for batch 693 : 0.20689290761947632\n",
      "Training loss for batch 694 : 0.059432435780763626\n",
      "Training loss for batch 695 : 0.7278736233711243\n",
      "Training loss for batch 696 : 0.2986493408679962\n",
      "Training loss for batch 697 : 0.08104813098907471\n",
      "Training loss for batch 698 : 0.4394152760505676\n",
      "Training loss for batch 699 : 0.0741051658987999\n",
      "Training loss for batch 700 : 0.27038702368736267\n",
      "Training loss for batch 701 : 0.24121445417404175\n",
      "Training loss for batch 702 : 0.02117617428302765\n",
      "Training loss for batch 703 : 0.25835132598876953\n",
      "Training loss for batch 704 : 0.26884692907333374\n",
      "Training loss for batch 705 : 0.57831209897995\n",
      "Training loss for batch 706 : 0.43935221433639526\n",
      "Training loss for batch 707 : 0.055552367120981216\n",
      "Training loss for batch 708 : 0.18642275035381317\n",
      "Training loss for batch 709 : 0.0\n",
      "Training loss for batch 710 : 0.14401865005493164\n",
      "Training loss for batch 711 : 0.5751833915710449\n",
      "Training loss for batch 712 : 0.3233136534690857\n",
      "Training loss for batch 713 : 0.36626332998275757\n",
      "Training loss for batch 714 : 0.3341844975948334\n",
      "Training loss for batch 715 : 0.13405781984329224\n",
      "Training loss for batch 716 : 0.13178174197673798\n",
      "Training loss for batch 717 : 0.04520692676305771\n",
      "Training loss for batch 718 : 0.13919496536254883\n",
      "Training loss for batch 719 : 0.06434081494808197\n",
      "Training loss for batch 720 : 0.14608511328697205\n",
      "Training loss for batch 721 : 0.5488645434379578\n",
      "Training loss for batch 722 : 0.07263275980949402\n",
      "Training loss for batch 723 : 0.2781883776187897\n",
      "Training loss for batch 724 : 0.27083620429039\n",
      "Training loss for batch 725 : 0.15920792520046234\n",
      "Training loss for batch 726 : 0.2196352630853653\n",
      "Training loss for batch 727 : 0.19011381268501282\n",
      "Training loss for batch 728 : 0.3148384094238281\n",
      "Training loss for batch 729 : 0.2473464459180832\n",
      "Training loss for batch 730 : 0.1488802134990692\n",
      "Training loss for batch 731 : 0.19331401586532593\n",
      "Training loss for batch 732 : 0.009188851341605186\n",
      "Training loss for batch 733 : 0.34619200229644775\n",
      "Training loss for batch 734 : 0.0026255331467837095\n",
      "Training loss for batch 735 : 0.4361425042152405\n",
      "Training loss for batch 736 : 0.14727269113063812\n",
      "Training loss for batch 737 : 0.15262727439403534\n",
      "Training loss for batch 738 : 0.48677384853363037\n",
      "Training loss for batch 739 : 0.16772718727588654\n",
      "Training loss for batch 740 : 0.11221595108509064\n",
      "Training loss for batch 741 : 0.3089582920074463\n",
      "Training loss for batch 742 : 0.3878185749053955\n",
      "Training loss for batch 743 : 0.06594027578830719\n",
      "Training loss for batch 744 : 0.6574403047561646\n",
      "Training loss for batch 745 : 0.2745782136917114\n",
      "Training loss for batch 746 : 0.2295415997505188\n",
      "Training loss for batch 747 : 0.5081711411476135\n",
      "Training loss for batch 748 : 0.5204557776451111\n",
      "Training loss for batch 749 : 0.011897246353328228\n",
      "Training loss for batch 750 : 0.16580668091773987\n",
      "Training loss for batch 751 : 0.25234708189964294\n",
      "Training loss for batch 752 : 0.39481428265571594\n",
      "Training loss for batch 753 : 0.2369629144668579\n",
      "Training loss for batch 754 : 0.3001679480075836\n",
      "Training loss for batch 755 : 0.16711024940013885\n",
      "Training loss for batch 756 : 0.08495981246232986\n",
      "Training loss for batch 757 : 0.28797608613967896\n",
      "Training loss for batch 758 : 0.16895830631256104\n",
      "Training loss for batch 759 : 0.10677764564752579\n",
      "Training loss for batch 760 : 0.3660213351249695\n",
      "Training loss for batch 761 : 0.25066322088241577\n",
      "Training loss for batch 762 : 0.0\n",
      "Training loss for batch 763 : 0.21352191269397736\n",
      "Training loss for batch 764 : 0.14622655510902405\n",
      "Training loss for batch 765 : 0.3698381781578064\n",
      "Training loss for batch 766 : 0.12543301284313202\n",
      "Training loss for batch 767 : 0.1914350539445877\n",
      "Training loss for batch 768 : 0.05776437744498253\n",
      "Training loss for batch 769 : 0.0802871435880661\n",
      "Training loss for batch 770 : 0.12407831102609634\n",
      "Training loss for batch 771 : 0.481272429227829\n",
      "Training loss for batch 772 : 0.1896054744720459\n",
      "Training loss for batch 773 : 0.16753099858760834\n",
      "Training loss for batch 774 : 0.2528918981552124\n",
      "Training loss for batch 775 : 0.24131125211715698\n",
      "Training loss for batch 776 : 0.15613165497779846\n",
      "Training loss for batch 777 : 0.2810423970222473\n",
      "Training loss for batch 778 : 0.26963627338409424\n",
      "Training loss for batch 779 : 0.5030745267868042\n",
      "Training loss for batch 780 : 0.2317231297492981\n",
      "Training loss for batch 781 : 0.22454528510570526\n",
      "Training loss for batch 782 : 0.005681206937879324\n",
      "Training loss for batch 783 : 0.3408539593219757\n",
      "Training loss for batch 784 : 0.10929081588983536\n",
      "Training loss for batch 785 : 0.21452446281909943\n",
      "Training loss for batch 786 : 0.4036351144313812\n",
      "Training loss for batch 787 : 0.11302587389945984\n",
      "Training loss for batch 788 : 0.15538838505744934\n",
      "Training loss for batch 789 : 0.33656787872314453\n",
      "Training loss for batch 790 : 0.22806750237941742\n",
      "Training loss for batch 791 : 0.1310664415359497\n",
      "Training loss for batch 792 : 0.24389535188674927\n",
      "Training loss for batch 793 : 0.32442381978034973\n",
      "Training loss for batch 794 : 0.596581220626831\n",
      "Training loss for batch 795 : 0.25918248295783997\n",
      "Training loss for batch 796 : 0.22750921547412872\n",
      "Training loss for batch 797 : 0.33765938878059387\n",
      "Training loss for batch 798 : 0.26410913467407227\n",
      "Training loss for batch 799 : 0.06799879670143127\n",
      "Training loss for batch 800 : 0.3458074629306793\n",
      "Training loss for batch 801 : 0.12666043639183044\n",
      "Training loss for batch 802 : 0.2518313527107239\n",
      "Training loss for batch 803 : 0.0473904050886631\n",
      "Training loss for batch 804 : 0.3772144019603729\n",
      "Training loss for batch 805 : 0.3580813705921173\n",
      "Training loss for batch 806 : 0.15514302253723145\n",
      "Training loss for batch 807 : 0.32237502932548523\n",
      "Training loss for batch 808 : 0.17645134031772614\n",
      "Training loss for batch 809 : 0.1995152235031128\n",
      "Training loss for batch 810 : 0.3568708896636963\n",
      "Training loss for batch 811 : 0.16069845855236053\n",
      "Training loss for batch 812 : 0.3310989439487457\n",
      "Training loss for batch 813 : 0.14382688701152802\n",
      "Training loss for batch 814 : 0.3114854097366333\n",
      "Training loss for batch 815 : 0.09782083332538605\n",
      "Training loss for batch 816 : 0.34997954964637756\n",
      "Training loss for batch 817 : 0.1274893879890442\n",
      "Training loss for batch 818 : 0.34222233295440674\n",
      "Training loss for batch 819 : 0.24169203639030457\n",
      "Training loss for batch 820 : 0.04631516709923744\n",
      "Training loss for batch 821 : 0.34091001749038696\n",
      "Training loss for batch 822 : 0.04787851497530937\n",
      "Training loss for batch 823 : 0.29159873723983765\n",
      "Training loss for batch 824 : 0.19691641628742218\n",
      "Training loss for batch 825 : 0.13528485596179962\n",
      "Training loss for batch 826 : 0.17090144753456116\n",
      "Training loss for batch 827 : 0.11573638021945953\n",
      "Training loss for batch 828 : 0.12523719668388367\n",
      "Training loss for batch 829 : 0.47208231687545776\n",
      "Training loss for batch 830 : 0.4108654856681824\n",
      "Training loss for batch 831 : 0.010388195514678955\n",
      "Training loss for batch 832 : 0.1367592215538025\n",
      "Training loss for batch 833 : 0.21472075581550598\n",
      "Training loss for batch 834 : 0.3270496129989624\n",
      "Training loss for batch 835 : 0.5096643567085266\n",
      "Training loss for batch 836 : 0.05808234214782715\n",
      "Training loss for batch 837 : 0.023861106485128403\n",
      "Training loss for batch 838 : 0.0766100361943245\n",
      "Training loss for batch 839 : 0.2965514361858368\n",
      "Training loss for batch 840 : 0.4257381558418274\n",
      "Training loss for batch 841 : 0.25468456745147705\n",
      "Training loss for batch 842 : 0.14569956064224243\n",
      "Training loss for batch 843 : 0.2413472831249237\n",
      "Training loss for batch 844 : 0.3165811598300934\n",
      "Training loss for batch 845 : 0.06673698872327805\n",
      "Training loss for batch 846 : 0.08552295714616776\n",
      "Training loss for batch 847 : 0.2466879040002823\n",
      "Training loss for batch 848 : 0.42210596799850464\n",
      "Training loss for batch 849 : 0.1784166842699051\n",
      "Training loss for batch 850 : 0.17129914462566376\n",
      "Training loss for batch 851 : 0.025075450539588928\n",
      "Training loss for batch 852 : 0.08789974451065063\n",
      "Training loss for batch 853 : 0.3598026633262634\n",
      "Training loss for batch 854 : 0.39951610565185547\n",
      "Training loss for batch 855 : 0.009462326765060425\n",
      "Training loss for batch 856 : 0.5152851939201355\n",
      "Training loss for batch 857 : 0.0326789915561676\n",
      "Training loss for batch 858 : 0.07524706423282623\n",
      "Training loss for batch 859 : 0.1752505749464035\n",
      "Training loss for batch 860 : 0.29396989941596985\n",
      "Training loss for batch 861 : 0.12148354947566986\n",
      "Training loss for batch 862 : 0.0980953499674797\n",
      "Training loss for batch 863 : 0.2590680718421936\n",
      "Training loss for batch 864 : 0.38247767090797424\n",
      "Training loss for batch 865 : 0.11779958754777908\n",
      "Training loss for batch 866 : 0.04010271281003952\n",
      "Training loss for batch 867 : 0.3565256595611572\n",
      "Training loss for batch 868 : 0.41769516468048096\n",
      "Training loss for batch 869 : 0.2255222201347351\n",
      "Training loss for batch 870 : 0.11782019585371017\n",
      "Training loss for batch 871 : 0.34867140650749207\n",
      "Training loss for batch 872 : 0.11266787350177765\n",
      "Training loss for batch 873 : 0.6432879567146301\n",
      "Training loss for batch 874 : 0.06158065050840378\n",
      "Training loss for batch 875 : 0.33259227871894836\n",
      "Training loss for batch 876 : 0.1143416091799736\n",
      "Training loss for batch 877 : 0.2939475178718567\n",
      "Training loss for batch 878 : 0.14859765768051147\n",
      "Training loss for batch 879 : 0.38118910789489746\n",
      "Training loss for batch 880 : 0.0757376030087471\n",
      "Training loss for batch 881 : 0.5455140471458435\n",
      "Training loss for batch 882 : 0.5144585371017456\n",
      "Training loss for batch 883 : 0.33187276124954224\n",
      "Training loss for batch 884 : 0.2182771861553192\n",
      "Training loss for batch 885 : 0.06165635585784912\n",
      "Training loss for batch 886 : 0.38807082176208496\n",
      "Training loss for batch 887 : 0.2258770763874054\n",
      "Training loss for batch 888 : 0.2429521083831787\n",
      "Training loss for batch 889 : 0.04867265000939369\n",
      "Training loss for batch 890 : 0.022968055680394173\n",
      "Training loss for batch 891 : 0.24742163717746735\n",
      "Training loss for batch 892 : 0.13858047127723694\n",
      "Training loss for batch 893 : 0.05703768879175186\n",
      "Training loss for batch 894 : 0.28648728132247925\n",
      "Training loss for batch 895 : 0.07587625086307526\n",
      "Training loss for batch 896 : 0.3422110080718994\n",
      "Training loss for batch 897 : 0.24904249608516693\n",
      "Training loss for batch 898 : 0.4332597851753235\n",
      "Training loss for batch 899 : 0.09653599560260773\n",
      "Training loss for batch 900 : 0.013602428138256073\n",
      "Training loss for batch 901 : 0.40774938464164734\n",
      "Training loss for batch 902 : 0.5814931392669678\n",
      "Training loss for batch 903 : 0.17114222049713135\n",
      "Training loss for batch 904 : 0.0996769592165947\n",
      "Training loss for batch 905 : 0.1177225336432457\n",
      "Training loss for batch 906 : 0.1967093050479889\n",
      "Training loss for batch 907 : 0.4191911518573761\n",
      "Training loss for batch 908 : 0.1408429890871048\n",
      "Training loss for batch 909 : 0.2527342140674591\n",
      "Training loss for batch 910 : 0.13626037538051605\n",
      "Training loss for batch 911 : 0.11315044015645981\n",
      "Training loss for batch 912 : 0.1731591671705246\n",
      "Training loss for batch 913 : 0.2473931461572647\n",
      "Training loss for batch 914 : 0.12336792051792145\n",
      "Training loss for batch 915 : 0.01087128184735775\n",
      "Training loss for batch 916 : 0.23871806263923645\n",
      "Training loss for batch 917 : 0.05524265766143799\n",
      "Training loss for batch 918 : 0.2772611975669861\n",
      "Training loss for batch 919 : 0.16125506162643433\n",
      "Training loss for batch 920 : 0.18321505188941956\n",
      "Training loss for batch 921 : 0.08261337131261826\n",
      "Training loss for batch 922 : 0.14240196347236633\n",
      "Training loss for batch 923 : 0.23597726225852966\n",
      "Training loss for batch 924 : 0.09111060947179794\n",
      "Training loss for batch 925 : 0.0002660002501215786\n",
      "Training loss for batch 926 : 0.08439598232507706\n",
      "Training loss for batch 927 : 0.2536388039588928\n",
      "Training loss for batch 928 : 0.14973530173301697\n",
      "Training loss for batch 929 : 0.3018032908439636\n",
      "Training loss for batch 930 : 0.014294427819550037\n",
      "Training loss for batch 931 : 0.3265223801136017\n",
      "Training loss for batch 932 : 0.2630859315395355\n",
      "Training loss for batch 933 : 0.02522505447268486\n",
      "Training loss for batch 934 : 0.15458370745182037\n",
      "Training loss for batch 935 : 0.0968414917588234\n",
      "Training loss for batch 936 : 0.29444313049316406\n",
      "Training loss for batch 937 : 0.04970476031303406\n",
      "Training loss for batch 938 : 0.2135719656944275\n",
      "Training loss for batch 939 : 0.14540235698223114\n",
      "Training loss for batch 940 : 0.14743800461292267\n",
      "Training loss for batch 941 : 0.49136608839035034\n",
      "Training loss for batch 942 : 0.25559866428375244\n",
      "Training loss for batch 943 : 0.0694928988814354\n",
      "Training loss for batch 944 : 0.3058270215988159\n",
      "Training loss for batch 945 : 0.01665673963725567\n",
      "Training loss for batch 946 : 0.15529583394527435\n",
      "Training loss for batch 947 : 0.3172803223133087\n",
      "Training loss for batch 948 : 0.1694096177816391\n",
      "Training loss for batch 949 : 0.03681928291916847\n",
      "Training loss for batch 950 : 0.2260151356458664\n",
      "Training loss for batch 951 : 0.41220325231552124\n",
      "Training loss for batch 952 : 0.28506967425346375\n",
      "Training loss for batch 953 : 0.22706809639930725\n",
      "Training loss for batch 954 : 0.12294726818799973\n",
      "Training loss for batch 955 : 0.18890316784381866\n",
      "Training loss for batch 956 : 0.28798574209213257\n",
      "Training loss for batch 957 : 0.4383046329021454\n",
      "Training loss for batch 958 : 0.3143889307975769\n",
      "Training loss for batch 959 : 0.29530587792396545\n",
      "Training loss for batch 960 : 0.1687263399362564\n",
      "Training loss for batch 961 : 0.3143329918384552\n",
      "Training loss for batch 962 : 0.2534293830394745\n",
      "Training loss for batch 963 : 0.38006582856178284\n",
      "Training loss for batch 964 : 0.22646638751029968\n",
      "Training loss for batch 965 : 0.24890650808811188\n",
      "Training loss for batch 966 : 0.2852391004562378\n",
      "Training loss for batch 967 : 0.12229514122009277\n",
      "Training loss for batch 968 : 0.10357575118541718\n",
      "Training loss for batch 969 : 0.20244790613651276\n",
      "Training loss for batch 970 : 0.007008721586316824\n",
      "Training loss for batch 971 : 0.32498621940612793\n",
      "Training loss for batch 972 : 0.06973773241043091\n",
      "Training loss for batch 973 : 0.25842076539993286\n",
      "Training loss for batch 974 : 0.37655213475227356\n",
      "Training loss for batch 975 : 0.22823597490787506\n",
      "Training loss for batch 976 : 0.19171346724033356\n",
      "Training loss for batch 977 : 0.3672679662704468\n",
      "Training loss for batch 978 : 0.3443315625190735\n",
      "Training loss for batch 979 : 0.05341103672981262\n",
      "Training loss for batch 980 : 0.25970470905303955\n",
      "Training loss for batch 981 : 0.30397775769233704\n",
      "Training loss for batch 982 : 0.10710962116718292\n",
      "Training loss for batch 983 : 0.2790490686893463\n",
      "Training loss for batch 984 : 0.0911087766289711\n",
      "Training loss for batch 985 : 0.027503371238708496\n",
      "Training loss for batch 986 : 0.008350729942321777\n",
      "Training loss for batch 987 : 0.2912179231643677\n",
      "Training loss for batch 988 : 0.2694356143474579\n",
      "Training loss for batch 989 : 0.26891934871673584\n",
      "Training loss for batch 990 : 0.07031671702861786\n",
      "Training loss for batch 991 : 0.14133770763874054\n",
      "Training loss for batch 992 : 0.30237463116645813\n",
      "Training loss for batch 993 : 0.04375956952571869\n",
      "Training loss for batch 994 : 0.42200183868408203\n",
      "Training loss for batch 995 : 0.32302406430244446\n",
      "Training loss for batch 996 : 0.13858704268932343\n",
      "Training loss for batch 997 : 0.2417449653148651\n",
      "Training loss for batch 998 : 0.04920843616127968\n",
      "Training loss for batch 999 : 0.12487415969371796\n",
      "Training loss for batch 1000 : 0.26155588030815125\n",
      "Training loss for batch 1001 : 0.04017917811870575\n",
      "Training loss for batch 1002 : 0.5045709609985352\n",
      "Training loss for batch 1003 : 0.2551351487636566\n",
      "Training loss for batch 1004 : 0.21638771891593933\n",
      "Training loss for batch 1005 : 0.13884446024894714\n",
      "Training loss for batch 1006 : 0.058715738356113434\n",
      "Training loss for batch 1007 : 0.1606283038854599\n",
      "Training loss for batch 1008 : 0.1099761426448822\n",
      "Training loss for batch 1009 : 0.4042986333370209\n",
      "Training loss for batch 1010 : 0.10057336091995239\n",
      "Training loss for batch 1011 : 0.41684266924858093\n",
      "Training loss for batch 1012 : 0.16275861859321594\n",
      "Training loss for batch 1013 : 0.05752930790185928\n",
      "Training loss for batch 1014 : 0.2055864930152893\n",
      "Training loss for batch 1015 : 0.37850162386894226\n",
      "Training loss for batch 1016 : 0.22583481669425964\n",
      "Training loss for batch 1017 : 0.08750882744789124\n",
      "Training loss for batch 1018 : 0.07065074890851974\n",
      "Training loss for batch 1019 : 0.15565036237239838\n",
      "Training loss for batch 1020 : 0.07462023943662643\n",
      "Training loss for batch 1021 : 0.3334328234195709\n",
      "Training loss for batch 1022 : 0.0977129191160202\n",
      "Training loss for batch 1023 : 0.4090932607650757\n",
      "Training loss for batch 1024 : 0.04201195761561394\n",
      "Training loss for batch 1025 : 0.12891848385334015\n",
      "Training loss for batch 1026 : 0.263971745967865\n",
      "Training loss for batch 1027 : 0.03208469599485397\n",
      "Training loss for batch 1028 : 0.13014982640743256\n",
      "Training loss for batch 1029 : 0.09239887446165085\n",
      "Training loss for batch 1030 : 0.3092457354068756\n",
      "Training loss for batch 1031 : 0.11991960555315018\n",
      "Training loss for batch 1032 : 0.1153692826628685\n",
      "Training loss for batch 1033 : 0.02428339049220085\n",
      "Training loss for batch 1034 : 0.22469566762447357\n",
      "Training loss for batch 1035 : 0.3118135333061218\n",
      "Training loss for batch 1036 : 0.0909000113606453\n",
      "Training loss for batch 1037 : 0.05600522458553314\n",
      "Training loss for batch 1038 : 0.10807090252637863\n",
      "Training loss for batch 1039 : 0.04178696870803833\n",
      "Training loss for batch 1040 : 0.33365511894226074\n",
      "Training loss for batch 1041 : 0.18191391229629517\n",
      "Training loss for batch 1042 : 0.19592179358005524\n",
      "Training loss for batch 1043 : 0.13828133046627045\n",
      "Training loss for batch 1044 : 0.02462904527783394\n",
      "Training loss for batch 1045 : 0.49197277426719666\n",
      "Training loss for batch 1046 : 0.09911349415779114\n",
      "Training loss for batch 1047 : 0.1393405646085739\n",
      "Training loss for batch 1048 : 0.12811824679374695\n",
      "Training loss for batch 1049 : 0.4988284111022949\n",
      "Training loss for batch 1050 : 0.10812757164239883\n",
      "Training loss for batch 1051 : 0.14334553480148315\n",
      "Training loss for batch 1052 : 0.27224817872047424\n",
      "Training loss for batch 1053 : 0.40215715765953064\n",
      "Training loss for batch 1054 : 0.04569612070918083\n",
      "Training loss for batch 1055 : 0.06405608355998993\n",
      "Training loss for batch 1056 : 0.045878373086452484\n",
      "Training loss for batch 1057 : 0.17985151708126068\n",
      "Training loss for batch 1058 : 0.14578889310359955\n",
      "Training loss for batch 1059 : 0.28515487909317017\n",
      "Training loss for batch 1060 : 0.12691114842891693\n",
      "Training loss for batch 1061 : 0.7658582925796509\n",
      "Training loss for batch 1062 : 0.30122190713882446\n",
      "Training loss for batch 1063 : 0.35355642437934875\n",
      "Training loss for batch 1064 : 0.31616947054862976\n",
      "Training loss for batch 1065 : 0.3829617500305176\n",
      "Training loss for batch 1066 : 0.08303875476121902\n",
      "Training loss for batch 1067 : 0.052835769951343536\n",
      "Training loss for batch 1068 : 0.16199514269828796\n",
      "Training loss for batch 1069 : 0.2161048948764801\n",
      "Training loss for batch 1070 : 0.24771781265735626\n",
      "Training loss for batch 1071 : 0.11956819146871567\n",
      "Training loss for batch 1072 : 0.1642819494009018\n",
      "Training loss for batch 1073 : 0.5877375602722168\n",
      "Training loss for batch 1074 : 0.31945687532424927\n",
      "Training loss for batch 1075 : 0.32679352164268494\n",
      "Training loss for batch 1076 : 0.1154843121767044\n",
      "Training loss for batch 1077 : 0.05156933516263962\n",
      "Training loss for batch 1078 : 0.31036117672920227\n",
      "Training loss for batch 1079 : 0.07760029286146164\n",
      "Training loss for batch 1080 : 0.11199009418487549\n",
      "Training loss for batch 1081 : 0.1500994861125946\n",
      "Training loss for batch 1082 : 0.37870147824287415\n",
      "Training loss for batch 1083 : 0.26490309834480286\n",
      "Training loss for batch 1084 : 0.3687558174133301\n",
      "Training loss for batch 1085 : 0.1607995629310608\n",
      "Training loss for batch 1086 : 0.36913180351257324\n",
      "Training loss for batch 1087 : 0.4354203939437866\n",
      "Training loss for batch 1088 : 0.24406640231609344\n",
      "Training loss for batch 1089 : 0.14269644021987915\n",
      "Training loss for batch 1090 : 0.007525046821683645\n",
      "Training loss for batch 1091 : 0.08224846422672272\n",
      "Training loss for batch 1092 : 0.043461885303258896\n",
      "Training loss for batch 1093 : 0.3080458641052246\n",
      "Training loss for batch 1094 : 0.11569870263338089\n",
      "Training loss for batch 1095 : 0.19146646559238434\n",
      "Training loss for batch 1096 : 0.2723575234413147\n",
      "Training loss for batch 1097 : 0.2918053865432739\n",
      "Training loss for batch 1098 : 0.2437947392463684\n",
      "Training loss for batch 1099 : 0.30219367146492004\n",
      "Training loss for batch 1100 : 0.2699068486690521\n",
      "Training loss for batch 1101 : 0.028893083333969116\n",
      "Training loss for batch 1102 : 0.07670924067497253\n",
      "Training loss for batch 1103 : 0.13353055715560913\n",
      "Training loss for batch 1104 : 0.38231778144836426\n",
      "Training loss for batch 1105 : 0.019742120057344437\n",
      "Training loss for batch 1106 : 0.0\n",
      "Training loss for batch 1107 : 0.45405110716819763\n",
      "Training loss for batch 1108 : 0.34914880990982056\n",
      "Training loss for batch 1109 : 0.12034016847610474\n",
      "Training loss for batch 1110 : 0.2536856532096863\n",
      "Training loss for batch 1111 : 0.05997414514422417\n",
      "Training loss for batch 1112 : 0.003123680828139186\n",
      "Training loss for batch 1113 : 0.1611832082271576\n",
      "Training loss for batch 1114 : 0.210857093334198\n",
      "Training loss for batch 1115 : 0.14260269701480865\n",
      "Training loss for batch 1116 : 0.002303140936419368\n",
      "Training loss for batch 1117 : 0.2442914992570877\n",
      "Training loss for batch 1118 : 0.04552096128463745\n",
      "Training loss for batch 1119 : 0.23661014437675476\n",
      "Training loss for batch 1120 : 0.44626426696777344\n",
      "Training loss for batch 1121 : 0.10881980508565903\n",
      "Training loss for batch 1122 : 0.037622272968292236\n",
      "Training loss for batch 1123 : 0.3256562054157257\n",
      "Training loss for batch 1124 : 0.3670945167541504\n",
      "Training loss for batch 1125 : 0.03607378900051117\n",
      "Training loss for batch 1126 : 0.07236377149820328\n",
      "Training loss for batch 1127 : 0.10500822961330414\n",
      "Training loss for batch 1128 : 0.2576960623264313\n",
      "Training loss for batch 1129 : 0.21399599313735962\n",
      "Training loss for batch 1130 : 0.16268625855445862\n",
      "Training loss for batch 1131 : 0.4008256196975708\n",
      "Training loss for batch 1132 : 0.002990830922499299\n",
      "Training loss for batch 1133 : 0.09175282716751099\n",
      "Training loss for batch 1134 : 0.29283949732780457\n",
      "Training loss for batch 1135 : 0.3456110954284668\n",
      "Training loss for batch 1136 : 0.078148253262043\n",
      "Training loss for batch 1137 : 0.18123920261859894\n",
      "Training loss for batch 1138 : 0.165090411901474\n",
      "Training loss for batch 1139 : 0.3246077001094818\n",
      "Training loss for batch 1140 : 0.06752310693264008\n",
      "Training loss for batch 1141 : 0.08016739785671234\n",
      "Training loss for batch 1142 : 0.3031858801841736\n",
      "Training loss for batch 1143 : 0.40998268127441406\n",
      "Training loss for batch 1144 : 0.09966478496789932\n",
      "Training loss for batch 1145 : 0.03688760846853256\n",
      "Training loss for batch 1146 : 0.19464582204818726\n",
      "Training loss for batch 1147 : 0.24463512003421783\n",
      "Training loss for batch 1148 : 0.30468273162841797\n",
      "Training loss for batch 1149 : 0.24894139170646667\n",
      "Training loss for batch 1150 : 0.40204036235809326\n",
      "Training loss for batch 1151 : 0.055599041283130646\n",
      "Training loss for batch 1152 : 0.15650348365306854\n",
      "Training loss for batch 1153 : 0.040192198008298874\n",
      "Training loss for batch 1154 : 0.2577539384365082\n",
      "Training loss for batch 1155 : 0.6841457486152649\n",
      "Training loss for batch 1156 : 0.7640177011489868\n",
      "Training loss for batch 1157 : 0.08643568307161331\n",
      "Training loss for batch 1158 : 0.3076518177986145\n",
      "Training loss for batch 1159 : 0.6166520118713379\n",
      "Training loss for batch 1160 : 0.3462105989456177\n",
      "Training loss for batch 1161 : 0.0926656648516655\n",
      "Training loss for batch 1162 : 0.11814313381910324\n",
      "Training loss for batch 1163 : 0.04826595261693001\n",
      "Training loss for batch 1164 : 0.3552027642726898\n",
      "Training loss for batch 1165 : 0.04422390088438988\n",
      "Training loss for batch 1166 : 0.34136614203453064\n",
      "Training loss for batch 1167 : 0.13083110749721527\n",
      "Training loss for batch 1168 : 0.515483558177948\n",
      "Training loss for batch 1169 : 0.08572316914796829\n",
      "Training loss for batch 1170 : 0.010694722644984722\n",
      "Training loss for batch 1171 : 0.13923941552639008\n",
      "Training loss for batch 1172 : 0.2652871608734131\n",
      "Training loss for batch 1173 : 0.3449004292488098\n",
      "Training loss for batch 1174 : 0.02640005759894848\n",
      "Training loss for batch 1175 : 0.31575167179107666\n",
      "Training loss for batch 1176 : 0.24522313475608826\n",
      "Training loss for batch 1177 : 0.12656620144844055\n",
      "Training loss for batch 1178 : 0.27877137064933777\n",
      "Training loss for batch 1179 : 0.16932816803455353\n",
      "Training loss for batch 1180 : 0.4008128345012665\n",
      "Training loss for batch 1181 : 0.3237593472003937\n",
      "Training loss for batch 1182 : 0.4742589592933655\n",
      "Training loss for batch 1183 : 0.05389533191919327\n",
      "Training loss for batch 1184 : 0.19384516775608063\n",
      "Training loss for batch 1185 : 0.22415991127490997\n",
      "Training loss for batch 1186 : 0.10724730044603348\n",
      "Training loss for batch 1187 : 0.07807797938585281\n",
      "Training loss for batch 1188 : 0.24126479029655457\n",
      "Training loss for batch 1189 : 0.09093531966209412\n",
      "Training loss for batch 1190 : 0.018843909725546837\n",
      "Training loss for batch 1191 : 0.10351505130529404\n",
      "Training loss for batch 1192 : 0.060753338038921356\n",
      "Training loss for batch 1193 : 0.1153050884604454\n",
      "Training loss for batch 1194 : 0.08177968859672546\n",
      "Training loss for batch 1195 : 0.28296801447868347\n",
      "Training loss for batch 1196 : 0.32964223623275757\n",
      "Training loss for batch 1197 : 0.13246266543865204\n",
      "Training loss for batch 1198 : 0.2342979460954666\n",
      "Training loss for batch 1199 : 0.22944329679012299\n",
      "Training loss for batch 1200 : 0.2703913748264313\n",
      "Training loss for batch 1201 : 0.03959093242883682\n",
      "Training loss for batch 1202 : 0.3743211030960083\n",
      "Training loss for batch 1203 : 0.22061672806739807\n",
      "Training loss for batch 1204 : 0.12374720722436905\n",
      "Training loss for batch 1205 : 0.04764886200428009\n",
      "Training loss for batch 1206 : 0.10481952130794525\n",
      "Training loss for batch 1207 : 0.004496091976761818\n",
      "Training loss for batch 1208 : 0.1870725303888321\n",
      "Training loss for batch 1209 : 0.3528973460197449\n",
      "Training loss for batch 1210 : 0.22851069271564484\n",
      "Training loss for batch 1211 : 0.12351113557815552\n",
      "Training loss for batch 1212 : 0.32104218006134033\n",
      "Training loss for batch 1213 : 0.0940081849694252\n",
      "Training loss for batch 1214 : 0.45368093252182007\n",
      "Training loss for batch 1215 : 0.34348219633102417\n",
      "Training loss for batch 1216 : 0.8573563694953918\n",
      "Training loss for batch 1217 : 0.18033307790756226\n",
      "Training loss for batch 1218 : 0.2820849120616913\n",
      "Training loss for batch 1219 : 0.13704650104045868\n",
      "Training loss for batch 1220 : 0.09595617651939392\n",
      "Training loss for batch 1221 : 0.5447389483451843\n",
      "Training loss for batch 1222 : 0.19969356060028076\n",
      "Training loss for batch 1223 : 0.39893651008605957\n",
      "Training loss for batch 1224 : 0.37796175479888916\n",
      "Training loss for batch 1225 : 0.3330412805080414\n",
      "Training loss for batch 1226 : 0.16005726158618927\n",
      "Training loss for batch 1227 : 0.08816088736057281\n",
      "Training loss for batch 1228 : 0.5368149280548096\n",
      "Training loss for batch 1229 : 0.3738632798194885\n",
      "Training loss for batch 1230 : 0.1030845046043396\n",
      "Training loss for batch 1231 : 0.006377577781677246\n",
      "Training loss for batch 1232 : 0.21804179251194\n",
      "Training loss for batch 1233 : 0.07500352710485458\n",
      "Training loss for batch 1234 : 0.3130943179130554\n",
      "Training loss for batch 1235 : 0.23401109874248505\n",
      "Training loss for batch 1236 : 0.2759343385696411\n",
      "Training loss for batch 1237 : 0.11629960685968399\n",
      "Training loss for batch 1238 : 0.12329608201980591\n",
      "Training loss for batch 1239 : 0.16469159722328186\n",
      "Training loss for batch 1240 : 0.06385014206171036\n",
      "Training loss for batch 1241 : 0.08390158414840698\n",
      "Training loss for batch 1242 : 0.020875951275229454\n",
      "Training loss for batch 1243 : 0.30700600147247314\n",
      "Training loss for batch 1244 : 0.1774381399154663\n",
      "Training loss for batch 1245 : 0.0038570198230445385\n",
      "Training loss for batch 1246 : 0.35889822244644165\n",
      "Training loss for batch 1247 : 0.18078257143497467\n",
      "Training loss for batch 1248 : 1.172500491142273\n",
      "Training loss for batch 1249 : 0.07847939431667328\n",
      "Training loss for batch 1250 : 0.04937796667218208\n",
      "Training loss for batch 1251 : 0.14023558795452118\n",
      "Training loss for batch 1252 : 0.057083867490291595\n",
      "Training loss for batch 1253 : 0.2942241430282593\n",
      "Training loss for batch 1254 : 0.08153850585222244\n",
      "Training loss for batch 1255 : 0.09218139201402664\n",
      "Training loss for batch 1256 : 0.17693184316158295\n",
      "Training loss for batch 1257 : 0.11526107043027878\n",
      "Training loss for batch 1258 : 0.24667374789714813\n",
      "Training loss for batch 1259 : 0.3889043927192688\n",
      "Training loss for batch 1260 : 0.5057539939880371\n",
      "Training loss for batch 1261 : 0.22571246325969696\n",
      "Training loss for batch 1262 : 0.1371411383152008\n",
      "Training loss for batch 1263 : 0.3431669771671295\n",
      "Training loss for batch 1264 : 0.16280511021614075\n",
      "Training loss for batch 1265 : 0.06098886579275131\n",
      "Training loss for batch 1266 : 0.42061924934387207\n",
      "Training loss for batch 1267 : 0.12943728268146515\n",
      "Training loss for batch 1268 : 0.15113292634487152\n",
      "Training loss for batch 1269 : 0.6546474099159241\n",
      "Training loss for batch 1270 : 0.2473537027835846\n",
      "Training loss for batch 1271 : 0.31513696908950806\n",
      "Training loss for batch 1272 : 0.13597704470157623\n",
      "Training loss for batch 1273 : 0.12796682119369507\n",
      "Training loss for batch 1274 : 0.04916134104132652\n",
      "Training loss for batch 1275 : 0.11568494141101837\n",
      "Training loss for batch 1276 : 0.3930944800376892\n",
      "Training loss for batch 1277 : 0.21379508078098297\n",
      "Training loss for batch 1278 : 0.04701384902000427\n",
      "Training loss for batch 1279 : 0.13495415449142456\n",
      "Training loss for batch 1280 : 0.2557580769062042\n",
      "Training loss for batch 1281 : 0.05056070536375046\n",
      "Training loss for batch 1282 : 0.057069726288318634\n",
      "Training loss for batch 1283 : 0.2561687231063843\n",
      "Training loss for batch 1284 : 0.14780716598033905\n",
      "Training loss for batch 1285 : 0.3439733684062958\n",
      "Training loss for batch 1286 : 0.3517642915248871\n",
      "Training loss for batch 1287 : 0.1381922960281372\n",
      "Training loss for batch 1288 : 0.09753400832414627\n",
      "Training loss for batch 1289 : 0.010510709136724472\n",
      "Training loss for batch 1290 : 0.28044864535331726\n",
      "Training loss for batch 1291 : 0.13893672823905945\n",
      "Training loss for batch 1292 : 0.6515347957611084\n",
      "Training loss for batch 1293 : 0.2233610451221466\n",
      "Training loss for batch 1294 : 0.0022871920373290777\n",
      "Training loss for batch 1295 : 0.21328336000442505\n",
      "Training loss for batch 1296 : 0.08117693662643433\n",
      "Training loss for batch 1297 : 0.19728980958461761\n",
      "Training loss for batch 1298 : 0.2103090137243271\n",
      "Training loss for batch 1299 : 0.4508780241012573\n",
      "Training loss for batch 1300 : 0.33424270153045654\n",
      "Training loss for batch 1301 : 0.1907322108745575\n",
      "Training loss for batch 1302 : 0.34265151619911194\n",
      "Training loss for batch 1303 : 0.1349223107099533\n",
      "Training loss for batch 1304 : 0.09365838766098022\n",
      "Training loss for batch 1305 : 0.2892017960548401\n",
      "Training loss for batch 1306 : 0.19877852499485016\n",
      "Training loss for batch 1307 : 0.13116198778152466\n",
      "Training loss for batch 1308 : 0.2353249490261078\n",
      "Training loss for batch 1309 : 0.19984328746795654\n",
      "Training loss for batch 1310 : 0.11773200333118439\n",
      "Training loss for batch 1311 : 0.2923926115036011\n",
      "Training loss for batch 1312 : 0.39498499035835266\n",
      "Training loss for batch 1313 : 0.2847920358181\n",
      "Training loss for batch 1314 : 0.39556604623794556\n",
      "Training loss for batch 1315 : 0.11391747742891312\n",
      "Training loss for batch 1316 : 0.06751498579978943\n",
      "Training loss for batch 1317 : 0.45105990767478943\n",
      "Training loss for batch 1318 : 0.13561922311782837\n",
      "Training loss for batch 1319 : 0.27131563425064087\n",
      "Training loss for batch 1320 : 0.6883483529090881\n",
      "Training loss for batch 1321 : 0.33141764998435974\n",
      "Training loss for batch 1322 : 0.3250178098678589\n",
      "Training loss for batch 1323 : 0.26888832449913025\n",
      "Training loss for batch 1324 : 0.44336557388305664\n",
      "Training loss for batch 1325 : 0.12351877987384796\n",
      "Training loss for batch 1326 : 0.40200546383857727\n",
      "Training loss for batch 1327 : 0.34707924723625183\n",
      "Training loss for batch 1328 : 0.6270343065261841\n",
      "Training loss for batch 1329 : 0.35808905959129333\n",
      "Training loss for batch 1330 : 0.28923752903938293\n",
      "Training loss for batch 1331 : 0.3027285635471344\n",
      "Training loss for batch 1332 : 0.2672089636325836\n",
      "Training loss for batch 1333 : 0.10101904720067978\n",
      "Training loss for batch 1334 : 0.1181277334690094\n",
      "Training loss for batch 1335 : 0.2459815889596939\n",
      "Training loss for batch 1336 : 0.1757546216249466\n",
      "Training loss for batch 1337 : 0.05023839324712753\n",
      "Training loss for batch 1338 : 0.015546649694442749\n",
      "Training loss for batch 1339 : 0.3272155821323395\n",
      "Training loss for batch 1340 : 0.21278363466262817\n",
      "Training loss for batch 1341 : 0.2090403139591217\n",
      "Training loss for batch 1342 : 0.17251239717006683\n",
      "Training loss for batch 1343 : 0.002459065057337284\n",
      "Training loss for batch 1344 : 0.28920361399650574\n",
      "Training loss for batch 1345 : 0.14101921021938324\n",
      "Training loss for batch 1346 : 0.15497365593910217\n",
      "Training loss for batch 1347 : 0.03431868553161621\n",
      "Training loss for batch 1348 : 0.47924938797950745\n",
      "Training loss for batch 1349 : 0.11798465251922607\n",
      "Training loss for batch 1350 : 0.20825818181037903\n",
      "Training loss for batch 1351 : 0.13237136602401733\n",
      "Training loss for batch 1352 : 0.0957435667514801\n",
      "Training loss for batch 1353 : 0.28429991006851196\n",
      "Training loss for batch 1354 : 0.14719127118587494\n",
      "Training loss for batch 1355 : 0.35695427656173706\n",
      "Training loss for batch 1356 : 0.33627066016197205\n",
      "Training loss for batch 1357 : 0.26288753747940063\n",
      "Training loss for batch 1358 : 0.2912149131298065\n",
      "Training loss for batch 1359 : 0.23792296648025513\n",
      "Training loss for batch 1360 : 0.27950677275657654\n",
      "Training loss for batch 1361 : 0.19422568380832672\n",
      "Training loss for batch 1362 : 0.48240989446640015\n",
      "Training loss for batch 1363 : 0.14229252934455872\n",
      "Training loss for batch 1364 : 0.18125933408737183\n",
      "Training loss for batch 1365 : 0.13488011062145233\n",
      "Training loss for batch 1366 : 0.12897418439388275\n",
      "Training loss for batch 1367 : 0.011610034853219986\n",
      "Training loss for batch 1368 : 0.2703733742237091\n",
      "Training loss for batch 1369 : 0.387505441904068\n",
      "Training loss for batch 1370 : 0.10830999165773392\n",
      "Training loss for batch 1371 : 0.17153511941432953\n",
      "Training loss for batch 1372 : 0.1546434760093689\n",
      "Training loss for batch 1373 : 0.24442873895168304\n",
      "Training loss for batch 1374 : 0.10286997258663177\n",
      "Training loss for batch 1375 : 0.2509036064147949\n",
      "Training loss for batch 1376 : 0.08461792021989822\n",
      "Training loss for batch 1377 : 0.2917380630970001\n",
      "Training loss for batch 1378 : 0.05075877904891968\n",
      "Training loss for batch 1379 : 0.2701716125011444\n",
      "Training loss for batch 1380 : 0.2634735107421875\n",
      "Training loss for batch 1381 : 0.26684144139289856\n",
      "Training loss for batch 1382 : 0.4425874948501587\n",
      "Training loss for batch 1383 : 0.265299916267395\n",
      "Training loss for batch 1384 : 0.30302804708480835\n",
      "Training loss for batch 1385 : 0.08442053198814392\n",
      "Training loss for batch 1386 : 0.1818169802427292\n",
      "Training loss for batch 1387 : 0.15564841032028198\n",
      "Training loss for batch 1388 : 0.45217955112457275\n",
      "Training loss for batch 1389 : 0.0\n",
      "Training loss for batch 1390 : 0.30515968799591064\n",
      "Training loss for batch 1391 : 0.18305200338363647\n",
      "Training loss for batch 1392 : 0.15131784975528717\n",
      "Training loss for batch 1393 : 0.3000674247741699\n",
      "Training loss for batch 1394 : 0.07343635708093643\n",
      "Training loss for batch 1395 : 0.22370392084121704\n",
      "Training loss for batch 1396 : 0.028364717960357666\n",
      "Training loss for batch 1397 : 0.32355546951293945\n",
      "Training loss for batch 1398 : 0.163504958152771\n",
      "Training loss for batch 1399 : 0.2636367082595825\n",
      "Training loss for batch 1400 : 0.28005847334861755\n",
      "Training loss for batch 1401 : 0.33664557337760925\n",
      "Training loss for batch 1402 : 0.2906471788883209\n",
      "Training loss for batch 1403 : 0.18803995847702026\n",
      "Training loss for batch 1404 : 0.32305386662483215\n",
      "Training loss for batch 1405 : 0.19958177208900452\n",
      "Training loss for batch 1406 : 0.07131623476743698\n",
      "Training loss for batch 1407 : 0.23802976310253143\n",
      "Training loss for batch 1408 : 0.340747594833374\n",
      "Training loss for batch 1409 : 0.4259107708930969\n",
      "Training loss for batch 1410 : 0.04470721259713173\n",
      "Training loss for batch 1411 : 0.05948418751358986\n",
      "Training loss for batch 1412 : 0.5109115839004517\n",
      "Training loss for batch 1413 : 0.3366504907608032\n",
      "Training loss for batch 1414 : 0.21328206360340118\n",
      "Training loss for batch 1415 : 0.18795859813690186\n",
      "Training loss for batch 1416 : 0.0803925096988678\n",
      "Training loss for batch 1417 : 0.2062932848930359\n",
      "Training loss for batch 1418 : 0.27033352851867676\n",
      "Training loss for batch 1419 : 0.03600747138261795\n",
      "Training loss for batch 1420 : 0.1482406109571457\n",
      "Training loss for batch 1421 : 0.07038602232933044\n",
      "Training loss for batch 1422 : 0.5487884283065796\n",
      "Training loss for batch 1423 : 0.20829518139362335\n",
      "Training loss for batch 1424 : 0.3693509101867676\n",
      "Training loss for batch 1425 : 0.5276824831962585\n",
      "Training loss for batch 1426 : 0.2361137866973877\n",
      "Training loss for batch 1427 : 0.2842491567134857\n",
      "Training loss for batch 1428 : 0.07221276313066483\n",
      "Training loss for batch 1429 : 0.2877776622772217\n",
      "Training loss for batch 1430 : 0.14139515161514282\n",
      "Training loss for batch 1431 : 0.03523850440979004\n",
      "Training loss for batch 1432 : 0.12252642959356308\n",
      "Training loss for batch 1433 : 0.03886619582772255\n",
      "Training loss for batch 1434 : 0.0454770028591156\n",
      "Training loss for batch 1435 : 0.05691525340080261\n",
      "Training loss for batch 1436 : 0.2657858431339264\n",
      "Training loss for batch 1437 : 0.4528316855430603\n",
      "Training loss for batch 1438 : 0.4487655758857727\n",
      "Training loss for batch 1439 : 0.2758246064186096\n",
      "Training loss for batch 1440 : 0.23140184581279755\n",
      "Training loss for batch 1441 : 0.6053169369697571\n",
      "Training loss for batch 1442 : 0.03693345934152603\n",
      "Training loss for batch 1443 : 0.3534495532512665\n",
      "Training loss for batch 1444 : 0.1970384269952774\n",
      "Training loss for batch 1445 : 0.5727366805076599\n",
      "Training loss for batch 1446 : 0.051932528614997864\n",
      "Training loss for batch 1447 : 0.385525643825531\n",
      "Training loss for batch 1448 : 0.14487825334072113\n",
      "Training loss for batch 1449 : 0.7053282856941223\n",
      "Training loss for batch 1450 : 0.3894062936306\n",
      "Training loss for batch 1451 : 0.07697567343711853\n",
      "Training loss for batch 1452 : 0.13403135538101196\n",
      "Training loss for batch 1453 : 0.22303375601768494\n",
      "Training loss for batch 1454 : 0.29325181245803833\n",
      "Training loss for batch 1455 : 0.22780190408229828\n",
      "Training loss for batch 1456 : 0.02791752479970455\n",
      "Training loss for batch 1457 : 0.35003936290740967\n",
      "Training loss for batch 1458 : 0.24399706721305847\n",
      "Training loss for batch 1459 : 0.006433825008571148\n",
      "Training loss for batch 1460 : 0.17489752173423767\n",
      "Training loss for batch 1461 : 0.11216051131486893\n",
      "Training loss for batch 1462 : 0.06992758065462112\n",
      "Training loss for batch 1463 : 0.19485488533973694\n",
      "Training loss for batch 1464 : 0.03836105763912201\n",
      "Training loss for batch 1465 : 0.04358229413628578\n",
      "Training loss for batch 1466 : 0.04377424716949463\n",
      "Training loss for batch 1467 : 0.13359901309013367\n",
      "Training loss for batch 1468 : 0.6451455950737\n",
      "Training loss for batch 1469 : 0.21307241916656494\n",
      "Training loss for batch 1470 : 0.298659086227417\n",
      "Training loss for batch 1471 : 0.09124638140201569\n",
      "Training loss for batch 1472 : 0.40661460161209106\n",
      "Training loss for batch 1473 : 0.34140336513519287\n",
      "Training loss for batch 1474 : 0.5053905248641968\n",
      "Training loss for batch 1475 : 0.14337722957134247\n",
      "Training loss for batch 1476 : 0.20301780104637146\n",
      "Training loss for batch 1477 : 0.5302802324295044\n",
      "Training loss for batch 1478 : 0.3636927604675293\n",
      "Training loss for batch 1479 : 0.25045883655548096\n",
      "Training loss for batch 1480 : 0.09461036324501038\n",
      "Training loss for batch 1481 : 0.07034720480442047\n",
      "Training loss for batch 1482 : 0.01653566025197506\n",
      "Training loss for batch 1483 : 0.1507679671049118\n",
      "Training loss for batch 1484 : 0.178813636302948\n",
      "Training loss for batch 1485 : 0.00017764634685590863\n",
      "Training loss for batch 1486 : 0.40390869975090027\n",
      "Training loss for batch 1487 : 0.4062833786010742\n",
      "Training loss for batch 1488 : 0.10916285216808319\n",
      "Training loss for batch 1489 : 0.4250212013721466\n",
      "Training loss for batch 1490 : 0.0741189643740654\n",
      "Training loss for batch 1491 : 0.0\n",
      "Training loss for batch 1492 : 0.5119077563285828\n",
      "Training loss for batch 1493 : 0.1007428914308548\n",
      "Training loss for batch 1494 : 0.13704194128513336\n",
      "Training loss for batch 1495 : 0.2607608735561371\n",
      "Training loss for batch 1496 : 0.060134075582027435\n",
      "Training loss for batch 1497 : 0.022902946919202805\n",
      "Training loss for batch 1498 : 0.016839731484651566\n",
      "Training loss for batch 1499 : 0.421541303396225\n",
      "Training loss for batch 1500 : 0.1552508920431137\n",
      "Training loss for batch 1501 : 0.13967853784561157\n",
      "Training loss for batch 1502 : 0.25202006101608276\n",
      "Training loss for batch 1503 : 0.2098984718322754\n",
      "Training loss for batch 1504 : 0.2261243760585785\n",
      "Training loss for batch 1505 : 0.13955943286418915\n",
      "Training loss for batch 1506 : 0.11051002144813538\n",
      "Training loss for batch 1507 : 0.12356004863977432\n",
      "Training loss for batch 1508 : 0.18033528327941895\n",
      "Training loss for batch 1509 : 0.2944459021091461\n",
      "Training loss for batch 1510 : 0.2654331922531128\n",
      "Training loss for batch 1511 : 0.26551222801208496\n",
      "Training loss for batch 1512 : 0.09854162484407425\n",
      "Training loss for batch 1513 : 0.31712961196899414\n",
      "Training loss for batch 1514 : 0.011984752491116524\n",
      "Training loss for batch 1515 : 0.5332766771316528\n",
      "Training loss for batch 1516 : 0.3822167217731476\n",
      "Training loss for batch 1517 : 0.10839993506669998\n",
      "Training loss for batch 1518 : 0.10229886323213577\n",
      "Training loss for batch 1519 : 0.14171646535396576\n",
      "Training loss for batch 1520 : 0.38605186343193054\n",
      "Training loss for batch 1521 : 0.17663288116455078\n",
      "Training loss for batch 1522 : 0.0\n",
      "Training loss for batch 1523 : 0.07769167423248291\n",
      "Training loss for batch 1524 : 0.08206493407487869\n",
      "Training loss for batch 1525 : 0.12801218032836914\n",
      "Training loss for batch 1526 : 0.028913121670484543\n",
      "Training loss for batch 1527 : 0.17606057226657867\n",
      "Training loss for batch 1528 : 0.3689739406108856\n",
      "Training loss for batch 1529 : 0.10182036459445953\n",
      "Training loss for batch 1530 : 0.23089782893657684\n",
      "Training loss for batch 1531 : 0.17163868248462677\n",
      "Training loss for batch 1532 : 0.09224148094654083\n",
      "Training loss for batch 1533 : 0.23337005078792572\n",
      "Training loss for batch 1534 : 0.27659255266189575\n",
      "Training loss for batch 1535 : 0.6486755609512329\n",
      "Training loss for batch 1536 : 0.10369200259447098\n",
      "Training loss for batch 1537 : 0.5962843894958496\n",
      "Training loss for batch 1538 : 0.22278399765491486\n",
      "Training loss for batch 1539 : 0.18661074340343475\n",
      "Training loss for batch 1540 : 0.007784655783325434\n",
      "Training loss for batch 1541 : 0.24941861629486084\n",
      "Training loss for batch 1542 : 0.2522795796394348\n",
      "Training loss for batch 1543 : 0.09306085109710693\n",
      "Training loss for batch 1544 : 0.33852189779281616\n",
      "Training loss for batch 1545 : 0.11728242039680481\n",
      "Training loss for batch 1546 : 0.06249082088470459\n",
      "Training loss for batch 1547 : 0.36009353399276733\n",
      "Training loss for batch 1548 : 0.3386577069759369\n",
      "Training loss for batch 1549 : 0.24698595702648163\n",
      "Training loss for batch 1550 : 0.038117773830890656\n",
      "Training loss for batch 1551 : 0.06699047237634659\n",
      "Training loss for batch 1552 : 0.05166095867753029\n",
      "Training loss for batch 1553 : 0.18407922983169556\n",
      "Training loss for batch 1554 : 0.1534367799758911\n",
      "Training loss for batch 1555 : 0.12998506426811218\n",
      "Training loss for batch 1556 : 0.37893104553222656\n",
      "Training loss for batch 1557 : 0.3195194602012634\n",
      "Training loss for batch 1558 : 0.019919518381357193\n",
      "Training loss for batch 1559 : 0.4166611433029175\n",
      "Training loss for batch 1560 : 0.0\n",
      "Training loss for batch 1561 : 0.06224507838487625\n",
      "Training loss for batch 1562 : 0.0547974668443203\n",
      "Training loss for batch 1563 : 0.186025932431221\n",
      "Training loss for batch 1564 : 0.03005429171025753\n",
      "Training loss for batch 1565 : 0.08568757027387619\n",
      "Training loss for batch 1566 : 0.18418319523334503\n",
      "Training loss for batch 1567 : 0.03403491899371147\n",
      "Training loss for batch 1568 : 0.507825493812561\n",
      "Training loss for batch 1569 : 0.17705532908439636\n",
      "Training loss for batch 1570 : 0.3464447259902954\n",
      "Training loss for batch 1571 : 0.1190522313117981\n",
      "Training loss for batch 1572 : 0.16690658032894135\n",
      "Training loss for batch 1573 : 0.2827672064304352\n",
      "Training loss for batch 1574 : 0.2255801409482956\n",
      "Training loss for batch 1575 : 0.07570922374725342\n",
      "Training loss for batch 1576 : 0.015844419598579407\n",
      "Training loss for batch 1577 : 0.11013442277908325\n",
      "Training loss for batch 1578 : 0.005251636262983084\n",
      "Training loss for batch 1579 : 0.5699825286865234\n",
      "Training loss for batch 1580 : 0.08406995236873627\n",
      "Training loss for batch 1581 : 0.11602479964494705\n",
      "Training loss for batch 1582 : 0.3014964461326599\n",
      "Training loss for batch 1583 : 0.025672825053334236\n",
      "Training loss for batch 1584 : 0.3036266267299652\n",
      "Training loss for batch 1585 : 0.14381058514118195\n",
      "Training loss for batch 1586 : 0.30325156450271606\n",
      "Training loss for batch 1587 : 0.03146542236208916\n",
      "Training loss for batch 1588 : 0.1684991419315338\n",
      "Training loss for batch 1589 : 0.32541701197624207\n",
      "Training loss for batch 1590 : 0.28602030873298645\n",
      "Training loss for batch 1591 : 0.3002166450023651\n",
      "Training loss for batch 1592 : 0.04258153215050697\n",
      "Training loss for batch 1593 : 0.08449531346559525\n",
      "Training loss for batch 1594 : 0.3673055171966553\n",
      "Training loss for batch 1595 : 0.32057541608810425\n",
      "Training loss for batch 1596 : 0.061001792550086975\n",
      "Training loss for batch 1597 : 0.21830685436725616\n",
      "Training loss for batch 1598 : 0.1507640928030014\n",
      "Training loss for batch 1599 : 0.15875717997550964\n",
      "Training loss for batch 1600 : 0.5400275588035583\n",
      "Training loss for batch 1601 : 0.4947894215583801\n",
      "Training loss for batch 1602 : 0.16965997219085693\n",
      "Training loss for batch 1603 : 0.16614580154418945\n",
      "Training loss for batch 1604 : 0.3024650514125824\n",
      "Training loss for batch 1605 : 0.18665781617164612\n",
      "Training loss for batch 1606 : 0.09559543430805206\n",
      "Training loss for batch 1607 : 0.20396745204925537\n",
      "Training loss for batch 1608 : 0.23053519427776337\n",
      "Training loss for batch 1609 : 0.10009762644767761\n",
      "Training loss for batch 1610 : 0.02308083511888981\n",
      "Training loss for batch 1611 : 0.05848580598831177\n",
      "Training loss for batch 1612 : 0.16421477496623993\n",
      "Training loss for batch 1613 : 0.10383904725313187\n",
      "Training loss for batch 1614 : 0.11217354238033295\n",
      "Training loss for batch 1615 : 0.04555626958608627\n",
      "Training loss for batch 1616 : 0.10327485203742981\n",
      "Training loss for batch 1617 : 0.2903154492378235\n",
      "Training loss for batch 1618 : 0.39949968457221985\n",
      "Training loss for batch 1619 : 0.21582770347595215\n",
      "Training loss for batch 1620 : 0.14933525025844574\n",
      "Training loss for batch 1621 : 0.3668207824230194\n",
      "Training loss for batch 1622 : 0.3584442138671875\n",
      "Training loss for batch 1623 : 0.2467796504497528\n",
      "Training loss for batch 1624 : 0.06086765229701996\n",
      "Training loss for batch 1625 : 0.3896380364894867\n",
      "Training loss for batch 1626 : 0.037641871720552444\n",
      "Training loss for batch 1627 : 0.576095461845398\n",
      "Training loss for batch 1628 : 0.20579880475997925\n",
      "Training loss for batch 1629 : 0.08807980269193649\n",
      "Training loss for batch 1630 : 0.021872635930776596\n",
      "Training loss for batch 1631 : 0.08350012451410294\n",
      "Training loss for batch 1632 : 0.4178358316421509\n",
      "Training loss for batch 1633 : 0.3623962998390198\n",
      "Training loss for batch 1634 : 0.3493196666240692\n",
      "Training loss for batch 1635 : 0.0994168370962143\n",
      "Training loss for batch 1636 : 0.5894894003868103\n",
      "Training loss for batch 1637 : 0.1862727850675583\n",
      "Training loss for batch 1638 : 0.420207142829895\n",
      "Training loss for batch 1639 : 0.13293491303920746\n",
      "Training loss for batch 1640 : 0.3594690263271332\n",
      "Training loss for batch 1641 : 0.014848172664642334\n",
      "Training loss for batch 1642 : 0.3091784119606018\n",
      "Training loss for batch 1643 : 0.26627328991889954\n",
      "Training loss for batch 1644 : 0.09932462126016617\n",
      "Training loss for batch 1645 : 0.05553760379552841\n",
      "Training loss for batch 1646 : 0.3846200406551361\n",
      "Training loss for batch 1647 : 0.03558781370520592\n",
      "Training loss for batch 1648 : 0.4024108350276947\n",
      "Training loss for batch 1649 : 0.19981129467487335\n",
      "Training loss for batch 1650 : 0.21019291877746582\n",
      "Training loss for batch 1651 : 0.06851284950971603\n",
      "Training loss for batch 1652 : 0.45728743076324463\n",
      "Training loss for batch 1653 : 0.09506996721029282\n",
      "Training loss for batch 1654 : 0.17600290477275848\n",
      "Training loss for batch 1655 : 0.41598278284072876\n",
      "Training loss for batch 1656 : 0.12786176800727844\n",
      "Training loss for batch 1657 : 0.3380286991596222\n",
      "Training loss for batch 1658 : 0.19779330492019653\n",
      "Training loss for batch 1659 : 0.5049659013748169\n",
      "Training loss for batch 1660 : 0.3554249405860901\n",
      "Training loss for batch 1661 : 0.4239480495452881\n",
      "Training loss for batch 1662 : 0.2849854528903961\n",
      "Training loss for batch 1663 : 0.0683145746588707\n",
      "Training loss for batch 1664 : 0.47463542222976685\n",
      "Training loss for batch 1665 : 0.14990711212158203\n",
      "Training loss for batch 1666 : 0.13999958336353302\n",
      "Training loss for batch 1667 : 0.15459883213043213\n",
      "Training loss for batch 1668 : 0.10645705461502075\n",
      "Training loss for batch 1669 : 0.07824096083641052\n",
      "Training loss for batch 1670 : 0.12036070227622986\n",
      "Training loss for batch 1671 : 0.3517073094844818\n",
      "Training loss for batch 1672 : 0.04481693357229233\n",
      "Training loss for batch 1673 : 0.17226269841194153\n",
      "Training loss for batch 1674 : 0.26172125339508057\n",
      "Training loss for batch 1675 : 0.09612604230642319\n",
      "Training loss for batch 1676 : 0.14979350566864014\n",
      "Training loss for batch 1677 : 0.12931084632873535\n",
      "Training loss for batch 1678 : 0.09969177842140198\n",
      "Training loss for batch 1679 : 0.03621504828333855\n",
      "Training loss for batch 1680 : 0.18682748079299927\n",
      "Training loss for batch 1681 : 0.43766915798187256\n",
      "Training loss for batch 1682 : 0.2555115818977356\n",
      "Training loss for batch 1683 : 0.43267419934272766\n",
      "Training loss for batch 1684 : 0.11191151291131973\n",
      "Training loss for batch 1685 : 0.13956186175346375\n",
      "Training loss for batch 1686 : 0.034394122660160065\n",
      "Training loss for batch 1687 : 0.33357223868370056\n",
      "Training loss for batch 1688 : 0.7033764123916626\n",
      "Training loss for batch 1689 : 0.29326802492141724\n",
      "Training loss for batch 1690 : 0.21311482787132263\n",
      "Training loss for batch 1691 : 0.3384164571762085\n",
      "Training loss for batch 1692 : 0.42974039912223816\n",
      "Training loss for batch 1693 : 0.35218527913093567\n",
      "Training loss for batch 1694 : 0.030201932415366173\n",
      "Training loss for batch 1695 : 0.4445784091949463\n",
      "Training loss for batch 1696 : 0.2881348133087158\n",
      "Training loss for batch 1697 : 0.021636372432112694\n",
      "Training loss for batch 1698 : 0.4170790910720825\n",
      "Training loss for batch 1699 : 0.2257283627986908\n",
      "Training loss for batch 1700 : 0.07798047363758087\n",
      "Training loss for batch 1701 : 0.21660912036895752\n",
      "Training loss for batch 1702 : 0.35282546281814575\n",
      "Training loss for batch 1703 : 0.35879164934158325\n",
      "Training loss for batch 1704 : 0.17410288751125336\n",
      "Training loss for batch 1705 : 0.11220664530992508\n",
      "Training loss for batch 1706 : 0.18427503108978271\n",
      "Training loss for batch 1707 : 0.39833077788352966\n",
      "Training loss for batch 1708 : 0.10067977011203766\n",
      "Training loss for batch 1709 : 0.05827245116233826\n",
      "Training loss for batch 1710 : 0.09215717762708664\n",
      "Training loss for batch 1711 : 0.4032912254333496\n",
      "Training loss for batch 1712 : 0.15593314170837402\n",
      "Training loss for batch 1713 : 0.04884710535407066\n",
      "Training loss for batch 1714 : 0.18825753033161163\n",
      "Training loss for batch 1715 : 0.3137795627117157\n",
      "Training loss for batch 1716 : 0.32035374641418457\n",
      "Training loss for batch 1717 : 0.16155695915222168\n",
      "Training loss for batch 1718 : 0.3461085855960846\n",
      "Training loss for batch 1719 : 0.36722850799560547\n",
      "Training loss for batch 1720 : 0.21876507997512817\n",
      "Training loss for batch 1721 : 0.20527561008930206\n",
      "Training loss for batch 1722 : 0.030264096334576607\n",
      "Training loss for batch 1723 : 0.009891083464026451\n",
      "Training loss for batch 1724 : 0.39071887731552124\n",
      "Training loss for batch 1725 : 0.17900843918323517\n",
      "Training loss for batch 1726 : 0.26659080386161804\n",
      "Training loss for batch 1727 : 0.19347698986530304\n",
      "Training loss for batch 1728 : 0.21846596896648407\n",
      "Training loss for batch 1729 : 0.15385983884334564\n",
      "Training loss for batch 1730 : 0.08194833993911743\n",
      "Training loss for batch 1731 : 0.3943277597427368\n",
      "Training loss for batch 1732 : 0.12100477516651154\n",
      "Training loss for batch 1733 : 0.1447356641292572\n",
      "Training loss for batch 1734 : 0.15965750813484192\n",
      "Training loss for batch 1735 : 0.33628642559051514\n",
      "Training loss for batch 1736 : 0.2765270173549652\n",
      "Training loss for batch 1737 : 0.14210930466651917\n",
      "Training loss for batch 1738 : 0.5208451151847839\n",
      "Training loss for batch 1739 : 0.039655741304159164\n",
      "Training loss for batch 1740 : 0.35110539197921753\n",
      "Training loss for batch 1741 : 0.26360225677490234\n",
      "Training loss for batch 1742 : 0.07961592823266983\n",
      "Training loss for batch 1743 : 0.25001007318496704\n",
      "Training loss for batch 1744 : 0.44364821910858154\n",
      "Training loss for batch 1745 : 0.214747354388237\n",
      "Training loss for batch 1746 : 0.08653853833675385\n",
      "Training loss for batch 1747 : 0.20043793320655823\n",
      "Training loss for batch 1748 : 0.05761292204260826\n",
      "Training loss for batch 1749 : 0.007847238332033157\n",
      "Training loss for batch 1750 : 0.2886614203453064\n",
      "Training loss for batch 1751 : 0.14910461008548737\n",
      "Training loss for batch 1752 : 0.5731480717658997\n",
      "Training loss for batch 1753 : 0.25296708941459656\n",
      "Training loss for batch 1754 : 0.06055394932627678\n",
      "Training loss for batch 1755 : 0.2590060532093048\n",
      "Training loss for batch 1756 : 0.2904437482357025\n",
      "Training loss for batch 1757 : 0.0991135984659195\n",
      "Training loss for batch 1758 : 0.0\n",
      "Training loss for batch 1759 : 0.16376745700836182\n",
      "Training loss for batch 1760 : 0.3232535123825073\n",
      "Training loss for batch 1761 : 0.43842947483062744\n",
      "Training loss for batch 1762 : 0.3248453736305237\n",
      "Training loss for batch 1763 : 0.2574046850204468\n",
      "Training loss for batch 1764 : 0.3548068404197693\n",
      "Training loss for batch 1765 : 0.3787182569503784\n",
      "Training loss for batch 1766 : 0.12992671132087708\n",
      "Training loss for batch 1767 : 0.005318760871887207\n",
      "Training loss for batch 1768 : 0.07088753581047058\n",
      "Training loss for batch 1769 : 0.33068352937698364\n",
      "Training loss for batch 1770 : 0.2445380985736847\n",
      "Training loss for batch 1771 : 0.20285987854003906\n",
      "Training loss for batch 1772 : 0.16435202956199646\n",
      "Training loss for batch 1773 : 0.3174627423286438\n",
      "Training loss for batch 1774 : 0.4506477117538452\n",
      "Training loss for batch 1775 : 0.2838588356971741\n",
      "Training loss for batch 1776 : 0.39889445900917053\n",
      "Training loss for batch 1777 : 0.2598972022533417\n",
      "Training loss for batch 1778 : 0.24559880793094635\n",
      "Training loss for batch 1779 : 0.358445405960083\n",
      "Training loss for batch 1780 : 0.14480571448802948\n",
      "Training loss for batch 1781 : 0.35224705934524536\n",
      "Training loss for batch 1782 : 0.3509445786476135\n",
      "Training loss for batch 1783 : 0.156814306974411\n",
      "Training loss for batch 1784 : 0.4268262982368469\n",
      "Training loss for batch 1785 : 0.2723768651485443\n",
      "Training loss for batch 1786 : 0.2473611980676651\n",
      "Training loss for batch 1787 : 0.1307830512523651\n",
      "Training loss for batch 1788 : 0.1706918329000473\n",
      "Training loss for batch 1789 : 0.3980029821395874\n",
      "Training loss for batch 1790 : 0.056459978222846985\n",
      "Training loss for batch 1791 : 0.2689569294452667\n",
      "Training loss for batch 1792 : 0.24640852212905884\n",
      "Training loss for batch 1793 : 0.4729022979736328\n",
      "Training loss for batch 1794 : 0.0902264341711998\n",
      "Training loss for batch 1795 : 0.03706144541501999\n",
      "Training loss for batch 1796 : 0.23248329758644104\n",
      "Training loss for batch 1797 : 0.03616037592291832\n",
      "Training loss for batch 1798 : 0.4267008602619171\n",
      "Training loss for batch 1799 : 0.03918839991092682\n",
      "Training loss for batch 1800 : 0.29985854029655457\n",
      "Training loss for batch 1801 : 0.07786055654287338\n",
      "Training loss for batch 1802 : 0.24419187009334564\n",
      "Training loss for batch 1803 : 0.10982237011194229\n",
      "Training loss for batch 1804 : 0.1372382938861847\n",
      "Training loss for batch 1805 : 0.27667146921157837\n",
      "Training loss for batch 1806 : 0.07441513240337372\n",
      "Training loss for batch 1807 : 0.009195557795464993\n",
      "Training loss for batch 1808 : 0.16032734513282776\n",
      "Training loss for batch 1809 : 0.08714113384485245\n",
      "Training loss for batch 1810 : 0.066182941198349\n",
      "Training loss for batch 1811 : 0.12611041963100433\n",
      "Training loss for batch 1812 : 0.12689857184886932\n",
      "Training loss for batch 1813 : 0.06964458525180817\n",
      "Training loss for batch 1814 : 0.20383761823177338\n",
      "Training loss for batch 1815 : 0.23156090080738068\n",
      "Training loss for batch 1816 : 0.5141403079032898\n",
      "Training loss for batch 1817 : 0.20918889343738556\n",
      "Training loss for batch 1818 : 0.6008155941963196\n",
      "Training loss for batch 1819 : 0.07962299883365631\n",
      "Training loss for batch 1820 : 0.12608183920383453\n",
      "Training loss for batch 1821 : 0.00415519904345274\n",
      "Training loss for batch 1822 : 0.14204266667366028\n",
      "Training loss for batch 1823 : 0.04912570118904114\n",
      "Training loss for batch 1824 : 0.726434588432312\n",
      "Training loss for batch 1825 : 0.7844756841659546\n",
      "Training loss for batch 1826 : 0.1636723130941391\n",
      "Training loss for batch 1827 : 0.12753653526306152\n",
      "Training loss for batch 1828 : 0.108836330473423\n",
      "Training loss for batch 1829 : 0.3975827097892761\n",
      "Training loss for batch 1830 : 0.2126702070236206\n",
      "Training loss for batch 1831 : 0.13077132403850555\n",
      "Training loss for batch 1832 : 0.09201975166797638\n",
      "Training loss for batch 1833 : 0.18707256019115448\n",
      "Training loss for batch 1834 : 0.4088197648525238\n",
      "Training loss for batch 1835 : 0.29406020045280457\n",
      "Training loss for batch 1836 : 0.42500507831573486\n",
      "Training loss for batch 1837 : 0.24422848224639893\n",
      "Training loss for batch 1838 : 0.3523311913013458\n",
      "Training loss for batch 1839 : 0.09233804792165756\n",
      "Training loss for batch 1840 : 0.08906903117895126\n",
      "Training loss for batch 1841 : 0.19539153575897217\n",
      "Training loss for batch 1842 : 0.15099412202835083\n",
      "Training loss for batch 1843 : 0.29356008768081665\n",
      "Training loss for batch 1844 : 0.1498889923095703\n",
      "Training loss for batch 1845 : 0.0714813694357872\n",
      "Training loss for batch 1846 : 0.08970416337251663\n",
      "Training loss for batch 1847 : 0.28884780406951904\n",
      "Training loss for batch 1848 : 0.11404027789831161\n",
      "Training loss for batch 1849 : 0.05175286903977394\n",
      "Training loss for batch 1850 : 0.09279371798038483\n",
      "Training loss for batch 1851 : 0.14654113352298737\n",
      "Training loss for batch 1852 : 0.03356677293777466\n",
      "Training loss for batch 1853 : 0.09247136116027832\n",
      "Training loss for batch 1854 : 0.09117662161588669\n",
      "Training loss for batch 1855 : 0.04033629223704338\n",
      "Training loss for batch 1856 : 0.15722227096557617\n",
      "Training loss for batch 1857 : 0.264334499835968\n",
      "Training loss for batch 1858 : 0.2338927686214447\n",
      "Training loss for batch 1859 : 0.04838999733328819\n",
      "Training loss for batch 1860 : 0.2738237977027893\n",
      "Training loss for batch 1861 : 0.05047924071550369\n",
      "Training loss for batch 1862 : 0.04123449698090553\n",
      "Training loss for batch 1863 : 0.21662896871566772\n",
      "Training loss for batch 1864 : 0.23577944934368134\n",
      "Training loss for batch 1865 : 0.14895343780517578\n",
      "Training loss for batch 1866 : 0.38746654987335205\n",
      "Training loss for batch 1867 : 0.35305294394493103\n",
      "Training loss for batch 1868 : 0.1891939491033554\n",
      "Training loss for batch 1869 : 0.06480516493320465\n",
      "Training loss for batch 1870 : 0.23403681814670563\n",
      "Training loss for batch 1871 : 0.031484778970479965\n",
      "Training loss for batch 1872 : 0.18370753526687622\n",
      "Training loss for batch 1873 : 0.02000974677503109\n",
      "Training loss for batch 1874 : 0.284958153963089\n",
      "Training loss for batch 1875 : 0.1923481971025467\n",
      "Training loss for batch 1876 : 0.25255897641181946\n",
      "Training loss for batch 1877 : 0.265874981880188\n",
      "Training loss for batch 1878 : 0.5233663320541382\n",
      "Training loss for batch 1879 : 0.2596570551395416\n",
      "Training loss for batch 1880 : 0.44208964705467224\n",
      "Training loss for batch 1881 : 0.31582027673721313\n",
      "Training loss for batch 1882 : 0.1073971763253212\n",
      "Training loss for batch 1883 : 0.45890042185783386\n",
      "Training loss for batch 1884 : 0.2817537188529968\n",
      "Training loss for batch 1885 : 0.13102275133132935\n",
      "Training loss for batch 1886 : 0.0\n",
      "Training loss for batch 1887 : 0.281042218208313\n",
      "Training loss for batch 1888 : 0.14204291999340057\n",
      "Training loss for batch 1889 : 0.17446227371692657\n",
      "Training loss for batch 1890 : 0.1582850068807602\n",
      "Training loss for batch 1891 : 0.5011103749275208\n",
      "Training loss for batch 1892 : 0.2018924206495285\n",
      "Training loss for batch 1893 : 0.05114956945180893\n",
      "Training loss for batch 1894 : 0.2201688289642334\n",
      "Training loss for batch 1895 : 0.019577881321310997\n",
      "Training loss for batch 1896 : 0.051705487072467804\n",
      "Training loss for batch 1897 : 0.10623376816511154\n",
      "Training loss for batch 1898 : 0.16914360225200653\n",
      "Training loss for batch 1899 : 0.483669251203537\n",
      "Training loss for batch 1900 : 0.28688764572143555\n",
      "Training loss for batch 1901 : 0.09061911702156067\n",
      "Training loss for batch 1902 : 0.11901248246431351\n",
      "Training loss for batch 1903 : 0.09409181773662567\n",
      "Training loss for batch 1904 : 0.3369907736778259\n",
      "Training loss for batch 1905 : 0.018259646371006966\n",
      "Training loss for batch 1906 : 0.19184786081314087\n",
      "Training loss for batch 1907 : 0.0\n",
      "Training loss for batch 1908 : 0.10289939492940903\n",
      "Training loss for batch 1909 : 0.019069155678153038\n",
      "Training loss for batch 1910 : 0.16784273087978363\n",
      "Training loss for batch 1911 : 0.251385897397995\n",
      "Training loss for batch 1912 : 0.351169615983963\n",
      "Training loss for batch 1913 : 0.296237975358963\n",
      "Training loss for batch 1914 : 0.12544845044612885\n",
      "Training loss for batch 1915 : 0.1323005110025406\n",
      "Training loss for batch 1916 : 0.0773356631398201\n",
      "Training loss for batch 1917 : 0.19744570553302765\n",
      "Training loss for batch 1918 : 0.03097980096936226\n",
      "Training loss for batch 1919 : 1.7017126083374023e-05\n",
      "Training loss for batch 1920 : 0.20080940425395966\n",
      "Training loss for batch 1921 : 0.3122531771659851\n",
      "Training loss for batch 1922 : 0.08245441317558289\n",
      "Training loss for batch 1923 : 0.6952992081642151\n",
      "Training loss for batch 1924 : 0.12084940075874329\n",
      "Training loss for batch 1925 : 0.46724918484687805\n",
      "Training loss for batch 1926 : 0.4176017642021179\n",
      "Training loss for batch 1927 : 0.17038919031620026\n",
      "Training loss for batch 1928 : 0.21305876970291138\n",
      "Training loss for batch 1929 : 0.5373755097389221\n",
      "Training loss for batch 1930 : 0.10982207208871841\n",
      "Training loss for batch 1931 : 0.22268716990947723\n",
      "Training loss for batch 1932 : 0.06289361417293549\n",
      "Training loss for batch 1933 : 0.4127594232559204\n",
      "Training loss for batch 1934 : 0.41307973861694336\n",
      "Training loss for batch 1935 : 0.3895670175552368\n",
      "Training loss for batch 1936 : 0.11643481254577637\n",
      "Training loss for batch 1937 : 0.35683637857437134\n",
      "Training loss for batch 1938 : 0.1278940737247467\n",
      "Training loss for batch 1939 : 0.26064008474349976\n",
      "Training loss for batch 1940 : 0.27508264780044556\n",
      "Training loss for batch 1941 : 0.28272321820259094\n",
      "Training loss for batch 1942 : 0.2652239501476288\n",
      "Training loss for batch 1943 : 0.19856224954128265\n",
      "Training loss for batch 1944 : 0.2885369658470154\n",
      "Training loss for batch 1945 : 0.059202663600444794\n",
      "Training loss for batch 1946 : 0.5909438133239746\n",
      "Training loss for batch 1947 : 0.13525725901126862\n",
      "Training loss for batch 1948 : 0.15562035143375397\n",
      "Training loss for batch 1949 : 0.05715154856443405\n",
      "Training loss for batch 1950 : 0.05410503223538399\n",
      "Training loss for batch 1951 : 0.02420709654688835\n",
      "Training loss for batch 1952 : 0.08001480996608734\n",
      "Training loss for batch 1953 : 0.5403509736061096\n",
      "Training loss for batch 1954 : 0.07738028466701508\n",
      "Training loss for batch 1955 : 0.07800254970788956\n",
      "Training loss for batch 1956 : 0.24332304298877716\n",
      "Training loss for batch 1957 : 0.026840083301067352\n",
      "Training loss for batch 1958 : 0.22618520259857178\n",
      "Training loss for batch 1959 : 0.10139956325292587\n",
      "Training loss for batch 1960 : 0.30756309628486633\n",
      "Training loss for batch 1961 : 0.10318788141012192\n",
      "Training loss for batch 1962 : 0.30943819880485535\n",
      "Training loss for batch 1963 : 0.6026297807693481\n",
      "Training loss for batch 1964 : 0.2721478343009949\n",
      "Training loss for batch 1965 : 0.400150328874588\n",
      "Training loss for batch 1966 : 0.3452054262161255\n",
      "Training loss for batch 1967 : 0.10736241191625595\n",
      "Training loss for batch 1968 : 0.3160936236381531\n",
      "Training loss for batch 1969 : 0.19994588196277618\n",
      "Training loss for batch 1970 : 0.4396044611930847\n",
      "Training loss for batch 1971 : 0.23028036952018738\n",
      "Training loss for batch 1972 : 0.4493027329444885\n",
      "Training loss for batch 1973 : 0.005673784296959639\n",
      "Training loss for batch 1974 : 0.4695667326450348\n",
      "Training loss for batch 1975 : 0.0034185368567705154\n",
      "Training loss for batch 1976 : 0.26959237456321716\n",
      "Training loss for batch 1977 : 0.17282618582248688\n",
      "Training loss for batch 1978 : 0.15515542030334473\n",
      "Training loss for batch 1979 : 0.39281320571899414\n",
      "Training loss for batch 1980 : 0.08368510007858276\n",
      "Training loss for batch 1981 : 0.01891177147626877\n",
      "Training loss for batch 1982 : 0.08502442389726639\n",
      "Training loss for batch 1983 : 0.14045877754688263\n",
      "Training loss for batch 1984 : 0.10564699023962021\n",
      "Training loss for batch 1985 : 0.26318177580833435\n",
      "Training loss for batch 1986 : 0.18433821201324463\n",
      "Training loss for batch 1987 : 0.26309043169021606\n",
      "Training loss for batch 1988 : 0.055148739367723465\n",
      "Training loss for batch 1989 : 0.22646966576576233\n",
      "Training loss for batch 1990 : 0.07477282732725143\n",
      "Training loss for batch 1991 : 0.12706850469112396\n",
      "Training loss for batch 1992 : 0.5347974300384521\n",
      "Training loss for batch 1993 : 0.12355568259954453\n",
      "Training loss for batch 1994 : 0.26549094915390015\n",
      "Training loss for batch 1995 : 0.39485296607017517\n",
      "Training loss for batch 1996 : 0.17573365569114685\n",
      "Training loss for batch 1997 : 0.11803663522005081\n",
      "Training loss for batch 1998 : 0.199228897690773\n",
      "Training loss for batch 1999 : 0.3014609217643738\n",
      "Training loss for batch 2000 : 0.0\n",
      "Training loss for batch 2001 : 0.3015783727169037\n",
      "Training loss for batch 2002 : 0.23509685695171356\n",
      "Training loss for batch 2003 : 0.09336208552122116\n",
      "Training loss for batch 2004 : 0.22575896978378296\n",
      "Training loss for batch 2005 : 0.2638014554977417\n",
      "Training loss for batch 2006 : 0.32404962182044983\n",
      "Training loss for batch 2007 : 0.0053950948640704155\n",
      "Training loss for batch 2008 : 0.16847987473011017\n",
      "Training loss for batch 2009 : 0.03143991902470589\n",
      "Training loss for batch 2010 : 0.3071221709251404\n",
      "Training loss for batch 2011 : 0.25381898880004883\n",
      "Training loss for batch 2012 : 0.0\n",
      "Training loss for batch 2013 : 0.1387663632631302\n",
      "Training loss for batch 2014 : 0.08445911854505539\n",
      "Training loss for batch 2015 : 0.04958499222993851\n",
      "Training loss for batch 2016 : 0.08599983900785446\n",
      "Training loss for batch 2017 : 0.20627416670322418\n",
      "Training loss for batch 2018 : 0.5088433027267456\n",
      "Training loss for batch 2019 : 0.2727135121822357\n",
      "Training loss for batch 2020 : 0.45006540417671204\n",
      "Training loss for batch 2021 : 0.3989786207675934\n",
      "Training loss for batch 2022 : 0.2755419611930847\n",
      "Training loss for batch 2023 : 0.049937669187784195\n",
      "Training loss for batch 2024 : 0.18250823020935059\n",
      "Training loss for batch 2025 : 0.20038041472434998\n",
      "Training loss for batch 2026 : 0.07117357105016708\n",
      "Training loss for batch 2027 : 0.1256018579006195\n",
      "Training loss for batch 2028 : 0.1338176131248474\n",
      "Training loss for batch 2029 : 0.07382123917341232\n",
      "Training loss for batch 2030 : 0.05454932153224945\n",
      "Training loss for batch 2031 : 0.05480199307203293\n",
      "Training loss for batch 2032 : 0.04280320554971695\n",
      "Training loss for batch 2033 : 0.3623460531234741\n",
      "Training loss for batch 2034 : 0.3252227008342743\n",
      "Training loss for batch 2035 : 0.4514063000679016\n",
      "Training loss for batch 2036 : 0.36200156807899475\n",
      "Training loss for batch 2037 : 0.05549432337284088\n",
      "Training loss for batch 2038 : 0.014726769179105759\n",
      "Training loss for batch 2039 : 0.45461246371269226\n",
      "Training loss for batch 2040 : 0.11673376709222794\n",
      "Training loss for batch 2041 : 0.10940113663673401\n",
      "Training loss for batch 2042 : 0.2506996691226959\n",
      "Training loss for batch 2043 : 0.4487791061401367\n",
      "Training loss for batch 2044 : 0.38446539640426636\n",
      "Training loss for batch 2045 : 0.05859464779496193\n",
      "Training loss for batch 2046 : 0.1718612015247345\n",
      "Training loss for batch 2047 : 0.3599932789802551\n",
      "Training loss for batch 2048 : 0.06436430662870407\n",
      "Training loss for batch 2049 : 0.06463560461997986\n",
      "Training loss for batch 2050 : 0.06871208548545837\n",
      "Training loss for batch 2051 : 0.45196694135665894\n",
      "Training loss for batch 2052 : 0.39447587728500366\n",
      "Training loss for batch 2053 : 0.278426855802536\n",
      "Training loss for batch 2054 : 0.1553119271993637\n",
      "Training loss for batch 2055 : 0.06606397777795792\n",
      "Training loss for batch 2056 : 0.34090957045555115\n",
      "Training loss for batch 2057 : 0.5956776142120361\n",
      "Training loss for batch 2058 : 0.07359041273593903\n",
      "Training loss for batch 2059 : 0.265724241733551\n",
      "Training loss for batch 2060 : 0.547711193561554\n",
      "Training loss for batch 2061 : 0.0017615854740142822\n",
      "Training loss for batch 2062 : 0.020057599991559982\n",
      "Training loss for batch 2063 : 0.2931014895439148\n",
      "Training loss for batch 2064 : 0.08114899694919586\n",
      "Training loss for batch 2065 : 0.20132581889629364\n",
      "Training loss for batch 2066 : 0.03946448490023613\n",
      "Training loss for batch 2067 : 0.1837470382452011\n",
      "Training loss for batch 2068 : 0.054618652909994125\n",
      "Training loss for batch 2069 : 0.004181061405688524\n",
      "Training loss for batch 2070 : 0.18536318838596344\n",
      "Training loss for batch 2071 : 0.28244128823280334\n",
      "Training loss for batch 2072 : 0.21104271709918976\n",
      "Training loss for batch 2073 : 0.4768622815608978\n",
      "Training loss for batch 2074 : 0.08507997542619705\n",
      "Training loss for batch 2075 : 0.03369539976119995\n",
      "Training loss for batch 2076 : 0.33942440152168274\n",
      "Training loss for batch 2077 : 0.438739538192749\n",
      "Training loss for batch 2078 : 0.3396458923816681\n",
      "Training loss for batch 2079 : 0.27624428272247314\n",
      "Training loss for batch 2080 : 0.15003059804439545\n",
      "Training loss for batch 2081 : 0.06189388409256935\n",
      "Training loss for batch 2082 : 0.2929219603538513\n",
      "Training loss for batch 2083 : 0.18547724187374115\n",
      "Training loss for batch 2084 : 0.22901777923107147\n",
      "Training loss for batch 2085 : 0.1036689281463623\n",
      "Training loss for batch 2086 : 0.2490496188402176\n",
      "Training loss for batch 2087 : 0.4074135422706604\n",
      "Training loss for batch 2088 : 0.10239344090223312\n",
      "Training loss for batch 2089 : 0.02921362780034542\n",
      "Training loss for batch 2090 : 0.12658606469631195\n",
      "Training loss for batch 2091 : 0.3519178628921509\n",
      "Training loss for batch 2092 : 0.017784705385565758\n",
      "Training loss for batch 2093 : 0.09757065773010254\n",
      "Training loss for batch 2094 : 0.23052284121513367\n",
      "Training loss for batch 2095 : 0.014400732703506947\n",
      "Training loss for batch 2096 : 0.018828134983778\n",
      "Training loss for batch 2097 : 0.018285056576132774\n",
      "Training loss for batch 2098 : 0.07994632422924042\n",
      "Training loss for batch 2099 : 0.07333643734455109\n",
      "Training loss for batch 2100 : 0.19140410423278809\n",
      "Training loss for batch 2101 : 0.023843254894018173\n",
      "Training loss for batch 2102 : 0.11671016365289688\n",
      "Training loss for batch 2103 : 0.028949983417987823\n",
      "Training loss for batch 2104 : 0.24183307588100433\n",
      "Training loss for batch 2105 : 0.1558780074119568\n",
      "Training loss for batch 2106 : 0.0876665860414505\n",
      "Training loss for batch 2107 : 0.10092843323945999\n",
      "Training loss for batch 2108 : 0.14422765374183655\n",
      "Training loss for batch 2109 : 0.013295183889567852\n",
      "Training loss for batch 2110 : 0.6201440691947937\n",
      "Training loss for batch 2111 : 0.04622124508023262\n",
      "Training loss for batch 2112 : 0.1481286883354187\n",
      "Training loss for batch 2113 : 0.2723153233528137\n",
      "Training loss for batch 2114 : 0.0\n",
      "Training loss for batch 2115 : 0.08672283589839935\n",
      "Training loss for batch 2116 : 0.15286459028720856\n",
      "Training loss for batch 2117 : 0.21136651933193207\n",
      "Training loss for batch 2118 : 0.1819579005241394\n",
      "Training loss for batch 2119 : 0.0942399725317955\n",
      "Training loss for batch 2120 : 0.24392640590667725\n",
      "Training loss for batch 2121 : 0.548194944858551\n",
      "Training loss for batch 2122 : 0.23267985880374908\n",
      "Training loss for batch 2123 : 0.039121322333812714\n",
      "Training loss for batch 2124 : 0.36988791823387146\n",
      "Training loss for batch 2125 : 0.2111164629459381\n",
      "Training loss for batch 2126 : 0.09180017560720444\n",
      "Training loss for batch 2127 : 0.20077821612358093\n",
      "Training loss for batch 2128 : 0.24072401225566864\n",
      "Training loss for batch 2129 : 0.0\n",
      "Training loss for batch 2130 : 0.482069730758667\n",
      "Training loss for batch 2131 : 0.7133371829986572\n",
      "Training loss for batch 2132 : 0.07133074849843979\n",
      "Training loss for batch 2133 : 0.07959764450788498\n",
      "Training loss for batch 2134 : 0.0962681919336319\n",
      "Training loss for batch 2135 : 0.1898113191127777\n",
      "Training loss for batch 2136 : 0.353571355342865\n",
      "Training loss for batch 2137 : 0.2097666710615158\n",
      "Training loss for batch 2138 : 0.05112973600625992\n",
      "Training loss for batch 2139 : 0.23764197528362274\n",
      "Training loss for batch 2140 : 0.12518444657325745\n",
      "Training loss for batch 2141 : 1.2066579074598849e-05\n",
      "Training loss for batch 2142 : 0.14098148047924042\n",
      "Training loss for batch 2143 : 0.02967674285173416\n",
      "Training loss for batch 2144 : 0.009054956026375294\n",
      "Training loss for batch 2145 : 0.0\n",
      "Training loss for batch 2146 : 0.013257662765681744\n",
      "Training loss for batch 2147 : 0.2627168893814087\n",
      "Training loss for batch 2148 : 0.016847696155309677\n",
      "Training loss for batch 2149 : 0.12239110469818115\n",
      "Training loss for batch 2150 : 0.07970928400754929\n",
      "Training loss for batch 2151 : 0.366653174161911\n",
      "Training loss for batch 2152 : 0.5775680541992188\n",
      "Training loss for batch 2153 : 0.4103712737560272\n",
      "Training loss for batch 2154 : 0.08758032321929932\n",
      "Training loss for batch 2155 : 0.10785310715436935\n",
      "Training loss for batch 2156 : 0.31005552411079407\n",
      "Training loss for batch 2157 : 0.29520827531814575\n",
      "Training loss for batch 2158 : 0.05930701270699501\n",
      "Training loss for batch 2159 : 0.07864835858345032\n",
      "Training loss for batch 2160 : 0.20046654343605042\n",
      "Training loss for batch 2161 : 0.16436026990413666\n",
      "Training loss for batch 2162 : 0.1729273945093155\n",
      "Training loss for batch 2163 : 0.03685218095779419\n",
      "Training loss for batch 2164 : 0.26758018136024475\n",
      "Training loss for batch 2165 : 0.02423354983329773\n",
      "Training loss for batch 2166 : 0.30672356486320496\n",
      "Training loss for batch 2167 : 0.038975805044174194\n",
      "Training loss for batch 2168 : 0.03294434770941734\n",
      "Training loss for batch 2169 : 0.06664243340492249\n",
      "Training loss for batch 2170 : 0.4951949417591095\n",
      "Training loss for batch 2171 : 0.01693536527454853\n",
      "Training loss for batch 2172 : 0.2380710393190384\n",
      "Training loss for batch 2173 : 0.3636178970336914\n",
      "Training loss for batch 2174 : 0.0952981561422348\n",
      "Training loss for batch 2175 : 0.1291460245847702\n",
      "Training loss for batch 2176 : 0.043008774518966675\n",
      "Training loss for batch 2177 : 0.20056945085525513\n",
      "Training loss for batch 2178 : 0.24529144167900085\n",
      "Training loss for batch 2179 : 0.6420885920524597\n",
      "Training loss for batch 2180 : 0.35890716314315796\n",
      "Training loss for batch 2181 : 0.04314575344324112\n",
      "Training loss for batch 2182 : 0.009099218994379044\n",
      "Training loss for batch 2183 : 0.1654917299747467\n",
      "Training loss for batch 2184 : 0.1939050555229187\n",
      "Training loss for batch 2185 : 0.29357320070266724\n",
      "Training loss for batch 2186 : 0.04894908890128136\n",
      "Training loss for batch 2187 : 0.14248831570148468\n",
      "Training loss for batch 2188 : 0.13045792281627655\n",
      "Training loss for batch 2189 : 0.6103563904762268\n",
      "Training loss for batch 2190 : 0.5922502875328064\n",
      "Training loss for batch 2191 : 0.028529228642582893\n",
      "Training loss for batch 2192 : 0.335202693939209\n",
      "Training loss for batch 2193 : 0.1584266722202301\n",
      "Training loss for batch 2194 : 0.2954532206058502\n",
      "Training loss for batch 2195 : 0.12766295671463013\n",
      "Training loss for batch 2196 : 0.3092244565486908\n",
      "Training loss for batch 2197 : 0.16823889315128326\n",
      "Training loss for batch 2198 : 0.10810548812150955\n",
      "Training loss for batch 2199 : 0.276658833026886\n",
      "Training loss for batch 2200 : 0.5901827216148376\n",
      "Training loss for batch 2201 : 0.010711113922297955\n",
      "Training loss for batch 2202 : 0.056244075298309326\n",
      "Training loss for batch 2203 : 0.5253008604049683\n",
      "Training loss for batch 2204 : 0.01891474239528179\n",
      "Training loss for batch 2205 : 0.17671804130077362\n",
      "Training loss for batch 2206 : 0.16009023785591125\n",
      "Training loss for batch 2207 : 0.4391217827796936\n",
      "Training loss for batch 2208 : 0.035458941012620926\n",
      "Training loss for batch 2209 : 0.15084552764892578\n",
      "Training loss for batch 2210 : 0.1793060451745987\n",
      "Training loss for batch 2211 : 0.16328758001327515\n",
      "Training loss for batch 2212 : 0.22496086359024048\n",
      "Training loss for batch 2213 : 0.10588973760604858\n",
      "Training loss for batch 2214 : 0.44711577892303467\n",
      "Training loss for batch 2215 : 0.21392542123794556\n",
      "Training loss for batch 2216 : 0.25080883502960205\n",
      "Training loss for batch 2217 : 0.2320910096168518\n",
      "Training loss for batch 2218 : 0.23947495222091675\n",
      "Training loss for batch 2219 : 0.2747509777545929\n",
      "Training loss for batch 2220 : 0.6490088105201721\n",
      "Training loss for batch 2221 : 0.11887984722852707\n",
      "Training loss for batch 2222 : 0.3724173307418823\n",
      "Training loss for batch 2223 : 0.0008449126034975052\n",
      "Training loss for batch 2224 : 0.25095483660697937\n",
      "Training loss for batch 2225 : 0.3390103578567505\n",
      "Training loss for batch 2226 : 0.10943188518285751\n",
      "Training loss for batch 2227 : 0.07326038926839828\n",
      "Training loss for batch 2228 : 0.16412757337093353\n",
      "Training loss for batch 2229 : 0.0894869863986969\n",
      "Training loss for batch 2230 : 0.4268511235713959\n",
      "Training loss for batch 2231 : 0.0\n",
      "Training loss for batch 2232 : 0.13575105369091034\n",
      "Training loss for batch 2233 : 0.0602167584002018\n",
      "Training loss for batch 2234 : 0.2222992479801178\n",
      "Training loss for batch 2235 : 0.3779809772968292\n",
      "Training loss for batch 2236 : 0.272455632686615\n",
      "Training loss for batch 2237 : 0.4872588813304901\n",
      "Training loss for batch 2238 : 0.12306863814592361\n",
      "Training loss for batch 2239 : 0.15940172970294952\n",
      "Training loss for batch 2240 : 0.11088079959154129\n",
      "Training loss for batch 2241 : 0.15065401792526245\n",
      "Training loss for batch 2242 : 0.029537824913859367\n",
      "Training loss for batch 2243 : 0.09179936349391937\n",
      "Training loss for batch 2244 : 0.15536819398403168\n",
      "Training loss for batch 2245 : 0.35765111446380615\n",
      "Training loss for batch 2246 : 0.2575009763240814\n",
      "Training loss for batch 2247 : 0.6394463777542114\n",
      "Training loss for batch 2248 : 0.1940208524465561\n",
      "Training loss for batch 2249 : 0.038907360285520554\n",
      "Training loss for batch 2250 : 0.07986149936914444\n",
      "Training loss for batch 2251 : 0.06231909245252609\n",
      "Training loss for batch 2252 : 0.3475392758846283\n",
      "Training loss for batch 2253 : 0.015003621578216553\n",
      "Training loss for batch 2254 : 0.024027520790696144\n",
      "Training loss for batch 2255 : 0.15300337970256805\n",
      "Training loss for batch 2256 : 0.2016187310218811\n",
      "Training loss for batch 2257 : 0.07768767327070236\n",
      "Training loss for batch 2258 : 0.10542170703411102\n",
      "Training loss for batch 2259 : 0.17157039046287537\n",
      "Training loss for batch 2260 : 0.1190035343170166\n",
      "Training loss for batch 2261 : 0.04661456122994423\n",
      "Training loss for batch 2262 : 0.4066419303417206\n",
      "Training loss for batch 2263 : 0.03721517696976662\n",
      "Training loss for batch 2264 : 0.14875337481498718\n",
      "Training loss for batch 2265 : 0.10294774174690247\n",
      "Training loss for batch 2266 : 0.15288150310516357\n",
      "Training loss for batch 2267 : 0.10256730765104294\n",
      "Training loss for batch 2268 : 0.03543117642402649\n",
      "Training loss for batch 2269 : 0.2830745279788971\n",
      "Training loss for batch 2270 : 0.47002673149108887\n",
      "Training loss for batch 2271 : 0.1539902538061142\n",
      "Training loss for batch 2272 : 0.18079373240470886\n",
      "Training loss for batch 2273 : 0.0809200257062912\n",
      "Training loss for batch 2274 : 0.11164235323667526\n",
      "Training loss for batch 2275 : 0.029131149873137474\n",
      "Training loss for batch 2276 : 0.0941232293844223\n",
      "Training loss for batch 2277 : 0.14050552248954773\n",
      "Training loss for batch 2278 : 0.10768217593431473\n",
      "Training loss for batch 2279 : 0.01900399662554264\n",
      "Training loss for batch 2280 : 0.0752144381403923\n",
      "Training loss for batch 2281 : 0.17335455119609833\n",
      "Training loss for batch 2282 : 0.5076664090156555\n",
      "Training loss for batch 2283 : 0.33589544892311096\n",
      "Training loss for batch 2284 : 0.008145446889102459\n",
      "Training loss for batch 2285 : 0.08208239078521729\n",
      "Training loss for batch 2286 : 0.14614209532737732\n",
      "Training loss for batch 2287 : 0.48173585534095764\n",
      "Training loss for batch 2288 : 0.10024361312389374\n",
      "Training loss for batch 2289 : 0.05853281542658806\n",
      "Training loss for batch 2290 : 0.056235816329717636\n",
      "Training loss for batch 2291 : 0.07705529779195786\n",
      "Training loss for batch 2292 : 0.2610045373439789\n",
      "Training loss for batch 2293 : 0.20379075407981873\n",
      "Training loss for batch 2294 : 0.14215266704559326\n",
      "Training loss for batch 2295 : 0.24123446643352509\n",
      "Training loss for batch 2296 : 0.043494272977113724\n",
      "Training loss for batch 2297 : 0.0\n",
      "Training loss for batch 2298 : 0.18746641278266907\n",
      "Training loss for batch 2299 : 0.39148324728012085\n",
      "Training loss for batch 2300 : 0.5329199433326721\n",
      "Training loss for batch 2301 : 0.030573859810829163\n",
      "Training loss for batch 2302 : 0.3603285253047943\n",
      "Training loss for batch 2303 : 0.06829699873924255\n",
      "Training loss for batch 2304 : 0.07751043140888214\n",
      "Training loss for batch 2305 : 0.10877764225006104\n",
      "Training loss for batch 2306 : 0.20785744488239288\n",
      "Training loss for batch 2307 : 0.00020193378441035748\n",
      "Training loss for batch 2308 : 0.17310400307178497\n",
      "Training loss for batch 2309 : 0.11968641728162766\n",
      "Training loss for batch 2310 : 0.07699550688266754\n",
      "Training loss for batch 2311 : 0.25408637523651123\n",
      "Training loss for batch 2312 : 0.10334654152393341\n",
      "Training loss for batch 2313 : 0.058170922100543976\n",
      "Training loss for batch 2314 : 0.07410681992769241\n",
      "Training loss for batch 2315 : 0.0070111751556396484\n",
      "Training loss for batch 2316 : 0.16622145473957062\n",
      "Training loss for batch 2317 : 0.408542662858963\n",
      "Training loss for batch 2318 : 0.1860140562057495\n",
      "Training loss for batch 2319 : 0.8885395526885986\n",
      "Training loss for batch 2320 : 0.20081210136413574\n",
      "Training loss for batch 2321 : 0.019803544506430626\n",
      "Training loss for batch 2322 : 0.03334883227944374\n",
      "Training loss for batch 2323 : 0.17946428060531616\n",
      "Training loss for batch 2324 : 0.4029977321624756\n",
      "Training loss for batch 2325 : 0.37462395429611206\n",
      "Training loss for batch 2326 : 0.010395682416856289\n",
      "Training loss for batch 2327 : 0.2151137739419937\n",
      "Training loss for batch 2328 : 0.20517221093177795\n",
      "Training loss for batch 2329 : 0.0\n",
      "Training loss for batch 2330 : 0.34374040365219116\n",
      "Training loss for batch 2331 : 0.023913085460662842\n",
      "Training loss for batch 2332 : 0.011575191281735897\n",
      "Training loss for batch 2333 : 0.28009581565856934\n",
      "Training loss for batch 2334 : 0.29554811120033264\n",
      "Training loss for batch 2335 : 0.40950292348861694\n",
      "Training loss for batch 2336 : 0.38085564970970154\n",
      "Training loss for batch 2337 : 0.12957307696342468\n",
      "Training loss for batch 2338 : 0.11018741130828857\n",
      "Training loss for batch 2339 : 0.15164072811603546\n",
      "Training loss for batch 2340 : 0.687006413936615\n",
      "Training loss for batch 2341 : 0.3900638818740845\n",
      "Training loss for batch 2342 : 0.08750863373279572\n",
      "Training loss for batch 2343 : 0.05644121766090393\n",
      "Training loss for batch 2344 : 0.6864234209060669\n",
      "Training loss for batch 2345 : 0.08515805751085281\n",
      "Training loss for batch 2346 : 0.12063270062208176\n",
      "Training loss for batch 2347 : 0.15577369928359985\n",
      "Training loss for batch 2348 : 0.006518644280731678\n",
      "Training loss for batch 2349 : 0.3895030617713928\n",
      "Training loss for batch 2350 : 0.0644492506980896\n",
      "Training loss for batch 2351 : 0.06898805499076843\n",
      "Training loss for batch 2352 : 0.15062958002090454\n",
      "Training loss for batch 2353 : 0.09750650823116302\n",
      "Training loss for batch 2354 : 0.2090969830751419\n",
      "Training loss for batch 2355 : 0.41421955823898315\n",
      "Training loss for batch 2356 : 0.0248935017734766\n",
      "Training loss for batch 2357 : 0.4435577988624573\n",
      "Training loss for batch 2358 : 0.21960894763469696\n",
      "Training loss for batch 2359 : 0.06685546785593033\n",
      "Training loss for batch 2360 : 0.08153024315834045\n",
      "Training loss for batch 2361 : 0.23397989571094513\n",
      "Training loss for batch 2362 : 0.019489184021949768\n",
      "Training loss for batch 2363 : 0.1597898304462433\n",
      "Training loss for batch 2364 : 0.3569631278514862\n",
      "Training loss for batch 2365 : 0.32368987798690796\n",
      "Training loss for batch 2366 : 0.1732979714870453\n",
      "Training loss for batch 2367 : 0.38002708554267883\n",
      "Training loss for batch 2368 : 0.08154036849737167\n",
      "Training loss for batch 2369 : 0.27206042408943176\n",
      "Training loss for batch 2370 : 0.09592648595571518\n",
      "Training loss for batch 2371 : 0.2842668294906616\n",
      "Training loss for batch 2372 : 0.25224536657333374\n",
      "Training loss for batch 2373 : 0.07338368892669678\n",
      "Training loss for batch 2374 : 0.3548825979232788\n",
      "Training loss for batch 2375 : 0.3627479076385498\n",
      "Training loss for batch 2376 : 0.037556618452072144\n",
      "Training loss for batch 2377 : 0.13484904170036316\n",
      "Training loss for batch 2378 : 0.28979432582855225\n",
      "Training loss for batch 2379 : 0.058455053716897964\n",
      "Training loss for batch 2380 : 0.07106468081474304\n",
      "Training loss for batch 2381 : 0.33731603622436523\n",
      "Training loss for batch 2382 : 0.11308994144201279\n",
      "Training loss for batch 2383 : 0.0\n",
      "Training loss for batch 2384 : 0.04591095447540283\n",
      "Training loss for batch 2385 : 0.20732425153255463\n",
      "Training loss for batch 2386 : 0.1552492380142212\n",
      "Training loss for batch 2387 : 0.07218115031719208\n",
      "Training loss for batch 2388 : 0.005846332758665085\n",
      "Training loss for batch 2389 : 0.6666446924209595\n",
      "Training loss for batch 2390 : 0.2241044044494629\n",
      "Training loss for batch 2391 : 0.3300105929374695\n",
      "Training loss for batch 2392 : 0.12288527935743332\n",
      "Training loss for batch 2393 : 0.4882736802101135\n",
      "Training loss for batch 2394 : 0.5371617078781128\n",
      "Training loss for batch 2395 : 0.20568330585956573\n",
      "Training loss for batch 2396 : 0.3139716684818268\n",
      "Training loss for batch 2397 : 0.25527119636535645\n",
      "Training loss for batch 2398 : 0.4890846908092499\n",
      "Training loss for batch 2399 : 0.11794646829366684\n",
      "Training loss for batch 2400 : 0.1356223225593567\n",
      "Training loss for batch 2401 : 0.16612274944782257\n",
      "Training loss for batch 2402 : 0.49166709184646606\n",
      "Training loss for batch 2403 : 0.009789417497813702\n",
      "Training loss for batch 2404 : 0.09442699700593948\n",
      "Training loss for batch 2405 : 0.34851399064064026\n",
      "Training loss for batch 2406 : 0.4366137683391571\n",
      "Training loss for batch 2407 : 0.6138681173324585\n",
      "Training loss for batch 2408 : 0.1300467997789383\n",
      "Training loss for batch 2409 : 0.008859493769705296\n",
      "Training loss for batch 2410 : 0.07186252623796463\n",
      "Training loss for batch 2411 : 0.22684744000434875\n",
      "Training loss for batch 2412 : 0.3591011166572571\n",
      "Training loss for batch 2413 : 0.43604153394699097\n",
      "Training loss for batch 2414 : 0.1337788850069046\n",
      "Training loss for batch 2415 : 0.36096298694610596\n",
      "Training loss for batch 2416 : 0.4765094816684723\n",
      "Training loss for batch 2417 : 0.3358842134475708\n",
      "Training loss for batch 2418 : 1.0536037683486938\n",
      "Training loss for batch 2419 : 0.3721066415309906\n",
      "Training loss for batch 2420 : 0.1786414533853531\n",
      "Training loss for batch 2421 : 0.3093928098678589\n",
      "Training loss for batch 2422 : 0.34309089183807373\n",
      "Training loss for batch 2423 : 0.5135546326637268\n",
      "Training loss for batch 2424 : 0.2890343964099884\n",
      "Training loss for batch 2425 : 0.3284185230731964\n",
      "Training loss for batch 2426 : 0.009996481239795685\n",
      "Training loss for batch 2427 : 0.10877947509288788\n",
      "Training loss for batch 2428 : 0.3195319175720215\n",
      "Training loss for batch 2429 : 0.0623420774936676\n",
      "Training loss for batch 2430 : 0.1902555227279663\n",
      "Training loss for batch 2431 : 0.11528167128562927\n",
      "Training loss for batch 2432 : 0.2438417226076126\n",
      "Training loss for batch 2433 : 0.026338445022702217\n",
      "Training loss for batch 2434 : 0.14066819846630096\n",
      "Training loss for batch 2435 : 0.03871805593371391\n",
      "Training loss for batch 2436 : 0.013658454641699791\n",
      "Training loss for batch 2437 : 0.012971600517630577\n",
      "Training loss for batch 2438 : 0.20702239871025085\n",
      "Training loss for batch 2439 : 0.27332863211631775\n",
      "Training loss for batch 2440 : 0.1024014875292778\n",
      "Training loss for batch 2441 : 0.4807010591030121\n",
      "Training loss for batch 2442 : 0.17758528888225555\n",
      "Training loss for batch 2443 : 0.3067459166049957\n",
      "Training loss for batch 2444 : 0.2762579023838043\n",
      "Training loss for batch 2445 : 0.13654811680316925\n",
      "Training loss for batch 2446 : 0.11231491714715958\n",
      "Training loss for batch 2447 : 0.031205087900161743\n",
      "Training loss for batch 2448 : 0.2890526056289673\n",
      "Training loss for batch 2449 : 0.27252432703971863\n",
      "Training loss for batch 2450 : 0.10553843528032303\n",
      "Training loss for batch 2451 : 0.5019887685775757\n",
      "Training loss for batch 2452 : 0.46827077865600586\n",
      "Training loss for batch 2453 : 0.4188065826892853\n",
      "Training loss for batch 2454 : 0.16935551166534424\n",
      "Training loss for batch 2455 : 0.19062727689743042\n",
      "Training loss for batch 2456 : 0.014125009998679161\n",
      "Training loss for batch 2457 : 0.42489808797836304\n",
      "Training loss for batch 2458 : 0.046389080584049225\n",
      "Training loss for batch 2459 : 0.8377406001091003\n",
      "Training loss for batch 2460 : 0.19262772798538208\n",
      "Training loss for batch 2461 : 0.02655978314578533\n",
      "Training loss for batch 2462 : 0.07154669612646103\n",
      "Training loss for batch 2463 : 0.46904027462005615\n",
      "Training loss for batch 2464 : 0.05509296804666519\n",
      "Training loss for batch 2465 : 0.10337209701538086\n",
      "Training loss for batch 2466 : 0.3538316786289215\n",
      "Training loss for batch 2467 : 0.03649092838168144\n",
      "Training loss for batch 2468 : 0.1373433768749237\n",
      "Training loss for batch 2469 : 0.09170949459075928\n",
      "Training loss for batch 2470 : 0.3922732472419739\n",
      "Training loss for batch 2471 : 0.2785533666610718\n",
      "Training loss for batch 2472 : 0.003763726446777582\n",
      "Training loss for batch 2473 : 0.035136040300130844\n",
      "Training loss for batch 2474 : 0.12182728201150894\n",
      "Training loss for batch 2475 : 0.07132799178361893\n",
      "Training loss for batch 2476 : 0.22280454635620117\n",
      "Training loss for batch 2477 : 0.22230540215969086\n",
      "Training loss for batch 2478 : 0.28579282760620117\n",
      "Training loss for batch 2479 : 0.7482842803001404\n",
      "Training loss for batch 2480 : 0.0643511712551117\n",
      "Training loss for batch 2481 : 0.34104597568511963\n",
      "Training loss for batch 2482 : 0.3977949321269989\n",
      "Training loss for batch 2483 : 0.1081656739115715\n",
      "Training loss for batch 2484 : 0.15740545094013214\n",
      "Training loss for batch 2485 : 0.21017268300056458\n",
      "Training loss for batch 2486 : 0.09632350504398346\n",
      "Training loss for batch 2487 : 0.28123119473457336\n",
      "Training loss for batch 2488 : 0.13089051842689514\n",
      "Training loss for batch 2489 : 0.08158806711435318\n",
      "Training loss for batch 2490 : 0.25795093178749084\n",
      "Training loss for batch 2491 : 0.26543503999710083\n",
      "Training loss for batch 2492 : 0.2628382444381714\n",
      "Training loss for batch 2493 : 0.030460596084594727\n",
      "Training loss for batch 2494 : 0.2493220865726471\n",
      "Training loss for batch 2495 : 0.23436319828033447\n",
      "Training loss for batch 2496 : 0.37565475702285767\n",
      "Training loss for batch 2497 : 0.1384923905134201\n",
      "Training loss for batch 2498 : 0.2850194275379181\n",
      "Training loss for batch 2499 : 0.2709425985813141\n",
      "Training loss for batch 2500 : 0.1277361959218979\n",
      "Training loss for batch 2501 : 0.11754391342401505\n",
      "Training loss for batch 2502 : 0.232937753200531\n",
      "Training loss for batch 2503 : 0.25874555110931396\n",
      "Training loss for batch 2504 : 0.0842866376042366\n",
      "Training loss for batch 2505 : 0.06039296090602875\n",
      "Training loss for batch 2506 : 0.2977272868156433\n",
      "Training loss for batch 2507 : 0.04509275034070015\n",
      "Training loss for batch 2508 : 0.23936748504638672\n",
      "Training loss for batch 2509 : 0.21458634734153748\n",
      "Training loss for batch 2510 : 0.0710819885134697\n",
      "Training loss for batch 2511 : 0.02684326469898224\n",
      "Training loss for batch 2512 : 0.3809606432914734\n",
      "Training loss for batch 2513 : 0.018072180449962616\n",
      "Training loss for batch 2514 : 0.12863381206989288\n",
      "Training loss for batch 2515 : 0.07845526188611984\n",
      "Training loss for batch 2516 : 0.05147683620452881\n",
      "Training loss for batch 2517 : 0.02299567125737667\n",
      "Training loss for batch 2518 : 0.3766779899597168\n",
      "Training loss for batch 2519 : 0.21613658964633942\n",
      "Training loss for batch 2520 : 0.11825219541788101\n",
      "Training loss for batch 2521 : 0.0007099509239196777\n",
      "Training loss for batch 2522 : 0.3332359194755554\n",
      "Training loss for batch 2523 : 0.4894932806491852\n",
      "Training loss for batch 2524 : 0.3059583306312561\n",
      "Training loss for batch 2525 : 0.07313935458660126\n",
      "Training loss for batch 2526 : 0.261615514755249\n",
      "Training loss for batch 2527 : 0.12961746752262115\n",
      "Training loss for batch 2528 : 0.17277511954307556\n",
      "Training loss for batch 2529 : 0.3461882770061493\n",
      "Training loss for batch 2530 : 0.4683709442615509\n",
      "Training loss for batch 2531 : 0.6320269107818604\n",
      "Training loss for batch 2532 : 0.07415731996297836\n",
      "Training loss for batch 2533 : 0.30695319175720215\n",
      "Training loss for batch 2534 : 0.24885325133800507\n",
      "Training loss for batch 2535 : 0.43128466606140137\n",
      "Training loss for batch 2536 : 0.007505782879889011\n",
      "Training loss for batch 2537 : 0.030467122793197632\n",
      "Training loss for batch 2538 : 0.1496947705745697\n",
      "Training loss for batch 2539 : 0.4271584451198578\n",
      "Training loss for batch 2540 : 0.3709411919116974\n",
      "Training loss for batch 2541 : 0.5221795439720154\n",
      "Training loss for batch 2542 : 0.4124625027179718\n",
      "Training loss for batch 2543 : 0.2963663935661316\n",
      "Training loss for batch 2544 : 0.13652025163173676\n",
      "Training loss for batch 2545 : 0.1972913146018982\n",
      "Training loss for batch 2546 : 0.01997649110853672\n",
      "Training loss for batch 2547 : 0.07883211970329285\n",
      "Training loss for batch 2548 : 0.3174496293067932\n",
      "Training loss for batch 2549 : 0.08537466824054718\n",
      "Training loss for batch 2550 : 0.06605556607246399\n",
      "Training loss for batch 2551 : 0.4042661488056183\n",
      "Training loss for batch 2552 : 0.0\n",
      "Training loss for batch 2553 : 0.19878798723220825\n",
      "Training loss for batch 2554 : 0.1585577130317688\n",
      "Training loss for batch 2555 : 0.3785448968410492\n",
      "Training loss for batch 2556 : 0.010677248239517212\n",
      "Training loss for batch 2557 : 0.15377263724803925\n",
      "Training loss for batch 2558 : 0.04901745915412903\n",
      "Training loss for batch 2559 : 0.2463098019361496\n",
      "Training loss for batch 2560 : 0.10405145585536957\n",
      "Training loss for batch 2561 : 0.2893148958683014\n",
      "Training loss for batch 2562 : 0.018091442063450813\n",
      "Training loss for batch 2563 : 0.1002199798822403\n",
      "Training loss for batch 2564 : 0.055642370134592056\n",
      "Training loss for batch 2565 : 0.13108280301094055\n",
      "Training loss for batch 2566 : 0.24664269387722015\n",
      "Training loss for batch 2567 : 0.05333302170038223\n",
      "Training loss for batch 2568 : 0.3355833888053894\n",
      "Training loss for batch 2569 : 0.11185656487941742\n",
      "Training loss for batch 2570 : 0.19905322790145874\n",
      "Training loss for batch 2571 : 0.5133049488067627\n",
      "Training loss for batch 2572 : 0.1693154126405716\n",
      "Training loss for batch 2573 : 0.3165959417819977\n",
      "Training loss for batch 2574 : 0.2869424521923065\n",
      "Training loss for batch 2575 : 0.22398479282855988\n",
      "Training loss for batch 2576 : 0.3296787142753601\n",
      "Training loss for batch 2577 : 0.2344077080488205\n",
      "Training loss for batch 2578 : 0.23055502772331238\n",
      "Training loss for batch 2579 : 0.23605087399482727\n",
      "Training loss for batch 2580 : 0.3886581361293793\n",
      "Training loss for batch 2581 : 0.03838254138827324\n",
      "Training loss for batch 2582 : 0.17849336564540863\n",
      "Training loss for batch 2583 : 0.2436044067144394\n",
      "Training loss for batch 2584 : 0.3996942639350891\n",
      "Training loss for batch 2585 : 0.36782410740852356\n",
      "Training loss for batch 2586 : 0.18891116976737976\n",
      "Training loss for batch 2587 : 0.1416657716035843\n",
      "Training loss for batch 2588 : 0.1360466033220291\n",
      "Training loss for batch 2589 : 0.23321767151355743\n",
      "Training loss for batch 2590 : 0.25113773345947266\n",
      "Training loss for batch 2591 : 0.4431374669075012\n",
      "Training loss for batch 2592 : 0.1543402075767517\n",
      "Training loss for batch 2593 : 0.16993030905723572\n",
      "Training loss for batch 2594 : 0.09873879700899124\n",
      "Training loss for batch 2595 : 0.015248076058924198\n",
      "Training loss for batch 2596 : 0.08988577127456665\n",
      "Training loss for batch 2597 : 0.14221231639385223\n",
      "Training loss for batch 2598 : 0.17570042610168457\n",
      "Training loss for batch 2599 : 0.09636358916759491\n",
      "Training loss for batch 2600 : 0.08165916055440903\n",
      "Training loss for batch 2601 : 0.17775925993919373\n",
      "Training loss for batch 2602 : 0.08376195281744003\n",
      "Training loss for batch 2603 : 0.2631058096885681\n",
      "Training loss for batch 2604 : 0.24688902497291565\n",
      "Training loss for batch 2605 : 0.15362922847270966\n",
      "Training loss for batch 2606 : 0.15565165877342224\n",
      "Training loss for batch 2607 : 0.1544599086046219\n",
      "Training loss for batch 2608 : 0.21190260350704193\n",
      "Training loss for batch 2609 : 0.34090226888656616\n",
      "Training loss for batch 2610 : 0.40446093678474426\n",
      "Training loss for batch 2611 : 0.16078950464725494\n",
      "Training loss for batch 2612 : 0.22439618408679962\n",
      "Training loss for batch 2613 : 0.1421027034521103\n",
      "Training loss for batch 2614 : 0.21087488532066345\n",
      "Training loss for batch 2615 : 0.01944001019001007\n",
      "Training loss for batch 2616 : 0.0\n",
      "Training loss for batch 2617 : 0.3442288935184479\n",
      "Training loss for batch 2618 : 0.21862554550170898\n",
      "Training loss for batch 2619 : 0.08304324746131897\n",
      "Training loss for batch 2620 : 0.06191019341349602\n",
      "Training loss for batch 2621 : 0.07603762298822403\n",
      "Training loss for batch 2622 : 0.4245952069759369\n",
      "Training loss for batch 2623 : 0.08285290002822876\n",
      "Training loss for batch 2624 : 0.2660108208656311\n",
      "Training loss for batch 2625 : 0.11450512707233429\n",
      "Training loss for batch 2626 : 0.02863455004990101\n",
      "Training loss for batch 2627 : 0.13122744858264923\n",
      "Training loss for batch 2628 : 0.33495548367500305\n",
      "Training loss for batch 2629 : 0.024792535230517387\n",
      "Training loss for batch 2630 : 0.4122315049171448\n",
      "Training loss for batch 2631 : 0.1610359251499176\n",
      "Training loss for batch 2632 : 0.4095701575279236\n",
      "Training loss for batch 2633 : 0.25085362792015076\n",
      "Training loss for batch 2634 : 0.037061356008052826\n",
      "Training loss for batch 2635 : 0.38872450590133667\n",
      "Training loss for batch 2636 : 0.31208211183547974\n",
      "Training loss for batch 2637 : 0.229169562458992\n",
      "Training loss for batch 2638 : 0.40083882212638855\n",
      "Training loss for batch 2639 : 0.04342343658208847\n",
      "Training loss for batch 2640 : 0.6067703366279602\n",
      "Training loss for batch 2641 : 0.1463155746459961\n",
      "Training loss for batch 2642 : 0.1656392514705658\n",
      "Training loss for batch 2643 : 0.3107687830924988\n",
      "Training loss for batch 2644 : 0.022503184154629707\n",
      "Training loss for batch 2645 : 0.09250400960445404\n",
      "Training loss for batch 2646 : 0.25871652364730835\n",
      "Training loss for batch 2647 : 0.15282587707042694\n",
      "Training loss for batch 2648 : 0.1512848138809204\n",
      "Training loss for batch 2649 : 0.40209123492240906\n",
      "Training loss for batch 2650 : 0.4734934866428375\n",
      "Training loss for batch 2651 : 0.28300607204437256\n",
      "Training loss for batch 2652 : 0.013728532940149307\n",
      "Training loss for batch 2653 : 0.23159416019916534\n",
      "Training loss for batch 2654 : 0.03789578005671501\n",
      "Training loss for batch 2655 : 0.4232761263847351\n",
      "Training loss for batch 2656 : 0.33479997515678406\n",
      "Training loss for batch 2657 : 0.42451533675193787\n",
      "Training loss for batch 2658 : 0.11795375496149063\n",
      "Training loss for batch 2659 : 0.17146633565425873\n",
      "Training loss for batch 2660 : 0.22730286419391632\n",
      "Training loss for batch 2661 : 0.14269094169139862\n",
      "Training loss for batch 2662 : 0.13232466578483582\n",
      "Training loss for batch 2663 : 0.0737847164273262\n",
      "Training loss for batch 2664 : 0.1055523231625557\n",
      "Training loss for batch 2665 : 0.20116232335567474\n",
      "Training loss for batch 2666 : 0.480337917804718\n",
      "Training loss for batch 2667 : 0.13528810441493988\n",
      "Training loss for batch 2668 : 0.3205389976501465\n",
      "Training loss for batch 2669 : 0.13574007153511047\n",
      "Training loss for batch 2670 : 0.19506923854351044\n",
      "Training loss for batch 2671 : 0.09478424489498138\n",
      "Training loss for batch 2672 : 0.3273603320121765\n",
      "Training loss for batch 2673 : 0.25946593284606934\n",
      "Training loss for batch 2674 : 0.24829909205436707\n",
      "Training loss for batch 2675 : 0.21007272601127625\n",
      "Training loss for batch 2676 : 0.1348486691713333\n",
      "Training loss for batch 2677 : 0.13323813676834106\n",
      "Training loss for batch 2678 : 0.5303038358688354\n",
      "Training loss for batch 2679 : 0.023831075057387352\n",
      "Training loss for batch 2680 : 0.17105083167552948\n",
      "Training loss for batch 2681 : 0.021257659420371056\n",
      "Training loss for batch 2682 : 0.09939923882484436\n",
      "Training loss for batch 2683 : 0.09203939884901047\n",
      "Training loss for batch 2684 : 0.386764258146286\n",
      "Training loss for batch 2685 : 0.21129286289215088\n",
      "Training loss for batch 2686 : 0.19928234815597534\n",
      "Training loss for batch 2687 : 0.19616222381591797\n",
      "Training loss for batch 2688 : 0.09022903442382812\n",
      "Training loss for batch 2689 : 0.23209284245967865\n",
      "Training loss for batch 2690 : 0.09651504456996918\n",
      "Training loss for batch 2691 : 0.2371080219745636\n",
      "Training loss for batch 2692 : 0.5729559659957886\n",
      "Training loss for batch 2693 : 0.5222280621528625\n",
      "Training loss for batch 2694 : 0.05940651893615723\n",
      "Training loss for batch 2695 : 0.06039631366729736\n",
      "Training loss for batch 2696 : 0.45440220832824707\n",
      "Training loss for batch 2697 : 0.15979672968387604\n",
      "Training loss for batch 2698 : 0.4689992368221283\n",
      "Training loss for batch 2699 : 0.07115455716848373\n",
      "Training loss for batch 2700 : 0.5692881345748901\n",
      "Training loss for batch 2701 : 0.10303690284490585\n",
      "Training loss for batch 2702 : 0.0454704724252224\n",
      "Training loss for batch 2703 : 0.48674342036247253\n",
      "Training loss for batch 2704 : 0.4285082221031189\n",
      "Training loss for batch 2705 : 0.4468858242034912\n",
      "Training loss for batch 2706 : 0.019111406058073044\n",
      "Training loss for batch 2707 : 0.06937329471111298\n",
      "Training loss for batch 2708 : 0.1268993616104126\n",
      "Training loss for batch 2709 : 0.2735045850276947\n",
      "Training loss for batch 2710 : 0.38659408688545227\n",
      "Training loss for batch 2711 : 0.25712698698043823\n",
      "Training loss for batch 2712 : 0.0048046777956187725\n",
      "Training loss for batch 2713 : 0.409665584564209\n",
      "Training loss for batch 2714 : 0.17289908230304718\n",
      "Training loss for batch 2715 : 0.018427422270178795\n",
      "Training loss for batch 2716 : 0.007070058491080999\n",
      "Training loss for batch 2717 : 0.08821439743041992\n",
      "Training loss for batch 2718 : 0.11087822169065475\n",
      "Training loss for batch 2719 : 0.3057463765144348\n",
      "Training loss for batch 2720 : 0.11278680711984634\n",
      "Training loss for batch 2721 : 0.4363006353378296\n",
      "Training loss for batch 2722 : 0.38439223170280457\n",
      "Training loss for batch 2723 : 0.18877539038658142\n",
      "Training loss for batch 2724 : 0.4620553255081177\n",
      "Training loss for batch 2725 : 0.33086511492729187\n",
      "Training loss for batch 2726 : 0.5005497932434082\n",
      "Training loss for batch 2727 : 0.16641531884670258\n",
      "Training loss for batch 2728 : 0.1221773624420166\n",
      "Training loss for batch 2729 : 0.3175974488258362\n",
      "Training loss for batch 2730 : 0.2669646143913269\n",
      "Training loss for batch 2731 : 0.24700558185577393\n",
      "Training loss for batch 2732 : 0.40138229727745056\n",
      "Training loss for batch 2733 : 0.3206542730331421\n",
      "Training loss for batch 2734 : 0.38607022166252136\n",
      "Training loss for batch 2735 : 0.3193054795265198\n",
      "Training loss for batch 2736 : 0.3534083664417267\n",
      "Training loss for batch 2737 : 0.31820186972618103\n",
      "Training loss for batch 2738 : 0.24902372062206268\n",
      "Training loss for batch 2739 : 0.5180587768554688\n",
      "Training loss for batch 2740 : 0.4207903742790222\n",
      "Training loss for batch 2741 : 0.1114225685596466\n",
      "Training loss for batch 2742 : 0.22021792829036713\n",
      "Training loss for batch 2743 : 0.22093160450458527\n",
      "Training loss for batch 2744 : 0.3978430926799774\n",
      "Training loss for batch 2745 : 0.19773828983306885\n",
      "Training loss for batch 2746 : 0.16324350237846375\n",
      "Training loss for batch 2747 : 0.11801454424858093\n",
      "Training loss for batch 2748 : 0.2148936539888382\n",
      "Training loss for batch 2749 : 0.13763132691383362\n",
      "Training loss for batch 2750 : 0.4419921040534973\n",
      "Training loss for batch 2751 : 0.1332932412624359\n",
      "Training loss for batch 2752 : 0.029821690171957016\n",
      "Training loss for batch 2753 : 0.38178062438964844\n",
      "Training loss for batch 2754 : 0.08113448321819305\n",
      "Training loss for batch 2755 : 0.5156142711639404\n",
      "Training loss for batch 2756 : 0.33435484766960144\n",
      "Training loss for batch 2757 : 0.30053073167800903\n",
      "Training loss for batch 2758 : 0.1655300259590149\n",
      "Training loss for batch 2759 : 0.03777690976858139\n",
      "Training loss for batch 2760 : 0.19586923718452454\n",
      "Training loss for batch 2761 : 0.2082768678665161\n",
      "Training loss for batch 2762 : 0.12337376177310944\n",
      "Training loss for batch 2763 : 0.08057039231061935\n",
      "Training loss for batch 2764 : 0.49635395407676697\n",
      "Training loss for batch 2765 : 5.441177199827507e-05\n",
      "Training loss for batch 2766 : 0.32555732131004333\n",
      "Training loss for batch 2767 : 0.42631012201309204\n",
      "Training loss for batch 2768 : 0.18659061193466187\n",
      "Training loss for batch 2769 : 0.3784436881542206\n",
      "Training loss for batch 2770 : 0.1451931595802307\n",
      "Training loss for batch 2771 : 0.24088935554027557\n",
      "Training loss for batch 2772 : 0.5013635754585266\n",
      "Training loss for batch 2773 : 0.2252551168203354\n",
      "Training loss for batch 2774 : 0.26136866211891174\n",
      "Training loss for batch 2775 : 0.30880624055862427\n",
      "Training loss for batch 2776 : 0.20721179246902466\n",
      "Training loss for batch 2777 : 0.16361089050769806\n",
      "Training loss for batch 2778 : 0.01521045807749033\n",
      "Training loss for batch 2779 : 0.16507621109485626\n",
      "Training loss for batch 2780 : 0.3466247618198395\n",
      "Training loss for batch 2781 : 0.2192634493112564\n",
      "Training loss for batch 2782 : 0.16550669074058533\n",
      "Training loss for batch 2783 : 0.5445927977561951\n",
      "Training loss for batch 2784 : 0.08907672762870789\n",
      "Training loss for batch 2785 : 0.09637511521577835\n",
      "Training loss for batch 2786 : 0.0690523311495781\n",
      "Training loss for batch 2787 : 0.11865677684545517\n",
      "Training loss for batch 2788 : 0.023482535034418106\n",
      "Training loss for batch 2789 : 0.13571912050247192\n",
      "Training loss for batch 2790 : 0.3822314143180847\n",
      "Training loss for batch 2791 : 0.37717586755752563\n",
      "Training loss for batch 2792 : 0.15225233137607574\n",
      "Training loss for batch 2793 : 0.294164776802063\n",
      "Training loss for batch 2794 : 0.09044086188077927\n",
      "Training loss for batch 2795 : 0.15869778394699097\n",
      "Training loss for batch 2796 : 0.13553330302238464\n",
      "Training loss for batch 2797 : 0.15870261192321777\n",
      "Training loss for batch 2798 : 0.03826584666967392\n",
      "Training loss for batch 2799 : 0.13779139518737793\n",
      "Training loss for batch 2800 : 0.128130242228508\n",
      "Training loss for batch 2801 : 0.19108174741268158\n",
      "Training loss for batch 2802 : 0.33404016494750977\n",
      "Training loss for batch 2803 : 0.146300807595253\n",
      "Training loss for batch 2804 : 0.3147754371166229\n",
      "Training loss for batch 2805 : 0.2931331992149353\n",
      "Training loss for batch 2806 : 0.04595344513654709\n",
      "Training loss for batch 2807 : 0.2959558367729187\n",
      "Training loss for batch 2808 : 0.4829273521900177\n",
      "Training loss for batch 2809 : 0.3795461654663086\n",
      "Training loss for batch 2810 : 0.044085532426834106\n",
      "Training loss for batch 2811 : 0.29539215564727783\n",
      "Training loss for batch 2812 : 0.5203339457511902\n",
      "Training loss for batch 2813 : 0.3276841938495636\n",
      "Training loss for batch 2814 : 0.043243277817964554\n",
      "Training loss for batch 2815 : 0.10509782284498215\n",
      "Training loss for batch 2816 : 0.21323008835315704\n",
      "Training loss for batch 2817 : 0.26260069012641907\n",
      "Training loss for batch 2818 : 0.33418071269989014\n",
      "Training loss for batch 2819 : 0.2847472131252289\n",
      "Training loss for batch 2820 : 0.3649478256702423\n",
      "Training loss for batch 2821 : 0.15776073932647705\n",
      "Training loss for batch 2822 : 0.2217811942100525\n",
      "Training loss for batch 2823 : 0.01921406388282776\n",
      "Training loss for batch 2824 : 0.13167499005794525\n",
      "Training loss for batch 2825 : 0.24730950593948364\n",
      "Training loss for batch 2826 : 0.35599762201309204\n",
      "Training loss for batch 2827 : 0.0993293896317482\n",
      "Training loss for batch 2828 : 0.10449060797691345\n",
      "Training loss for batch 2829 : 0.03902527689933777\n",
      "Training loss for batch 2830 : 0.16457027196884155\n",
      "Training loss for batch 2831 : 0.22178958356380463\n",
      "Training loss for batch 2832 : 0.08682030439376831\n",
      "Training loss for batch 2833 : 0.5896565914154053\n",
      "Training loss for batch 2834 : 0.16524408757686615\n",
      "Training loss for batch 2835 : 0.12749642133712769\n",
      "Training loss for batch 2836 : 0.09095519781112671\n",
      "Training loss for batch 2837 : 0.32880091667175293\n",
      "Training loss for batch 2838 : 0.5703608989715576\n",
      "Training loss for batch 2839 : 0.05342051386833191\n",
      "Training loss for batch 2840 : 0.1601872742176056\n",
      "Training loss for batch 2841 : 0.21482649445533752\n",
      "Training loss for batch 2842 : 0.3271688222885132\n",
      "Training loss for batch 2843 : 0.06432834267616272\n",
      "Training loss for batch 2844 : 0.10684829950332642\n",
      "Training loss for batch 2845 : 0.10245024412870407\n",
      "Training loss for batch 2846 : 0.11903679370880127\n",
      "Training loss for batch 2847 : 0.21178478002548218\n",
      "Training loss for batch 2848 : 0.13940928876399994\n",
      "Training loss for batch 2849 : 0.07489603757858276\n",
      "Training loss for batch 2850 : 0.046197544783353806\n",
      "Training loss for batch 2851 : 0.28483065962791443\n",
      "Training loss for batch 2852 : 0.32713326811790466\n",
      "Training loss for batch 2853 : 0.09151174873113632\n",
      "Training loss for batch 2854 : 0.3500620126724243\n",
      "Training loss for batch 2855 : 0.33587825298309326\n",
      "Training loss for batch 2856 : 0.3296588659286499\n",
      "Training loss for batch 2857 : 0.07665815204381943\n",
      "Training loss for batch 2858 : 0.16213613748550415\n",
      "Training loss for batch 2859 : 0.2445826232433319\n",
      "Training loss for batch 2860 : 0.5696699023246765\n",
      "Training loss for batch 2861 : 0.534060537815094\n",
      "Training loss for batch 2862 : 0.296042263507843\n",
      "Training loss for batch 2863 : 0.3125166594982147\n",
      "Training loss for batch 2864 : 0.12561103701591492\n",
      "Training loss for batch 2865 : 0.00012000169954262674\n",
      "Training loss for batch 2866 : 0.9395225048065186\n",
      "Training loss for batch 2867 : 0.30731791257858276\n",
      "Training loss for batch 2868 : 0.3599831759929657\n",
      "Training loss for batch 2869 : 0.7969504594802856\n",
      "Training loss for batch 2870 : 0.38568630814552307\n",
      "Training loss for batch 2871 : 0.44441983103752136\n",
      "Training loss for batch 2872 : 0.23335640132427216\n",
      "Training loss for batch 2873 : 0.4685730040073395\n",
      "Training loss for batch 2874 : 0.4418753683567047\n",
      "Training loss for batch 2875 : 0.037580203264951706\n",
      "Training loss for batch 2876 : 0.30607056617736816\n",
      "Training loss for batch 2877 : 0.28394415974617004\n",
      "Training loss for batch 2878 : 0.0197043027728796\n",
      "Training loss for batch 2879 : 0.15432652831077576\n",
      "Training loss for batch 2880 : 0.1348704993724823\n",
      "Training loss for batch 2881 : 0.15911291539669037\n",
      "Training loss for batch 2882 : 0.17954814434051514\n",
      "Training loss for batch 2883 : 0.38899198174476624\n",
      "Training loss for batch 2884 : 0.06075583025813103\n",
      "Training loss for batch 2885 : 0.20893405377864838\n",
      "Training loss for batch 2886 : 0.10462900996208191\n",
      "Training loss for batch 2887 : 0.4983084201812744\n",
      "Training loss for batch 2888 : 0.17886218428611755\n",
      "Training loss for batch 2889 : 0.15895380079746246\n",
      "Training loss for batch 2890 : 0.15071626007556915\n",
      "Training loss for batch 2891 : 0.022540241479873657\n",
      "Training loss for batch 2892 : 0.25447508692741394\n",
      "Training loss for batch 2893 : 0.3174428939819336\n",
      "Training loss for batch 2894 : 0.09117356687784195\n",
      "Training loss for batch 2895 : 0.1724325716495514\n",
      "Training loss for batch 2896 : 0.03685375303030014\n",
      "Training loss for batch 2897 : 0.16500988602638245\n",
      "Training loss for batch 2898 : 0.14010140299797058\n",
      "Training loss for batch 2899 : 0.2851373851299286\n",
      "Training loss for batch 2900 : 0.28740745782852173\n",
      "Training loss for batch 2901 : 0.2431715726852417\n",
      "Training loss for batch 2902 : 0.3095700442790985\n",
      "Training loss for batch 2903 : 0.17822793126106262\n",
      "Training loss for batch 2904 : 0.141392782330513\n",
      "Training loss for batch 2905 : 0.400204062461853\n",
      "Training loss for batch 2906 : 0.3146713376045227\n",
      "Training loss for batch 2907 : 0.09880349040031433\n",
      "Training loss for batch 2908 : 0.07818623632192612\n",
      "Training loss for batch 2909 : 0.06536417454481125\n",
      "Training loss for batch 2910 : 0.43844521045684814\n",
      "Training loss for batch 2911 : 0.11197376251220703\n",
      "Training loss for batch 2912 : 0.20770907402038574\n",
      "Training loss for batch 2913 : 0.06844450533390045\n",
      "Training loss for batch 2914 : 0.3920283913612366\n",
      "Training loss for batch 2915 : 0.22106018662452698\n",
      "Training loss for batch 2916 : 0.13638389110565186\n",
      "Training loss for batch 2917 : 0.41973206400871277\n",
      "Training loss for batch 2918 : 0.20389385521411896\n",
      "Training loss for batch 2919 : 0.10766381025314331\n",
      "Training loss for batch 2920 : 0.07204854488372803\n",
      "Training loss for batch 2921 : 0.06290978938341141\n",
      "Training loss for batch 2922 : 0.02778766304254532\n",
      "Training loss for batch 2923 : 0.2850334048271179\n",
      "Training loss for batch 2924 : 0.20796610414981842\n",
      "Training loss for batch 2925 : 0.06344189494848251\n",
      "Training loss for batch 2926 : 0.10646156966686249\n",
      "Training loss for batch 2927 : 0.26286283135414124\n",
      "Training loss for batch 2928 : 0.17396703362464905\n",
      "Training loss for batch 2929 : 0.22751882672309875\n",
      "Training loss for batch 2930 : 0.4462265968322754\n",
      "Training loss for batch 2931 : 0.1680535078048706\n",
      "Training loss for batch 2932 : 0.07297255098819733\n",
      "Training loss for batch 2933 : 0.2351115345954895\n",
      "Training loss for batch 2934 : 0.006490389816462994\n",
      "Training loss for batch 2935 : 1.025745153427124\n",
      "Training loss for batch 2936 : 0.03772689774632454\n",
      "Training loss for batch 2937 : 0.16923470795154572\n",
      "Training loss for batch 2938 : 0.02172839641571045\n",
      "Training loss for batch 2939 : 0.28329214453697205\n",
      "Training loss for batch 2940 : 0.03780553862452507\n",
      "Training loss for batch 2941 : 0.23824304342269897\n",
      "Training loss for batch 2942 : 0.16777871549129486\n",
      "Training loss for batch 2943 : 0.10904897749423981\n",
      "Training loss for batch 2944 : 0.21666841208934784\n",
      "Training loss for batch 2945 : 0.28603285551071167\n",
      "Training loss for batch 2946 : 0.2347191721200943\n",
      "Training loss for batch 2947 : 0.2868027985095978\n",
      "Training loss for batch 2948 : 0.38044074177742004\n",
      "Training loss for batch 2949 : 0.3005172610282898\n",
      "Training loss for batch 2950 : 0.2973487377166748\n",
      "Training loss for batch 2951 : 0.14779409766197205\n",
      "Training loss for batch 2952 : 0.1667739748954773\n",
      "Training loss for batch 2953 : 0.16720129549503326\n",
      "Training loss for batch 2954 : 0.23998524248600006\n",
      "Training loss for batch 2955 : 0.026821319013834\n",
      "Training loss for batch 2956 : 0.1151174008846283\n",
      "Training loss for batch 2957 : 0.24622131884098053\n",
      "Training loss for batch 2958 : 0.16133929789066315\n",
      "Training loss for batch 2959 : 0.2342166006565094\n",
      "Training loss for batch 2960 : 0.05791756510734558\n",
      "Training loss for batch 2961 : 0.08522384613752365\n",
      "Training loss for batch 2962 : 0.18831272423267365\n",
      "Training loss for batch 2963 : 0.03162427991628647\n",
      "Training loss for batch 2964 : 0.09338612109422684\n",
      "Training loss for batch 2965 : 0.08328969031572342\n",
      "Training loss for batch 2966 : 0.11080651730298996\n",
      "Training loss for batch 2967 : 0.16056233644485474\n",
      "Training loss for batch 2968 : 0.2546142637729645\n",
      "Training loss for batch 2969 : 0.28759464621543884\n",
      "Training loss for batch 2970 : 0.5382463932037354\n",
      "Training loss for batch 2971 : 0.11130739003419876\n",
      "Training loss for batch 2972 : 0.0018608735408633947\n",
      "Training loss for batch 2973 : 0.15245550870895386\n",
      "Training loss for batch 2974 : 0.020187167450785637\n",
      "Training loss for batch 2975 : 0.18820394575595856\n",
      "Training loss for batch 2976 : 0.04864397272467613\n",
      "Training loss for batch 2977 : 0.062454018741846085\n",
      "Training loss for batch 2978 : 0.6765012145042419\n",
      "Training loss for batch 2979 : 0.119253970682621\n",
      "Training loss for batch 2980 : 0.18321558833122253\n",
      "Training loss for batch 2981 : 0.17315484583377838\n",
      "Training loss for batch 2982 : 0.13404163718223572\n",
      "Training loss for batch 2983 : 0.22591783106327057\n",
      "Training loss for batch 2984 : 0.12347009778022766\n",
      "Training loss for batch 2985 : 0.37696221470832825\n",
      "Training loss for batch 2986 : 0.48506638407707214\n",
      "Training loss for batch 2987 : 0.046647846698760986\n",
      "Training loss for batch 2988 : 0.04284249246120453\n",
      "Training loss for batch 2989 : 0.20494966208934784\n",
      "Training loss for batch 2990 : 0.3026738464832306\n",
      "Training loss for batch 2991 : 0.3063461184501648\n",
      "Training loss for batch 2992 : 0.19162651896476746\n",
      "Training loss for batch 2993 : 0.4315677583217621\n",
      "Training loss for batch 2994 : 0.23203720152378082\n",
      "Training loss for batch 2995 : 0.04149053990840912\n",
      "Training loss for batch 2996 : 0.10940057784318924\n",
      "Training loss for batch 2997 : 0.21715055406093597\n",
      "Training loss for batch 2998 : 0.028176072984933853\n",
      "Training loss for batch 2999 : 0.3479746878147125\n",
      "Training loss for batch 3000 : 0.030612865462899208\n",
      "Training loss for batch 3001 : 0.05701599270105362\n",
      "Training loss for batch 3002 : 0.343290239572525\n",
      "Training loss for batch 3003 : 0.21449874341487885\n",
      "Training loss for batch 3004 : 0.5706220269203186\n",
      "Training loss for batch 3005 : 0.10640908777713776\n",
      "Training loss for batch 3006 : 0.19221507012844086\n",
      "Training loss for batch 3007 : 0.22404663264751434\n",
      "Training loss for batch 3008 : 0.07856639474630356\n",
      "Training loss for batch 3009 : 0.14048981666564941\n",
      "Training loss for batch 3010 : 0.4612518846988678\n",
      "Training loss for batch 3011 : 0.1608504056930542\n",
      "Training loss for batch 3012 : 0.29161760210990906\n",
      "Training loss for batch 3013 : 0.09846288710832596\n",
      "Training loss for batch 3014 : 0.22395190596580505\n",
      "Training loss for batch 3015 : 0.3958277106285095\n",
      "Training loss for batch 3016 : 0.13112060725688934\n",
      "Training loss for batch 3017 : 0.09130730479955673\n",
      "Training loss for batch 3018 : 0.25870266556739807\n",
      "Training loss for batch 3019 : 0.24850396811962128\n",
      "Training loss for batch 3020 : 0.3133719563484192\n",
      "Training loss for batch 3021 : 0.21088719367980957\n",
      "Training loss for batch 3022 : 0.14507736265659332\n",
      "Training loss for batch 3023 : 0.2222440242767334\n",
      "Training loss for batch 3024 : 0.05168087035417557\n",
      "Training loss for batch 3025 : 0.2002062052488327\n",
      "Training loss for batch 3026 : 0.3538881242275238\n",
      "Training loss for batch 3027 : 0.18568731844425201\n",
      "Training loss for batch 3028 : 0.6080092191696167\n",
      "Training loss for batch 3029 : 0.23884348571300507\n",
      "Training loss for batch 3030 : 0.15669867396354675\n",
      "Training loss for batch 3031 : 0.1785426139831543\n",
      "Training loss for batch 3032 : 0.08373328298330307\n",
      "Training loss for batch 3033 : 0.3617423176765442\n",
      "Training loss for batch 3034 : 0.10426810383796692\n",
      "Training loss for batch 3035 : 0.3858577013015747\n",
      "Training loss for batch 3036 : 0.41815152764320374\n",
      "Training loss for batch 3037 : 0.11056175827980042\n",
      "Training loss for batch 3038 : 0.5514629483222961\n",
      "Training loss for batch 3039 : 0.08905832469463348\n",
      "Training loss for batch 3040 : 0.1848861575126648\n",
      "Training loss for batch 3041 : 0.30541589856147766\n",
      "Training loss for batch 3042 : 0.3459697663784027\n",
      "Training loss for batch 3043 : 0.08535413444042206\n",
      "Training loss for batch 3044 : 0.3127497434616089\n",
      "Training loss for batch 3045 : 0.27993303537368774\n",
      "Training loss for batch 3046 : 0.17726823687553406\n",
      "Training loss for batch 3047 : 0.04821467027068138\n",
      "Training loss for batch 3048 : 0.0200507752597332\n",
      "Training loss for batch 3049 : 0.0795990452170372\n",
      "Training loss for batch 3050 : 0.3297564387321472\n",
      "Training loss for batch 3051 : 0.38285210728645325\n",
      "Training loss for batch 3052 : 0.1304222196340561\n",
      "Training loss for batch 3053 : 0.17198660969734192\n",
      "Training loss for batch 3054 : 0.09301183372735977\n",
      "Training loss for batch 3055 : 0.22495011985301971\n",
      "Training loss for batch 3056 : 0.6052323579788208\n",
      "Training loss for batch 3057 : 0.19831179082393646\n",
      "Training loss for batch 3058 : 0.31092989444732666\n",
      "Training loss for batch 3059 : 0.13634149730205536\n",
      "Training loss for batch 3060 : 0.031027283519506454\n",
      "Training loss for batch 3061 : 0.2620212733745575\n",
      "Training loss for batch 3062 : 0.38445961475372314\n",
      "Training loss for batch 3063 : 0.09328396618366241\n",
      "Training loss for batch 3064 : 0.37864524126052856\n",
      "Training loss for batch 3065 : 0.22220472991466522\n",
      "Training loss for batch 3066 : 0.03040262870490551\n",
      "Training loss for batch 3067 : 0.4132835566997528\n",
      "Training loss for batch 3068 : 0.38742223381996155\n",
      "Training loss for batch 3069 : 0.12891265749931335\n",
      "Training loss for batch 3070 : 0.28631216287612915\n",
      "Training loss for batch 3071 : 0.19035103917121887\n",
      "Training loss for batch 3072 : 0.08183947205543518\n",
      "Training loss for batch 3073 : 0.11625196039676666\n",
      "Training loss for batch 3074 : 0.0656946450471878\n",
      "Training loss for batch 3075 : 0.272278755903244\n",
      "Training loss for batch 3076 : 0.18573138117790222\n",
      "Training loss for batch 3077 : 0.2538203299045563\n",
      "Training loss for batch 3078 : 0.36985790729522705\n",
      "Training loss for batch 3079 : 0.3165212571620941\n",
      "Training loss for batch 3080 : 0.09331634640693665\n",
      "Training loss for batch 3081 : 0.29984593391418457\n",
      "Training loss for batch 3082 : 0.19545483589172363\n",
      "Training loss for batch 3083 : 0.051121264696121216\n",
      "Training loss for batch 3084 : 0.31479185819625854\n",
      "Training loss for batch 3085 : 0.2928343117237091\n",
      "Training loss for batch 3086 : 0.3541150689125061\n",
      "Training loss for batch 3087 : 0.23072533309459686\n",
      "Training loss for batch 3088 : 0.0756758451461792\n",
      "Training loss for batch 3089 : 0.12295622378587723\n",
      "Training loss for batch 3090 : 0.26580438017845154\n",
      "Training loss for batch 3091 : 0.08627443015575409\n",
      "Training loss for batch 3092 : 0.06467025727033615\n",
      "Training loss for batch 3093 : 0.33942073583602905\n",
      "Training loss for batch 3094 : 0.0010511125437915325\n",
      "Training loss for batch 3095 : 0.1419477015733719\n",
      "Training loss for batch 3096 : 0.4860475957393646\n",
      "Training loss for batch 3097 : 0.20361410081386566\n",
      "Training loss for batch 3098 : 0.09558706730604172\n",
      "Training loss for batch 3099 : 0.4253963828086853\n",
      "Training loss for batch 3100 : 0.05820492282509804\n",
      "Training loss for batch 3101 : 0.4986790716648102\n",
      "Training loss for batch 3102 : 0.18361493945121765\n",
      "Training loss for batch 3103 : 0.16117249429225922\n",
      "Training loss for batch 3104 : 0.004061952233314514\n",
      "Training loss for batch 3105 : 0.20421607792377472\n",
      "Training loss for batch 3106 : 0.08206477761268616\n",
      "Training loss for batch 3107 : 0.07484570890665054\n",
      "Training loss for batch 3108 : 0.11055534332990646\n",
      "Training loss for batch 3109 : 0.3054901361465454\n",
      "Training loss for batch 3110 : 0.09796568751335144\n",
      "Training loss for batch 3111 : 0.22635547816753387\n",
      "Training loss for batch 3112 : 0.08500852435827255\n",
      "Training loss for batch 3113 : 0.09060104191303253\n",
      "Training loss for batch 3114 : 0.06987205892801285\n",
      "Training loss for batch 3115 : 0.17478467524051666\n",
      "Training loss for batch 3116 : 0.17325162887573242\n",
      "Training loss for batch 3117 : 0.20502059161663055\n",
      "Training loss for batch 3118 : 0.731086254119873\n",
      "Training loss for batch 3119 : 0.25717470049858093\n",
      "Training loss for batch 3120 : 0.19829532504081726\n",
      "Training loss for batch 3121 : 0.1831611692905426\n",
      "Training loss for batch 3122 : 0.15422190725803375\n",
      "Training loss for batch 3123 : 0.051707081496715546\n",
      "Training loss for batch 3124 : 0.24142314493656158\n",
      "Training loss for batch 3125 : 0.10533370077610016\n",
      "Training loss for batch 3126 : 0.08325750380754471\n",
      "Training loss for batch 3127 : 0.10641907155513763\n",
      "Training loss for batch 3128 : 0.23332896828651428\n",
      "Training loss for batch 3129 : 0.05264393612742424\n",
      "Training loss for batch 3130 : 0.09408363699913025\n",
      "Training loss for batch 3131 : 0.12207073718309402\n",
      "Training loss for batch 3132 : 0.1831999123096466\n",
      "Training loss for batch 3133 : 0.4931498169898987\n",
      "Training loss for batch 3134 : 0.07622890919446945\n",
      "Training loss for batch 3135 : 0.01928543671965599\n",
      "Training loss for batch 3136 : 0.5902116298675537\n",
      "Training loss for batch 3137 : 0.12127796560525894\n",
      "Training loss for batch 3138 : 0.14845652878284454\n",
      "Training loss for batch 3139 : 0.10641533881425858\n",
      "Training loss for batch 3140 : 0.07197599112987518\n",
      "Training loss for batch 3141 : 0.2942914366722107\n",
      "Training loss for batch 3142 : 0.5001378655433655\n",
      "Training loss for batch 3143 : 0.23850150406360626\n",
      "Training loss for batch 3144 : 0.26847440004348755\n",
      "Training loss for batch 3145 : 0.11727973073720932\n",
      "Training loss for batch 3146 : 0.349485844373703\n",
      "Training loss for batch 3147 : 0.09327101707458496\n",
      "Training loss for batch 3148 : 0.3154809772968292\n",
      "Training loss for batch 3149 : 0.3344855308532715\n",
      "Training loss for batch 3150 : 0.08493047207593918\n",
      "Training loss for batch 3151 : 0.08758926391601562\n",
      "Training loss for batch 3152 : 0.31613993644714355\n",
      "Training loss for batch 3153 : 0.031073085963726044\n",
      "Training loss for batch 3154 : 0.2522087097167969\n",
      "Training loss for batch 3155 : 0.3079778850078583\n",
      "Training loss for batch 3156 : 0.18727992475032806\n",
      "Training loss for batch 3157 : 0.08255232125520706\n",
      "Training loss for batch 3158 : 0.2676374614238739\n",
      "Training loss for batch 3159 : 0.12036624550819397\n",
      "Training loss for batch 3160 : 0.08397780358791351\n",
      "Training loss for batch 3161 : 0.24879930913448334\n",
      "Training loss for batch 3162 : 0.1299114227294922\n",
      "Training loss for batch 3163 : 0.30750590562820435\n",
      "Training loss for batch 3164 : 0.2972152531147003\n",
      "Training loss for batch 3165 : 0.00450731348246336\n",
      "Training loss for batch 3166 : 0.30952370166778564\n",
      "Training loss for batch 3167 : 0.23255567252635956\n",
      "Training loss for batch 3168 : 0.3268730938434601\n",
      "Training loss for batch 3169 : 0.3331030309200287\n",
      "Training loss for batch 3170 : 0.1919783353805542\n",
      "Training loss for batch 3171 : 0.1359694004058838\n",
      "Training loss for batch 3172 : 0.22218851745128632\n",
      "Training loss for batch 3173 : 0.3036598265171051\n",
      "Training loss for batch 3174 : 0.23740410804748535\n",
      "Training loss for batch 3175 : 0.19576294720172882\n",
      "Training loss for batch 3176 : 0.4069129526615143\n",
      "Training loss for batch 3177 : 0.05823870003223419\n",
      "Training loss for batch 3178 : 0.27584266662597656\n",
      "Training loss for batch 3179 : 0.19496996700763702\n",
      "Training loss for batch 3180 : 0.0974256619811058\n",
      "Training loss for batch 3181 : 0.061451517045497894\n",
      "Training loss for batch 3182 : 0.3048025965690613\n",
      "Training loss for batch 3183 : 0.16027452051639557\n",
      "Training loss for batch 3184 : 0.1299724578857422\n",
      "Training loss for batch 3185 : 0.29895246028900146\n",
      "Training loss for batch 3186 : 0.48268648982048035\n",
      "Training loss for batch 3187 : 0.30157962441444397\n",
      "Training loss for batch 3188 : 0.232042595744133\n",
      "Training loss for batch 3189 : 0.4019758105278015\n",
      "Training loss for batch 3190 : 0.0730699971318245\n",
      "Training loss for batch 3191 : 0.28824466466903687\n",
      "Training loss for batch 3192 : 0.6669228076934814\n",
      "Training loss for batch 3193 : 0.3402203321456909\n",
      "Training loss for batch 3194 : 0.2362077385187149\n",
      "Training loss for batch 3195 : 0.46518534421920776\n",
      "Training loss for batch 3196 : 0.21406468749046326\n",
      "Training loss for batch 3197 : 0.2307603806257248\n",
      "Training loss for batch 3198 : 0.14380426704883575\n",
      "Training loss for batch 3199 : 0.6392810344696045\n",
      "Training loss for batch 3200 : 0.21931278705596924\n",
      "Training loss for batch 3201 : 0.0515725314617157\n",
      "Training loss for batch 3202 : 0.035451460629701614\n",
      "Training loss for batch 3203 : 0.2228785902261734\n",
      "Training loss for batch 3204 : 0.03497195616364479\n",
      "Training loss for batch 3205 : 0.0033002642448991537\n",
      "Training loss for batch 3206 : 0.11372533440589905\n",
      "Training loss for batch 3207 : 0.04805281385779381\n",
      "Training loss for batch 3208 : 0.0\n",
      "Training loss for batch 3209 : 0.07526501268148422\n",
      "Training loss for batch 3210 : 0.48453351855278015\n",
      "Training loss for batch 3211 : 0.15342746675014496\n",
      "Training loss for batch 3212 : 0.09423938393592834\n",
      "Training loss for batch 3213 : 0.33464476466178894\n",
      "Training loss for batch 3214 : 0.043350700289011\n",
      "Training loss for batch 3215 : 0.23764051496982574\n",
      "Training loss for batch 3216 : 0.3524310886859894\n",
      "Training loss for batch 3217 : 0.19279049336910248\n",
      "Training loss for batch 3218 : 0.019197842106223106\n",
      "Training loss for batch 3219 : 0.22420039772987366\n",
      "Training loss for batch 3220 : 0.296178936958313\n",
      "Training loss for batch 3221 : 0.05383145436644554\n",
      "Training loss for batch 3222 : 0.10483911633491516\n",
      "Training loss for batch 3223 : 0.0413687638938427\n",
      "Training loss for batch 3224 : 0.02124049700796604\n",
      "Training loss for batch 3225 : 0.07218138873577118\n",
      "Training loss for batch 3226 : 0.3605409264564514\n",
      "Training loss for batch 3227 : 0.24182280898094177\n",
      "Training loss for batch 3228 : 0.23911625146865845\n",
      "Training loss for batch 3229 : 0.5434884428977966\n",
      "Training loss for batch 3230 : 0.23527047038078308\n",
      "Training loss for batch 3231 : 0.11737339943647385\n",
      "Training loss for batch 3232 : 0.16356274485588074\n",
      "Training loss for batch 3233 : 0.1578601747751236\n",
      "Training loss for batch 3234 : 0.4018738865852356\n",
      "Training loss for batch 3235 : 0.08167202770709991\n",
      "Training loss for batch 3236 : 0.2792988121509552\n",
      "Training loss for batch 3237 : 0.10374680906534195\n",
      "Training loss for batch 3238 : 0.3500128984451294\n",
      "Training loss for batch 3239 : 0.429472416639328\n",
      "Training loss for batch 3240 : 0.207381471991539\n",
      "Training loss for batch 3241 : 0.05024366080760956\n",
      "Training loss for batch 3242 : 0.1195538192987442\n",
      "Training loss for batch 3243 : 0.2755570709705353\n",
      "Training loss for batch 3244 : 0.10267619788646698\n",
      "Training loss for batch 3245 : 0.07696755975484848\n",
      "Training loss for batch 3246 : 0.043524399399757385\n",
      "Training loss for batch 3247 : 0.17048412561416626\n",
      "Training loss for batch 3248 : 0.09288230538368225\n",
      "Training loss for batch 3249 : 0.29875174164772034\n",
      "Training loss for batch 3250 : 0.19083748757839203\n",
      "Training loss for batch 3251 : 0.47063755989074707\n",
      "Training loss for batch 3252 : 0.29681530594825745\n",
      "Training loss for batch 3253 : 0.563126802444458\n",
      "Training loss for batch 3254 : 0.038008496165275574\n",
      "Training loss for batch 3255 : 0.21329689025878906\n",
      "Training loss for batch 3256 : 0.35937491059303284\n",
      "Training loss for batch 3257 : 0.24234513938426971\n",
      "Training loss for batch 3258 : 0.07987219095230103\n",
      "Training loss for batch 3259 : 0.30236533284187317\n",
      "Training loss for batch 3260 : 0.060884278267621994\n",
      "Training loss for batch 3261 : 0.16841140389442444\n",
      "Training loss for batch 3262 : 0.10144853591918945\n",
      "Training loss for batch 3263 : 0.15557876229286194\n",
      "Training loss for batch 3264 : 0.4383781850337982\n",
      "Training loss for batch 3265 : 0.27182772755622864\n",
      "Training loss for batch 3266 : 0.04646020010113716\n",
      "Training loss for batch 3267 : 1.1040366888046265\n",
      "Training loss for batch 3268 : 0.13833586871623993\n",
      "Training loss for batch 3269 : 0.12813115119934082\n",
      "Training loss for batch 3270 : 0.2672666907310486\n",
      "Training loss for batch 3271 : 0.12167637050151825\n",
      "Training loss for batch 3272 : 0.18076826632022858\n",
      "Training loss for batch 3273 : 0.12685073912143707\n",
      "Training loss for batch 3274 : 0.1746637374162674\n",
      "Training loss for batch 3275 : 0.1166597530245781\n",
      "Training loss for batch 3276 : 0.07899400591850281\n",
      "Training loss for batch 3277 : 0.02841561660170555\n",
      "Training loss for batch 3278 : 0.004392080008983612\n",
      "Training loss for batch 3279 : 0.047472260892391205\n",
      "Training loss for batch 3280 : 0.1958673596382141\n",
      "Training loss for batch 3281 : 0.325936496257782\n",
      "Training loss for batch 3282 : 0.377828449010849\n",
      "Training loss for batch 3283 : 0.37989991903305054\n",
      "Training loss for batch 3284 : 0.2357841581106186\n",
      "Training loss for batch 3285 : 0.6786165833473206\n",
      "Training loss for batch 3286 : 0.03655357286334038\n",
      "Training loss for batch 3287 : 0.07426879554986954\n",
      "Training loss for batch 3288 : 0.2648269534111023\n",
      "Training loss for batch 3289 : 0.23298905789852142\n",
      "Training loss for batch 3290 : 0.2130923867225647\n",
      "Training loss for batch 3291 : 0.26477357745170593\n",
      "Training loss for batch 3292 : 0.4864066243171692\n",
      "Training loss for batch 3293 : 0.44140422344207764\n",
      "Training loss for batch 3294 : 0.17162369191646576\n",
      "Training loss for batch 3295 : 0.20463459193706512\n",
      "Training loss for batch 3296 : 0.11982931196689606\n",
      "Training loss for batch 3297 : 0.6337440013885498\n",
      "Training loss for batch 3298 : 0.10227519273757935\n",
      "Training loss for batch 3299 : 0.22453522682189941\n",
      "Training loss for batch 3300 : 0.12353131920099258\n",
      "Training loss for batch 3301 : 0.2532842755317688\n",
      "Training loss for batch 3302 : 0.17195537686347961\n",
      "Training loss for batch 3303 : 0.34939077496528625\n",
      "Training loss for batch 3304 : 0.08406156301498413\n",
      "Training loss for batch 3305 : 0.2314230501651764\n",
      "Training loss for batch 3306 : 0.0603477768599987\n",
      "Training loss for batch 3307 : 0.1395995020866394\n",
      "Training loss for batch 3308 : 0.0066848876886069775\n",
      "Training loss for batch 3309 : 0.3287765383720398\n",
      "Training loss for batch 3310 : 0.3375285267829895\n",
      "Training loss for batch 3311 : 0.009508928284049034\n",
      "Training loss for batch 3312 : 0.13388681411743164\n",
      "Training loss for batch 3313 : 0.4442884922027588\n",
      "Training loss for batch 3314 : 0.22557078301906586\n",
      "Training loss for batch 3315 : 0.015142927877604961\n",
      "Training loss for batch 3316 : 0.22830791771411896\n",
      "Training loss for batch 3317 : 0.05311495438218117\n",
      "Training loss for batch 3318 : 0.14582069218158722\n",
      "Training loss for batch 3319 : 0.11206470429897308\n",
      "Training loss for batch 3320 : 0.03995904326438904\n",
      "Training loss for batch 3321 : 0.3760702311992645\n",
      "Training loss for batch 3322 : 0.10607600212097168\n",
      "Training loss for batch 3323 : 0.035833317786455154\n",
      "Training loss for batch 3324 : 0.16874966025352478\n",
      "Training loss for batch 3325 : 0.05279389023780823\n",
      "Training loss for batch 3326 : 0.09200851619243622\n",
      "Training loss for batch 3327 : 0.002382735488936305\n",
      "Training loss for batch 3328 : 0.4185730218887329\n",
      "Training loss for batch 3329 : 0.004421816673129797\n",
      "Training loss for batch 3330 : 0.05494184046983719\n",
      "Training loss for batch 3331 : 0.3430251479148865\n",
      "Training loss for batch 3332 : 0.1881047934293747\n",
      "Training loss for batch 3333 : 0.2998311221599579\n",
      "Training loss for batch 3334 : 0.7965234518051147\n",
      "Training loss for batch 3335 : 0.36572498083114624\n",
      "Training loss for batch 3336 : 0.05622822046279907\n",
      "Training loss for batch 3337 : 0.025194477289915085\n",
      "Training loss for batch 3338 : 0.1180247887969017\n",
      "Training loss for batch 3339 : 0.333843469619751\n",
      "Training loss for batch 3340 : 0.2707626521587372\n",
      "Training loss for batch 3341 : 0.25291818380355835\n",
      "Training loss for batch 3342 : 0.08696451783180237\n",
      "Training loss for batch 3343 : 0.26301679015159607\n",
      "Training loss for batch 3344 : 0.3589699864387512\n",
      "Training loss for batch 3345 : 0.36464032530784607\n",
      "Training loss for batch 3346 : 0.14896360039710999\n",
      "Training loss for batch 3347 : 0.04636185243725777\n",
      "Training loss for batch 3348 : 0.048195961862802505\n",
      "Training loss for batch 3349 : 0.1538757085800171\n",
      "Training loss for batch 3350 : 0.10006755590438843\n",
      "Training loss for batch 3351 : 0.13434723019599915\n",
      "Training loss for batch 3352 : 0.3316449522972107\n",
      "Training loss for batch 3353 : 0.004674205090850592\n",
      "Training loss for batch 3354 : 0.26665255427360535\n",
      "Training loss for batch 3355 : 0.006223718635737896\n",
      "Training loss for batch 3356 : 0.059166520833969116\n",
      "Training loss for batch 3357 : 0.014866352081298828\n",
      "Training loss for batch 3358 : 0.5249903798103333\n",
      "Training loss for batch 3359 : 0.2478419691324234\n",
      "Training loss for batch 3360 : 0.5045695900917053\n",
      "Training loss for batch 3361 : 0.4428033232688904\n",
      "Training loss for batch 3362 : 0.4271780252456665\n",
      "Training loss for batch 3363 : 0.10439349710941315\n",
      "Training loss for batch 3364 : 0.7404583692550659\n",
      "Training loss for batch 3365 : 0.15950684249401093\n",
      "Training loss for batch 3366 : 0.2664946913719177\n",
      "Training loss for batch 3367 : 0.2101745307445526\n",
      "Training loss for batch 3368 : 0.19149968028068542\n",
      "Training loss for batch 3369 : 0.06689060479402542\n",
      "Training loss for batch 3370 : 0.18986250460147858\n",
      "Training loss for batch 3371 : 0.25016212463378906\n",
      "Training loss for batch 3372 : 0.3965506851673126\n",
      "Training loss for batch 3373 : 0.015931308269500732\n",
      "Training loss for batch 3374 : 0.35512325167655945\n",
      "Training loss for batch 3375 : 0.08917847275733948\n",
      "Training loss for batch 3376 : 0.15221187472343445\n",
      "Training loss for batch 3377 : 0.3352399468421936\n",
      "Training loss for batch 3378 : 0.04041483998298645\n",
      "Training loss for batch 3379 : 0.2730763256549835\n",
      "Training loss for batch 3380 : 0.28790855407714844\n",
      "Training loss for batch 3381 : 0.12866345047950745\n",
      "Training loss for batch 3382 : 0.45391926169395447\n",
      "Training loss for batch 3383 : 0.26545318961143494\n",
      "Training loss for batch 3384 : 0.10692655295133591\n",
      "Training loss for batch 3385 : 0.11571989208459854\n",
      "Training loss for batch 3386 : 0.13283522427082062\n",
      "Training loss for batch 3387 : 0.22385971248149872\n",
      "Training loss for batch 3388 : 0.31980225443840027\n",
      "Training loss for batch 3389 : 0.019832585006952286\n",
      "Training loss for batch 3390 : 0.2501384913921356\n",
      "Training loss for batch 3391 : 0.1063053160905838\n",
      "Training loss for batch 3392 : 0.18754814565181732\n",
      "Training loss for batch 3393 : 0.18435736000537872\n",
      "Training loss for batch 3394 : 0.23148508369922638\n",
      "Training loss for batch 3395 : 0.2591364085674286\n",
      "Training loss for batch 3396 : 0.24389763176441193\n",
      "Training loss for batch 3397 : 0.32751840353012085\n",
      "Training loss for batch 3398 : 0.22951403260231018\n",
      "Training loss for batch 3399 : 0.054496653378009796\n",
      "Training loss for batch 3400 : 0.08653377741575241\n",
      "Training loss for batch 3401 : 0.15855297446250916\n",
      "Training loss for batch 3402 : 0.09720996022224426\n",
      "Training loss for batch 3403 : 0.11621468514204025\n",
      "Training loss for batch 3404 : 0.6411384344100952\n",
      "Training loss for batch 3405 : 0.19798225164413452\n",
      "Training loss for batch 3406 : 0.10658381134271622\n",
      "Training loss for batch 3407 : 0.42004719376564026\n",
      "Training loss for batch 3408 : 0.1102338582277298\n",
      "Training loss for batch 3409 : 0.862610936164856\n",
      "Training loss for batch 3410 : 0.04572746530175209\n",
      "Training loss for batch 3411 : 0.12845562398433685\n",
      "Training loss for batch 3412 : 0.1522802710533142\n",
      "Training loss for batch 3413 : 0.19474273920059204\n",
      "Training loss for batch 3414 : 0.31219983100891113\n",
      "Training loss for batch 3415 : 0.16380274295806885\n",
      "Training loss for batch 3416 : 0.17041677236557007\n",
      "Training loss for batch 3417 : 0.07902234047651291\n",
      "Training loss for batch 3418 : 0.051060646772384644\n",
      "Training loss for batch 3419 : 0.3610266149044037\n",
      "Training loss for batch 3420 : 0.1478215456008911\n",
      "Training loss for batch 3421 : 0.07940451800823212\n",
      "Training loss for batch 3422 : 0.02515605092048645\n",
      "Training loss for batch 3423 : 0.03985258936882019\n",
      "Training loss for batch 3424 : 0.03903484344482422\n",
      "Training loss for batch 3425 : 0.4175030291080475\n",
      "Training loss for batch 3426 : 0.17051610350608826\n",
      "Training loss for batch 3427 : 0.1242406815290451\n",
      "Training loss for batch 3428 : 0.08095323294401169\n",
      "Training loss for batch 3429 : 0.25630590319633484\n",
      "Training loss for batch 3430 : 0.034591786563396454\n",
      "Training loss for batch 3431 : 0.0014285346260294318\n",
      "Training loss for batch 3432 : 0.0369502529501915\n",
      "Training loss for batch 3433 : 0.011595495045185089\n",
      "Training loss for batch 3434 : 0.1402648687362671\n",
      "Training loss for batch 3435 : 0.4171072244644165\n",
      "Training loss for batch 3436 : 0.040630683302879333\n",
      "Training loss for batch 3437 : 0.2548729181289673\n",
      "Training loss for batch 3438 : 0.096808522939682\n",
      "Training loss for batch 3439 : 0.09672773629426956\n",
      "Training loss for batch 3440 : 0.5313569903373718\n",
      "Training loss for batch 3441 : 0.28752967715263367\n",
      "Training loss for batch 3442 : 0.12313874065876007\n",
      "Training loss for batch 3443 : 0.13547033071517944\n",
      "Training loss for batch 3444 : 0.1283990740776062\n",
      "Training loss for batch 3445 : 0.2780691087245941\n",
      "Training loss for batch 3446 : 0.279916912317276\n",
      "Training loss for batch 3447 : 0.3388960361480713\n",
      "Training loss for batch 3448 : 0.31292396783828735\n",
      "Training loss for batch 3449 : 0.35290929675102234\n",
      "Training loss for batch 3450 : 0.3225836753845215\n",
      "Training loss for batch 3451 : 0.01907658576965332\n",
      "Training loss for batch 3452 : 0.30520814657211304\n",
      "Training loss for batch 3453 : 0.2572697401046753\n",
      "Training loss for batch 3454 : 0.08425620943307877\n",
      "Training loss for batch 3455 : 0.34586119651794434\n",
      "Training loss for batch 3456 : 0.09427426755428314\n",
      "Training loss for batch 3457 : 0.03344936668872833\n",
      "Training loss for batch 3458 : 0.2487703263759613\n",
      "Training loss for batch 3459 : 0.07643333822488785\n",
      "Training loss for batch 3460 : 0.0\n",
      "Training loss for batch 3461 : 0.06953896582126617\n",
      "Training loss for batch 3462 : 0.0015861988067626953\n",
      "Training loss for batch 3463 : 0.10900422185659409\n",
      "Training loss for batch 3464 : 0.01448715291917324\n",
      "Training loss for batch 3465 : 0.36272695660591125\n",
      "Training loss for batch 3466 : 0.01351829245686531\n",
      "Training loss for batch 3467 : 0.5415757894515991\n",
      "Training loss for batch 3468 : 0.24717214703559875\n",
      "Training loss for batch 3469 : 0.3280763030052185\n",
      "Training loss for batch 3470 : 0.10267271101474762\n",
      "Training loss for batch 3471 : 0.4557195007801056\n",
      "Training loss for batch 3472 : 0.43752315640449524\n",
      "Training loss for batch 3473 : 0.42466652393341064\n",
      "Training loss for batch 3474 : 0.037599362432956696\n",
      "Training loss for batch 3475 : 0.2012520134449005\n",
      "Training loss for batch 3476 : 0.2754140794277191\n",
      "Training loss for batch 3477 : 0.050677429884672165\n",
      "Training loss for batch 3478 : 0.4586777985095978\n",
      "Training loss for batch 3479 : 0.9195476174354553\n",
      "Training loss for batch 3480 : 0.29121896624565125\n",
      "Training loss for batch 3481 : 0.02401149272918701\n",
      "Training loss for batch 3482 : 0.09931628406047821\n",
      "Training loss for batch 3483 : 0.09708472341299057\n",
      "Training loss for batch 3484 : 0.15731866657733917\n",
      "Training loss for batch 3485 : 0.21728548407554626\n",
      "Training loss for batch 3486 : 0.1034039705991745\n",
      "Training loss for batch 3487 : 0.252567321062088\n",
      "Training loss for batch 3488 : 0.2019694447517395\n",
      "Training loss for batch 3489 : 0.1119416281580925\n",
      "Training loss for batch 3490 : 0.3215789794921875\n",
      "Training loss for batch 3491 : 0.24569013714790344\n",
      "Training loss for batch 3492 : 0.031196098774671555\n",
      "Training loss for batch 3493 : 0.019891157746315002\n",
      "Training loss for batch 3494 : 0.002989669796079397\n",
      "Training loss for batch 3495 : 0.21752917766571045\n",
      "Training loss for batch 3496 : 0.21255308389663696\n",
      "Training loss for batch 3497 : 0.014042059890925884\n",
      "Training loss for batch 3498 : 0.1253090649843216\n",
      "Training loss for batch 3499 : 0.02533501386642456\n",
      "Training loss for batch 3500 : 0.09336140751838684\n",
      "Training loss for batch 3501 : 0.05760268121957779\n",
      "Training loss for batch 3502 : 0.14511296153068542\n",
      "Training loss for batch 3503 : 0.17947587370872498\n",
      "Training loss for batch 3504 : 0.3166449964046478\n",
      "Training loss for batch 3505 : 0.6305490136146545\n",
      "Training loss for batch 3506 : 0.4186338484287262\n",
      "Training loss for batch 3507 : 0.8916077017784119\n",
      "Training loss for batch 3508 : 0.1413368433713913\n",
      "Training loss for batch 3509 : 0.013582125306129456\n",
      "Training loss for batch 3510 : 0.06456702947616577\n",
      "Training loss for batch 3511 : 0.15270979702472687\n",
      "Training loss for batch 3512 : 0.3412066102027893\n",
      "Training loss for batch 3513 : 0.16126030683517456\n",
      "Training loss for batch 3514 : 0.3314025402069092\n",
      "Training loss for batch 3515 : 0.06894094496965408\n",
      "Training loss for batch 3516 : 0.04647362232208252\n",
      "Training loss for batch 3517 : 0.1688372939825058\n",
      "Training loss for batch 3518 : 0.36153343319892883\n",
      "Training loss for batch 3519 : 0.35463181138038635\n",
      "Training loss for batch 3520 : 0.09915132820606232\n",
      "Training loss for batch 3521 : 0.11104225367307663\n",
      "Training loss for batch 3522 : 0.3066214919090271\n",
      "Training loss for batch 3523 : 0.006519585847854614\n",
      "Training loss for batch 3524 : 0.00108877825550735\n",
      "Training loss for batch 3525 : 0.18197175860404968\n",
      "Training loss for batch 3526 : 0.34858566522598267\n",
      "Training loss for batch 3527 : 0.058323152363300323\n",
      "Training loss for batch 3528 : 0.10854346305131912\n",
      "Training loss for batch 3529 : 0.5308948755264282\n",
      "Training loss for batch 3530 : 0.5782634615898132\n",
      "Training loss for batch 3531 : 0.11074607074260712\n",
      "Training loss for batch 3532 : 0.2557509243488312\n",
      "Training loss for batch 3533 : 0.4376317858695984\n",
      "Training loss for batch 3534 : 0.4461193084716797\n",
      "Training loss for batch 3535 : 0.09489902853965759\n",
      "Training loss for batch 3536 : 0.6577513217926025\n",
      "Training loss for batch 3537 : 0.1844724416732788\n",
      "Training loss for batch 3538 : 0.29988449811935425\n",
      "Training loss for batch 3539 : 0.3407561480998993\n",
      "Training loss for batch 3540 : 0.26234835386276245\n",
      "Training loss for batch 3541 : 0.07928729802370071\n",
      "Training loss for batch 3542 : 0.08188492804765701\n",
      "Training loss for batch 3543 : 0.15314966440200806\n",
      "Training loss for batch 3544 : 0.2501636743545532\n",
      "Training loss for batch 3545 : 0.329364150762558\n",
      "Training loss for batch 3546 : 0.04885739088058472\n",
      "Training loss for batch 3547 : 0.21758639812469482\n",
      "Training loss for batch 3548 : 0.4705309569835663\n",
      "Training loss for batch 3549 : 0.22652305662631989\n",
      "Training loss for batch 3550 : 0.10634500533342361\n",
      "Training loss for batch 3551 : 0.1834135502576828\n",
      "Training loss for batch 3552 : 0.03852205350995064\n",
      "Training loss for batch 3553 : 0.053252603858709335\n",
      "Training loss for batch 3554 : 0.07936963438987732\n",
      "Training loss for batch 3555 : 0.10877293348312378\n",
      "Training loss for batch 3556 : 0.28548839688301086\n",
      "Training loss for batch 3557 : 0.07288897037506104\n",
      "Training loss for batch 3558 : 0.6208842396736145\n",
      "Training loss for batch 3559 : 0.44911545515060425\n",
      "Training loss for batch 3560 : 0.2231910079717636\n",
      "Training loss for batch 3561 : 0.27978047728538513\n",
      "Training loss for batch 3562 : 0.3467000424861908\n",
      "Training loss for batch 3563 : 0.09389257431030273\n",
      "Training loss for batch 3564 : 0.35920774936676025\n",
      "Training loss for batch 3565 : 0.5172024965286255\n",
      "Training loss for batch 3566 : 0.20780478417873383\n",
      "Training loss for batch 3567 : 0.457684189081192\n",
      "Training loss for batch 3568 : 0.4746483862400055\n",
      "Training loss for batch 3569 : 0.2710423469543457\n",
      "Training loss for batch 3570 : 0.13355277478694916\n",
      "Training loss for batch 3571 : 0.0\n",
      "Training loss for batch 3572 : 0.00044445478124544024\n",
      "Training loss for batch 3573 : 0.0411929227411747\n",
      "Training loss for batch 3574 : 0.025095725432038307\n",
      "Training loss for batch 3575 : 0.07856784015893936\n",
      "Training loss for batch 3576 : 0.05377855524420738\n",
      "Training loss for batch 3577 : 0.23296122252941132\n",
      "Training loss for batch 3578 : 0.015392806380987167\n",
      "Training loss for batch 3579 : 0.06859883666038513\n",
      "Training loss for batch 3580 : 0.25465887784957886\n",
      "Training loss for batch 3581 : 0.37848713994026184\n",
      "Training loss for batch 3582 : 0.2686033248901367\n",
      "Training loss for batch 3583 : 0.15202206373214722\n",
      "Training loss for batch 3584 : 0.2961965501308441\n",
      "Training loss for batch 3585 : 0.0714082345366478\n",
      "Training loss for batch 3586 : 0.45721352100372314\n",
      "Training loss for batch 3587 : 0.04875597357749939\n",
      "Training loss for batch 3588 : 0.30337420105934143\n",
      "Training loss for batch 3589 : 0.00980081595480442\n",
      "Training loss for batch 3590 : 0.0472266748547554\n",
      "Training loss for batch 3591 : 0.34440940618515015\n",
      "Training loss for batch 3592 : 0.2002130001783371\n",
      "Training loss for batch 3593 : 0.34550583362579346\n",
      "Training loss for batch 3594 : 0.22128354012966156\n",
      "Training loss for batch 3595 : 0.5612254738807678\n",
      "Training loss for batch 3596 : 0.23922821879386902\n",
      "Training loss for batch 3597 : 0.11525696516036987\n",
      "Training loss for batch 3598 : 0.1734015941619873\n",
      "Training loss for batch 3599 : 0.35795411467552185\n",
      "Training loss for batch 3600 : 0.237699493765831\n",
      "Training loss for batch 3601 : 0.09193520992994308\n",
      "Training loss for batch 3602 : 0.11910189688205719\n",
      "Training loss for batch 3603 : 0.22785505652427673\n",
      "Training loss for batch 3604 : 0.1724262833595276\n",
      "Training loss for batch 3605 : 0.034715183079242706\n",
      "Training loss for batch 3606 : 0.03159482032060623\n",
      "Training loss for batch 3607 : 0.1472904086112976\n",
      "Training loss for batch 3608 : 0.04140058159828186\n",
      "Training loss for batch 3609 : 0.1760302484035492\n",
      "Training loss for batch 3610 : 0.29922643303871155\n",
      "Training loss for batch 3611 : 0.23714770376682281\n",
      "Training loss for batch 3612 : 0.15689024329185486\n",
      "Training loss for batch 3613 : 0.25422781705856323\n",
      "Training loss for batch 3614 : 0.2931458353996277\n",
      "Training loss for batch 3615 : 0.1549483835697174\n",
      "Training loss for batch 3616 : 0.19184179604053497\n",
      "Training loss for batch 3617 : 0.36464953422546387\n",
      "Training loss for batch 3618 : 0.3887447118759155\n",
      "Training loss for batch 3619 : 0.07149714231491089\n",
      "Training loss for batch 3620 : 0.22406980395317078\n",
      "Training loss for batch 3621 : 6.33994786767289e-05\n",
      "Training loss for batch 3622 : 0.14047299325466156\n",
      "Training loss for batch 3623 : 0.2781620919704437\n",
      "Training loss for batch 3624 : 0.3534075617790222\n",
      "Training loss for batch 3625 : 0.4683987498283386\n",
      "Training loss for batch 3626 : 0.8444047570228577\n",
      "Training loss for batch 3627 : 0.32234665751457214\n",
      "Training loss for batch 3628 : 0.27270373702049255\n",
      "Training loss for batch 3629 : 0.3496237099170685\n",
      "Training loss for batch 3630 : 0.16165617108345032\n",
      "Training loss for batch 3631 : 0.5399205088615417\n",
      "Training loss for batch 3632 : 0.30576393008232117\n",
      "Training loss for batch 3633 : 0.26174458861351013\n",
      "Training loss for batch 3634 : 0.02238958328962326\n",
      "Training loss for batch 3635 : 0.08650718629360199\n",
      "Training loss for batch 3636 : 0.16290055215358734\n",
      "Training loss for batch 3637 : 0.17191582918167114\n",
      "Training loss for batch 3638 : 0.5710489153862\n",
      "Training loss for batch 3639 : 0.39622849225997925\n",
      "Training loss for batch 3640 : 0.4752430021762848\n",
      "Training loss for batch 3641 : 0.0\n",
      "Training loss for batch 3642 : 0.005894928239285946\n",
      "Training loss for batch 3643 : 0.03493667021393776\n",
      "Training loss for batch 3644 : 0.04113781824707985\n",
      "Training loss for batch 3645 : 0.19228258728981018\n",
      "Training loss for batch 3646 : 0.08917470276355743\n",
      "Training loss for batch 3647 : 0.18711654841899872\n",
      "Training loss for batch 3648 : 0.2400708794593811\n",
      "Training loss for batch 3649 : 0.004796097055077553\n",
      "Training loss for batch 3650 : 0.4531550407409668\n",
      "Training loss for batch 3651 : 0.05391921103000641\n",
      "Training loss for batch 3652 : 0.17837102711200714\n",
      "Training loss for batch 3653 : 0.11059561371803284\n",
      "Training loss for batch 3654 : 0.10351377725601196\n",
      "Training loss for batch 3655 : 0.18422187864780426\n",
      "Training loss for batch 3656 : 0.0770222544670105\n",
      "Training loss for batch 3657 : 0.18672771751880646\n",
      "Training loss for batch 3658 : 0.13208149373531342\n",
      "Training loss for batch 3659 : 0.2809298634529114\n",
      "Training loss for batch 3660 : 0.055717602372169495\n",
      "Training loss for batch 3661 : 0.15565824508666992\n",
      "Training loss for batch 3662 : 0.0\n",
      "Training loss for batch 3663 : 0.3084922432899475\n",
      "Training loss for batch 3664 : 0.42229071259498596\n",
      "Training loss for batch 3665 : 0.15710526704788208\n",
      "Training loss for batch 3666 : 0.3476761281490326\n",
      "Training loss for batch 3667 : 0.09224076569080353\n",
      "Training loss for batch 3668 : 0.33749932050704956\n",
      "Training loss for batch 3669 : 0.1815718412399292\n",
      "Training loss for batch 3670 : 0.2840084731578827\n",
      "Training loss for batch 3671 : 0.0422903373837471\n",
      "Training loss for batch 3672 : 0.3105206787586212\n",
      "Training loss for batch 3673 : 0.21539904177188873\n",
      "Training loss for batch 3674 : 0.27680543065071106\n",
      "Training loss for batch 3675 : 0.2223901003599167\n",
      "Training loss for batch 3676 : 0.12387420237064362\n",
      "Training loss for batch 3677 : 0.20742303133010864\n",
      "Training loss for batch 3678 : 0.1844913214445114\n",
      "Training loss for batch 3679 : 0.39171648025512695\n",
      "Training loss for batch 3680 : 0.06329236179590225\n",
      "Training loss for batch 3681 : 0.03612102195620537\n",
      "Training loss for batch 3682 : 0.13351456820964813\n",
      "Training loss for batch 3683 : 0.0025204222183674574\n",
      "Training loss for batch 3684 : 0.2050296515226364\n",
      "Training loss for batch 3685 : 0.28969496488571167\n",
      "Training loss for batch 3686 : 0.3921678066253662\n",
      "Training loss for batch 3687 : 0.07461501657962799\n",
      "Training loss for batch 3688 : 0.28164708614349365\n",
      "Training loss for batch 3689 : 0.21052667498588562\n",
      "Training loss for batch 3690 : 0.40202805399894714\n",
      "Training loss for batch 3691 : 0.3465665280818939\n",
      "Training loss for batch 3692 : 0.3490903675556183\n",
      "Training loss for batch 3693 : 0.24575750529766083\n",
      "Training loss for batch 3694 : 0.1650155931711197\n",
      "Training loss for batch 3695 : 0.44227567315101624\n",
      "Training loss for batch 3696 : 0.5574652552604675\n",
      "Training loss for batch 3697 : 0.38798823952674866\n",
      "Training loss for batch 3698 : 0.24993129074573517\n",
      "Training loss for batch 3699 : 0.22627227008342743\n",
      "Training loss for batch 3700 : 0.29020050168037415\n",
      "Training loss for batch 3701 : 0.14223788678646088\n",
      "Training loss for batch 3702 : 0.026332326233386993\n",
      "Training loss for batch 3703 : 0.27571678161621094\n",
      "Training loss for batch 3704 : 0.20766715705394745\n",
      "Training loss for batch 3705 : 0.09953134506940842\n",
      "Training loss for batch 3706 : 0.38130393624305725\n",
      "Training loss for batch 3707 : 0.22881005704402924\n",
      "Training loss for batch 3708 : 0.12437635660171509\n",
      "Training loss for batch 3709 : 0.19250333309173584\n",
      "Training loss for batch 3710 : 0.33120134472846985\n",
      "Training loss for batch 3711 : 0.20055708289146423\n",
      "Training loss for batch 3712 : 0.0\n",
      "Training loss for batch 3713 : 0.21360626816749573\n",
      "Training loss for batch 3714 : 0.5579725503921509\n",
      "Training loss for batch 3715 : 0.03374157473444939\n",
      "Training loss for batch 3716 : 0.26943692564964294\n",
      "Training loss for batch 3717 : 0.0009359452524222434\n",
      "Training loss for batch 3718 : 0.30385318398475647\n",
      "Training loss for batch 3719 : 0.08058889955282211\n",
      "Training loss for batch 3720 : 0.03489122912287712\n",
      "Training loss for batch 3721 : 0.36633002758026123\n",
      "Training loss for batch 3722 : 0.046165380626916885\n",
      "Training loss for batch 3723 : 0.2322954535484314\n",
      "Training loss for batch 3724 : 0.1893581748008728\n",
      "Training loss for batch 3725 : 0.13509626686573029\n",
      "Training loss for batch 3726 : 0.12674644589424133\n",
      "Training loss for batch 3727 : 0.05601641163229942\n",
      "Training loss for batch 3728 : 0.27629998326301575\n",
      "Training loss for batch 3729 : 0.3451646864414215\n",
      "Training loss for batch 3730 : 0.1749206781387329\n",
      "Training loss for batch 3731 : 0.3088492751121521\n",
      "Training loss for batch 3732 : 0.06475719064474106\n",
      "Training loss for batch 3733 : 0.06406283378601074\n",
      "Training loss for batch 3734 : 0.08782553672790527\n",
      "Training loss for batch 3735 : 0.4922262728214264\n",
      "Training loss for batch 3736 : 0.005448381416499615\n",
      "Training loss for batch 3737 : 0.15178392827510834\n",
      "Training loss for batch 3738 : 0.3100726306438446\n",
      "Training loss for batch 3739 : 0.06922535598278046\n",
      "Training loss for batch 3740 : 0.5541150569915771\n",
      "Training loss for batch 3741 : 0.36938127875328064\n",
      "Training loss for batch 3742 : 0.025400375947356224\n",
      "Training loss for batch 3743 : 0.03250601142644882\n",
      "Training loss for batch 3744 : 0.06994020938873291\n",
      "Training loss for batch 3745 : 0.24039095640182495\n",
      "Training loss for batch 3746 : 0.17447787523269653\n",
      "Training loss for batch 3747 : 0.06981941312551498\n",
      "Training loss for batch 3748 : 0.2521144151687622\n",
      "Training loss for batch 3749 : 0.3508875370025635\n",
      "Training loss for batch 3750 : 0.09423008561134338\n",
      "Training loss for batch 3751 : 0.13820263743400574\n",
      "Training loss for batch 3752 : 0.29412412643432617\n",
      "Training loss for batch 3753 : 0.18841074407100677\n",
      "Training loss for batch 3754 : 0.23449638485908508\n",
      "Training loss for batch 3755 : 0.16976647078990936\n",
      "Training loss for batch 3756 : 0.12166740745306015\n",
      "Training loss for batch 3757 : 0.15465238690376282\n",
      "Training loss for batch 3758 : 0.1324148327112198\n",
      "Training loss for batch 3759 : 0.23923112452030182\n",
      "Training loss for batch 3760 : 0.014518561773002148\n",
      "Training loss for batch 3761 : 0.2854401171207428\n",
      "Training loss for batch 3762 : 0.0025976202450692654\n",
      "Training loss for batch 3763 : 0.3771969974040985\n",
      "Training loss for batch 3764 : 0.15555992722511292\n",
      "Training loss for batch 3765 : 0.22485119104385376\n",
      "Training loss for batch 3766 : 0.06663699448108673\n",
      "Training loss for batch 3767 : 0.4893076419830322\n",
      "Training loss for batch 3768 : 0.2205209583044052\n",
      "Training loss for batch 3769 : 0.14562706649303436\n",
      "Training loss for batch 3770 : 0.02729395404458046\n",
      "Training loss for batch 3771 : 0.2051355391740799\n",
      "Training loss for batch 3772 : 0.12099090218544006\n",
      "Training loss for batch 3773 : 0.20083126425743103\n",
      "Training loss for batch 3774 : 0.18945376574993134\n",
      "Training loss for batch 3775 : 0.013131479732692242\n",
      "Training loss for batch 3776 : 0.06085556745529175\n",
      "Training loss for batch 3777 : 0.09448271989822388\n",
      "Training loss for batch 3778 : 0.3740813732147217\n",
      "Training loss for batch 3779 : 0.13543623685836792\n",
      "Training loss for batch 3780 : 0.09467541426420212\n",
      "Training loss for batch 3781 : 0.08956687897443771\n",
      "Training loss for batch 3782 : 0.5880439281463623\n",
      "Training loss for batch 3783 : 0.08762269467115402\n",
      "Training loss for batch 3784 : 0.1365414261817932\n",
      "Training loss for batch 3785 : 0.05564076825976372\n",
      "Training loss for batch 3786 : 0.8110060691833496\n",
      "Training loss for batch 3787 : 0.1839175522327423\n",
      "Training loss for batch 3788 : 0.15094126760959625\n",
      "Training loss for batch 3789 : 0.2029108852148056\n",
      "Training loss for batch 3790 : 0.11042112112045288\n",
      "Training loss for batch 3791 : 0.021489206701517105\n",
      "Training loss for batch 3792 : 0.24418920278549194\n",
      "Training loss for batch 3793 : 0.14904272556304932\n",
      "Training loss for batch 3794 : 0.018493574112653732\n",
      "Training loss for batch 3795 : 0.162706196308136\n",
      "Training loss for batch 3796 : 0.2803095281124115\n",
      "Training loss for batch 3797 : 0.38999754190444946\n",
      "Training loss for batch 3798 : 0.1783704310655594\n",
      "Training loss for batch 3799 : 0.18340922892093658\n",
      "Training loss for batch 3800 : 0.02526385337114334\n",
      "Training loss for batch 3801 : 0.17680443823337555\n",
      "Training loss for batch 3802 : 0.40543699264526367\n",
      "Training loss for batch 3803 : 0.29748809337615967\n",
      "Training loss for batch 3804 : 0.3307969570159912\n",
      "Training loss for batch 3805 : 0.3092109262943268\n",
      "Training loss for batch 3806 : 0.12289971113204956\n",
      "Training loss for batch 3807 : 0.22381719946861267\n",
      "Training loss for batch 3808 : 0.4697558879852295\n",
      "Training loss for batch 3809 : 0.08201305568218231\n",
      "Training loss for batch 3810 : 0.2922710180282593\n",
      "Training loss for batch 3811 : 0.4084523618221283\n",
      "Training loss for batch 3812 : 0.2920384109020233\n",
      "Training loss for batch 3813 : 0.31557464599609375\n",
      "Training loss for batch 3814 : 0.25374841690063477\n",
      "Training loss for batch 3815 : 0.25629889965057373\n",
      "Training loss for batch 3816 : 0.04523329436779022\n",
      "Training loss for batch 3817 : 0.1483912467956543\n",
      "Training loss for batch 3818 : 0.1770608127117157\n",
      "Training loss for batch 3819 : 0.19156309962272644\n",
      "Training loss for batch 3820 : 0.21936289966106415\n",
      "Training loss for batch 3821 : 0.19324785470962524\n",
      "Training loss for batch 3822 : 0.1867460012435913\n",
      "Training loss for batch 3823 : 0.34956875443458557\n",
      "Training loss for batch 3824 : 0.3508881628513336\n",
      "Training loss for batch 3825 : 0.11277347803115845\n",
      "Training loss for batch 3826 : 0.8380900025367737\n",
      "Training loss for batch 3827 : 0.10283664613962173\n",
      "Training loss for batch 3828 : 0.03233528509736061\n",
      "Training loss for batch 3829 : 0.20165182650089264\n",
      "Training loss for batch 3830 : 0.2664223313331604\n",
      "Training loss for batch 3831 : 0.15628165006637573\n",
      "Training loss for batch 3832 : 0.3345315158367157\n",
      "Training loss for batch 3833 : 0.06114606931805611\n",
      "Training loss for batch 3834 : 0.13261613249778748\n",
      "Training loss for batch 3835 : 0.15479227900505066\n",
      "Training loss for batch 3836 : 0.0699102059006691\n",
      "Training loss for batch 3837 : 0.17733348906040192\n",
      "Training loss for batch 3838 : 0.5317721962928772\n",
      "Training loss for batch 3839 : 0.22323806583881378\n",
      "Training loss for batch 3840 : 0.33086276054382324\n",
      "Training loss for batch 3841 : 0.07460187375545502\n",
      "Training loss for batch 3842 : 0.38936352729797363\n",
      "Training loss for batch 3843 : 0.09702134877443314\n",
      "Training loss for batch 3844 : 0.0826372429728508\n",
      "Training loss for batch 3845 : 0.053305137902498245\n",
      "Training loss for batch 3846 : 0.1187148243188858\n",
      "Training loss for batch 3847 : 0.39963045716285706\n",
      "Training loss for batch 3848 : 0.30248960852622986\n",
      "Training loss for batch 3849 : 0.05482107773423195\n",
      "Training loss for batch 3850 : 0.029734352603554726\n",
      "Training loss for batch 3851 : 0.1699824184179306\n",
      "Training loss for batch 3852 : 0.24159270524978638\n",
      "Training loss for batch 3853 : 0.11470625549554825\n",
      "Training loss for batch 3854 : 0.0740470290184021\n",
      "Training loss for batch 3855 : 0.17419429123401642\n",
      "Training loss for batch 3856 : 0.47944656014442444\n",
      "Training loss for batch 3857 : 0.09660185873508453\n",
      "Training loss for batch 3858 : 0.005120910704135895\n",
      "Training loss for batch 3859 : 0.05218639224767685\n",
      "Training loss for batch 3860 : 0.7509704828262329\n",
      "Training loss for batch 3861 : 0.0654902532696724\n",
      "Training loss for batch 3862 : 0.13948015868663788\n",
      "Training loss for batch 3863 : 0.2530768811702728\n",
      "Training loss for batch 3864 : 0.5148004293441772\n",
      "Training loss for batch 3865 : 0.24875731766223907\n",
      "Training loss for batch 3866 : 0.21217623353004456\n",
      "Training loss for batch 3867 : 0.44795653223991394\n",
      "Training loss for batch 3868 : 0.21668756008148193\n",
      "Training loss for batch 3869 : 0.12007716298103333\n",
      "Training loss for batch 3870 : 0.428217351436615\n",
      "Training loss for batch 3871 : 0.32301926612854004\n",
      "Training loss for batch 3872 : 0.006270706653594971\n",
      "Training loss for batch 3873 : 0.20161786675453186\n",
      "Training loss for batch 3874 : 0.20687688887119293\n",
      "Training loss for batch 3875 : 0.25154849886894226\n",
      "Training loss for batch 3876 : 0.25525516271591187\n",
      "Training loss for batch 3877 : 0.16960816085338593\n",
      "Training loss for batch 3878 : 0.020187130197882652\n",
      "Training loss for batch 3879 : 0.24908050894737244\n",
      "Training loss for batch 3880 : 0.4299810528755188\n",
      "Training loss for batch 3881 : 0.1882503479719162\n",
      "Training loss for batch 3882 : 0.15696042776107788\n",
      "Training loss for batch 3883 : 0.13067863881587982\n",
      "Training loss for batch 3884 : 0.11593103408813477\n",
      "Training loss for batch 3885 : 0.3619557023048401\n",
      "Training loss for batch 3886 : 0.23768150806427002\n",
      "Training loss for batch 3887 : 0.2760959267616272\n",
      "Training loss for batch 3888 : 0.09423311054706573\n",
      "Training loss for batch 3889 : 0.027484234422445297\n",
      "Training loss for batch 3890 : 0.20488935708999634\n",
      "Training loss for batch 3891 : 0.1620585024356842\n",
      "Training loss for batch 3892 : 0.17854683101177216\n",
      "Training loss for batch 3893 : 0.3738386929035187\n",
      "Training loss for batch 3894 : 0.07052325457334518\n",
      "Training loss for batch 3895 : 0.2845460772514343\n",
      "Training loss for batch 3896 : 0.3154463469982147\n",
      "Training loss for batch 3897 : 0.383025586605072\n",
      "Training loss for batch 3898 : 0.19675424695014954\n",
      "Training loss for batch 3899 : 0.31137266755104065\n",
      "Training loss for batch 3900 : 0.04306738078594208\n",
      "Training loss for batch 3901 : 0.10627944767475128\n",
      "Training loss for batch 3902 : 0.2563970386981964\n",
      "Training loss for batch 3903 : 0.2147091031074524\n",
      "Training loss for batch 3904 : 0.14995218813419342\n",
      "Training loss for batch 3905 : 0.501488208770752\n",
      "Training loss for batch 3906 : 0.17966905236244202\n",
      "Training loss for batch 3907 : 0.2226772904396057\n",
      "Training loss for batch 3908 : 0.1607370227575302\n",
      "Training loss for batch 3909 : 0.2907519042491913\n",
      "Training loss for batch 3910 : 0.2428516298532486\n",
      "Training loss for batch 3911 : 0.08177147805690765\n",
      "Training loss for batch 3912 : 0.4063691198825836\n",
      "Training loss for batch 3913 : 0.3332212269306183\n",
      "Training loss for batch 3914 : 0.5121427774429321\n",
      "Training loss for batch 3915 : 0.23173339664936066\n",
      "Training loss for batch 3916 : 0.10312207043170929\n",
      "Training loss for batch 3917 : 0.033696748316287994\n",
      "Training loss for batch 3918 : 0.4123215973377228\n",
      "Training loss for batch 3919 : 0.10462377965450287\n",
      "Training loss for batch 3920 : 0.05797360837459564\n",
      "Training loss for batch 3921 : 0.29260510206222534\n",
      "Training loss for batch 3922 : 0.053358957171440125\n",
      "Training loss for batch 3923 : 0.24998502433300018\n",
      "Training loss for batch 3924 : 0.2036178708076477\n",
      "Training loss for batch 3925 : 0.4375004470348358\n",
      "Training loss for batch 3926 : 0.10230366885662079\n",
      "Training loss for batch 3927 : 0.5227790474891663\n",
      "Training loss for batch 3928 : 0.09993182867765427\n",
      "Training loss for batch 3929 : 0.36223292350769043\n",
      "Training loss for batch 3930 : 0.33202847838401794\n",
      "Training loss for batch 3931 : 0.12579573690891266\n",
      "Training loss for batch 3932 : 0.14066748321056366\n",
      "Training loss for batch 3933 : 0.1683325171470642\n",
      "Training loss for batch 3934 : 0.4765964150428772\n",
      "Training loss for batch 3935 : 0.3231266736984253\n",
      "Training loss for batch 3936 : 0.20893540978431702\n",
      "Training loss for batch 3937 : 0.23130857944488525\n",
      "Training loss for batch 3938 : 0.056182414293289185\n",
      "Training loss for batch 3939 : 0.018388403579592705\n",
      "Training loss for batch 3940 : 0.08698929101228714\n",
      "Training loss for batch 3941 : 0.007019232027232647\n",
      "Training loss for batch 3942 : 0.09597306698560715\n",
      "Training loss for batch 3943 : 0.3155660331249237\n",
      "Training loss for batch 3944 : 0.34129613637924194\n",
      "Training loss for batch 3945 : 0.16372239589691162\n",
      "Training loss for batch 3946 : 0.4120815694332123\n",
      "Training loss for batch 3947 : 0.2093145251274109\n",
      "Training loss for batch 3948 : 0.12191233038902283\n",
      "Training loss for batch 3949 : 0.13628093898296356\n",
      "Training loss for batch 3950 : 0.14600896835327148\n",
      "Training loss for batch 3951 : 0.7169652581214905\n",
      "Training loss for batch 3952 : 0.3627622723579407\n",
      "Training loss for batch 3953 : 0.029945602640509605\n",
      "Training loss for batch 3954 : 0.0802096351981163\n",
      "Training loss for batch 3955 : 0.28617745637893677\n",
      "Training loss for batch 3956 : 0.18484392762184143\n",
      "Training loss for batch 3957 : 0.1121634989976883\n",
      "Training loss for batch 3958 : 0.22364068031311035\n",
      "Training loss for batch 3959 : 0.08359982073307037\n",
      "Training loss for batch 3960 : 0.04516933858394623\n",
      "Training loss for batch 3961 : 0.26367324590682983\n",
      "Training loss for batch 3962 : 0.28124475479125977\n",
      "Training loss for batch 3963 : 0.08520302921533585\n",
      "Training loss for batch 3964 : 0.010027091950178146\n",
      "Training loss for batch 3965 : 0.503477156162262\n",
      "Training loss for batch 3966 : 0.002775664208456874\n",
      "Training loss for batch 3967 : 0.24778148531913757\n",
      "Training loss for batch 3968 : 0.3358459770679474\n",
      "Training loss for batch 3969 : 0.582014262676239\n",
      "Training loss for batch 3970 : 0.131622776389122\n",
      "Training loss for batch 3971 : 0.23340106010437012\n",
      "Training loss for batch 3972 : 0.0691237524151802\n",
      "Training loss for batch 3973 : 0.14074915647506714\n",
      "Training loss for batch 3974 : 0.08248673379421234\n",
      "Training loss for batch 3975 : 0.05420604720711708\n",
      "Training loss for batch 3976 : 0.20549823343753815\n",
      "Training loss for batch 3977 : 0.17661474645137787\n",
      "Training loss for batch 3978 : 0.10596317052841187\n",
      "Training loss for batch 3979 : 0.30347928404808044\n",
      "Training loss for batch 3980 : 0.2723532021045685\n",
      "Training loss for batch 3981 : 0.007787764072418213\n",
      "Training loss for batch 3982 : 0.09787948429584503\n",
      "Training loss for batch 3983 : 0.3737933039665222\n",
      "Training loss for batch 3984 : 0.09751419723033905\n",
      "Training loss for batch 3985 : 0.2767750322818756\n",
      "Training loss for batch 3986 : 0.11094360053539276\n",
      "Training loss for batch 3987 : 0.15969663858413696\n",
      "Training loss for batch 3988 : 0.22358141839504242\n",
      "Training loss for batch 3989 : 0.34279027581214905\n",
      "Training loss for batch 3990 : 0.017580438405275345\n",
      "Training loss for batch 3991 : 0.4771738052368164\n",
      "Training loss for batch 3992 : 0.030327241867780685\n",
      "Training loss for batch 3993 : 0.33497536182403564\n",
      "Training loss for batch 3994 : 0.2567775249481201\n",
      "Training loss for batch 3995 : 0.22154872119426727\n",
      "Training loss for batch 3996 : 0.2590929865837097\n",
      "Training loss for batch 3997 : 0.22096030414104462\n",
      "Training loss for batch 3998 : 0.6476478576660156\n",
      "Training loss for batch 3999 : 0.24345450103282928\n",
      "Training loss for batch 4000 : 0.12578269839286804\n",
      "Training loss for batch 4001 : 0.24581848084926605\n",
      "Training loss for batch 4002 : 0.09881359338760376\n",
      "Training loss for batch 4003 : 0.0704602599143982\n",
      "Training loss for batch 4004 : 0.020772462710738182\n",
      "Training loss for batch 4005 : 0.10694430768489838\n",
      "Training loss for batch 4006 : 0.1471254527568817\n",
      "Training loss for batch 4007 : 0.392252117395401\n",
      "Training loss for batch 4008 : 0.2862681448459625\n",
      "Training loss for batch 4009 : 0.17115141451358795\n",
      "Training loss for batch 4010 : 0.1891346275806427\n",
      "Training loss for batch 4011 : 0.0034351476933807135\n",
      "Training loss for batch 4012 : 0.04807090014219284\n",
      "Training loss for batch 4013 : 0.44135212898254395\n",
      "Training loss for batch 4014 : 0.03666909784078598\n",
      "Training loss for batch 4015 : 0.4444269835948944\n",
      "Training loss for batch 4016 : 0.07042913138866425\n",
      "Training loss for batch 4017 : 0.21074999868869781\n",
      "Training loss for batch 4018 : 0.31063467264175415\n",
      "Training loss for batch 4019 : 0.1704028993844986\n",
      "Training loss for batch 4020 : 0.09729786217212677\n",
      "Training loss for batch 4021 : 0.5661135315895081\n",
      "Training loss for batch 4022 : 0.11740929633378983\n",
      "Training loss for batch 4023 : 0.04868580028414726\n",
      "Training loss for batch 4024 : 0.02940804325044155\n",
      "Training loss for batch 4025 : 0.06021853908896446\n",
      "Training loss for batch 4026 : 0.46831825375556946\n",
      "Training loss for batch 4027 : 0.4720500409603119\n",
      "Training loss for batch 4028 : 0.4937087893486023\n",
      "Training loss for batch 4029 : 0.044398773461580276\n",
      "Training loss for batch 4030 : 0.210264652967453\n",
      "Training loss for batch 4031 : 0.5862240195274353\n",
      "Training loss for batch 4032 : 0.4808556139469147\n",
      "Training loss for batch 4033 : 0.3098992109298706\n",
      "Training loss for batch 4034 : 0.23127125203609467\n",
      "Training loss for batch 4035 : 0.2830464243888855\n",
      "Training loss for batch 4036 : 0.7294936180114746\n",
      "Training loss for batch 4037 : 0.35522884130477905\n",
      "Training loss for batch 4038 : 0.005948533769696951\n",
      "Training loss for batch 4039 : 0.12557373940944672\n",
      "Training loss for batch 4040 : 0.07443346083164215\n",
      "Training loss for batch 4041 : 0.2544669806957245\n",
      "Training loss for batch 4042 : 0.27613380551338196\n",
      "Training loss for batch 4043 : 0.04656028375029564\n",
      "Training loss for batch 4044 : 0.24924327433109283\n",
      "Training loss for batch 4045 : 0.31159868836402893\n",
      "Training loss for batch 4046 : 0.14655795693397522\n",
      "Training loss for batch 4047 : 0.04490169882774353\n",
      "Training loss for batch 4048 : 0.1152498871088028\n",
      "Training loss for batch 4049 : 0.35498562455177307\n",
      "Training loss for batch 4050 : 0.22995547950267792\n",
      "Training loss for batch 4051 : 0.026091519743204117\n",
      "Training loss for batch 4052 : 0.47157105803489685\n",
      "Training loss for batch 4053 : 0.023131968453526497\n",
      "Training loss for batch 4054 : 0.2224201261997223\n",
      "Training loss for batch 4055 : 0.42538413405418396\n",
      "Training loss for batch 4056 : 0.19216790795326233\n",
      "Training loss for batch 4057 : 0.39878079295158386\n",
      "Training loss for batch 4058 : 0.34128016233444214\n",
      "Training loss for batch 4059 : 0.36222776770591736\n",
      "Training loss for batch 4060 : 0.21850277483463287\n",
      "Training loss for batch 4061 : 0.0038437843322753906\n",
      "Training loss for batch 4062 : 0.13845644891262054\n",
      "Training loss for batch 4063 : 0.1258104145526886\n",
      "Training loss for batch 4064 : 0.1538821905851364\n",
      "Training loss for batch 4065 : 0.10318555682897568\n",
      "Training loss for batch 4066 : 0.15601187944412231\n",
      "Training loss for batch 4067 : 0.013809313997626305\n",
      "Training loss for batch 4068 : 0.13895031809806824\n",
      "Training loss for batch 4069 : 0.5196383595466614\n",
      "Training loss for batch 4070 : 0.13310274481773376\n",
      "Training loss for batch 4071 : 0.08773024380207062\n",
      "Training loss for batch 4072 : 0.2628578841686249\n",
      "Training loss for batch 4073 : 0.301061749458313\n",
      "Training loss for batch 4074 : 0.4289472997188568\n",
      "Training loss for batch 4075 : 0.5104882717132568\n",
      "Training loss for batch 4076 : 0.4142948389053345\n",
      "Training loss for batch 4077 : 0.46900972723960876\n",
      "Training loss for batch 4078 : 0.45235946774482727\n",
      "Training loss for batch 4079 : 0.2622276544570923\n",
      "Training loss for batch 4080 : 0.12737829983234406\n",
      "Training loss for batch 4081 : 0.1679258495569229\n",
      "Training loss for batch 4082 : 0.13103125989437103\n",
      "Training loss for batch 4083 : 0.23790422081947327\n",
      "Training loss for batch 4084 : 0.020470520481467247\n",
      "Training loss for batch 4085 : 0.4054292142391205\n",
      "Training loss for batch 4086 : 0.061399802565574646\n",
      "Training loss for batch 4087 : 0.13218830525875092\n",
      "Training loss for batch 4088 : 0.002448568819090724\n",
      "Training loss for batch 4089 : 0.012873610481619835\n",
      "Training loss for batch 4090 : 0.698150098323822\n",
      "Training loss for batch 4091 : 0.1477542370557785\n",
      "Training loss for batch 4092 : 0.20964282751083374\n",
      "Training loss for batch 4093 : 0.5224089622497559\n",
      "Training loss for batch 4094 : 0.2000650018453598\n",
      "Training loss for batch 4095 : 0.0581703744828701\n",
      "Training loss for batch 4096 : 0.08685986697673798\n",
      "Training loss for batch 4097 : 0.2989738881587982\n",
      "Training loss for batch 4098 : 0.0995788648724556\n",
      "Training loss for batch 4099 : 0.19691498577594757\n",
      "Training loss for batch 4100 : 0.09422460198402405\n",
      "Training loss for batch 4101 : 0.18405193090438843\n",
      "Training loss for batch 4102 : 0.5167272686958313\n",
      "Training loss for batch 4103 : 0.35538792610168457\n",
      "Training loss for batch 4104 : 0.11934290081262589\n",
      "Training loss for batch 4105 : 0.4176732897758484\n",
      "Training loss for batch 4106 : 0.12619824707508087\n",
      "Training loss for batch 4107 : 0.03206842765212059\n",
      "Training loss for batch 4108 : 0.6799048781394958\n",
      "Training loss for batch 4109 : 0.31975725293159485\n",
      "Training loss for batch 4110 : 0.21226562559604645\n",
      "Training loss for batch 4111 : 0.36433982849121094\n",
      "Training loss for batch 4112 : 0.26585811376571655\n",
      "Training loss for batch 4113 : 0.2078118622303009\n",
      "Training loss for batch 4114 : 0.07405828684568405\n",
      "Training loss for batch 4115 : 0.2911214530467987\n",
      "Training loss for batch 4116 : 0.26256805658340454\n",
      "Training loss for batch 4117 : 0.3846213221549988\n",
      "Training loss for batch 4118 : 0.09547491371631622\n",
      "Training loss for batch 4119 : 0.4700176417827606\n",
      "Training loss for batch 4120 : 0.26890403032302856\n",
      "Training loss for batch 4121 : 0.2973248064517975\n",
      "Training loss for batch 4122 : 0.1866602599620819\n",
      "Training loss for batch 4123 : 0.08277346193790436\n",
      "Training loss for batch 4124 : 0.1381298154592514\n",
      "Training loss for batch 4125 : 0.09645765274763107\n",
      "Training loss for batch 4126 : 0.1564648300409317\n",
      "Training loss for batch 4127 : 0.3204275965690613\n",
      "Training loss for batch 4128 : 0.17810724675655365\n",
      "Training loss for batch 4129 : 0.028574399650096893\n",
      "Training loss for batch 4130 : 0.11884172260761261\n",
      "Training loss for batch 4131 : 0.0177651047706604\n",
      "Training loss for batch 4132 : 0.25882861018180847\n",
      "Training loss for batch 4133 : 0.4227752089500427\n",
      "Training loss for batch 4134 : 0.034873563796281815\n",
      "Training loss for batch 4135 : 0.01725735329091549\n",
      "Training loss for batch 4136 : 0.04100075364112854\n",
      "Training loss for batch 4137 : 0.12279142439365387\n",
      "Training loss for batch 4138 : 0.10580449551343918\n",
      "Training loss for batch 4139 : 0.17809560894966125\n",
      "Training loss for batch 4140 : 0.8775593638420105\n",
      "Training loss for batch 4141 : 0.633563220500946\n",
      "Training loss for batch 4142 : 0.029639504849910736\n",
      "Training loss for batch 4143 : 0.4582422077655792\n",
      "Training loss for batch 4144 : 0.029955776408314705\n",
      "Training loss for batch 4145 : 0.2405206263065338\n",
      "Training loss for batch 4146 : 0.249668687582016\n",
      "Training loss for batch 4147 : 0.16942834854125977\n",
      "Training loss for batch 4148 : 0.36768561601638794\n",
      "Training loss for batch 4149 : 0.15102161467075348\n",
      "Training loss for batch 4150 : 0.15081778168678284\n",
      "Training loss for batch 4151 : 0.21770206093788147\n",
      "Training loss for batch 4152 : 0.2166699767112732\n",
      "Training loss for batch 4153 : 0.42493122816085815\n",
      "Training loss for batch 4154 : 0.1367054134607315\n",
      "Training loss for batch 4155 : 0.04965449124574661\n",
      "Training loss for batch 4156 : 0.11686471849679947\n",
      "Training loss for batch 4157 : 0.11616828292608261\n",
      "Training loss for batch 4158 : 0.032453469932079315\n",
      "Training loss for batch 4159 : 0.2279992252588272\n",
      "Training loss for batch 4160 : 0.2203068733215332\n",
      "Training loss for batch 4161 : 0.2176235318183899\n",
      "Training loss for batch 4162 : 0.21656359732151031\n",
      "Training loss for batch 4163 : 0.036946851760149\n",
      "Training loss for batch 4164 : 0.04701218754053116\n",
      "Training loss for batch 4165 : 0.28929102420806885\n",
      "Training loss for batch 4166 : 0.014001141302287579\n",
      "Training loss for batch 4167 : 0.4211665391921997\n",
      "Training loss for batch 4168 : 0.11808290332555771\n",
      "Training loss for batch 4169 : 0.04441726207733154\n",
      "Training loss for batch 4170 : 0.05566505715250969\n",
      "Training loss for batch 4171 : 0.014701374806463718\n",
      "Training loss for batch 4172 : 0.17169593274593353\n",
      "Training loss for batch 4173 : 0.17886883020401\n",
      "Training loss for batch 4174 : 0.39335936307907104\n",
      "Training loss for batch 4175 : 0.171846404671669\n",
      "Training loss for batch 4176 : 0.37471672892570496\n",
      "Training loss for batch 4177 : 0.05676618963479996\n",
      "Training loss for batch 4178 : 0.031802594661712646\n",
      "Training loss for batch 4179 : 0.0\n",
      "Training loss for batch 4180 : 0.01682392694056034\n",
      "Training loss for batch 4181 : 0.2058531641960144\n",
      "Training loss for batch 4182 : 0.04075487703084946\n",
      "Training loss for batch 4183 : 0.1391996145248413\n",
      "Training loss for batch 4184 : 0.06136209890246391\n",
      "Training loss for batch 4185 : 0.29760095477104187\n",
      "Training loss for batch 4186 : 0.5423453450202942\n",
      "Training loss for batch 4187 : 0.36921027302742004\n",
      "Training loss for batch 4188 : 0.059646278619766235\n",
      "Training loss for batch 4189 : 0.09294088929891586\n",
      "Training loss for batch 4190 : 0.22694016993045807\n",
      "Training loss for batch 4191 : 0.1304585039615631\n",
      "Training loss for batch 4192 : 0.1168583407998085\n",
      "Training loss for batch 4193 : 0.03389035165309906\n",
      "Training loss for batch 4194 : 0.401450514793396\n",
      "Training loss for batch 4195 : 0.19694264233112335\n",
      "Training loss for batch 4196 : 0.1877908855676651\n",
      "Training loss for batch 4197 : 0.0011989276390522718\n",
      "Training loss for batch 4198 : 0.13707596063613892\n",
      "Training loss for batch 4199 : 0.11501911282539368\n",
      "Training loss for batch 4200 : 0.07592246681451797\n",
      "Training loss for batch 4201 : 0.16129761934280396\n",
      "Training loss for batch 4202 : 0.025243958458304405\n",
      "Training loss for batch 4203 : 0.2977200746536255\n",
      "Training loss for batch 4204 : 0.3695717453956604\n",
      "Training loss for batch 4205 : 0.30046549439430237\n",
      "Training loss for batch 4206 : 0.075111024081707\n",
      "Training loss for batch 4207 : 0.4189523160457611\n",
      "Training loss for batch 4208 : 0.3130616843700409\n",
      "Training loss for batch 4209 : 0.3255411982536316\n",
      "Training loss for batch 4210 : 0.3588210642337799\n",
      "Training loss for batch 4211 : 0.34742265939712524\n",
      "Training loss for batch 4212 : 0.11566072702407837\n",
      "Training loss for batch 4213 : 0.16861243546009064\n",
      "Training loss for batch 4214 : 0.4717860221862793\n",
      "Training loss for batch 4215 : 0.2739202380180359\n",
      "Training loss for batch 4216 : 0.20036713778972626\n",
      "Training loss for batch 4217 : 0.18623624742031097\n",
      "Training loss for batch 4218 : 0.10985805094242096\n",
      "Training loss for batch 4219 : 0.12678703665733337\n",
      "Training loss for batch 4220 : 0.18749795854091644\n",
      "Training loss for batch 4221 : 0.2924591898918152\n",
      "Training loss for batch 4222 : 0.2200079709291458\n",
      "Training loss for batch 4223 : 0.003225137945264578\n",
      "Training loss for batch 4224 : 0.48906397819519043\n",
      "Training loss for batch 4225 : 0.17211732268333435\n",
      "Training loss for batch 4226 : 0.28844061493873596\n",
      "Training loss for batch 4227 : 0.19075296819210052\n",
      "Training loss for batch 4228 : 0.06776396185159683\n",
      "Training loss for batch 4229 : 0.02688225917518139\n",
      "Training loss for batch 4230 : 0.20172801613807678\n",
      "Training loss for batch 4231 : 0.08865522593259811\n",
      "Training loss for batch 4232 : 0.2076358199119568\n",
      "Training loss for batch 4233 : 0.43967780470848083\n",
      "Training loss for batch 4234 : 0.0746861919760704\n",
      "Training loss for batch 4235 : 0.23079772293567657\n",
      "Training loss for batch 4236 : 0.2784782946109772\n",
      "Training loss for batch 4237 : 0.3200896978378296\n",
      "Training loss for batch 4238 : 0.12026109546422958\n",
      "Training loss for batch 4239 : 0.12790560722351074\n",
      "Training loss for batch 4240 : 0.056395482271909714\n",
      "Training loss for batch 4241 : 0.19448278844356537\n",
      "Training loss for batch 4242 : 0.010860835202038288\n",
      "Training loss for batch 4243 : 0.41111814975738525\n",
      "Training loss for batch 4244 : 0.48339390754699707\n",
      "Training loss for batch 4245 : 0.011195152997970581\n",
      "Training loss for batch 4246 : 0.06965899467468262\n",
      "Training loss for batch 4247 : 0.12237316370010376\n",
      "Training loss for batch 4248 : 0.09195782244205475\n",
      "Training loss for batch 4249 : 0.06879913061857224\n",
      "Training loss for batch 4250 : 0.4430479109287262\n",
      "Training loss for batch 4251 : 0.17388994991779327\n",
      "Training loss for batch 4252 : 0.11252285540103912\n",
      "Training loss for batch 4253 : 0.2918638288974762\n",
      "Training loss for batch 4254 : 0.4776706099510193\n",
      "Training loss for batch 4255 : 0.4389057755470276\n",
      "Training loss for batch 4256 : 0.2578447461128235\n",
      "Training loss for batch 4257 : 0.26742199063301086\n",
      "Training loss for batch 4258 : 0.10327920317649841\n",
      "Training loss for batch 4259 : 0.038024142384529114\n",
      "Training loss for batch 4260 : 0.25259777903556824\n",
      "Training loss for batch 4261 : 0.3062863349914551\n",
      "Training loss for batch 4262 : 0.27392318844795227\n",
      "Training loss for batch 4263 : 0.12662619352340698\n",
      "Training loss for batch 4264 : 0.10187758505344391\n",
      "Training loss for batch 4265 : 0.12892040610313416\n",
      "Training loss for batch 4266 : 0.3101470470428467\n",
      "Training loss for batch 4267 : 0.2819540500640869\n",
      "Training loss for batch 4268 : 0.2545555830001831\n",
      "Training loss for batch 4269 : 0.10260424017906189\n",
      "Training loss for batch 4270 : 0.1830531805753708\n",
      "Training loss for batch 4271 : 0.3818191587924957\n",
      "Training loss for batch 4272 : 0.03624792769551277\n",
      "Training loss for batch 4273 : 0.3281295597553253\n",
      "Training loss for batch 4274 : 0.2836245596408844\n",
      "Training loss for batch 4275 : 0.2667936086654663\n",
      "Training loss for batch 4276 : 0.27268514037132263\n",
      "Training loss for batch 4277 : 0.13472750782966614\n",
      "Training loss for batch 4278 : 0.12186571210622787\n",
      "Training loss for batch 4279 : 0.22096757590770721\n",
      "Training loss for batch 4280 : 0.17205564677715302\n",
      "Training loss for batch 4281 : 0.2091892808675766\n",
      "Training loss for batch 4282 : 0.09771640598773956\n",
      "Training loss for batch 4283 : 0.20537833869457245\n",
      "Training loss for batch 4284 : 0.13107798993587494\n",
      "Training loss for batch 4285 : 0.2625884711742401\n",
      "Training loss for batch 4286 : 0.17282742261886597\n",
      "Training loss for batch 4287 : 0.12169084697961807\n",
      "Training loss for batch 4288 : 0.030746469274163246\n",
      "Training loss for batch 4289 : 0.22123722732067108\n",
      "Training loss for batch 4290 : 0.13903723657131195\n",
      "Training loss for batch 4291 : 0.3507584035396576\n",
      "Training loss for batch 4292 : 0.10538268834352493\n",
      "Training loss for batch 4293 : 0.3409627079963684\n",
      "Training loss for batch 4294 : 0.14724764227867126\n",
      "Training loss for batch 4295 : 0.07550700008869171\n",
      "Training loss for batch 4296 : 0.07158391922712326\n",
      "Training loss for batch 4297 : 0.09627999365329742\n",
      "Training loss for batch 4298 : 0.13679535686969757\n",
      "Training loss for batch 4299 : 0.28367185592651367\n",
      "Training loss for batch 4300 : 0.2731432616710663\n",
      "Training loss for batch 4301 : 0.0\n",
      "Training loss for batch 4302 : 0.2634985148906708\n",
      "Training loss for batch 4303 : 0.003527869936078787\n",
      "Training loss for batch 4304 : 4.779299342771992e-05\n",
      "Training loss for batch 4305 : 0.1684499830007553\n",
      "Training loss for batch 4306 : 0.045999206602573395\n",
      "Training loss for batch 4307 : 0.10031996667385101\n",
      "Training loss for batch 4308 : 0.26088351011276245\n",
      "Training loss for batch 4309 : 0.10392487794160843\n",
      "Training loss for batch 4310 : 0.1051100492477417\n",
      "Training loss for batch 4311 : 0.18566231429576874\n",
      "Training loss for batch 4312 : 0.0934501588344574\n",
      "Training loss for batch 4313 : 0.1838238537311554\n",
      "Training loss for batch 4314 : 0.3414204716682434\n",
      "Training loss for batch 4315 : 0.38459616899490356\n",
      "Training loss for batch 4316 : 0.2709718346595764\n",
      "Training loss for batch 4317 : 0.046627141535282135\n",
      "Training loss for batch 4318 : 0.28900960087776184\n",
      "Training loss for batch 4319 : 0.015100865624845028\n",
      "Training loss for batch 4320 : 0.11652158945798874\n",
      "Training loss for batch 4321 : 0.3636125326156616\n",
      "Training loss for batch 4322 : 0.27723050117492676\n",
      "Training loss for batch 4323 : 0.43940648436546326\n",
      "Training loss for batch 4324 : 0.22099389135837555\n",
      "Training loss for batch 4325 : 0.3217636048793793\n",
      "Training loss for batch 4326 : 0.12680938839912415\n",
      "Training loss for batch 4327 : 0.07429831475019455\n",
      "Training loss for batch 4328 : 0.16106797754764557\n",
      "Training loss for batch 4329 : 0.3428933620452881\n",
      "Training loss for batch 4330 : 0.16121816635131836\n",
      "Training loss for batch 4331 : 0.07252497225999832\n",
      "Training loss for batch 4332 : 0.24496275186538696\n",
      "Training loss for batch 4333 : 0.037629686295986176\n",
      "Training loss for batch 4334 : 0.30474570393562317\n",
      "Training loss for batch 4335 : 0.32853618264198303\n",
      "Training loss for batch 4336 : 0.26732414960861206\n",
      "Training loss for batch 4337 : 0.005238002166152\n",
      "Training loss for batch 4338 : 0.19288212060928345\n",
      "Training loss for batch 4339 : 0.6231982111930847\n",
      "Training loss for batch 4340 : 0.21583615243434906\n",
      "Training loss for batch 4341 : 0.031179623678326607\n",
      "Training loss for batch 4342 : 0.12556517124176025\n",
      "Training loss for batch 4343 : 0.622463047504425\n",
      "Training loss for batch 4344 : 0.633573055267334\n",
      "Training loss for batch 4345 : 0.3644901216030121\n",
      "Training loss for batch 4346 : 0.22630424797534943\n",
      "Training loss for batch 4347 : 0.12169593572616577\n",
      "Training loss for batch 4348 : 0.010636595077812672\n",
      "Training loss for batch 4349 : 0.044619716703891754\n",
      "Training loss for batch 4350 : 0.0398871973156929\n",
      "Training loss for batch 4351 : 0.1318516731262207\n",
      "Training loss for batch 4352 : 0.721435546875\n",
      "Training loss for batch 4353 : 0.31279149651527405\n",
      "Training loss for batch 4354 : 0.45746445655822754\n",
      "Training loss for batch 4355 : 0.2454938143491745\n",
      "Training loss for batch 4356 : 0.21352942287921906\n",
      "Training loss for batch 4357 : 0.049947407096624374\n",
      "Training loss for batch 4358 : 0.2197655886411667\n",
      "Training loss for batch 4359 : 0.08912678062915802\n",
      "Training loss for batch 4360 : 0.025755416601896286\n",
      "Training loss for batch 4361 : 0.10282649844884872\n",
      "Training loss for batch 4362 : 0.06136677786707878\n",
      "Training loss for batch 4363 : 0.3053646683692932\n",
      "Training loss for batch 4364 : 0.020787356421351433\n",
      "Training loss for batch 4365 : 0.30392563343048096\n",
      "Training loss for batch 4366 : 0.29418841004371643\n",
      "Training loss for batch 4367 : 0.12837374210357666\n",
      "Training loss for batch 4368 : 0.1496795117855072\n",
      "Training loss for batch 4369 : 0.389644980430603\n",
      "Training loss for batch 4370 : 0.1936674565076828\n",
      "Training loss for batch 4371 : 0.1438198834657669\n",
      "Training loss for batch 4372 : 0.34453532099723816\n",
      "Training loss for batch 4373 : 0.2525933086872101\n",
      "Training loss for batch 4374 : 0.047268614172935486\n",
      "Training loss for batch 4375 : 0.12490435689687729\n",
      "Training loss for batch 4376 : 0.23711547255516052\n",
      "Training loss for batch 4377 : 0.23839770257472992\n",
      "Training loss for batch 4378 : 0.10330584645271301\n",
      "Training loss for batch 4379 : 0.3957742750644684\n",
      "Training loss for batch 4380 : 0.3127027750015259\n",
      "Training loss for batch 4381 : 0.08851926773786545\n",
      "Training loss for batch 4382 : 0.26859432458877563\n",
      "Training loss for batch 4383 : 0.2250644564628601\n",
      "Training loss for batch 4384 : 0.033891938626766205\n",
      "Training loss for batch 4385 : 0.056533776223659515\n",
      "Training loss for batch 4386 : 0.24265709519386292\n",
      "Training loss for batch 4387 : 0.43803897500038147\n",
      "Training loss for batch 4388 : 0.06947679072618484\n",
      "Training loss for batch 4389 : 0.009560207836329937\n",
      "Training loss for batch 4390 : 0.2562543749809265\n",
      "Training loss for batch 4391 : 0.2328166514635086\n",
      "Training loss for batch 4392 : 0.05051693320274353\n",
      "Training loss for batch 4393 : 0.32247889041900635\n",
      "Training loss for batch 4394 : 0.06343762576580048\n",
      "Training loss for batch 4395 : 0.025031255558133125\n",
      "Training loss for batch 4396 : 0.0\n",
      "Training loss for batch 4397 : 0.174565851688385\n",
      "Training loss for batch 4398 : 0.06328126043081284\n",
      "Training loss for batch 4399 : 0.14509756863117218\n",
      "Training loss for batch 4400 : 0.07354437559843063\n",
      "Training loss for batch 4401 : 0.1958494931459427\n",
      "Training loss for batch 4402 : 0.3055763244628906\n",
      "Training loss for batch 4403 : 0.13799013197422028\n",
      "Training loss for batch 4404 : 0.17195411026477814\n",
      "Training loss for batch 4405 : 0.051329903304576874\n",
      "Training loss for batch 4406 : 0.04712721332907677\n",
      "Training loss for batch 4407 : 0.09739507734775543\n",
      "Training loss for batch 4408 : 0.12089403718709946\n",
      "Training loss for batch 4409 : 0.0015221437206491828\n",
      "Training loss for batch 4410 : 0.3121142089366913\n",
      "Training loss for batch 4411 : 0.1122322604060173\n",
      "Training loss for batch 4412 : 0.39060860872268677\n",
      "Training loss for batch 4413 : 0.18527643382549286\n",
      "Training loss for batch 4414 : 0.07092414796352386\n",
      "Training loss for batch 4415 : 0.0810961201786995\n",
      "Training loss for batch 4416 : 0.21124669909477234\n",
      "Training loss for batch 4417 : 0.5766364336013794\n",
      "Training loss for batch 4418 : 0.2348015308380127\n",
      "Training loss for batch 4419 : 0.0\n",
      "Training loss for batch 4420 : 0.40085163712501526\n",
      "Training loss for batch 4421 : 0.35609227418899536\n",
      "Training loss for batch 4422 : 0.040668465197086334\n",
      "Training loss for batch 4423 : 0.05635595694184303\n",
      "Training loss for batch 4424 : 0.2147166132926941\n",
      "Training loss for batch 4425 : 0.26525136828422546\n",
      "Training loss for batch 4426 : 0.04123788699507713\n",
      "Training loss for batch 4427 : 0.058646440505981445\n",
      "Training loss for batch 4428 : 0.22637812793254852\n",
      "Training loss for batch 4429 : 0.3941757082939148\n",
      "Training loss for batch 4430 : 0.22778931260108948\n",
      "Training loss for batch 4431 : 0.4535956084728241\n",
      "Training loss for batch 4432 : 0.340239942073822\n",
      "Training loss for batch 4433 : 0.0006095793796703219\n",
      "Training loss for batch 4434 : 0.043377380818128586\n",
      "Training loss for batch 4435 : 0.32997018098831177\n",
      "Training loss for batch 4436 : 0.32075920701026917\n",
      "Training loss for batch 4437 : 0.15330666303634644\n",
      "Training loss for batch 4438 : 0.28959667682647705\n",
      "Training loss for batch 4439 : 0.05083164572715759\n",
      "Training loss for batch 4440 : 0.11273643374443054\n",
      "Training loss for batch 4441 : 0.2757532596588135\n",
      "Training loss for batch 4442 : 0.020424913614988327\n",
      "Training loss for batch 4443 : 0.0653236135840416\n",
      "Training loss for batch 4444 : 0.36784687638282776\n",
      "Training loss for batch 4445 : 0.0\n",
      "Training loss for batch 4446 : 0.08397088944911957\n",
      "Training loss for batch 4447 : 0.07924746721982956\n",
      "Training loss for batch 4448 : 0.14399653673171997\n",
      "Training loss for batch 4449 : 0.07297331094741821\n",
      "Training loss for batch 4450 : 0.021333616226911545\n",
      "Training loss for batch 4451 : 0.3760380446910858\n",
      "Training loss for batch 4452 : 0.32655835151672363\n",
      "Training loss for batch 4453 : 0.5071953535079956\n",
      "Training loss for batch 4454 : 0.14246071875095367\n",
      "Training loss for batch 4455 : 0.018521388992667198\n",
      "Training loss for batch 4456 : 0.20458440482616425\n",
      "Training loss for batch 4457 : 0.05871899053454399\n",
      "Training loss for batch 4458 : 0.31473803520202637\n",
      "Training loss for batch 4459 : 0.34331014752388\n",
      "Training loss for batch 4460 : 0.07302220910787582\n",
      "Training loss for batch 4461 : 0.0755147635936737\n",
      "Training loss for batch 4462 : 0.15231387317180634\n",
      "Training loss for batch 4463 : 0.6950334906578064\n",
      "Training loss for batch 4464 : 0.5294418931007385\n",
      "Training loss for batch 4465 : 0.1889197677373886\n",
      "Training loss for batch 4466 : 0.05511368811130524\n",
      "Training loss for batch 4467 : 0.312224417924881\n",
      "Training loss for batch 4468 : 0.2277866005897522\n",
      "Training loss for batch 4469 : 0.4286431670188904\n",
      "Training loss for batch 4470 : 0.03791353851556778\n",
      "Training loss for batch 4471 : 0.029906069859862328\n",
      "Training loss for batch 4472 : 0.9897332191467285\n",
      "Training loss for batch 4473 : 0.35468560457229614\n",
      "Training loss for batch 4474 : 0.07097543776035309\n",
      "Training loss for batch 4475 : 0.1524723917245865\n",
      "Training loss for batch 4476 : 0.2825719714164734\n",
      "Training loss for batch 4477 : 0.9819404482841492\n",
      "Training loss for batch 4478 : 0.2112089842557907\n",
      "Training loss for batch 4479 : 0.07995809614658356\n",
      "Training loss for batch 4480 : 0.3422674536705017\n",
      "Training loss for batch 4481 : 0.37438681721687317\n",
      "Training loss for batch 4482 : 0.06760553270578384\n",
      "Training loss for batch 4483 : 0.46779099106788635\n",
      "Training loss for batch 4484 : 0.10958845168352127\n",
      "Training loss for batch 4485 : 0.041623976081609726\n",
      "Training loss for batch 4486 : 0.047991469502449036\n",
      "Training loss for batch 4487 : 0.4258505403995514\n",
      "Training loss for batch 4488 : 0.24040578305721283\n",
      "Training loss for batch 4489 : 0.43222537636756897\n",
      "Training loss for batch 4490 : 0.11104384064674377\n",
      "Training loss for batch 4491 : 0.184895321726799\n",
      "Training loss for batch 4492 : 0.190614715218544\n",
      "Training loss for batch 4493 : 0.10378553718328476\n",
      "Training loss for batch 4494 : 0.032881930470466614\n",
      "Training loss for batch 4495 : 0.2562781572341919\n",
      "Training loss for batch 4496 : 0.21493403613567352\n",
      "Training loss for batch 4497 : 0.5033310651779175\n",
      "Training loss for batch 4498 : 0.4383860230445862\n",
      "Training loss for batch 4499 : 0.531670093536377\n",
      "Training loss for batch 4500 : 0.39880308508872986\n",
      "Training loss for batch 4501 : 0.41373029351234436\n",
      "Training loss for batch 4502 : 0.38838866353034973\n",
      "Training loss for batch 4503 : 0.163002610206604\n",
      "Training loss for batch 4504 : 0.06562136113643646\n",
      "Training loss for batch 4505 : 0.08087577670812607\n",
      "Training loss for batch 4506 : 0.36934781074523926\n",
      "Training loss for batch 4507 : 0.02272428758442402\n",
      "Training loss for batch 4508 : 0.172470822930336\n",
      "Training loss for batch 4509 : 0.3318854570388794\n",
      "Training loss for batch 4510 : 0.5684652328491211\n",
      "Training loss for batch 4511 : 0.04789289832115173\n",
      "Training loss for batch 4512 : 0.28732308745384216\n",
      "Training loss for batch 4513 : 0.11325161159038544\n",
      "Training loss for batch 4514 : 0.08935385197401047\n",
      "Training loss for batch 4515 : 0.2639581859111786\n",
      "Training loss for batch 4516 : 0.06990064680576324\n",
      "Training loss for batch 4517 : 0.30064624547958374\n",
      "Training loss for batch 4518 : 0.4739256501197815\n",
      "Training loss for batch 4519 : 0.3502854108810425\n",
      "Training loss for batch 4520 : 0.05852454900741577\n",
      "Training loss for batch 4521 : 0.28375622630119324\n",
      "Training loss for batch 4522 : 0.18543294072151184\n",
      "Training loss for batch 4523 : 0.2435513138771057\n",
      "Training loss for batch 4524 : 0.21974633634090424\n",
      "Training loss for batch 4525 : 0.044827044010162354\n",
      "Training loss for batch 4526 : 0.3786146342754364\n",
      "Training loss for batch 4527 : 0.04252035543322563\n",
      "Training loss for batch 4528 : 0.7067312598228455\n",
      "Training loss for batch 4529 : 0.2659465968608856\n",
      "Training loss for batch 4530 : 0.18175886571407318\n",
      "Training loss for batch 4531 : 0.14309361577033997\n",
      "Training loss for batch 4532 : 0.12640532851219177\n",
      "Training loss for batch 4533 : 0.20265309512615204\n",
      "Training loss for batch 4534 : 0.06595402956008911\n",
      "Training loss for batch 4535 : 0.17857211828231812\n",
      "Training loss for batch 4536 : 0.1271902173757553\n",
      "Training loss for batch 4537 : 0.28232359886169434\n",
      "Training loss for batch 4538 : 0.25483474135398865\n",
      "Training loss for batch 4539 : 0.5771682262420654\n",
      "Training loss for batch 4540 : 0.22881139814853668\n",
      "Training loss for batch 4541 : 0.10262412577867508\n",
      "Training loss for batch 4542 : 0.10018416494131088\n",
      "Training loss for batch 4543 : 0.09083068370819092\n",
      "Training loss for batch 4544 : 0.1691368818283081\n",
      "Training loss for batch 4545 : 0.35820913314819336\n",
      "Training loss for batch 4546 : 0.3074590265750885\n",
      "Training loss for batch 4547 : 0.4448208808898926\n",
      "Training loss for batch 4548 : 0.5857055187225342\n",
      "Training loss for batch 4549 : 0.0644637942314148\n",
      "Training loss for batch 4550 : 0.2833595275878906\n",
      "Training loss for batch 4551 : 0.2406778633594513\n",
      "Training loss for batch 4552 : 0.20139996707439423\n",
      "Training loss for batch 4553 : 1.2771941423416138\n",
      "Training loss for batch 4554 : 0.4935835003852844\n",
      "Training loss for batch 4555 : 0.30802837014198303\n",
      "Training loss for batch 4556 : 0.06137692555785179\n",
      "Training loss for batch 4557 : 0.20944324135780334\n",
      "Training loss for batch 4558 : 0.23552142083644867\n",
      "Training loss for batch 4559 : 0.0\n",
      "Training loss for batch 4560 : 0.08197095990180969\n",
      "Training loss for batch 4561 : 0.20152293145656586\n",
      "Training loss for batch 4562 : 0.024693099781870842\n",
      "Training loss for batch 4563 : 0.4384661614894867\n",
      "Training loss for batch 4564 : 0.17566117644309998\n",
      "Training loss for batch 4565 : 0.24023905396461487\n",
      "Training loss for batch 4566 : 0.17210140824317932\n",
      "Training loss for batch 4567 : 0.06880786269903183\n",
      "Training loss for batch 4568 : 0.6551005840301514\n",
      "Training loss for batch 4569 : 0.4813803732395172\n",
      "Training loss for batch 4570 : 0.08511766791343689\n",
      "Training loss for batch 4571 : 0.11125902831554413\n",
      "Training loss for batch 4572 : 0.06309385597705841\n",
      "Training loss for batch 4573 : 0.31783565878868103\n",
      "Training loss for batch 4574 : 0.17750975489616394\n",
      "Training loss for batch 4575 : 0.0911354199051857\n",
      "Training loss for batch 4576 : 0.4208182990550995\n",
      "Training loss for batch 4577 : 0.22372260689735413\n",
      "Training loss for batch 4578 : 0.21146635711193085\n",
      "Training loss for batch 4579 : 0.05956792086362839\n",
      "Training loss for batch 4580 : 0.0\n",
      "Training loss for batch 4581 : 0.1624385416507721\n",
      "Training loss for batch 4582 : 0.00985473021864891\n",
      "Training loss for batch 4583 : 0.29895418882369995\n",
      "Training loss for batch 4584 : 0.23829299211502075\n",
      "Training loss for batch 4585 : 0.12178287655115128\n",
      "Training loss for batch 4586 : 0.26044031977653503\n",
      "Training loss for batch 4587 : 0.07866236567497253\n",
      "Training loss for batch 4588 : 0.03829853609204292\n",
      "Training loss for batch 4589 : 0.36232301592826843\n",
      "Training loss for batch 4590 : 0.0061896494589746\n",
      "Training loss for batch 4591 : 0.12057282030582428\n",
      "Training loss for batch 4592 : 0.2974559962749481\n",
      "Training loss for batch 4593 : 0.2828258275985718\n",
      "Training loss for batch 4594 : 0.5296311378479004\n",
      "Training loss for batch 4595 : 0.13427738845348358\n",
      "Training loss for batch 4596 : 0.0\n",
      "Training loss for batch 4597 : 0.29721105098724365\n",
      "Training loss for batch 4598 : 0.2500387728214264\n",
      "Training loss for batch 4599 : 0.2930063009262085\n",
      "Training loss for batch 4600 : 0.2722525894641876\n",
      "Training loss for batch 4601 : 0.15955451130867004\n",
      "Training loss for batch 4602 : 0.28669509291648865\n",
      "Training loss for batch 4603 : 0.24872708320617676\n",
      "Training loss for batch 4604 : 0.04859678074717522\n",
      "Training loss for batch 4605 : 0.4466400444507599\n",
      "Training loss for batch 4606 : 0.16393384337425232\n",
      "Training loss for batch 4607 : 0.03476417064666748\n",
      "Training loss for batch 4608 : 0.09624598920345306\n",
      "Training loss for batch 4609 : 0.21918454766273499\n",
      "Training loss for batch 4610 : 0.08589287102222443\n",
      "Training loss for batch 4611 : 0.5955396294593811\n",
      "Training loss for batch 4612 : 0.27558037638664246\n",
      "Training loss for batch 4613 : 0.19532136619091034\n",
      "Training loss for batch 4614 : 0.22700083255767822\n",
      "Training loss for batch 4615 : 0.6347125768661499\n",
      "Training loss for batch 4616 : 0.2018348127603531\n",
      "Training loss for batch 4617 : 0.16322147846221924\n",
      "Training loss for batch 4618 : 0.3763500154018402\n",
      "Training loss for batch 4619 : 0.08153869956731796\n",
      "Training loss for batch 4620 : 0.17262154817581177\n",
      "Training loss for batch 4621 : 0.053750328719615936\n",
      "Training loss for batch 4622 : 0.047850530594587326\n",
      "Training loss for batch 4623 : 0.12076404690742493\n",
      "Training loss for batch 4624 : 0.2940356135368347\n",
      "Training loss for batch 4625 : 0.2414689064025879\n",
      "Training loss for batch 4626 : 0.23715467751026154\n",
      "Training loss for batch 4627 : 0.12044600397348404\n",
      "Training loss for batch 4628 : 0.09704844653606415\n",
      "Training loss for batch 4629 : 0.1689639538526535\n",
      "Training loss for batch 4630 : 0.02422356605529785\n",
      "Training loss for batch 4631 : 0.2385883629322052\n",
      "Training loss for batch 4632 : 0.33333703875541687\n",
      "Training loss for batch 4633 : 0.11499059200286865\n",
      "Training loss for batch 4634 : 0.1356576383113861\n",
      "Training loss for batch 4635 : 0.14426875114440918\n",
      "Training loss for batch 4636 : 0.2770303189754486\n",
      "Training loss for batch 4637 : 0.20584391057491302\n",
      "Training loss for batch 4638 : 0.24620488286018372\n",
      "Training loss for batch 4639 : 0.2962529957294464\n",
      "Training loss for batch 4640 : 0.26355838775634766\n",
      "Training loss for batch 4641 : 0.109849713742733\n",
      "Training loss for batch 4642 : 0.47101855278015137\n",
      "Training loss for batch 4643 : 0.139888733625412\n",
      "Training loss for batch 4644 : 0.4785224497318268\n",
      "Training loss for batch 4645 : 0.6300800442695618\n",
      "Training loss for batch 4646 : 0.28724056482315063\n",
      "Training loss for batch 4647 : 0.2050245702266693\n",
      "Training loss for batch 4648 : 0.1695934683084488\n",
      "Training loss for batch 4649 : 0.014708280563354492\n",
      "Training loss for batch 4650 : 0.038328494876623154\n",
      "Training loss for batch 4651 : 0.0732756182551384\n",
      "Training loss for batch 4652 : 0.4870988726615906\n",
      "Training loss for batch 4653 : 0.0\n",
      "Training loss for batch 4654 : 0.018763847649097443\n",
      "Training loss for batch 4655 : 0.24215272068977356\n",
      "Training loss for batch 4656 : 0.15248799324035645\n",
      "Training loss for batch 4657 : 0.29540911316871643\n",
      "Training loss for batch 4658 : 0.42460867762565613\n",
      "Training loss for batch 4659 : 0.1562611311674118\n",
      "Training loss for batch 4660 : 0.19520209729671478\n",
      "Training loss for batch 4661 : 0.25475776195526123\n",
      "Training loss for batch 4662 : 0.20007826387882233\n",
      "Training loss for batch 4663 : 0.7303917407989502\n",
      "Training loss for batch 4664 : 0.23048092424869537\n",
      "Training loss for batch 4665 : 0.6142645478248596\n",
      "Training loss for batch 4666 : 0.09969683736562729\n",
      "Training loss for batch 4667 : 0.138091579079628\n",
      "Training loss for batch 4668 : 0.041667088866233826\n",
      "Training loss for batch 4669 : 0.027369100600481033\n",
      "Training loss for batch 4670 : 0.6537182331085205\n",
      "Training loss for batch 4671 : 0.2858402132987976\n",
      "Training loss for batch 4672 : 0.13178908824920654\n",
      "Training loss for batch 4673 : 0.09067585319280624\n",
      "Training loss for batch 4674 : 0.24807032942771912\n",
      "Training loss for batch 4675 : 0.5696538090705872\n",
      "Training loss for batch 4676 : 0.1903994381427765\n",
      "Training loss for batch 4677 : 0.16324804723262787\n",
      "Training loss for batch 4678 : 0.027571901679039\n",
      "Training loss for batch 4679 : 0.09476269781589508\n",
      "Training loss for batch 4680 : 0.6435174942016602\n",
      "Training loss for batch 4681 : 0.0\n",
      "Training loss for batch 4682 : 0.15286552906036377\n",
      "Training loss for batch 4683 : 0.028239663690328598\n",
      "Training loss for batch 4684 : 0.13890275359153748\n",
      "Training loss for batch 4685 : 0.3528894782066345\n",
      "Training loss for batch 4686 : 0.22058098018169403\n",
      "Training loss for batch 4687 : 0.17901866137981415\n",
      "Training loss for batch 4688 : 0.12460929900407791\n",
      "Training loss for batch 4689 : 0.09767583757638931\n",
      "Training loss for batch 4690 : 0.2660088539123535\n",
      "Training loss for batch 4691 : 0.3252589702606201\n",
      "Training loss for batch 4692 : 0.12137421220541\n",
      "Training loss for batch 4693 : 0.2964708209037781\n",
      "Training loss for batch 4694 : 0.5023193955421448\n",
      "Training loss for batch 4695 : 0.10496429353952408\n",
      "Training loss for batch 4696 : 0.1202029138803482\n",
      "Training loss for batch 4697 : 0.11010242253541946\n",
      "Training loss for batch 4698 : 0.1683788299560547\n",
      "Training loss for batch 4699 : 0.17244188487529755\n",
      "Training loss for batch 4700 : 0.2156846672296524\n",
      "Training loss for batch 4701 : 0.16884024441242218\n",
      "Training loss for batch 4702 : 0.08866807818412781\n",
      "Training loss for batch 4703 : 0.490774005651474\n",
      "Training loss for batch 4704 : 0.42395737767219543\n",
      "Training loss for batch 4705 : 0.5234764814376831\n",
      "Training loss for batch 4706 : 0.2993462085723877\n",
      "Training loss for batch 4707 : 0.226473867893219\n",
      "Training loss for batch 4708 : 0.3402232825756073\n",
      "Training loss for batch 4709 : 0.11660709232091904\n",
      "Training loss for batch 4710 : 0.3117719292640686\n",
      "Training loss for batch 4711 : 0.14704453945159912\n",
      "Training loss for batch 4712 : 0.43356457352638245\n",
      "Training loss for batch 4713 : 0.34987086057662964\n",
      "Training loss for batch 4714 : 0.4203375279903412\n",
      "Training loss for batch 4715 : 0.10313364863395691\n",
      "Training loss for batch 4716 : 0.031082473695278168\n",
      "Training loss for batch 4717 : 0.3970552086830139\n",
      "Training loss for batch 4718 : 0.1441461592912674\n",
      "Training loss for batch 4719 : 0.5892668962478638\n",
      "Training loss for batch 4720 : 0.06676249951124191\n",
      "Training loss for batch 4721 : 0.6474273204803467\n",
      "Training loss for batch 4722 : 0.06434062868356705\n",
      "Training loss for batch 4723 : 0.16949573159217834\n",
      "Training loss for batch 4724 : 0.11398347467184067\n",
      "Training loss for batch 4725 : 0.29825401306152344\n",
      "Training loss for batch 4726 : 0.024338314309716225\n",
      "Training loss for batch 4727 : 0.1674824059009552\n",
      "Training loss for batch 4728 : 0.06173292547464371\n",
      "Training loss for batch 4729 : 0.14197370409965515\n",
      "Training loss for batch 4730 : 0.2548213601112366\n",
      "Training loss for batch 4731 : 0.10483818501234055\n",
      "Training loss for batch 4732 : 0.13448958098888397\n",
      "Training loss for batch 4733 : 0.11151305586099625\n",
      "Training loss for batch 4734 : 0.08549364656209946\n",
      "Training loss for batch 4735 : 0.05823492631316185\n",
      "Training loss for batch 4736 : 0.2178398221731186\n",
      "Training loss for batch 4737 : 0.4881761074066162\n",
      "Training loss for batch 4738 : 0.06271874904632568\n",
      "Training loss for batch 4739 : 0.124068483710289\n",
      "Training loss for batch 4740 : 0.2370510846376419\n",
      "Training loss for batch 4741 : 0.030764544382691383\n",
      "Training loss for batch 4742 : 0.015130008570849895\n",
      "Training loss for batch 4743 : 0.2978517711162567\n",
      "Training loss for batch 4744 : 0.2810518443584442\n",
      "Training loss for batch 4745 : 0.0900290384888649\n",
      "Training loss for batch 4746 : 0.40715450048446655\n",
      "Training loss for batch 4747 : 0.12502224743366241\n",
      "Training loss for batch 4748 : 0.15340709686279297\n",
      "Training loss for batch 4749 : 0.15970952808856964\n",
      "Training loss for batch 4750 : 0.13439422845840454\n",
      "Training loss for batch 4751 : 0.15869168937206268\n",
      "Training loss for batch 4752 : 0.14568740129470825\n",
      "Training loss for batch 4753 : 0.231449156999588\n",
      "Training loss for batch 4754 : 0.08870436996221542\n",
      "Training loss for batch 4755 : 0.025041669607162476\n",
      "Training loss for batch 4756 : 0.4291791021823883\n",
      "Training loss for batch 4757 : 0.06472983211278915\n",
      "Training loss for batch 4758 : 0.10401412844657898\n",
      "Training loss for batch 4759 : 0.31584715843200684\n",
      "Training loss for batch 4760 : 0.35305383801460266\n",
      "Training loss for batch 4761 : 0.2739957273006439\n",
      "Training loss for batch 4762 : 0.16773968935012817\n",
      "Training loss for batch 4763 : 0.1563473790884018\n",
      "Training loss for batch 4764 : 0.15590371191501617\n",
      "Training loss for batch 4765 : 0.0011594891548156738\n",
      "Training loss for batch 4766 : 0.19100818037986755\n",
      "Training loss for batch 4767 : 0.09582813084125519\n",
      "Training loss for batch 4768 : 0.12755438685417175\n",
      "Training loss for batch 4769 : 0.052966371178627014\n",
      "Training loss for batch 4770 : 0.22921545803546906\n",
      "Training loss for batch 4771 : 0.3670857846736908\n",
      "Training loss for batch 4772 : 0.2271861582994461\n",
      "Training loss for batch 4773 : 0.5191460847854614\n",
      "Training loss for batch 4774 : 0.1514219343662262\n",
      "Training loss for batch 4775 : 0.06342735886573792\n",
      "Training loss for batch 4776 : 0.21292009949684143\n",
      "Training loss for batch 4777 : 0.09174783527851105\n",
      "Training loss for batch 4778 : 0.1465274542570114\n",
      "Training loss for batch 4779 : 0.2869357764720917\n",
      "Training loss for batch 4780 : 0.663036584854126\n",
      "Training loss for batch 4781 : 0.2442677617073059\n",
      "Training loss for batch 4782 : 0.26128965616226196\n",
      "Training loss for batch 4783 : 0.16874630749225616\n",
      "Training loss for batch 4784 : 1.0259660482406616\n",
      "Training loss for batch 4785 : 0.37964507937431335\n",
      "Training loss for batch 4786 : 0.2799830138683319\n",
      "Training loss for batch 4787 : 0.21771875023841858\n",
      "Training loss for batch 4788 : 0.32953566312789917\n",
      "Training loss for batch 4789 : 0.07552877068519592\n",
      "Training loss for batch 4790 : 0.09530670940876007\n",
      "Training loss for batch 4791 : 0.34399715065956116\n",
      "Training loss for batch 4792 : 0.01077907346189022\n",
      "Training loss for batch 4793 : 0.1092546209692955\n",
      "Training loss for batch 4794 : 0.3830786347389221\n",
      "Training loss for batch 4795 : 0.13544048368930817\n",
      "Training loss for batch 4796 : 0.20661163330078125\n",
      "Training loss for batch 4797 : 0.301098108291626\n",
      "Training loss for batch 4798 : 0.7187450528144836\n",
      "Training loss for batch 4799 : 0.5088686943054199\n",
      "Training loss for batch 4800 : 0.0809643343091011\n",
      "Training loss for batch 4801 : 0.3748934268951416\n",
      "Training loss for batch 4802 : 0.05879403278231621\n",
      "Training loss for batch 4803 : 0.25780269503593445\n",
      "Training loss for batch 4804 : 0.32051515579223633\n",
      "Training loss for batch 4805 : 0.0919179916381836\n",
      "Training loss for batch 4806 : 0.014042368158698082\n",
      "Training loss for batch 4807 : 0.3385719954967499\n",
      "Training loss for batch 4808 : 0.22963672876358032\n",
      "Training loss for batch 4809 : 0.01158806774765253\n",
      "Training loss for batch 4810 : 0.2986238896846771\n",
      "Training loss for batch 4811 : 0.21778173744678497\n",
      "Training loss for batch 4812 : 0.05873902514576912\n",
      "Training loss for batch 4813 : 0.45675092935562134\n",
      "Training loss for batch 4814 : 0.28341811895370483\n",
      "Training loss for batch 4815 : 0.28229913115501404\n",
      "Training loss for batch 4816 : 0.14102007448673248\n",
      "Training loss for batch 4817 : 0.24431656301021576\n",
      "Training loss for batch 4818 : 0.11328449845314026\n",
      "Training loss for batch 4819 : 0.1739092469215393\n",
      "Training loss for batch 4820 : 0.08013654500246048\n",
      "Training loss for batch 4821 : 0.1276339441537857\n",
      "Training loss for batch 4822 : 0.2820936441421509\n",
      "Training loss for batch 4823 : 0.43182024359703064\n",
      "Training loss for batch 4824 : 0.23537275195121765\n",
      "Training loss for batch 4825 : 0.441776305437088\n",
      "Training loss for batch 4826 : 0.13223761320114136\n",
      "Training loss for batch 4827 : 0.28581589460372925\n",
      "Training loss for batch 4828 : 0.5671290159225464\n",
      "Training loss for batch 4829 : 0.31325259804725647\n",
      "Training loss for batch 4830 : 0.127998486161232\n",
      "Training loss for batch 4831 : 0.33942973613739014\n",
      "Training loss for batch 4832 : 0.08564256131649017\n",
      "Training loss for batch 4833 : 0.26665374636650085\n",
      "Training loss for batch 4834 : 0.04013409838080406\n",
      "Training loss for batch 4835 : 0.6813392639160156\n",
      "Training loss for batch 4836 : 0.3340218663215637\n",
      "Training loss for batch 4837 : 0.32211652398109436\n",
      "Training loss for batch 4838 : 0.19273896515369415\n",
      "Training loss for batch 4839 : 0.4274718761444092\n",
      "Training loss for batch 4840 : 0.13356439769268036\n",
      "Training loss for batch 4841 : 0.04362398013472557\n",
      "Training loss for batch 4842 : 0.5059148073196411\n",
      "Training loss for batch 4843 : 0.14233936369419098\n",
      "Training loss for batch 4844 : 0.6978402137756348\n",
      "Training loss for batch 4845 : 0.43178120255470276\n",
      "Training loss for batch 4846 : 0.10319095849990845\n",
      "Training loss for batch 4847 : 0.315620481967926\n",
      "Training loss for batch 4848 : 0.0014001032104715705\n",
      "Training loss for batch 4849 : 0.06306828558444977\n",
      "Training loss for batch 4850 : 0.4466080963611603\n",
      "Training loss for batch 4851 : 0.1715872436761856\n",
      "Training loss for batch 4852 : 0.14052800834178925\n",
      "Training loss for batch 4853 : 0.24166752398014069\n",
      "Training loss for batch 4854 : 0.7660687565803528\n",
      "Training loss for batch 4855 : 0.15909159183502197\n",
      "Training loss for batch 4856 : 0.1352710872888565\n",
      "Training loss for batch 4857 : 0.3022401034832001\n",
      "Training loss for batch 4858 : 0.13442285358905792\n",
      "Training loss for batch 4859 : 0.14613878726959229\n",
      "Training loss for batch 4860 : 0.1418195366859436\n",
      "Training loss for batch 4861 : 0.20984558761119843\n",
      "Training loss for batch 4862 : 0.4519098401069641\n",
      "Training loss for batch 4863 : 0.10540521144866943\n",
      "Training loss for batch 4864 : 0.09682369232177734\n",
      "Training loss for batch 4865 : 0.10046648979187012\n",
      "Training loss for batch 4866 : 0.3088480830192566\n",
      "Training loss for batch 4867 : 0.36522841453552246\n",
      "Training loss for batch 4868 : 0.18228435516357422\n",
      "Training loss for batch 4869 : 0.4007386267185211\n",
      "Training loss for batch 4870 : 0.35745322704315186\n",
      "Training loss for batch 4871 : 0.0693577229976654\n",
      "Training loss for batch 4872 : 0.17851504683494568\n",
      "Training loss for batch 4873 : 0.11021237075328827\n",
      "Training loss for batch 4874 : 0.4712780714035034\n",
      "Training loss for batch 4875 : 0.3884183168411255\n",
      "Training loss for batch 4876 : 0.4539376199245453\n",
      "Training loss for batch 4877 : 0.22485707700252533\n",
      "Training loss for batch 4878 : 0.09791405498981476\n",
      "Training loss for batch 4879 : 0.23244696855545044\n",
      "Training loss for batch 4880 : 0.22146178781986237\n",
      "Training loss for batch 4881 : 0.3968923091888428\n",
      "Training loss for batch 4882 : 0.26423245668411255\n",
      "Training loss for batch 4883 : 0.1626003086566925\n",
      "Training loss for batch 4884 : 0.10710227489471436\n",
      "Training loss for batch 4885 : 0.014099412597715855\n",
      "Training loss for batch 4886 : 0.1362673044204712\n",
      "Training loss for batch 4887 : 0.14301183819770813\n",
      "Training loss for batch 4888 : 0.04605889320373535\n",
      "Training loss for batch 4889 : 0.27186110615730286\n",
      "Training loss for batch 4890 : 0.2410861998796463\n",
      "Training loss for batch 4891 : 0.2506776452064514\n",
      "Training loss for batch 4892 : 0.013036249205470085\n",
      "Training loss for batch 4893 : 0.24725668132305145\n",
      "Training loss for batch 4894 : 0.2505994737148285\n",
      "Training loss for batch 4895 : 0.10858974605798721\n",
      "Training loss for batch 4896 : 0.33199962973594666\n",
      "Training loss for batch 4897 : 0.12709073722362518\n",
      "Training loss for batch 4898 : 0.31624290347099304\n",
      "Training loss for batch 4899 : 0.2066878080368042\n",
      "Training loss for batch 4900 : 0.688453197479248\n",
      "Training loss for batch 4901 : 0.4043824076652527\n",
      "Training loss for batch 4902 : 0.5858428478240967\n",
      "Training loss for batch 4903 : 0.5625991821289062\n",
      "Training loss for batch 4904 : 0.1560789793729782\n",
      "Training loss for batch 4905 : 0.22507412731647491\n",
      "Training loss for batch 4906 : 0.1387997716665268\n",
      "Training loss for batch 4907 : 0.3933485448360443\n",
      "Training loss for batch 4908 : 0.08101349323987961\n",
      "Training loss for batch 4909 : 0.19289526343345642\n",
      "Training loss for batch 4910 : 0.11299189925193787\n",
      "Training loss for batch 4911 : 0.18669871985912323\n",
      "Training loss for batch 4912 : 0.3398444354534149\n",
      "Training loss for batch 4913 : 0.33460533618927\n",
      "Training loss for batch 4914 : 0.09786102175712585\n",
      "Training loss for batch 4915 : 0.110616035759449\n",
      "Training loss for batch 4916 : 0.12610065937042236\n",
      "Training loss for batch 4917 : 0.13196341693401337\n",
      "Training loss for batch 4918 : 0.20179837942123413\n",
      "Training loss for batch 4919 : 0.1306171417236328\n",
      "Training loss for batch 4920 : 0.2822640538215637\n",
      "Training loss for batch 4921 : 0.2371847778558731\n",
      "Training loss for batch 4922 : 0.11692957580089569\n",
      "Training loss for batch 4923 : 0.1821541041135788\n",
      "Training loss for batch 4924 : 0.11755181849002838\n",
      "Training loss for batch 4925 : 0.2752876281738281\n",
      "Training loss for batch 4926 : 0.14190784096717834\n",
      "Training loss for batch 4927 : 0.4407896399497986\n",
      "Training loss for batch 4928 : 0.22026635706424713\n",
      "Training loss for batch 4929 : 0.3549782633781433\n",
      "Training loss for batch 4930 : 0.022876743227243423\n",
      "Training loss for batch 4931 : 0.2064167708158493\n",
      "Training loss for batch 4932 : 0.45007267594337463\n",
      "Training loss for batch 4933 : 0.018488222733139992\n",
      "Training loss for batch 4934 : 0.31616783142089844\n",
      "Training loss for batch 4935 : 0.006819953676313162\n",
      "Training loss for batch 4936 : 0.061195313930511475\n",
      "Training loss for batch 4937 : 0.03388781473040581\n",
      "Training loss for batch 4938 : 0.20065566897392273\n",
      "Training loss for batch 4939 : 0.3087722957134247\n",
      "Training loss for batch 4940 : 0.36188462376594543\n",
      "Training loss for batch 4941 : 0.04393821954727173\n",
      "Training loss for batch 4942 : 0.17355933785438538\n",
      "Training loss for batch 4943 : 0.2142721265554428\n",
      "Training loss for batch 4944 : 0.030317001044750214\n",
      "Training loss for batch 4945 : 0.46519559621810913\n",
      "Training loss for batch 4946 : 0.024295460432767868\n",
      "Training loss for batch 4947 : 0.24909262359142303\n",
      "Training loss for batch 4948 : 0.2683814764022827\n",
      "Training loss for batch 4949 : 0.22787714004516602\n",
      "Training loss for batch 4950 : 0.055972419679164886\n",
      "Training loss for batch 4951 : 0.3506086766719818\n",
      "Training loss for batch 4952 : 0.294187992811203\n",
      "Training loss for batch 4953 : 0.18143263459205627\n",
      "Training loss for batch 4954 : 0.3140624165534973\n",
      "Training loss for batch 4955 : 0.45897823572158813\n",
      "Training loss for batch 4956 : 0.3173559904098511\n",
      "Training loss for batch 4957 : 0.04312437027692795\n",
      "Training loss for batch 4958 : 0.06358431279659271\n",
      "Training loss for batch 4959 : 0.44396358728408813\n",
      "Training loss for batch 4960 : 0.23322400450706482\n",
      "Training loss for batch 4961 : 0.34276282787323\n",
      "Training loss for batch 4962 : 0.2826120853424072\n",
      "Training loss for batch 4963 : 0.06229947507381439\n",
      "Training loss for batch 4964 : 0.5243983864784241\n",
      "Training loss for batch 4965 : 0.17120008170604706\n",
      "Training loss for batch 4966 : 0.33208927512168884\n",
      "Training loss for batch 4967 : 0.2922840118408203\n",
      "Training loss for batch 4968 : 0.2740679085254669\n",
      "Training loss for batch 4969 : 0.2539309561252594\n",
      "Training loss for batch 4970 : 0.11239609867334366\n",
      "Training loss for batch 4971 : 0.04476028308272362\n",
      "Training loss for batch 4972 : 0.30945971608161926\n",
      "Training loss for batch 4973 : 0.2817833125591278\n",
      "Training loss for batch 4974 : 0.0965879037976265\n",
      "Training loss for batch 4975 : 0.37752223014831543\n",
      "Training loss for batch 4976 : 0.11210101842880249\n",
      "Training loss for batch 4977 : 0.17613327503204346\n",
      "Training loss for batch 4978 : 0.04418095201253891\n",
      "Training loss for batch 4979 : 0.005881657358258963\n",
      "Training loss for batch 4980 : 0.45139771699905396\n",
      "Training loss for batch 4981 : 0.002918943762779236\n",
      "Training loss for batch 4982 : 0.19913984835147858\n",
      "Training loss for batch 4983 : 0.03568391129374504\n",
      "Training loss for batch 4984 : 0.31762897968292236\n",
      "Training loss for batch 4985 : 0.09875936061143875\n",
      "Training loss for batch 4986 : 0.43865519762039185\n",
      "Training loss for batch 4987 : 0.0\n",
      "Training loss for batch 4988 : 0.0\n",
      "Training loss for batch 4989 : 0.2539844512939453\n",
      "Training loss for batch 4990 : 0.03315003961324692\n",
      "Training loss for batch 4991 : 0.04522735998034477\n",
      "Training loss for batch 4992 : 0.10308046638965607\n",
      "Training loss for batch 4993 : 0.11462393403053284\n",
      "Training loss for batch 4994 : 0.22839759290218353\n",
      "Training loss for batch 4995 : 0.16582855582237244\n",
      "Training loss for batch 4996 : 0.08641383051872253\n",
      "Training loss for batch 4997 : 0.15192565321922302\n",
      "Training loss for batch 4998 : 0.1371949315071106\n",
      "Training loss for batch 4999 : 0.20803220570087433\n",
      "Training loss for batch 5000 : 0.6453841924667358\n",
      "Training loss for batch 5001 : 0.32556694746017456\n",
      "Training loss for batch 5002 : 0.09915578365325928\n",
      "Training loss for batch 5003 : 0.23448394238948822\n",
      "Training loss for batch 5004 : 0.0052724312990903854\n",
      "Training loss for batch 5005 : 0.03291240707039833\n",
      "Training loss for batch 5006 : 0.1752787083387375\n",
      "Training loss for batch 5007 : 0.12938818335533142\n",
      "Training loss for batch 5008 : 0.10129641741514206\n",
      "Training loss for batch 5009 : 0.4377947151660919\n",
      "Training loss for batch 5010 : 0.15524649620056152\n",
      "Training loss for batch 5011 : 0.04528472572565079\n",
      "Training loss for batch 5012 : 0.03809269890189171\n",
      "Training loss for batch 5013 : 0.1835838109254837\n",
      "Training loss for batch 5014 : 0.16860231757164001\n",
      "Training loss for batch 5015 : 0.44592955708503723\n",
      "Training loss for batch 5016 : 0.6337012648582458\n",
      "Training loss for batch 5017 : 0.4606754183769226\n",
      "Training loss for batch 5018 : 0.20833460986614227\n",
      "Training loss for batch 5019 : 0.43847885727882385\n",
      "Training loss for batch 5020 : 0.49472662806510925\n",
      "Training loss for batch 5021 : 0.2820832431316376\n",
      "Training loss for batch 5022 : 0.2588762640953064\n",
      "Training loss for batch 5023 : 0.29054635763168335\n",
      "Training loss for batch 5024 : 0.5519878268241882\n",
      "Training loss for batch 5025 : 0.006108035799115896\n",
      "Training loss for batch 5026 : 0.3866335451602936\n",
      "Training loss for batch 5027 : 0.2100965678691864\n",
      "Training loss for batch 5028 : 0.032635848969221115\n",
      "Training loss for batch 5029 : 0.14171075820922852\n",
      "Training loss for batch 5030 : 0.08898930251598358\n",
      "Training loss for batch 5031 : 0.343621164560318\n",
      "Training loss for batch 5032 : 0.13581225275993347\n",
      "Training loss for batch 5033 : 0.2917095124721527\n",
      "Training loss for batch 5034 : 0.31617507338523865\n",
      "Training loss for batch 5035 : 0.8278910517692566\n",
      "Training loss for batch 5036 : 0.48793601989746094\n",
      "Training loss for batch 5037 : 0.1760864108800888\n",
      "Training loss for batch 5038 : 0.21943159401416779\n",
      "Training loss for batch 5039 : 0.35121920704841614\n",
      "Training loss for batch 5040 : 0.3110591769218445\n",
      "Training loss for batch 5041 : 0.1269427090883255\n",
      "Training loss for batch 5042 : 0.3787766098976135\n",
      "Training loss for batch 5043 : 0.1658114790916443\n",
      "Training loss for batch 5044 : 0.04174008592963219\n",
      "Training loss for batch 5045 : 0.024197664111852646\n",
      "Training loss for batch 5046 : 0.22573810815811157\n",
      "Training loss for batch 5047 : 0.11224468797445297\n",
      "Training loss for batch 5048 : 0.09731127321720123\n",
      "Training loss for batch 5049 : 0.40564510226249695\n",
      "Training loss for batch 5050 : 0.4109683930873871\n",
      "Training loss for batch 5051 : 0.22047285735607147\n",
      "Training loss for batch 5052 : 0.25215521454811096\n",
      "Training loss for batch 5053 : 0.22041714191436768\n",
      "Training loss for batch 5054 : 0.48084038496017456\n",
      "Training loss for batch 5055 : 0.09972713887691498\n",
      "Training loss for batch 5056 : 0.33529067039489746\n",
      "Training loss for batch 5057 : 0.4255223274230957\n",
      "Training loss for batch 5058 : 0.4406397044658661\n",
      "Training loss for batch 5059 : 0.06992001831531525\n",
      "Training loss for batch 5060 : 0.2545563876628876\n",
      "Training loss for batch 5061 : 0.0635254830121994\n",
      "Training loss for batch 5062 : 0.07291106879711151\n",
      "Training loss for batch 5063 : 0.012731297872960567\n",
      "Training loss for batch 5064 : 0.5665299892425537\n",
      "Training loss for batch 5065 : 0.15887320041656494\n",
      "Training loss for batch 5066 : 0.1444520801305771\n",
      "Training loss for batch 5067 : 0.13389229774475098\n",
      "Training loss for batch 5068 : 0.30350935459136963\n",
      "Training loss for batch 5069 : 0.6317245364189148\n",
      "Training loss for batch 5070 : 0.06268662959337234\n",
      "Training loss for batch 5071 : 0.21504850685596466\n",
      "Training loss for batch 5072 : 0.22653509676456451\n",
      "Training loss for batch 5073 : 0.16377097368240356\n",
      "Training loss for batch 5074 : 0.6222502589225769\n",
      "Training loss for batch 5075 : 0.14119504392147064\n",
      "Training loss for batch 5076 : 0.34956619143486023\n",
      "Training loss for batch 5077 : 0.008424520492553711\n",
      "Training loss for batch 5078 : 0.13238796591758728\n",
      "Training loss for batch 5079 : 0.020829053595662117\n",
      "Training loss for batch 5080 : 0.3378140926361084\n",
      "Training loss for batch 5081 : 0.14891240000724792\n",
      "Training loss for batch 5082 : 0.30441516637802124\n",
      "Training loss for batch 5083 : 0.20795990526676178\n",
      "Training loss for batch 5084 : 0.08264387398958206\n",
      "Training loss for batch 5085 : 0.07339492440223694\n",
      "Training loss for batch 5086 : 0.007128169294446707\n",
      "Training loss for batch 5087 : 0.13734257221221924\n",
      "Training loss for batch 5088 : 0.10996543616056442\n",
      "Training loss for batch 5089 : 0.11886215209960938\n",
      "Training loss for batch 5090 : 0.09925861656665802\n",
      "Training loss for batch 5091 : 0.31819072365760803\n",
      "Training loss for batch 5092 : 0.1008998453617096\n",
      "Training loss for batch 5093 : 0.10801269114017487\n",
      "Training loss for batch 5094 : 0.3131657838821411\n",
      "Training loss for batch 5095 : 0.24384954571723938\n",
      "Training loss for batch 5096 : 0.24043722450733185\n",
      "Training loss for batch 5097 : 0.22870184481143951\n",
      "Training loss for batch 5098 : 0.3381381630897522\n",
      "Training loss for batch 5099 : 0.16509279608726501\n",
      "Training loss for batch 5100 : 0.22630448639392853\n",
      "Training loss for batch 5101 : 0.08527008444070816\n",
      "Training loss for batch 5102 : 0.15120720863342285\n",
      "Training loss for batch 5103 : 0.046471625566482544\n",
      "Training loss for batch 5104 : 0.21828721463680267\n",
      "Training loss for batch 5105 : 0.06863546371459961\n",
      "Training loss for batch 5106 : 0.1083541065454483\n",
      "Training loss for batch 5107 : 0.11278187483549118\n",
      "Training loss for batch 5108 : 0.07124646753072739\n",
      "Training loss for batch 5109 : 0.3525504171848297\n",
      "Training loss for batch 5110 : 0.08054643124341965\n",
      "Training loss for batch 5111 : 0.1810912787914276\n",
      "Training loss for batch 5112 : 0.19725832343101501\n",
      "Training loss for batch 5113 : 0.3613641858100891\n",
      "Training loss for batch 5114 : 0.2079748660326004\n",
      "Training loss for batch 5115 : 0.11274679005146027\n",
      "Training loss for batch 5116 : 0.461203008890152\n",
      "Training loss for batch 5117 : 0.04431159049272537\n",
      "Training loss for batch 5118 : 0.08636713027954102\n",
      "Training loss for batch 5119 : 0.3077770173549652\n",
      "Training loss for batch 5120 : 0.16950707137584686\n",
      "Training loss for batch 5121 : 0.1529349833726883\n",
      "Training loss for batch 5122 : 0.47618773579597473\n",
      "Training loss for batch 5123 : 0.13753041625022888\n",
      "Training loss for batch 5124 : 0.2070206105709076\n",
      "Training loss for batch 5125 : 0.21188859641551971\n",
      "Training loss for batch 5126 : 0.22061561048030853\n",
      "Training loss for batch 5127 : 0.39726895093917847\n",
      "Training loss for batch 5128 : 0.10756947100162506\n",
      "Training loss for batch 5129 : 0.046088576316833496\n",
      "Training loss for batch 5130 : 0.2874417304992676\n",
      "Training loss for batch 5131 : 0.3649403750896454\n",
      "Training loss for batch 5132 : 0.340644896030426\n",
      "Training loss for batch 5133 : 0.1808774620294571\n",
      "Training loss for batch 5134 : 0.058959655463695526\n",
      "Training loss for batch 5135 : 0.2266329675912857\n",
      "Training loss for batch 5136 : 0.043854471296072006\n",
      "Training loss for batch 5137 : 0.1256897747516632\n",
      "Training loss for batch 5138 : 0.32052168250083923\n",
      "Training loss for batch 5139 : 0.057391420006752014\n",
      "Training loss for batch 5140 : 0.14485639333724976\n",
      "Training loss for batch 5141 : 0.22638718783855438\n",
      "Training loss for batch 5142 : 0.47523605823516846\n",
      "Training loss for batch 5143 : 0.05110901594161987\n",
      "Training loss for batch 5144 : 0.4798811972141266\n",
      "Training loss for batch 5145 : 0.3417343199253082\n",
      "Training loss for batch 5146 : 0.0782611221075058\n",
      "Training loss for batch 5147 : 0.11017683148384094\n",
      "Training loss for batch 5148 : 0.0\n",
      "Training loss for batch 5149 : 0.027988243848085403\n",
      "Training loss for batch 5150 : 0.2962748110294342\n",
      "Training loss for batch 5151 : 0.4235605299472809\n",
      "Training loss for batch 5152 : 0.4201296269893646\n",
      "Training loss for batch 5153 : 0.09874670952558517\n",
      "Training loss for batch 5154 : 0.10021353513002396\n",
      "Training loss for batch 5155 : 0.5272244811058044\n",
      "Training loss for batch 5156 : 0.1805572360754013\n",
      "Training loss for batch 5157 : 0.18986637890338898\n",
      "Training loss for batch 5158 : 0.050036314874887466\n",
      "Training loss for batch 5159 : 0.3266696035861969\n",
      "Training loss for batch 5160 : 0.0023422837257385254\n",
      "Training loss for batch 5161 : 0.20223967730998993\n",
      "Training loss for batch 5162 : 0.29343658685684204\n",
      "Training loss for batch 5163 : 0.3705763816833496\n",
      "Training loss for batch 5164 : 0.29284200072288513\n",
      "Training loss for batch 5165 : 0.5326465964317322\n",
      "Training loss for batch 5166 : 0.03267010673880577\n",
      "Training loss for batch 5167 : 0.3055199682712555\n",
      "Training loss for batch 5168 : 0.22698140144348145\n",
      "Training loss for batch 5169 : 0.3203507661819458\n",
      "Training loss for batch 5170 : 0.05558357387781143\n",
      "Training loss for batch 5171 : 0.06921987235546112\n",
      "Training loss for batch 5172 : 0.1725577712059021\n",
      "Training loss for batch 5173 : 0.20139604806900024\n",
      "Training loss for batch 5174 : 0.374615877866745\n",
      "Training loss for batch 5175 : 0.09966805577278137\n",
      "Training loss for batch 5176 : 0.1094607561826706\n",
      "Training loss for batch 5177 : 0.1073530912399292\n",
      "Training loss for batch 5178 : 0.13145725429058075\n",
      "Training loss for batch 5179 : 0.1617826521396637\n",
      "Training loss for batch 5180 : 0.06661560386419296\n",
      "Training loss for batch 5181 : 0.02369476854801178\n",
      "Training loss for batch 5182 : 0.13377675414085388\n",
      "Training loss for batch 5183 : 0.3212852478027344\n",
      "Training loss for batch 5184 : 0.6575255393981934\n",
      "Training loss for batch 5185 : 0.14195504784584045\n",
      "Training loss for batch 5186 : 0.5548993349075317\n",
      "Training loss for batch 5187 : 0.24554389715194702\n",
      "Training loss for batch 5188 : 0.009910215623676777\n",
      "Training loss for batch 5189 : 0.15141145884990692\n",
      "Training loss for batch 5190 : 0.09906836599111557\n",
      "Training loss for batch 5191 : 0.19281989336013794\n",
      "Training loss for batch 5192 : 0.1667894572019577\n",
      "Training loss for batch 5193 : 0.16580046713352203\n",
      "Training loss for batch 5194 : 0.23203638195991516\n",
      "Training loss for batch 5195 : 0.24341389536857605\n",
      "Training loss for batch 5196 : 0.004333104006946087\n",
      "Training loss for batch 5197 : 0.3108713924884796\n",
      "Training loss for batch 5198 : 0.2619195878505707\n",
      "Training loss for batch 5199 : 0.1435665786266327\n",
      "Training loss for batch 5200 : 0.15758661925792694\n",
      "Training loss for batch 5201 : 0.10641549527645111\n",
      "Training loss for batch 5202 : 0.2695084512233734\n",
      "Training loss for batch 5203 : 0.25282391905784607\n",
      "Training loss for batch 5204 : 0.5866437554359436\n",
      "Training loss for batch 5205 : 0.20386533439159393\n",
      "Training loss for batch 5206 : 0.39474794268608093\n",
      "Training loss for batch 5207 : 0.2750397324562073\n",
      "Training loss for batch 5208 : 0.21482765674591064\n",
      "Training loss for batch 5209 : 0.19552084803581238\n",
      "Training loss for batch 5210 : 0.1145954579114914\n",
      "Training loss for batch 5211 : 0.30637794733047485\n",
      "Training loss for batch 5212 : 0.37333157658576965\n",
      "Training loss for batch 5213 : 0.5517879724502563\n",
      "Training loss for batch 5214 : 0.0014619231224060059\n",
      "Training loss for batch 5215 : 0.2233314961194992\n",
      "Training loss for batch 5216 : 0.0\n",
      "Training loss for batch 5217 : 0.3656168580055237\n",
      "Training loss for batch 5218 : 0.012709853239357471\n",
      "Training loss for batch 5219 : 0.3710349202156067\n",
      "Training loss for batch 5220 : 0.5307985544204712\n",
      "Training loss for batch 5221 : 0.19236832857131958\n",
      "Training loss for batch 5222 : 0.317376047372818\n",
      "Training loss for batch 5223 : 0.20548760890960693\n",
      "Training loss for batch 5224 : 0.1740831732749939\n",
      "Training loss for batch 5225 : 0.0\n",
      "Training loss for batch 5226 : 0.10608848184347153\n",
      "Training loss for batch 5227 : 0.01995975710451603\n",
      "Training loss for batch 5228 : 0.14277693629264832\n",
      "Training loss for batch 5229 : 0.1604764312505722\n",
      "Training loss for batch 5230 : 0.36299651861190796\n",
      "Training loss for batch 5231 : 0.06255626678466797\n",
      "Training loss for batch 5232 : 0.13347870111465454\n",
      "Training loss for batch 5233 : 0.31309741735458374\n",
      "Training loss for batch 5234 : 0.1304822713136673\n",
      "Training loss for batch 5235 : 0.21364733576774597\n",
      "Training loss for batch 5236 : 0.2410275936126709\n",
      "Training loss for batch 5237 : 0.36351728439331055\n",
      "Training loss for batch 5238 : 0.03572345897555351\n",
      "Training loss for batch 5239 : 0.24970194697380066\n",
      "Training loss for batch 5240 : 0.17151618003845215\n",
      "Training loss for batch 5241 : 0.1539831906557083\n",
      "Training loss for batch 5242 : 0.034323520958423615\n",
      "Training loss for batch 5243 : 0.5605208277702332\n",
      "Training loss for batch 5244 : 0.18514426052570343\n",
      "Training loss for batch 5245 : 0.06791181117296219\n",
      "Training loss for batch 5246 : 0.24894191324710846\n",
      "Training loss for batch 5247 : 0.30964815616607666\n",
      "Training loss for batch 5248 : 0.14898477494716644\n",
      "Training loss for batch 5249 : 0.28054070472717285\n",
      "Training loss for batch 5250 : 0.11123894900083542\n",
      "Training loss for batch 5251 : 0.20755748450756073\n",
      "Training loss for batch 5252 : 0.15948471426963806\n",
      "Training loss for batch 5253 : 0.3508639335632324\n",
      "Training loss for batch 5254 : 0.07746091485023499\n",
      "Training loss for batch 5255 : 0.2624497711658478\n",
      "Training loss for batch 5256 : 0.36178624629974365\n",
      "Training loss for batch 5257 : 0.08691660314798355\n",
      "Training loss for batch 5258 : 0.23492373526096344\n",
      "Training loss for batch 5259 : 0.19336232542991638\n",
      "Training loss for batch 5260 : 0.3504219353199005\n",
      "Training loss for batch 5261 : 0.4060898423194885\n",
      "Training loss for batch 5262 : 0.323386013507843\n",
      "Training loss for batch 5263 : 0.2446686327457428\n",
      "Training loss for batch 5264 : 0.07366026192903519\n",
      "Training loss for batch 5265 : 0.14314252138137817\n",
      "Training loss for batch 5266 : 0.29470598697662354\n",
      "Training loss for batch 5267 : 0.05884784087538719\n",
      "Training loss for batch 5268 : 0.027659814804792404\n",
      "Training loss for batch 5269 : 0.16721494495868683\n",
      "Training loss for batch 5270 : 0.2483266443014145\n",
      "Training loss for batch 5271 : 0.3650110960006714\n",
      "Training loss for batch 5272 : 0.26848918199539185\n",
      "Training loss for batch 5273 : 0.08080501854419708\n",
      "Training loss for batch 5274 : 0.2596316933631897\n",
      "Training loss for batch 5275 : 0.011445297859609127\n",
      "Training loss for batch 5276 : 0.14365531504154205\n",
      "Training loss for batch 5277 : 0.12044333666563034\n",
      "Training loss for batch 5278 : 0.15804368257522583\n",
      "Training loss for batch 5279 : 0.22294814884662628\n",
      "Training loss for batch 5280 : 0.3165544271469116\n",
      "Training loss for batch 5281 : 0.09015671163797379\n",
      "Training loss for batch 5282 : 0.19842706620693207\n",
      "Training loss for batch 5283 : 0.2424994707107544\n",
      "Training loss for batch 5284 : 0.6951890587806702\n",
      "Training loss for batch 5285 : 0.246473491191864\n",
      "Training loss for batch 5286 : 0.18969348073005676\n",
      "Training loss for batch 5287 : 0.38514429330825806\n",
      "Training loss for batch 5288 : 0.16166818141937256\n",
      "Training loss for batch 5289 : 0.25349000096321106\n",
      "Training loss for batch 5290 : 0.08950014412403107\n",
      "Training loss for batch 5291 : 0.277112752199173\n",
      "Training loss for batch 5292 : 0.018008925020694733\n",
      "Training loss for batch 5293 : 0.2709377408027649\n",
      "Training loss for batch 5294 : 0.2808457612991333\n",
      "Training loss for batch 5295 : 0.21505306661128998\n",
      "Training loss for batch 5296 : 0.16522715985774994\n",
      "Training loss for batch 5297 : 0.03994079306721687\n",
      "Training loss for batch 5298 : 0.3384183347225189\n",
      "Training loss for batch 5299 : 0.4205683469772339\n",
      "Training loss for batch 5300 : 0.0668821856379509\n",
      "Training loss for batch 5301 : 0.06501613557338715\n",
      "Training loss for batch 5302 : 0.29701393842697144\n",
      "Training loss for batch 5303 : 0.04913736879825592\n",
      "Training loss for batch 5304 : 0.20929986238479614\n",
      "Training loss for batch 5305 : 0.09166782349348068\n",
      "Training loss for batch 5306 : 0.4177396893501282\n",
      "Training loss for batch 5307 : 0.003356671193614602\n",
      "Training loss for batch 5308 : 0.17318490147590637\n",
      "Training loss for batch 5309 : 0.45780104398727417\n",
      "Training loss for batch 5310 : 0.21900451183319092\n",
      "Training loss for batch 5311 : 0.4108909070491791\n",
      "Training loss for batch 5312 : 0.09250075370073318\n",
      "Training loss for batch 5313 : 0.5366615653038025\n",
      "Training loss for batch 5314 : 0.1108931377530098\n",
      "Training loss for batch 5315 : 0.19584228098392487\n",
      "Training loss for batch 5316 : 0.2952777147293091\n",
      "Training loss for batch 5317 : 0.372432142496109\n",
      "Training loss for batch 5318 : 0.14872542023658752\n",
      "Training loss for batch 5319 : 0.30447477102279663\n",
      "Training loss for batch 5320 : 0.4841896593570709\n",
      "Training loss for batch 5321 : 0.06640570610761642\n",
      "Training loss for batch 5322 : 0.6368617415428162\n",
      "Training loss for batch 5323 : 0.0005097925313748419\n",
      "Training loss for batch 5324 : 0.141419917345047\n",
      "Training loss for batch 5325 : 0.35485681891441345\n",
      "Training loss for batch 5326 : 0.09223202615976334\n",
      "Training loss for batch 5327 : 0.35673603415489197\n",
      "Training loss for batch 5328 : 0.1455775946378708\n",
      "Training loss for batch 5329 : 0.37712109088897705\n",
      "Training loss for batch 5330 : 0.2978694438934326\n",
      "Training loss for batch 5331 : 0.08617059886455536\n",
      "Training loss for batch 5332 : 0.0065019032917916775\n",
      "Training loss for batch 5333 : 0.7529488801956177\n",
      "Training loss for batch 5334 : 0.22695022821426392\n",
      "Training loss for batch 5335 : 0.48557382822036743\n",
      "Training loss for batch 5336 : 0.3585152328014374\n",
      "Training loss for batch 5337 : 0.35593464970588684\n",
      "Training loss for batch 5338 : 0.6849672794342041\n",
      "Training loss for batch 5339 : 0.09818151593208313\n",
      "Training loss for batch 5340 : 0.2673187553882599\n",
      "Training loss for batch 5341 : 0.059285543859004974\n",
      "Training loss for batch 5342 : 0.43360790610313416\n",
      "Training loss for batch 5343 : 0.3269401490688324\n",
      "Training loss for batch 5344 : 0.6969632506370544\n",
      "Training loss for batch 5345 : 0.19010227918624878\n",
      "Training loss for batch 5346 : 0.13531261682510376\n",
      "Training loss for batch 5347 : 0.2067231982946396\n",
      "Training loss for batch 5348 : 0.11215879023075104\n",
      "Training loss for batch 5349 : 0.35729125142097473\n",
      "Training loss for batch 5350 : 0.13705705106258392\n",
      "Training loss for batch 5351 : 0.29840973019599915\n",
      "Training loss for batch 5352 : 0.3538737893104553\n",
      "Training loss for batch 5353 : 0.35054144263267517\n",
      "Training loss for batch 5354 : 0.31075480580329895\n",
      "Training loss for batch 5355 : 0.013880676589906216\n",
      "Training loss for batch 5356 : 0.10883091390132904\n",
      "Training loss for batch 5357 : 0.06528959423303604\n",
      "Training loss for batch 5358 : 0.0563708133995533\n",
      "Training loss for batch 5359 : 0.16142180562019348\n",
      "Training loss for batch 5360 : 0.12786754965782166\n",
      "Training loss for batch 5361 : 0.12805823981761932\n",
      "Training loss for batch 5362 : 0.028372200205922127\n",
      "Training loss for batch 5363 : 0.09098629653453827\n",
      "Training loss for batch 5364 : 0.13272687792778015\n",
      "Training loss for batch 5365 : 0.2046525627374649\n",
      "Training loss for batch 5366 : 0.09258846938610077\n",
      "Training loss for batch 5367 : 0.07142075151205063\n",
      "Training loss for batch 5368 : 0.11217494308948517\n",
      "Training loss for batch 5369 : 0.1815928816795349\n",
      "Training loss for batch 5370 : 0.11087776720523834\n",
      "Training loss for batch 5371 : 0.5761186480522156\n",
      "Training loss for batch 5372 : 0.20088601112365723\n",
      "Training loss for batch 5373 : 0.39274418354034424\n",
      "Training loss for batch 5374 : 0.13383744657039642\n",
      "Training loss for batch 5375 : 0.4905139207839966\n",
      "Training loss for batch 5376 : 0.3757711350917816\n",
      "Training loss for batch 5377 : 0.19464266300201416\n",
      "Training loss for batch 5378 : 0.0732751116156578\n",
      "Training loss for batch 5379 : 0.1591222882270813\n",
      "Training loss for batch 5380 : 0.0823579952120781\n",
      "Training loss for batch 5381 : 0.0\n",
      "Training loss for batch 5382 : 0.12244629114866257\n",
      "Training loss for batch 5383 : 0.060394421219825745\n",
      "Training loss for batch 5384 : 0.23232127726078033\n",
      "Training loss for batch 5385 : 0.07588154077529907\n",
      "Training loss for batch 5386 : 0.3925405740737915\n",
      "Training loss for batch 5387 : 0.023304151371121407\n",
      "Training loss for batch 5388 : 0.13881517946720123\n",
      "Training loss for batch 5389 : 0.4469684064388275\n",
      "Training loss for batch 5390 : 0.2867618799209595\n",
      "Training loss for batch 5391 : 0.0483863465487957\n",
      "Training loss for batch 5392 : 0.4581012725830078\n",
      "Training loss for batch 5393 : 0.032124221324920654\n",
      "Training loss for batch 5394 : 0.22338123619556427\n",
      "Training loss for batch 5395 : 0.12112481147050858\n",
      "Training loss for batch 5396 : 0.16785448789596558\n",
      "Training loss for batch 5397 : 0.37049925327301025\n",
      "Training loss for batch 5398 : 0.24607069790363312\n",
      "Training loss for batch 5399 : 0.15829479694366455\n",
      "Training loss for batch 5400 : 0.0002645549539010972\n",
      "Training loss for batch 5401 : 0.03975810110569\n",
      "Training loss for batch 5402 : 0.2153763622045517\n",
      "Training loss for batch 5403 : 0.14443321526050568\n",
      "Training loss for batch 5404 : 0.07577037066221237\n",
      "Training loss for batch 5405 : 0.4638090133666992\n",
      "Training loss for batch 5406 : 0.5591451525688171\n",
      "Training loss for batch 5407 : 0.1955946981906891\n",
      "Training loss for batch 5408 : 0.14721304178237915\n",
      "Training loss for batch 5409 : 0.27193188667297363\n",
      "Training loss for batch 5410 : 0.10481220483779907\n",
      "Training loss for batch 5411 : 0.3407023847103119\n",
      "Training loss for batch 5412 : 0.2174203246831894\n",
      "Training loss for batch 5413 : 0.15480387210845947\n",
      "Training loss for batch 5414 : 0.25845611095428467\n",
      "Training loss for batch 5415 : 0.17107626795768738\n",
      "Training loss for batch 5416 : 0.15007929503917694\n",
      "Training loss for batch 5417 : 0.45755675435066223\n",
      "Training loss for batch 5418 : 0.12811560928821564\n",
      "Training loss for batch 5419 : 0.1863253265619278\n",
      "Training loss for batch 5420 : 0.07711713016033173\n",
      "Training loss for batch 5421 : 0.04740948975086212\n",
      "Training loss for batch 5422 : 0.07328277081251144\n",
      "Training loss for batch 5423 : 0.09537405520677567\n",
      "Training loss for batch 5424 : 0.31506335735321045\n",
      "Training loss for batch 5425 : 0.15584321320056915\n",
      "Training loss for batch 5426 : 0.22569070756435394\n",
      "Training loss for batch 5427 : 0.12612424790859222\n",
      "Training loss for batch 5428 : 0.04334477707743645\n",
      "Training loss for batch 5429 : 0.5235804319381714\n",
      "Training loss for batch 5430 : 0.3139401376247406\n",
      "Training loss for batch 5431 : 0.3208988308906555\n",
      "Training loss for batch 5432 : 0.04776870459318161\n",
      "Training loss for batch 5433 : 0.19241513311862946\n",
      "Training loss for batch 5434 : 0.0003384053707122803\n",
      "Training loss for batch 5435 : 0.2822555601596832\n",
      "Training loss for batch 5436 : 0.11339372396469116\n",
      "Training loss for batch 5437 : 0.10665587335824966\n",
      "Training loss for batch 5438 : 0.05932845547795296\n",
      "Training loss for batch 5439 : 0.8663259744644165\n",
      "Training loss for batch 5440 : 0.18288354575634003\n",
      "Training loss for batch 5441 : 0.162417933344841\n",
      "Training loss for batch 5442 : 0.12381795793771744\n",
      "Training loss for batch 5443 : 0.04493909329175949\n",
      "Training loss for batch 5444 : 0.19529496133327484\n",
      "Training loss for batch 5445 : 0.4391371011734009\n",
      "Training loss for batch 5446 : 0.057112276554107666\n",
      "Training loss for batch 5447 : 0.16471923887729645\n",
      "Training loss for batch 5448 : 0.1324804127216339\n",
      "Training loss for batch 5449 : 0.0183272548019886\n",
      "Training loss for batch 5450 : 0.1986345499753952\n",
      "Training loss for batch 5451 : 0.2412223219871521\n",
      "Training loss for batch 5452 : 0.10702213644981384\n",
      "Training loss for batch 5453 : 0.19677859544754028\n",
      "Training loss for batch 5454 : 0.18254560232162476\n",
      "Training loss for batch 5455 : 0.054439056664705276\n",
      "Training loss for batch 5456 : 0.5332197546958923\n",
      "Training loss for batch 5457 : 0.1198350042104721\n",
      "Training loss for batch 5458 : 0.3671412765979767\n",
      "Training loss for batch 5459 : 0.11798994243144989\n",
      "Training loss for batch 5460 : 0.2725640833377838\n",
      "Training loss for batch 5461 : 0.3381269574165344\n",
      "Training loss for batch 5462 : 0.24765224754810333\n",
      "Training loss for batch 5463 : 0.08583690971136093\n",
      "Training loss for batch 5464 : 0.29618561267852783\n",
      "Training loss for batch 5465 : 0.38445818424224854\n",
      "Training loss for batch 5466 : 0.02697695977985859\n",
      "Training loss for batch 5467 : 0.16779954731464386\n",
      "Training loss for batch 5468 : 0.05545986071228981\n",
      "Training loss for batch 5469 : 0.08749677985906601\n",
      "Training loss for batch 5470 : 0.17329497635364532\n",
      "Training loss for batch 5471 : 0.36380308866500854\n",
      "Training loss for batch 5472 : 0.2510763108730316\n",
      "Training loss for batch 5473 : 0.015433430671691895\n",
      "Training loss for batch 5474 : 0.5014678835868835\n",
      "Training loss for batch 5475 : 0.05814293026924133\n",
      "Training loss for batch 5476 : 0.13557825982570648\n",
      "Training loss for batch 5477 : 0.2806812822818756\n",
      "Training loss for batch 5478 : 0.2623255252838135\n",
      "Training loss for batch 5479 : 0.08679599314928055\n",
      "Training loss for batch 5480 : 0.30728232860565186\n",
      "Training loss for batch 5481 : 0.2633465826511383\n",
      "Training loss for batch 5482 : 0.09690532088279724\n",
      "Training loss for batch 5483 : 0.3710313141345978\n",
      "Training loss for batch 5484 : 0.07714580744504929\n",
      "Training loss for batch 5485 : 0.2932368814945221\n",
      "Training loss for batch 5486 : 0.39239057898521423\n",
      "Training loss for batch 5487 : 0.43822920322418213\n",
      "Training loss for batch 5488 : 0.12081360071897507\n",
      "Training loss for batch 5489 : 0.24937188625335693\n",
      "Training loss for batch 5490 : 0.029556123539805412\n",
      "Training loss for batch 5491 : 0.09004218131303787\n",
      "Training loss for batch 5492 : 0.08338702470064163\n",
      "Training loss for batch 5493 : 0.1340726763010025\n",
      "Training loss for batch 5494 : 0.10570121556520462\n",
      "Training loss for batch 5495 : 0.34410327672958374\n",
      "Training loss for batch 5496 : 0.03321981057524681\n",
      "Training loss for batch 5497 : 0.33620113134384155\n",
      "Training loss for batch 5498 : 0.14570672810077667\n",
      "Training loss for batch 5499 : 0.1724136620759964\n",
      "Training loss for batch 5500 : 0.500264048576355\n",
      "Training loss for batch 5501 : 0.28451427817344666\n",
      "Training loss for batch 5502 : 0.06212163344025612\n",
      "Training loss for batch 5503 : 0.3695686459541321\n",
      "Training loss for batch 5504 : 0.20999328792095184\n",
      "Training loss for batch 5505 : 0.9057921767234802\n",
      "Training loss for batch 5506 : 0.2237769216299057\n",
      "Training loss for batch 5507 : 0.4778288006782532\n",
      "Training loss for batch 5508 : 0.1365637332201004\n",
      "Training loss for batch 5509 : 0.13794229924678802\n",
      "Training loss for batch 5510 : 0.08255118131637573\n",
      "Training loss for batch 5511 : 0.07589971274137497\n",
      "Training loss for batch 5512 : 0.07226008921861649\n",
      "Training loss for batch 5513 : 0.26906296610832214\n",
      "Training loss for batch 5514 : 0.20752917230129242\n",
      "Training loss for batch 5515 : 0.07300811260938644\n",
      "Training loss for batch 5516 : 0.003664642572402954\n",
      "Training loss for batch 5517 : 0.2864205837249756\n",
      "Training loss for batch 5518 : 0.44874170422554016\n",
      "Training loss for batch 5519 : 0.07084645330905914\n",
      "Training loss for batch 5520 : 0.32015755772590637\n",
      "Training loss for batch 5521 : 0.19502529501914978\n",
      "Training loss for batch 5522 : 0.229725182056427\n",
      "Training loss for batch 5523 : 0.22788311541080475\n",
      "Training loss for batch 5524 : 0.06132624298334122\n",
      "Training loss for batch 5525 : 0.14870592951774597\n",
      "Training loss for batch 5526 : 0.0\n",
      "Training loss for batch 5527 : 0.09347157180309296\n",
      "Training loss for batch 5528 : 0.03527318686246872\n",
      "Training loss for batch 5529 : 0.2564277946949005\n",
      "Training loss for batch 5530 : 0.11813899874687195\n",
      "Training loss for batch 5531 : 0.29154857993125916\n",
      "Training loss for batch 5532 : 0.43024852871894836\n",
      "Training loss for batch 5533 : 0.1360214352607727\n",
      "Training loss for batch 5534 : 0.04533318802714348\n",
      "Training loss for batch 5535 : 0.2729761600494385\n",
      "Training loss for batch 5536 : 0.4432389736175537\n",
      "Training loss for batch 5537 : 0.11037299782037735\n",
      "Training loss for batch 5538 : 0.26002007722854614\n",
      "Training loss for batch 5539 : 0.398861825466156\n",
      "Training loss for batch 5540 : 0.2364145666360855\n",
      "Training loss for batch 5541 : 0.3719763159751892\n",
      "Training loss for batch 5542 : 0.10482091456651688\n",
      "Training loss for batch 5543 : 0.05884384736418724\n",
      "Training loss for batch 5544 : 0.26171553134918213\n",
      "Training loss for batch 5545 : 0.006961424369364977\n",
      "Training loss for batch 5546 : 0.07542477548122406\n",
      "Training loss for batch 5547 : 0.43232032656669617\n",
      "Training loss for batch 5548 : 0.30019691586494446\n",
      "Training loss for batch 5549 : 0.04402052238583565\n",
      "Training loss for batch 5550 : 0.1272411197423935\n",
      "Training loss for batch 5551 : 0.07335656136274338\n",
      "Training loss for batch 5552 : 0.5547338724136353\n",
      "Training loss for batch 5553 : 0.1392352133989334\n",
      "Training loss for batch 5554 : 0.11582401394844055\n",
      "Training loss for batch 5555 : 0.2779428958892822\n",
      "Training loss for batch 5556 : 0.2053612768650055\n",
      "Training loss for batch 5557 : 0.393450528383255\n",
      "Training loss for batch 5558 : 0.03966774791479111\n",
      "Training loss for batch 5559 : 0.6698389649391174\n",
      "Training loss for batch 5560 : 0.21405628323554993\n",
      "Training loss for batch 5561 : 0.06027296930551529\n",
      "Training loss for batch 5562 : 0.1400981843471527\n",
      "Training loss for batch 5563 : 0.01082901656627655\n",
      "Training loss for batch 5564 : 0.04879371449351311\n",
      "Training loss for batch 5565 : 0.06084437668323517\n",
      "Training loss for batch 5566 : 0.0\n",
      "Training loss for batch 5567 : 0.11069199442863464\n",
      "Training loss for batch 5568 : 0.1273222267627716\n",
      "Training loss for batch 5569 : 0.1640777289867401\n",
      "Training loss for batch 5570 : 0.06075647845864296\n",
      "Training loss for batch 5571 : 0.08677280694246292\n",
      "Training loss for batch 5572 : 0.03375658765435219\n",
      "Training loss for batch 5573 : 0.4672382175922394\n",
      "Training loss for batch 5574 : 0.5736628770828247\n",
      "Training loss for batch 5575 : 0.0105575667694211\n",
      "Training loss for batch 5576 : 0.019333835691213608\n",
      "Training loss for batch 5577 : 0.10843263566493988\n",
      "Training loss for batch 5578 : 0.17531991004943848\n",
      "Training loss for batch 5579 : 0.2592598497867584\n",
      "Training loss for batch 5580 : 0.25619226694107056\n",
      "Training loss for batch 5581 : 0.5995610356330872\n",
      "Training loss for batch 5582 : 0.03355022519826889\n",
      "Training loss for batch 5583 : 0.07142867892980576\n",
      "Training loss for batch 5584 : 0.31838318705558777\n",
      "Training loss for batch 5585 : 0.4901336133480072\n",
      "Training loss for batch 5586 : 0.45565110445022583\n",
      "Training loss for batch 5587 : 0.28266650438308716\n",
      "Training loss for batch 5588 : 0.06365896016359329\n",
      "Training loss for batch 5589 : 0.15771327912807465\n",
      "Training loss for batch 5590 : 0.02119453437626362\n",
      "Training loss for batch 5591 : 0.10673752427101135\n",
      "Training loss for batch 5592 : 0.1607523113489151\n",
      "Training loss for batch 5593 : 0.41845688223838806\n",
      "Training loss for batch 5594 : 0.3208290636539459\n",
      "Training loss for batch 5595 : 0.012734842486679554\n",
      "Training loss for batch 5596 : 0.034373097121715546\n",
      "Training loss for batch 5597 : 0.27356216311454773\n",
      "Training loss for batch 5598 : 0.4188765287399292\n",
      "Training loss for batch 5599 : 0.050544947385787964\n",
      "Training loss for batch 5600 : 0.10612722486257553\n",
      "Training loss for batch 5601 : 0.18273524940013885\n",
      "Training loss for batch 5602 : 0.0947817713022232\n",
      "Training loss for batch 5603 : 0.015385529026389122\n",
      "Training loss for batch 5604 : 0.13394345343112946\n",
      "Training loss for batch 5605 : 0.1288733184337616\n",
      "Training loss for batch 5606 : 0.14493276178836823\n",
      "Training loss for batch 5607 : 0.21303825080394745\n",
      "Training loss for batch 5608 : 0.08037970215082169\n",
      "Training loss for batch 5609 : 0.21725736558437347\n",
      "Training loss for batch 5610 : 0.13211269676685333\n",
      "Training loss for batch 5611 : 0.1730884462594986\n",
      "Training loss for batch 5612 : 0.48478689789772034\n",
      "Training loss for batch 5613 : 0.4657667875289917\n",
      "Training loss for batch 5614 : 0.38329675793647766\n",
      "Training loss for batch 5615 : 0.2922420799732208\n",
      "Training loss for batch 5616 : 0.2776804268360138\n",
      "Training loss for batch 5617 : 0.08654747903347015\n",
      "Training loss for batch 5618 : 0.42782238125801086\n",
      "Training loss for batch 5619 : 0.4898974895477295\n",
      "Training loss for batch 5620 : 0.7553704977035522\n",
      "Training loss for batch 5621 : 0.0006008744239807129\n",
      "Training loss for batch 5622 : 0.06490899622440338\n",
      "Training loss for batch 5623 : 0.05585883557796478\n",
      "Training loss for batch 5624 : 0.20369961857795715\n",
      "Training loss for batch 5625 : 0.7160261869430542\n",
      "Training loss for batch 5626 : 0.15736299753189087\n",
      "Training loss for batch 5627 : 0.09317318350076675\n",
      "Training loss for batch 5628 : 0.09252087771892548\n",
      "Training loss for batch 5629 : 0.19176124036312103\n",
      "Training loss for batch 5630 : 0.2516632080078125\n",
      "Training loss for batch 5631 : 0.24950134754180908\n",
      "Training loss for batch 5632 : 0.12948419153690338\n",
      "Training loss for batch 5633 : 0.20071333646774292\n",
      "Training loss for batch 5634 : 0.08657453954219818\n",
      "Training loss for batch 5635 : 0.39493680000305176\n",
      "Training loss for batch 5636 : 0.5114094018936157\n",
      "Training loss for batch 5637 : 0.01905708573758602\n",
      "Training loss for batch 5638 : 0.3413688838481903\n",
      "Training loss for batch 5639 : 0.2813778817653656\n",
      "Training loss for batch 5640 : 0.10665848851203918\n",
      "Training loss for batch 5641 : 0.18453443050384521\n",
      "Training loss for batch 5642 : 0.2869850993156433\n",
      "Training loss for batch 5643 : 0.29555749893188477\n",
      "Training loss for batch 5644 : 0.2102949023246765\n",
      "Training loss for batch 5645 : 0.06339674443006516\n",
      "Training loss for batch 5646 : 0.2829170227050781\n",
      "Training loss for batch 5647 : 0.3147519528865814\n",
      "Training loss for batch 5648 : 0.14499428868293762\n",
      "Training loss for batch 5649 : 0.47829970717430115\n",
      "Training loss for batch 5650 : 0.029202062636613846\n",
      "Training loss for batch 5651 : 0.19259001314640045\n",
      "Training loss for batch 5652 : 0.0802403911948204\n",
      "Training loss for batch 5653 : 0.11541103571653366\n",
      "Training loss for batch 5654 : 0.09624786674976349\n",
      "Training loss for batch 5655 : 0.04543887451291084\n",
      "Training loss for batch 5656 : 0.20270293951034546\n",
      "Training loss for batch 5657 : 0.04268111288547516\n",
      "Training loss for batch 5658 : 0.4113278388977051\n",
      "Training loss for batch 5659 : 0.13513921201229095\n",
      "Training loss for batch 5660 : 0.06986317038536072\n",
      "Training loss for batch 5661 : 0.14187724888324738\n",
      "Training loss for batch 5662 : 0.480813592672348\n",
      "Training loss for batch 5663 : 0.5060316324234009\n",
      "Training loss for batch 5664 : 0.24096225202083588\n",
      "Training loss for batch 5665 : 0.025135885924100876\n",
      "Training loss for batch 5666 : 0.09460774064064026\n",
      "Training loss for batch 5667 : 0.2754887342453003\n",
      "Training loss for batch 5668 : 0.3143966495990753\n",
      "Training loss for batch 5669 : 0.2917029857635498\n",
      "Training loss for batch 5670 : 0.17174041271209717\n",
      "Training loss for batch 5671 : 0.37590375542640686\n",
      "Training loss for batch 5672 : 0.31117093563079834\n",
      "Training loss for batch 5673 : 0.4858880043029785\n",
      "Training loss for batch 5674 : 0.3741604685783386\n",
      "Training loss for batch 5675 : 0.21393784880638123\n",
      "Training loss for batch 5676 : 0.2441043257713318\n",
      "Training loss for batch 5677 : 0.008608123287558556\n",
      "Training loss for batch 5678 : 0.002102680504322052\n",
      "Training loss for batch 5679 : 0.333948016166687\n",
      "Training loss for batch 5680 : 0.2406076192855835\n",
      "Training loss for batch 5681 : 0.03606991469860077\n",
      "Training loss for batch 5682 : 0.008067203685641289\n",
      "Training loss for batch 5683 : 0.4319620132446289\n",
      "Training loss for batch 5684 : 0.376027911901474\n",
      "Training loss for batch 5685 : 0.13231420516967773\n",
      "Training loss for batch 5686 : 0.4186364412307739\n",
      "Training loss for batch 5687 : 0.17922361195087433\n",
      "Training loss for batch 5688 : 0.10835270583629608\n",
      "Training loss for batch 5689 : 0.5419795513153076\n",
      "Training loss for batch 5690 : 0.07357276231050491\n",
      "Training loss for batch 5691 : 0.30843865871429443\n",
      "Training loss for batch 5692 : 0.12953710556030273\n",
      "Training loss for batch 5693 : 0.10033586621284485\n",
      "Training loss for batch 5694 : 0.013741055503487587\n",
      "Training loss for batch 5695 : 0.0676308423280716\n",
      "Training loss for batch 5696 : 0.28546687960624695\n",
      "Training loss for batch 5697 : 0.31971096992492676\n",
      "Training loss for batch 5698 : 0.31584432721138\n",
      "Training loss for batch 5699 : 0.18892762064933777\n",
      "Training loss for batch 5700 : 0.09783366322517395\n",
      "Training loss for batch 5701 : 0.079742930829525\n",
      "Training loss for batch 5702 : 0.4519387483596802\n",
      "Training loss for batch 5703 : 0.02036439999938011\n",
      "Training loss for batch 5704 : 0.25770503282546997\n",
      "Training loss for batch 5705 : 0.48139283061027527\n",
      "Training loss for batch 5706 : 0.33850011229515076\n",
      "Training loss for batch 5707 : 0.5406649112701416\n",
      "Training loss for batch 5708 : 0.3545551598072052\n",
      "Training loss for batch 5709 : 0.04610345885157585\n",
      "Training loss for batch 5710 : 0.1790507584810257\n",
      "Training loss for batch 5711 : 0.12570367753505707\n",
      "Training loss for batch 5712 : 0.003840334014967084\n",
      "Training loss for batch 5713 : 0.13009342551231384\n",
      "Training loss for batch 5714 : 0.23998773097991943\n",
      "Training loss for batch 5715 : 0.18787524104118347\n",
      "Training loss for batch 5716 : 0.154676154255867\n",
      "Training loss for batch 5717 : 0.3501416742801666\n",
      "Training loss for batch 5718 : 0.2053612321615219\n",
      "Training loss for batch 5719 : 0.05615083500742912\n",
      "Training loss for batch 5720 : 0.05605513975024223\n",
      "Training loss for batch 5721 : 1.0762079954147339\n",
      "Training loss for batch 5722 : 0.6403632164001465\n",
      "Training loss for batch 5723 : 0.22649049758911133\n",
      "Training loss for batch 5724 : 0.07545660436153412\n",
      "Training loss for batch 5725 : 0.3477931320667267\n",
      "Training loss for batch 5726 : 0.2971976101398468\n",
      "Training loss for batch 5727 : 0.2246648669242859\n",
      "Training loss for batch 5728 : 0.18118663132190704\n",
      "Training loss for batch 5729 : 0.29839032888412476\n",
      "Training loss for batch 5730 : 0.26589471101760864\n",
      "Training loss for batch 5731 : 0.33276981115341187\n",
      "Training loss for batch 5732 : 0.16758005321025848\n",
      "Training loss for batch 5733 : 0.14504757523536682\n",
      "Training loss for batch 5734 : 0.4211626946926117\n",
      "Training loss for batch 5735 : 0.1968318521976471\n",
      "Training loss for batch 5736 : 0.09231805056333542\n",
      "Training loss for batch 5737 : 0.12186909466981888\n",
      "Training loss for batch 5738 : 0.26158586144447327\n",
      "Training loss for batch 5739 : 0.043163470923900604\n",
      "Training loss for batch 5740 : 0.13469569385051727\n",
      "Training loss for batch 5741 : 0.1574375331401825\n",
      "Training loss for batch 5742 : 0.1583978831768036\n",
      "Training loss for batch 5743 : 0.2679763436317444\n",
      "Training loss for batch 5744 : 0.29243242740631104\n",
      "Training loss for batch 5745 : 0.16038088500499725\n",
      "Training loss for batch 5746 : 0.4321399927139282\n",
      "Training loss for batch 5747 : 0.3726089596748352\n",
      "Training loss for batch 5748 : 0.32424426078796387\n",
      "Training loss for batch 5749 : 0.3517395257949829\n",
      "Training loss for batch 5750 : 0.15907835960388184\n",
      "Training loss for batch 5751 : 0.06580127775669098\n",
      "Training loss for batch 5752 : 0.0479658804833889\n",
      "Training loss for batch 5753 : 0.26586225628852844\n",
      "Training loss for batch 5754 : 0.15356361865997314\n",
      "Training loss for batch 5755 : 0.18633826076984406\n",
      "Training loss for batch 5756 : 0.5212666988372803\n",
      "Training loss for batch 5757 : 0.030835550278425217\n",
      "Training loss for batch 5758 : 0.27187100052833557\n",
      "Training loss for batch 5759 : 0.30003252625465393\n",
      "Training loss for batch 5760 : 0.156284898519516\n",
      "Training loss for batch 5761 : 0.0\n",
      "Training loss for batch 5762 : 0.1775178760290146\n",
      "Training loss for batch 5763 : 0.3944413661956787\n",
      "Training loss for batch 5764 : 0.22216318547725677\n",
      "Training loss for batch 5765 : 0.40286731719970703\n",
      "Training loss for batch 5766 : 0.05367164686322212\n",
      "Training loss for batch 5767 : 0.07976484298706055\n",
      "Training loss for batch 5768 : 0.05660384148359299\n",
      "Training loss for batch 5769 : 0.17111530900001526\n",
      "Training loss for batch 5770 : 0.18248556554317474\n",
      "Training loss for batch 5771 : 0.30078545212745667\n",
      "Training loss for batch 5772 : 0.41274961829185486\n",
      "Training loss for batch 5773 : 0.12748217582702637\n",
      "Training loss for batch 5774 : 0.0981270894408226\n",
      "Training loss for batch 5775 : 0.028054436668753624\n",
      "Training loss for batch 5776 : 0.1288861632347107\n",
      "Training loss for batch 5777 : 0.2886482775211334\n",
      "Training loss for batch 5778 : 0.17823997139930725\n",
      "Training loss for batch 5779 : 0.02590072713792324\n",
      "Training loss for batch 5780 : 0.093551404774189\n",
      "Training loss for batch 5781 : 0.194424107670784\n",
      "Training loss for batch 5782 : 0.34743544459342957\n",
      "Training loss for batch 5783 : 0.15438175201416016\n",
      "Training loss for batch 5784 : 0.011808552779257298\n",
      "Training loss for batch 5785 : 0.06021833419799805\n",
      "Training loss for batch 5786 : 0.08544014394283295\n",
      "Training loss for batch 5787 : 0.1258537769317627\n",
      "Training loss for batch 5788 : 0.15985442698001862\n",
      "Training loss for batch 5789 : 0.34883999824523926\n",
      "Training loss for batch 5790 : 0.03494769707322121\n",
      "Training loss for batch 5791 : 0.33118852972984314\n",
      "Training loss for batch 5792 : 0.15411685407161713\n",
      "Training loss for batch 5793 : 0.3794020116329193\n",
      "Training loss for batch 5794 : 0.30167096853256226\n",
      "Training loss for batch 5795 : 0.04718056321144104\n",
      "Training loss for batch 5796 : 0.06727753579616547\n",
      "Training loss for batch 5797 : 0.1751411259174347\n",
      "Training loss for batch 5798 : 0.25776949524879456\n",
      "Training loss for batch 5799 : 0.08005675673484802\n",
      "Training loss for batch 5800 : 0.3590407967567444\n",
      "Training loss for batch 5801 : 0.820538341999054\n",
      "Training loss for batch 5802 : 0.11046626418828964\n",
      "Training loss for batch 5803 : 0.30925673246383667\n",
      "Training loss for batch 5804 : 0.1398535966873169\n",
      "Training loss for batch 5805 : 0.07664669305086136\n",
      "Training loss for batch 5806 : 0.04451781511306763\n",
      "Training loss for batch 5807 : 0.0\n",
      "Training loss for batch 5808 : 0.3089900314807892\n",
      "Training loss for batch 5809 : 0.4168957769870758\n",
      "Training loss for batch 5810 : 0.021759619936347008\n",
      "Training loss for batch 5811 : 0.06942954659461975\n",
      "Training loss for batch 5812 : 0.138924241065979\n",
      "Training loss for batch 5813 : 0.19420653581619263\n",
      "Training loss for batch 5814 : 0.08839826285839081\n",
      "Training loss for batch 5815 : 0.061419527977705\n",
      "Training loss for batch 5816 : 0.2469378113746643\n",
      "Training loss for batch 5817 : 0.09621138125658035\n",
      "Training loss for batch 5818 : 0.44804883003234863\n",
      "Training loss for batch 5819 : 0.2063262015581131\n",
      "Training loss for batch 5820 : 0.16433550417423248\n",
      "Training loss for batch 5821 : 0.36742281913757324\n",
      "Training loss for batch 5822 : 0.019531715661287308\n",
      "Training loss for batch 5823 : 0.6543301343917847\n",
      "Training loss for batch 5824 : 0.16754662990570068\n",
      "Training loss for batch 5825 : 0.16861478984355927\n",
      "Training loss for batch 5826 : 0.10055071115493774\n",
      "Training loss for batch 5827 : 0.17550526559352875\n",
      "Training loss for batch 5828 : 0.03604170307517052\n",
      "Training loss for batch 5829 : 0.003926098346710205\n",
      "Training loss for batch 5830 : 0.007682684808969498\n",
      "Training loss for batch 5831 : 0.21386033296585083\n",
      "Training loss for batch 5832 : 0.028141528367996216\n",
      "Training loss for batch 5833 : 0.3788106441497803\n",
      "Training loss for batch 5834 : 0.36637216806411743\n",
      "Training loss for batch 5835 : 0.5902987718582153\n",
      "Training loss for batch 5836 : 0.5664341449737549\n",
      "Training loss for batch 5837 : 0.3512130081653595\n",
      "Training loss for batch 5838 : 0.2663138806819916\n",
      "Training loss for batch 5839 : 0.09220372885465622\n",
      "Training loss for batch 5840 : 0.6070041656494141\n",
      "Training loss for batch 5841 : 0.12119502574205399\n",
      "Training loss for batch 5842 : 0.05533814802765846\n",
      "Training loss for batch 5843 : 0.02929571270942688\n",
      "Training loss for batch 5844 : 0.0010571579914540052\n",
      "Training loss for batch 5845 : 0.2967252731323242\n",
      "Training loss for batch 5846 : 0.26889848709106445\n",
      "Training loss for batch 5847 : 0.03265261650085449\n",
      "Training loss for batch 5848 : 0.0\n",
      "Training loss for batch 5849 : 0.589320957660675\n",
      "Training loss for batch 5850 : 0.24314020574092865\n",
      "Training loss for batch 5851 : 0.3103804290294647\n",
      "Training loss for batch 5852 : 0.10040438175201416\n",
      "Training loss for batch 5853 : 0.2782813310623169\n",
      "Training loss for batch 5854 : 0.2504393458366394\n",
      "Training loss for batch 5855 : 0.10258074849843979\n",
      "Training loss for batch 5856 : 0.03798741474747658\n",
      "Training loss for batch 5857 : 0.44919759035110474\n",
      "Training loss for batch 5858 : 0.24543438851833344\n",
      "Training loss for batch 5859 : 0.3469846546649933\n",
      "Training loss for batch 5860 : 0.2827177345752716\n",
      "Training loss for batch 5861 : 0.17171433568000793\n",
      "Training loss for batch 5862 : 0.324089378118515\n",
      "Training loss for batch 5863 : 0.43445250391960144\n",
      "Training loss for batch 5864 : 0.3378954231739044\n",
      "Training loss for batch 5865 : 0.0\n",
      "Training loss for batch 5866 : 0.13284096121788025\n",
      "Training loss for batch 5867 : 0.20590820908546448\n",
      "Training loss for batch 5868 : 0.16629496216773987\n",
      "Training loss for batch 5869 : 0.0\n",
      "Training loss for batch 5870 : 0.16608984768390656\n",
      "Training loss for batch 5871 : 0.2672601640224457\n",
      "Training loss for batch 5872 : 0.015351650305092335\n",
      "Training loss for batch 5873 : 0.07499931752681732\n",
      "Training loss for batch 5874 : 0.31351688504219055\n",
      "Training loss for batch 5875 : 0.14290589094161987\n",
      "Training loss for batch 5876 : 0.19475404918193817\n",
      "Training loss for batch 5877 : 0.2569954991340637\n",
      "Training loss for batch 5878 : 0.5188250541687012\n",
      "Training loss for batch 5879 : 0.3738493025302887\n",
      "Training loss for batch 5880 : 0.5539290904998779\n",
      "Training loss for batch 5881 : 0.042546357959508896\n",
      "Training loss for batch 5882 : 0.3220430612564087\n",
      "Training loss for batch 5883 : 0.5088318586349487\n",
      "Training loss for batch 5884 : 0.14195610582828522\n",
      "Training loss for batch 5885 : 0.4274154305458069\n",
      "Training loss for batch 5886 : 0.09614849835634232\n",
      "Training loss for batch 5887 : 0.12183325737714767\n",
      "Training loss for batch 5888 : 0.4495725631713867\n",
      "Training loss for batch 5889 : 0.21401281654834747\n",
      "Training loss for batch 5890 : 0.1291879415512085\n",
      "Training loss for batch 5891 : 0.14284777641296387\n",
      "Training loss for batch 5892 : 0.4989745020866394\n",
      "Training loss for batch 5893 : 0.17112910747528076\n",
      "Training loss for batch 5894 : 0.04415014386177063\n",
      "Training loss for batch 5895 : 0.08086945861577988\n",
      "Training loss for batch 5896 : 0.8385215997695923\n",
      "Training loss for batch 5897 : 0.04801267385482788\n",
      "Training loss for batch 5898 : 0.3743883967399597\n",
      "Training loss for batch 5899 : 0.21183066070079803\n",
      "Training loss for batch 5900 : 0.1598881036043167\n",
      "Training loss for batch 5901 : 0.3927893340587616\n",
      "Training loss for batch 5902 : 0.07682482898235321\n",
      "Training loss for batch 5903 : 0.3886477053165436\n",
      "Training loss for batch 5904 : 0.2693941295146942\n",
      "Training loss for batch 5905 : 0.23754703998565674\n",
      "Training loss for batch 5906 : 0.130662202835083\n",
      "Training loss for batch 5907 : 0.11010389029979706\n",
      "Training loss for batch 5908 : 0.07110162079334259\n",
      "Training loss for batch 5909 : 0.13066285848617554\n",
      "Training loss for batch 5910 : 0.01309953723102808\n",
      "Training loss for batch 5911 : 0.2617809772491455\n",
      "Training loss for batch 5912 : 0.2562360465526581\n",
      "Training loss for batch 5913 : 0.34464138746261597\n",
      "Training loss for batch 5914 : 0.47351348400115967\n",
      "Training loss for batch 5915 : 0.20106473565101624\n",
      "Training loss for batch 5916 : 1.0320950746536255\n",
      "Training loss for batch 5917 : 0.029594771564006805\n",
      "Training loss for batch 5918 : 0.001010916312225163\n",
      "Training loss for batch 5919 : 0.045438002794981\n",
      "Training loss for batch 5920 : 0.2704092860221863\n",
      "Training loss for batch 5921 : 0.21283839643001556\n",
      "Training loss for batch 5922 : 0.05567759647965431\n",
      "Training loss for batch 5923 : 0.09121531248092651\n",
      "Training loss for batch 5924 : 0.07356230169534683\n",
      "Training loss for batch 5925 : 0.11780387163162231\n",
      "Training loss for batch 5926 : 0.03272233158349991\n",
      "Training loss for batch 5927 : 0.02210615761578083\n",
      "Training loss for batch 5928 : 0.04132486507296562\n",
      "Training loss for batch 5929 : 0.06242341920733452\n",
      "Training loss for batch 5930 : 0.2294638752937317\n",
      "Training loss for batch 5931 : 0.46016231179237366\n",
      "Training loss for batch 5932 : 0.13905099034309387\n",
      "Training loss for batch 5933 : 0.2651173174381256\n",
      "Training loss for batch 5934 : 0.10661675781011581\n",
      "Training loss for batch 5935 : 0.2400641292333603\n",
      "Training loss for batch 5936 : 0.19861233234405518\n",
      "Training loss for batch 5937 : 0.1761360466480255\n",
      "Training loss for batch 5938 : 0.5051676630973816\n",
      "Training loss for batch 5939 : 0.13983756303787231\n",
      "Training loss for batch 5940 : 0.16716480255126953\n",
      "Training loss for batch 5941 : 0.3297331631183624\n",
      "Training loss for batch 5942 : 0.06234630197286606\n",
      "Training loss for batch 5943 : 0.2968018651008606\n",
      "Training loss for batch 5944 : 0.23937785625457764\n",
      "Training loss for batch 5945 : 0.5220233201980591\n",
      "Training loss for batch 5946 : 0.02847873978316784\n",
      "Training loss for batch 5947 : 0.6792346239089966\n",
      "Training loss for batch 5948 : 0.2323935627937317\n",
      "Training loss for batch 5949 : 0.4048103094100952\n",
      "Training loss for batch 5950 : 0.21863843500614166\n",
      "Training loss for batch 5951 : 0.14923788607120514\n",
      "Training loss for batch 5952 : 0.14952833950519562\n",
      "Training loss for batch 5953 : 0.2601275146007538\n",
      "Training loss for batch 5954 : 0.14630216360092163\n",
      "Training loss for batch 5955 : 0.05992569029331207\n",
      "Training loss for batch 5956 : 0.22051778435707092\n",
      "Training loss for batch 5957 : 0.04697984457015991\n",
      "Training loss for batch 5958 : 0.4972890615463257\n",
      "Training loss for batch 5959 : 0.09093666076660156\n",
      "Training loss for batch 5960 : 0.10678224265575409\n",
      "Training loss for batch 5961 : 0.3272040784358978\n",
      "Training loss for batch 5962 : 0.313599556684494\n",
      "Training loss for batch 5963 : 0.39203280210494995\n",
      "Training loss for batch 5964 : 0.15047003328800201\n",
      "Training loss for batch 5965 : 0.16439200937747955\n",
      "Training loss for batch 5966 : 0.050782885402441025\n",
      "Training loss for batch 5967 : 0.1308889389038086\n",
      "Training loss for batch 5968 : 0.070881687104702\n",
      "Training loss for batch 5969 : 0.12964166700839996\n",
      "Training loss for batch 5970 : 0.1575717180967331\n",
      "Training loss for batch 5971 : 0.5702688097953796\n",
      "Training loss for batch 5972 : 0.35032984614372253\n",
      "Training loss for batch 5973 : 0.09010952711105347\n",
      "Training loss for batch 5974 : 0.053062327206134796\n",
      "Training loss for batch 5975 : 0.06900863349437714\n",
      "Training loss for batch 5976 : 0.40417608618736267\n",
      "Training loss for batch 5977 : 0.0032984218560159206\n",
      "Training loss for batch 5978 : 0.027692021802067757\n",
      "Training loss for batch 5979 : 0.32294660806655884\n",
      "Training loss for batch 5980 : 0.10234291106462479\n",
      "Training loss for batch 5981 : 0.09888987243175507\n",
      "Training loss for batch 5982 : 0.2988239824771881\n",
      "Training loss for batch 5983 : 0.42953547835350037\n",
      "Training loss for batch 5984 : 0.25933539867401123\n",
      "Training loss for batch 5985 : 0.21603690087795258\n",
      "Training loss for batch 5986 : 0.0\n",
      "Training loss for batch 5987 : 0.1659189909696579\n",
      "Training loss for batch 5988 : 0.015341553837060928\n",
      "Training loss for batch 5989 : 0.332538902759552\n",
      "Training loss for batch 5990 : 0.21154558658599854\n",
      "Training loss for batch 5991 : 0.01775187999010086\n",
      "Training loss for batch 5992 : 0.1329529881477356\n",
      "Training loss for batch 5993 : 0.0010597745422273874\n",
      "Training loss for batch 5994 : 0.157490536570549\n",
      "Training loss for batch 5995 : 0.04015110433101654\n",
      "Training loss for batch 5996 : 0.025362737476825714\n",
      "Training loss for batch 5997 : 0.09446100145578384\n",
      "Training loss for batch 5998 : 0.03470967337489128\n",
      "Training loss for batch 5999 : 0.5018770694732666\n",
      "Training loss for batch 6000 : 0.23399695754051208\n",
      "Training loss for batch 6001 : 0.2716037631034851\n",
      "Training loss for batch 6002 : 0.04143880680203438\n",
      "Training loss for batch 6003 : 0.23883362114429474\n",
      "Training loss for batch 6004 : 0.05967927724123001\n",
      "Training loss for batch 6005 : 0.07080845534801483\n",
      "Training loss for batch 6006 : 0.12617430090904236\n",
      "Training loss for batch 6007 : 0.27259254455566406\n",
      "Training loss for batch 6008 : 0.02130822092294693\n",
      "Training loss for batch 6009 : 0.323679655790329\n",
      "Training loss for batch 6010 : 0.3846210837364197\n",
      "Training loss for batch 6011 : 0.5098003149032593\n",
      "Training loss for batch 6012 : 0.28558266162872314\n",
      "Training loss for batch 6013 : 0.05742467939853668\n",
      "Training loss for batch 6014 : 0.19229574501514435\n",
      "Training loss for batch 6015 : 0.0466947928071022\n",
      "Training loss for batch 6016 : 0.29587310552597046\n",
      "Training loss for batch 6017 : 0.010559892281889915\n",
      "Training loss for batch 6018 : 0.7017356157302856\n",
      "Training loss for batch 6019 : 0.1394544541835785\n",
      "Training loss for batch 6020 : 0.5313071012496948\n",
      "Training loss for batch 6021 : 0.10802724957466125\n",
      "Training loss for batch 6022 : 0.1987287700176239\n",
      "Training loss for batch 6023 : 0.28502947092056274\n",
      "Training loss for batch 6024 : 0.02933383733034134\n",
      "Training loss for batch 6025 : 0.08557968586683273\n",
      "Training loss for batch 6026 : 0.27114537358283997\n",
      "Training loss for batch 6027 : 0.43338826298713684\n",
      "Training loss for batch 6028 : 0.0975472703576088\n",
      "Training loss for batch 6029 : 0.011241596192121506\n",
      "Training loss for batch 6030 : 0.10698744654655457\n",
      "Training loss for batch 6031 : 0.18607409298419952\n",
      "Training loss for batch 6032 : 0.3101307451725006\n",
      "Training loss for batch 6033 : 0.12249057739973068\n",
      "Training loss for batch 6034 : 0.2340775430202484\n",
      "Training loss for batch 6035 : 0.1462615579366684\n",
      "Training loss for batch 6036 : 0.035203494131565094\n",
      "Training loss for batch 6037 : 0.07000784575939178\n",
      "Training loss for batch 6038 : 0.8010298013687134\n",
      "Training loss for batch 6039 : 0.012605875730514526\n",
      "Training loss for batch 6040 : 0.15453605353832245\n",
      "Training loss for batch 6041 : 0.22950200736522675\n",
      "Training loss for batch 6042 : 0.17254696786403656\n",
      "Training loss for batch 6043 : 0.22245316207408905\n",
      "Training loss for batch 6044 : 0.12547555565834045\n",
      "Training loss for batch 6045 : 0.0\n",
      "Training loss for batch 6046 : 0.16946354508399963\n",
      "Training loss for batch 6047 : 0.31454214453697205\n",
      "Training loss for batch 6048 : 0.319781631231308\n",
      "Training loss for batch 6049 : 0.11739060282707214\n",
      "Training loss for batch 6050 : 0.28974610567092896\n",
      "Training loss for batch 6051 : 0.1983877420425415\n",
      "Training loss for batch 6052 : 0.160292387008667\n",
      "Training loss for batch 6053 : 0.047428347170352936\n",
      "Training loss for batch 6054 : 0.3377836048603058\n",
      "Training loss for batch 6055 : 0.17341186106204987\n",
      "Training loss for batch 6056 : 0.536085844039917\n",
      "Training loss for batch 6057 : 0.4198080003261566\n",
      "Training loss for batch 6058 : 0.2885923981666565\n",
      "Training loss for batch 6059 : 0.33637210726737976\n",
      "Training loss for batch 6060 : 0.13045640289783478\n",
      "Training loss for batch 6061 : 0.0546228364109993\n",
      "Training loss for batch 6062 : 0.18857398629188538\n",
      "Training loss for batch 6063 : 0.8184999227523804\n",
      "Training loss for batch 6064 : 0.27322858572006226\n",
      "Training loss for batch 6065 : 0.35058584809303284\n",
      "Training loss for batch 6066 : 0.10143830627202988\n",
      "Training loss for batch 6067 : 0.06967401504516602\n",
      "Training loss for batch 6068 : 0.19156305491924286\n",
      "Training loss for batch 6069 : 0.11185530573129654\n",
      "Training loss for batch 6070 : 0.1863371580839157\n",
      "Training loss for batch 6071 : 0.2888597249984741\n",
      "Training loss for batch 6072 : 0.2205783575773239\n",
      "Training loss for batch 6073 : 0.06444817781448364\n",
      "Training loss for batch 6074 : 0.05472147464752197\n",
      "Training loss for batch 6075 : 0.138161763548851\n",
      "Training loss for batch 6076 : 0.48808753490448\n",
      "Training loss for batch 6077 : 0.40111541748046875\n",
      "Training loss for batch 6078 : 0.24852798879146576\n",
      "Training loss for batch 6079 : 0.22763971984386444\n",
      "Training loss for batch 6080 : 0.04628663882613182\n",
      "Training loss for batch 6081 : 0.3224552869796753\n",
      "Training loss for batch 6082 : 0.09191839396953583\n",
      "Training loss for batch 6083 : 0.12531085312366486\n",
      "Training loss for batch 6084 : 0.07893159985542297\n",
      "Training loss for batch 6085 : 0.37147730588912964\n",
      "Training loss for batch 6086 : 0.14708352088928223\n",
      "Training loss for batch 6087 : 0.35355499386787415\n",
      "Training loss for batch 6088 : 0.04784437641501427\n",
      "Training loss for batch 6089 : 0.08402860164642334\n",
      "Training loss for batch 6090 : 0.19405242800712585\n",
      "Training loss for batch 6091 : 0.24697652459144592\n",
      "Training loss for batch 6092 : 0.23471592366695404\n",
      "Training loss for batch 6093 : 0.02043810859322548\n",
      "Training loss for batch 6094 : 0.10886506736278534\n",
      "Training loss for batch 6095 : 0.2617393732070923\n",
      "Training loss for batch 6096 : 0.004609601106494665\n",
      "Training loss for batch 6097 : 0.30404239892959595\n",
      "Training loss for batch 6098 : 0.22154554724693298\n",
      "Training loss for batch 6099 : 0.13394159078598022\n",
      "Training loss for batch 6100 : 0.14457005262374878\n",
      "Training loss for batch 6101 : 0.21971626579761505\n",
      "Training loss for batch 6102 : 0.18475231528282166\n",
      "Training loss for batch 6103 : 0.10564403980970383\n",
      "Training loss for batch 6104 : 0.32295891642570496\n",
      "Training loss for batch 6105 : 0.2670465111732483\n",
      "Training loss for batch 6106 : 0.060167133808135986\n",
      "Training loss for batch 6107 : 0.4193514883518219\n",
      "Training loss for batch 6108 : 0.18202190101146698\n",
      "Training loss for batch 6109 : 0.3442010283470154\n",
      "Training loss for batch 6110 : 0.26452213525772095\n",
      "Training loss for batch 6111 : 0.29872435331344604\n",
      "Training loss for batch 6112 : 0.017104148864746094\n",
      "Training loss for batch 6113 : 0.08148104697465897\n",
      "Training loss for batch 6114 : 0.002712756162509322\n",
      "Training loss for batch 6115 : 0.3387030363082886\n",
      "Training loss for batch 6116 : 0.08425173163414001\n",
      "Training loss for batch 6117 : 0.03292258828878403\n",
      "Training loss for batch 6118 : 0.13026916980743408\n",
      "Training loss for batch 6119 : 0.1935875117778778\n",
      "Training loss for batch 6120 : 0.19156010448932648\n",
      "Training loss for batch 6121 : 0.12529630959033966\n",
      "Training loss for batch 6122 : 0.14502939581871033\n",
      "Training loss for batch 6123 : 0.22224418818950653\n",
      "Training loss for batch 6124 : 0.09788083285093307\n",
      "Training loss for batch 6125 : 0.5108060240745544\n",
      "Training loss for batch 6126 : 0.09572341293096542\n",
      "Training loss for batch 6127 : 1.035677433013916\n",
      "Training loss for batch 6128 : 0.17911837995052338\n",
      "Training loss for batch 6129 : 0.2215164303779602\n",
      "Training loss for batch 6130 : 0.16133511066436768\n",
      "Training loss for batch 6131 : 0.0\n",
      "Training loss for batch 6132 : 0.5709094405174255\n",
      "Training loss for batch 6133 : 0.09942091256380081\n",
      "Training loss for batch 6134 : 0.07004453241825104\n",
      "Training loss for batch 6135 : 0.5497162938117981\n",
      "Training loss for batch 6136 : 0.08217261731624603\n",
      "Training loss for batch 6137 : 0.36306700110435486\n",
      "Training loss for batch 6138 : 0.5903525948524475\n",
      "Training loss for batch 6139 : 0.19194746017456055\n",
      "Training loss for batch 6140 : 0.3796703517436981\n",
      "Training loss for batch 6141 : 0.22015221416950226\n",
      "Training loss for batch 6142 : 0.06767770648002625\n",
      "Training loss for batch 6143 : 0.2107098251581192\n",
      "Training loss for batch 6144 : 0.1880616843700409\n",
      "Training loss for batch 6145 : 0.0010917683830484748\n",
      "Training loss for batch 6146 : 0.2089136391878128\n",
      "Training loss for batch 6147 : 0.14675261080265045\n",
      "Training loss for batch 6148 : 0.10727575421333313\n",
      "Training loss for batch 6149 : 0.2623055577278137\n",
      "Training loss for batch 6150 : 0.6055378317832947\n",
      "Training loss for batch 6151 : 0.49715861678123474\n",
      "Training loss for batch 6152 : 0.029550204053521156\n",
      "Training loss for batch 6153 : 0.08458039909601212\n",
      "Training loss for batch 6154 : 0.05459943041205406\n",
      "Training loss for batch 6155 : 0.1397937685251236\n",
      "Training loss for batch 6156 : 0.0819658488035202\n",
      "Training loss for batch 6157 : 0.06072969734668732\n",
      "Training loss for batch 6158 : 0.18429119884967804\n",
      "Training loss for batch 6159 : 0.26628559827804565\n",
      "Training loss for batch 6160 : 0.2539667785167694\n",
      "Training loss for batch 6161 : 0.007904390804469585\n",
      "Training loss for batch 6162 : 0.11058436334133148\n",
      "Training loss for batch 6163 : 0.35422369837760925\n",
      "Training loss for batch 6164 : 0.21109256148338318\n",
      "Training loss for batch 6165 : 0.09184596687555313\n",
      "Training loss for batch 6166 : 0.04049904644489288\n",
      "Training loss for batch 6167 : 0.7033442854881287\n",
      "Training loss for batch 6168 : 0.04682907462120056\n",
      "Training loss for batch 6169 : 0.13975410163402557\n",
      "Training loss for batch 6170 : 0.02172224223613739\n",
      "Training loss for batch 6171 : 0.21213415265083313\n",
      "Training loss for batch 6172 : 0.2660667598247528\n",
      "Training loss for batch 6173 : 0.1650359183549881\n",
      "Training loss for batch 6174 : 0.08335695415735245\n",
      "Training loss for batch 6175 : 0.0\n",
      "Training loss for batch 6176 : 0.21314483880996704\n",
      "Training loss for batch 6177 : 0.40759769082069397\n",
      "Training loss for batch 6178 : 0.28200405836105347\n",
      "Training loss for batch 6179 : 0.35594701766967773\n",
      "Training loss for batch 6180 : 0.2238539606332779\n",
      "Training loss for batch 6181 : 0.16860546171665192\n",
      "Training loss for batch 6182 : 0.09727714210748672\n",
      "Training loss for batch 6183 : 0.27967745065689087\n",
      "Training loss for batch 6184 : 0.28902384638786316\n",
      "Training loss for batch 6185 : 0.06606197357177734\n",
      "Training loss for batch 6186 : 0.04920681565999985\n",
      "Training loss for batch 6187 : 0.11223511397838593\n",
      "Training loss for batch 6188 : 0.4171825647354126\n",
      "Training loss for batch 6189 : 0.14298807084560394\n",
      "Training loss for batch 6190 : 0.05564919114112854\n",
      "Training loss for batch 6191 : 0.23289531469345093\n",
      "Training loss for batch 6192 : 0.11735693365335464\n",
      "Training loss for batch 6193 : 0.029550140723586082\n",
      "Training loss for batch 6194 : 0.045084863901138306\n",
      "Training loss for batch 6195 : 0.31539997458457947\n",
      "Training loss for batch 6196 : 0.34322673082351685\n",
      "Training loss for batch 6197 : 0.042554136365652084\n",
      "Training loss for batch 6198 : 0.5205532908439636\n",
      "Training loss for batch 6199 : 0.06872548162937164\n",
      "Training loss for batch 6200 : 0.3115895986557007\n",
      "Training loss for batch 6201 : 0.10914753377437592\n",
      "Training loss for batch 6202 : 0.05235488340258598\n",
      "Training loss for batch 6203 : 0.31558236479759216\n",
      "Training loss for batch 6204 : 0.30914220213890076\n",
      "Training loss for batch 6205 : 0.11950339376926422\n",
      "Training loss for batch 6206 : 0.14719437062740326\n",
      "Training loss for batch 6207 : 0.18078681826591492\n",
      "Training loss for batch 6208 : 0.12150146067142487\n",
      "Training loss for batch 6209 : 0.24257998168468475\n",
      "Training loss for batch 6210 : 0.2389274686574936\n",
      "Training loss for batch 6211 : 0.2903173863887787\n",
      "Training loss for batch 6212 : 0.5383890271186829\n",
      "Training loss for batch 6213 : 0.2866833806037903\n",
      "Training loss for batch 6214 : 0.1683986485004425\n",
      "Training loss for batch 6215 : 0.07123330235481262\n",
      "Training loss for batch 6216 : 0.43349146842956543\n",
      "Training loss for batch 6217 : 0.2554820477962494\n",
      "Training loss for batch 6218 : 0.01862306520342827\n",
      "Training loss for batch 6219 : 0.3555488884449005\n",
      "Training loss for batch 6220 : 0.35659337043762207\n",
      "Training loss for batch 6221 : 0.0782093033194542\n",
      "Training loss for batch 6222 : 0.4079418480396271\n",
      "Training loss for batch 6223 : 0.130086287856102\n",
      "Training loss for batch 6224 : 0.2338017374277115\n",
      "Training loss for batch 6225 : 0.15339088439941406\n",
      "Training loss for batch 6226 : 0.020870717242360115\n",
      "Training loss for batch 6227 : 0.05406155437231064\n",
      "Training loss for batch 6228 : 0.22198638319969177\n",
      "Training loss for batch 6229 : 0.18954049050807953\n",
      "Training loss for batch 6230 : 0.06538732349872589\n",
      "Training loss for batch 6231 : 0.11046697944402695\n",
      "Training loss for batch 6232 : 0.284709095954895\n",
      "Training loss for batch 6233 : 0.1458164006471634\n",
      "Training loss for batch 6234 : 0.05514953285455704\n",
      "Training loss for batch 6235 : 0.1951485574245453\n",
      "Training loss for batch 6236 : 0.11102258414030075\n",
      "Training loss for batch 6237 : 0.19734598696231842\n",
      "Training loss for batch 6238 : 0.050444334745407104\n",
      "Training loss for batch 6239 : 0.21251052618026733\n",
      "Training loss for batch 6240 : 0.23491694033145905\n",
      "Training loss for batch 6241 : 0.1368311196565628\n",
      "Training loss for batch 6242 : 0.1394214630126953\n",
      "Training loss for batch 6243 : 0.10544133931398392\n",
      "Training loss for batch 6244 : 0.2442895472049713\n",
      "Training loss for batch 6245 : 0.06789832562208176\n",
      "Training loss for batch 6246 : 0.058835145086050034\n",
      "Training loss for batch 6247 : 0.25428473949432373\n",
      "Training loss for batch 6248 : 0.10061986744403839\n",
      "Training loss for batch 6249 : 0.10721933096647263\n",
      "Training loss for batch 6250 : 0.17920958995819092\n",
      "Training loss for batch 6251 : 0.7066619396209717\n",
      "Training loss for batch 6252 : 0.041287463158369064\n",
      "Training loss for batch 6253 : 0.07006542384624481\n",
      "Training loss for batch 6254 : 0.07169915735721588\n",
      "Training loss for batch 6255 : 0.18061469495296478\n",
      "Training loss for batch 6256 : 0.05920202285051346\n",
      "Training loss for batch 6257 : 0.3253306448459625\n",
      "Training loss for batch 6258 : 0.13697974383831024\n",
      "Training loss for batch 6259 : 0.3972148001194\n",
      "Training loss for batch 6260 : 0.3777174651622772\n",
      "Training loss for batch 6261 : 0.17281942069530487\n",
      "Training loss for batch 6262 : 0.011340235359966755\n",
      "Training loss for batch 6263 : 0.1914481222629547\n",
      "Training loss for batch 6264 : 0.09772197157144547\n",
      "Training loss for batch 6265 : 0.5172972083091736\n",
      "Training loss for batch 6266 : 0.3204173743724823\n",
      "Training loss for batch 6267 : 0.2155495584011078\n",
      "Training loss for batch 6268 : 0.10141202062368393\n",
      "Training loss for batch 6269 : 0.14339610934257507\n",
      "Training loss for batch 6270 : 0.7571402788162231\n",
      "Training loss for batch 6271 : 0.016858257353305817\n",
      "Training loss for batch 6272 : 0.1546086072921753\n",
      "Training loss for batch 6273 : 0.0016891040140762925\n",
      "Training loss for batch 6274 : 0.38523203134536743\n",
      "Training loss for batch 6275 : 0.2610221803188324\n",
      "Training loss for batch 6276 : 0.10347005724906921\n",
      "Training loss for batch 6277 : 0.12598247826099396\n",
      "Training loss for batch 6278 : 0.4635204076766968\n",
      "Training loss for batch 6279 : 0.06847494840621948\n",
      "Training loss for batch 6280 : 0.18150469660758972\n",
      "Training loss for batch 6281 : 0.24055978655815125\n",
      "Training loss for batch 6282 : 0.3579491376876831\n",
      "Training loss for batch 6283 : 0.18605859577655792\n",
      "Training loss for batch 6284 : 0.15371118485927582\n",
      "Training loss for batch 6285 : 0.01147698238492012\n",
      "Training loss for batch 6286 : 0.4243023991584778\n",
      "Training loss for batch 6287 : 0.07377133518457413\n",
      "Training loss for batch 6288 : 0.3340958058834076\n",
      "Training loss for batch 6289 : 0.008334199897944927\n",
      "Training loss for batch 6290 : 0.15435495972633362\n",
      "Training loss for batch 6291 : 0.3246292173862457\n",
      "Training loss for batch 6292 : 0.03379281982779503\n",
      "Training loss for batch 6293 : 0.1979988068342209\n",
      "Training loss for batch 6294 : 0.2175597995519638\n",
      "Training loss for batch 6295 : 0.40148335695266724\n",
      "Training loss for batch 6296 : 0.2667112946510315\n",
      "Training loss for batch 6297 : 0.2175832837820053\n",
      "Training loss for batch 6298 : 0.3550945520401001\n",
      "Training loss for batch 6299 : 0.06078749895095825\n",
      "Training loss for batch 6300 : 0.0978078842163086\n",
      "Training loss for batch 6301 : 0.3379872739315033\n",
      "Training loss for batch 6302 : 0.24287042021751404\n",
      "Training loss for batch 6303 : 0.039811860769987106\n",
      "Training loss for batch 6304 : 0.5850540399551392\n",
      "Training loss for batch 6305 : 0.12780173122882843\n",
      "Training loss for batch 6306 : 0.39250653982162476\n",
      "Training loss for batch 6307 : 0.2885446548461914\n",
      "Training loss for batch 6308 : 0.02583331987261772\n",
      "Training loss for batch 6309 : 0.029834521934390068\n",
      "Training loss for batch 6310 : 0.25379782915115356\n",
      "Training loss for batch 6311 : 0.2877419888973236\n",
      "Training loss for batch 6312 : 0.17071005702018738\n",
      "Training loss for batch 6313 : 0.47501134872436523\n",
      "Training loss for batch 6314 : 0.25029247999191284\n",
      "Training loss for batch 6315 : 0.09153641760349274\n",
      "Training loss for batch 6316 : 0.24301902949810028\n",
      "Training loss for batch 6317 : 0.13488365709781647\n",
      "Training loss for batch 6318 : 0.10718263685703278\n",
      "Training loss for batch 6319 : 0.39710691571235657\n",
      "Training loss for batch 6320 : 0.29020440578460693\n",
      "Training loss for batch 6321 : 0.11880901455879211\n",
      "Training loss for batch 6322 : 0.1502850353717804\n",
      "Training loss for batch 6323 : 0.2567026615142822\n",
      "Training loss for batch 6324 : 0.47862744331359863\n",
      "Training loss for batch 6325 : 0.03437666967511177\n",
      "Training loss for batch 6326 : 0.36324769258499146\n",
      "Training loss for batch 6327 : 0.5046813488006592\n",
      "Training loss for batch 6328 : 0.006421667989343405\n",
      "Training loss for batch 6329 : 0.3363071382045746\n",
      "Training loss for batch 6330 : 0.12381471693515778\n",
      "Training loss for batch 6331 : 0.10722336918115616\n",
      "Training loss for batch 6332 : 0.021331390365958214\n",
      "Training loss for batch 6333 : 0.5473223328590393\n",
      "Training loss for batch 6334 : 0.15644915401935577\n",
      "Training loss for batch 6335 : 0.20151565968990326\n",
      "Training loss for batch 6336 : 0.3321627378463745\n",
      "Training loss for batch 6337 : 0.1891804337501526\n",
      "Training loss for batch 6338 : 0.034015025943517685\n",
      "Training loss for batch 6339 : 0.6538375020027161\n",
      "Training loss for batch 6340 : 0.0\n",
      "Training loss for batch 6341 : 0.4133303463459015\n",
      "Training loss for batch 6342 : 0.36610323190689087\n",
      "Training loss for batch 6343 : 0.29106074571609497\n",
      "Training loss for batch 6344 : 0.03250803053379059\n",
      "Training loss for batch 6345 : 0.1987893283367157\n",
      "Training loss for batch 6346 : 0.34575772285461426\n",
      "Training loss for batch 6347 : 0.12133734673261642\n",
      "Training loss for batch 6348 : 0.7754663228988647\n",
      "Training loss for batch 6349 : 0.103958860039711\n",
      "Training loss for batch 6350 : 0.25347352027893066\n",
      "Training loss for batch 6351 : 0.0610627606511116\n",
      "Training loss for batch 6352 : 0.02806386537849903\n",
      "Training loss for batch 6353 : 0.10197382420301437\n",
      "Training loss for batch 6354 : 0.34367498755455017\n",
      "Training loss for batch 6355 : 0.004260361194610596\n",
      "Training loss for batch 6356 : 0.23965895175933838\n",
      "Training loss for batch 6357 : 0.018360694870352745\n",
      "Training loss for batch 6358 : 0.4782923758029938\n",
      "Training loss for batch 6359 : 0.44769370555877686\n",
      "Training loss for batch 6360 : 0.045752815902233124\n",
      "Training loss for batch 6361 : 0.10650911182165146\n",
      "Training loss for batch 6362 : 0.44100308418273926\n",
      "Training loss for batch 6363 : 0.8408401012420654\n",
      "Training loss for batch 6364 : 0.08952786028385162\n",
      "Training loss for batch 6365 : 0.025685684755444527\n",
      "Training loss for batch 6366 : 0.01630661077797413\n",
      "Training loss for batch 6367 : 0.26954537630081177\n",
      "Training loss for batch 6368 : 0.6505299210548401\n",
      "Training loss for batch 6369 : 0.30393844842910767\n",
      "Training loss for batch 6370 : 0.01166160125285387\n",
      "Training loss for batch 6371 : 0.10934435576200485\n",
      "Training loss for batch 6372 : 0.03698282688856125\n",
      "Training loss for batch 6373 : 0.496529757976532\n",
      "Training loss for batch 6374 : 0.14499257504940033\n",
      "Training loss for batch 6375 : 0.38766998052597046\n",
      "Training loss for batch 6376 : 0.0567638985812664\n",
      "Training loss for batch 6377 : 0.6171196103096008\n",
      "Training loss for batch 6378 : 0.08124726265668869\n",
      "Training loss for batch 6379 : 0.3609018921852112\n",
      "Training loss for batch 6380 : 0.03427113592624664\n",
      "Training loss for batch 6381 : 0.1028096079826355\n",
      "Training loss for batch 6382 : 0.2320084124803543\n",
      "Training loss for batch 6383 : 0.30156388878822327\n",
      "Training loss for batch 6384 : 0.08519723266363144\n",
      "Training loss for batch 6385 : 0.06819307059049606\n",
      "Training loss for batch 6386 : 0.04458984360098839\n",
      "Training loss for batch 6387 : 0.40787410736083984\n",
      "Training loss for batch 6388 : 0.10695716738700867\n",
      "Training loss for batch 6389 : 0.09205514937639236\n",
      "Training loss for batch 6390 : 0.29146522283554077\n",
      "Training loss for batch 6391 : 0.04954969137907028\n",
      "Training loss for batch 6392 : 0.008958004415035248\n",
      "Training loss for batch 6393 : 0.08785209059715271\n",
      "Training loss for batch 6394 : 0.3704391121864319\n",
      "Training loss for batch 6395 : 0.16780534386634827\n",
      "Training loss for batch 6396 : 0.14102667570114136\n",
      "Training loss for batch 6397 : 0.20874935388565063\n",
      "Training loss for batch 6398 : 0.2310006469488144\n",
      "Training loss for batch 6399 : 0.37403592467308044\n",
      "Training loss for batch 6400 : 0.06851038336753845\n",
      "Training loss for batch 6401 : 0.29658013582229614\n",
      "Training loss for batch 6402 : 0.16765941679477692\n",
      "Training loss for batch 6403 : 0.03864985331892967\n",
      "Training loss for batch 6404 : 0.4514676034450531\n",
      "Training loss for batch 6405 : 0.17827250063419342\n",
      "Training loss for batch 6406 : 0.1527617871761322\n",
      "Training loss for batch 6407 : 0.1355101615190506\n",
      "Training loss for batch 6408 : 0.12205726653337479\n",
      "Training loss for batch 6409 : 0.03790261596441269\n",
      "Training loss for batch 6410 : 0.46073591709136963\n",
      "Training loss for batch 6411 : 0.03495212271809578\n",
      "Training loss for batch 6412 : 0.14896078407764435\n",
      "Training loss for batch 6413 : 0.25319430232048035\n",
      "Training loss for batch 6414 : 0.12769822776317596\n",
      "Training loss for batch 6415 : 0.28697869181632996\n",
      "Training loss for batch 6416 : 0.24132639169692993\n",
      "Training loss for batch 6417 : 0.07576704025268555\n",
      "Training loss for batch 6418 : 0.46475592255592346\n",
      "Training loss for batch 6419 : 0.031354062259197235\n",
      "Training loss for batch 6420 : 0.2671363353729248\n",
      "Training loss for batch 6421 : 0.20707231760025024\n",
      "Training loss for batch 6422 : 0.384870707988739\n",
      "Training loss for batch 6423 : 0.0520632378757\n",
      "Training loss for batch 6424 : 0.25363239645957947\n",
      "Training loss for batch 6425 : 0.364707887172699\n",
      "Training loss for batch 6426 : 0.20607410371303558\n",
      "Training loss for batch 6427 : 0.3449036180973053\n",
      "Training loss for batch 6428 : 0.03373399376869202\n",
      "Training loss for batch 6429 : 0.09303798526525497\n",
      "Training loss for batch 6430 : 0.30712127685546875\n",
      "Training loss for batch 6431 : 0.08148004114627838\n",
      "Training loss for batch 6432 : 0.09236996620893478\n",
      "Training loss for batch 6433 : 0.3358924388885498\n",
      "Training loss for batch 6434 : 0.37759828567504883\n",
      "Training loss for batch 6435 : 0.437755286693573\n",
      "Training loss for batch 6436 : 0.17868317663669586\n",
      "Training loss for batch 6437 : 0.22087997198104858\n",
      "Training loss for batch 6438 : 0.42110997438430786\n",
      "Training loss for batch 6439 : 0.051262471824884415\n",
      "Training loss for batch 6440 : 0.13096047937870026\n",
      "Training loss for batch 6441 : 0.06731592118740082\n",
      "Training loss for batch 6442 : 0.36279770731925964\n",
      "Training loss for batch 6443 : 0.07983268797397614\n",
      "Training loss for batch 6444 : 0.43851596117019653\n",
      "Training loss for batch 6445 : 0.4917878210544586\n",
      "Training loss for batch 6446 : 0.19583167135715485\n",
      "Training loss for batch 6447 : 0.47731029987335205\n",
      "Training loss for batch 6448 : 0.17083892226219177\n",
      "Training loss for batch 6449 : 0.2300422042608261\n",
      "Training loss for batch 6450 : 0.23596274852752686\n",
      "Training loss for batch 6451 : 0.3989202082157135\n",
      "Training loss for batch 6452 : 0.3733459413051605\n",
      "Training loss for batch 6453 : 0.26311802864074707\n",
      "Training loss for batch 6454 : 0.46848058700561523\n",
      "Training loss for batch 6455 : 0.3279173970222473\n",
      "Training loss for batch 6456 : 0.47709426283836365\n",
      "Training loss for batch 6457 : 0.33633825182914734\n",
      "Training loss for batch 6458 : 0.5324347615242004\n",
      "Training loss for batch 6459 : 0.3045455813407898\n",
      "Training loss for batch 6460 : 0.4664653539657593\n",
      "Training loss for batch 6461 : 0.09122881293296814\n",
      "Training loss for batch 6462 : 0.08806507289409637\n",
      "Training loss for batch 6463 : 0.5203630924224854\n",
      "Training loss for batch 6464 : 0.37477630376815796\n",
      "Training loss for batch 6465 : 0.22992409765720367\n",
      "Training loss for batch 6466 : 0.15389756858348846\n",
      "Training loss for batch 6467 : 0.5302662253379822\n",
      "Training loss for batch 6468 : 0.04116395115852356\n",
      "Training loss for batch 6469 : 0.4170948565006256\n",
      "Training loss for batch 6470 : 0.16763950884342194\n",
      "Training loss for batch 6471 : 0.012325456365942955\n",
      "Training loss for batch 6472 : 0.019733399152755737\n",
      "Training loss for batch 6473 : 0.06156988814473152\n",
      "Training loss for batch 6474 : 0.010406335815787315\n",
      "Training loss for batch 6475 : 0.2732509970664978\n",
      "Training loss for batch 6476 : 0.13742192089557648\n",
      "Training loss for batch 6477 : 0.1479763686656952\n",
      "Training loss for batch 6478 : 0.1044616848230362\n",
      "Training loss for batch 6479 : 0.039614830166101456\n",
      "Training loss for batch 6480 : 0.012712336145341396\n",
      "Training loss for batch 6481 : 0.3020985424518585\n",
      "Training loss for batch 6482 : 0.19731014966964722\n",
      "Training loss for batch 6483 : 0.10749879479408264\n",
      "Training loss for batch 6484 : 0.04700813814997673\n",
      "Training loss for batch 6485 : 0.12185842543840408\n",
      "Training loss for batch 6486 : 0.09076064079999924\n",
      "Training loss for batch 6487 : 0.27042651176452637\n",
      "Training loss for batch 6488 : 0.5351830124855042\n",
      "Training loss for batch 6489 : 0.1865633726119995\n",
      "Training loss for batch 6490 : 0.04563654959201813\n",
      "Training loss for batch 6491 : 0.31263554096221924\n",
      "Training loss for batch 6492 : 0.12593916058540344\n",
      "Training loss for batch 6493 : 0.05560823529958725\n",
      "Training loss for batch 6494 : 0.0\n",
      "Training loss for batch 6495 : 0.14929799735546112\n",
      "Training loss for batch 6496 : 0.039834119379520416\n",
      "Training loss for batch 6497 : 0.3655841648578644\n",
      "Training loss for batch 6498 : 0.005016076844185591\n",
      "Training loss for batch 6499 : 0.17020255327224731\n",
      "Training loss for batch 6500 : 0.4152548611164093\n",
      "Training loss for batch 6501 : 0.1361503005027771\n",
      "Training loss for batch 6502 : 0.2502165734767914\n",
      "Training loss for batch 6503 : 0.06082472205162048\n",
      "Training loss for batch 6504 : 0.13680362701416016\n",
      "Training loss for batch 6505 : 0.36908042430877686\n",
      "Training loss for batch 6506 : 0.11123543977737427\n",
      "Training loss for batch 6507 : 0.415347158908844\n",
      "Training loss for batch 6508 : 0.03706904873251915\n",
      "Training loss for batch 6509 : 0.32757917046546936\n",
      "Training loss for batch 6510 : 0.3793342411518097\n",
      "Training loss for batch 6511 : 0.4431385099887848\n",
      "Training loss for batch 6512 : 0.04734835773706436\n",
      "Training loss for batch 6513 : 0.3964748978614807\n",
      "Training loss for batch 6514 : 0.09599772095680237\n",
      "Training loss for batch 6515 : 0.1356813609600067\n",
      "Training loss for batch 6516 : 0.5855197310447693\n",
      "Training loss for batch 6517 : 0.02596414089202881\n",
      "Training loss for batch 6518 : 0.01788128912448883\n",
      "Training loss for batch 6519 : 0.3097486197948456\n",
      "Training loss for batch 6520 : 0.003071089740842581\n",
      "Training loss for batch 6521 : 0.32675281167030334\n",
      "Training loss for batch 6522 : 0.44005900621414185\n",
      "Training loss for batch 6523 : 0.05498075112700462\n",
      "Training loss for batch 6524 : 0.04072478041052818\n",
      "Training loss for batch 6525 : 0.045966945588588715\n",
      "Training loss for batch 6526 : 0.7516199350357056\n",
      "Training loss for batch 6527 : 0.03743525967001915\n",
      "Training loss for batch 6528 : 0.09992484748363495\n",
      "Training loss for batch 6529 : 0.2544510066509247\n",
      "Training loss for batch 6530 : 0.1263377070426941\n",
      "Training loss for batch 6531 : 0.14977401494979858\n",
      "Training loss for batch 6532 : 0.04907243698835373\n",
      "Training loss for batch 6533 : 0.34097591042518616\n",
      "Training loss for batch 6534 : 0.27358314394950867\n",
      "Training loss for batch 6535 : 0.2877000570297241\n",
      "Training loss for batch 6536 : 0.21849709749221802\n",
      "Training loss for batch 6537 : 0.546728789806366\n",
      "Training loss for batch 6538 : 0.0\n",
      "Training loss for batch 6539 : 0.2353515326976776\n",
      "Training loss for batch 6540 : 0.18559885025024414\n",
      "Training loss for batch 6541 : 0.11825709789991379\n",
      "Training loss for batch 6542 : 0.06897618621587753\n",
      "Training loss for batch 6543 : 0.18939310312271118\n",
      "Training loss for batch 6544 : 0.06777293980121613\n",
      "Training loss for batch 6545 : 0.2715921103954315\n",
      "Training loss for batch 6546 : 0.3002793788909912\n",
      "Training loss for batch 6547 : 0.11964012682437897\n",
      "Training loss for batch 6548 : 0.23548272252082825\n",
      "Training loss for batch 6549 : 0.3907477855682373\n",
      "Training loss for batch 6550 : 0.039951011538505554\n",
      "Training loss for batch 6551 : 0.06904856115579605\n",
      "Training loss for batch 6552 : 0.1071624681353569\n",
      "Training loss for batch 6553 : 0.32178425788879395\n",
      "Training loss for batch 6554 : 0.0826946422457695\n",
      "Training loss for batch 6555 : 0.027252502739429474\n",
      "Training loss for batch 6556 : 0.6807926297187805\n",
      "Training loss for batch 6557 : 0.257320761680603\n",
      "Training loss for batch 6558 : 0.40499019622802734\n",
      "Training loss for batch 6559 : 0.38929516077041626\n",
      "Training loss for batch 6560 : 0.6607109308242798\n",
      "Training loss for batch 6561 : 0.10057583451271057\n",
      "Training loss for batch 6562 : 0.10833403468132019\n",
      "Training loss for batch 6563 : 0.3692307472229004\n",
      "Training loss for batch 6564 : 0.13813695311546326\n",
      "Training loss for batch 6565 : 0.16578707098960876\n",
      "Training loss for batch 6566 : 0.20710580050945282\n",
      "Training loss for batch 6567 : 0.13904990255832672\n",
      "Training loss for batch 6568 : 0.686180591583252\n",
      "Training loss for batch 6569 : 0.10421500355005264\n",
      "Training loss for batch 6570 : 0.45089828968048096\n",
      "Training loss for batch 6571 : 0.054688405245542526\n",
      "Training loss for batch 6572 : 0.429157018661499\n",
      "Training loss for batch 6573 : 0.027691872790455818\n",
      "Training loss for batch 6574 : 0.36021751165390015\n",
      "Training loss for batch 6575 : 0.09745999425649643\n",
      "Training loss for batch 6576 : 0.054921697825193405\n",
      "Training loss for batch 6577 : 0.0820058137178421\n",
      "Training loss for batch 6578 : 0.05036938935518265\n",
      "Training loss for batch 6579 : 0.1273263394832611\n",
      "Training loss for batch 6580 : 0.35985106229782104\n",
      "Training loss for batch 6581 : 0.07991842925548553\n",
      "Training loss for batch 6582 : 0.12141432613134384\n",
      "Training loss for batch 6583 : 0.2694943845272064\n",
      "Training loss for batch 6584 : 0.4708876609802246\n",
      "Training loss for batch 6585 : 0.08542875945568085\n",
      "Training loss for batch 6586 : 0.35450366139411926\n",
      "Training loss for batch 6587 : 0.3920760154724121\n",
      "Training loss for batch 6588 : 0.027469929307699203\n",
      "Training loss for batch 6589 : 0.16007204353809357\n",
      "Training loss for batch 6590 : 0.3215755820274353\n",
      "Training loss for batch 6591 : 0.08003675192594528\n",
      "Training loss for batch 6592 : 0.22422058880329132\n",
      "Training loss for batch 6593 : 0.17063041031360626\n",
      "Training loss for batch 6594 : 0.19261930882930756\n",
      "Training loss for batch 6595 : 0.35055598616600037\n",
      "Training loss for batch 6596 : 0.3675130605697632\n",
      "Training loss for batch 6597 : 0.13175839185714722\n",
      "Training loss for batch 6598 : 0.1692689061164856\n",
      "Training loss for batch 6599 : 0.40380507707595825\n",
      "Training loss for batch 6600 : 0.04674082249403\n",
      "Training loss for batch 6601 : 0.042155101895332336\n",
      "Training loss for batch 6602 : 0.36235201358795166\n",
      "Training loss for batch 6603 : 0.33806312084198\n",
      "Training loss for batch 6604 : 0.2934410572052002\n",
      "Training loss for batch 6605 : 0.11003869771957397\n",
      "Training loss for batch 6606 : 0.07164766639471054\n",
      "Training loss for batch 6607 : 0.059647999703884125\n",
      "Training loss for batch 6608 : 0.0\n",
      "Training loss for batch 6609 : 0.26759469509124756\n",
      "Training loss for batch 6610 : 0.4645797312259674\n",
      "Training loss for batch 6611 : 0.03724711015820503\n",
      "Training loss for batch 6612 : 0.11076827347278595\n",
      "Training loss for batch 6613 : 0.1280737668275833\n",
      "Training loss for batch 6614 : 0.32518482208251953\n",
      "Training loss for batch 6615 : 0.12816983461380005\n",
      "Training loss for batch 6616 : 0.1097230613231659\n",
      "Training loss for batch 6617 : 0.234211266040802\n",
      "Training loss for batch 6618 : 0.546880304813385\n",
      "Training loss for batch 6619 : 0.32927221059799194\n",
      "Training loss for batch 6620 : 0.040354613214731216\n",
      "Training loss for batch 6621 : 0.39020946621894836\n",
      "Training loss for batch 6622 : 0.2716158330440521\n",
      "Training loss for batch 6623 : 0.46716195344924927\n",
      "Training loss for batch 6624 : 0.19994506239891052\n",
      "Training loss for batch 6625 : 0.1298334300518036\n",
      "Training loss for batch 6626 : 0.3769647479057312\n",
      "Training loss for batch 6627 : 0.46863433718681335\n",
      "Training loss for batch 6628 : 0.036988209933042526\n",
      "Training loss for batch 6629 : 0.017349744215607643\n",
      "Training loss for batch 6630 : 0.05258210748434067\n",
      "Training loss for batch 6631 : 0.4422641396522522\n",
      "Training loss for batch 6632 : 0.2700462341308594\n",
      "Training loss for batch 6633 : 0.049566444009542465\n",
      "Training loss for batch 6634 : 0.3521352708339691\n",
      "Training loss for batch 6635 : 0.3177187740802765\n",
      "Training loss for batch 6636 : 0.029367417097091675\n",
      "Training loss for batch 6637 : 0.46102404594421387\n",
      "Training loss for batch 6638 : 0.3223785161972046\n",
      "Training loss for batch 6639 : 0.3519366681575775\n",
      "Training loss for batch 6640 : 0.31534498929977417\n",
      "Training loss for batch 6641 : 0.43136081099510193\n",
      "Training loss for batch 6642 : 0.523291826248169\n",
      "Training loss for batch 6643 : 0.10482935607433319\n",
      "Training loss for batch 6644 : 0.06677156686782837\n",
      "Training loss for batch 6645 : 0.20102019608020782\n",
      "Training loss for batch 6646 : 0.26981040835380554\n",
      "Training loss for batch 6647 : 0.3342750072479248\n",
      "Training loss for batch 6648 : 0.16320037841796875\n",
      "Training loss for batch 6649 : 0.2240343540906906\n",
      "Training loss for batch 6650 : 0.0373668298125267\n",
      "Training loss for batch 6651 : 0.07801056653261185\n",
      "Training loss for batch 6652 : 0.041430603712797165\n",
      "Training loss for batch 6653 : 0.49478408694267273\n",
      "Training loss for batch 6654 : 0.03150854632258415\n",
      "Training loss for batch 6655 : 0.03760889545083046\n",
      "Training loss for batch 6656 : 0.09136300534009933\n",
      "Training loss for batch 6657 : 0.5712506771087646\n",
      "Training loss for batch 6658 : 0.05000234767794609\n",
      "Training loss for batch 6659 : 0.3074333071708679\n",
      "Training loss for batch 6660 : 0.42225420475006104\n",
      "Training loss for batch 6661 : 0.03520672395825386\n",
      "Training loss for batch 6662 : 0.22058427333831787\n",
      "Training loss for batch 6663 : 0.028573237359523773\n",
      "Training loss for batch 6664 : 0.062430452555418015\n",
      "Training loss for batch 6665 : 0.017662478610873222\n",
      "Training loss for batch 6666 : 0.1075088232755661\n",
      "Training loss for batch 6667 : 0.009956326335668564\n",
      "Training loss for batch 6668 : 0.33297309279441833\n",
      "Training loss for batch 6669 : 0.07295814901590347\n",
      "Training loss for batch 6670 : 0.2559534013271332\n",
      "Training loss for batch 6671 : 0.245026633143425\n",
      "Training loss for batch 6672 : 0.004396542906761169\n",
      "Training loss for batch 6673 : 0.14467747509479523\n",
      "Training loss for batch 6674 : 0.5078337788581848\n",
      "Training loss for batch 6675 : 0.1881411224603653\n",
      "Training loss for batch 6676 : 0.0142383873462677\n",
      "Training loss for batch 6677 : 0.007746458053588867\n",
      "Training loss for batch 6678 : 0.2718176245689392\n",
      "Training loss for batch 6679 : 0.4103353023529053\n",
      "Training loss for batch 6680 : 0.11769544333219528\n",
      "Training loss for batch 6681 : 0.029678888618946075\n",
      "Training loss for batch 6682 : 0.1799502968788147\n",
      "Training loss for batch 6683 : 0.049306176602840424\n",
      "Training loss for batch 6684 : 0.2846892774105072\n",
      "Training loss for batch 6685 : 0.06656838953495026\n",
      "Training loss for batch 6686 : 0.32694458961486816\n",
      "Training loss for batch 6687 : 0.10557982325553894\n",
      "Training loss for batch 6688 : 0.021041685715317726\n",
      "Training loss for batch 6689 : 0.3945387899875641\n",
      "Training loss for batch 6690 : 0.08401922136545181\n",
      "Training loss for batch 6691 : 0.11698281764984131\n",
      "Training loss for batch 6692 : 0.16214194893836975\n",
      "Training loss for batch 6693 : 0.12033116817474365\n",
      "Training loss for batch 6694 : 0.30645424127578735\n",
      "Training loss for batch 6695 : 0.026897264644503593\n",
      "Training loss for batch 6696 : 0.4032706320285797\n",
      "Training loss for batch 6697 : 0.668743908405304\n",
      "Training loss for batch 6698 : 0.12213016301393509\n",
      "Training loss for batch 6699 : 0.14148300886154175\n",
      "Training loss for batch 6700 : 0.0190181415528059\n",
      "Training loss for batch 6701 : 0.31009870767593384\n",
      "Training loss for batch 6702 : 0.0003551443514879793\n",
      "Training loss for batch 6703 : 0.23179903626441956\n",
      "Training loss for batch 6704 : 0.26886793971061707\n",
      "Training loss for batch 6705 : 0.20276613533496857\n",
      "Training loss for batch 6706 : 0.4445630609989166\n",
      "Training loss for batch 6707 : 0.11489042639732361\n",
      "Training loss for batch 6708 : 0.12453865259885788\n",
      "Training loss for batch 6709 : 0.8374934792518616\n",
      "Training loss for batch 6710 : 0.7501319646835327\n",
      "Training loss for batch 6711 : 0.28928956389427185\n",
      "Training loss for batch 6712 : 0.04663452133536339\n",
      "Training loss for batch 6713 : 0.41714760661125183\n",
      "Training loss for batch 6714 : 0.18293601274490356\n",
      "Training loss for batch 6715 : 0.0019111045403406024\n",
      "Training loss for batch 6716 : 0.048520542681217194\n",
      "Training loss for batch 6717 : 0.07492756843566895\n",
      "Training loss for batch 6718 : 0.03970528393983841\n",
      "Training loss for batch 6719 : 0.22402054071426392\n",
      "Training loss for batch 6720 : 9.14434640435502e-05\n",
      "Training loss for batch 6721 : 0.8130114078521729\n",
      "Training loss for batch 6722 : 0.27773812413215637\n",
      "Training loss for batch 6723 : 0.09841633588075638\n",
      "Training loss for batch 6724 : 0.3315442204475403\n",
      "Training loss for batch 6725 : 0.07462506741285324\n",
      "Training loss for batch 6726 : 0.03649077191948891\n",
      "Training loss for batch 6727 : 0.38322460651397705\n",
      "Training loss for batch 6728 : 0.11449593305587769\n",
      "Training loss for batch 6729 : 0.17353808879852295\n",
      "Training loss for batch 6730 : 0.6185240149497986\n",
      "Training loss for batch 6731 : 0.05736345425248146\n",
      "Training loss for batch 6732 : 0.0\n",
      "Training loss for batch 6733 : 0.3990001380443573\n",
      "Training loss for batch 6734 : 0.17712120711803436\n",
      "Training loss for batch 6735 : 0.13436856865882874\n",
      "Training loss for batch 6736 : 0.011007438413798809\n",
      "Training loss for batch 6737 : 0.293571412563324\n",
      "Training loss for batch 6738 : 0.19479738175868988\n",
      "Training loss for batch 6739 : 0.056044965982437134\n",
      "Training loss for batch 6740 : 0.28425130248069763\n",
      "Training loss for batch 6741 : 0.7968519926071167\n",
      "Training loss for batch 6742 : 0.2948513627052307\n",
      "Training loss for batch 6743 : 0.0728207677602768\n",
      "Training loss for batch 6744 : 0.043053288012742996\n",
      "Training loss for batch 6745 : 0.04197659716010094\n",
      "Training loss for batch 6746 : 0.3678039014339447\n",
      "Training loss for batch 6747 : 0.12596465647220612\n",
      "Training loss for batch 6748 : 0.04843500629067421\n",
      "Training loss for batch 6749 : 0.5061080455780029\n",
      "Training loss for batch 6750 : 0.12280485779047012\n",
      "Training loss for batch 6751 : 0.038080863654613495\n",
      "Training loss for batch 6752 : 0.29944315552711487\n",
      "Training loss for batch 6753 : 0.12720870971679688\n",
      "Training loss for batch 6754 : 0.06647411733865738\n",
      "Training loss for batch 6755 : 0.01354322861880064\n",
      "Training loss for batch 6756 : 0.03715985268354416\n",
      "Training loss for batch 6757 : 0.11573608964681625\n",
      "Training loss for batch 6758 : 0.054680369794368744\n",
      "Training loss for batch 6759 : 0.19626197218894958\n",
      "Training loss for batch 6760 : 0.6156755685806274\n",
      "Training loss for batch 6761 : 0.531468391418457\n",
      "Training loss for batch 6762 : 0.38330477476119995\n",
      "Training loss for batch 6763 : 0.1695658415555954\n",
      "Training loss for batch 6764 : 0.002300570486113429\n",
      "Training loss for batch 6765 : 0.2759549915790558\n",
      "Training loss for batch 6766 : 0.041260726749897\n",
      "Training loss for batch 6767 : 0.27202844619750977\n",
      "Training loss for batch 6768 : 0.4289063811302185\n",
      "Training loss for batch 6769 : 0.23561058938503265\n",
      "Training loss for batch 6770 : 0.26269105076789856\n",
      "Training loss for batch 6771 : 0.2096477597951889\n",
      "Training loss for batch 6772 : 0.3876429796218872\n",
      "Training loss for batch 6773 : 0.013491563498973846\n",
      "Training loss for batch 6774 : 0.20490586757659912\n",
      "Training loss for batch 6775 : 0.36136767268180847\n",
      "Training loss for batch 6776 : 0.2020157128572464\n",
      "Training loss for batch 6777 : 0.30054575204849243\n",
      "Training loss for batch 6778 : 0.18988780677318573\n",
      "Training loss for batch 6779 : 0.045094240456819534\n",
      "Training loss for batch 6780 : 0.4072984755039215\n",
      "Training loss for batch 6781 : 0.4032110571861267\n",
      "Training loss for batch 6782 : 0.003077129367738962\n",
      "Training loss for batch 6783 : 0.03689011558890343\n",
      "Training loss for batch 6784 : 0.34683388471603394\n",
      "Training loss for batch 6785 : 0.10317869484424591\n",
      "Training loss for batch 6786 : 0.04215531796216965\n",
      "Training loss for batch 6787 : 0.3935587704181671\n",
      "Training loss for batch 6788 : 0.0010936459293588996\n",
      "Training loss for batch 6789 : 0.4167037010192871\n",
      "Training loss for batch 6790 : 0.007131859660148621\n",
      "Training loss for batch 6791 : 0.02174314856529236\n",
      "Training loss for batch 6792 : 0.14029225707054138\n",
      "Training loss for batch 6793 : 0.1563592404127121\n",
      "Training loss for batch 6794 : 0.09916798770427704\n",
      "Training loss for batch 6795 : 0.3386325240135193\n",
      "Training loss for batch 6796 : 0.5052917003631592\n",
      "Training loss for batch 6797 : 0.6657640933990479\n",
      "Training loss for batch 6798 : 0.1412511169910431\n",
      "Training loss for batch 6799 : 0.14440186321735382\n",
      "Training loss for batch 6800 : 0.15654003620147705\n",
      "Training loss for batch 6801 : 0.18356287479400635\n",
      "Training loss for batch 6802 : 0.011169498786330223\n",
      "Training loss for batch 6803 : 0.16745242476463318\n",
      "Training loss for batch 6804 : 0.44606873393058777\n",
      "Training loss for batch 6805 : 0.013890018686652184\n",
      "Training loss for batch 6806 : 0.5771401524543762\n",
      "Training loss for batch 6807 : 0.16547398269176483\n",
      "Training loss for batch 6808 : 0.015497340820729733\n",
      "Training loss for batch 6809 : 0.43148860335350037\n",
      "Training loss for batch 6810 : 0.0060514709912240505\n",
      "Training loss for batch 6811 : 0.06729245185852051\n",
      "Training loss for batch 6812 : 0.015495519153773785\n",
      "Training loss for batch 6813 : 0.31268852949142456\n",
      "Training loss for batch 6814 : 0.13461841642856598\n",
      "Training loss for batch 6815 : 0.2868066430091858\n",
      "Training loss for batch 6816 : 0.04765273630619049\n",
      "Training loss for batch 6817 : 0.3370510935783386\n",
      "Training loss for batch 6818 : 0.19884948432445526\n",
      "Training loss for batch 6819 : 0.3929074704647064\n",
      "Training loss for batch 6820 : 0.22591564059257507\n",
      "Training loss for batch 6821 : 0.11331517994403839\n",
      "Training loss for batch 6822 : 0.47643715143203735\n",
      "Training loss for batch 6823 : 0.24379266798496246\n",
      "Training loss for batch 6824 : 0.016405196860432625\n",
      "Training loss for batch 6825 : 0.11177480965852737\n",
      "Training loss for batch 6826 : 0.013323565945029259\n",
      "Training loss for batch 6827 : 0.10990364849567413\n",
      "Training loss for batch 6828 : 0.10192926228046417\n",
      "Training loss for batch 6829 : 0.3953273594379425\n",
      "Training loss for batch 6830 : 0.2665000855922699\n",
      "Training loss for batch 6831 : 0.27594637870788574\n",
      "Training loss for batch 6832 : 0.06052621454000473\n",
      "Training loss for batch 6833 : 0.09829111397266388\n",
      "Training loss for batch 6834 : 0.23546244204044342\n",
      "Training loss for batch 6835 : 0.1897350251674652\n",
      "Training loss for batch 6836 : 0.07338596880435944\n",
      "Training loss for batch 6837 : 0.09787663072347641\n",
      "Training loss for batch 6838 : 0.4045565128326416\n",
      "Training loss for batch 6839 : 0.3033682107925415\n",
      "Training loss for batch 6840 : 0.0711219534277916\n",
      "Training loss for batch 6841 : 0.295956015586853\n",
      "Training loss for batch 6842 : 0.5400243401527405\n",
      "Training loss for batch 6843 : 0.0047509754076600075\n",
      "Training loss for batch 6844 : 0.33624765276908875\n",
      "Training loss for batch 6845 : 0.05160745233297348\n",
      "Training loss for batch 6846 : 0.46043142676353455\n",
      "Training loss for batch 6847 : 0.12657158076763153\n",
      "Training loss for batch 6848 : 0.16043715178966522\n",
      "Training loss for batch 6849 : 0.19972586631774902\n",
      "Training loss for batch 6850 : 0.18803881108760834\n",
      "Training loss for batch 6851 : 0.3350941240787506\n",
      "Training loss for batch 6852 : 0.09282974153757095\n",
      "Training loss for batch 6853 : 0.12139958143234253\n",
      "Training loss for batch 6854 : 0.1589960902929306\n",
      "Training loss for batch 6855 : 0.27214518189430237\n",
      "Training loss for batch 6856 : 0.4417313039302826\n",
      "Training loss for batch 6857 : 0.03544951230287552\n",
      "Training loss for batch 6858 : 0.0180674958974123\n",
      "Training loss for batch 6859 : 0.11068474501371384\n",
      "Training loss for batch 6860 : 0.12039384245872498\n",
      "Training loss for batch 6861 : 0.31495189666748047\n",
      "Training loss for batch 6862 : 0.7744995951652527\n",
      "Training loss for batch 6863 : 0.26972338557243347\n",
      "Training loss for batch 6864 : 0.2972804307937622\n",
      "Training loss for batch 6865 : 0.38512730598449707\n",
      "Training loss for batch 6866 : 0.36729899048805237\n",
      "Training loss for batch 6867 : 0.19463226199150085\n",
      "Training loss for batch 6868 : 0.40305936336517334\n",
      "Training loss for batch 6869 : 0.3613388240337372\n",
      "Training loss for batch 6870 : 0.2560802400112152\n",
      "Training loss for batch 6871 : 0.6058772206306458\n",
      "Training loss for batch 6872 : 0.24842365086078644\n",
      "Training loss for batch 6873 : 0.2506721615791321\n",
      "Training loss for batch 6874 : 0.13095666468143463\n",
      "Training loss for batch 6875 : 0.039957206696271896\n",
      "Training loss for batch 6876 : 0.2737342417240143\n",
      "Training loss for batch 6877 : 0.13452674448490143\n",
      "Training loss for batch 6878 : 0.2847697138786316\n",
      "Training loss for batch 6879 : 0.023158682510256767\n",
      "Training loss for batch 6880 : 0.616333544254303\n",
      "Training loss for batch 6881 : 0.18047069013118744\n",
      "Training loss for batch 6882 : 0.533605694770813\n",
      "Training loss for batch 6883 : 0.05529415234923363\n",
      "Training loss for batch 6884 : 0.02761933207511902\n",
      "Training loss for batch 6885 : 0.7620664834976196\n",
      "Training loss for batch 6886 : 0.26181092858314514\n",
      "Training loss for batch 6887 : 0.10500966757535934\n",
      "Training loss for batch 6888 : 0.2887035608291626\n",
      "Training loss for batch 6889 : 0.617440402507782\n",
      "Training loss for batch 6890 : 0.2304319441318512\n",
      "Training loss for batch 6891 : 0.0007068713894113898\n",
      "Training loss for batch 6892 : 0.09141942858695984\n",
      "Training loss for batch 6893 : 0.1125144436955452\n",
      "Training loss for batch 6894 : 0.4152959883213043\n",
      "Training loss for batch 6895 : 0.018368078395724297\n",
      "Training loss for batch 6896 : 0.199002206325531\n",
      "Training loss for batch 6897 : 0.3153265416622162\n",
      "Training loss for batch 6898 : 0.49689334630966187\n",
      "Training loss for batch 6899 : 0.4725063443183899\n",
      "Training loss for batch 6900 : 0.00020739022875204682\n",
      "Training loss for batch 6901 : 0.06703974306583405\n",
      "Training loss for batch 6902 : 0.38531970977783203\n",
      "Training loss for batch 6903 : 0.0061047677882015705\n",
      "Training loss for batch 6904 : 0.24954389035701752\n",
      "Training loss for batch 6905 : 0.31743547320365906\n",
      "Training loss for batch 6906 : 0.04503106698393822\n",
      "Training loss for batch 6907 : 0.16318753361701965\n",
      "Training loss for batch 6908 : 0.27161896228790283\n",
      "Training loss for batch 6909 : 0.6522473096847534\n",
      "Training loss for batch 6910 : 0.3335568904876709\n",
      "Training loss for batch 6911 : 0.44975918531417847\n",
      "Training loss for batch 6912 : 0.15987175703048706\n",
      "Training loss for batch 6913 : 0.03521212190389633\n",
      "Training loss for batch 6914 : 0.05334397777915001\n",
      "Training loss for batch 6915 : 0.4360111355781555\n",
      "Training loss for batch 6916 : 0.13122567534446716\n",
      "Training loss for batch 6917 : 0.4357946217060089\n",
      "Training loss for batch 6918 : 0.03192421793937683\n",
      "Training loss for batch 6919 : 0.17666327953338623\n",
      "Training loss for batch 6920 : 0.18830333650112152\n",
      "Training loss for batch 6921 : 0.23934321105480194\n",
      "Training loss for batch 6922 : 0.22035211324691772\n",
      "Training loss for batch 6923 : 0.0\n",
      "Training loss for batch 6924 : 0.0038597784005105495\n",
      "Training loss for batch 6925 : 0.17342987656593323\n",
      "Training loss for batch 6926 : 0.16248269379138947\n",
      "Training loss for batch 6927 : 0.0\n",
      "Training loss for batch 6928 : 0.2975499629974365\n",
      "Training loss for batch 6929 : 0.3165859878063202\n",
      "Training loss for batch 6930 : 0.06769227236509323\n",
      "Training loss for batch 6931 : 0.4878012537956238\n",
      "Training loss for batch 6932 : 0.10643340647220612\n",
      "Training loss for batch 6933 : 0.08842067420482635\n",
      "Training loss for batch 6934 : 0.42362314462661743\n",
      "Training loss for batch 6935 : 0.04703298956155777\n",
      "Training loss for batch 6936 : 0.07910649478435516\n",
      "Training loss for batch 6937 : 0.15008820593357086\n",
      "Training loss for batch 6938 : 0.09064333885908127\n",
      "Training loss for batch 6939 : 0.3291046917438507\n",
      "Training loss for batch 6940 : 0.017330249771475792\n",
      "Training loss for batch 6941 : 0.08650892227888107\n",
      "Training loss for batch 6942 : 0.3554149270057678\n",
      "Training loss for batch 6943 : 0.2957628071308136\n",
      "Training loss for batch 6944 : 0.1653399020433426\n",
      "Training loss for batch 6945 : 0.37968000769615173\n",
      "Training loss for batch 6946 : 0.1641542911529541\n",
      "Training loss for batch 6947 : 0.0696641281247139\n",
      "Training loss for batch 6948 : 0.11759044229984283\n",
      "Training loss for batch 6949 : 0.06801405549049377\n",
      "Training loss for batch 6950 : 0.16069622337818146\n",
      "Training loss for batch 6951 : 0.05464067682623863\n",
      "Training loss for batch 6952 : 0.6973419785499573\n",
      "Training loss for batch 6953 : 0.2479705959558487\n",
      "Training loss for batch 6954 : 0.42843350768089294\n",
      "Training loss for batch 6955 : 0.32188183069229126\n",
      "Training loss for batch 6956 : 0.09197428077459335\n",
      "Training loss for batch 6957 : 0.03318796679377556\n",
      "Training loss for batch 6958 : 0.38359710574150085\n",
      "Training loss for batch 6959 : 0.0589396171271801\n",
      "Training loss for batch 6960 : 0.28020888566970825\n",
      "Training loss for batch 6961 : 0.1769484281539917\n",
      "Training loss for batch 6962 : 0.21210789680480957\n",
      "Training loss for batch 6963 : 0.14194273948669434\n",
      "Training loss for batch 6964 : 0.18504059314727783\n",
      "Training loss for batch 6965 : 0.3780342638492584\n",
      "Training loss for batch 6966 : 0.37232303619384766\n",
      "Training loss for batch 6967 : 0.5160852074623108\n",
      "Training loss for batch 6968 : 0.15563693642616272\n",
      "Training loss for batch 6969 : 0.11580277234315872\n",
      "Training loss for batch 6970 : 0.2890520393848419\n",
      "Training loss for batch 6971 : 0.13632819056510925\n",
      "Training loss for batch 6972 : 0.6678862571716309\n",
      "Training loss for batch 6973 : 0.11179637163877487\n",
      "Training loss for batch 6974 : 0.05163780227303505\n",
      "Training loss for batch 6975 : 0.31101325154304504\n",
      "Training loss for batch 6976 : 0.06171312928199768\n",
      "Training loss for batch 6977 : 0.40939557552337646\n",
      "Training loss for batch 6978 : 0.3186291754245758\n",
      "Training loss for batch 6979 : 0.10292775928974152\n",
      "Training loss for batch 6980 : 0.2816895842552185\n",
      "Training loss for batch 6981 : 0.0810079574584961\n",
      "Training loss for batch 6982 : 0.024954086169600487\n",
      "Training loss for batch 6983 : 0.1479582041501999\n",
      "Training loss for batch 6984 : 0.1632331758737564\n",
      "Training loss for batch 6985 : 0.46554097533226013\n",
      "Training loss for batch 6986 : 0.10371991991996765\n",
      "Training loss for batch 6987 : 0.24213331937789917\n",
      "Training loss for batch 6988 : 0.34592902660369873\n",
      "Training loss for batch 6989 : 0.263203889131546\n",
      "Training loss for batch 6990 : 0.3497367799282074\n",
      "Training loss for batch 6991 : 0.4003288149833679\n",
      "Training loss for batch 6992 : 0.14960229396820068\n",
      "Training loss for batch 6993 : 0.2661314904689789\n",
      "Training loss for batch 6994 : 0.4329833388328552\n",
      "Training loss for batch 6995 : 0.2906927764415741\n",
      "Training loss for batch 6996 : 0.23551540076732635\n",
      "Training loss for batch 6997 : 0.31158676743507385\n",
      "Training loss for batch 6998 : 0.08879354596138\n",
      "Training loss for batch 6999 : 0.3947550356388092\n",
      "Training loss for batch 7000 : 0.07117383927106857\n",
      "Training loss for batch 7001 : 0.3143199384212494\n",
      "Training loss for batch 7002 : 0.024829141795635223\n",
      "Training loss for batch 7003 : 0.14097243547439575\n",
      "Training loss for batch 7004 : 0.18793052434921265\n",
      "Training loss for batch 7005 : 0.04490910470485687\n",
      "Training loss for batch 7006 : 0.08174058049917221\n",
      "Training loss for batch 7007 : 0.07752089202404022\n",
      "Training loss for batch 7008 : 0.034473370760679245\n",
      "Training loss for batch 7009 : 0.10987608134746552\n",
      "Training loss for batch 7010 : 0.0\n",
      "Training loss for batch 7011 : 0.13016480207443237\n",
      "Training loss for batch 7012 : 0.30065879225730896\n",
      "Training loss for batch 7013 : 0.31076666712760925\n",
      "Training loss for batch 7014 : 0.0435725674033165\n",
      "Training loss for batch 7015 : 0.08392024040222168\n",
      "Training loss for batch 7016 : 0.5533563494682312\n",
      "Training loss for batch 7017 : 0.12168237566947937\n",
      "Training loss for batch 7018 : 0.29775428771972656\n",
      "Training loss for batch 7019 : 0.18222786486148834\n",
      "Training loss for batch 7020 : 0.12453795969486237\n",
      "Training loss for batch 7021 : 0.09544041752815247\n",
      "Training loss for batch 7022 : 0.4003864824771881\n",
      "Training loss for batch 7023 : 0.3576946556568146\n",
      "Training loss for batch 7024 : 0.07942667603492737\n",
      "Training loss for batch 7025 : 0.21963977813720703\n",
      "Training loss for batch 7026 : 0.1538538932800293\n",
      "Training loss for batch 7027 : 0.17866532504558563\n",
      "Training loss for batch 7028 : 0.02338891662657261\n",
      "Training loss for batch 7029 : 0.11396261304616928\n",
      "Training loss for batch 7030 : 0.012843851931393147\n",
      "Training loss for batch 7031 : 0.07863567769527435\n",
      "Training loss for batch 7032 : 0.4414650499820709\n",
      "Training loss for batch 7033 : 0.14172621071338654\n",
      "Training loss for batch 7034 : 0.4347592294216156\n",
      "Training loss for batch 7035 : 0.14226134121418\n",
      "Training loss for batch 7036 : 0.08633701503276825\n",
      "Training loss for batch 7037 : 0.1450774222612381\n",
      "Training loss for batch 7038 : 0.4834844470024109\n",
      "Training loss for batch 7039 : 0.0031837683636695147\n",
      "Training loss for batch 7040 : 0.6657566428184509\n",
      "Training loss for batch 7041 : 0.07655057311058044\n",
      "Training loss for batch 7042 : 0.022500446066260338\n",
      "Training loss for batch 7043 : 0.4038998484611511\n",
      "Training loss for batch 7044 : 0.2645634412765503\n",
      "Training loss for batch 7045 : 0.1382797211408615\n",
      "Training loss for batch 7046 : 0.5286558866500854\n",
      "Training loss for batch 7047 : 0.19768930971622467\n",
      "Training loss for batch 7048 : 0.10597310215234756\n",
      "Training loss for batch 7049 : 0.013082225807011127\n",
      "Training loss for batch 7050 : 0.08894495666027069\n",
      "Training loss for batch 7051 : 0.5166105031967163\n",
      "Training loss for batch 7052 : 0.15851204097270966\n",
      "Training loss for batch 7053 : 0.34625640511512756\n",
      "Training loss for batch 7054 : 0.053325407207012177\n",
      "Training loss for batch 7055 : 0.3415612578392029\n",
      "Training loss for batch 7056 : 0.08795664459466934\n",
      "Training loss for batch 7057 : 0.29474174976348877\n",
      "Training loss for batch 7058 : 0.056308768689632416\n",
      "Training loss for batch 7059 : 0.38863861560821533\n",
      "Training loss for batch 7060 : 0.3803118169307709\n",
      "Training loss for batch 7061 : 0.2011047750711441\n",
      "Training loss for batch 7062 : 0.1743655800819397\n",
      "Training loss for batch 7063 : 0.45418235659599304\n",
      "Training loss for batch 7064 : 0.0\n",
      "Training loss for batch 7065 : 0.04528769105672836\n",
      "Training loss for batch 7066 : 0.04559151083230972\n",
      "Training loss for batch 7067 : 0.24839238822460175\n",
      "Training loss for batch 7068 : 0.22005368769168854\n",
      "Training loss for batch 7069 : 0.09088346362113953\n",
      "Training loss for batch 7070 : 0.04191949963569641\n",
      "Training loss for batch 7071 : 0.42260974645614624\n",
      "Training loss for batch 7072 : 0.00219293893314898\n",
      "Training loss for batch 7073 : 0.007429602090269327\n",
      "Training loss for batch 7074 : 0.14594238996505737\n",
      "Training loss for batch 7075 : 0.19700339436531067\n",
      "Training loss for batch 7076 : 0.011317899450659752\n",
      "Training loss for batch 7077 : 0.11376754194498062\n",
      "Training loss for batch 7078 : 0.006471544504165649\n",
      "Training loss for batch 7079 : 0.06846408545970917\n",
      "Training loss for batch 7080 : 0.417476087808609\n",
      "Training loss for batch 7081 : 0.24864603579044342\n",
      "Training loss for batch 7082 : 0.17333632707595825\n",
      "Training loss for batch 7083 : 0.1508609801530838\n",
      "Training loss for batch 7084 : 0.24035514891147614\n",
      "Training loss for batch 7085 : 0.24188174307346344\n",
      "Training loss for batch 7086 : 0.01901950128376484\n",
      "Training loss for batch 7087 : 0.049120575189590454\n",
      "Training loss for batch 7088 : 0.37607109546661377\n",
      "Training loss for batch 7089 : 0.025872355327010155\n",
      "Training loss for batch 7090 : 0.2726658880710602\n",
      "Training loss for batch 7091 : 0.09972440451383591\n",
      "Training loss for batch 7092 : 0.10897067189216614\n",
      "Training loss for batch 7093 : 0.39874204993247986\n",
      "Training loss for batch 7094 : 0.2719748020172119\n",
      "Training loss for batch 7095 : 0.52924644947052\n",
      "Training loss for batch 7096 : 0.12693551182746887\n",
      "Training loss for batch 7097 : 0.4058562219142914\n",
      "Training loss for batch 7098 : 0.10091902315616608\n",
      "Training loss for batch 7099 : 0.3560192584991455\n",
      "Training loss for batch 7100 : 0.18878646194934845\n",
      "Training loss for batch 7101 : 0.28133055567741394\n",
      "Training loss for batch 7102 : 0.44461578130722046\n",
      "Training loss for batch 7103 : 0.09784283488988876\n",
      "Training loss for batch 7104 : 0.1623566746711731\n",
      "Training loss for batch 7105 : 0.020953772589564323\n",
      "Training loss for batch 7106 : 0.044898342341184616\n",
      "Training loss for batch 7107 : 0.0\n",
      "Training loss for batch 7108 : 0.8266525864601135\n",
      "Training loss for batch 7109 : 0.04811372607946396\n",
      "Training loss for batch 7110 : 0.2393684983253479\n",
      "Training loss for batch 7111 : 0.5320494771003723\n",
      "Training loss for batch 7112 : 0.010497486218810081\n",
      "Training loss for batch 7113 : 0.027371546253561974\n",
      "Training loss for batch 7114 : 0.10343509167432785\n",
      "Training loss for batch 7115 : 0.005934710148721933\n",
      "Training loss for batch 7116 : 0.0577465258538723\n",
      "Training loss for batch 7117 : 0.6066476106643677\n",
      "Training loss for batch 7118 : 0.3069574236869812\n",
      "Training loss for batch 7119 : 0.05124397575855255\n",
      "Training loss for batch 7120 : 0.012487098574638367\n",
      "Training loss for batch 7121 : 0.0027661225758492947\n",
      "Training loss for batch 7122 : 0.22552120685577393\n",
      "Training loss for batch 7123 : 0.18894003331661224\n",
      "Training loss for batch 7124 : 0.01650201715528965\n",
      "Training loss for batch 7125 : 0.014072959311306477\n",
      "Training loss for batch 7126 : 0.12490174919366837\n",
      "Training loss for batch 7127 : 0.22400198876857758\n",
      "Training loss for batch 7128 : 0.18806160986423492\n",
      "Training loss for batch 7129 : 0.5396745800971985\n",
      "Training loss for batch 7130 : 0.05927472189068794\n",
      "Training loss for batch 7131 : 0.3237926661968231\n",
      "Training loss for batch 7132 : 0.10091718286275864\n",
      "Training loss for batch 7133 : 0.13631044328212738\n",
      "Training loss for batch 7134 : 0.15763957798480988\n",
      "Training loss for batch 7135 : 0.6876517534255981\n",
      "Training loss for batch 7136 : 0.6622123718261719\n",
      "Training loss for batch 7137 : 0.0838543176651001\n",
      "Training loss for batch 7138 : 0.31468501687049866\n",
      "Training loss for batch 7139 : 0.37170782685279846\n",
      "Training loss for batch 7140 : 0.11608370393514633\n",
      "Training loss for batch 7141 : 0.011453370563685894\n",
      "Training loss for batch 7142 : 0.03853634372353554\n",
      "Training loss for batch 7143 : 0.13763083517551422\n",
      "Training loss for batch 7144 : 0.2944541275501251\n",
      "Training loss for batch 7145 : 0.023663470521569252\n",
      "Training loss for batch 7146 : 0.08846914768218994\n",
      "Training loss for batch 7147 : 0.1821979135274887\n",
      "Training loss for batch 7148 : 0.06104423105716705\n",
      "Training loss for batch 7149 : 0.39233654737472534\n",
      "Training loss for batch 7150 : 0.34258362650871277\n",
      "Training loss for batch 7151 : 0.09487540274858475\n",
      "Training loss for batch 7152 : 0.11729831993579865\n",
      "Training loss for batch 7153 : 0.21081411838531494\n",
      "Training loss for batch 7154 : 0.010067861527204514\n",
      "Training loss for batch 7155 : 0.30150991678237915\n",
      "Training loss for batch 7156 : 0.19445155560970306\n",
      "Training loss for batch 7157 : 0.14335466921329498\n",
      "Training loss for batch 7158 : 0.3026941418647766\n",
      "Training loss for batch 7159 : 0.4133949875831604\n",
      "Training loss for batch 7160 : 0.4853055775165558\n",
      "Training loss for batch 7161 : 0.433745801448822\n",
      "Training loss for batch 7162 : 0.5785198211669922\n",
      "Training loss for batch 7163 : 0.041336774826049805\n",
      "Training loss for batch 7164 : 0.047872137278318405\n",
      "Training loss for batch 7165 : 0.1715901792049408\n",
      "Training loss for batch 7166 : 0.04261661693453789\n",
      "Training loss for batch 7167 : 0.08338647335767746\n",
      "Training loss for batch 7168 : 0.12839969992637634\n",
      "Training loss for batch 7169 : 0.13584810495376587\n",
      "Training loss for batch 7170 : 0.1430901139974594\n",
      "Training loss for batch 7171 : 0.15430736541748047\n",
      "Training loss for batch 7172 : 0.04065101593732834\n",
      "Training loss for batch 7173 : 0.563959538936615\n",
      "Training loss for batch 7174 : 0.21660877764225006\n",
      "Training loss for batch 7175 : 0.038329411298036575\n",
      "Training loss for batch 7176 : 0.16836397349834442\n",
      "Training loss for batch 7177 : 0.025509700179100037\n",
      "Training loss for batch 7178 : 0.2617833912372589\n",
      "Training loss for batch 7179 : 0.29455187916755676\n",
      "Training loss for batch 7180 : 0.05854276567697525\n",
      "Training loss for batch 7181 : 0.15430115163326263\n",
      "Training loss for batch 7182 : 0.012847274541854858\n",
      "Training loss for batch 7183 : 0.15601010620594025\n",
      "Training loss for batch 7184 : 0.030161263421177864\n",
      "Training loss for batch 7185 : 0.1585492640733719\n",
      "Training loss for batch 7186 : 0.04979792982339859\n",
      "Training loss for batch 7187 : 0.034466348588466644\n",
      "Training loss for batch 7188 : 0.0798296332359314\n",
      "Training loss for batch 7189 : 0.288968026638031\n",
      "Training loss for batch 7190 : 0.048933207988739014\n",
      "Training loss for batch 7191 : 0.028120320290327072\n",
      "Training loss for batch 7192 : 0.05811920389533043\n",
      "Training loss for batch 7193 : 0.26495814323425293\n",
      "Training loss for batch 7194 : 0.007883886806666851\n",
      "Training loss for batch 7195 : 0.005402678623795509\n",
      "Training loss for batch 7196 : 0.04279336333274841\n",
      "Training loss for batch 7197 : 0.08075078576803207\n",
      "Training loss for batch 7198 : 0.17830698192119598\n",
      "Training loss for batch 7199 : 0.24674706161022186\n",
      "Training loss for batch 7200 : 0.45414531230926514\n",
      "Training loss for batch 7201 : 0.015447375364601612\n",
      "Training loss for batch 7202 : 0.42569807171821594\n",
      "Training loss for batch 7203 : 0.15096913278102875\n",
      "Training loss for batch 7204 : 0.17721225321292877\n",
      "Training loss for batch 7205 : 0.09586597234010696\n",
      "Training loss for batch 7206 : 0.11074173450469971\n",
      "Training loss for batch 7207 : 0.1778978854417801\n",
      "Training loss for batch 7208 : 0.15065176784992218\n",
      "Training loss for batch 7209 : 0.46399426460266113\n",
      "Training loss for batch 7210 : 0.08551879972219467\n",
      "Training loss for batch 7211 : 0.09330499917268753\n",
      "Training loss for batch 7212 : 0.335815966129303\n",
      "Training loss for batch 7213 : 0.41970062255859375\n",
      "Training loss for batch 7214 : 0.0007020074408501387\n",
      "Training loss for batch 7215 : 0.34731170535087585\n",
      "Training loss for batch 7216 : 0.4268079102039337\n",
      "Training loss for batch 7217 : 0.4950510859489441\n",
      "Training loss for batch 7218 : 0.442989706993103\n",
      "Training loss for batch 7219 : 0.08916017413139343\n",
      "Training loss for batch 7220 : 0.047832369804382324\n",
      "Training loss for batch 7221 : 0.5348614454269409\n",
      "Training loss for batch 7222 : 0.11339911073446274\n",
      "Training loss for batch 7223 : 0.014086644165217876\n",
      "Training loss for batch 7224 : 0.29184165596961975\n",
      "Training loss for batch 7225 : 0.3609906733036041\n",
      "Training loss for batch 7226 : 0.3048267066478729\n",
      "Training loss for batch 7227 : 0.4972124993801117\n",
      "Training loss for batch 7228 : 0.26696017384529114\n",
      "Training loss for batch 7229 : 0.33238524198532104\n",
      "Training loss for batch 7230 : 0.2570800185203552\n",
      "Training loss for batch 7231 : 0.1353677213191986\n",
      "Training loss for batch 7232 : 0.0831894502043724\n",
      "Training loss for batch 7233 : 0.18482746183872223\n",
      "Training loss for batch 7234 : 0.07274527847766876\n",
      "Training loss for batch 7235 : 0.20272715389728546\n",
      "Training loss for batch 7236 : 0.1416163444519043\n",
      "Training loss for batch 7237 : 0.024995287880301476\n",
      "Training loss for batch 7238 : 0.0\n",
      "Training loss for batch 7239 : 0.053944602608680725\n",
      "Training loss for batch 7240 : 0.2243095338344574\n",
      "Training loss for batch 7241 : 0.199370339512825\n",
      "Training loss for batch 7242 : 0.003725675866007805\n",
      "Training loss for batch 7243 : 0.0681116059422493\n",
      "Training loss for batch 7244 : 0.031776413321495056\n",
      "Training loss for batch 7245 : 0.3812658488750458\n",
      "Training loss for batch 7246 : 0.32724377512931824\n",
      "Training loss for batch 7247 : 0.20999227464199066\n",
      "Training loss for batch 7248 : 0.09812904894351959\n",
      "Training loss for batch 7249 : 0.13884232938289642\n",
      "Training loss for batch 7250 : 0.25802287459373474\n",
      "Training loss for batch 7251 : 0.07129349559545517\n",
      "Training loss for batch 7252 : 0.3456439673900604\n",
      "Training loss for batch 7253 : 0.013032099232077599\n",
      "Training loss for batch 7254 : 0.19481365382671356\n",
      "Training loss for batch 7255 : 0.48349449038505554\n",
      "Training loss for batch 7256 : 0.2329283207654953\n",
      "Training loss for batch 7257 : 0.19653603434562683\n",
      "Training loss for batch 7258 : 0.18385560810565948\n",
      "Training loss for batch 7259 : 0.018106818199157715\n",
      "Training loss for batch 7260 : 0.08190911263227463\n",
      "Training loss for batch 7261 : 0.15644443035125732\n",
      "Training loss for batch 7262 : 0.31255263090133667\n",
      "Training loss for batch 7263 : 0.22447827458381653\n",
      "Training loss for batch 7264 : 0.3395492732524872\n",
      "Training loss for batch 7265 : 0.16456201672554016\n",
      "Training loss for batch 7266 : 0.30402249097824097\n",
      "Training loss for batch 7267 : 0.5405719876289368\n",
      "Training loss for batch 7268 : 0.15811732411384583\n",
      "Training loss for batch 7269 : 0.0\n",
      "Training loss for batch 7270 : 0.40856075286865234\n",
      "Training loss for batch 7271 : 0.015713732689619064\n",
      "Training loss for batch 7272 : 0.19111432135105133\n",
      "Training loss for batch 7273 : 0.33800461888313293\n",
      "Training loss for batch 7274 : 0.1844530999660492\n",
      "Training loss for batch 7275 : 0.5187764763832092\n",
      "Training loss for batch 7276 : 0.009505629539489746\n",
      "Training loss for batch 7277 : 0.20620201528072357\n",
      "Training loss for batch 7278 : 0.4492673873901367\n",
      "Training loss for batch 7279 : 0.635254979133606\n",
      "Training loss for batch 7280 : 0.46554869413375854\n",
      "Training loss for batch 7281 : 0.25136759877204895\n",
      "Training loss for batch 7282 : 0.04840933158993721\n",
      "Training loss for batch 7283 : 0.1495111882686615\n",
      "Training loss for batch 7284 : 0.30599141120910645\n",
      "Training loss for batch 7285 : 0.4479987621307373\n",
      "Training loss for batch 7286 : 0.09376983344554901\n",
      "Training loss for batch 7287 : 0.08412016928195953\n",
      "Training loss for batch 7288 : 0.12990030646324158\n",
      "Training loss for batch 7289 : 0.10124337673187256\n",
      "Training loss for batch 7290 : 0.2737687826156616\n",
      "Training loss for batch 7291 : 0.017564883455634117\n",
      "Training loss for batch 7292 : 0.23256395757198334\n",
      "Training loss for batch 7293 : 0.13250921666622162\n",
      "Training loss for batch 7294 : 0.04487070441246033\n",
      "Training loss for batch 7295 : 0.2092766910791397\n",
      "Training loss for batch 7296 : 0.10534174740314484\n",
      "Training loss for batch 7297 : 0.5325325131416321\n",
      "Training loss for batch 7298 : 0.28135818243026733\n",
      "Training loss for batch 7299 : 0.2936684787273407\n",
      "Training loss for batch 7300 : 0.10822898149490356\n",
      "Training loss for batch 7301 : 0.5200899839401245\n",
      "Training loss for batch 7302 : 0.5469847321510315\n",
      "Training loss for batch 7303 : 0.10107547044754028\n",
      "Training loss for batch 7304 : 0.11746034026145935\n",
      "Training loss for batch 7305 : 0.19656214118003845\n",
      "Training loss for batch 7306 : 0.3952130079269409\n",
      "Training loss for batch 7307 : 0.11787398904561996\n",
      "Training loss for batch 7308 : 0.4578934907913208\n",
      "Training loss for batch 7309 : 0.022559549659490585\n",
      "Training loss for batch 7310 : 0.2956660985946655\n",
      "Training loss for batch 7311 : 0.27411580085754395\n",
      "Training loss for batch 7312 : 0.19792097806930542\n",
      "Training loss for batch 7313 : 0.45976725220680237\n",
      "Training loss for batch 7314 : 0.02724061720073223\n",
      "Training loss for batch 7315 : 0.2286171019077301\n",
      "Training loss for batch 7316 : 0.06623423844575882\n",
      "Training loss for batch 7317 : 0.11277399957180023\n",
      "Training loss for batch 7318 : 0.3480682075023651\n",
      "Training loss for batch 7319 : 0.06664007157087326\n",
      "Training loss for batch 7320 : 0.3349840044975281\n",
      "Training loss for batch 7321 : 0.1532374769449234\n",
      "Training loss for batch 7322 : 0.08256730437278748\n",
      "Training loss for batch 7323 : 0.07254327833652496\n",
      "Training loss for batch 7324 : 0.49056556820869446\n",
      "Training loss for batch 7325 : 0.04154454171657562\n",
      "Training loss for batch 7326 : 0.2572341859340668\n",
      "Training loss for batch 7327 : 0.31620508432388306\n",
      "Training loss for batch 7328 : 0.006926616188138723\n",
      "Training loss for batch 7329 : 0.4021655023097992\n",
      "Training loss for batch 7330 : 0.22949770092964172\n",
      "Training loss for batch 7331 : 0.04872719198465347\n",
      "Training loss for batch 7332 : 0.08262617141008377\n",
      "Training loss for batch 7333 : 0.05289461463689804\n",
      "Training loss for batch 7334 : 0.0918852910399437\n",
      "Training loss for batch 7335 : 0.15932229161262512\n",
      "Training loss for batch 7336 : 0.048152245581150055\n",
      "Training loss for batch 7337 : 0.08509904146194458\n",
      "Training loss for batch 7338 : 0.12963902950286865\n",
      "Training loss for batch 7339 : 0.3315061628818512\n",
      "Training loss for batch 7340 : 0.28803884983062744\n",
      "Training loss for batch 7341 : 0.11230330914258957\n",
      "Training loss for batch 7342 : 0.0021007657051086426\n",
      "Training loss for batch 7343 : 0.11678595840930939\n",
      "Training loss for batch 7344 : 0.2521127164363861\n",
      "Training loss for batch 7345 : 0.18109779059886932\n",
      "Training loss for batch 7346 : 0.24919208884239197\n",
      "Training loss for batch 7347 : 0.08032327145338058\n",
      "Training loss for batch 7348 : 0.13619209825992584\n",
      "Training loss for batch 7349 : 0.0804484412074089\n",
      "Training loss for batch 7350 : 0.4023033678531647\n",
      "Training loss for batch 7351 : 0.022835731506347656\n",
      "Training loss for batch 7352 : 0.0006931027164682746\n",
      "Training loss for batch 7353 : 0.07068029046058655\n",
      "Training loss for batch 7354 : 0.23923732340335846\n",
      "Training loss for batch 7355 : 0.15398196876049042\n",
      "Training loss for batch 7356 : 0.08669473230838776\n",
      "Training loss for batch 7357 : 0.305457204580307\n",
      "Training loss for batch 7358 : 0.3292464315891266\n",
      "Training loss for batch 7359 : 0.30327674746513367\n",
      "Training loss for batch 7360 : 0.007251075468957424\n",
      "Training loss for batch 7361 : 0.5919450521469116\n",
      "Training loss for batch 7362 : 0.22991392016410828\n",
      "Training loss for batch 7363 : 0.25910526514053345\n",
      "Training loss for batch 7364 : 0.32741010189056396\n",
      "Training loss for batch 7365 : 0.1361563354730606\n",
      "Training loss for batch 7366 : 0.08200164139270782\n",
      "Training loss for batch 7367 : 0.10539956390857697\n",
      "Training loss for batch 7368 : 0.1754695177078247\n",
      "Training loss for batch 7369 : 0.0676591768860817\n",
      "Training loss for batch 7370 : 0.4982994794845581\n",
      "Training loss for batch 7371 : 0.29981377720832825\n",
      "Training loss for batch 7372 : 0.14197881519794464\n",
      "Training loss for batch 7373 : 0.0038220088463276625\n",
      "Training loss for batch 7374 : 0.08782096952199936\n",
      "Training loss for batch 7375 : 0.1668356955051422\n",
      "Training loss for batch 7376 : 0.560213565826416\n",
      "Training loss for batch 7377 : 0.07319115102291107\n",
      "Training loss for batch 7378 : 0.14066927134990692\n",
      "Training loss for batch 7379 : 0.43855908513069153\n",
      "Training loss for batch 7380 : 0.36093685030937195\n",
      "Training loss for batch 7381 : 0.5605785250663757\n",
      "Training loss for batch 7382 : 0.08166386187076569\n",
      "Training loss for batch 7383 : 0.17859409749507904\n",
      "Training loss for batch 7384 : 0.07920243591070175\n",
      "Training loss for batch 7385 : 0.19770126044750214\n",
      "Training loss for batch 7386 : 0.6132528185844421\n",
      "Training loss for batch 7387 : 0.055306486785411835\n",
      "Training loss for batch 7388 : 0.049639247357845306\n",
      "Training loss for batch 7389 : 0.1703619360923767\n",
      "Training loss for batch 7390 : 0.04824734479188919\n",
      "Training loss for batch 7391 : 0.11620282381772995\n",
      "Training loss for batch 7392 : 0.12686364352703094\n",
      "Training loss for batch 7393 : 0.08682094514369965\n",
      "Training loss for batch 7394 : 0.2697948217391968\n",
      "Training loss for batch 7395 : 0.26421085000038147\n",
      "Training loss for batch 7396 : 0.23030385375022888\n",
      "Training loss for batch 7397 : 0.3034147024154663\n",
      "Training loss for batch 7398 : 0.642855703830719\n",
      "Training loss for batch 7399 : 0.19256189465522766\n",
      "Training loss for batch 7400 : 0.11916234344244003\n",
      "Training loss for batch 7401 : 0.16088683903217316\n",
      "Training loss for batch 7402 : 0.047249387949705124\n",
      "Training loss for batch 7403 : 0.10807832330465317\n",
      "Training loss for batch 7404 : 0.6061052083969116\n",
      "Training loss for batch 7405 : 0.06113334745168686\n",
      "Training loss for batch 7406 : 0.05108986422419548\n",
      "Training loss for batch 7407 : 0.08155997842550278\n",
      "Training loss for batch 7408 : 0.31254175305366516\n",
      "Training loss for batch 7409 : 0.34576088190078735\n",
      "Training loss for batch 7410 : 0.10111956298351288\n",
      "Training loss for batch 7411 : 0.25464197993278503\n",
      "Training loss for batch 7412 : 0.05166434869170189\n",
      "Training loss for batch 7413 : 0.16016991436481476\n",
      "Training loss for batch 7414 : 0.0\n",
      "Training loss for batch 7415 : 0.25023332238197327\n",
      "Training loss for batch 7416 : 0.1692752093076706\n",
      "Training loss for batch 7417 : 0.12019997835159302\n",
      "Training loss for batch 7418 : 0.024473290890455246\n",
      "Training loss for batch 7419 : 0.09646373987197876\n",
      "Training loss for batch 7420 : 0.15153071284294128\n",
      "Training loss for batch 7421 : 0.058499984443187714\n",
      "Training loss for batch 7422 : 0.09185231477022171\n",
      "Training loss for batch 7423 : 0.2512509822845459\n",
      "Training loss for batch 7424 : 0.03304329887032509\n",
      "Training loss for batch 7425 : 0.09324892610311508\n",
      "Training loss for batch 7426 : 0.28287240862846375\n",
      "Training loss for batch 7427 : 0.14370261132717133\n",
      "Training loss for batch 7428 : 0.19623030722141266\n",
      "Training loss for batch 7429 : 0.0821647047996521\n",
      "Training loss for batch 7430 : 0.052261512726545334\n",
      "Training loss for batch 7431 : 0.14641711115837097\n",
      "Training loss for batch 7432 : 0.138194277882576\n",
      "Training loss for batch 7433 : 0.5255733728408813\n",
      "Training loss for batch 7434 : 0.04674825072288513\n",
      "Training loss for batch 7435 : 0.19964465498924255\n",
      "Training loss for batch 7436 : 0.20679476857185364\n",
      "Training loss for batch 7437 : 0.19079162180423737\n",
      "Training loss for batch 7438 : 0.06296542286872864\n",
      "Training loss for batch 7439 : 0.4209648072719574\n",
      "Training loss for batch 7440 : 0.2622348666191101\n",
      "Training loss for batch 7441 : 0.11131792515516281\n",
      "Training loss for batch 7442 : 0.359172523021698\n",
      "Training loss for batch 7443 : 0.17556516826152802\n",
      "Training loss for batch 7444 : 0.45345398783683777\n",
      "Training loss for batch 7445 : 0.14235363900661469\n",
      "Training loss for batch 7446 : 0.15938669443130493\n",
      "Training loss for batch 7447 : 0.18168313801288605\n",
      "Training loss for batch 7448 : 0.06345897912979126\n",
      "Training loss for batch 7449 : 0.25400468707084656\n",
      "Training loss for batch 7450 : 0.07776720821857452\n",
      "Training loss for batch 7451 : 0.09364152699708939\n",
      "Training loss for batch 7452 : 0.39220187067985535\n",
      "Training loss for batch 7453 : 0.07920455187559128\n",
      "Training loss for batch 7454 : 0.06128735467791557\n",
      "Training loss for batch 7455 : 0.15025551617145538\n",
      "Training loss for batch 7456 : 0.07465647161006927\n",
      "Training loss for batch 7457 : 0.2396409809589386\n",
      "Training loss for batch 7458 : 0.10638441890478134\n",
      "Training loss for batch 7459 : 0.25594034790992737\n",
      "Training loss for batch 7460 : 0.08853067457675934\n",
      "Training loss for batch 7461 : 0.10763930529356003\n",
      "Training loss for batch 7462 : 0.29805871844291687\n",
      "Training loss for batch 7463 : 0.22229096293449402\n",
      "Training loss for batch 7464 : 0.19551904499530792\n",
      "Training loss for batch 7465 : 0.0039360783994197845\n",
      "Training loss for batch 7466 : 0.5098522901535034\n",
      "Training loss for batch 7467 : 0.06687333434820175\n",
      "Training loss for batch 7468 : 0.12367232888936996\n",
      "Training loss for batch 7469 : 0.25940024852752686\n",
      "Training loss for batch 7470 : 0.40549030900001526\n",
      "Training loss for batch 7471 : 0.2308921217918396\n",
      "Training loss for batch 7472 : 0.16863305866718292\n",
      "Training loss for batch 7473 : 0.24781180918216705\n",
      "Training loss for batch 7474 : 0.20311503112316132\n",
      "Training loss for batch 7475 : 0.08648223429918289\n",
      "Training loss for batch 7476 : 0.18618614971637726\n",
      "Training loss for batch 7477 : 0.26496896147727966\n",
      "Training loss for batch 7478 : 0.5725172758102417\n",
      "Training loss for batch 7479 : 0.2004891186952591\n",
      "Training loss for batch 7480 : 0.3845345973968506\n",
      "Training loss for batch 7481 : 0.06933882087469101\n",
      "Training loss for batch 7482 : 0.25159162282943726\n",
      "Training loss for batch 7483 : 0.12667758762836456\n",
      "Training loss for batch 7484 : 0.5266361236572266\n",
      "Training loss for batch 7485 : 0.13301746547222137\n",
      "Training loss for batch 7486 : 0.36291804909706116\n",
      "Training loss for batch 7487 : 0.8210141062736511\n",
      "Training loss for batch 7488 : 0.41895678639411926\n",
      "Training loss for batch 7489 : 0.011034568771719933\n",
      "Training loss for batch 7490 : 0.2503173053264618\n",
      "Training loss for batch 7491 : 0.20204314589500427\n",
      "Training loss for batch 7492 : 0.06065131723880768\n",
      "Training loss for batch 7493 : 0.14399391412734985\n",
      "Training loss for batch 7494 : 0.05337922275066376\n",
      "Training loss for batch 7495 : 0.2259555459022522\n",
      "Training loss for batch 7496 : 0.3468107283115387\n",
      "Training loss for batch 7497 : 0.5519722700119019\n",
      "Training loss for batch 7498 : 0.15833598375320435\n",
      "Training loss for batch 7499 : 0.021608997136354446\n",
      "Training loss for batch 7500 : 0.10811588168144226\n",
      "Training loss for batch 7501 : 0.44987836480140686\n",
      "Training loss for batch 7502 : 0.36062031984329224\n",
      "Training loss for batch 7503 : 0.1664176881313324\n",
      "Training loss for batch 7504 : 0.09262336045503616\n",
      "Training loss for batch 7505 : 0.05472269281744957\n",
      "Training loss for batch 7506 : 0.04656925052404404\n",
      "Training loss for batch 7507 : 0.3223603367805481\n",
      "Training loss for batch 7508 : 0.025768868625164032\n",
      "Training loss for batch 7509 : 0.13080252707004547\n",
      "Training loss for batch 7510 : 0.2815546989440918\n",
      "Training loss for batch 7511 : 0.2689993977546692\n",
      "Training loss for batch 7512 : 0.2221546769142151\n",
      "Training loss for batch 7513 : 0.44213011860847473\n",
      "Training loss for batch 7514 : 0.010146870277822018\n",
      "Training loss for batch 7515 : 0.4139624834060669\n",
      "Training loss for batch 7516 : 0.6238173842430115\n",
      "Training loss for batch 7517 : 0.5762987732887268\n",
      "Training loss for batch 7518 : 0.593937873840332\n",
      "Training loss for batch 7519 : 0.20939289033412933\n",
      "Training loss for batch 7520 : 0.11474429816007614\n",
      "Training loss for batch 7521 : 0.17995496094226837\n",
      "Training loss for batch 7522 : 0.314116895198822\n",
      "Training loss for batch 7523 : 0.005182038061320782\n",
      "Training loss for batch 7524 : 0.0699160248041153\n",
      "Training loss for batch 7525 : 0.3189689517021179\n",
      "Training loss for batch 7526 : 0.24212613701820374\n",
      "Training loss for batch 7527 : 0.5233964920043945\n",
      "Training loss for batch 7528 : 0.07697303593158722\n",
      "Training loss for batch 7529 : 0.02799236588180065\n",
      "Training loss for batch 7530 : 0.6623502373695374\n",
      "Training loss for batch 7531 : 0.12168151885271072\n",
      "Training loss for batch 7532 : 0.15621890127658844\n",
      "Training loss for batch 7533 : 0.27912071347236633\n",
      "Training loss for batch 7534 : 0.48214760422706604\n",
      "Training loss for batch 7535 : 0.15459971129894257\n",
      "Training loss for batch 7536 : 0.3005034625530243\n",
      "Training loss for batch 7537 : 0.18599584698677063\n",
      "Training loss for batch 7538 : 0.41441911458969116\n",
      "Training loss for batch 7539 : 0.3102138042449951\n",
      "Training loss for batch 7540 : 0.2762991189956665\n",
      "Training loss for batch 7541 : 0.15742714703083038\n",
      "Training loss for batch 7542 : 0.5210009217262268\n",
      "Training loss for batch 7543 : 0.19420523941516876\n",
      "Training loss for batch 7544 : 0.09748023003339767\n",
      "Training loss for batch 7545 : 0.17806950211524963\n",
      "Training loss for batch 7546 : 0.3481557071208954\n",
      "Training loss for batch 7547 : 0.003991964738816023\n",
      "Training loss for batch 7548 : 0.23809635639190674\n",
      "Training loss for batch 7549 : 0.007792198099195957\n",
      "Training loss for batch 7550 : 0.17511625587940216\n",
      "Training loss for batch 7551 : 0.2745664417743683\n",
      "Training loss for batch 7552 : 0.1526753008365631\n",
      "Training loss for batch 7553 : 0.2180762141942978\n",
      "Training loss for batch 7554 : 0.025404067710042\n",
      "Training loss for batch 7555 : 0.10659167915582657\n",
      "Training loss for batch 7556 : 0.38473016023635864\n",
      "Training loss for batch 7557 : 0.01417387556284666\n",
      "Training loss for batch 7558 : 0.3106421232223511\n",
      "Training loss for batch 7559 : 0.5199806094169617\n",
      "Training loss for batch 7560 : 0.061795759946107864\n",
      "Training loss for batch 7561 : 0.1340247243642807\n",
      "Training loss for batch 7562 : 0.1245044618844986\n",
      "Training loss for batch 7563 : 0.1619175672531128\n",
      "Training loss for batch 7564 : 0.23125813901424408\n",
      "Training loss for batch 7565 : 0.2871202528476715\n",
      "Training loss for batch 7566 : 0.1031154990196228\n",
      "Training loss for batch 7567 : 0.7142810821533203\n",
      "Training loss for batch 7568 : 0.4487970173358917\n",
      "Training loss for batch 7569 : 0.2227495163679123\n",
      "Training loss for batch 7570 : 0.5210531949996948\n",
      "Training loss for batch 7571 : 0.05596363916993141\n",
      "Training loss for batch 7572 : 0.06541900336742401\n",
      "Training loss for batch 7573 : 0.163718119263649\n",
      "Training loss for batch 7574 : 0.2660832107067108\n",
      "Training loss for batch 7575 : 0.03808245062828064\n",
      "Training loss for batch 7576 : 0.6062079071998596\n",
      "Training loss for batch 7577 : 0.5553277730941772\n",
      "Training loss for batch 7578 : 0.37702837586402893\n",
      "Training loss for batch 7579 : 0.016263118013739586\n",
      "Training loss for batch 7580 : 0.027855634689331055\n",
      "Training loss for batch 7581 : 0.6611202359199524\n",
      "Training loss for batch 7582 : 0.1452237069606781\n",
      "Training loss for batch 7583 : 0.26508599519729614\n",
      "Training loss for batch 7584 : 0.40206050872802734\n",
      "Training loss for batch 7585 : 0.08562054485082626\n",
      "Training loss for batch 7586 : 0.6073130369186401\n",
      "Training loss for batch 7587 : 0.3907502591609955\n",
      "Training loss for batch 7588 : 0.37094300985336304\n",
      "Training loss for batch 7589 : 0.3220912516117096\n",
      "Training loss for batch 7590 : 0.2994026243686676\n",
      "Training loss for batch 7591 : 0.15587358176708221\n",
      "Training loss for batch 7592 : 0.4375779628753662\n",
      "Training loss for batch 7593 : 0.22442370653152466\n",
      "Training loss for batch 7594 : 0.027979791164398193\n",
      "Training loss for batch 7595 : 0.10128480195999146\n",
      "Training loss for batch 7596 : 0.09532763063907623\n",
      "Training loss for batch 7597 : 0.26431965827941895\n",
      "Training loss for batch 7598 : 0.17520615458488464\n",
      "Training loss for batch 7599 : 0.05163578316569328\n",
      "Training loss for batch 7600 : 0.12710623443126678\n",
      "Training loss for batch 7601 : 0.26260465383529663\n",
      "Training loss for batch 7602 : 0.14613138139247894\n",
      "Training loss for batch 7603 : 0.32661256194114685\n",
      "Training loss for batch 7604 : 0.06691257655620575\n",
      "Training loss for batch 7605 : 0.14676713943481445\n",
      "Training loss for batch 7606 : 0.2517111003398895\n",
      "Training loss for batch 7607 : 0.11060234159231186\n",
      "Training loss for batch 7608 : 0.22405223548412323\n",
      "Training loss for batch 7609 : 0.06532374024391174\n",
      "Training loss for batch 7610 : 0.011983318254351616\n",
      "Training loss for batch 7611 : 0.13118739426136017\n",
      "Training loss for batch 7612 : 0.17424160242080688\n",
      "Training loss for batch 7613 : 0.41973677277565\n",
      "Training loss for batch 7614 : 0.16911017894744873\n",
      "Training loss for batch 7615 : 0.23814252018928528\n",
      "Training loss for batch 7616 : 0.0364612452685833\n",
      "Training loss for batch 7617 : 0.10709545016288757\n",
      "Training loss for batch 7618 : 0.17641562223434448\n",
      "Training loss for batch 7619 : 0.15756604075431824\n",
      "Training loss for batch 7620 : 0.10350185632705688\n",
      "Training loss for batch 7621 : 0.48460251092910767\n",
      "Training loss for batch 7622 : 0.12165071070194244\n",
      "Training loss for batch 7623 : 0.04095495119690895\n",
      "Training loss for batch 7624 : 0.26707664132118225\n",
      "Training loss for batch 7625 : 0.34198614954948425\n",
      "Training loss for batch 7626 : 0.08671640604734421\n",
      "Training loss for batch 7627 : 0.47837793827056885\n",
      "Training loss for batch 7628 : 0.18594998121261597\n",
      "Training loss for batch 7629 : 0.23104512691497803\n",
      "Training loss for batch 7630 : 0.377058744430542\n",
      "Training loss for batch 7631 : 0.33177098631858826\n",
      "Training loss for batch 7632 : 0.0002983510494232178\n",
      "Training loss for batch 7633 : 0.23042653501033783\n",
      "Training loss for batch 7634 : 0.2862236797809601\n",
      "Training loss for batch 7635 : 0.0\n",
      "Training loss for batch 7636 : 0.31256791949272156\n",
      "Training loss for batch 7637 : 0.25378328561782837\n",
      "Training loss for batch 7638 : 0.06251414120197296\n",
      "Training loss for batch 7639 : 0.15291747450828552\n",
      "Training loss for batch 7640 : 0.27099987864494324\n",
      "Training loss for batch 7641 : 0.12861114740371704\n",
      "Training loss for batch 7642 : 0.35329627990722656\n",
      "Training loss for batch 7643 : 0.12264522165060043\n",
      "Training loss for batch 7644 : 0.0\n",
      "Training loss for batch 7645 : 0.052549682557582855\n",
      "Training loss for batch 7646 : 0.05861440673470497\n",
      "Training loss for batch 7647 : 0.09573303908109665\n",
      "Training loss for batch 7648 : 0.44387972354888916\n",
      "Training loss for batch 7649 : 0.5897852778434753\n",
      "Training loss for batch 7650 : 0.1473693549633026\n",
      "Training loss for batch 7651 : 0.19519440829753876\n",
      "Training loss for batch 7652 : 0.07244469970464706\n",
      "Training loss for batch 7653 : 0.10946653038263321\n",
      "Training loss for batch 7654 : 0.09852910786867142\n",
      "Training loss for batch 7655 : 0.12257326394319534\n",
      "Training loss for batch 7656 : 0.15138420462608337\n",
      "Training loss for batch 7657 : 0.13076786696910858\n",
      "Training loss for batch 7658 : 0.3743742108345032\n",
      "Training loss for batch 7659 : 0.03941461443901062\n",
      "Training loss for batch 7660 : 0.07989632338285446\n",
      "Training loss for batch 7661 : 0.2450469583272934\n",
      "Training loss for batch 7662 : 0.2204100340604782\n",
      "Training loss for batch 7663 : 0.13108453154563904\n",
      "Training loss for batch 7664 : 0.1911504715681076\n",
      "Training loss for batch 7665 : 0.262481153011322\n",
      "Training loss for batch 7666 : 0.38435524702072144\n",
      "Training loss for batch 7667 : 0.14508575201034546\n",
      "Training loss for batch 7668 : 0.5759892463684082\n",
      "Training loss for batch 7669 : 0.2154942899942398\n",
      "Training loss for batch 7670 : 0.0\n",
      "Training loss for batch 7671 : 0.05080351606011391\n",
      "Training loss for batch 7672 : 0.7381227612495422\n",
      "Training loss for batch 7673 : 0.46985873579978943\n",
      "Training loss for batch 7674 : 0.12570656836032867\n",
      "Training loss for batch 7675 : 0.2944035828113556\n",
      "Training loss for batch 7676 : 0.3432631194591522\n",
      "Training loss for batch 7677 : 0.07051960378885269\n",
      "Training loss for batch 7678 : 0.19996193051338196\n",
      "Training loss for batch 7679 : 0.30675554275512695\n",
      "Training loss for batch 7680 : 0.15488402545452118\n",
      "Training loss for batch 7681 : 0.42528051137924194\n",
      "Training loss for batch 7682 : 0.0482814647257328\n",
      "Training loss for batch 7683 : 0.13586285710334778\n",
      "Training loss for batch 7684 : 0.04950357601046562\n",
      "Training loss for batch 7685 : 0.39347290992736816\n",
      "Training loss for batch 7686 : 0.21906697750091553\n",
      "Training loss for batch 7687 : 0.3323817253112793\n",
      "Training loss for batch 7688 : 0.29486510157585144\n",
      "Training loss for batch 7689 : 0.3831941783428192\n",
      "Training loss for batch 7690 : 0.080690898001194\n",
      "Training loss for batch 7691 : 0.06228489801287651\n",
      "Training loss for batch 7692 : 0.14342623949050903\n",
      "Training loss for batch 7693 : 0.41918864846229553\n",
      "Training loss for batch 7694 : 0.27089861035346985\n",
      "Training loss for batch 7695 : 0.33869558572769165\n",
      "Training loss for batch 7696 : 0.046784091740846634\n",
      "Training loss for batch 7697 : 0.14150449633598328\n",
      "Training loss for batch 7698 : 0.3031769096851349\n",
      "Training loss for batch 7699 : 0.3263779878616333\n",
      "Training loss for batch 7700 : 0.4412643015384674\n",
      "Training loss for batch 7701 : 0.05621708184480667\n",
      "Training loss for batch 7702 : 0.12974895536899567\n",
      "Training loss for batch 7703 : 0.00208714185282588\n",
      "Training loss for batch 7704 : 0.1595010906457901\n",
      "Training loss for batch 7705 : 0.10963036864995956\n",
      "Training loss for batch 7706 : 0.15381407737731934\n",
      "Training loss for batch 7707 : 0.3290800452232361\n",
      "Training loss for batch 7708 : 0.17019499838352203\n",
      "Training loss for batch 7709 : 0.3169732093811035\n",
      "Training loss for batch 7710 : 0.007845651358366013\n",
      "Training loss for batch 7711 : 0.00032421137439087033\n",
      "Training loss for batch 7712 : 0.4601841866970062\n",
      "Training loss for batch 7713 : 0.28779277205467224\n",
      "Training loss for batch 7714 : 0.039366211742162704\n",
      "Training loss for batch 7715 : 0.2862977087497711\n",
      "Training loss for batch 7716 : 0.13833695650100708\n",
      "Training loss for batch 7717 : 0.06515376269817352\n",
      "Training loss for batch 7718 : 0.3816486597061157\n",
      "Training loss for batch 7719 : 0.10011982172727585\n",
      "Training loss for batch 7720 : 0.11340349912643433\n",
      "Training loss for batch 7721 : 0.19841882586479187\n",
      "Training loss for batch 7722 : 0.10232903063297272\n",
      "Training loss for batch 7723 : 0.11215796321630478\n",
      "Training loss for batch 7724 : 0.060352813452482224\n",
      "Training loss for batch 7725 : 0.4029857814311981\n",
      "Training loss for batch 7726 : 0.2928653657436371\n",
      "Training loss for batch 7727 : 0.12352420389652252\n",
      "Training loss for batch 7728 : 0.07623063772916794\n",
      "Training loss for batch 7729 : 0.19312363862991333\n",
      "Training loss for batch 7730 : 0.15590281784534454\n",
      "Training loss for batch 7731 : 0.06219739839434624\n",
      "Training loss for batch 7732 : 0.024403033778071404\n",
      "Training loss for batch 7733 : 0.1073916032910347\n",
      "Training loss for batch 7734 : 0.06750861555337906\n",
      "Training loss for batch 7735 : 0.5214139819145203\n",
      "Training loss for batch 7736 : 0.1914711445569992\n",
      "Training loss for batch 7737 : 0.09568988531827927\n",
      "Training loss for batch 7738 : 0.08215699344873428\n",
      "Training loss for batch 7739 : 0.04704049974679947\n",
      "Training loss for batch 7740 : 0.17441797256469727\n",
      "Training loss for batch 7741 : 0.289664626121521\n",
      "Training loss for batch 7742 : 0.12198440730571747\n",
      "Training loss for batch 7743 : 0.6725273728370667\n",
      "Training loss for batch 7744 : 0.24466802179813385\n",
      "Training loss for batch 7745 : 0.08407267928123474\n",
      "Training loss for batch 7746 : 0.2136998325586319\n",
      "Training loss for batch 7747 : 0.37652453780174255\n",
      "Training loss for batch 7748 : 0.3836328089237213\n",
      "Training loss for batch 7749 : 0.11962177604436874\n",
      "Training loss for batch 7750 : 0.00319308671168983\n",
      "Training loss for batch 7751 : 0.07267384976148605\n",
      "Training loss for batch 7752 : 0.5617115497589111\n",
      "Training loss for batch 7753 : 1.0807064771652222\n",
      "Training loss for batch 7754 : 0.13158445060253143\n",
      "Training loss for batch 7755 : 0.3460277318954468\n",
      "Training loss for batch 7756 : 0.24544039368629456\n",
      "Training loss for batch 7757 : 0.11413032561540604\n",
      "Training loss for batch 7758 : 0.13514204323291779\n",
      "Training loss for batch 7759 : 0.0\n",
      "Training loss for batch 7760 : 0.43338221311569214\n",
      "Training loss for batch 7761 : 0.04719169810414314\n",
      "Training loss for batch 7762 : 0.47486236691474915\n",
      "Training loss for batch 7763 : 0.6305349469184875\n",
      "Training loss for batch 7764 : 0.028732826933264732\n",
      "Training loss for batch 7765 : 0.04753644019365311\n",
      "Training loss for batch 7766 : 0.016571786254644394\n",
      "Training loss for batch 7767 : 0.2485569566488266\n",
      "Training loss for batch 7768 : 0.3873942792415619\n",
      "Training loss for batch 7769 : 0.26597365736961365\n",
      "Training loss for batch 7770 : 0.04334273561835289\n",
      "Training loss for batch 7771 : 0.1656404733657837\n",
      "Training loss for batch 7772 : 0.4376656711101532\n",
      "Training loss for batch 7773 : 0.1712799221277237\n",
      "Training loss for batch 7774 : 0.011781645007431507\n",
      "Training loss for batch 7775 : 0.46033889055252075\n",
      "Training loss for batch 7776 : 0.03598375990986824\n",
      "Training loss for batch 7777 : 0.25075167417526245\n",
      "Training loss for batch 7778 : 0.226377472281456\n",
      "Training loss for batch 7779 : 0.4351479709148407\n",
      "Training loss for batch 7780 : 0.5312942862510681\n",
      "Training loss for batch 7781 : 0.10814014822244644\n",
      "Training loss for batch 7782 : 0.3336276113986969\n",
      "Training loss for batch 7783 : 0.05111762881278992\n",
      "Training loss for batch 7784 : 0.2585828900337219\n",
      "Training loss for batch 7785 : 0.30304205417633057\n",
      "Training loss for batch 7786 : 0.0015027234330773354\n",
      "Training loss for batch 7787 : 0.18454641103744507\n",
      "Training loss for batch 7788 : 0.13717706501483917\n",
      "Training loss for batch 7789 : 0.06862259656190872\n",
      "Training loss for batch 7790 : 0.37623482942581177\n",
      "Training loss for batch 7791 : 0.10486240684986115\n",
      "Training loss for batch 7792 : 0.10593076050281525\n",
      "Training loss for batch 7793 : 0.2520706355571747\n",
      "Training loss for batch 7794 : 0.025063473731279373\n",
      "Training loss for batch 7795 : 0.14021484553813934\n",
      "Training loss for batch 7796 : 0.04484586790204048\n",
      "Training loss for batch 7797 : 0.12293712794780731\n",
      "Training loss for batch 7798 : 0.3478549122810364\n",
      "Training loss for batch 7799 : 0.6075549125671387\n",
      "Training loss for batch 7800 : 0.3697017431259155\n",
      "Training loss for batch 7801 : 0.009706730954349041\n",
      "Training loss for batch 7802 : 0.16544277966022491\n",
      "Training loss for batch 7803 : 0.19723361730575562\n",
      "Training loss for batch 7804 : 0.1999005824327469\n",
      "Training loss for batch 7805 : 0.30888330936431885\n",
      "Training loss for batch 7806 : 0.02401105687022209\n",
      "Training loss for batch 7807 : 0.2571786344051361\n",
      "Training loss for batch 7808 : 0.0711170956492424\n",
      "Training loss for batch 7809 : 0.011889209970831871\n",
      "Training loss for batch 7810 : 0.08319498598575592\n",
      "Training loss for batch 7811 : 0.06652359664440155\n",
      "Training loss for batch 7812 : 0.08815963566303253\n",
      "Training loss for batch 7813 : 0.059212565422058105\n",
      "Training loss for batch 7814 : 0.1024218499660492\n",
      "Training loss for batch 7815 : 0.2211158126592636\n",
      "Training loss for batch 7816 : 0.056471873074769974\n",
      "Training loss for batch 7817 : 0.10354604572057724\n",
      "Training loss for batch 7818 : 0.11407850682735443\n",
      "Training loss for batch 7819 : 0.26373758912086487\n",
      "Training loss for batch 7820 : 0.214987650513649\n",
      "Training loss for batch 7821 : 0.410912424325943\n",
      "Training loss for batch 7822 : 0.028500501066446304\n",
      "Training loss for batch 7823 : 0.2593597173690796\n",
      "Training loss for batch 7824 : 0.06799060106277466\n",
      "Training loss for batch 7825 : 0.29979637265205383\n",
      "Training loss for batch 7826 : 0.059518299996852875\n",
      "Training loss for batch 7827 : 0.08846282958984375\n",
      "Training loss for batch 7828 : 0.3489883244037628\n",
      "Training loss for batch 7829 : 0.14861366152763367\n",
      "Training loss for batch 7830 : 0.26058247685432434\n",
      "Training loss for batch 7831 : 0.14813485741615295\n",
      "Training loss for batch 7832 : 0.2428813874721527\n",
      "Training loss for batch 7833 : 0.5422583818435669\n",
      "Training loss for batch 7834 : 0.13222892582416534\n",
      "Training loss for batch 7835 : 0.05858765169978142\n",
      "Training loss for batch 7836 : 0.4848434627056122\n",
      "Training loss for batch 7837 : 0.0877419114112854\n",
      "Training loss for batch 7838 : 0.04591618478298187\n",
      "Training loss for batch 7839 : 0.13262678682804108\n",
      "Training loss for batch 7840 : 0.30587950348854065\n",
      "Training loss for batch 7841 : 0.3946518301963806\n",
      "Training loss for batch 7842 : 0.1844872385263443\n",
      "Training loss for batch 7843 : 0.17933127284049988\n",
      "Training loss for batch 7844 : 0.10559029877185822\n",
      "Training loss for batch 7845 : 0.11991823464632034\n",
      "Training loss for batch 7846 : 0.08997439593076706\n",
      "Training loss for batch 7847 : 0.18397971987724304\n",
      "Training loss for batch 7848 : 0.3261416554450989\n",
      "Training loss for batch 7849 : 0.1675611287355423\n",
      "Training loss for batch 7850 : 0.4318307638168335\n",
      "Training loss for batch 7851 : 0.11833669245243073\n",
      "Training loss for batch 7852 : 0.09982278943061829\n",
      "Training loss for batch 7853 : 0.059699416160583496\n",
      "Training loss for batch 7854 : 0.12858366966247559\n",
      "Training loss for batch 7855 : 0.6373704075813293\n",
      "Training loss for batch 7856 : 0.05894380062818527\n",
      "Training loss for batch 7857 : 0.2927127778530121\n",
      "Training loss for batch 7858 : 0.46120479702949524\n",
      "Training loss for batch 7859 : 0.36985599994659424\n",
      "Training loss for batch 7860 : 0.019235095009207726\n",
      "Training loss for batch 7861 : 0.21358470618724823\n",
      "Training loss for batch 7862 : 0.07430067658424377\n",
      "Training loss for batch 7863 : 0.6628992557525635\n",
      "Training loss for batch 7864 : 0.01005970872938633\n",
      "Training loss for batch 7865 : 0.19852599501609802\n",
      "Training loss for batch 7866 : 0.37573811411857605\n",
      "Training loss for batch 7867 : 0.268144816160202\n",
      "Training loss for batch 7868 : 0.10925876349210739\n",
      "Training loss for batch 7869 : 0.4017973244190216\n",
      "Training loss for batch 7870 : 0.6959265470504761\n",
      "Training loss for batch 7871 : 0.3157290816307068\n",
      "Training loss for batch 7872 : 0.06329368054866791\n",
      "Training loss for batch 7873 : 0.22092141211032867\n",
      "Training loss for batch 7874 : 0.1394941210746765\n",
      "Training loss for batch 7875 : 0.33678388595581055\n",
      "Training loss for batch 7876 : 0.33991995453834534\n",
      "Training loss for batch 7877 : 0.05028510093688965\n",
      "Training loss for batch 7878 : 0.6101358532905579\n",
      "Training loss for batch 7879 : 0.6069008708000183\n",
      "Training loss for batch 7880 : 0.40161094069480896\n",
      "Training loss for batch 7881 : 0.42385903000831604\n",
      "Training loss for batch 7882 : 0.27012404799461365\n",
      "Training loss for batch 7883 : 0.2944589853286743\n",
      "Training loss for batch 7884 : 0.06125393882393837\n",
      "Training loss for batch 7885 : 0.07608360052108765\n",
      "Training loss for batch 7886 : 0.34726274013519287\n",
      "Training loss for batch 7887 : 0.05864420160651207\n",
      "Training loss for batch 7888 : 0.09280514717102051\n",
      "Training loss for batch 7889 : 0.1634599268436432\n",
      "Training loss for batch 7890 : 0.25629866123199463\n",
      "Training loss for batch 7891 : 0.09499341249465942\n",
      "Training loss for batch 7892 : 0.1979977786540985\n",
      "Training loss for batch 7893 : 0.2312648743391037\n",
      "Training loss for batch 7894 : 0.40191614627838135\n",
      "Training loss for batch 7895 : 0.04346088692545891\n",
      "Training loss for batch 7896 : 0.572291374206543\n",
      "Training loss for batch 7897 : 0.22276702523231506\n",
      "Training loss for batch 7898 : 0.02895391546189785\n",
      "Training loss for batch 7899 : 0.2647683620452881\n",
      "Training loss for batch 7900 : 0.3726387023925781\n",
      "Training loss for batch 7901 : 0.1925155371427536\n",
      "Training loss for batch 7902 : 0.19165877997875214\n",
      "Training loss for batch 7903 : 0.23674583435058594\n",
      "Training loss for batch 7904 : 0.2105131298303604\n",
      "Training loss for batch 7905 : 0.27174556255340576\n",
      "Training loss for batch 7906 : 0.22221210598945618\n",
      "Training loss for batch 7907 : 0.22568675875663757\n",
      "Training loss for batch 7908 : 0.031292468309402466\n",
      "Training loss for batch 7909 : 0.29826173186302185\n",
      "Training loss for batch 7910 : 0.3378089964389801\n",
      "Training loss for batch 7911 : 0.3288569450378418\n",
      "Training loss for batch 7912 : 0.03767424821853638\n",
      "Training loss for batch 7913 : 0.03137173876166344\n",
      "Training loss for batch 7914 : 0.43282055854797363\n",
      "Training loss for batch 7915 : 0.2780972123146057\n",
      "Training loss for batch 7916 : 0.40273988246917725\n",
      "Training loss for batch 7917 : 0.27277666330337524\n",
      "Training loss for batch 7918 : 0.23418359458446503\n",
      "Training loss for batch 7919 : 0.5525791645050049\n",
      "Training loss for batch 7920 : 0.3202545940876007\n",
      "Training loss for batch 7921 : 0.16777624189853668\n",
      "Training loss for batch 7922 : 0.4344083070755005\n",
      "Training loss for batch 7923 : 0.849339485168457\n",
      "Training loss for batch 7924 : 0.44039344787597656\n",
      "Training loss for batch 7925 : 0.020181506872177124\n",
      "Training loss for batch 7926 : 0.008774269372224808\n",
      "Training loss for batch 7927 : 0.11828496307134628\n",
      "Training loss for batch 7928 : 0.1805557906627655\n",
      "Training loss for batch 7929 : 0.3229474425315857\n",
      "Training loss for batch 7930 : 0.18736064434051514\n",
      "Training loss for batch 7931 : 0.09599172323942184\n",
      "Training loss for batch 7932 : 0.12144549936056137\n",
      "Training loss for batch 7933 : 0.1962537169456482\n",
      "Training loss for batch 7934 : 0.09311466664075851\n",
      "Training loss for batch 7935 : 0.1760750263929367\n",
      "Training loss for batch 7936 : 0.061878353357315063\n",
      "Training loss for batch 7937 : 0.06326643377542496\n",
      "Training loss for batch 7938 : 0.12490223348140717\n",
      "Training loss for batch 7939 : 0.10089116543531418\n",
      "Training loss for batch 7940 : 0.28729504346847534\n",
      "Training loss for batch 7941 : 0.01085691712796688\n",
      "Training loss for batch 7942 : 0.34581539034843445\n",
      "Training loss for batch 7943 : 0.15224812924861908\n",
      "Training loss for batch 7944 : 0.29310524463653564\n",
      "Training loss for batch 7945 : 0.2608862519264221\n",
      "Training loss for batch 7946 : 0.2522664964199066\n",
      "Training loss for batch 7947 : 0.17942897975444794\n",
      "Training loss for batch 7948 : 0.41545790433883667\n",
      "Training loss for batch 7949 : 0.1377042979001999\n",
      "Training loss for batch 7950 : 0.08334246277809143\n",
      "Training loss for batch 7951 : 0.26428353786468506\n",
      "Training loss for batch 7952 : 0.2788238823413849\n",
      "Training loss for batch 7953 : 0.1484774649143219\n",
      "Training loss for batch 7954 : 0.24738642573356628\n",
      "Training loss for batch 7955 : 0.2007519006729126\n",
      "Training loss for batch 7956 : 0.39386194944381714\n",
      "Training loss for batch 7957 : 0.48653435707092285\n",
      "Training loss for batch 7958 : 0.07280874997377396\n",
      "Training loss for batch 7959 : 0.10470255464315414\n",
      "Training loss for batch 7960 : 0.1381273865699768\n",
      "Training loss for batch 7961 : 0.2527768313884735\n",
      "Training loss for batch 7962 : 0.2603186368942261\n",
      "Training loss for batch 7963 : 0.19956304132938385\n",
      "Training loss for batch 7964 : 0.30653077363967896\n",
      "Training loss for batch 7965 : 0.3296857178211212\n",
      "Training loss for batch 7966 : 0.3741461932659149\n",
      "Training loss for batch 7967 : 0.0293886661529541\n",
      "Training loss for batch 7968 : 0.27807682752609253\n",
      "Training loss for batch 7969 : 0.039218537509441376\n",
      "Training loss for batch 7970 : 0.4214198589324951\n",
      "Training loss for batch 7971 : 0.36527445912361145\n",
      "Training loss for batch 7972 : 0.3251393735408783\n",
      "Training loss for batch 7973 : 0.23349833488464355\n",
      "Training loss for batch 7974 : 0.06910765171051025\n",
      "Training loss for batch 7975 : 0.1698952168226242\n",
      "Training loss for batch 7976 : 0.22945865988731384\n",
      "Training loss for batch 7977 : 0.15972891449928284\n",
      "Training loss for batch 7978 : 0.2503127455711365\n",
      "Training loss for batch 7979 : 0.0768362283706665\n",
      "Training loss for batch 7980 : 0.37524011731147766\n",
      "Training loss for batch 7981 : 0.004370709415525198\n",
      "Training loss for batch 7982 : 0.52708899974823\n",
      "Training loss for batch 7983 : 0.08133059740066528\n",
      "Training loss for batch 7984 : 0.12042050063610077\n",
      "Training loss for batch 7985 : 0.2344014197587967\n",
      "Training loss for batch 7986 : 0.1145050972700119\n",
      "Training loss for batch 7987 : 0.028888940811157227\n",
      "Training loss for batch 7988 : 0.016355466097593307\n",
      "Training loss for batch 7989 : 0.05879977345466614\n",
      "Training loss for batch 7990 : 0.07772239297628403\n",
      "Training loss for batch 7991 : 0.08581488579511642\n",
      "Training loss for batch 7992 : 0.13272298872470856\n",
      "Training loss for batch 7993 : 0.04697068780660629\n",
      "Training loss for batch 7994 : 0.3328591287136078\n",
      "Training loss for batch 7995 : 0.2015605866909027\n",
      "Training loss for batch 7996 : 0.16156645119190216\n",
      "Training loss for batch 7997 : 0.057323385030031204\n",
      "Training loss for batch 7998 : 0.16129355132579803\n",
      "Training loss for batch 7999 : 0.13066835701465607\n",
      "Training loss for batch 8000 : 0.16759900748729706\n",
      "Training loss for batch 8001 : 0.15030252933502197\n",
      "Training loss for batch 8002 : 0.2579497694969177\n",
      "Training loss for batch 8003 : 0.46527212858200073\n",
      "Training loss for batch 8004 : 0.20914511382579803\n",
      "Training loss for batch 8005 : 0.06563983857631683\n",
      "Training loss for batch 8006 : 0.23437577486038208\n",
      "Training loss for batch 8007 : 0.22417078912258148\n",
      "Training loss for batch 8008 : 0.07096653431653976\n",
      "Training loss for batch 8009 : 0.26795560121536255\n",
      "Training loss for batch 8010 : 0.13480298221111298\n",
      "Training loss for batch 8011 : 0.26583153009414673\n",
      "Training loss for batch 8012 : 0.26039811968803406\n",
      "Training loss for batch 8013 : 0.07320377975702286\n",
      "Training loss for batch 8014 : 0.2725628614425659\n",
      "Training loss for batch 8015 : 0.21038708090782166\n",
      "Training loss for batch 8016 : 0.08656110614538193\n",
      "Training loss for batch 8017 : 0.39156997203826904\n",
      "Training loss for batch 8018 : 0.19863241910934448\n",
      "Training loss for batch 8019 : 0.2573760449886322\n",
      "Training loss for batch 8020 : 0.13490737974643707\n",
      "Training loss for batch 8021 : 0.1262197643518448\n",
      "Training loss for batch 8022 : 0.00410499656572938\n",
      "Training loss for batch 8023 : 0.29593148827552795\n",
      "Training loss for batch 8024 : 0.3924869894981384\n",
      "Training loss for batch 8025 : 0.11441902071237564\n",
      "Training loss for batch 8026 : 0.08955729007720947\n",
      "Training loss for batch 8027 : 0.08357622474431992\n",
      "Training loss for batch 8028 : 0.4226456880569458\n",
      "Training loss for batch 8029 : 0.1218482181429863\n",
      "Training loss for batch 8030 : 0.25206121802330017\n",
      "Training loss for batch 8031 : 0.18740352988243103\n",
      "Training loss for batch 8032 : 0.338154673576355\n",
      "Training loss for batch 8033 : 0.07197434455156326\n",
      "Training loss for batch 8034 : 0.14734308421611786\n",
      "Training loss for batch 8035 : 0.3934481739997864\n",
      "Training loss for batch 8036 : 0.35293054580688477\n",
      "Training loss for batch 8037 : 0.16145309805870056\n",
      "Training loss for batch 8038 : 0.01742950640618801\n",
      "Training loss for batch 8039 : 0.07278649508953094\n",
      "Training loss for batch 8040 : 0.0\n",
      "Training loss for batch 8041 : 0.300272136926651\n",
      "Training loss for batch 8042 : 0.001534606097266078\n",
      "Training loss for batch 8043 : 0.715972363948822\n",
      "Training loss for batch 8044 : 0.6730290651321411\n",
      "Training loss for batch 8045 : 0.15166693925857544\n",
      "Training loss for batch 8046 : 0.3225367069244385\n",
      "Training loss for batch 8047 : 0.06272131204605103\n",
      "Training loss for batch 8048 : 0.03961828351020813\n",
      "Training loss for batch 8049 : 0.44391390681266785\n",
      "Training loss for batch 8050 : 0.1774909347295761\n",
      "Training loss for batch 8051 : 0.2425733357667923\n",
      "Training loss for batch 8052 : 0.2426423728466034\n",
      "Training loss for batch 8053 : 0.1387329250574112\n",
      "Training loss for batch 8054 : 0.04034517705440521\n",
      "Training loss for batch 8055 : 0.003689096076413989\n",
      "Training loss for batch 8056 : 0.1898929327726364\n",
      "Training loss for batch 8057 : 0.28720685839653015\n",
      "Training loss for batch 8058 : 0.34845635294914246\n",
      "Training loss for batch 8059 : 0.059369321912527084\n",
      "Training loss for batch 8060 : 0.0\n",
      "Training loss for batch 8061 : 0.2541792094707489\n",
      "Training loss for batch 8062 : 0.24975939095020294\n",
      "Training loss for batch 8063 : 0.13663816452026367\n",
      "Training loss for batch 8064 : 0.05532042682170868\n",
      "Training loss for batch 8065 : 0.08776078373193741\n",
      "Training loss for batch 8066 : 0.11956802010536194\n",
      "Training loss for batch 8067 : 0.5048958659172058\n",
      "Training loss for batch 8068 : 0.610135555267334\n",
      "Training loss for batch 8069 : 0.22083474695682526\n",
      "Training loss for batch 8070 : 0.2705846428871155\n",
      "Training loss for batch 8071 : 0.5104886889457703\n",
      "Training loss for batch 8072 : 0.3847973048686981\n",
      "Training loss for batch 8073 : 0.48359227180480957\n",
      "Training loss for batch 8074 : 0.35861796140670776\n",
      "Training loss for batch 8075 : 0.24336139857769012\n",
      "Training loss for batch 8076 : 0.026989061385393143\n",
      "Training loss for batch 8077 : 0.23366792500019073\n",
      "Training loss for batch 8078 : 0.3925493657588959\n",
      "Training loss for batch 8079 : 0.3656553030014038\n",
      "Training loss for batch 8080 : 0.6602116227149963\n",
      "Training loss for batch 8081 : 0.258476197719574\n",
      "Training loss for batch 8082 : 0.06313950568437576\n",
      "Training loss for batch 8083 : 0.28924477100372314\n",
      "Training loss for batch 8084 : 0.14562390744686127\n",
      "Training loss for batch 8085 : 0.27046847343444824\n",
      "Training loss for batch 8086 : 0.2981022596359253\n",
      "Training loss for batch 8087 : 0.3734925091266632\n",
      "Training loss for batch 8088 : 0.17728398740291595\n",
      "Training loss for batch 8089 : 0.14182282984256744\n",
      "Training loss for batch 8090 : 0.337999552488327\n",
      "Training loss for batch 8091 : 0.3043764531612396\n",
      "Training loss for batch 8092 : 0.20381201803684235\n",
      "Training loss for batch 8093 : 0.3911439776420593\n",
      "Training loss for batch 8094 : 0.136655792593956\n",
      "Training loss for batch 8095 : 0.23745392262935638\n",
      "Training loss for batch 8096 : 0.07732445746660233\n",
      "Training loss for batch 8097 : 0.042596589773893356\n",
      "Training loss for batch 8098 : 0.03983793407678604\n",
      "Training loss for batch 8099 : 0.32638490200042725\n",
      "Training loss for batch 8100 : 0.0\n",
      "Training loss for batch 8101 : 0.09840811789035797\n",
      "Training loss for batch 8102 : 0.030520988628268242\n",
      "Training loss for batch 8103 : 0.25816377997398376\n",
      "Training loss for batch 8104 : 0.3162766993045807\n",
      "Training loss for batch 8105 : 0.17738929390907288\n",
      "Training loss for batch 8106 : 0.30516526103019714\n",
      "Training loss for batch 8107 : 0.36696115136146545\n",
      "Training loss for batch 8108 : 0.14344476163387299\n",
      "Training loss for batch 8109 : 0.22041064500808716\n",
      "Training loss for batch 8110 : 0.23852454125881195\n",
      "Training loss for batch 8111 : 0.33452871441841125\n",
      "Training loss for batch 8112 : 0.2985703647136688\n",
      "Training loss for batch 8113 : 0.5682104825973511\n",
      "Training loss for batch 8114 : 0.40017420053482056\n",
      "Training loss for batch 8115 : 0.14192631840705872\n",
      "Training loss for batch 8116 : 0.11775209754705429\n",
      "Training loss for batch 8117 : 0.050417300313711166\n",
      "Training loss for batch 8118 : 0.17355188727378845\n",
      "Training loss for batch 8119 : 0.30133283138275146\n",
      "Training loss for batch 8120 : 0.4106146991252899\n",
      "Training loss for batch 8121 : 0.20853185653686523\n",
      "Training loss for batch 8122 : 0.685783326625824\n",
      "Training loss for batch 8123 : 0.14022576808929443\n",
      "Training loss for batch 8124 : 0.266581267118454\n",
      "Training loss for batch 8125 : 0.3461601138114929\n",
      "Training loss for batch 8126 : 0.20815467834472656\n",
      "Training loss for batch 8127 : 0.33376893401145935\n",
      "Training loss for batch 8128 : 0.058558493852615356\n",
      "Training loss for batch 8129 : 0.17584294080734253\n",
      "Training loss for batch 8130 : 0.015389005653560162\n",
      "Training loss for batch 8131 : 0.4087100327014923\n",
      "Training loss for batch 8132 : 0.3047550618648529\n",
      "Training loss for batch 8133 : 0.24966035783290863\n",
      "Training loss for batch 8134 : 0.20155341923236847\n",
      "Training loss for batch 8135 : 0.3035460114479065\n",
      "Training loss for batch 8136 : 0.2842384874820709\n",
      "Training loss for batch 8137 : 0.21046572923660278\n",
      "Training loss for batch 8138 : 0.2807110846042633\n",
      "Training loss for batch 8139 : 0.21585114300251007\n",
      "Training loss for batch 8140 : 0.3263459801673889\n",
      "Training loss for batch 8141 : 0.529629111289978\n",
      "Training loss for batch 8142 : 0.17289601266384125\n",
      "Training loss for batch 8143 : 0.10363675653934479\n",
      "Training loss for batch 8144 : 0.09708606451749802\n",
      "Training loss for batch 8145 : 0.10030300915241241\n",
      "Training loss for batch 8146 : 0.05820054933428764\n",
      "Training loss for batch 8147 : 0.3935776948928833\n",
      "Training loss for batch 8148 : 0.06674207746982574\n",
      "Training loss for batch 8149 : 0.14481519162654877\n",
      "Training loss for batch 8150 : 0.2019633799791336\n",
      "Training loss for batch 8151 : 0.08866623044013977\n",
      "Training loss for batch 8152 : 0.5378150343894958\n",
      "Training loss for batch 8153 : 0.1429450660943985\n",
      "Training loss for batch 8154 : 0.38991421461105347\n",
      "Training loss for batch 8155 : 0.13191750645637512\n",
      "Training loss for batch 8156 : 0.07700614631175995\n",
      "Training loss for batch 8157 : 0.18733635544776917\n",
      "Training loss for batch 8158 : 0.3058362603187561\n",
      "Training loss for batch 8159 : 0.167179137468338\n",
      "Training loss for batch 8160 : 0.32056644558906555\n",
      "Training loss for batch 8161 : 0.22925089299678802\n",
      "Training loss for batch 8162 : 0.10215533524751663\n",
      "Training loss for batch 8163 : 0.056783515959978104\n",
      "Training loss for batch 8164 : 0.08452659845352173\n",
      "Training loss for batch 8165 : 0.34000861644744873\n",
      "Training loss for batch 8166 : 0.3072827160358429\n",
      "Training loss for batch 8167 : 0.12143366038799286\n",
      "Training loss for batch 8168 : 0.024313373491168022\n",
      "Training loss for batch 8169 : 0.14227519929409027\n",
      "Training loss for batch 8170 : 0.28008848428726196\n",
      "Training loss for batch 8171 : 0.4005652964115143\n",
      "Training loss for batch 8172 : 0.06394612789154053\n",
      "Training loss for batch 8173 : 0.22452302277088165\n",
      "Training loss for batch 8174 : 0.30276986956596375\n",
      "Training loss for batch 8175 : 0.07249036431312561\n",
      "Training loss for batch 8176 : 0.08902405202388763\n",
      "Training loss for batch 8177 : 0.16661034524440765\n",
      "Training loss for batch 8178 : 0.004972150083631277\n",
      "Training loss for batch 8179 : 0.08352291584014893\n",
      "Training loss for batch 8180 : 0.26469460129737854\n",
      "Training loss for batch 8181 : 0.1171012669801712\n",
      "Training loss for batch 8182 : 0.23124000430107117\n",
      "Training loss for batch 8183 : 0.2869586944580078\n",
      "Training loss for batch 8184 : 0.24853834509849548\n",
      "Training loss for batch 8185 : 0.05843725800514221\n",
      "Training loss for batch 8186 : 0.07668203115463257\n",
      "Training loss for batch 8187 : 0.48320651054382324\n",
      "Training loss for batch 8188 : 0.3031822443008423\n",
      "Training loss for batch 8189 : 0.0692344382405281\n",
      "Training loss for batch 8190 : 0.5440468192100525\n",
      "Training loss for batch 8191 : 0.14890038967132568\n",
      "Training loss for batch 8192 : 0.45513904094696045\n",
      "Training loss for batch 8193 : 0.04735925793647766\n",
      "Training loss for batch 8194 : 0.17608825862407684\n",
      "Training loss for batch 8195 : 0.13834813237190247\n",
      "Training loss for batch 8196 : 0.1779601126909256\n",
      "Training loss for batch 8197 : 0.5549072027206421\n",
      "Training loss for batch 8198 : 0.04715230315923691\n",
      "Training loss for batch 8199 : 0.3444373607635498\n",
      "Training loss for batch 8200 : 0.14761950075626373\n",
      "Training loss for batch 8201 : 0.011023092083632946\n",
      "Training loss for batch 8202 : 0.3679318428039551\n",
      "Training loss for batch 8203 : 0.10978349298238754\n",
      "Training loss for batch 8204 : 0.4636017978191376\n",
      "Training loss for batch 8205 : 0.14108259975910187\n",
      "Training loss for batch 8206 : 0.12101886421442032\n",
      "Training loss for batch 8207 : 0.07257276773452759\n",
      "Training loss for batch 8208 : 0.07647345960140228\n",
      "Training loss for batch 8209 : 0.4747050106525421\n",
      "Training loss for batch 8210 : 0.13413746654987335\n",
      "Training loss for batch 8211 : 0.2900609076023102\n",
      "Training loss for batch 8212 : 0.0009620686760172248\n",
      "Training loss for batch 8213 : 0.23746533691883087\n",
      "Training loss for batch 8214 : 0.24429231882095337\n",
      "Training loss for batch 8215 : 0.39557138085365295\n",
      "Training loss for batch 8216 : 0.26962631940841675\n",
      "Training loss for batch 8217 : 0.13245417177677155\n",
      "Training loss for batch 8218 : 0.03621441870927811\n",
      "Training loss for batch 8219 : 0.22278521955013275\n",
      "Training loss for batch 8220 : 0.27877277135849\n",
      "Training loss for batch 8221 : 0.024334078654646873\n",
      "Training loss for batch 8222 : 0.30593153834342957\n",
      "Training loss for batch 8223 : 0.1964244842529297\n",
      "Training loss for batch 8224 : 0.3034590184688568\n",
      "Training loss for batch 8225 : 0.38883692026138306\n",
      "Training loss for batch 8226 : 0.3131895065307617\n",
      "Training loss for batch 8227 : 0.3076663315296173\n",
      "Training loss for batch 8228 : 0.0709119513630867\n",
      "Training loss for batch 8229 : 0.1222522184252739\n",
      "Training loss for batch 8230 : 0.002790093654766679\n",
      "Training loss for batch 8231 : 0.2373495250940323\n",
      "Training loss for batch 8232 : 0.09326894581317902\n",
      "Training loss for batch 8233 : 0.23194493353366852\n",
      "Training loss for batch 8234 : 0.3914361596107483\n",
      "Training loss for batch 8235 : 0.020993860438466072\n",
      "Training loss for batch 8236 : 0.462356835603714\n",
      "Training loss for batch 8237 : 0.2586086392402649\n",
      "Training loss for batch 8238 : 0.12855279445648193\n",
      "Training loss for batch 8239 : 0.11490492522716522\n",
      "Training loss for batch 8240 : 0.02454724907875061\n",
      "Training loss for batch 8241 : 0.2693178951740265\n",
      "Training loss for batch 8242 : 0.07784539461135864\n",
      "Training loss for batch 8243 : 0.6522403359413147\n",
      "Training loss for batch 8244 : 0.09007100760936737\n",
      "Training loss for batch 8245 : 0.008159174583852291\n",
      "Training loss for batch 8246 : 0.18079711496829987\n",
      "Training loss for batch 8247 : 0.07404182851314545\n",
      "Training loss for batch 8248 : 0.27349168062210083\n",
      "Training loss for batch 8249 : 0.28026556968688965\n",
      "Training loss for batch 8250 : 0.1724102944135666\n",
      "Training loss for batch 8251 : 0.11221521347761154\n",
      "Training loss for batch 8252 : 0.3006364703178406\n",
      "Training loss for batch 8253 : 0.15835405886173248\n",
      "Training loss for batch 8254 : 0.4703669548034668\n",
      "Training loss for batch 8255 : 0.34495946764945984\n",
      "Training loss for batch 8256 : 0.5193278193473816\n",
      "Training loss for batch 8257 : 0.6570676565170288\n",
      "Training loss for batch 8258 : 0.20855313539505005\n",
      "Training loss for batch 8259 : 0.32798463106155396\n",
      "Training loss for batch 8260 : 0.3845467269420624\n",
      "Training loss for batch 8261 : 0.18704237043857574\n",
      "Training loss for batch 8262 : 0.040411729365587234\n",
      "Training loss for batch 8263 : 0.26641708612442017\n",
      "Training loss for batch 8264 : 0.0732799544930458\n",
      "Training loss for batch 8265 : 0.13418905436992645\n",
      "Training loss for batch 8266 : 0.08520954847335815\n",
      "Training loss for batch 8267 : 0.35311439633369446\n",
      "Training loss for batch 8268 : 0.02846008725464344\n",
      "Training loss for batch 8269 : 0.3266467750072479\n",
      "Training loss for batch 8270 : 0.3003576993942261\n",
      "Training loss for batch 8271 : 0.00014647362695541233\n",
      "Training loss for batch 8272 : 0.668321967124939\n",
      "Training loss for batch 8273 : 0.4588747024536133\n",
      "Training loss for batch 8274 : 0.08391139656305313\n",
      "Training loss for batch 8275 : 0.11405074596405029\n",
      "Training loss for batch 8276 : 0.09057772904634476\n",
      "Training loss for batch 8277 : 0.38745954632759094\n",
      "Training loss for batch 8278 : 0.1783268004655838\n",
      "Training loss for batch 8279 : 0.32262763381004333\n",
      "Training loss for batch 8280 : 0.3875163793563843\n",
      "Training loss for batch 8281 : 0.050681352615356445\n",
      "Training loss for batch 8282 : 0.13582047820091248\n",
      "Training loss for batch 8283 : 0.11058221757411957\n",
      "Training loss for batch 8284 : 0.11260224878787994\n",
      "Training loss for batch 8285 : 0.15379643440246582\n",
      "Training loss for batch 8286 : 0.23932383954524994\n",
      "Training loss for batch 8287 : 0.6183200478553772\n",
      "Training loss for batch 8288 : 0.03101891092956066\n",
      "Training loss for batch 8289 : 0.09881750494241714\n",
      "Training loss for batch 8290 : 0.20483598113059998\n",
      "Training loss for batch 8291 : 0.24782170355319977\n",
      "Training loss for batch 8292 : 0.150584876537323\n",
      "Training loss for batch 8293 : 0.23750323057174683\n",
      "Training loss for batch 8294 : 0.03756074979901314\n",
      "Training loss for batch 8295 : 0.3929644227027893\n",
      "Training loss for batch 8296 : 0.44060537219047546\n",
      "Training loss for batch 8297 : 0.2923180162906647\n",
      "Training loss for batch 8298 : 0.2855156362056732\n",
      "Training loss for batch 8299 : 0.4439437985420227\n",
      "Training loss for batch 8300 : 0.35839512944221497\n",
      "Training loss for batch 8301 : 0.09150511026382446\n",
      "Training loss for batch 8302 : 0.2273770570755005\n",
      "Training loss for batch 8303 : 0.49272915720939636\n",
      "Training loss for batch 8304 : 0.11307291686534882\n",
      "Training loss for batch 8305 : 0.1552334725856781\n",
      "Training loss for batch 8306 : 0.11577422171831131\n",
      "Training loss for batch 8307 : 1.037217140197754\n",
      "Parameter containing:\n",
      "tensor(0.0372, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.0372042655944824\n",
      "Training loss for batch 1 : 1.037186861038208\n",
      "Training loss for batch 2 : 1.0371651649475098\n",
      "Training loss for batch 3 : 1.037139892578125\n",
      "Training loss for batch 4 : 1.037111759185791\n",
      "Training loss for batch 5 : 1.0370808839797974\n",
      "Training loss for batch 6 : 1.0370478630065918\n",
      "Training loss for batch 7 : 1.0370129346847534\n",
      "Training loss for batch 8 : 1.0369763374328613\n",
      "Training loss for batch 9 : 1.0369384288787842\n",
      "Training loss for batch 10 : 1.036899447441101\n",
      "Training loss for batch 11 : 1.0368595123291016\n",
      "Training loss for batch 12 : 0.33458641171455383\n",
      "Training loss for batch 13 : 0.11613521724939346\n",
      "Training loss for batch 14 : 0.24650974571704865\n",
      "Training loss for batch 15 : 0.20099318027496338\n",
      "Training loss for batch 16 : 7.945327524794266e-06\n",
      "Training loss for batch 17 : 0.12983126938343048\n",
      "Training loss for batch 18 : 0.1892603039741516\n",
      "Training loss for batch 19 : 0.26219868659973145\n",
      "Training loss for batch 20 : 0.07155230641365051\n",
      "Training loss for batch 21 : 0.011371016502380371\n",
      "Training loss for batch 22 : 0.15106280148029327\n",
      "Training loss for batch 23 : 0.00738900899887085\n",
      "Training loss for batch 24 : 0.13804951310157776\n",
      "Training loss for batch 25 : 0.2794182598590851\n",
      "Training loss for batch 26 : 0.2732628285884857\n",
      "Training loss for batch 27 : 0.09864859282970428\n",
      "Training loss for batch 28 : 0.11206322908401489\n",
      "Training loss for batch 29 : 0.2686752378940582\n",
      "Training loss for batch 30 : 0.6540458798408508\n",
      "Training loss for batch 31 : 0.037824422121047974\n",
      "Training loss for batch 32 : 0.2044408917427063\n",
      "Training loss for batch 33 : 0.04589417949318886\n",
      "Training loss for batch 34 : 0.06269723176956177\n",
      "Training loss for batch 35 : 0.411651074886322\n",
      "Training loss for batch 36 : 0.3494608998298645\n",
      "Training loss for batch 37 : 0.10500825196504593\n",
      "Training loss for batch 38 : 0.08634114265441895\n",
      "Training loss for batch 39 : 0.012559138238430023\n",
      "Training loss for batch 40 : 0.01276038121432066\n",
      "Training loss for batch 41 : 0.24791154265403748\n",
      "Training loss for batch 42 : 0.2835715711116791\n",
      "Training loss for batch 43 : 0.0807466134428978\n",
      "Training loss for batch 44 : 0.05416388064622879\n",
      "Training loss for batch 45 : 0.030312906950712204\n",
      "Training loss for batch 46 : 0.2610871195793152\n",
      "Training loss for batch 47 : 0.32213714718818665\n",
      "Training loss for batch 48 : 0.07891282439231873\n",
      "Training loss for batch 49 : 0.14174123108386993\n",
      "Training loss for batch 50 : 0.08108597248792648\n",
      "Training loss for batch 51 : 0.008910792879760265\n",
      "Training loss for batch 52 : 0.02693999372422695\n",
      "Training loss for batch 53 : 0.18714943528175354\n",
      "Training loss for batch 54 : 0.19154468178749084\n",
      "Training loss for batch 55 : 0.12704826891422272\n",
      "Training loss for batch 56 : 0.12254533916711807\n",
      "Training loss for batch 57 : 0.31180277466773987\n",
      "Training loss for batch 58 : 0.20191405713558197\n",
      "Training loss for batch 59 : 0.2782065272331238\n",
      "Training loss for batch 60 : 0.16535446047782898\n",
      "Training loss for batch 61 : 0.24883252382278442\n",
      "Training loss for batch 62 : 0.03764976188540459\n",
      "Training loss for batch 63 : 0.14850540459156036\n",
      "Training loss for batch 64 : 0.2657531201839447\n",
      "Training loss for batch 65 : 0.1379120796918869\n",
      "Training loss for batch 66 : 0.05561402440071106\n",
      "Training loss for batch 67 : 0.015538236126303673\n",
      "Training loss for batch 68 : 0.056085970252752304\n",
      "Training loss for batch 69 : 0.32998958230018616\n",
      "Training loss for batch 70 : 0.033121734857559204\n",
      "Training loss for batch 71 : 0.10477086156606674\n",
      "Training loss for batch 72 : 0.19454173743724823\n",
      "Training loss for batch 73 : 0.14669948816299438\n",
      "Training loss for batch 74 : 0.16873453557491302\n",
      "Training loss for batch 75 : 0.24021323025226593\n",
      "Training loss for batch 76 : 0.3860628008842468\n",
      "Training loss for batch 77 : 0.23635448515415192\n",
      "Training loss for batch 78 : 0.01709161326289177\n",
      "Training loss for batch 79 : 0.3708307147026062\n",
      "Training loss for batch 80 : 0.24065345525741577\n",
      "Training loss for batch 81 : 0.09105721861124039\n",
      "Training loss for batch 82 : 0.14729678630828857\n",
      "Training loss for batch 83 : 0.026075568050146103\n",
      "Training loss for batch 84 : 0.08161633461713791\n",
      "Training loss for batch 85 : 0.3702055513858795\n",
      "Training loss for batch 86 : 0.10339552909135818\n",
      "Training loss for batch 87 : 0.5124309659004211\n",
      "Training loss for batch 88 : 0.2884827256202698\n",
      "Training loss for batch 89 : 0.27301958203315735\n",
      "Training loss for batch 90 : 0.18257710337638855\n",
      "Training loss for batch 91 : 0.0077687399461865425\n",
      "Training loss for batch 92 : 0.0084141930565238\n",
      "Training loss for batch 93 : 0.1625594049692154\n",
      "Training loss for batch 94 : 0.13602317869663239\n",
      "Training loss for batch 95 : 0.20097559690475464\n",
      "Training loss for batch 96 : 0.044809434562921524\n",
      "Training loss for batch 97 : 0.4267628490924835\n",
      "Training loss for batch 98 : 0.20274406671524048\n",
      "Training loss for batch 99 : 0.05135664343833923\n",
      "Training loss for batch 100 : 0.0903397798538208\n",
      "Training loss for batch 101 : 0.020892739295959473\n",
      "Training loss for batch 102 : 0.2699565291404724\n",
      "Training loss for batch 103 : 0.2200806736946106\n",
      "Training loss for batch 104 : 0.06967178732156754\n",
      "Training loss for batch 105 : 0.04255560785531998\n",
      "Training loss for batch 106 : 0.2745583653450012\n",
      "Training loss for batch 107 : 0.10711325705051422\n",
      "Training loss for batch 108 : 0.04784664139151573\n",
      "Training loss for batch 109 : 0.0\n",
      "Training loss for batch 110 : 0.07883553951978683\n",
      "Training loss for batch 111 : 0.14244437217712402\n",
      "Training loss for batch 112 : 0.17150507867336273\n",
      "Training loss for batch 113 : 0.09434990584850311\n",
      "Training loss for batch 114 : 0.0\n",
      "Training loss for batch 115 : 0.04925037547945976\n",
      "Training loss for batch 116 : 0.2350160777568817\n",
      "Training loss for batch 117 : 0.03570675849914551\n",
      "Training loss for batch 118 : 0.2602081894874573\n",
      "Training loss for batch 119 : 0.0\n",
      "Training loss for batch 120 : 0.08475305885076523\n",
      "Training loss for batch 121 : 0.01211482286453247\n",
      "Training loss for batch 122 : 0.05102626234292984\n",
      "Training loss for batch 123 : 0.2624482214450836\n",
      "Training loss for batch 124 : 0.1412973552942276\n",
      "Training loss for batch 125 : 0.060119397938251495\n",
      "Training loss for batch 126 : 0.01726408675312996\n",
      "Training loss for batch 127 : 0.025328120216727257\n",
      "Training loss for batch 128 : 0.1002463698387146\n",
      "Training loss for batch 129 : 0.03259993717074394\n",
      "Training loss for batch 130 : 0.19776442646980286\n",
      "Training loss for batch 131 : 0.03941970691084862\n",
      "Training loss for batch 132 : 0.000343254127074033\n",
      "Training loss for batch 133 : 0.1676408052444458\n",
      "Training loss for batch 134 : 0.30950111150741577\n",
      "Training loss for batch 135 : 0.19624626636505127\n",
      "Training loss for batch 136 : 0.254395991563797\n",
      "Training loss for batch 137 : 0.09468327462673187\n",
      "Training loss for batch 138 : 0.1516856849193573\n",
      "Training loss for batch 139 : 0.09638339281082153\n",
      "Training loss for batch 140 : 0.5608212947845459\n",
      "Training loss for batch 141 : 0.02609170973300934\n",
      "Training loss for batch 142 : 0.17884303629398346\n",
      "Training loss for batch 143 : 0.011408885940909386\n",
      "Training loss for batch 144 : 0.18622680008411407\n",
      "Training loss for batch 145 : 0.007094877306371927\n",
      "Training loss for batch 146 : 0.08104056864976883\n",
      "Training loss for batch 147 : 0.0\n",
      "Training loss for batch 148 : 0.09598991274833679\n",
      "Training loss for batch 149 : 0.14644163846969604\n",
      "Training loss for batch 150 : 0.02723424881696701\n",
      "Training loss for batch 151 : 0.17321565747261047\n",
      "Training loss for batch 152 : 0.06677296757698059\n",
      "Training loss for batch 153 : 0.09985935688018799\n",
      "Training loss for batch 154 : 0.24885810911655426\n",
      "Training loss for batch 155 : 0.40478822588920593\n",
      "Training loss for batch 156 : 0.020291272550821304\n",
      "Training loss for batch 157 : 0.17376971244812012\n",
      "Training loss for batch 158 : 0.006143656093627214\n",
      "Training loss for batch 159 : 0.03232000395655632\n",
      "Training loss for batch 160 : 0.14414215087890625\n",
      "Training loss for batch 161 : 0.04531467333436012\n",
      "Training loss for batch 162 : 0.002029689960181713\n",
      "Training loss for batch 163 : 0.0082613630220294\n",
      "Training loss for batch 164 : 0.48248016834259033\n",
      "Training loss for batch 165 : 0.2011147290468216\n",
      "Training loss for batch 166 : 0.4541109800338745\n",
      "Training loss for batch 167 : 0.3454727530479431\n",
      "Training loss for batch 168 : 0.2794017493724823\n",
      "Training loss for batch 169 : 0.1817401796579361\n",
      "Training loss for batch 170 : 0.07275142520666122\n",
      "Training loss for batch 171 : 0.3675856590270996\n",
      "Training loss for batch 172 : 0.49262818694114685\n",
      "Training loss for batch 173 : 0.3045012354850769\n",
      "Training loss for batch 174 : 0.07829241454601288\n",
      "Training loss for batch 175 : 0.19742457568645477\n",
      "Training loss for batch 176 : 0.2912180721759796\n",
      "Training loss for batch 177 : 0.08337685465812683\n",
      "Training loss for batch 178 : 0.3239246606826782\n",
      "Training loss for batch 179 : 0.7320050597190857\n",
      "Training loss for batch 180 : 0.2894923686981201\n",
      "Training loss for batch 181 : 0.09827207773923874\n",
      "Training loss for batch 182 : 0.045769017189741135\n",
      "Training loss for batch 183 : 0.21998946368694305\n",
      "Training loss for batch 184 : 0.20359008014202118\n",
      "Training loss for batch 185 : 0.15704375505447388\n",
      "Training loss for batch 186 : 0.2430385947227478\n",
      "Training loss for batch 187 : 0.05385773628950119\n",
      "Training loss for batch 188 : 0.2417951375246048\n",
      "Training loss for batch 189 : 0.06989297270774841\n",
      "Training loss for batch 190 : 0.018812429159879684\n",
      "Training loss for batch 191 : 0.06094226986169815\n",
      "Training loss for batch 192 : 0.13452762365341187\n",
      "Training loss for batch 193 : 0.4086948037147522\n",
      "Training loss for batch 194 : 0.1192195788025856\n",
      "Training loss for batch 195 : 0.3395949602127075\n",
      "Training loss for batch 196 : 0.17735785245895386\n",
      "Training loss for batch 197 : 0.07214286178350449\n",
      "Training loss for batch 198 : 0.08896660804748535\n",
      "Training loss for batch 199 : 0.1409396380186081\n",
      "Training loss for batch 200 : 0.05776961147785187\n",
      "Training loss for batch 201 : 0.018222235143184662\n",
      "Training loss for batch 202 : 0.12591756880283356\n",
      "Training loss for batch 203 : 0.17610490322113037\n",
      "Training loss for batch 204 : 0.22724251449108124\n",
      "Training loss for batch 205 : 0.0008785467362031341\n",
      "Training loss for batch 206 : 0.29344332218170166\n",
      "Training loss for batch 207 : 0.25868281722068787\n",
      "Training loss for batch 208 : 0.07579798251390457\n",
      "Training loss for batch 209 : 0.12251095473766327\n",
      "Training loss for batch 210 : 0.6174120903015137\n",
      "Training loss for batch 211 : 0.5268321633338928\n",
      "Training loss for batch 212 : 0.08962246775627136\n",
      "Training loss for batch 213 : 0.17465810477733612\n",
      "Training loss for batch 214 : 0.10972598195075989\n",
      "Training loss for batch 215 : 0.10240459442138672\n",
      "Training loss for batch 216 : 0.06823568046092987\n",
      "Training loss for batch 217 : 0.20986340939998627\n",
      "Training loss for batch 218 : 0.06846906989812851\n",
      "Training loss for batch 219 : 0.3114876449108124\n",
      "Training loss for batch 220 : 0.2845665514469147\n",
      "Training loss for batch 221 : 0.5098671317100525\n",
      "Training loss for batch 222 : 0.1320832371711731\n",
      "Training loss for batch 223 : 0.1999978870153427\n",
      "Training loss for batch 224 : 0.224792018532753\n",
      "Training loss for batch 225 : 0.26068612933158875\n",
      "Training loss for batch 226 : 0.34503835439682007\n",
      "Training loss for batch 227 : 0.24727852642536163\n",
      "Training loss for batch 228 : 0.09892770648002625\n",
      "Training loss for batch 229 : 0.16788636147975922\n",
      "Training loss for batch 230 : 0.007931925356388092\n",
      "Training loss for batch 231 : 0.15759344398975372\n",
      "Training loss for batch 232 : 0.09051956236362457\n",
      "Training loss for batch 233 : 0.37994256615638733\n",
      "Training loss for batch 234 : 0.08909609913825989\n",
      "Training loss for batch 235 : 0.2780204117298126\n",
      "Training loss for batch 236 : 0.41852158308029175\n",
      "Training loss for batch 237 : 0.4066867232322693\n",
      "Training loss for batch 238 : 0.011294152587652206\n",
      "Training loss for batch 239 : 0.05319599807262421\n",
      "Training loss for batch 240 : 0.14605727791786194\n",
      "Training loss for batch 241 : 0.04391787201166153\n",
      "Training loss for batch 242 : 0.1538551300764084\n",
      "Training loss for batch 243 : 0.010373654775321484\n",
      "Training loss for batch 244 : 0.17391645908355713\n",
      "Training loss for batch 245 : 0.1952677220106125\n",
      "Training loss for batch 246 : 0.27039995789527893\n",
      "Training loss for batch 247 : 0.4014025032520294\n",
      "Training loss for batch 248 : 0.08287205547094345\n",
      "Training loss for batch 249 : 0.06574953347444534\n",
      "Training loss for batch 250 : 0.3309417963027954\n",
      "Training loss for batch 251 : 0.034272871911525726\n",
      "Training loss for batch 252 : 0.030951201915740967\n",
      "Training loss for batch 253 : 0.39410901069641113\n",
      "Training loss for batch 254 : 0.11713176965713501\n",
      "Training loss for batch 255 : 0.017222458496689796\n",
      "Training loss for batch 256 : 0.46414950489997864\n",
      "Training loss for batch 257 : 0.04048989713191986\n",
      "Training loss for batch 258 : 0.324226051568985\n",
      "Training loss for batch 259 : 0.05444415658712387\n",
      "Training loss for batch 260 : 0.15209296345710754\n",
      "Training loss for batch 261 : 0.12431670725345612\n",
      "Training loss for batch 262 : 0.18406067788600922\n",
      "Training loss for batch 263 : 0.07802171260118484\n",
      "Training loss for batch 264 : 0.2889253497123718\n",
      "Training loss for batch 265 : 0.064088836312294\n",
      "Training loss for batch 266 : 0.04118899628520012\n",
      "Training loss for batch 267 : 0.27469533681869507\n",
      "Training loss for batch 268 : 0.062108539044857025\n",
      "Training loss for batch 269 : 0.0\n",
      "Training loss for batch 270 : 0.14914867281913757\n",
      "Training loss for batch 271 : 0.401888906955719\n",
      "Training loss for batch 272 : 0.10579927265644073\n",
      "Training loss for batch 273 : 0.01574174501001835\n",
      "Training loss for batch 274 : 0.1333637833595276\n",
      "Training loss for batch 275 : 0.04816767945885658\n",
      "Training loss for batch 276 : 0.04077884554862976\n",
      "Training loss for batch 277 : 0.28847309947013855\n",
      "Training loss for batch 278 : 0.24984405934810638\n",
      "Training loss for batch 279 : 0.3264599144458771\n",
      "Training loss for batch 280 : 0.11448906362056732\n",
      "Training loss for batch 281 : 0.20379190146923065\n",
      "Training loss for batch 282 : 0.0602886937558651\n",
      "Training loss for batch 283 : 0.4461616277694702\n",
      "Training loss for batch 284 : 0.3721126616001129\n",
      "Training loss for batch 285 : 0.05668223276734352\n",
      "Training loss for batch 286 : 0.2431294173002243\n",
      "Training loss for batch 287 : 0.05861557647585869\n",
      "Training loss for batch 288 : 0.0\n",
      "Training loss for batch 289 : 0.00793910026550293\n",
      "Training loss for batch 290 : 0.01601623371243477\n",
      "Training loss for batch 291 : 0.06839893758296967\n",
      "Training loss for batch 292 : 0.17594565451145172\n",
      "Training loss for batch 293 : 0.24017196893692017\n",
      "Training loss for batch 294 : 0.2029132843017578\n",
      "Training loss for batch 295 : 0.14985492825508118\n",
      "Training loss for batch 296 : 0.25209274888038635\n",
      "Training loss for batch 297 : 0.056287094950675964\n",
      "Training loss for batch 298 : 0.22923679649829865\n",
      "Training loss for batch 299 : 0.10502274334430695\n",
      "Training loss for batch 300 : 0.11581999063491821\n",
      "Training loss for batch 301 : 0.09040363878011703\n",
      "Training loss for batch 302 : 0.30231064558029175\n",
      "Training loss for batch 303 : 0.17503142356872559\n",
      "Training loss for batch 304 : 0.14300252497196198\n",
      "Training loss for batch 305 : 0.2782914936542511\n",
      "Training loss for batch 306 : 0.05031803995370865\n",
      "Training loss for batch 307 : 0.32811257243156433\n",
      "Training loss for batch 308 : 0.2812739908695221\n",
      "Training loss for batch 309 : 0.3426416218280792\n",
      "Training loss for batch 310 : 0.2851685583591461\n",
      "Training loss for batch 311 : 0.01241236925125122\n",
      "Training loss for batch 312 : 0.22728842496871948\n",
      "Training loss for batch 313 : 0.1283281445503235\n",
      "Training loss for batch 314 : 0.0025968824047595263\n",
      "Training loss for batch 315 : 0.2938237190246582\n",
      "Training loss for batch 316 : 0.013556734658777714\n",
      "Training loss for batch 317 : 0.12061306834220886\n",
      "Training loss for batch 318 : 0.22605708241462708\n",
      "Training loss for batch 319 : 0.14649342000484467\n",
      "Training loss for batch 320 : 0.2929662764072418\n",
      "Training loss for batch 321 : 0.09372738748788834\n",
      "Training loss for batch 322 : 0.41194212436676025\n",
      "Training loss for batch 323 : 0.3857114315032959\n",
      "Training loss for batch 324 : 0.08329102396965027\n",
      "Training loss for batch 325 : 0.1509018838405609\n",
      "Training loss for batch 326 : 0.6643899083137512\n",
      "Training loss for batch 327 : 0.21462860703468323\n",
      "Training loss for batch 328 : 0.3135887682437897\n",
      "Training loss for batch 329 : 0.06265586614608765\n",
      "Training loss for batch 330 : 0.190167635679245\n",
      "Training loss for batch 331 : 0.044994477182626724\n",
      "Training loss for batch 332 : 0.15795724093914032\n",
      "Training loss for batch 333 : 0.2594473958015442\n",
      "Training loss for batch 334 : 0.2619408965110779\n",
      "Training loss for batch 335 : 0.08622009307146072\n",
      "Training loss for batch 336 : 0.031990040093660355\n",
      "Training loss for batch 337 : 0.08328306674957275\n",
      "Training loss for batch 338 : 0.4058166444301605\n",
      "Training loss for batch 339 : 0.21675987541675568\n",
      "Training loss for batch 340 : 0.23847205936908722\n",
      "Training loss for batch 341 : 0.2107352763414383\n",
      "Training loss for batch 342 : 0.1614304631948471\n",
      "Training loss for batch 343 : 0.07711146771907806\n",
      "Training loss for batch 344 : 0.11922666430473328\n",
      "Training loss for batch 345 : 0.13498175144195557\n",
      "Training loss for batch 346 : 0.23200567066669464\n",
      "Training loss for batch 347 : 0.003760636318475008\n",
      "Training loss for batch 348 : 0.3940163254737854\n",
      "Training loss for batch 349 : 0.2780838906764984\n",
      "Training loss for batch 350 : 0.30954188108444214\n",
      "Training loss for batch 351 : 0.03259533643722534\n",
      "Training loss for batch 352 : 0.013057748787105083\n",
      "Training loss for batch 353 : 0.4836398661136627\n",
      "Training loss for batch 354 : 0.34916648268699646\n",
      "Training loss for batch 355 : 0.01929522678256035\n",
      "Training loss for batch 356 : 0.12636810541152954\n",
      "Training loss for batch 357 : 0.05995490401983261\n",
      "Training loss for batch 358 : 0.554651141166687\n",
      "Training loss for batch 359 : 0.27923136949539185\n",
      "Training loss for batch 360 : 0.26722046732902527\n",
      "Training loss for batch 361 : 0.300180047750473\n",
      "Training loss for batch 362 : 0.29165932536125183\n",
      "Training loss for batch 363 : 0.32133930921554565\n",
      "Training loss for batch 364 : 0.22741909325122833\n",
      "Training loss for batch 365 : 0.15105846524238586\n",
      "Training loss for batch 366 : 0.11329249292612076\n",
      "Training loss for batch 367 : 0.16433487832546234\n",
      "Training loss for batch 368 : 0.19772669672966003\n",
      "Training loss for batch 369 : 0.2315218299627304\n",
      "Training loss for batch 370 : 0.07341675460338593\n",
      "Training loss for batch 371 : 0.1464644968509674\n",
      "Training loss for batch 372 : 0.2001943439245224\n",
      "Training loss for batch 373 : 0.09484118223190308\n",
      "Training loss for batch 374 : 0.377852201461792\n",
      "Training loss for batch 375 : 0.22094503045082092\n",
      "Training loss for batch 376 : 0.17197413742542267\n",
      "Training loss for batch 377 : 0.3446793258190155\n",
      "Training loss for batch 378 : 0.04909590631723404\n",
      "Training loss for batch 379 : 0.1398478001356125\n",
      "Training loss for batch 380 : 0.4636927843093872\n",
      "Training loss for batch 381 : 0.3438306152820587\n",
      "Training loss for batch 382 : 0.06054455786943436\n",
      "Training loss for batch 383 : 0.16182436048984528\n",
      "Training loss for batch 384 : 0.4375058114528656\n",
      "Training loss for batch 385 : 0.3385013937950134\n",
      "Training loss for batch 386 : 0.4432336986064911\n",
      "Training loss for batch 387 : 0.12483356893062592\n",
      "Training loss for batch 388 : 0.07978536188602448\n",
      "Training loss for batch 389 : 0.09687206894159317\n",
      "Training loss for batch 390 : 0.3887764811515808\n",
      "Training loss for batch 391 : 0.1087033599615097\n",
      "Training loss for batch 392 : 0.30886536836624146\n",
      "Training loss for batch 393 : 0.15055914223194122\n",
      "Training loss for batch 394 : 0.2646115720272064\n",
      "Training loss for batch 395 : 0.11069801449775696\n",
      "Training loss for batch 396 : 0.052637841552495956\n",
      "Training loss for batch 397 : 0.24643443524837494\n",
      "Training loss for batch 398 : 0.25723209977149963\n",
      "Training loss for batch 399 : 0.181651771068573\n",
      "Training loss for batch 400 : 0.24289436638355255\n",
      "Training loss for batch 401 : 0.06087176874279976\n",
      "Training loss for batch 402 : 0.2237853854894638\n",
      "Training loss for batch 403 : 0.9201818704605103\n",
      "Training loss for batch 404 : 0.15039479732513428\n",
      "Training loss for batch 405 : 0.19310151040554047\n",
      "Training loss for batch 406 : 0.19394509494304657\n",
      "Training loss for batch 407 : 0.2443457394838333\n",
      "Training loss for batch 408 : 0.3795754611492157\n",
      "Training loss for batch 409 : 0.2721899449825287\n",
      "Training loss for batch 410 : 0.10696418583393097\n",
      "Training loss for batch 411 : 0.17750242352485657\n",
      "Training loss for batch 412 : 0.0281933955848217\n",
      "Training loss for batch 413 : 0.035967472940683365\n",
      "Training loss for batch 414 : 0.39222025871276855\n",
      "Training loss for batch 415 : 0.09150702506303787\n",
      "Training loss for batch 416 : 0.33812785148620605\n",
      "Training loss for batch 417 : 0.1923963576555252\n",
      "Training loss for batch 418 : 0.1555355042219162\n",
      "Training loss for batch 419 : 0.42145228385925293\n",
      "Training loss for batch 420 : 0.28510764241218567\n",
      "Training loss for batch 421 : 0.2343292534351349\n",
      "Training loss for batch 422 : 0.14469172060489655\n",
      "Training loss for batch 423 : 0.1443309783935547\n",
      "Training loss for batch 424 : 0.2140282690525055\n",
      "Training loss for batch 425 : 0.045380864292383194\n",
      "Training loss for batch 426 : 0.229255810379982\n",
      "Training loss for batch 427 : 0.20532135665416718\n",
      "Training loss for batch 428 : 0.10893871635198593\n",
      "Training loss for batch 429 : 0.08599310368299484\n",
      "Training loss for batch 430 : 0.4566047191619873\n",
      "Training loss for batch 431 : 0.23877477645874023\n",
      "Training loss for batch 432 : 0.19216269254684448\n",
      "Training loss for batch 433 : 0.35037076473236084\n",
      "Training loss for batch 434 : 0.05398827791213989\n",
      "Training loss for batch 435 : 0.11923902481794357\n",
      "Training loss for batch 436 : 0.172879159450531\n",
      "Training loss for batch 437 : 0.364037424325943\n",
      "Training loss for batch 438 : 0.19144338369369507\n",
      "Training loss for batch 439 : 0.06869523972272873\n",
      "Training loss for batch 440 : 0.14577795565128326\n",
      "Training loss for batch 441 : 0.4597691297531128\n",
      "Training loss for batch 442 : 0.19148695468902588\n",
      "Training loss for batch 443 : 0.04523388668894768\n",
      "Training loss for batch 444 : 0.2875492572784424\n",
      "Training loss for batch 445 : 0.0927259549498558\n",
      "Training loss for batch 446 : 0.0355059839785099\n",
      "Training loss for batch 447 : 0.14422094821929932\n",
      "Training loss for batch 448 : 0.27321431040763855\n",
      "Training loss for batch 449 : 0.23173770308494568\n",
      "Training loss for batch 450 : 0.21798919141292572\n",
      "Training loss for batch 451 : 0.09499916434288025\n",
      "Training loss for batch 452 : 0.36443307995796204\n",
      "Training loss for batch 453 : 0.19134557247161865\n",
      "Training loss for batch 454 : 0.0542445071041584\n",
      "Training loss for batch 455 : 0.5177408456802368\n",
      "Training loss for batch 456 : 0.12011900544166565\n",
      "Training loss for batch 457 : 0.002477987203747034\n",
      "Training loss for batch 458 : 0.3653583228588104\n",
      "Training loss for batch 459 : 0.30031394958496094\n",
      "Training loss for batch 460 : 0.16259963810443878\n",
      "Training loss for batch 461 : 0.11884799599647522\n",
      "Training loss for batch 462 : 0.08810463547706604\n",
      "Training loss for batch 463 : 0.22289051115512848\n",
      "Training loss for batch 464 : 0.04103825241327286\n",
      "Training loss for batch 465 : 0.012199219316244125\n",
      "Training loss for batch 466 : 0.1740882694721222\n",
      "Training loss for batch 467 : 0.1320190727710724\n",
      "Training loss for batch 468 : 0.6238009333610535\n",
      "Training loss for batch 469 : 0.2251194566488266\n",
      "Training loss for batch 470 : 0.014582601375877857\n",
      "Training loss for batch 471 : 0.2046070098876953\n",
      "Training loss for batch 472 : 0.10279044508934021\n",
      "Training loss for batch 473 : 0.23891541361808777\n",
      "Training loss for batch 474 : 0.17581763863563538\n",
      "Training loss for batch 475 : 0.09709696471691132\n",
      "Training loss for batch 476 : 0.05098782479763031\n",
      "Training loss for batch 477 : 0.2557608485221863\n",
      "Training loss for batch 478 : 0.07332787662744522\n",
      "Training loss for batch 479 : 0.15111155807971954\n",
      "Training loss for batch 480 : 0.07765817642211914\n",
      "Training loss for batch 481 : 0.1547461897134781\n",
      "Training loss for batch 482 : 0.09522660076618195\n",
      "Training loss for batch 483 : 0.004990242421627045\n",
      "Training loss for batch 484 : 0.038269706070423126\n",
      "Training loss for batch 485 : 0.14987894892692566\n",
      "Training loss for batch 486 : 0.3649131953716278\n",
      "Training loss for batch 487 : 0.01639978215098381\n",
      "Training loss for batch 488 : 0.4034261405467987\n",
      "Training loss for batch 489 : 0.14769808948040009\n",
      "Training loss for batch 490 : 0.32147789001464844\n",
      "Training loss for batch 491 : 0.32936111092567444\n",
      "Training loss for batch 492 : 0.059659041464328766\n",
      "Training loss for batch 493 : 0.17439758777618408\n",
      "Training loss for batch 494 : 0.3957086205482483\n",
      "Training loss for batch 495 : 0.2189348191022873\n",
      "Training loss for batch 496 : 0.002025703666731715\n",
      "Training loss for batch 497 : 0.07934992760419846\n",
      "Training loss for batch 498 : 0.02606215514242649\n",
      "Training loss for batch 499 : 0.25259432196617126\n",
      "Training loss for batch 500 : 0.1780640184879303\n",
      "Training loss for batch 501 : 0.3219776153564453\n",
      "Training loss for batch 502 : 0.1633124202489853\n",
      "Training loss for batch 503 : 0.018517788499593735\n",
      "Training loss for batch 504 : 0.12295147776603699\n",
      "Training loss for batch 505 : 0.19025921821594238\n",
      "Training loss for batch 506 : 0.34568050503730774\n",
      "Training loss for batch 507 : 0.07900618761777878\n",
      "Training loss for batch 508 : 0.27882131934165955\n",
      "Training loss for batch 509 : 0.31436678767204285\n",
      "Training loss for batch 510 : 0.17197075486183167\n",
      "Training loss for batch 511 : 0.16773882508277893\n",
      "Training loss for batch 512 : 0.10031568259000778\n",
      "Training loss for batch 513 : 0.16431817412376404\n",
      "Training loss for batch 514 : 0.3647950291633606\n",
      "Training loss for batch 515 : 0.16504153609275818\n",
      "Training loss for batch 516 : 0.3743017315864563\n",
      "Training loss for batch 517 : 0.09498228132724762\n",
      "Training loss for batch 518 : 0.07789796590805054\n",
      "Training loss for batch 519 : 0.02280273847281933\n",
      "Training loss for batch 520 : 0.3009791076183319\n",
      "Training loss for batch 521 : 0.33284854888916016\n",
      "Training loss for batch 522 : 0.13950179517269135\n",
      "Training loss for batch 523 : 0.7143166661262512\n",
      "Training loss for batch 524 : 0.06582654267549515\n",
      "Training loss for batch 525 : 0.1337304562330246\n",
      "Training loss for batch 526 : 0.0\n",
      "Training loss for batch 527 : 0.30901917815208435\n",
      "Training loss for batch 528 : 0.08649735152721405\n",
      "Training loss for batch 529 : 0.4101851284503937\n",
      "Training loss for batch 530 : 0.0854952335357666\n",
      "Training loss for batch 531 : 0.00745276128873229\n",
      "Training loss for batch 532 : 0.2867748439311981\n",
      "Training loss for batch 533 : 0.19484974443912506\n",
      "Training loss for batch 534 : 0.12863647937774658\n",
      "Training loss for batch 535 : 0.5463618636131287\n",
      "Training loss for batch 536 : 0.601031482219696\n",
      "Training loss for batch 537 : 0.10346309095621109\n",
      "Training loss for batch 538 : 0.2204807698726654\n",
      "Training loss for batch 539 : 0.437593936920166\n",
      "Training loss for batch 540 : 0.21745161712169647\n",
      "Training loss for batch 541 : 0.15994490683078766\n",
      "Training loss for batch 542 : 0.21588090062141418\n",
      "Training loss for batch 543 : 0.12830641865730286\n",
      "Training loss for batch 544 : 0.2527614235877991\n",
      "Training loss for batch 545 : 0.01650099828839302\n",
      "Training loss for batch 546 : 0.3437379002571106\n",
      "Training loss for batch 547 : 0.051674406975507736\n",
      "Training loss for batch 548 : 0.22955222427845\n",
      "Training loss for batch 549 : 0.18333199620246887\n",
      "Training loss for batch 550 : 0.022484665736556053\n",
      "Training loss for batch 551 : 0.004095187410712242\n",
      "Training loss for batch 552 : 0.00021840135741513222\n",
      "Training loss for batch 553 : 0.17775696516036987\n",
      "Training loss for batch 554 : 0.13796181976795197\n",
      "Training loss for batch 555 : 0.07304395735263824\n",
      "Training loss for batch 556 : 0.2742308974266052\n",
      "Training loss for batch 557 : 0.06427831202745438\n",
      "Training loss for batch 558 : 0.27294832468032837\n",
      "Training loss for batch 559 : 0.045002907514572144\n",
      "Training loss for batch 560 : 0.1103009283542633\n",
      "Training loss for batch 561 : 0.3585751950740814\n",
      "Training loss for batch 562 : 0.26847773790359497\n",
      "Training loss for batch 563 : 0.0\n",
      "Training loss for batch 564 : 0.24169617891311646\n",
      "Training loss for batch 565 : 0.18039941787719727\n",
      "Training loss for batch 566 : 0.49079200625419617\n",
      "Training loss for batch 567 : 0.07151962071657181\n",
      "Training loss for batch 568 : 0.061590712517499924\n",
      "Training loss for batch 569 : 0.06294186413288116\n",
      "Training loss for batch 570 : 0.1327381581068039\n",
      "Training loss for batch 571 : 0.38062170147895813\n",
      "Training loss for batch 572 : 0.28347623348236084\n",
      "Training loss for batch 573 : 0.2464236170053482\n",
      "Training loss for batch 574 : 0.042809173464775085\n",
      "Training loss for batch 575 : 0.18874208629131317\n",
      "Training loss for batch 576 : 0.07766490429639816\n",
      "Training loss for batch 577 : 0.26182934641838074\n",
      "Training loss for batch 578 : 0.17894020676612854\n",
      "Training loss for batch 579 : 0.5112989544868469\n",
      "Training loss for batch 580 : 0.07476229965686798\n",
      "Training loss for batch 581 : 0.34097418189048767\n",
      "Training loss for batch 582 : 0.2131255716085434\n",
      "Training loss for batch 583 : 0.2748185694217682\n",
      "Training loss for batch 584 : 0.09713340550661087\n",
      "Training loss for batch 585 : 0.1815817505121231\n",
      "Training loss for batch 586 : 0.12445922195911407\n",
      "Training loss for batch 587 : 0.05633317306637764\n",
      "Training loss for batch 588 : 0.16462160646915436\n",
      "Training loss for batch 589 : 0.04889337345957756\n",
      "Training loss for batch 590 : 0.2861045002937317\n",
      "Training loss for batch 591 : 0.16765306890010834\n",
      "Training loss for batch 592 : 0.3941822350025177\n",
      "Training loss for batch 593 : 0.10360255837440491\n",
      "Training loss for batch 594 : 0.10112913697957993\n",
      "Training loss for batch 595 : 0.4484404921531677\n",
      "Training loss for batch 596 : 0.17562538385391235\n",
      "Training loss for batch 597 : 0.08798494935035706\n",
      "Training loss for batch 598 : 0.018473127856850624\n",
      "Training loss for batch 599 : 0.40358108282089233\n",
      "Training loss for batch 600 : 0.2820402681827545\n",
      "Training loss for batch 601 : 0.16508863866329193\n",
      "Training loss for batch 602 : 0.20877350866794586\n",
      "Training loss for batch 603 : 0.2583387792110443\n",
      "Training loss for batch 604 : 0.3316320478916168\n",
      "Training loss for batch 605 : 0.2540866434574127\n",
      "Training loss for batch 606 : 0.4224095046520233\n",
      "Training loss for batch 607 : 0.24755151569843292\n",
      "Training loss for batch 608 : 0.20431166887283325\n",
      "Training loss for batch 609 : 0.13837547600269318\n",
      "Training loss for batch 610 : 0.1845722645521164\n",
      "Training loss for batch 611 : 0.07695508003234863\n",
      "Training loss for batch 612 : 0.12129715085029602\n",
      "Training loss for batch 613 : 0.04051925241947174\n",
      "Training loss for batch 614 : 0.10581067204475403\n",
      "Training loss for batch 615 : 0.10991766303777695\n",
      "Training loss for batch 616 : 0.24848666787147522\n",
      "Training loss for batch 617 : 0.11915910989046097\n",
      "Training loss for batch 618 : 0.2513355612754822\n",
      "Training loss for batch 619 : 0.15873970091342926\n",
      "Training loss for batch 620 : 0.4756719768047333\n",
      "Training loss for batch 621 : 0.06319180130958557\n",
      "Training loss for batch 622 : 0.01385484542697668\n",
      "Training loss for batch 623 : 0.11543744057416916\n",
      "Training loss for batch 624 : 0.021342579275369644\n",
      "Training loss for batch 625 : 0.08265142887830734\n",
      "Training loss for batch 626 : 0.035241082310676575\n",
      "Training loss for batch 627 : 0.001497579738497734\n",
      "Training loss for batch 628 : 0.13131161034107208\n",
      "Training loss for batch 629 : 0.16788356006145477\n",
      "Training loss for batch 630 : 0.041765496134757996\n",
      "Training loss for batch 631 : 0.10965254157781601\n",
      "Training loss for batch 632 : 0.06906094402074814\n",
      "Training loss for batch 633 : 0.08520611375570297\n",
      "Training loss for batch 634 : 0.021559279412031174\n",
      "Training loss for batch 635 : 0.3877136707305908\n",
      "Training loss for batch 636 : 0.1801273077726364\n",
      "Training loss for batch 637 : 0.011240526102483273\n",
      "Training loss for batch 638 : 0.17607896029949188\n",
      "Training loss for batch 639 : 0.2765052616596222\n",
      "Training loss for batch 640 : 0.42243558168411255\n",
      "Training loss for batch 641 : 0.36335450410842896\n",
      "Training loss for batch 642 : 0.23226124048233032\n",
      "Training loss for batch 643 : 0.12441907823085785\n",
      "Training loss for batch 644 : 0.16729335486888885\n",
      "Training loss for batch 645 : 0.4100828468799591\n",
      "Training loss for batch 646 : 0.05953366309404373\n",
      "Training loss for batch 647 : 0.19019941985607147\n",
      "Training loss for batch 648 : 0.05325913056731224\n",
      "Training loss for batch 649 : 0.1407727152109146\n",
      "Training loss for batch 650 : 0.35963356494903564\n",
      "Training loss for batch 651 : 0.041789356619119644\n",
      "Training loss for batch 652 : 0.09974265843629837\n",
      "Training loss for batch 653 : 0.326925665140152\n",
      "Training loss for batch 654 : 0.28839999437332153\n",
      "Training loss for batch 655 : 0.06657294183969498\n",
      "Training loss for batch 656 : 0.11900614202022552\n",
      "Training loss for batch 657 : 0.0\n",
      "Training loss for batch 658 : 0.10260914266109467\n",
      "Training loss for batch 659 : 0.16267940402030945\n",
      "Training loss for batch 660 : 0.06043144688010216\n",
      "Training loss for batch 661 : 0.01612173579633236\n",
      "Training loss for batch 662 : 0.24227017164230347\n",
      "Training loss for batch 663 : 0.35591623187065125\n",
      "Training loss for batch 664 : 0.03260492533445358\n",
      "Training loss for batch 665 : 0.15483328700065613\n",
      "Training loss for batch 666 : 0.0\n",
      "Training loss for batch 667 : 0.024491820484399796\n",
      "Training loss for batch 668 : 0.10636670142412186\n",
      "Training loss for batch 669 : 0.7016049027442932\n",
      "Training loss for batch 670 : 0.3192949593067169\n",
      "Training loss for batch 671 : 0.04183525592088699\n",
      "Training loss for batch 672 : 0.5122957825660706\n",
      "Training loss for batch 673 : 0.3927710950374603\n",
      "Training loss for batch 674 : 0.06489387899637222\n",
      "Training loss for batch 675 : 0.2896578013896942\n",
      "Training loss for batch 676 : 0.4679766297340393\n",
      "Training loss for batch 677 : 0.10089621692895889\n",
      "Training loss for batch 678 : 0.06188889220356941\n",
      "Training loss for batch 679 : 0.1517333984375\n",
      "Training loss for batch 680 : 0.13806681334972382\n",
      "Training loss for batch 681 : 0.002897560829296708\n",
      "Training loss for batch 682 : 0.19127890467643738\n",
      "Training loss for batch 683 : 0.7146807312965393\n",
      "Training loss for batch 684 : 0.14110171794891357\n",
      "Training loss for batch 685 : 0.029929377138614655\n",
      "Training loss for batch 686 : 0.05115410313010216\n",
      "Training loss for batch 687 : 0.3923208713531494\n",
      "Training loss for batch 688 : 0.0495435893535614\n",
      "Training loss for batch 689 : 0.24033145606517792\n",
      "Training loss for batch 690 : 0.1350957453250885\n",
      "Training loss for batch 691 : 0.23926721513271332\n",
      "Training loss for batch 692 : 0.13685083389282227\n",
      "Training loss for batch 693 : 0.20477083325386047\n",
      "Training loss for batch 694 : 0.008710762485861778\n",
      "Training loss for batch 695 : 0.1285587102174759\n",
      "Training loss for batch 696 : 0.15335603058338165\n",
      "Training loss for batch 697 : 0.07124171406030655\n",
      "Training loss for batch 698 : 0.08369765430688858\n",
      "Training loss for batch 699 : 0.6488482356071472\n",
      "Training loss for batch 700 : 0.1614362597465515\n",
      "Training loss for batch 701 : 0.3720431923866272\n",
      "Training loss for batch 702 : 0.346106618642807\n",
      "Training loss for batch 703 : 0.11479663103818893\n",
      "Training loss for batch 704 : 0.04322357848286629\n",
      "Training loss for batch 705 : 0.09393914043903351\n",
      "Training loss for batch 706 : 0.15975479781627655\n",
      "Training loss for batch 707 : 0.3009939193725586\n",
      "Training loss for batch 708 : 0.12438739836215973\n",
      "Training loss for batch 709 : 0.0506313256919384\n",
      "Training loss for batch 710 : 0.22417236864566803\n",
      "Training loss for batch 711 : 0.015397889539599419\n",
      "Training loss for batch 712 : 0.10328985750675201\n",
      "Training loss for batch 713 : 0.18470633029937744\n",
      "Training loss for batch 714 : 0.1184813529253006\n",
      "Training loss for batch 715 : 0.20234054327011108\n",
      "Training loss for batch 716 : 0.12441486865282059\n",
      "Training loss for batch 717 : 0.20579956471920013\n",
      "Training loss for batch 718 : 0.03748439624905586\n",
      "Training loss for batch 719 : 0.13663257658481598\n",
      "Training loss for batch 720 : 0.07502816617488861\n",
      "Training loss for batch 721 : 0.20503780245780945\n",
      "Training loss for batch 722 : 0.014482667669653893\n",
      "Training loss for batch 723 : 0.09977376461029053\n",
      "Training loss for batch 724 : 0.2853669226169586\n",
      "Training loss for batch 725 : 0.3424261808395386\n",
      "Training loss for batch 726 : 0.3928995132446289\n",
      "Training loss for batch 727 : 0.09949100762605667\n",
      "Training loss for batch 728 : 0.3152559995651245\n",
      "Training loss for batch 729 : 0.5719589591026306\n",
      "Training loss for batch 730 : 0.07437825947999954\n",
      "Training loss for batch 731 : 0.10317985713481903\n",
      "Training loss for batch 732 : 0.28542011976242065\n",
      "Training loss for batch 733 : 0.012858843430876732\n",
      "Training loss for batch 734 : 0.04241195321083069\n",
      "Training loss for batch 735 : 0.2501944899559021\n",
      "Training loss for batch 736 : 0.14172327518463135\n",
      "Training loss for batch 737 : 0.14383289217948914\n",
      "Training loss for batch 738 : 0.425857812166214\n",
      "Training loss for batch 739 : 0.0637790709733963\n",
      "Training loss for batch 740 : 0.10326828807592392\n",
      "Training loss for batch 741 : 0.3865680396556854\n",
      "Training loss for batch 742 : 0.1403903216123581\n",
      "Training loss for batch 743 : 0.19255545735359192\n",
      "Training loss for batch 744 : 0.12634116411209106\n",
      "Training loss for batch 745 : 0.10568474233150482\n",
      "Training loss for batch 746 : 0.13035237789154053\n",
      "Training loss for batch 747 : 0.3387013375759125\n",
      "Training loss for batch 748 : 0.07865665853023529\n",
      "Training loss for batch 749 : 0.05357382446527481\n",
      "Training loss for batch 750 : 0.25719162821769714\n",
      "Training loss for batch 751 : 0.0538262315094471\n",
      "Training loss for batch 752 : 0.2656468152999878\n",
      "Training loss for batch 753 : 0.3253329396247864\n",
      "Training loss for batch 754 : 0.2943958342075348\n",
      "Training loss for batch 755 : 0.09764619171619415\n",
      "Training loss for batch 756 : 0.06527617573738098\n",
      "Training loss for batch 757 : 0.30942806601524353\n",
      "Training loss for batch 758 : 0.03715953975915909\n",
      "Training loss for batch 759 : 0.19631768763065338\n",
      "Training loss for batch 760 : 0.11774086207151413\n",
      "Training loss for batch 761 : 0.29371294379234314\n",
      "Training loss for batch 762 : 0.10037205368280411\n",
      "Training loss for batch 763 : 0.47220098972320557\n",
      "Training loss for batch 764 : 0.2352798879146576\n",
      "Training loss for batch 765 : 0.29157230257987976\n",
      "Training loss for batch 766 : 0.05771705508232117\n",
      "Training loss for batch 767 : 0.20447476208209991\n",
      "Training loss for batch 768 : 0.20048920810222626\n",
      "Training loss for batch 769 : 0.25344571471214294\n",
      "Training loss for batch 770 : 0.0576288104057312\n",
      "Training loss for batch 771 : 0.08109990507364273\n",
      "Training loss for batch 772 : 0.10018029063940048\n",
      "Training loss for batch 773 : 0.15992070734500885\n",
      "Training loss for batch 774 : 0.139301136136055\n",
      "Training loss for batch 775 : 0.287176251411438\n",
      "Training loss for batch 776 : 0.0035287439823150635\n",
      "Training loss for batch 777 : 0.27277490496635437\n",
      "Training loss for batch 778 : 0.03811049833893776\n",
      "Training loss for batch 779 : 0.0\n",
      "Training loss for batch 780 : 0.06863471120595932\n",
      "Training loss for batch 781 : 0.2121841162443161\n",
      "Training loss for batch 782 : 0.28034988045692444\n",
      "Training loss for batch 783 : 0.15785662829875946\n",
      "Training loss for batch 784 : 0.12241379916667938\n",
      "Training loss for batch 785 : 0.10216300934553146\n",
      "Training loss for batch 786 : 0.25430601835250854\n",
      "Training loss for batch 787 : 0.18391163647174835\n",
      "Training loss for batch 788 : 0.11326270550489426\n",
      "Training loss for batch 789 : 0.3454338312149048\n",
      "Training loss for batch 790 : 0.38030073046684265\n",
      "Training loss for batch 791 : 0.19643950462341309\n",
      "Training loss for batch 792 : 0.19280724227428436\n",
      "Training loss for batch 793 : 0.0701318085193634\n",
      "Training loss for batch 794 : 0.11451548337936401\n",
      "Training loss for batch 795 : 0.49915146827697754\n",
      "Training loss for batch 796 : 0.08142217993736267\n",
      "Training loss for batch 797 : 0.04123207926750183\n",
      "Training loss for batch 798 : 0.17522715032100677\n",
      "Training loss for batch 799 : 0.2723613679409027\n",
      "Training loss for batch 800 : 0.4312373697757721\n",
      "Training loss for batch 801 : 0.17386971414089203\n",
      "Training loss for batch 802 : 0.23009611666202545\n",
      "Training loss for batch 803 : 0.2684498429298401\n",
      "Training loss for batch 804 : 0.07800798863172531\n",
      "Training loss for batch 805 : 0.04514065757393837\n",
      "Training loss for batch 806 : 0.35884231328964233\n",
      "Training loss for batch 807 : 0.3291342258453369\n",
      "Training loss for batch 808 : 0.20487143099308014\n",
      "Training loss for batch 809 : 0.1643732637166977\n",
      "Training loss for batch 810 : 0.07042231410741806\n",
      "Training loss for batch 811 : 0.06787759065628052\n",
      "Training loss for batch 812 : 0.0781811997294426\n",
      "Training loss for batch 813 : 0.006049089599400759\n",
      "Training loss for batch 814 : 0.20677323639392853\n",
      "Training loss for batch 815 : 0.13965123891830444\n",
      "Training loss for batch 816 : 0.1544351577758789\n",
      "Training loss for batch 817 : 0.16301319003105164\n",
      "Training loss for batch 818 : 0.34556517004966736\n",
      "Training loss for batch 819 : 0.06007269769906998\n",
      "Training loss for batch 820 : 0.19818061590194702\n",
      "Training loss for batch 821 : 0.013078136369585991\n",
      "Training loss for batch 822 : 0.32281869649887085\n",
      "Training loss for batch 823 : 0.186964750289917\n",
      "Training loss for batch 824 : 0.12896540760993958\n",
      "Training loss for batch 825 : 0.14214618504047394\n",
      "Training loss for batch 826 : 0.21781660616397858\n",
      "Training loss for batch 827 : 0.0747733861207962\n",
      "Training loss for batch 828 : 0.43641167879104614\n",
      "Training loss for batch 829 : 0.41179099678993225\n",
      "Training loss for batch 830 : 0.38078930974006653\n",
      "Training loss for batch 831 : 0.1083594486117363\n",
      "Training loss for batch 832 : 0.15168459713459015\n",
      "Training loss for batch 833 : 0.06094331294298172\n",
      "Training loss for batch 834 : 0.1452045887708664\n",
      "Training loss for batch 835 : 0.03586438670754433\n",
      "Training loss for batch 836 : 0.3146868646144867\n",
      "Training loss for batch 837 : 0.1771618276834488\n",
      "Training loss for batch 838 : 0.289569616317749\n",
      "Training loss for batch 839 : 0.009107736870646477\n",
      "Training loss for batch 840 : 0.013246324844658375\n",
      "Training loss for batch 841 : 0.008515676483511925\n",
      "Training loss for batch 842 : 0.0772055983543396\n",
      "Training loss for batch 843 : 0.0973593145608902\n",
      "Training loss for batch 844 : 0.20429234206676483\n",
      "Training loss for batch 845 : 0.07959946990013123\n",
      "Training loss for batch 846 : 0.05166715383529663\n",
      "Training loss for batch 847 : 0.19944489002227783\n",
      "Training loss for batch 848 : 0.10915844887495041\n",
      "Training loss for batch 849 : 0.2768994867801666\n",
      "Training loss for batch 850 : 0.027557678520679474\n",
      "Training loss for batch 851 : 0.05602296441793442\n",
      "Training loss for batch 852 : 0.1887563019990921\n",
      "Training loss for batch 853 : 0.08183630555868149\n",
      "Training loss for batch 854 : 0.024785717949271202\n",
      "Training loss for batch 855 : 0.17164330184459686\n",
      "Training loss for batch 856 : 0.11270058900117874\n",
      "Training loss for batch 857 : 0.0030961534939706326\n",
      "Training loss for batch 858 : 0.18420889973640442\n",
      "Training loss for batch 859 : 0.09545794129371643\n",
      "Training loss for batch 860 : 0.267228901386261\n",
      "Training loss for batch 861 : 0.036287274211645126\n",
      "Training loss for batch 862 : 0.03066837042570114\n",
      "Training loss for batch 863 : 0.14017578959465027\n",
      "Training loss for batch 864 : 0.051835574209690094\n",
      "Training loss for batch 865 : 0.0\n",
      "Training loss for batch 866 : 0.23391762375831604\n",
      "Training loss for batch 867 : 0.42789167165756226\n",
      "Training loss for batch 868 : 0.139587864279747\n",
      "Training loss for batch 869 : 0.009886364452540874\n",
      "Training loss for batch 870 : 0.11196980625391006\n",
      "Training loss for batch 871 : 0.3731919825077057\n",
      "Training loss for batch 872 : 0.34770822525024414\n",
      "Training loss for batch 873 : 0.2449491322040558\n",
      "Training loss for batch 874 : 0.13685297966003418\n",
      "Training loss for batch 875 : 0.045548561960458755\n",
      "Training loss for batch 876 : 0.25321948528289795\n",
      "Training loss for batch 877 : 0.0319441594183445\n",
      "Training loss for batch 878 : 0.26090195775032043\n",
      "Training loss for batch 879 : 0.16568608582019806\n",
      "Training loss for batch 880 : 0.5324378609657288\n",
      "Training loss for batch 881 : 0.15481925010681152\n",
      "Training loss for batch 882 : 0.1272229701280594\n",
      "Training loss for batch 883 : 0.049304500222206116\n",
      "Training loss for batch 884 : 0.06717190146446228\n",
      "Training loss for batch 885 : 0.22295808792114258\n",
      "Training loss for batch 886 : 0.26785728335380554\n",
      "Training loss for batch 887 : 0.1760340929031372\n",
      "Training loss for batch 888 : 0.042558059096336365\n",
      "Training loss for batch 889 : 0.3579106628894806\n",
      "Training loss for batch 890 : 0.07151084393262863\n",
      "Training loss for batch 891 : 0.07309591770172119\n",
      "Training loss for batch 892 : 0.38501811027526855\n",
      "Training loss for batch 893 : 0.3109959065914154\n",
      "Training loss for batch 894 : 0.09192489087581635\n",
      "Training loss for batch 895 : 0.14659000933170319\n",
      "Training loss for batch 896 : 0.15957707166671753\n",
      "Training loss for batch 897 : 0.062448516488075256\n",
      "Training loss for batch 898 : 0.10877446085214615\n",
      "Training loss for batch 899 : 0.09989512711763382\n",
      "Training loss for batch 900 : 0.25464457273483276\n",
      "Training loss for batch 901 : 0.07517663389444351\n",
      "Training loss for batch 902 : 0.14857620000839233\n",
      "Training loss for batch 903 : 0.3907332718372345\n",
      "Training loss for batch 904 : 0.33856821060180664\n",
      "Training loss for batch 905 : 0.45463722944259644\n",
      "Training loss for batch 906 : 0.06919503957033157\n",
      "Training loss for batch 907 : 0.329864501953125\n",
      "Training loss for batch 908 : 0.09664806723594666\n",
      "Training loss for batch 909 : 0.2511959969997406\n",
      "Training loss for batch 910 : 0.42046278715133667\n",
      "Training loss for batch 911 : 0.11443527042865753\n",
      "Training loss for batch 912 : 0.04759969189763069\n",
      "Training loss for batch 913 : 0.4960865378379822\n",
      "Training loss for batch 914 : 0.3087821900844574\n",
      "Training loss for batch 915 : 0.3384489417076111\n",
      "Training loss for batch 916 : 0.2851596772670746\n",
      "Training loss for batch 917 : 0.0\n",
      "Training loss for batch 918 : 0.025604546070098877\n",
      "Training loss for batch 919 : 0.16641035676002502\n",
      "Training loss for batch 920 : 0.18413226306438446\n",
      "Training loss for batch 921 : 0.05525191128253937\n",
      "Training loss for batch 922 : 0.06966686248779297\n",
      "Training loss for batch 923 : 0.10089949518442154\n",
      "Training loss for batch 924 : 0.10148809105157852\n",
      "Training loss for batch 925 : 0.6031701564788818\n",
      "Training loss for batch 926 : 0.13649265468120575\n",
      "Training loss for batch 927 : 0.0982171893119812\n",
      "Training loss for batch 928 : 0.2367299348115921\n",
      "Training loss for batch 929 : 0.12174355983734131\n",
      "Training loss for batch 930 : 0.153175488114357\n",
      "Training loss for batch 931 : 0.3737245500087738\n",
      "Training loss for batch 932 : 0.1827799677848816\n",
      "Training loss for batch 933 : 0.05816143751144409\n",
      "Training loss for batch 934 : 0.16181863844394684\n",
      "Training loss for batch 935 : 0.2830232083797455\n",
      "Training loss for batch 936 : 0.12858927249908447\n",
      "Training loss for batch 937 : 0.4010317325592041\n",
      "Training loss for batch 938 : 0.06692580878734589\n",
      "Training loss for batch 939 : 0.4412766396999359\n",
      "Training loss for batch 940 : 0.01190338097512722\n",
      "Training loss for batch 941 : 0.23898649215698242\n",
      "Training loss for batch 942 : 0.20674489438533783\n",
      "Training loss for batch 943 : 0.08969986438751221\n",
      "Training loss for batch 944 : 0.07731886953115463\n",
      "Training loss for batch 945 : 0.16837671399116516\n",
      "Training loss for batch 946 : 0.020565103739500046\n",
      "Training loss for batch 947 : 0.10492055118083954\n",
      "Training loss for batch 948 : 0.07730168104171753\n",
      "Training loss for batch 949 : 0.20333072543144226\n",
      "Training loss for batch 950 : 0.08779320865869522\n",
      "Training loss for batch 951 : 0.2411162108182907\n",
      "Training loss for batch 952 : 0.2257421463727951\n",
      "Training loss for batch 953 : 0.16063033044338226\n",
      "Training loss for batch 954 : 0.6439771056175232\n",
      "Training loss for batch 955 : 0.36557260155677795\n",
      "Training loss for batch 956 : 0.17598925530910492\n",
      "Training loss for batch 957 : 0.36884206533432007\n",
      "Training loss for batch 958 : 0.009852520190179348\n",
      "Training loss for batch 959 : 0.12267692387104034\n",
      "Training loss for batch 960 : 0.18133887648582458\n",
      "Training loss for batch 961 : 0.025025205686688423\n",
      "Training loss for batch 962 : 0.06126118823885918\n",
      "Training loss for batch 963 : 0.10976612567901611\n",
      "Training loss for batch 964 : 0.27308598160743713\n",
      "Training loss for batch 965 : 0.09733495861291885\n",
      "Training loss for batch 966 : 0.10175096243619919\n",
      "Training loss for batch 967 : 0.03744734078645706\n",
      "Training loss for batch 968 : 0.2887111306190491\n",
      "Training loss for batch 969 : 3.591001586755738e-05\n",
      "Training loss for batch 970 : 0.14355944097042084\n",
      "Training loss for batch 971 : 0.3301759958267212\n",
      "Training loss for batch 972 : 0.06586428731679916\n",
      "Training loss for batch 973 : 0.3644537627696991\n",
      "Training loss for batch 974 : 0.1449948102235794\n",
      "Training loss for batch 975 : 0.10238035768270493\n",
      "Training loss for batch 976 : 0.42140430212020874\n",
      "Training loss for batch 977 : 0.16177062690258026\n",
      "Training loss for batch 978 : 0.0967470034956932\n",
      "Training loss for batch 979 : 0.3079621195793152\n",
      "Training loss for batch 980 : 0.040666501969099045\n",
      "Training loss for batch 981 : 0.3756830096244812\n",
      "Training loss for batch 982 : 0.0415816605091095\n",
      "Training loss for batch 983 : 0.3344666659832001\n",
      "Training loss for batch 984 : 0.14176520705223083\n",
      "Training loss for batch 985 : 0.139100581407547\n",
      "Training loss for batch 986 : 0.2172120064496994\n",
      "Training loss for batch 987 : 0.048282284289598465\n",
      "Training loss for batch 988 : 0.03410143032670021\n",
      "Training loss for batch 989 : 0.3029954433441162\n",
      "Training loss for batch 990 : 0.030768636614084244\n",
      "Training loss for batch 991 : 0.11687307804822922\n",
      "Training loss for batch 992 : 0.07954829186201096\n",
      "Training loss for batch 993 : 0.12940581142902374\n",
      "Training loss for batch 994 : 0.16457699239253998\n",
      "Training loss for batch 995 : 0.014771735295653343\n",
      "Training loss for batch 996 : 0.47523748874664307\n",
      "Training loss for batch 997 : 0.24257652461528778\n",
      "Training loss for batch 998 : 0.058459650725126266\n",
      "Training loss for batch 999 : 0.11038538068532944\n",
      "Training loss for batch 1000 : 0.14050370454788208\n",
      "Training loss for batch 1001 : 0.2218277007341385\n",
      "Training loss for batch 1002 : 0.07392236590385437\n",
      "Training loss for batch 1003 : 0.1296425759792328\n",
      "Training loss for batch 1004 : 0.13646700978279114\n",
      "Training loss for batch 1005 : 0.007296582218259573\n",
      "Training loss for batch 1006 : 0.35409653186798096\n",
      "Training loss for batch 1007 : 0.06794477254152298\n",
      "Training loss for batch 1008 : 0.11130945384502411\n",
      "Training loss for batch 1009 : 0.11681205034255981\n",
      "Training loss for batch 1010 : 0.011189660057425499\n",
      "Training loss for batch 1011 : 0.07404471188783646\n",
      "Training loss for batch 1012 : 0.03219977393746376\n",
      "Training loss for batch 1013 : 0.03766701743006706\n",
      "Training loss for batch 1014 : 0.5429506897926331\n",
      "Training loss for batch 1015 : 0.10743649303913116\n",
      "Training loss for batch 1016 : 0.08338432013988495\n",
      "Training loss for batch 1017 : 0.2998395264148712\n",
      "Training loss for batch 1018 : 0.09395761787891388\n",
      "Training loss for batch 1019 : 0.11369714140892029\n",
      "Training loss for batch 1020 : 0.3092469274997711\n",
      "Training loss for batch 1021 : 0.1969490796327591\n",
      "Training loss for batch 1022 : 0.166380375623703\n",
      "Training loss for batch 1023 : 0.10395539551973343\n",
      "Training loss for batch 1024 : 0.4022638499736786\n",
      "Training loss for batch 1025 : 0.17551688849925995\n",
      "Training loss for batch 1026 : 0.04321744665503502\n",
      "Training loss for batch 1027 : 0.3353707492351532\n",
      "Training loss for batch 1028 : 0.40213876962661743\n",
      "Training loss for batch 1029 : 0.032426465302705765\n",
      "Training loss for batch 1030 : 0.04188207909464836\n",
      "Training loss for batch 1031 : 0.2584449350833893\n",
      "Training loss for batch 1032 : 0.06348102539777756\n",
      "Training loss for batch 1033 : 7.092989108059555e-05\n",
      "Training loss for batch 1034 : 0.26819202303886414\n",
      "Training loss for batch 1035 : 0.10484347492456436\n",
      "Training loss for batch 1036 : 0.02410726808011532\n",
      "Training loss for batch 1037 : 0.36160778999328613\n",
      "Training loss for batch 1038 : 0.04476621747016907\n",
      "Training loss for batch 1039 : 0.08268550038337708\n",
      "Training loss for batch 1040 : 0.08889635652303696\n",
      "Training loss for batch 1041 : 0.12450262904167175\n",
      "Training loss for batch 1042 : 0.04507523775100708\n",
      "Training loss for batch 1043 : 0.31204086542129517\n",
      "Training loss for batch 1044 : 0.020899344235658646\n",
      "Training loss for batch 1045 : 0.06269507110118866\n",
      "Training loss for batch 1046 : 0.21714036166667938\n",
      "Training loss for batch 1047 : 0.12753230333328247\n",
      "Training loss for batch 1048 : 0.25125789642333984\n",
      "Training loss for batch 1049 : 0.20649851858615875\n",
      "Training loss for batch 1050 : 0.11646179854869843\n",
      "Training loss for batch 1051 : 0.02371327206492424\n",
      "Training loss for batch 1052 : 0.16609466075897217\n",
      "Training loss for batch 1053 : 0.008277454413473606\n",
      "Training loss for batch 1054 : 0.14175787568092346\n",
      "Training loss for batch 1055 : 0.0404498465359211\n",
      "Training loss for batch 1056 : 0.3768051862716675\n",
      "Training loss for batch 1057 : 0.0730758011341095\n",
      "Training loss for batch 1058 : 0.2688574194908142\n",
      "Training loss for batch 1059 : 0.003189579350873828\n",
      "Training loss for batch 1060 : 0.21763965487480164\n",
      "Training loss for batch 1061 : 0.3716934025287628\n",
      "Training loss for batch 1062 : 0.3629213571548462\n",
      "Training loss for batch 1063 : 0.32734009623527527\n",
      "Training loss for batch 1064 : 0.31030169129371643\n",
      "Training loss for batch 1065 : 0.0035768128000199795\n",
      "Training loss for batch 1066 : 0.5208938121795654\n",
      "Training loss for batch 1067 : 0.02920457534492016\n",
      "Training loss for batch 1068 : 0.16810160875320435\n",
      "Training loss for batch 1069 : 0.16618779301643372\n",
      "Training loss for batch 1070 : 0.3494381904602051\n",
      "Training loss for batch 1071 : 0.07268159836530685\n",
      "Training loss for batch 1072 : 0.3728342056274414\n",
      "Training loss for batch 1073 : 0.0993739441037178\n",
      "Training loss for batch 1074 : 0.2057262659072876\n",
      "Training loss for batch 1075 : 0.24304112792015076\n",
      "Training loss for batch 1076 : 0.13624410331249237\n",
      "Training loss for batch 1077 : 0.3113710880279541\n",
      "Training loss for batch 1078 : 0.5688639879226685\n",
      "Training loss for batch 1079 : 0.13345938920974731\n",
      "Training loss for batch 1080 : 0.13127706944942474\n",
      "Training loss for batch 1081 : 0.061980895698070526\n",
      "Training loss for batch 1082 : 0.03239315003156662\n",
      "Training loss for batch 1083 : 0.3637896478176117\n",
      "Training loss for batch 1084 : 0.15298263728618622\n",
      "Training loss for batch 1085 : 0.26903972029685974\n",
      "Training loss for batch 1086 : 0.386044442653656\n",
      "Training loss for batch 1087 : 0.5318567752838135\n",
      "Training loss for batch 1088 : 0.32820945978164673\n",
      "Training loss for batch 1089 : 0.042569469660520554\n",
      "Training loss for batch 1090 : 0.267183393239975\n",
      "Training loss for batch 1091 : 0.18446826934814453\n",
      "Training loss for batch 1092 : 0.06708359718322754\n",
      "Training loss for batch 1093 : 0.2534615993499756\n",
      "Training loss for batch 1094 : 0.2598974406719208\n",
      "Training loss for batch 1095 : 0.27413201332092285\n",
      "Training loss for batch 1096 : 0.38694512844085693\n",
      "Training loss for batch 1097 : 0.28052181005477905\n",
      "Training loss for batch 1098 : 0.17783387005329132\n",
      "Training loss for batch 1099 : 0.08903014659881592\n",
      "Training loss for batch 1100 : 0.0659068301320076\n",
      "Training loss for batch 1101 : 0.11266837269067764\n",
      "Training loss for batch 1102 : 0.20612765848636627\n",
      "Training loss for batch 1103 : 0.2098744511604309\n",
      "Training loss for batch 1104 : 0.0952121838927269\n",
      "Training loss for batch 1105 : 0.09382229298353195\n",
      "Training loss for batch 1106 : 0.26100513339042664\n",
      "Training loss for batch 1107 : 0.39815399050712585\n",
      "Training loss for batch 1108 : 0.09612022340297699\n",
      "Training loss for batch 1109 : 0.07606173306703568\n",
      "Training loss for batch 1110 : 0.22385495901107788\n",
      "Training loss for batch 1111 : 0.19310015439987183\n",
      "Training loss for batch 1112 : 0.38935163617134094\n",
      "Training loss for batch 1113 : 0.01687997579574585\n",
      "Training loss for batch 1114 : 0.03849774971604347\n",
      "Training loss for batch 1115 : 0.0324372723698616\n",
      "Training loss for batch 1116 : 0.06620161235332489\n",
      "Training loss for batch 1117 : 0.18774987757205963\n",
      "Training loss for batch 1118 : 0.11878202855587006\n",
      "Training loss for batch 1119 : 9.318374623035197e-07\n",
      "Training loss for batch 1120 : 0.04080658406019211\n",
      "Training loss for batch 1121 : 0.033137671649456024\n",
      "Training loss for batch 1122 : 0.292922705411911\n",
      "Training loss for batch 1123 : 0.023271411657333374\n",
      "Training loss for batch 1124 : 0.22956064343452454\n",
      "Training loss for batch 1125 : 0.30628907680511475\n",
      "Training loss for batch 1126 : 0.5524271130561829\n",
      "Training loss for batch 1127 : 0.09679797291755676\n",
      "Training loss for batch 1128 : 0.3570462763309479\n",
      "Training loss for batch 1129 : 0.22646689414978027\n",
      "Training loss for batch 1130 : 0.22788162529468536\n",
      "Training loss for batch 1131 : 0.19944050908088684\n",
      "Training loss for batch 1132 : 0.06797793507575989\n",
      "Training loss for batch 1133 : 0.10003636032342911\n",
      "Training loss for batch 1134 : 0.12257314473390579\n",
      "Training loss for batch 1135 : 0.04489850252866745\n",
      "Training loss for batch 1136 : 0.1993943303823471\n",
      "Training loss for batch 1137 : 0.005865504499524832\n",
      "Training loss for batch 1138 : 0.07232372462749481\n",
      "Training loss for batch 1139 : 0.11135019361972809\n",
      "Training loss for batch 1140 : 0.08476101607084274\n",
      "Training loss for batch 1141 : 0.14341376721858978\n",
      "Training loss for batch 1142 : 0.40582215785980225\n",
      "Training loss for batch 1143 : 0.28913238644599915\n",
      "Training loss for batch 1144 : 0.2792379558086395\n",
      "Training loss for batch 1145 : 0.1278676688671112\n",
      "Training loss for batch 1146 : 0.13974887132644653\n",
      "Training loss for batch 1147 : 0.062189698219299316\n",
      "Training loss for batch 1148 : 0.10838489979505539\n",
      "Training loss for batch 1149 : 0.07384128123521805\n",
      "Training loss for batch 1150 : 0.24980628490447998\n",
      "Training loss for batch 1151 : 0.02420998178422451\n",
      "Training loss for batch 1152 : 0.06528478860855103\n",
      "Training loss for batch 1153 : 0.15430212020874023\n",
      "Training loss for batch 1154 : 0.0\n",
      "Training loss for batch 1155 : 0.0509631484746933\n",
      "Training loss for batch 1156 : 0.11144115775823593\n",
      "Training loss for batch 1157 : 0.18284356594085693\n",
      "Training loss for batch 1158 : 0.0421210378408432\n",
      "Training loss for batch 1159 : 0.07626906782388687\n",
      "Training loss for batch 1160 : 0.033535297960042953\n",
      "Training loss for batch 1161 : 0.1347503364086151\n",
      "Training loss for batch 1162 : 0.060291144996881485\n",
      "Training loss for batch 1163 : 0.1931593120098114\n",
      "Training loss for batch 1164 : 0.603664219379425\n",
      "Training loss for batch 1165 : 0.04434344917535782\n",
      "Training loss for batch 1166 : 0.060834456235170364\n",
      "Training loss for batch 1167 : 0.24770085513591766\n",
      "Training loss for batch 1168 : 0.08302353322505951\n",
      "Training loss for batch 1169 : 0.3655407726764679\n",
      "Training loss for batch 1170 : 0.18714986741542816\n",
      "Training loss for batch 1171 : 0.002866792492568493\n",
      "Training loss for batch 1172 : 0.15799269080162048\n",
      "Training loss for batch 1173 : 0.005179743282496929\n",
      "Training loss for batch 1174 : 0.17974220216274261\n",
      "Training loss for batch 1175 : 0.1794722080230713\n",
      "Training loss for batch 1176 : 1.1522552085807547e-05\n",
      "Training loss for batch 1177 : 0.10369538515806198\n",
      "Training loss for batch 1178 : 0.27064594626426697\n",
      "Training loss for batch 1179 : 0.10236167907714844\n",
      "Training loss for batch 1180 : 0.2910357415676117\n",
      "Training loss for batch 1181 : 0.08481159806251526\n",
      "Training loss for batch 1182 : 0.32962480187416077\n",
      "Training loss for batch 1183 : 0.05125705152750015\n",
      "Training loss for batch 1184 : 0.09070973843336105\n",
      "Training loss for batch 1185 : 0.37145936489105225\n",
      "Training loss for batch 1186 : 0.10549142956733704\n",
      "Training loss for batch 1187 : 0.10290617495775223\n",
      "Training loss for batch 1188 : 0.03348126262426376\n",
      "Training loss for batch 1189 : 0.4164939820766449\n",
      "Training loss for batch 1190 : 0.33460623025894165\n",
      "Training loss for batch 1191 : 0.38389045000076294\n",
      "Training loss for batch 1192 : 0.15175475180149078\n",
      "Training loss for batch 1193 : 0.1199965551495552\n",
      "Training loss for batch 1194 : 0.14210639894008636\n",
      "Training loss for batch 1195 : 0.20544740557670593\n",
      "Training loss for batch 1196 : 0.07326389849185944\n",
      "Training loss for batch 1197 : 0.14367283880710602\n",
      "Training loss for batch 1198 : 0.177882581949234\n",
      "Training loss for batch 1199 : 0.04073784500360489\n",
      "Training loss for batch 1200 : 0.6974352598190308\n",
      "Training loss for batch 1201 : 0.1611456423997879\n",
      "Training loss for batch 1202 : 0.2754548490047455\n",
      "Training loss for batch 1203 : 0.06032568961381912\n",
      "Training loss for batch 1204 : 0.2313326597213745\n",
      "Training loss for batch 1205 : 0.0\n",
      "Training loss for batch 1206 : 0.2396954596042633\n",
      "Training loss for batch 1207 : 0.24621586501598358\n",
      "Training loss for batch 1208 : 0.027871156111359596\n",
      "Training loss for batch 1209 : 0.13842640817165375\n",
      "Training loss for batch 1210 : 0.2313578873872757\n",
      "Training loss for batch 1211 : 0.04996826499700546\n",
      "Training loss for batch 1212 : 0.04606546461582184\n",
      "Training loss for batch 1213 : 0.08888610452413559\n",
      "Training loss for batch 1214 : 0.6773207783699036\n",
      "Training loss for batch 1215 : 0.19249071180820465\n",
      "Training loss for batch 1216 : 0.14132635295391083\n",
      "Training loss for batch 1217 : 0.02445959486067295\n",
      "Training loss for batch 1218 : 0.007468918338418007\n",
      "Training loss for batch 1219 : 0.009026247076690197\n",
      "Training loss for batch 1220 : 0.3355158269405365\n",
      "Training loss for batch 1221 : 0.1841532438993454\n",
      "Training loss for batch 1222 : 0.10041128098964691\n",
      "Training loss for batch 1223 : 0.2453734576702118\n",
      "Training loss for batch 1224 : 0.040959812700748444\n",
      "Training loss for batch 1225 : 0.4790921211242676\n",
      "Training loss for batch 1226 : 0.7979099154472351\n",
      "Training loss for batch 1227 : 0.09611023217439651\n",
      "Training loss for batch 1228 : 0.22393767535686493\n",
      "Training loss for batch 1229 : 0.004933913704007864\n",
      "Training loss for batch 1230 : 0.08482725918292999\n",
      "Training loss for batch 1231 : 0.3390560448169708\n",
      "Training loss for batch 1232 : 0.2282489538192749\n",
      "Training loss for batch 1233 : 0.016537800431251526\n",
      "Training loss for batch 1234 : 0.0\n",
      "Training loss for batch 1235 : 0.17868000268936157\n",
      "Training loss for batch 1236 : 0.21066096425056458\n",
      "Training loss for batch 1237 : 0.026060890406370163\n",
      "Training loss for batch 1238 : 0.1633533537387848\n",
      "Training loss for batch 1239 : 0.4047795534133911\n",
      "Training loss for batch 1240 : 0.052937209606170654\n",
      "Training loss for batch 1241 : 0.2159615457057953\n",
      "Training loss for batch 1242 : 0.2547193169593811\n",
      "Training loss for batch 1243 : 0.05981643125414848\n",
      "Training loss for batch 1244 : 0.3480951488018036\n",
      "Training loss for batch 1245 : 0.021265950053930283\n",
      "Training loss for batch 1246 : 0.3187912404537201\n",
      "Training loss for batch 1247 : 0.16143660247325897\n",
      "Training loss for batch 1248 : 0.21738386154174805\n",
      "Training loss for batch 1249 : 0.3830133378505707\n",
      "Training loss for batch 1250 : 0.48989734053611755\n",
      "Training loss for batch 1251 : 0.215479776263237\n",
      "Training loss for batch 1252 : 0.1335974484682083\n",
      "Training loss for batch 1253 : 0.219061478972435\n",
      "Training loss for batch 1254 : 0.23762229084968567\n",
      "Training loss for batch 1255 : 0.008043925277888775\n",
      "Training loss for batch 1256 : 0.3297364115715027\n",
      "Training loss for batch 1257 : 0.00017140814452432096\n",
      "Training loss for batch 1258 : 0.2183385193347931\n",
      "Training loss for batch 1259 : 0.37724751234054565\n",
      "Training loss for batch 1260 : 0.07518945634365082\n",
      "Training loss for batch 1261 : 0.18933254480361938\n",
      "Training loss for batch 1262 : 0.4029686152935028\n",
      "Training loss for batch 1263 : 0.16351830959320068\n",
      "Training loss for batch 1264 : 0.1547306776046753\n",
      "Training loss for batch 1265 : 0.17544899880886078\n",
      "Training loss for batch 1266 : 0.5022392868995667\n",
      "Training loss for batch 1267 : 0.3585164546966553\n",
      "Training loss for batch 1268 : 0.5376837253570557\n",
      "Training loss for batch 1269 : 0.09639135748147964\n",
      "Training loss for batch 1270 : 0.3461313843727112\n",
      "Training loss for batch 1271 : 0.037085022777318954\n",
      "Training loss for batch 1272 : 0.02007981203496456\n",
      "Training loss for batch 1273 : 0.10803286731243134\n",
      "Training loss for batch 1274 : 0.2690122723579407\n",
      "Training loss for batch 1275 : 0.041779957711696625\n",
      "Training loss for batch 1276 : 0.05064239352941513\n",
      "Training loss for batch 1277 : 0.1130557581782341\n",
      "Training loss for batch 1278 : 0.277694970369339\n",
      "Training loss for batch 1279 : 0.01764032244682312\n",
      "Training loss for batch 1280 : 0.10230173915624619\n",
      "Training loss for batch 1281 : 0.10311710089445114\n",
      "Training loss for batch 1282 : 0.4128257930278778\n",
      "Training loss for batch 1283 : 0.18357926607131958\n",
      "Training loss for batch 1284 : 0.2657732665538788\n",
      "Training loss for batch 1285 : 0.31275027990341187\n",
      "Training loss for batch 1286 : 0.25429651141166687\n",
      "Training loss for batch 1287 : 0.040081411600112915\n",
      "Training loss for batch 1288 : 0.026072343811392784\n",
      "Training loss for batch 1289 : 0.40081343054771423\n",
      "Training loss for batch 1290 : 0.2732429802417755\n",
      "Training loss for batch 1291 : 0.020048823207616806\n",
      "Training loss for batch 1292 : 0.3813084363937378\n",
      "Training loss for batch 1293 : 0.01756940223276615\n",
      "Training loss for batch 1294 : 0.05686597153544426\n",
      "Training loss for batch 1295 : 0.26923102140426636\n",
      "Training loss for batch 1296 : 0.02543794922530651\n",
      "Training loss for batch 1297 : 0.3037072718143463\n",
      "Training loss for batch 1298 : 0.0\n",
      "Training loss for batch 1299 : 0.14171431958675385\n",
      "Training loss for batch 1300 : 0.28645744919776917\n",
      "Training loss for batch 1301 : 0.14809483289718628\n",
      "Training loss for batch 1302 : 0.005706428550183773\n",
      "Training loss for batch 1303 : 0.35947200655937195\n",
      "Training loss for batch 1304 : 0.07362435758113861\n",
      "Training loss for batch 1305 : 0.10948885977268219\n",
      "Training loss for batch 1306 : 0.20446795225143433\n",
      "Training loss for batch 1307 : 0.19390371441841125\n",
      "Training loss for batch 1308 : 0.12198631465435028\n",
      "Training loss for batch 1309 : 0.018531808629631996\n",
      "Training loss for batch 1310 : 0.14901147782802582\n",
      "Training loss for batch 1311 : 0.10518883913755417\n",
      "Training loss for batch 1312 : 0.08217273652553558\n",
      "Training loss for batch 1313 : 0.2983853816986084\n",
      "Training loss for batch 1314 : 7.384773653029697e-06\n",
      "Training loss for batch 1315 : 0.03458675742149353\n",
      "Training loss for batch 1316 : 0.3079952597618103\n",
      "Training loss for batch 1317 : 0.1679811328649521\n",
      "Training loss for batch 1318 : 0.03773428499698639\n",
      "Training loss for batch 1319 : 0.05721529945731163\n",
      "Training loss for batch 1320 : 0.3366580009460449\n",
      "Training loss for batch 1321 : 0.004876385442912579\n",
      "Training loss for batch 1322 : 0.028960293158888817\n",
      "Training loss for batch 1323 : 0.15503165125846863\n",
      "Training loss for batch 1324 : 0.26338517665863037\n",
      "Training loss for batch 1325 : 0.14463847875595093\n",
      "Training loss for batch 1326 : 0.20269078016281128\n",
      "Training loss for batch 1327 : 0.027409806847572327\n",
      "Training loss for batch 1328 : 0.16506466269493103\n",
      "Training loss for batch 1329 : 0.06010669842362404\n",
      "Training loss for batch 1330 : 0.18153415620326996\n",
      "Training loss for batch 1331 : 0.264658659696579\n",
      "Training loss for batch 1332 : 0.1458771824836731\n",
      "Training loss for batch 1333 : 0.04252956807613373\n",
      "Training loss for batch 1334 : 0.2826509475708008\n",
      "Training loss for batch 1335 : 0.013019157573580742\n",
      "Training loss for batch 1336 : 0.0832555964589119\n",
      "Training loss for batch 1337 : 0.06859750300645828\n",
      "Training loss for batch 1338 : 0.1885155588388443\n",
      "Training loss for batch 1339 : 0.34726816415786743\n",
      "Training loss for batch 1340 : 0.16036595404148102\n",
      "Training loss for batch 1341 : 0.33725908398628235\n",
      "Training loss for batch 1342 : 0.08852483332157135\n",
      "Training loss for batch 1343 : 0.23547092080116272\n",
      "Training loss for batch 1344 : 0.0257107000797987\n",
      "Training loss for batch 1345 : 0.03136907145380974\n",
      "Training loss for batch 1346 : 0.2668478488922119\n",
      "Training loss for batch 1347 : 0.6951095461845398\n",
      "Training loss for batch 1348 : 0.1271493285894394\n",
      "Training loss for batch 1349 : 0.0\n",
      "Training loss for batch 1350 : 0.06126004830002785\n",
      "Training loss for batch 1351 : 0.47313350439071655\n",
      "Training loss for batch 1352 : 0.26011449098587036\n",
      "Training loss for batch 1353 : 0.12287916988134384\n",
      "Training loss for batch 1354 : 0.6587348580360413\n",
      "Training loss for batch 1355 : 0.03210553154349327\n",
      "Training loss for batch 1356 : 0.0\n",
      "Training loss for batch 1357 : 0.04894008859992027\n",
      "Training loss for batch 1358 : 0.49803391098976135\n",
      "Training loss for batch 1359 : 0.13592027127742767\n",
      "Training loss for batch 1360 : 0.024461936205625534\n",
      "Training loss for batch 1361 : 0.21498936414718628\n",
      "Training loss for batch 1362 : 0.31055912375450134\n",
      "Training loss for batch 1363 : 0.2570628523826599\n",
      "Training loss for batch 1364 : 0.31704264879226685\n",
      "Training loss for batch 1365 : 0.260733962059021\n",
      "Training loss for batch 1366 : 0.003744988003745675\n",
      "Training loss for batch 1367 : 0.125539168715477\n",
      "Training loss for batch 1368 : 0.17527955770492554\n",
      "Training loss for batch 1369 : 0.1468462198972702\n",
      "Training loss for batch 1370 : 0.028542665764689445\n",
      "Training loss for batch 1371 : 0.03521207720041275\n",
      "Training loss for batch 1372 : 0.0512947253882885\n",
      "Training loss for batch 1373 : 0.1691979616880417\n",
      "Training loss for batch 1374 : 0.04556245356798172\n",
      "Training loss for batch 1375 : 0.183400958776474\n",
      "Training loss for batch 1376 : 0.2045067399740219\n",
      "Training loss for batch 1377 : 0.20553357899188995\n",
      "Training loss for batch 1378 : 0.3528619110584259\n",
      "Training loss for batch 1379 : 0.11414231359958649\n",
      "Training loss for batch 1380 : 0.318900465965271\n",
      "Training loss for batch 1381 : 0.14338558912277222\n",
      "Training loss for batch 1382 : 0.4108547270298004\n",
      "Training loss for batch 1383 : 0.2631865441799164\n",
      "Training loss for batch 1384 : 0.0\n",
      "Training loss for batch 1385 : 0.05140695720911026\n",
      "Training loss for batch 1386 : 0.4750637412071228\n",
      "Training loss for batch 1387 : 0.019621528685092926\n",
      "Training loss for batch 1388 : 0.008541041053831577\n",
      "Training loss for batch 1389 : 0.12632304430007935\n",
      "Training loss for batch 1390 : 0.16782236099243164\n",
      "Training loss for batch 1391 : 0.4043029248714447\n",
      "Training loss for batch 1392 : 0.012411346659064293\n",
      "Training loss for batch 1393 : 0.027190139517188072\n",
      "Training loss for batch 1394 : 0.37280017137527466\n",
      "Training loss for batch 1395 : 0.07254692167043686\n",
      "Training loss for batch 1396 : 0.2050151377916336\n",
      "Training loss for batch 1397 : 0.04743015393614769\n",
      "Training loss for batch 1398 : 0.3937983214855194\n",
      "Training loss for batch 1399 : 0.010335479862987995\n",
      "Training loss for batch 1400 : 0.11946944892406464\n",
      "Training loss for batch 1401 : 0.11551305651664734\n",
      "Training loss for batch 1402 : 0.3854231834411621\n",
      "Training loss for batch 1403 : 0.14012569189071655\n",
      "Training loss for batch 1404 : 0.22613953053951263\n",
      "Training loss for batch 1405 : 0.0\n",
      "Training loss for batch 1406 : 0.08426540344953537\n",
      "Training loss for batch 1407 : 0.10071197152137756\n",
      "Training loss for batch 1408 : 0.0872713029384613\n",
      "Training loss for batch 1409 : 0.05904606729745865\n",
      "Training loss for batch 1410 : 0.007152165286242962\n",
      "Training loss for batch 1411 : 0.28500792384147644\n",
      "Training loss for batch 1412 : 0.1744520515203476\n",
      "Training loss for batch 1413 : 0.19332900643348694\n",
      "Training loss for batch 1414 : 0.17039580643177032\n",
      "Training loss for batch 1415 : 0.18600589036941528\n",
      "Training loss for batch 1416 : 0.0015116434078663588\n",
      "Training loss for batch 1417 : 0.24207061529159546\n",
      "Training loss for batch 1418 : 0.17320415377616882\n",
      "Training loss for batch 1419 : 0.45216241478919983\n",
      "Training loss for batch 1420 : 0.21576964855194092\n",
      "Training loss for batch 1421 : 0.12390156090259552\n",
      "Training loss for batch 1422 : 0.08423121273517609\n",
      "Training loss for batch 1423 : 0.03202420473098755\n",
      "Training loss for batch 1424 : 0.3114732801914215\n",
      "Training loss for batch 1425 : 0.06980723887681961\n",
      "Training loss for batch 1426 : 0.06102630868554115\n",
      "Training loss for batch 1427 : 0.14370585978031158\n",
      "Training loss for batch 1428 : 0.15140579640865326\n",
      "Training loss for batch 1429 : 0.18903221189975739\n",
      "Training loss for batch 1430 : 0.2938118875026703\n",
      "Training loss for batch 1431 : 0.09403157234191895\n",
      "Training loss for batch 1432 : 0.01758917234838009\n",
      "Training loss for batch 1433 : 0.10150588303804398\n",
      "Training loss for batch 1434 : 0.09185619652271271\n",
      "Training loss for batch 1435 : 0.5900020003318787\n",
      "Training loss for batch 1436 : 0.16081036627292633\n",
      "Training loss for batch 1437 : 0.054956335574388504\n",
      "Training loss for batch 1438 : 0.2836701571941376\n",
      "Training loss for batch 1439 : 0.3119637966156006\n",
      "Training loss for batch 1440 : 0.08767113834619522\n",
      "Training loss for batch 1441 : 0.1554609090089798\n",
      "Training loss for batch 1442 : 0.5410506725311279\n",
      "Training loss for batch 1443 : 0.009759078733623028\n",
      "Training loss for batch 1444 : 0.37611261010169983\n",
      "Training loss for batch 1445 : 0.23418743908405304\n",
      "Training loss for batch 1446 : 0.1293579638004303\n",
      "Training loss for batch 1447 : 0.07108314335346222\n",
      "Training loss for batch 1448 : 0.4259853661060333\n",
      "Training loss for batch 1449 : 0.1550067961215973\n",
      "Training loss for batch 1450 : 0.12461777031421661\n",
      "Training loss for batch 1451 : 0.1451878696680069\n",
      "Training loss for batch 1452 : 0.0788261741399765\n",
      "Training loss for batch 1453 : 0.04332919418811798\n",
      "Training loss for batch 1454 : 0.11723097413778305\n",
      "Training loss for batch 1455 : 0.258771687746048\n",
      "Training loss for batch 1456 : 0.18530675768852234\n",
      "Training loss for batch 1457 : 0.20118620991706848\n",
      "Training loss for batch 1458 : 0.282305508852005\n",
      "Training loss for batch 1459 : 0.07583650201559067\n",
      "Training loss for batch 1460 : 0.1661057025194168\n",
      "Training loss for batch 1461 : 0.17223010957241058\n",
      "Training loss for batch 1462 : 0.11764056235551834\n",
      "Training loss for batch 1463 : 0.26011425256729126\n",
      "Training loss for batch 1464 : 0.10628219693899155\n",
      "Training loss for batch 1465 : 0.012941163033246994\n",
      "Training loss for batch 1466 : 0.16235551238059998\n",
      "Training loss for batch 1467 : 0.30619844794273376\n",
      "Training loss for batch 1468 : 0.15697640180587769\n",
      "Training loss for batch 1469 : 0.05428463593125343\n",
      "Training loss for batch 1470 : 0.7602383494377136\n",
      "Training loss for batch 1471 : 0.044766832143068314\n",
      "Training loss for batch 1472 : 0.32867881655693054\n",
      "Training loss for batch 1473 : 0.12992849946022034\n",
      "Training loss for batch 1474 : 0.12346520274877548\n",
      "Training loss for batch 1475 : 0.06190468370914459\n",
      "Training loss for batch 1476 : 0.07913248240947723\n",
      "Training loss for batch 1477 : 0.16938483715057373\n",
      "Training loss for batch 1478 : 0.03725995868444443\n",
      "Training loss for batch 1479 : 0.12316102534532547\n",
      "Training loss for batch 1480 : 0.0007219811668619514\n",
      "Training loss for batch 1481 : 0.3071778118610382\n",
      "Training loss for batch 1482 : 0.3333163261413574\n",
      "Training loss for batch 1483 : 0.10709354281425476\n",
      "Training loss for batch 1484 : 0.25792422890663147\n",
      "Training loss for batch 1485 : 0.04693804681301117\n",
      "Training loss for batch 1486 : 0.2355649173259735\n",
      "Training loss for batch 1487 : 0.030871354043483734\n",
      "Training loss for batch 1488 : 0.4604020118713379\n",
      "Training loss for batch 1489 : 0.0770849734544754\n",
      "Training loss for batch 1490 : 0.011401703581213951\n",
      "Training loss for batch 1491 : 0.3399547338485718\n",
      "Training loss for batch 1492 : 0.0740460753440857\n",
      "Training loss for batch 1493 : 0.16462799906730652\n",
      "Training loss for batch 1494 : 0.1324589103460312\n",
      "Training loss for batch 1495 : 0.5857594609260559\n",
      "Training loss for batch 1496 : 0.031098883599042892\n",
      "Training loss for batch 1497 : 0.09056150168180466\n",
      "Training loss for batch 1498 : 0.1561877280473709\n",
      "Training loss for batch 1499 : 0.29740622639656067\n",
      "Training loss for batch 1500 : 0.10863419622182846\n",
      "Training loss for batch 1501 : 0.28777772188186646\n",
      "Training loss for batch 1502 : 0.23056261241436005\n",
      "Training loss for batch 1503 : 0.13053855299949646\n",
      "Training loss for batch 1504 : 0.0\n",
      "Training loss for batch 1505 : 0.3812546133995056\n",
      "Training loss for batch 1506 : 0.25957509875297546\n",
      "Training loss for batch 1507 : 0.1907041370868683\n",
      "Training loss for batch 1508 : 0.039070576429367065\n",
      "Training loss for batch 1509 : 0.12002041190862656\n",
      "Training loss for batch 1510 : 0.1818869411945343\n",
      "Training loss for batch 1511 : 0.3315430283546448\n",
      "Training loss for batch 1512 : 0.10247711092233658\n",
      "Training loss for batch 1513 : 0.10473952442407608\n",
      "Training loss for batch 1514 : 0.26411810517311096\n",
      "Training loss for batch 1515 : 0.044880643486976624\n",
      "Training loss for batch 1516 : 0.19458980858325958\n",
      "Training loss for batch 1517 : 0.11729789525270462\n",
      "Training loss for batch 1518 : 0.037152063101530075\n",
      "Training loss for batch 1519 : 0.1570313572883606\n",
      "Training loss for batch 1520 : 0.35044145584106445\n",
      "Training loss for batch 1521 : 0.11387372016906738\n",
      "Training loss for batch 1522 : 0.2016366422176361\n",
      "Training loss for batch 1523 : 0.5522602200508118\n",
      "Training loss for batch 1524 : 0.26833659410476685\n",
      "Training loss for batch 1525 : 0.14066630601882935\n",
      "Training loss for batch 1526 : 0.0\n",
      "Training loss for batch 1527 : 0.337817907333374\n",
      "Training loss for batch 1528 : 0.4130302369594574\n",
      "Training loss for batch 1529 : 0.34919050335884094\n",
      "Training loss for batch 1530 : 0.21590615808963776\n",
      "Training loss for batch 1531 : 0.05803205072879791\n",
      "Training loss for batch 1532 : 0.22497621178627014\n",
      "Training loss for batch 1533 : 0.2532927691936493\n",
      "Training loss for batch 1534 : 0.12412874400615692\n",
      "Training loss for batch 1535 : 0.21312013268470764\n",
      "Training loss for batch 1536 : 0.12902219593524933\n",
      "Training loss for batch 1537 : 0.2730880379676819\n",
      "Training loss for batch 1538 : 0.1874767392873764\n",
      "Training loss for batch 1539 : 0.03884711489081383\n",
      "Training loss for batch 1540 : 0.08607076853513718\n",
      "Training loss for batch 1541 : 0.04018527641892433\n",
      "Training loss for batch 1542 : 0.1441749483346939\n",
      "Training loss for batch 1543 : 0.3441172242164612\n",
      "Training loss for batch 1544 : 0.3829694986343384\n",
      "Training loss for batch 1545 : 0.20721335709095\n",
      "Training loss for batch 1546 : 0.27317360043525696\n",
      "Training loss for batch 1547 : 0.22366049885749817\n",
      "Training loss for batch 1548 : 0.04121682047843933\n",
      "Training loss for batch 1549 : 0.33607298135757446\n",
      "Training loss for batch 1550 : 0.23432190716266632\n",
      "Training loss for batch 1551 : 0.2230548858642578\n",
      "Training loss for batch 1552 : 0.018313338980078697\n",
      "Training loss for batch 1553 : 0.26748237013816833\n",
      "Training loss for batch 1554 : 0.08848369866609573\n",
      "Training loss for batch 1555 : 0.08225314319133759\n",
      "Training loss for batch 1556 : 0.30311617255210876\n",
      "Training loss for batch 1557 : 0.171784907579422\n",
      "Training loss for batch 1558 : 0.36054348945617676\n",
      "Training loss for batch 1559 : 0.04322217032313347\n",
      "Training loss for batch 1560 : 0.317924827337265\n",
      "Training loss for batch 1561 : 0.34379059076309204\n",
      "Training loss for batch 1562 : 0.20752482116222382\n",
      "Training loss for batch 1563 : 0.09814289957284927\n",
      "Training loss for batch 1564 : 0.21395154297351837\n",
      "Training loss for batch 1565 : 0.12146563827991486\n",
      "Training loss for batch 1566 : 0.2745850086212158\n",
      "Training loss for batch 1567 : 0.09003451466560364\n",
      "Training loss for batch 1568 : 0.19015498459339142\n",
      "Training loss for batch 1569 : 0.16730821132659912\n",
      "Training loss for batch 1570 : 0.05489031970500946\n",
      "Training loss for batch 1571 : 0.16342023015022278\n",
      "Training loss for batch 1572 : 0.04115408658981323\n",
      "Training loss for batch 1573 : 0.23731859028339386\n",
      "Training loss for batch 1574 : 0.18475523591041565\n",
      "Training loss for batch 1575 : 0.16551688313484192\n",
      "Training loss for batch 1576 : 0.2188635766506195\n",
      "Training loss for batch 1577 : 0.15909820795059204\n",
      "Training loss for batch 1578 : 0.19168353080749512\n",
      "Training loss for batch 1579 : 0.4370562434196472\n",
      "Training loss for batch 1580 : 0.0984431803226471\n",
      "Training loss for batch 1581 : 0.22694438695907593\n",
      "Training loss for batch 1582 : 0.12625350058078766\n",
      "Training loss for batch 1583 : 0.21322514116764069\n",
      "Training loss for batch 1584 : 0.061522215604782104\n",
      "Training loss for batch 1585 : 0.26296669244766235\n",
      "Training loss for batch 1586 : 0.23029135167598724\n",
      "Training loss for batch 1587 : 0.17567534744739532\n",
      "Training loss for batch 1588 : 0.24510544538497925\n",
      "Training loss for batch 1589 : 0.24939006567001343\n",
      "Training loss for batch 1590 : 0.030112752690911293\n",
      "Training loss for batch 1591 : 0.3480041027069092\n",
      "Training loss for batch 1592 : 0.07431607693433762\n",
      "Training loss for batch 1593 : 0.12204845249652863\n",
      "Training loss for batch 1594 : 0.11616100370883942\n",
      "Training loss for batch 1595 : 0.216151624917984\n",
      "Training loss for batch 1596 : 0.07265973836183548\n",
      "Training loss for batch 1597 : 0.21563856303691864\n",
      "Training loss for batch 1598 : 0.3699559271335602\n",
      "Training loss for batch 1599 : 0.11865773797035217\n",
      "Training loss for batch 1600 : 0.10572507977485657\n",
      "Training loss for batch 1601 : 0.0640094056725502\n",
      "Training loss for batch 1602 : 0.07337989658117294\n",
      "Training loss for batch 1603 : 0.296648234128952\n",
      "Training loss for batch 1604 : 0.1384800225496292\n",
      "Training loss for batch 1605 : 0.4041832387447357\n",
      "Training loss for batch 1606 : 0.3230070471763611\n",
      "Training loss for batch 1607 : 0.2263040989637375\n",
      "Training loss for batch 1608 : 0.1504976451396942\n",
      "Training loss for batch 1609 : 0.2970043122768402\n",
      "Training loss for batch 1610 : 0.10256680846214294\n",
      "Training loss for batch 1611 : 0.13486520946025848\n",
      "Training loss for batch 1612 : 0.12245666980743408\n",
      "Training loss for batch 1613 : 0.0460481233894825\n",
      "Training loss for batch 1614 : 0.12674713134765625\n",
      "Training loss for batch 1615 : 0.248432919383049\n",
      "Training loss for batch 1616 : 0.12419915199279785\n",
      "Training loss for batch 1617 : 0.2110755741596222\n",
      "Training loss for batch 1618 : 0.14296533167362213\n",
      "Training loss for batch 1619 : 0.17918948829174042\n",
      "Training loss for batch 1620 : 0.05152922868728638\n",
      "Training loss for batch 1621 : 0.02499600499868393\n",
      "Training loss for batch 1622 : 0.112653948366642\n",
      "Training loss for batch 1623 : 0.26390382647514343\n",
      "Training loss for batch 1624 : 0.26235663890838623\n",
      "Training loss for batch 1625 : 0.36701419949531555\n",
      "Training loss for batch 1626 : 0.2108607441186905\n",
      "Training loss for batch 1627 : 0.22790966928005219\n",
      "Training loss for batch 1628 : 0.26953157782554626\n",
      "Training loss for batch 1629 : 0.2190037965774536\n",
      "Training loss for batch 1630 : 0.14945459365844727\n",
      "Training loss for batch 1631 : 0.17218148708343506\n",
      "Training loss for batch 1632 : 0.0\n",
      "Training loss for batch 1633 : 0.16904360055923462\n",
      "Training loss for batch 1634 : 0.10245504975318909\n",
      "Training loss for batch 1635 : 0.12686224281787872\n",
      "Training loss for batch 1636 : 0.06614905595779419\n",
      "Training loss for batch 1637 : 0.07300212234258652\n",
      "Training loss for batch 1638 : 0.035162243992090225\n",
      "Training loss for batch 1639 : 0.47133636474609375\n",
      "Training loss for batch 1640 : 0.30484268069267273\n",
      "Training loss for batch 1641 : 0.2645101249217987\n",
      "Training loss for batch 1642 : 0.2800377607345581\n",
      "Training loss for batch 1643 : 0.08173462748527527\n",
      "Training loss for batch 1644 : 0.09912384301424026\n",
      "Training loss for batch 1645 : 0.09892574697732925\n",
      "Training loss for batch 1646 : 0.14490219950675964\n",
      "Training loss for batch 1647 : 0.24516421556472778\n",
      "Training loss for batch 1648 : 0.14571034908294678\n",
      "Training loss for batch 1649 : 0.11865442991256714\n",
      "Training loss for batch 1650 : 0.05121184512972832\n",
      "Training loss for batch 1651 : 0.03300944343209267\n",
      "Training loss for batch 1652 : 0.0020827054977416992\n",
      "Training loss for batch 1653 : 0.3210151791572571\n",
      "Training loss for batch 1654 : 0.08094502985477448\n",
      "Training loss for batch 1655 : 0.06986302882432938\n",
      "Training loss for batch 1656 : 0.15928228199481964\n",
      "Training loss for batch 1657 : 0.219969242811203\n",
      "Training loss for batch 1658 : 0.08239652961492538\n",
      "Training loss for batch 1659 : 0.2726627588272095\n",
      "Training loss for batch 1660 : 0.14765116572380066\n",
      "Training loss for batch 1661 : 0.042025916278362274\n",
      "Training loss for batch 1662 : 0.1972663253545761\n",
      "Training loss for batch 1663 : 0.3166314661502838\n",
      "Training loss for batch 1664 : 0.06149856746196747\n",
      "Training loss for batch 1665 : 0.03928558528423309\n",
      "Training loss for batch 1666 : 0.08649280667304993\n",
      "Training loss for batch 1667 : 0.23768247663974762\n",
      "Training loss for batch 1668 : 0.21952031552791595\n",
      "Training loss for batch 1669 : 0.1268676072359085\n",
      "Training loss for batch 1670 : 0.18891027569770813\n",
      "Training loss for batch 1671 : 0.05879569053649902\n",
      "Training loss for batch 1672 : 0.018944179639220238\n",
      "Training loss for batch 1673 : 0.26279333233833313\n",
      "Training loss for batch 1674 : 0.22355668246746063\n",
      "Training loss for batch 1675 : 0.04265633597970009\n",
      "Training loss for batch 1676 : 0.11093022674322128\n",
      "Training loss for batch 1677 : 0.1676909178495407\n",
      "Training loss for batch 1678 : 0.060109369456768036\n",
      "Training loss for batch 1679 : 0.3186638653278351\n",
      "Training loss for batch 1680 : 0.14549334347248077\n",
      "Training loss for batch 1681 : 0.2500339150428772\n",
      "Training loss for batch 1682 : 0.19662998616695404\n",
      "Training loss for batch 1683 : 0.04617132991552353\n",
      "Training loss for batch 1684 : 0.29169961810112\n",
      "Training loss for batch 1685 : 0.23121657967567444\n",
      "Training loss for batch 1686 : 0.17995859682559967\n",
      "Training loss for batch 1687 : 0.263841450214386\n",
      "Training loss for batch 1688 : 0.1456362009048462\n",
      "Training loss for batch 1689 : 0.4127039611339569\n",
      "Training loss for batch 1690 : 0.04711131751537323\n",
      "Training loss for batch 1691 : 0.1379678100347519\n",
      "Training loss for batch 1692 : 0.2200389802455902\n",
      "Training loss for batch 1693 : 0.08269386738538742\n",
      "Training loss for batch 1694 : 0.16037015616893768\n",
      "Training loss for batch 1695 : 0.333149254322052\n",
      "Training loss for batch 1696 : 0.008684476837515831\n",
      "Training loss for batch 1697 : 0.05029164254665375\n",
      "Training loss for batch 1698 : 0.17558027803897858\n",
      "Training loss for batch 1699 : 0.0695413202047348\n",
      "Training loss for batch 1700 : 0.12283197045326233\n",
      "Training loss for batch 1701 : 0.26296132802963257\n",
      "Training loss for batch 1702 : 0.07204949855804443\n",
      "Training loss for batch 1703 : 0.1912728250026703\n",
      "Training loss for batch 1704 : 0.1599450409412384\n",
      "Training loss for batch 1705 : 0.02349545992910862\n",
      "Training loss for batch 1706 : 0.11513233929872513\n",
      "Training loss for batch 1707 : 0.3302846848964691\n",
      "Training loss for batch 1708 : 0.1790790855884552\n",
      "Training loss for batch 1709 : 0.28710269927978516\n",
      "Training loss for batch 1710 : 0.3876558244228363\n",
      "Training loss for batch 1711 : 0.0912775918841362\n",
      "Training loss for batch 1712 : 0.17236468195915222\n",
      "Training loss for batch 1713 : 0.502114474773407\n",
      "Training loss for batch 1714 : 0.17740082740783691\n",
      "Training loss for batch 1715 : 0.10379967093467712\n",
      "Training loss for batch 1716 : 0.008099932223558426\n",
      "Training loss for batch 1717 : 0.00010480087803443894\n",
      "Training loss for batch 1718 : 0.3125782907009125\n",
      "Training loss for batch 1719 : 0.02196640707552433\n",
      "Training loss for batch 1720 : 0.051363881677389145\n",
      "Training loss for batch 1721 : 0.10905540734529495\n",
      "Training loss for batch 1722 : 0.15274228155612946\n",
      "Training loss for batch 1723 : 0.2684278190135956\n",
      "Training loss for batch 1724 : 0.3238745331764221\n",
      "Training loss for batch 1725 : 0.20503459870815277\n",
      "Training loss for batch 1726 : 0.18581221997737885\n",
      "Training loss for batch 1727 : 0.18596769869327545\n",
      "Training loss for batch 1728 : 0.05244605615735054\n",
      "Training loss for batch 1729 : 0.028683964163064957\n",
      "Training loss for batch 1730 : 0.12776505947113037\n",
      "Training loss for batch 1731 : 0.07536163181066513\n",
      "Training loss for batch 1732 : 0.1351071149110794\n",
      "Training loss for batch 1733 : 0.3341013491153717\n",
      "Training loss for batch 1734 : 0.09080591797828674\n",
      "Training loss for batch 1735 : 0.2683038115501404\n",
      "Training loss for batch 1736 : 0.06405727565288544\n",
      "Training loss for batch 1737 : 0.3732657730579376\n",
      "Training loss for batch 1738 : 0.058770593255758286\n",
      "Training loss for batch 1739 : 0.13924777507781982\n",
      "Training loss for batch 1740 : 0.06816443055868149\n",
      "Training loss for batch 1741 : 0.17004549503326416\n",
      "Training loss for batch 1742 : 0.09046853333711624\n",
      "Training loss for batch 1743 : 0.013687079772353172\n",
      "Training loss for batch 1744 : 0.19233977794647217\n",
      "Training loss for batch 1745 : 0.32362863421440125\n",
      "Training loss for batch 1746 : 0.13027529418468475\n",
      "Training loss for batch 1747 : 0.4152776896953583\n",
      "Training loss for batch 1748 : 0.11559988558292389\n",
      "Training loss for batch 1749 : 0.4357743561267853\n",
      "Training loss for batch 1750 : 0.007057934999465942\n",
      "Training loss for batch 1751 : 0.2590648829936981\n",
      "Training loss for batch 1752 : 0.5538397431373596\n",
      "Training loss for batch 1753 : 0.12494795769453049\n",
      "Training loss for batch 1754 : 0.049867816269397736\n",
      "Training loss for batch 1755 : 0.20346808433532715\n",
      "Training loss for batch 1756 : 0.31429821252822876\n",
      "Training loss for batch 1757 : 0.2203022539615631\n",
      "Training loss for batch 1758 : 0.039721980690956116\n",
      "Training loss for batch 1759 : 0.15619131922721863\n",
      "Training loss for batch 1760 : 0.31091731786727905\n",
      "Training loss for batch 1761 : 0.18584895133972168\n",
      "Training loss for batch 1762 : 0.10619799047708511\n",
      "Training loss for batch 1763 : 0.34801167249679565\n",
      "Training loss for batch 1764 : 0.31474167108535767\n",
      "Training loss for batch 1765 : 0.32462772727012634\n",
      "Training loss for batch 1766 : 0.007124662399291992\n",
      "Training loss for batch 1767 : 0.17416860163211823\n",
      "Training loss for batch 1768 : 0.2857949733734131\n",
      "Training loss for batch 1769 : 0.1764785200357437\n",
      "Training loss for batch 1770 : 0.06122990697622299\n",
      "Training loss for batch 1771 : 0.40891632437705994\n",
      "Training loss for batch 1772 : 0.03807363659143448\n",
      "Training loss for batch 1773 : 0.06199273094534874\n",
      "Training loss for batch 1774 : 0.15127070248126984\n",
      "Training loss for batch 1775 : 0.04982497915625572\n",
      "Training loss for batch 1776 : 0.2821311354637146\n",
      "Training loss for batch 1777 : 0.1402803212404251\n",
      "Training loss for batch 1778 : 0.2330678552389145\n",
      "Training loss for batch 1779 : 0.11222126334905624\n",
      "Training loss for batch 1780 : 0.23476947844028473\n",
      "Training loss for batch 1781 : 0.08332663029432297\n",
      "Training loss for batch 1782 : 0.053943656384944916\n",
      "Training loss for batch 1783 : 0.29359593987464905\n",
      "Training loss for batch 1784 : 0.3327142298221588\n",
      "Training loss for batch 1785 : 0.1191713958978653\n",
      "Training loss for batch 1786 : 0.14864273369312286\n",
      "Training loss for batch 1787 : 0.07311589270830154\n",
      "Training loss for batch 1788 : 0.124488964676857\n",
      "Training loss for batch 1789 : 0.07582246512174606\n",
      "Training loss for batch 1790 : 0.31858688592910767\n",
      "Training loss for batch 1791 : 0.07569964975118637\n",
      "Training loss for batch 1792 : 0.22078831493854523\n",
      "Training loss for batch 1793 : 0.08459652215242386\n",
      "Training loss for batch 1794 : 0.5623592138290405\n",
      "Training loss for batch 1795 : 0.29708969593048096\n",
      "Training loss for batch 1796 : 0.1012444794178009\n",
      "Training loss for batch 1797 : 0.021658560261130333\n",
      "Training loss for batch 1798 : 0.06636833399534225\n",
      "Training loss for batch 1799 : 0.20803827047348022\n",
      "Training loss for batch 1800 : 0.5291546583175659\n",
      "Training loss for batch 1801 : 0.19451619684696198\n",
      "Training loss for batch 1802 : 0.08859831094741821\n",
      "Training loss for batch 1803 : 0.1667003333568573\n",
      "Training loss for batch 1804 : 0.09036289900541306\n",
      "Training loss for batch 1805 : 0.10997142642736435\n",
      "Training loss for batch 1806 : 0.02856237255036831\n",
      "Training loss for batch 1807 : 0.1809324026107788\n",
      "Training loss for batch 1808 : 0.2894766628742218\n",
      "Training loss for batch 1809 : 0.30699485540390015\n",
      "Training loss for batch 1810 : 0.07608429342508316\n",
      "Training loss for batch 1811 : 0.2174346148967743\n",
      "Training loss for batch 1812 : 0.16542938351631165\n",
      "Training loss for batch 1813 : 0.2239825427532196\n",
      "Training loss for batch 1814 : 0.3161267042160034\n",
      "Training loss for batch 1815 : 0.05250757560133934\n",
      "Training loss for batch 1816 : 0.007043654564768076\n",
      "Training loss for batch 1817 : 0.004588712472468615\n",
      "Training loss for batch 1818 : 0.2579113245010376\n",
      "Training loss for batch 1819 : 0.4139130115509033\n",
      "Training loss for batch 1820 : 0.525065004825592\n",
      "Training loss for batch 1821 : 0.380236953496933\n",
      "Training loss for batch 1822 : 0.2073260247707367\n",
      "Training loss for batch 1823 : 0.08066398650407791\n",
      "Training loss for batch 1824 : 0.09726443886756897\n",
      "Training loss for batch 1825 : 0.1373540163040161\n",
      "Training loss for batch 1826 : 0.22071297466754913\n",
      "Training loss for batch 1827 : 0.1560864895582199\n",
      "Training loss for batch 1828 : 0.016185492277145386\n",
      "Training loss for batch 1829 : 0.05747289955615997\n",
      "Training loss for batch 1830 : 0.10122933238744736\n",
      "Training loss for batch 1831 : 0.00720647256821394\n",
      "Training loss for batch 1832 : 0.3035736382007599\n",
      "Training loss for batch 1833 : 0.010277440771460533\n",
      "Training loss for batch 1834 : 0.30614617466926575\n",
      "Training loss for batch 1835 : 0.10801903158426285\n",
      "Training loss for batch 1836 : 0.19230574369430542\n",
      "Training loss for batch 1837 : 0.312880277633667\n",
      "Training loss for batch 1838 : 0.00804808922111988\n",
      "Training loss for batch 1839 : 0.004450127482414246\n",
      "Training loss for batch 1840 : 0.21916703879833221\n",
      "Training loss for batch 1841 : 0.1692882776260376\n",
      "Training loss for batch 1842 : 0.06870939582586288\n",
      "Training loss for batch 1843 : 0.4234347343444824\n",
      "Training loss for batch 1844 : 0.24728719890117645\n",
      "Training loss for batch 1845 : 0.18964415788650513\n",
      "Training loss for batch 1846 : 0.22564111649990082\n",
      "Training loss for batch 1847 : 0.036419644951820374\n",
      "Training loss for batch 1848 : 0.5530825257301331\n",
      "Training loss for batch 1849 : 0.14102207124233246\n",
      "Training loss for batch 1850 : 0.005034003406763077\n",
      "Training loss for batch 1851 : 0.10350501537322998\n",
      "Training loss for batch 1852 : 0.22933508455753326\n",
      "Training loss for batch 1853 : 0.004109220579266548\n",
      "Training loss for batch 1854 : 0.23268893361091614\n",
      "Training loss for batch 1855 : 0.03740012273192406\n",
      "Training loss for batch 1856 : 0.22959578037261963\n",
      "Training loss for batch 1857 : 0.28179308772087097\n",
      "Training loss for batch 1858 : 0.03085099346935749\n",
      "Training loss for batch 1859 : 0.008533875457942486\n",
      "Training loss for batch 1860 : 0.4434228241443634\n",
      "Training loss for batch 1861 : 0.09112562984228134\n",
      "Training loss for batch 1862 : 0.17759622633457184\n",
      "Training loss for batch 1863 : 0.30825692415237427\n",
      "Training loss for batch 1864 : 0.03430676460266113\n",
      "Training loss for batch 1865 : 0.031678132712841034\n",
      "Training loss for batch 1866 : 0.10721299797296524\n",
      "Training loss for batch 1867 : 0.0200120210647583\n",
      "Training loss for batch 1868 : 0.0\n",
      "Training loss for batch 1869 : 0.5425248146057129\n",
      "Training loss for batch 1870 : 0.06918884068727493\n",
      "Training loss for batch 1871 : 0.36340534687042236\n",
      "Training loss for batch 1872 : 0.17780838906764984\n",
      "Training loss for batch 1873 : 0.204371839761734\n",
      "Training loss for batch 1874 : 0.0019015073776245117\n",
      "Training loss for batch 1875 : 0.04401791840791702\n",
      "Training loss for batch 1876 : 0.00275777792558074\n",
      "Training loss for batch 1877 : 0.07392062991857529\n",
      "Training loss for batch 1878 : 0.0844959020614624\n",
      "Training loss for batch 1879 : 0.07951155304908752\n",
      "Training loss for batch 1880 : 0.07867798209190369\n",
      "Training loss for batch 1881 : 0.2764376997947693\n",
      "Training loss for batch 1882 : 0.21918021142482758\n",
      "Training loss for batch 1883 : 0.2632054090499878\n",
      "Training loss for batch 1884 : 0.19613397121429443\n",
      "Training loss for batch 1885 : 0.04576070234179497\n",
      "Training loss for batch 1886 : 0.023205164819955826\n",
      "Training loss for batch 1887 : 0.2459256798028946\n",
      "Training loss for batch 1888 : 0.07710783928632736\n",
      "Training loss for batch 1889 : 0.11942775547504425\n",
      "Training loss for batch 1890 : 0.0\n",
      "Training loss for batch 1891 : 0.48144495487213135\n",
      "Training loss for batch 1892 : 0.02918066456913948\n",
      "Training loss for batch 1893 : 0.13657620549201965\n",
      "Training loss for batch 1894 : 0.08584465086460114\n",
      "Training loss for batch 1895 : 0.015105880796909332\n",
      "Training loss for batch 1896 : 0.16294069588184357\n",
      "Training loss for batch 1897 : 0.12014375627040863\n",
      "Training loss for batch 1898 : 0.3541870415210724\n",
      "Training loss for batch 1899 : 0.33908987045288086\n",
      "Training loss for batch 1900 : 0.15264804661273956\n",
      "Training loss for batch 1901 : 0.002176254987716675\n",
      "Training loss for batch 1902 : 0.29026082158088684\n",
      "Training loss for batch 1903 : 0.11756005138158798\n",
      "Training loss for batch 1904 : 0.0013704102020710707\n",
      "Training loss for batch 1905 : 0.03849811479449272\n",
      "Training loss for batch 1906 : 0.36864256858825684\n",
      "Training loss for batch 1907 : 0.4530794322490692\n",
      "Training loss for batch 1908 : 0.12330187857151031\n",
      "Training loss for batch 1909 : 0.11055677384138107\n",
      "Training loss for batch 1910 : 0.02333179861307144\n",
      "Training loss for batch 1911 : 0.025011807680130005\n",
      "Training loss for batch 1912 : 0.2651577591896057\n",
      "Training loss for batch 1913 : 0.36328256130218506\n",
      "Training loss for batch 1914 : 0.12360024452209473\n",
      "Training loss for batch 1915 : 0.30082863569259644\n",
      "Training loss for batch 1916 : 0.2500891089439392\n",
      "Training loss for batch 1917 : 0.372686505317688\n",
      "Training loss for batch 1918 : 0.1678844839334488\n",
      "Training loss for batch 1919 : 0.4348181486129761\n",
      "Training loss for batch 1920 : 0.0610896460711956\n",
      "Training loss for batch 1921 : 0.010644384659826756\n",
      "Training loss for batch 1922 : 0.0\n",
      "Training loss for batch 1923 : 0.21014785766601562\n",
      "Training loss for batch 1924 : 0.24006789922714233\n",
      "Training loss for batch 1925 : 0.0\n",
      "Training loss for batch 1926 : 0.017877738922834396\n",
      "Training loss for batch 1927 : 0.1489822268486023\n",
      "Training loss for batch 1928 : 0.3368636667728424\n",
      "Training loss for batch 1929 : 0.2138335257768631\n",
      "Training loss for batch 1930 : 0.3516078591346741\n",
      "Training loss for batch 1931 : 0.02265363186597824\n",
      "Training loss for batch 1932 : 0.036919087171554565\n",
      "Training loss for batch 1933 : 0.0016060705529525876\n",
      "Training loss for batch 1934 : 0.1497759073972702\n",
      "Training loss for batch 1935 : 0.10735588520765305\n",
      "Training loss for batch 1936 : 0.2235306352376938\n",
      "Training loss for batch 1937 : 0.06092409789562225\n",
      "Training loss for batch 1938 : 0.3320719599723816\n",
      "Training loss for batch 1939 : 0.15373627841472626\n",
      "Training loss for batch 1940 : 0.6842000484466553\n",
      "Training loss for batch 1941 : 0.15108178555965424\n",
      "Training loss for batch 1942 : 0.21623264253139496\n",
      "Training loss for batch 1943 : 0.05783674865961075\n",
      "Training loss for batch 1944 : 0.42911332845687866\n",
      "Training loss for batch 1945 : 0.09326692670583725\n",
      "Training loss for batch 1946 : 0.1551191657781601\n",
      "Training loss for batch 1947 : 0.21186833083629608\n",
      "Training loss for batch 1948 : 0.26996511220932007\n",
      "Training loss for batch 1949 : 0.057516008615493774\n",
      "Training loss for batch 1950 : 0.06962912529706955\n",
      "Training loss for batch 1951 : 0.15011540055274963\n",
      "Training loss for batch 1952 : 0.012830737978219986\n",
      "Training loss for batch 1953 : 0.14617809653282166\n",
      "Training loss for batch 1954 : 0.2587607800960541\n",
      "Training loss for batch 1955 : 0.22197043895721436\n",
      "Training loss for batch 1956 : 0.19862349331378937\n",
      "Training loss for batch 1957 : 0.054676350206136703\n",
      "Training loss for batch 1958 : 0.17716360092163086\n",
      "Training loss for batch 1959 : 0.29313021898269653\n",
      "Training loss for batch 1960 : 0.07857885211706161\n",
      "Training loss for batch 1961 : 0.21957279741764069\n",
      "Training loss for batch 1962 : 0.09748432040214539\n",
      "Training loss for batch 1963 : 0.14313824474811554\n",
      "Training loss for batch 1964 : 0.2583310604095459\n",
      "Training loss for batch 1965 : 0.3675146698951721\n",
      "Training loss for batch 1966 : 0.30075404047966003\n",
      "Training loss for batch 1967 : 0.2216673493385315\n",
      "Training loss for batch 1968 : 0.1339426040649414\n",
      "Training loss for batch 1969 : 0.372586727142334\n",
      "Training loss for batch 1970 : 0.044521085917949677\n",
      "Training loss for batch 1971 : 0.16972193121910095\n",
      "Training loss for batch 1972 : 0.025163570418953896\n",
      "Training loss for batch 1973 : 0.09513898938894272\n",
      "Training loss for batch 1974 : 0.09279853850603104\n",
      "Training loss for batch 1975 : 0.11358156055212021\n",
      "Training loss for batch 1976 : 0.010697266086935997\n",
      "Training loss for batch 1977 : 0.34778615832328796\n",
      "Training loss for batch 1978 : 0.17473605275154114\n",
      "Training loss for batch 1979 : 0.16213871538639069\n",
      "Training loss for batch 1980 : 0.37137433886528015\n",
      "Training loss for batch 1981 : 0.024154236540198326\n",
      "Training loss for batch 1982 : 0.03792496770620346\n",
      "Training loss for batch 1983 : 0.1462814211845398\n",
      "Training loss for batch 1984 : 0.13721902668476105\n",
      "Training loss for batch 1985 : 0.0\n",
      "Training loss for batch 1986 : 0.189820796251297\n",
      "Training loss for batch 1987 : 0.18306230008602142\n",
      "Training loss for batch 1988 : 0.09137480705976486\n",
      "Training loss for batch 1989 : 0.012371495366096497\n",
      "Training loss for batch 1990 : 0.041414011269807816\n",
      "Training loss for batch 1991 : 0.11196933686733246\n",
      "Training loss for batch 1992 : 0.05313102900981903\n",
      "Training loss for batch 1993 : 0.16140224039554596\n",
      "Training loss for batch 1994 : 0.21719370782375336\n",
      "Training loss for batch 1995 : 0.006958774756640196\n",
      "Training loss for batch 1996 : 0.08047180622816086\n",
      "Training loss for batch 1997 : 0.023158054798841476\n",
      "Training loss for batch 1998 : 0.46159783005714417\n",
      "Training loss for batch 1999 : 0.25480395555496216\n",
      "Training loss for batch 2000 : 0.021640688180923462\n",
      "Training loss for batch 2001 : 0.32391929626464844\n",
      "Training loss for batch 2002 : 8.471572800772265e-05\n",
      "Training loss for batch 2003 : 0.019791848957538605\n",
      "Training loss for batch 2004 : 0.13001328706741333\n",
      "Training loss for batch 2005 : 0.20503687858581543\n",
      "Training loss for batch 2006 : 0.046750884503126144\n",
      "Training loss for batch 2007 : 0.3864746689796448\n",
      "Training loss for batch 2008 : 0.36500194668769836\n",
      "Training loss for batch 2009 : 0.043100908398628235\n",
      "Training loss for batch 2010 : 0.03766607493162155\n",
      "Training loss for batch 2011 : 0.27899911999702454\n",
      "Training loss for batch 2012 : 0.2073381096124649\n",
      "Training loss for batch 2013 : 0.3791390359401703\n",
      "Training loss for batch 2014 : 0.38533008098602295\n",
      "Training loss for batch 2015 : 0.002850959775969386\n",
      "Training loss for batch 2016 : 0.28146424889564514\n",
      "Training loss for batch 2017 : 0.32040828466415405\n",
      "Training loss for batch 2018 : 0.31449320912361145\n",
      "Training loss for batch 2019 : 0.10811886936426163\n",
      "Training loss for batch 2020 : 0.36183130741119385\n",
      "Training loss for batch 2021 : 0.4127187430858612\n",
      "Training loss for batch 2022 : 0.139994278550148\n",
      "Training loss for batch 2023 : 0.10782979428768158\n",
      "Training loss for batch 2024 : 0.3374130427837372\n",
      "Training loss for batch 2025 : 0.03989579901099205\n",
      "Training loss for batch 2026 : 0.16654610633850098\n",
      "Training loss for batch 2027 : 0.06267721951007843\n",
      "Training loss for batch 2028 : 0.03597898781299591\n",
      "Training loss for batch 2029 : 0.17500749230384827\n",
      "Training loss for batch 2030 : 0.2952718734741211\n",
      "Training loss for batch 2031 : 0.0\n",
      "Training loss for batch 2032 : 0.0074368747882544994\n",
      "Training loss for batch 2033 : 0.1261083036661148\n",
      "Training loss for batch 2034 : 0.0685926005244255\n",
      "Training loss for batch 2035 : 0.405933678150177\n",
      "Training loss for batch 2036 : 0.06526515632867813\n",
      "Training loss for batch 2037 : 0.11090295761823654\n",
      "Training loss for batch 2038 : 0.186953604221344\n",
      "Training loss for batch 2039 : 0.20130236446857452\n",
      "Training loss for batch 2040 : 0.04914906993508339\n",
      "Training loss for batch 2041 : 0.22240151464939117\n",
      "Training loss for batch 2042 : 0.159415602684021\n",
      "Training loss for batch 2043 : 0.1979682892560959\n",
      "Training loss for batch 2044 : 0.11024729907512665\n",
      "Training loss for batch 2045 : 0.3460135757923126\n",
      "Training loss for batch 2046 : 0.05706999823451042\n",
      "Training loss for batch 2047 : 0.16604579985141754\n",
      "Training loss for batch 2048 : 0.08611433953046799\n",
      "Training loss for batch 2049 : 0.39128661155700684\n",
      "Training loss for batch 2050 : 0.19291990995407104\n",
      "Training loss for batch 2051 : 0.16967296600341797\n",
      "Training loss for batch 2052 : 0.0033560297451913357\n",
      "Training loss for batch 2053 : 0.06805067509412766\n",
      "Training loss for batch 2054 : 0.2196987271308899\n",
      "Training loss for batch 2055 : 0.11178965866565704\n",
      "Training loss for batch 2056 : 0.34713852405548096\n",
      "Training loss for batch 2057 : 0.0704413577914238\n",
      "Training loss for batch 2058 : 0.15767315030097961\n",
      "Training loss for batch 2059 : 0.05159824341535568\n",
      "Training loss for batch 2060 : 0.06017579883337021\n",
      "Training loss for batch 2061 : 0.04998040571808815\n",
      "Training loss for batch 2062 : 0.05297522991895676\n",
      "Training loss for batch 2063 : 0.4740518033504486\n",
      "Training loss for batch 2064 : 0.06329258531332016\n",
      "Training loss for batch 2065 : 0.06120998039841652\n",
      "Training loss for batch 2066 : 0.3355509638786316\n",
      "Training loss for batch 2067 : 0.07769419252872467\n",
      "Training loss for batch 2068 : 0.0062583745457232\n",
      "Training loss for batch 2069 : 0.42450955510139465\n",
      "Training loss for batch 2070 : 0.31484660506248474\n",
      "Training loss for batch 2071 : 0.28127357363700867\n",
      "Training loss for batch 2072 : 0.17279934883117676\n",
      "Training loss for batch 2073 : 0.1268930733203888\n",
      "Training loss for batch 2074 : 0.1078256368637085\n",
      "Training loss for batch 2075 : 0.20714586973190308\n",
      "Training loss for batch 2076 : 0.15665939450263977\n",
      "Training loss for batch 2077 : 0.22395317256450653\n",
      "Training loss for batch 2078 : 0.2590625584125519\n",
      "Training loss for batch 2079 : 0.009303580038249493\n",
      "Training loss for batch 2080 : 0.07582413405179977\n",
      "Training loss for batch 2081 : 0.18089163303375244\n",
      "Training loss for batch 2082 : 0.24447107315063477\n",
      "Training loss for batch 2083 : 0.10970653593540192\n",
      "Training loss for batch 2084 : 0.10685674846172333\n",
      "Training loss for batch 2085 : 0.07640485465526581\n",
      "Training loss for batch 2086 : 0.19598370790481567\n",
      "Training loss for batch 2087 : 0.419346421957016\n",
      "Training loss for batch 2088 : 0.05066496878862381\n",
      "Training loss for batch 2089 : 0.214549720287323\n",
      "Training loss for batch 2090 : 0.3311149775981903\n",
      "Training loss for batch 2091 : 0.38996586203575134\n",
      "Training loss for batch 2092 : 0.012689054012298584\n",
      "Training loss for batch 2093 : 0.5910186767578125\n",
      "Training loss for batch 2094 : 0.010828684084117413\n",
      "Training loss for batch 2095 : 0.032492902129888535\n",
      "Training loss for batch 2096 : 0.22300991415977478\n",
      "Training loss for batch 2097 : 0.38265547156333923\n",
      "Training loss for batch 2098 : 0.22880467772483826\n",
      "Training loss for batch 2099 : 0.20361240208148956\n",
      "Training loss for batch 2100 : 0.2790259122848511\n",
      "Training loss for batch 2101 : 0.11974544823169708\n",
      "Training loss for batch 2102 : 0.3700544536113739\n",
      "Training loss for batch 2103 : 0.2652970850467682\n",
      "Training loss for batch 2104 : 0.0\n",
      "Training loss for batch 2105 : 0.10471497476100922\n",
      "Training loss for batch 2106 : 0.3298685550689697\n",
      "Training loss for batch 2107 : 0.170988991856575\n",
      "Training loss for batch 2108 : 0.0\n",
      "Training loss for batch 2109 : 0.4283638596534729\n",
      "Training loss for batch 2110 : 0.3142508268356323\n",
      "Training loss for batch 2111 : 0.004239032976329327\n",
      "Training loss for batch 2112 : 0.34926366806030273\n",
      "Training loss for batch 2113 : 0.3947395980358124\n",
      "Training loss for batch 2114 : 0.1512524038553238\n",
      "Training loss for batch 2115 : 0.0362103171646595\n",
      "Training loss for batch 2116 : 0.0015827914467081428\n",
      "Training loss for batch 2117 : 0.08052229881286621\n",
      "Training loss for batch 2118 : 0.17422112822532654\n",
      "Training loss for batch 2119 : 0.05081843212246895\n",
      "Training loss for batch 2120 : 0.03620975837111473\n",
      "Training loss for batch 2121 : 0.2753547430038452\n",
      "Training loss for batch 2122 : 0.18326829373836517\n",
      "Training loss for batch 2123 : 0.13817749917507172\n",
      "Training loss for batch 2124 : 0.39194804430007935\n",
      "Training loss for batch 2125 : 0.18837204575538635\n",
      "Training loss for batch 2126 : 0.10477638244628906\n",
      "Training loss for batch 2127 : 0.21780197322368622\n",
      "Training loss for batch 2128 : 0.017383893951773643\n",
      "Training loss for batch 2129 : 0.0\n",
      "Training loss for batch 2130 : 0.0003459402360022068\n",
      "Training loss for batch 2131 : 0.04101642593741417\n",
      "Training loss for batch 2132 : 0.4668850600719452\n",
      "Training loss for batch 2133 : 0.018437584862113\n",
      "Training loss for batch 2134 : 0.22630532085895538\n",
      "Training loss for batch 2135 : 0.024867011234164238\n",
      "Training loss for batch 2136 : 0.012484217993915081\n",
      "Training loss for batch 2137 : 0.21710792183876038\n",
      "Training loss for batch 2138 : 0.09616666287183762\n",
      "Training loss for batch 2139 : 0.3708176910877228\n",
      "Training loss for batch 2140 : 0.33435189723968506\n",
      "Training loss for batch 2141 : 0.14911772310733795\n",
      "Training loss for batch 2142 : 0.08754389733076096\n",
      "Training loss for batch 2143 : 0.22279436886310577\n",
      "Training loss for batch 2144 : 0.00744706392288208\n",
      "Training loss for batch 2145 : 0.06104135140776634\n",
      "Training loss for batch 2146 : 0.10682006925344467\n",
      "Training loss for batch 2147 : 0.11588098108768463\n",
      "Training loss for batch 2148 : 0.259940505027771\n",
      "Training loss for batch 2149 : 0.2593071460723877\n",
      "Training loss for batch 2150 : 0.19610361754894257\n",
      "Training loss for batch 2151 : 0.10909263789653778\n",
      "Training loss for batch 2152 : 0.08119580149650574\n",
      "Training loss for batch 2153 : 0.015187578275799751\n",
      "Training loss for batch 2154 : 0.5740969777107239\n",
      "Training loss for batch 2155 : 0.2236202359199524\n",
      "Training loss for batch 2156 : 0.1171903908252716\n",
      "Training loss for batch 2157 : 0.25458604097366333\n",
      "Training loss for batch 2158 : 0.03853045403957367\n",
      "Training loss for batch 2159 : 0.1811019629240036\n",
      "Training loss for batch 2160 : 0.09045298397541046\n",
      "Training loss for batch 2161 : 0.07877032458782196\n",
      "Training loss for batch 2162 : 0.26194244623184204\n",
      "Training loss for batch 2163 : 0.1034439206123352\n",
      "Training loss for batch 2164 : 0.1491064429283142\n",
      "Training loss for batch 2165 : 0.2736208140850067\n",
      "Training loss for batch 2166 : 0.2709997594356537\n",
      "Training loss for batch 2167 : 0.3938537538051605\n",
      "Training loss for batch 2168 : 0.017277956008911133\n",
      "Training loss for batch 2169 : 0.16802963614463806\n",
      "Training loss for batch 2170 : 0.2398047298192978\n",
      "Training loss for batch 2171 : 0.4154665172100067\n",
      "Training loss for batch 2172 : 0.2198057770729065\n",
      "Training loss for batch 2173 : 0.14106512069702148\n",
      "Training loss for batch 2174 : 0.10874699801206589\n",
      "Training loss for batch 2175 : 0.1761569231748581\n",
      "Training loss for batch 2176 : 0.05488980934023857\n",
      "Training loss for batch 2177 : 0.007966816425323486\n",
      "Training loss for batch 2178 : 0.3506943881511688\n",
      "Training loss for batch 2179 : 0.33363285660743713\n",
      "Training loss for batch 2180 : 0.1638420820236206\n",
      "Training loss for batch 2181 : 0.10161187499761581\n",
      "Training loss for batch 2182 : 0.505135178565979\n",
      "Training loss for batch 2183 : 0.1561756432056427\n",
      "Training loss for batch 2184 : 0.35902392864227295\n",
      "Training loss for batch 2185 : 0.21787555515766144\n",
      "Training loss for batch 2186 : 0.014679697342216969\n",
      "Training loss for batch 2187 : 0.4413398206233978\n",
      "Training loss for batch 2188 : 0.5266745090484619\n",
      "Training loss for batch 2189 : 0.40192100405693054\n",
      "Training loss for batch 2190 : 0.1563131958246231\n",
      "Training loss for batch 2191 : 0.1638568490743637\n",
      "Training loss for batch 2192 : 0.30602648854255676\n",
      "Training loss for batch 2193 : 0.17371411621570587\n",
      "Training loss for batch 2194 : 0.0843740701675415\n",
      "Training loss for batch 2195 : 0.01750784181058407\n",
      "Training loss for batch 2196 : 0.19776910543441772\n",
      "Training loss for batch 2197 : 0.05211101472377777\n",
      "Training loss for batch 2198 : 0.027097342535853386\n",
      "Training loss for batch 2199 : 0.008557605557143688\n",
      "Training loss for batch 2200 : 0.034878894686698914\n",
      "Training loss for batch 2201 : 0.1053963452577591\n",
      "Training loss for batch 2202 : 0.019858144223690033\n",
      "Training loss for batch 2203 : 0.44900578260421753\n",
      "Training loss for batch 2204 : 0.0010300676804035902\n",
      "Training loss for batch 2205 : 0.3573478162288666\n",
      "Training loss for batch 2206 : 0.07644588500261307\n",
      "Training loss for batch 2207 : 0.26060763001441956\n",
      "Training loss for batch 2208 : 0.044182512909173965\n",
      "Training loss for batch 2209 : 0.374376505613327\n",
      "Training loss for batch 2210 : 0.18968908488750458\n",
      "Training loss for batch 2211 : 0.2750161588191986\n",
      "Training loss for batch 2212 : 0.3499530553817749\n",
      "Training loss for batch 2213 : 0.024488139897584915\n",
      "Training loss for batch 2214 : 0.14343082904815674\n",
      "Training loss for batch 2215 : 0.21035893261432648\n",
      "Training loss for batch 2216 : 0.20916272699832916\n",
      "Training loss for batch 2217 : 0.05295184254646301\n",
      "Training loss for batch 2218 : 0.1963849812746048\n",
      "Training loss for batch 2219 : 0.13962125778198242\n",
      "Training loss for batch 2220 : 0.37797123193740845\n",
      "Training loss for batch 2221 : 0.1058470755815506\n",
      "Training loss for batch 2222 : 0.28264081478118896\n",
      "Training loss for batch 2223 : 0.5165051221847534\n",
      "Training loss for batch 2224 : 0.35318833589553833\n",
      "Training loss for batch 2225 : 0.017921267077326775\n",
      "Training loss for batch 2226 : 0.4196789264678955\n",
      "Training loss for batch 2227 : 0.0\n",
      "Training loss for batch 2228 : 0.22534526884555817\n",
      "Training loss for batch 2229 : 0.35226696729660034\n",
      "Training loss for batch 2230 : 0.061470795422792435\n",
      "Training loss for batch 2231 : 0.017221327871084213\n",
      "Training loss for batch 2232 : 0.03408772125840187\n",
      "Training loss for batch 2233 : 0.1153622716665268\n",
      "Training loss for batch 2234 : 0.02054527960717678\n",
      "Training loss for batch 2235 : 0.3419489860534668\n",
      "Training loss for batch 2236 : 0.07649786025285721\n",
      "Training loss for batch 2237 : 0.10595548897981644\n",
      "Training loss for batch 2238 : 0.14922574162483215\n",
      "Training loss for batch 2239 : 0.13001026213169098\n",
      "Training loss for batch 2240 : 0.4287605285644531\n",
      "Training loss for batch 2241 : 0.3397942781448364\n",
      "Training loss for batch 2242 : 0.022965947166085243\n",
      "Training loss for batch 2243 : 0.297672837972641\n",
      "Training loss for batch 2244 : 0.2039850950241089\n",
      "Training loss for batch 2245 : 0.08510937541723251\n",
      "Training loss for batch 2246 : 0.028466003015637398\n",
      "Training loss for batch 2247 : 0.00035443762317299843\n",
      "Training loss for batch 2248 : 0.21228623390197754\n",
      "Training loss for batch 2249 : 0.3291540741920471\n",
      "Training loss for batch 2250 : 0.15569505095481873\n",
      "Training loss for batch 2251 : 0.13435479998588562\n",
      "Training loss for batch 2252 : 0.036050546914339066\n",
      "Training loss for batch 2253 : 0.13207058608531952\n",
      "Training loss for batch 2254 : 0.21199391782283783\n",
      "Training loss for batch 2255 : 0.1202545017004013\n",
      "Training loss for batch 2256 : 0.020598025992512703\n",
      "Training loss for batch 2257 : 0.000207761928322725\n",
      "Training loss for batch 2258 : 0.2277367264032364\n",
      "Training loss for batch 2259 : 0.4159153997898102\n",
      "Training loss for batch 2260 : 0.011409725062549114\n",
      "Training loss for batch 2261 : 0.23219607770442963\n",
      "Training loss for batch 2262 : 0.005181163549423218\n",
      "Training loss for batch 2263 : 0.0\n",
      "Training loss for batch 2264 : 0.06023400276899338\n",
      "Training loss for batch 2265 : 0.2413523942232132\n",
      "Training loss for batch 2266 : 0.01000523567199707\n",
      "Training loss for batch 2267 : 0.05427674949169159\n",
      "Training loss for batch 2268 : 0.0563727542757988\n",
      "Training loss for batch 2269 : 0.034252554178237915\n",
      "Training loss for batch 2270 : 0.11881566047668457\n",
      "Training loss for batch 2271 : 0.003240446327254176\n",
      "Training loss for batch 2272 : 0.010787938721477985\n",
      "Training loss for batch 2273 : 0.39290064573287964\n",
      "Training loss for batch 2274 : 0.24718458950519562\n",
      "Training loss for batch 2275 : 0.17836898565292358\n",
      "Training loss for batch 2276 : 0.14099888503551483\n",
      "Training loss for batch 2277 : 0.042511843144893646\n",
      "Training loss for batch 2278 : 0.6172519326210022\n",
      "Training loss for batch 2279 : 0.32218676805496216\n",
      "Training loss for batch 2280 : 0.10409224033355713\n",
      "Training loss for batch 2281 : 0.0\n",
      "Training loss for batch 2282 : 0.09180599451065063\n",
      "Training loss for batch 2283 : 0.08012722432613373\n",
      "Training loss for batch 2284 : 0.15023337304592133\n",
      "Training loss for batch 2285 : 0.048616498708724976\n",
      "Training loss for batch 2286 : 0.05228260159492493\n",
      "Training loss for batch 2287 : 0.7306531071662903\n",
      "Training loss for batch 2288 : 0.06458781659603119\n",
      "Training loss for batch 2289 : 0.06502138078212738\n",
      "Training loss for batch 2290 : 0.06528636813163757\n",
      "Training loss for batch 2291 : 0.02912372536957264\n",
      "Training loss for batch 2292 : 0.014882366172969341\n",
      "Training loss for batch 2293 : 0.0\n",
      "Training loss for batch 2294 : 0.16387908160686493\n",
      "Training loss for batch 2295 : 0.03575962409377098\n",
      "Training loss for batch 2296 : 0.13327009975910187\n",
      "Training loss for batch 2297 : 0.021450147032737732\n",
      "Training loss for batch 2298 : 0.09982344508171082\n",
      "Training loss for batch 2299 : 0.34676772356033325\n",
      "Training loss for batch 2300 : 0.22333292663097382\n",
      "Training loss for batch 2301 : 0.25216323137283325\n",
      "Training loss for batch 2302 : 0.30706122517585754\n",
      "Training loss for batch 2303 : 0.00960669107735157\n",
      "Training loss for batch 2304 : 0.037905994802713394\n",
      "Training loss for batch 2305 : 0.025993365794420242\n",
      "Training loss for batch 2306 : 0.10361529141664505\n",
      "Training loss for batch 2307 : 0.008065856993198395\n",
      "Training loss for batch 2308 : 0.06961879879236221\n",
      "Training loss for batch 2309 : 0.12771031260490417\n",
      "Training loss for batch 2310 : 0.07258622348308563\n",
      "Training loss for batch 2311 : 0.22512519359588623\n",
      "Training loss for batch 2312 : 0.0016731586074456573\n",
      "Training loss for batch 2313 : 0.24107646942138672\n",
      "Training loss for batch 2314 : 0.24564163386821747\n",
      "Training loss for batch 2315 : 0.3390938937664032\n",
      "Training loss for batch 2316 : 0.8448292016983032\n",
      "Training loss for batch 2317 : 0.11390073597431183\n",
      "Training loss for batch 2318 : 0.02170083485543728\n",
      "Training loss for batch 2319 : 0.0\n",
      "Training loss for batch 2320 : 0.24542535841464996\n",
      "Training loss for batch 2321 : 0.07657783478498459\n",
      "Training loss for batch 2322 : 0.01602400466799736\n",
      "Training loss for batch 2323 : 0.17002016305923462\n",
      "Training loss for batch 2324 : 0.03258702903985977\n",
      "Training loss for batch 2325 : 0.30444905161857605\n",
      "Training loss for batch 2326 : 0.3947505056858063\n",
      "Training loss for batch 2327 : 0.023770073428750038\n",
      "Training loss for batch 2328 : 0.021854020655155182\n",
      "Training loss for batch 2329 : 0.07488465309143066\n",
      "Training loss for batch 2330 : 0.03525927662849426\n",
      "Training loss for batch 2331 : 0.16150367259979248\n",
      "Training loss for batch 2332 : 0.17893895506858826\n",
      "Training loss for batch 2333 : 0.046554867178201675\n",
      "Training loss for batch 2334 : 0.33870476484298706\n",
      "Training loss for batch 2335 : 0.012450228445231915\n",
      "Training loss for batch 2336 : 0.060673464089632034\n",
      "Training loss for batch 2337 : 0.07867059856653214\n",
      "Training loss for batch 2338 : 0.23361168801784515\n",
      "Training loss for batch 2339 : 0.5710444450378418\n",
      "Training loss for batch 2340 : 0.4910634160041809\n",
      "Training loss for batch 2341 : 0.023116029798984528\n",
      "Training loss for batch 2342 : 0.1946825087070465\n",
      "Training loss for batch 2343 : 0.024711010977625847\n",
      "Training loss for batch 2344 : 0.02901478298008442\n",
      "Training loss for batch 2345 : 0.29497650265693665\n",
      "Training loss for batch 2346 : 0.12860912084579468\n",
      "Training loss for batch 2347 : 0.006176104303449392\n",
      "Training loss for batch 2348 : 0.04891947656869888\n",
      "Training loss for batch 2349 : 0.17549483478069305\n",
      "Training loss for batch 2350 : 0.015896564349532127\n",
      "Training loss for batch 2351 : 0.4879154860973358\n",
      "Training loss for batch 2352 : 0.04395630955696106\n",
      "Training loss for batch 2353 : 0.14625506103038788\n",
      "Training loss for batch 2354 : 0.36260226368904114\n",
      "Training loss for batch 2355 : 0.0015341639518737793\n",
      "Training loss for batch 2356 : 0.03540298342704773\n",
      "Training loss for batch 2357 : 0.34758326411247253\n",
      "Training loss for batch 2358 : 0.24072392284870148\n",
      "Training loss for batch 2359 : 0.1642019897699356\n",
      "Training loss for batch 2360 : 0.5248368978500366\n",
      "Training loss for batch 2361 : 0.4847542643547058\n",
      "Training loss for batch 2362 : 0.12428641319274902\n",
      "Training loss for batch 2363 : 0.21718555688858032\n",
      "Training loss for batch 2364 : 0.17803041636943817\n",
      "Training loss for batch 2365 : 0.038465097546577454\n",
      "Training loss for batch 2366 : 0.021534856408834457\n",
      "Training loss for batch 2367 : 0.605242133140564\n",
      "Training loss for batch 2368 : 0.2748000919818878\n",
      "Training loss for batch 2369 : 0.002848117845132947\n",
      "Training loss for batch 2370 : 0.023687278851866722\n",
      "Training loss for batch 2371 : 0.15836596488952637\n",
      "Training loss for batch 2372 : 0.1906691938638687\n",
      "Training loss for batch 2373 : 0.1888258308172226\n",
      "Training loss for batch 2374 : 0.022014174610376358\n",
      "Training loss for batch 2375 : 0.005624058190733194\n",
      "Training loss for batch 2376 : 0.12539096176624298\n",
      "Training loss for batch 2377 : 0.11100302636623383\n",
      "Training loss for batch 2378 : 0.27523648738861084\n",
      "Training loss for batch 2379 : 0.10219413042068481\n",
      "Training loss for batch 2380 : 0.11094535142183304\n",
      "Training loss for batch 2381 : 0.1892394870519638\n",
      "Training loss for batch 2382 : 0.3578559160232544\n",
      "Training loss for batch 2383 : 0.409247487783432\n",
      "Training loss for batch 2384 : 0.48220616579055786\n",
      "Training loss for batch 2385 : 0.43490391969680786\n",
      "Training loss for batch 2386 : 0.1459110975265503\n",
      "Training loss for batch 2387 : 0.0196856502443552\n",
      "Training loss for batch 2388 : 0.009833849966526031\n",
      "Training loss for batch 2389 : 0.07856988161802292\n",
      "Training loss for batch 2390 : 0.34382373094558716\n",
      "Training loss for batch 2391 : 0.10953569412231445\n",
      "Training loss for batch 2392 : 0.0596020333468914\n",
      "Training loss for batch 2393 : 0.0\n",
      "Training loss for batch 2394 : 0.3256158232688904\n",
      "Training loss for batch 2395 : 0.32848915457725525\n",
      "Training loss for batch 2396 : 0.14001058042049408\n",
      "Training loss for batch 2397 : 0.06442008167505264\n",
      "Training loss for batch 2398 : 0.2062823325395584\n",
      "Training loss for batch 2399 : 0.09377674013376236\n",
      "Training loss for batch 2400 : 0.14344145357608795\n",
      "Training loss for batch 2401 : 0.40813031792640686\n",
      "Training loss for batch 2402 : 0.012706609442830086\n",
      "Training loss for batch 2403 : 0.022899216040968895\n",
      "Training loss for batch 2404 : 0.09251253306865692\n",
      "Training loss for batch 2405 : 0.40009528398513794\n",
      "Training loss for batch 2406 : 0.2176775187253952\n",
      "Training loss for batch 2407 : 0.12131280452013016\n",
      "Training loss for batch 2408 : 0.09731151163578033\n",
      "Training loss for batch 2409 : 0.047130875289440155\n",
      "Training loss for batch 2410 : 0.036726199090480804\n",
      "Training loss for batch 2411 : 0.08520244807004929\n",
      "Training loss for batch 2412 : 0.029951004311442375\n",
      "Training loss for batch 2413 : 0.0\n",
      "Training loss for batch 2414 : 0.08934096246957779\n",
      "Training loss for batch 2415 : 0.05779837444424629\n",
      "Training loss for batch 2416 : 0.4284481108188629\n",
      "Training loss for batch 2417 : 0.3226046562194824\n",
      "Training loss for batch 2418 : 0.11681295186281204\n",
      "Training loss for batch 2419 : 0.30190974473953247\n",
      "Training loss for batch 2420 : 0.22536474466323853\n",
      "Training loss for batch 2421 : 0.26938527822494507\n",
      "Training loss for batch 2422 : 0.0\n",
      "Training loss for batch 2423 : 0.22910447418689728\n",
      "Training loss for batch 2424 : 0.1310981810092926\n",
      "Training loss for batch 2425 : 0.5491416454315186\n",
      "Training loss for batch 2426 : 0.22947117686271667\n",
      "Training loss for batch 2427 : 0.3745625913143158\n",
      "Training loss for batch 2428 : 0.13746443390846252\n",
      "Training loss for batch 2429 : 0.13625523447990417\n",
      "Training loss for batch 2430 : 0.2573744058609009\n",
      "Training loss for batch 2431 : 0.07579315453767776\n",
      "Training loss for batch 2432 : 0.21027249097824097\n",
      "Training loss for batch 2433 : 0.04469829052686691\n",
      "Training loss for batch 2434 : 0.5323823094367981\n",
      "Training loss for batch 2435 : 0.3807876408100128\n",
      "Training loss for batch 2436 : 0.04318828880786896\n",
      "Training loss for batch 2437 : 0.13607677817344666\n",
      "Training loss for batch 2438 : 0.1309593915939331\n",
      "Training loss for batch 2439 : 0.47223150730133057\n",
      "Training loss for batch 2440 : 0.012498414143919945\n",
      "Training loss for batch 2441 : 0.03431817516684532\n",
      "Training loss for batch 2442 : 0.11741672456264496\n",
      "Training loss for batch 2443 : 0.07912309467792511\n",
      "Training loss for batch 2444 : 0.1917600929737091\n",
      "Training loss for batch 2445 : 0.16957010328769684\n",
      "Training loss for batch 2446 : 0.7762295603752136\n",
      "Training loss for batch 2447 : 0.07982618361711502\n",
      "Training loss for batch 2448 : 0.07184276729822159\n",
      "Training loss for batch 2449 : 0.08693119138479233\n",
      "Training loss for batch 2450 : 0.07644917070865631\n",
      "Training loss for batch 2451 : 0.007040091790258884\n",
      "Training loss for batch 2452 : 0.04251393675804138\n",
      "Training loss for batch 2453 : 0.18939897418022156\n",
      "Training loss for batch 2454 : 0.13277965784072876\n",
      "Training loss for batch 2455 : 0.12605664134025574\n",
      "Training loss for batch 2456 : 0.09248372167348862\n",
      "Training loss for batch 2457 : 1.0967841148376465\n",
      "Training loss for batch 2458 : 0.2038414031267166\n",
      "Training loss for batch 2459 : 0.30449149012565613\n",
      "Training loss for batch 2460 : 0.19745902717113495\n",
      "Training loss for batch 2461 : 0.055812589824199677\n",
      "Training loss for batch 2462 : 0.17520509660243988\n",
      "Training loss for batch 2463 : 0.31136658787727356\n",
      "Training loss for batch 2464 : 0.02865641750395298\n",
      "Training loss for batch 2465 : 0.07960839569568634\n",
      "Training loss for batch 2466 : 0.33921223878860474\n",
      "Training loss for batch 2467 : 0.013210266828536987\n",
      "Training loss for batch 2468 : 0.008186202496290207\n",
      "Training loss for batch 2469 : 0.17995113134384155\n",
      "Training loss for batch 2470 : 0.21406632661819458\n",
      "Training loss for batch 2471 : 0.016020070761442184\n",
      "Training loss for batch 2472 : 0.10233219712972641\n",
      "Training loss for batch 2473 : 0.25190314650535583\n",
      "Training loss for batch 2474 : 0.1568521410226822\n",
      "Training loss for batch 2475 : 0.08207786828279495\n",
      "Training loss for batch 2476 : 0.36709922552108765\n",
      "Training loss for batch 2477 : 0.012538939714431763\n",
      "Training loss for batch 2478 : 0.003185175359249115\n",
      "Training loss for batch 2479 : 0.3531571328639984\n",
      "Training loss for batch 2480 : 0.03423251584172249\n",
      "Training loss for batch 2481 : 0.10904964059591293\n",
      "Training loss for batch 2482 : 0.23170843720436096\n",
      "Training loss for batch 2483 : 0.08375181257724762\n",
      "Training loss for batch 2484 : 0.1502651572227478\n",
      "Training loss for batch 2485 : 0.15265558660030365\n",
      "Training loss for batch 2486 : 0.48871901631355286\n",
      "Training loss for batch 2487 : 0.12674108147621155\n",
      "Training loss for batch 2488 : 0.09157301485538483\n",
      "Training loss for batch 2489 : 0.0005368292331695557\n",
      "Training loss for batch 2490 : 0.3253975212574005\n",
      "Training loss for batch 2491 : 0.038334328681230545\n",
      "Training loss for batch 2492 : 0.07792972773313522\n",
      "Training loss for batch 2493 : 0.027241313830018044\n",
      "Training loss for batch 2494 : 0.383122056722641\n",
      "Training loss for batch 2495 : 0.18690794706344604\n",
      "Training loss for batch 2496 : 0.5028801560401917\n",
      "Training loss for batch 2497 : 0.14235956966876984\n",
      "Training loss for batch 2498 : 0.15814708173274994\n",
      "Training loss for batch 2499 : 0.04241424798965454\n",
      "Training loss for batch 2500 : 0.4308438003063202\n",
      "Training loss for batch 2501 : 0.12471386790275574\n",
      "Training loss for batch 2502 : 0.30446726083755493\n",
      "Training loss for batch 2503 : 0.16611503064632416\n",
      "Training loss for batch 2504 : 0.0018426866736263037\n",
      "Training loss for batch 2505 : 0.18808594346046448\n",
      "Training loss for batch 2506 : 0.021590495482087135\n",
      "Training loss for batch 2507 : 0.09359493106603622\n",
      "Training loss for batch 2508 : 0.08811969310045242\n",
      "Training loss for batch 2509 : 0.14927850663661957\n",
      "Training loss for batch 2510 : 0.001448130700737238\n",
      "Training loss for batch 2511 : 0.37545061111450195\n",
      "Training loss for batch 2512 : 0.08795130252838135\n",
      "Training loss for batch 2513 : 0.06043943762779236\n",
      "Training loss for batch 2514 : 0.34624430537223816\n",
      "Training loss for batch 2515 : 0.390415757894516\n",
      "Training loss for batch 2516 : 0.3104427754878998\n",
      "Training loss for batch 2517 : 0.0021557242143899202\n",
      "Training loss for batch 2518 : 0.06073731184005737\n",
      "Training loss for batch 2519 : 0.10194145888090134\n",
      "Training loss for batch 2520 : 0.43329527974128723\n",
      "Training loss for batch 2521 : 0.1407526433467865\n",
      "Training loss for batch 2522 : 0.23669719696044922\n",
      "Training loss for batch 2523 : 0.1265724003314972\n",
      "Training loss for batch 2524 : 0.31068482995033264\n",
      "Training loss for batch 2525 : 0.26527732610702515\n",
      "Training loss for batch 2526 : 0.030044876039028168\n",
      "Training loss for batch 2527 : 0.03123198077082634\n",
      "Training loss for batch 2528 : 0.14194580912590027\n",
      "Training loss for batch 2529 : 0.14754417538642883\n",
      "Training loss for batch 2530 : 0.06891819089651108\n",
      "Training loss for batch 2531 : 0.4980621039867401\n",
      "Training loss for batch 2532 : 0.14077861607074738\n",
      "Training loss for batch 2533 : 0.033427152782678604\n",
      "Training loss for batch 2534 : 0.3024526834487915\n",
      "Training loss for batch 2535 : 0.06444968283176422\n",
      "Training loss for batch 2536 : 0.32434171438217163\n",
      "Training loss for batch 2537 : 0.11720302700996399\n",
      "Training loss for batch 2538 : 0.07357333600521088\n",
      "Training loss for batch 2539 : 0.21963603794574738\n",
      "Training loss for batch 2540 : 0.4108337163925171\n",
      "Training loss for batch 2541 : 0.020258024334907532\n",
      "Training loss for batch 2542 : 0.08833184093236923\n",
      "Training loss for batch 2543 : 0.013018098659813404\n",
      "Training loss for batch 2544 : 0.28505295515060425\n",
      "Training loss for batch 2545 : 0.4328984022140503\n",
      "Training loss for batch 2546 : 0.3096894323825836\n",
      "Training loss for batch 2547 : 0.09812524169683456\n",
      "Training loss for batch 2548 : 0.399893581867218\n",
      "Training loss for batch 2549 : 0.18474693596363068\n",
      "Training loss for batch 2550 : 0.006124932784587145\n",
      "Training loss for batch 2551 : 0.23357881605625153\n",
      "Training loss for batch 2552 : 0.14334505796432495\n",
      "Training loss for batch 2553 : 0.03472849726676941\n",
      "Training loss for batch 2554 : 0.03996032476425171\n",
      "Training loss for batch 2555 : 0.049231115728616714\n",
      "Training loss for batch 2556 : 0.0\n",
      "Training loss for batch 2557 : 0.025211835280060768\n",
      "Training loss for batch 2558 : 0.12209498137235641\n",
      "Training loss for batch 2559 : 0.4284400939941406\n",
      "Training loss for batch 2560 : 0.1065102219581604\n",
      "Training loss for batch 2561 : 0.048560258001089096\n",
      "Training loss for batch 2562 : 0.4183789789676666\n",
      "Training loss for batch 2563 : 0.00014692275726702064\n",
      "Training loss for batch 2564 : 0.17942175269126892\n",
      "Training loss for batch 2565 : 0.20098035037517548\n",
      "Training loss for batch 2566 : 0.05174442380666733\n",
      "Training loss for batch 2567 : 0.069730244576931\n",
      "Training loss for batch 2568 : 0.16797053813934326\n",
      "Training loss for batch 2569 : 0.6052705645561218\n",
      "Training loss for batch 2570 : 0.004550646059215069\n",
      "Training loss for batch 2571 : 0.055893998593091965\n",
      "Training loss for batch 2572 : 0.8201552629470825\n",
      "Training loss for batch 2573 : 0.052753694355487823\n",
      "Training loss for batch 2574 : 0.20480157434940338\n",
      "Training loss for batch 2575 : 0.01378303300589323\n",
      "Training loss for batch 2576 : 0.012668918818235397\n",
      "Training loss for batch 2577 : 0.13664816319942474\n",
      "Training loss for batch 2578 : 0.032150547951459885\n",
      "Training loss for batch 2579 : 0.169660747051239\n",
      "Training loss for batch 2580 : 0.07254669070243835\n",
      "Training loss for batch 2581 : 0.17934398353099823\n",
      "Training loss for batch 2582 : 0.19391976296901703\n",
      "Training loss for batch 2583 : 0.02270936779677868\n",
      "Training loss for batch 2584 : 0.13587944209575653\n",
      "Training loss for batch 2585 : 0.0755230188369751\n",
      "Training loss for batch 2586 : 0.32613909244537354\n",
      "Training loss for batch 2587 : 0.13023650646209717\n",
      "Training loss for batch 2588 : 0.2201220989227295\n",
      "Training loss for batch 2589 : 0.06485394388437271\n",
      "Training loss for batch 2590 : 0.10780716687440872\n",
      "Training loss for batch 2591 : 0.2670564353466034\n",
      "Training loss for batch 2592 : 0.5246219635009766\n",
      "Training loss for batch 2593 : 0.031104115769267082\n",
      "Training loss for batch 2594 : 0.0959254652261734\n",
      "Training loss for batch 2595 : 0.23352237045764923\n",
      "Training loss for batch 2596 : 0.27837058901786804\n",
      "Training loss for batch 2597 : 0.08194800466299057\n",
      "Training loss for batch 2598 : 0.29597678780555725\n",
      "Training loss for batch 2599 : 0.17594297230243683\n",
      "Training loss for batch 2600 : 0.4963391423225403\n",
      "Training loss for batch 2601 : 0.0261500533670187\n",
      "Training loss for batch 2602 : 0.339378297328949\n",
      "Training loss for batch 2603 : 0.14047172665596008\n",
      "Training loss for batch 2604 : 0.3433816730976105\n",
      "Training loss for batch 2605 : 0.529577374458313\n",
      "Training loss for batch 2606 : 0.08263484388589859\n",
      "Training loss for batch 2607 : 0.02891441434621811\n",
      "Training loss for batch 2608 : 0.08045316487550735\n",
      "Training loss for batch 2609 : 0.1967330425977707\n",
      "Training loss for batch 2610 : 0.1173572987318039\n",
      "Training loss for batch 2611 : 0.1280437707901001\n",
      "Training loss for batch 2612 : 0.2789570093154907\n",
      "Training loss for batch 2613 : 0.19761444628238678\n",
      "Training loss for batch 2614 : 0.12214213609695435\n",
      "Training loss for batch 2615 : 0.07301729917526245\n",
      "Training loss for batch 2616 : 0.28754040598869324\n",
      "Training loss for batch 2617 : 0.17282375693321228\n",
      "Training loss for batch 2618 : 0.3775826394557953\n",
      "Training loss for batch 2619 : 0.33164364099502563\n",
      "Training loss for batch 2620 : 0.09707213938236237\n",
      "Training loss for batch 2621 : 0.27888429164886475\n",
      "Training loss for batch 2622 : 0.0\n",
      "Training loss for batch 2623 : 0.17228730022907257\n",
      "Training loss for batch 2624 : 0.014742732048034668\n",
      "Training loss for batch 2625 : 0.3610699474811554\n",
      "Training loss for batch 2626 : 0.07675344496965408\n",
      "Training loss for batch 2627 : 0.16515721380710602\n",
      "Training loss for batch 2628 : 0.0\n",
      "Training loss for batch 2629 : 0.10264541208744049\n",
      "Training loss for batch 2630 : 0.16025368869304657\n",
      "Training loss for batch 2631 : 0.08509767055511475\n",
      "Training loss for batch 2632 : 0.23772750794887543\n",
      "Training loss for batch 2633 : 0.009272541850805283\n",
      "Training loss for batch 2634 : 0.07666877657175064\n",
      "Training loss for batch 2635 : 0.08344043046236038\n",
      "Training loss for batch 2636 : 0.07350816577672958\n",
      "Training loss for batch 2637 : 0.1884281039237976\n",
      "Training loss for batch 2638 : 0.014752290211617947\n",
      "Training loss for batch 2639 : 0.12240461260080338\n",
      "Training loss for batch 2640 : 0.013395227491855621\n",
      "Training loss for batch 2641 : 0.020640505477786064\n",
      "Training loss for batch 2642 : 0.19481633603572845\n",
      "Training loss for batch 2643 : 0.024404680356383324\n",
      "Training loss for batch 2644 : 0.04646073654294014\n",
      "Training loss for batch 2645 : 0.09969231486320496\n",
      "Training loss for batch 2646 : 0.15123574435710907\n",
      "Training loss for batch 2647 : 0.07173759490251541\n",
      "Training loss for batch 2648 : 0.03027769923210144\n",
      "Training loss for batch 2649 : 0.008250202052295208\n",
      "Training loss for batch 2650 : 0.45850181579589844\n",
      "Training loss for batch 2651 : 0.23618215322494507\n",
      "Training loss for batch 2652 : 0.5679007172584534\n",
      "Training loss for batch 2653 : 0.24513624608516693\n",
      "Training loss for batch 2654 : 0.3515976071357727\n",
      "Training loss for batch 2655 : 0.436215341091156\n",
      "Training loss for batch 2656 : 0.21844248473644257\n",
      "Training loss for batch 2657 : 0.2368900030851364\n",
      "Training loss for batch 2658 : 0.1264047473669052\n",
      "Training loss for batch 2659 : 0.21947480738162994\n",
      "Training loss for batch 2660 : 0.2521066665649414\n",
      "Training loss for batch 2661 : 0.014961936511099339\n",
      "Training loss for batch 2662 : 0.31799691915512085\n",
      "Training loss for batch 2663 : 0.16581131517887115\n",
      "Training loss for batch 2664 : 0.30724236369132996\n",
      "Training loss for batch 2665 : 0.1873231679201126\n",
      "Training loss for batch 2666 : 0.2787007987499237\n",
      "Training loss for batch 2667 : 0.14358212053775787\n",
      "Training loss for batch 2668 : 0.4476706087589264\n",
      "Training loss for batch 2669 : 0.04722040146589279\n",
      "Training loss for batch 2670 : 0.026793133467435837\n",
      "Training loss for batch 2671 : 0.06366178393363953\n",
      "Training loss for batch 2672 : 0.14004620909690857\n",
      "Training loss for batch 2673 : 0.06950046867132187\n",
      "Training loss for batch 2674 : 0.19447116553783417\n",
      "Training loss for batch 2675 : 0.47844818234443665\n",
      "Training loss for batch 2676 : 0.028920816257596016\n",
      "Training loss for batch 2677 : 0.18857310712337494\n",
      "Training loss for batch 2678 : 0.0415213406085968\n",
      "Training loss for batch 2679 : 0.2016289383172989\n",
      "Training loss for batch 2680 : 0.04523720592260361\n",
      "Training loss for batch 2681 : 0.2294142246246338\n",
      "Training loss for batch 2682 : 0.25265470147132874\n",
      "Training loss for batch 2683 : 0.15641751885414124\n",
      "Training loss for batch 2684 : 0.10133382678031921\n",
      "Training loss for batch 2685 : 0.13848842680454254\n",
      "Training loss for batch 2686 : 0.021826403215527534\n",
      "Training loss for batch 2687 : 0.00012126554793212563\n",
      "Training loss for batch 2688 : 0.38263651728630066\n",
      "Training loss for batch 2689 : 0.3281756043434143\n",
      "Training loss for batch 2690 : 0.22913725674152374\n",
      "Training loss for batch 2691 : 0.1134166494011879\n",
      "Training loss for batch 2692 : 0.062022190541028976\n",
      "Training loss for batch 2693 : 0.11504565924406052\n",
      "Training loss for batch 2694 : 0.11181905120611191\n",
      "Training loss for batch 2695 : 0.1817949116230011\n",
      "Training loss for batch 2696 : 0.11497728526592255\n",
      "Training loss for batch 2697 : 0.2223813682794571\n",
      "Training loss for batch 2698 : 0.10181990265846252\n",
      "Training loss for batch 2699 : 0.13417698442935944\n",
      "Training loss for batch 2700 : 0.14671841263771057\n",
      "Training loss for batch 2701 : 0.16466137766838074\n",
      "Training loss for batch 2702 : 0.05320847034454346\n",
      "Training loss for batch 2703 : 0.22823670506477356\n",
      "Training loss for batch 2704 : 0.1399518996477127\n",
      "Training loss for batch 2705 : 0.0021003982983529568\n",
      "Training loss for batch 2706 : 0.17361074686050415\n",
      "Training loss for batch 2707 : 0.03787839040160179\n",
      "Training loss for batch 2708 : 0.09493283927440643\n",
      "Training loss for batch 2709 : 0.35878509283065796\n",
      "Training loss for batch 2710 : 0.021181300282478333\n",
      "Training loss for batch 2711 : 0.07259780168533325\n",
      "Training loss for batch 2712 : 0.00014623226888943464\n",
      "Training loss for batch 2713 : 0.1145300343632698\n",
      "Training loss for batch 2714 : 0.39260321855545044\n",
      "Training loss for batch 2715 : 0.11484984308481216\n",
      "Training loss for batch 2716 : 0.07508829236030579\n",
      "Training loss for batch 2717 : 0.06494217365980148\n",
      "Training loss for batch 2718 : 0.17444488406181335\n",
      "Training loss for batch 2719 : 0.035037554800510406\n",
      "Training loss for batch 2720 : 0.0009400972630828619\n",
      "Training loss for batch 2721 : 0.0188736654818058\n",
      "Training loss for batch 2722 : 0.05630042031407356\n",
      "Training loss for batch 2723 : 0.1992858499288559\n",
      "Training loss for batch 2724 : 0.12654075026512146\n",
      "Training loss for batch 2725 : 0.23735575377941132\n",
      "Training loss for batch 2726 : 0.347061425447464\n",
      "Training loss for batch 2727 : 0.06106012314558029\n",
      "Training loss for batch 2728 : 0.10991787165403366\n",
      "Training loss for batch 2729 : 0.02092622220516205\n",
      "Training loss for batch 2730 : 0.501335620880127\n",
      "Training loss for batch 2731 : 0.08548790216445923\n",
      "Training loss for batch 2732 : 0.1444597840309143\n",
      "Training loss for batch 2733 : 0.42460697889328003\n",
      "Training loss for batch 2734 : 0.3774110674858093\n",
      "Training loss for batch 2735 : 0.14651742577552795\n",
      "Training loss for batch 2736 : 0.012248008511960506\n",
      "Training loss for batch 2737 : 0.29869818687438965\n",
      "Training loss for batch 2738 : 0.10415638983249664\n",
      "Training loss for batch 2739 : 0.24564290046691895\n",
      "Training loss for batch 2740 : 0.13052639365196228\n",
      "Training loss for batch 2741 : 0.0025565228424966335\n",
      "Training loss for batch 2742 : 0.041551217436790466\n",
      "Training loss for batch 2743 : 0.09776699542999268\n",
      "Training loss for batch 2744 : 0.17297914624214172\n",
      "Training loss for batch 2745 : 0.0028480689506977797\n",
      "Training loss for batch 2746 : 0.13193826377391815\n",
      "Training loss for batch 2747 : 0.31982743740081787\n",
      "Training loss for batch 2748 : 0.051815565675497055\n",
      "Training loss for batch 2749 : 0.32179543375968933\n",
      "Training loss for batch 2750 : 0.05641273409128189\n",
      "Training loss for batch 2751 : 0.21120348572731018\n",
      "Training loss for batch 2752 : 0.02948896214365959\n",
      "Training loss for batch 2753 : 0.11352777481079102\n",
      "Training loss for batch 2754 : 0.15221871435642242\n",
      "Training loss for batch 2755 : 0.17241936922073364\n",
      "Training loss for batch 2756 : 0.16703352332115173\n",
      "Training loss for batch 2757 : 0.2077917754650116\n",
      "Training loss for batch 2758 : 0.08095771074295044\n",
      "Training loss for batch 2759 : 0.016776690259575844\n",
      "Training loss for batch 2760 : 0.04211128503084183\n",
      "Training loss for batch 2761 : 0.21451088786125183\n",
      "Training loss for batch 2762 : 0.11598531901836395\n",
      "Training loss for batch 2763 : 0.27375659346580505\n",
      "Training loss for batch 2764 : 0.18881532549858093\n",
      "Training loss for batch 2765 : 0.1801270693540573\n",
      "Training loss for batch 2766 : 0.150838240981102\n",
      "Training loss for batch 2767 : 0.07116889953613281\n",
      "Training loss for batch 2768 : 0.0\n",
      "Training loss for batch 2769 : 0.08424459397792816\n",
      "Training loss for batch 2770 : 0.30060115456581116\n",
      "Training loss for batch 2771 : 0.5086620450019836\n",
      "Training loss for batch 2772 : 0.08675327897071838\n",
      "Training loss for batch 2773 : 0.24214570224285126\n",
      "Training loss for batch 2774 : 0.01478743739426136\n",
      "Training loss for batch 2775 : 0.17675980925559998\n",
      "Training loss for batch 2776 : 0.0\n",
      "Training loss for batch 2777 : 0.4354459345340729\n",
      "Training loss for batch 2778 : 0.12863051891326904\n",
      "Training loss for batch 2779 : 0.0521414689719677\n",
      "Training loss for batch 2780 : 0.06628837436437607\n",
      "Training loss for batch 2781 : 0.2515406906604767\n",
      "Training loss for batch 2782 : 0.0\n",
      "Training loss for batch 2783 : 0.2135220170021057\n",
      "Training loss for batch 2784 : 0.2546481192111969\n",
      "Training loss for batch 2785 : 0.02812185510993004\n",
      "Training loss for batch 2786 : 0.1927221268415451\n",
      "Training loss for batch 2787 : 0.07990085333585739\n",
      "Training loss for batch 2788 : 0.2441057413816452\n",
      "Training loss for batch 2789 : 0.07421755790710449\n",
      "Training loss for batch 2790 : 0.4889034628868103\n",
      "Training loss for batch 2791 : 0.07836978882551193\n",
      "Training loss for batch 2792 : 0.1700623780488968\n",
      "Training loss for batch 2793 : 0.3289070129394531\n",
      "Training loss for batch 2794 : 0.030146300792694092\n",
      "Training loss for batch 2795 : 0.14272193610668182\n",
      "Training loss for batch 2796 : 0.1730189174413681\n",
      "Training loss for batch 2797 : 0.12667617201805115\n",
      "Training loss for batch 2798 : 0.10099361091852188\n",
      "Training loss for batch 2799 : 0.006143887992948294\n",
      "Training loss for batch 2800 : 0.08276142925024033\n",
      "Training loss for batch 2801 : 0.06134516000747681\n",
      "Training loss for batch 2802 : 0.13218779861927032\n",
      "Training loss for batch 2803 : 0.1355043202638626\n",
      "Training loss for batch 2804 : 0.2689772844314575\n",
      "Training loss for batch 2805 : 0.08590005338191986\n",
      "Training loss for batch 2806 : 0.08549942821264267\n",
      "Training loss for batch 2807 : 0.2568590044975281\n",
      "Training loss for batch 2808 : 0.20865373313426971\n",
      "Training loss for batch 2809 : 0.0\n",
      "Training loss for batch 2810 : 0.09727291762828827\n",
      "Training loss for batch 2811 : 0.37751495838165283\n",
      "Training loss for batch 2812 : 0.11625082790851593\n",
      "Training loss for batch 2813 : 0.3848147392272949\n",
      "Training loss for batch 2814 : 0.03305388242006302\n",
      "Training loss for batch 2815 : 0.20150141417980194\n",
      "Training loss for batch 2816 : 0.0\n",
      "Training loss for batch 2817 : 0.11900850385427475\n",
      "Training loss for batch 2818 : 0.09048091620206833\n",
      "Training loss for batch 2819 : 0.007661737967282534\n",
      "Training loss for batch 2820 : 0.23121577501296997\n",
      "Training loss for batch 2821 : 0.4311639070510864\n",
      "Training loss for batch 2822 : 0.019723178818821907\n",
      "Training loss for batch 2823 : 0.10180701315402985\n",
      "Training loss for batch 2824 : 0.46549537777900696\n",
      "Training loss for batch 2825 : 0.38583362102508545\n",
      "Training loss for batch 2826 : 0.01703646406531334\n",
      "Training loss for batch 2827 : 0.022916611284017563\n",
      "Training loss for batch 2828 : 0.04643925279378891\n",
      "Training loss for batch 2829 : 0.17595340311527252\n",
      "Training loss for batch 2830 : 0.2254195362329483\n",
      "Training loss for batch 2831 : 0.270890474319458\n",
      "Training loss for batch 2832 : 0.2956708073616028\n",
      "Training loss for batch 2833 : 0.09202822297811508\n",
      "Training loss for batch 2834 : 0.016862737014889717\n",
      "Training loss for batch 2835 : 0.3483189344406128\n",
      "Training loss for batch 2836 : 0.1279301941394806\n",
      "Training loss for batch 2837 : 0.0\n",
      "Training loss for batch 2838 : 0.41321778297424316\n",
      "Training loss for batch 2839 : 0.6262040734291077\n",
      "Training loss for batch 2840 : 0.3623960316181183\n",
      "Training loss for batch 2841 : 0.08598803728818893\n",
      "Training loss for batch 2842 : 0.11663582175970078\n",
      "Training loss for batch 2843 : 0.0379071868956089\n",
      "Training loss for batch 2844 : 0.27735233306884766\n",
      "Training loss for batch 2845 : 0.3535364270210266\n",
      "Training loss for batch 2846 : 0.5575463771820068\n",
      "Training loss for batch 2847 : 0.3159697949886322\n",
      "Training loss for batch 2848 : 0.8318325877189636\n",
      "Training loss for batch 2849 : 0.29523900151252747\n",
      "Training loss for batch 2850 : 0.0585748553276062\n",
      "Training loss for batch 2851 : 0.3243359923362732\n",
      "Training loss for batch 2852 : 0.17699897289276123\n",
      "Training loss for batch 2853 : 0.34196552634239197\n",
      "Training loss for batch 2854 : 0.2660007178783417\n",
      "Training loss for batch 2855 : 5.6584030971862376e-05\n",
      "Training loss for batch 2856 : 0.3117867112159729\n",
      "Training loss for batch 2857 : 0.13976474106311798\n",
      "Training loss for batch 2858 : 0.1016005277633667\n",
      "Training loss for batch 2859 : 0.069257952272892\n",
      "Training loss for batch 2860 : 0.29921671748161316\n",
      "Training loss for batch 2861 : 0.32413989305496216\n",
      "Training loss for batch 2862 : 0.26365774869918823\n",
      "Training loss for batch 2863 : 0.1120244562625885\n",
      "Training loss for batch 2864 : 0.0340762585401535\n",
      "Training loss for batch 2865 : 0.15857920050621033\n",
      "Training loss for batch 2866 : 0.07390406727790833\n",
      "Training loss for batch 2867 : 0.15128225088119507\n",
      "Training loss for batch 2868 : 0.005970488302409649\n",
      "Training loss for batch 2869 : 0.0203075148165226\n",
      "Training loss for batch 2870 : 0.12649661302566528\n",
      "Training loss for batch 2871 : 0.08196302503347397\n",
      "Training loss for batch 2872 : 0.13785801827907562\n",
      "Training loss for batch 2873 : 0.2511129081249237\n",
      "Training loss for batch 2874 : 0.030313294380903244\n",
      "Training loss for batch 2875 : 0.06294991075992584\n",
      "Training loss for batch 2876 : 0.17658032476902008\n",
      "Training loss for batch 2877 : 0.06220011040568352\n",
      "Training loss for batch 2878 : 0.13684867322444916\n",
      "Training loss for batch 2879 : 0.3119116723537445\n",
      "Training loss for batch 2880 : 0.30855947732925415\n",
      "Training loss for batch 2881 : 0.33997541666030884\n",
      "Training loss for batch 2882 : 0.20758269727230072\n",
      "Training loss for batch 2883 : 0.05206610634922981\n",
      "Training loss for batch 2884 : 0.23779837787151337\n",
      "Training loss for batch 2885 : 0.15210692584514618\n",
      "Training loss for batch 2886 : 0.12783898413181305\n",
      "Training loss for batch 2887 : 0.1733022779226303\n",
      "Training loss for batch 2888 : 0.00592048978433013\n",
      "Training loss for batch 2889 : 0.0149638457223773\n",
      "Training loss for batch 2890 : 0.07201609015464783\n",
      "Training loss for batch 2891 : 0.20632442831993103\n",
      "Training loss for batch 2892 : 0.3007003962993622\n",
      "Training loss for batch 2893 : 0.36300957202911377\n",
      "Training loss for batch 2894 : 0.28045234084129333\n",
      "Training loss for batch 2895 : 0.16989769041538239\n",
      "Training loss for batch 2896 : 0.01272695604711771\n",
      "Training loss for batch 2897 : 0.19240574538707733\n",
      "Training loss for batch 2898 : 0.06594710052013397\n",
      "Training loss for batch 2899 : 0.33643415570259094\n",
      "Training loss for batch 2900 : 0.4310942590236664\n",
      "Training loss for batch 2901 : 0.14516706764698029\n",
      "Training loss for batch 2902 : 0.42861348390579224\n",
      "Training loss for batch 2903 : 0.19722045958042145\n",
      "Training loss for batch 2904 : 0.15764647722244263\n",
      "Training loss for batch 2905 : 0.13707567751407623\n",
      "Training loss for batch 2906 : 0.2287123203277588\n",
      "Training loss for batch 2907 : 0.09638684242963791\n",
      "Training loss for batch 2908 : 0.3738234043121338\n",
      "Training loss for batch 2909 : 0.0364079587161541\n",
      "Training loss for batch 2910 : 0.0772625058889389\n",
      "Training loss for batch 2911 : 0.33194753527641296\n",
      "Training loss for batch 2912 : 0.1610535979270935\n",
      "Training loss for batch 2913 : 0.06367762386798859\n",
      "Training loss for batch 2914 : 0.09408532083034515\n",
      "Training loss for batch 2915 : 0.0\n",
      "Training loss for batch 2916 : 0.4131462275981903\n",
      "Training loss for batch 2917 : 0.14763160049915314\n",
      "Training loss for batch 2918 : 0.23550213873386383\n",
      "Training loss for batch 2919 : 0.08791828155517578\n",
      "Training loss for batch 2920 : 0.501926600933075\n",
      "Training loss for batch 2921 : 0.027551760897040367\n",
      "Training loss for batch 2922 : 0.0875667929649353\n",
      "Training loss for batch 2923 : 0.05057331174612045\n",
      "Training loss for batch 2924 : 0.1071709394454956\n",
      "Training loss for batch 2925 : 0.255649209022522\n",
      "Training loss for batch 2926 : 0.3825242519378662\n",
      "Training loss for batch 2927 : 0.07497904449701309\n",
      "Training loss for batch 2928 : 0.25877639651298523\n",
      "Training loss for batch 2929 : 0.3788854777812958\n",
      "Training loss for batch 2930 : 0.11187588423490524\n",
      "Training loss for batch 2931 : 0.8912159204483032\n",
      "Training loss for batch 2932 : 0.16538476943969727\n",
      "Training loss for batch 2933 : 0.236519917845726\n",
      "Training loss for batch 2934 : 0.12098957598209381\n",
      "Training loss for batch 2935 : 0.39568161964416504\n",
      "Training loss for batch 2936 : 0.02432505413889885\n",
      "Training loss for batch 2937 : 0.27999016642570496\n",
      "Training loss for batch 2938 : 0.16499656438827515\n",
      "Training loss for batch 2939 : 0.031738944351673126\n",
      "Training loss for batch 2940 : 0.11161580681800842\n",
      "Training loss for batch 2941 : 0.2192651331424713\n",
      "Training loss for batch 2942 : 0.07959634065628052\n",
      "Training loss for batch 2943 : 0.1892571598291397\n",
      "Training loss for batch 2944 : 0.33847472071647644\n",
      "Training loss for batch 2945 : 0.13111677765846252\n",
      "Training loss for batch 2946 : 0.3778051435947418\n",
      "Training loss for batch 2947 : 0.0855131596326828\n",
      "Training loss for batch 2948 : 0.5848157405853271\n",
      "Training loss for batch 2949 : 0.4275950789451599\n",
      "Training loss for batch 2950 : 0.19921590387821198\n",
      "Training loss for batch 2951 : 0.6228212714195251\n",
      "Training loss for batch 2952 : 0.06034098193049431\n",
      "Training loss for batch 2953 : 0.16617238521575928\n",
      "Training loss for batch 2954 : 0.00013048226537648588\n",
      "Training loss for batch 2955 : 0.2003161907196045\n",
      "Training loss for batch 2956 : 0.009695682674646378\n",
      "Training loss for batch 2957 : 0.10469940304756165\n",
      "Training loss for batch 2958 : 0.11517561972141266\n",
      "Training loss for batch 2959 : 0.3118903338909149\n",
      "Training loss for batch 2960 : 0.12855923175811768\n",
      "Training loss for batch 2961 : 0.048940811306238174\n",
      "Training loss for batch 2962 : 0.07914254069328308\n",
      "Training loss for batch 2963 : 0.4117293059825897\n",
      "Training loss for batch 2964 : 0.46113884449005127\n",
      "Training loss for batch 2965 : 0.04442301765084267\n",
      "Training loss for batch 2966 : 0.27019816637039185\n",
      "Training loss for batch 2967 : 0.15001210570335388\n",
      "Training loss for batch 2968 : 0.316150426864624\n",
      "Training loss for batch 2969 : 0.1583370715379715\n",
      "Training loss for batch 2970 : 0.0976850762963295\n",
      "Training loss for batch 2971 : 0.11168354004621506\n",
      "Training loss for batch 2972 : 0.27289849519729614\n",
      "Training loss for batch 2973 : 0.24433787167072296\n",
      "Training loss for batch 2974 : 0.2857864797115326\n",
      "Training loss for batch 2975 : 0.2068175971508026\n",
      "Training loss for batch 2976 : 0.15431901812553406\n",
      "Training loss for batch 2977 : 0.11000017821788788\n",
      "Training loss for batch 2978 : 0.06340153515338898\n",
      "Training loss for batch 2979 : 0.32586869597435\n",
      "Training loss for batch 2980 : 0.1034749299287796\n",
      "Training loss for batch 2981 : 0.11704720556735992\n",
      "Training loss for batch 2982 : 0.43249616026878357\n",
      "Training loss for batch 2983 : 0.24541592597961426\n",
      "Training loss for batch 2984 : 0.04697322100400925\n",
      "Training loss for batch 2985 : 0.21699996292591095\n",
      "Training loss for batch 2986 : 0.3534882366657257\n",
      "Training loss for batch 2987 : 0.33305105566978455\n",
      "Training loss for batch 2988 : 0.15421703457832336\n",
      "Training loss for batch 2989 : 0.1855190247297287\n",
      "Training loss for batch 2990 : 0.13748759031295776\n",
      "Training loss for batch 2991 : 0.031175272539258003\n",
      "Training loss for batch 2992 : 0.3698558807373047\n",
      "Training loss for batch 2993 : 0.16090606153011322\n",
      "Training loss for batch 2994 : 0.11935499310493469\n",
      "Training loss for batch 2995 : 0.05673203244805336\n",
      "Training loss for batch 2996 : 0.1491502821445465\n",
      "Training loss for batch 2997 : 0.17238187789916992\n",
      "Training loss for batch 2998 : 0.30620846152305603\n",
      "Training loss for batch 2999 : 0.13009563088417053\n",
      "Training loss for batch 3000 : 0.028614550828933716\n",
      "Training loss for batch 3001 : 0.00048061212874017656\n",
      "Training loss for batch 3002 : 0.0009339765529148281\n",
      "Training loss for batch 3003 : 0.29373764991760254\n",
      "Training loss for batch 3004 : 0.1822415292263031\n",
      "Training loss for batch 3005 : 0.3805108070373535\n",
      "Training loss for batch 3006 : 0.1965523064136505\n",
      "Training loss for batch 3007 : 0.03814208507537842\n",
      "Training loss for batch 3008 : 0.3423996567726135\n",
      "Training loss for batch 3009 : 0.24385584890842438\n",
      "Training loss for batch 3010 : 0.3245042860507965\n",
      "Training loss for batch 3011 : 0.07696092873811722\n",
      "Training loss for batch 3012 : 0.0879446417093277\n",
      "Training loss for batch 3013 : 0.21248118579387665\n",
      "Training loss for batch 3014 : 0.34581953287124634\n",
      "Training loss for batch 3015 : 0.2179974764585495\n",
      "Training loss for batch 3016 : 0.3265102505683899\n",
      "Training loss for batch 3017 : 0.16743996739387512\n",
      "Training loss for batch 3018 : 0.22264419496059418\n",
      "Training loss for batch 3019 : 0.01636708900332451\n",
      "Training loss for batch 3020 : 0.2802623212337494\n",
      "Training loss for batch 3021 : 0.33256009221076965\n",
      "Training loss for batch 3022 : 0.26969122886657715\n",
      "Training loss for batch 3023 : 0.01696462742984295\n",
      "Training loss for batch 3024 : 0.029952218756079674\n",
      "Training loss for batch 3025 : 0.07797573506832123\n",
      "Training loss for batch 3026 : 0.17957693338394165\n",
      "Training loss for batch 3027 : 0.11131420731544495\n",
      "Training loss for batch 3028 : 0.2799600660800934\n",
      "Training loss for batch 3029 : 0.0828118845820427\n",
      "Training loss for batch 3030 : 0.25370848178863525\n",
      "Training loss for batch 3031 : 0.07932902872562408\n",
      "Training loss for batch 3032 : 0.07956158369779587\n",
      "Training loss for batch 3033 : 0.21545425057411194\n",
      "Training loss for batch 3034 : 0.1499328464269638\n",
      "Training loss for batch 3035 : 0.04829930141568184\n",
      "Training loss for batch 3036 : 0.021291662007570267\n",
      "Training loss for batch 3037 : 0.4527203440666199\n",
      "Training loss for batch 3038 : 0.40060827136039734\n",
      "Training loss for batch 3039 : 0.7818227410316467\n",
      "Training loss for batch 3040 : 0.188545361161232\n",
      "Training loss for batch 3041 : 0.07242221385240555\n",
      "Training loss for batch 3042 : 0.1014757826924324\n",
      "Training loss for batch 3043 : 0.27642399072647095\n",
      "Training loss for batch 3044 : 0.03236806020140648\n",
      "Training loss for batch 3045 : 0.21288639307022095\n",
      "Training loss for batch 3046 : 0.15660397708415985\n",
      "Training loss for batch 3047 : 0.20493125915527344\n",
      "Training loss for batch 3048 : 0.3433610200881958\n",
      "Training loss for batch 3049 : 0.46424126625061035\n",
      "Training loss for batch 3050 : 0.03694688528776169\n",
      "Training loss for batch 3051 : 0.06003183498978615\n",
      "Training loss for batch 3052 : 0.05593400076031685\n",
      "Training loss for batch 3053 : 0.052872415632009506\n",
      "Training loss for batch 3054 : 0.05555953457951546\n",
      "Training loss for batch 3055 : 0.05398457124829292\n",
      "Training loss for batch 3056 : 0.3039115369319916\n",
      "Training loss for batch 3057 : 0.031467780470848083\n",
      "Training loss for batch 3058 : 0.3245621621608734\n",
      "Training loss for batch 3059 : 0.21638520061969757\n",
      "Training loss for batch 3060 : 0.39414486289024353\n",
      "Training loss for batch 3061 : 0.00959402322769165\n",
      "Training loss for batch 3062 : 0.1861184537410736\n",
      "Training loss for batch 3063 : 0.2398596554994583\n",
      "Training loss for batch 3064 : 0.06672994047403336\n",
      "Training loss for batch 3065 : 0.23905977606773376\n",
      "Training loss for batch 3066 : 0.328916072845459\n",
      "Training loss for batch 3067 : 0.41416606307029724\n",
      "Training loss for batch 3068 : 0.13415764272212982\n",
      "Training loss for batch 3069 : 0.015967151150107384\n",
      "Training loss for batch 3070 : 0.05345752090215683\n",
      "Training loss for batch 3071 : 0.13007409870624542\n",
      "Training loss for batch 3072 : 0.20309849083423615\n",
      "Training loss for batch 3073 : 0.1391972005367279\n",
      "Training loss for batch 3074 : 0.3166467547416687\n",
      "Training loss for batch 3075 : 0.3534872233867645\n",
      "Training loss for batch 3076 : 0.009514461271464825\n",
      "Training loss for batch 3077 : 0.43953752517700195\n",
      "Training loss for batch 3078 : 0.12260846793651581\n",
      "Training loss for batch 3079 : 0.020898085087537766\n",
      "Training loss for batch 3080 : 0.018801983445882797\n",
      "Training loss for batch 3081 : 0.19578324258327484\n",
      "Training loss for batch 3082 : 0.21725307404994965\n",
      "Training loss for batch 3083 : 0.3334333598613739\n",
      "Training loss for batch 3084 : 0.1722838431596756\n",
      "Training loss for batch 3085 : 0.13919611275196075\n",
      "Training loss for batch 3086 : 0.4690728485584259\n",
      "Training loss for batch 3087 : 0.0\n",
      "Training loss for batch 3088 : 0.6235108971595764\n",
      "Training loss for batch 3089 : 0.32554784417152405\n",
      "Training loss for batch 3090 : 0.11593776941299438\n",
      "Training loss for batch 3091 : 0.17162227630615234\n",
      "Training loss for batch 3092 : 0.2277541160583496\n",
      "Training loss for batch 3093 : 0.07221151143312454\n",
      "Training loss for batch 3094 : 0.3402983844280243\n",
      "Training loss for batch 3095 : 0.8713905215263367\n",
      "Training loss for batch 3096 : 0.09843947738409042\n",
      "Training loss for batch 3097 : 0.11275278031826019\n",
      "Training loss for batch 3098 : 0.0603303536772728\n",
      "Training loss for batch 3099 : 0.28783175349235535\n",
      "Training loss for batch 3100 : 0.10738322138786316\n",
      "Training loss for batch 3101 : 0.14900648593902588\n",
      "Training loss for batch 3102 : 0.09239844977855682\n",
      "Training loss for batch 3103 : 0.2788791060447693\n",
      "Training loss for batch 3104 : 0.3417876064777374\n",
      "Training loss for batch 3105 : 0.33532097935676575\n",
      "Training loss for batch 3106 : 0.4301516115665436\n",
      "Training loss for batch 3107 : 0.2286052405834198\n",
      "Training loss for batch 3108 : 0.15512973070144653\n",
      "Training loss for batch 3109 : 0.06569888442754745\n",
      "Training loss for batch 3110 : 0.08332689851522446\n",
      "Training loss for batch 3111 : 0.06500323861837387\n",
      "Training loss for batch 3112 : 0.041803110390901566\n",
      "Training loss for batch 3113 : 0.1385955810546875\n",
      "Training loss for batch 3114 : 0.0\n",
      "Training loss for batch 3115 : 0.41253501176834106\n",
      "Training loss for batch 3116 : 0.03393206372857094\n",
      "Training loss for batch 3117 : 0.2919905185699463\n",
      "Training loss for batch 3118 : 0.08993037045001984\n",
      "Training loss for batch 3119 : 0.08628822863101959\n",
      "Training loss for batch 3120 : 0.0263854768127203\n",
      "Training loss for batch 3121 : 0.09848345816135406\n",
      "Training loss for batch 3122 : 0.10472595691680908\n",
      "Training loss for batch 3123 : 0.4542884826660156\n",
      "Training loss for batch 3124 : 0.0\n",
      "Training loss for batch 3125 : 0.5215169191360474\n",
      "Training loss for batch 3126 : 0.006126226857304573\n",
      "Training loss for batch 3127 : 0.07068195194005966\n",
      "Training loss for batch 3128 : 0.02572125941514969\n",
      "Training loss for batch 3129 : 0.20948833227157593\n",
      "Training loss for batch 3130 : 0.28880035877227783\n",
      "Training loss for batch 3131 : 0.19708850979804993\n",
      "Training loss for batch 3132 : 0.06335338205099106\n",
      "Training loss for batch 3133 : 0.11335578560829163\n",
      "Training loss for batch 3134 : 0.18005135655403137\n",
      "Training loss for batch 3135 : 0.08766548335552216\n",
      "Training loss for batch 3136 : 0.23715485632419586\n",
      "Training loss for batch 3137 : 0.20436327159404755\n",
      "Training loss for batch 3138 : 0.09511277079582214\n",
      "Training loss for batch 3139 : 0.022955486550927162\n",
      "Training loss for batch 3140 : 0.04784341901540756\n",
      "Training loss for batch 3141 : 0.3639132082462311\n",
      "Training loss for batch 3142 : 0.5382832288742065\n",
      "Training loss for batch 3143 : 0.0\n",
      "Training loss for batch 3144 : 0.1602035015821457\n",
      "Training loss for batch 3145 : 0.14101442694664001\n",
      "Training loss for batch 3146 : 0.02467559650540352\n",
      "Training loss for batch 3147 : 0.05173265561461449\n",
      "Training loss for batch 3148 : 0.03736334294080734\n",
      "Training loss for batch 3149 : 0.04173886403441429\n",
      "Training loss for batch 3150 : 0.14805997908115387\n",
      "Training loss for batch 3151 : 0.21062494814395905\n",
      "Training loss for batch 3152 : 0.28001436591148376\n",
      "Training loss for batch 3153 : 0.26285701990127563\n",
      "Training loss for batch 3154 : 0.030086982995271683\n",
      "Training loss for batch 3155 : 0.22819660604000092\n",
      "Training loss for batch 3156 : 0.25726449489593506\n",
      "Training loss for batch 3157 : 0.028998572379350662\n",
      "Training loss for batch 3158 : 0.1564178764820099\n",
      "Training loss for batch 3159 : 0.26931917667388916\n",
      "Training loss for batch 3160 : 0.08958394825458527\n",
      "Training loss for batch 3161 : 0.06284747272729874\n",
      "Training loss for batch 3162 : 0.20151589810848236\n",
      "Training loss for batch 3163 : 0.31742072105407715\n",
      "Training loss for batch 3164 : 0.6302723288536072\n",
      "Training loss for batch 3165 : 0.01351572573184967\n",
      "Training loss for batch 3166 : 0.4393373727798462\n",
      "Training loss for batch 3167 : 0.0752672329545021\n",
      "Training loss for batch 3168 : 0.0134026650339365\n",
      "Training loss for batch 3169 : 0.23574738204479218\n",
      "Training loss for batch 3170 : 0.15050141513347626\n",
      "Training loss for batch 3171 : 0.14459092915058136\n",
      "Training loss for batch 3172 : 0.3130539357662201\n",
      "Training loss for batch 3173 : 0.08960811793804169\n",
      "Training loss for batch 3174 : 0.583134114742279\n",
      "Training loss for batch 3175 : 0.5052850246429443\n",
      "Training loss for batch 3176 : 0.26858004927635193\n",
      "Training loss for batch 3177 : 0.24026651680469513\n",
      "Training loss for batch 3178 : 0.0511833131313324\n",
      "Training loss for batch 3179 : 0.04445873200893402\n",
      "Training loss for batch 3180 : 0.4782331883907318\n",
      "Training loss for batch 3181 : 0.34862279891967773\n",
      "Training loss for batch 3182 : 0.059535156935453415\n",
      "Training loss for batch 3183 : 0.13756822049617767\n",
      "Training loss for batch 3184 : 0.0314861461520195\n",
      "Training loss for batch 3185 : 0.05818622559309006\n",
      "Training loss for batch 3186 : 0.02005397155880928\n",
      "Training loss for batch 3187 : 0.02500971406698227\n",
      "Training loss for batch 3188 : 0.006286114454269409\n",
      "Training loss for batch 3189 : 0.21971838176250458\n",
      "Training loss for batch 3190 : 0.3364768624305725\n",
      "Training loss for batch 3191 : 0.0\n",
      "Training loss for batch 3192 : 0.13335905969142914\n",
      "Training loss for batch 3193 : 0.36565375328063965\n",
      "Training loss for batch 3194 : 0.23901301622390747\n",
      "Training loss for batch 3195 : 0.433115154504776\n",
      "Training loss for batch 3196 : 0.48484545946121216\n",
      "Training loss for batch 3197 : 0.29260367155075073\n",
      "Training loss for batch 3198 : 0.2889663577079773\n",
      "Training loss for batch 3199 : 0.04552057757973671\n",
      "Training loss for batch 3200 : 0.29435020685195923\n",
      "Training loss for batch 3201 : 0.17284594476222992\n",
      "Training loss for batch 3202 : 0.2850375175476074\n",
      "Training loss for batch 3203 : 0.38777104020118713\n",
      "Training loss for batch 3204 : 0.22558189928531647\n",
      "Training loss for batch 3205 : 0.10366103798151016\n",
      "Training loss for batch 3206 : 0.34428098797798157\n",
      "Training loss for batch 3207 : 0.09717366099357605\n",
      "Training loss for batch 3208 : 0.11180606484413147\n",
      "Training loss for batch 3209 : 0.027350198477506638\n",
      "Training loss for batch 3210 : 0.2753726541996002\n",
      "Training loss for batch 3211 : 0.13624335825443268\n",
      "Training loss for batch 3212 : 0.31932851672172546\n",
      "Training loss for batch 3213 : 0.12307008355855942\n",
      "Training loss for batch 3214 : 0.07355429977178574\n",
      "Training loss for batch 3215 : 0.06588201224803925\n",
      "Training loss for batch 3216 : 0.20369336009025574\n",
      "Training loss for batch 3217 : 0.06949013471603394\n",
      "Training loss for batch 3218 : 0.11530763655900955\n",
      "Training loss for batch 3219 : 0.25378653407096863\n",
      "Training loss for batch 3220 : 0.2777240574359894\n",
      "Training loss for batch 3221 : 0.18109926581382751\n",
      "Training loss for batch 3222 : 0.33356812596321106\n",
      "Training loss for batch 3223 : 0.04827919229865074\n",
      "Training loss for batch 3224 : 0.30263274908065796\n",
      "Training loss for batch 3225 : 0.3995276987552643\n",
      "Training loss for batch 3226 : 0.16816389560699463\n",
      "Training loss for batch 3227 : 0.13146832585334778\n",
      "Training loss for batch 3228 : 0.06267743557691574\n",
      "Training loss for batch 3229 : 0.35677608847618103\n",
      "Training loss for batch 3230 : 0.3232164978981018\n",
      "Training loss for batch 3231 : 0.05428897589445114\n",
      "Training loss for batch 3232 : 0.05644471198320389\n",
      "Training loss for batch 3233 : 0.18722859025001526\n",
      "Training loss for batch 3234 : 0.20887480676174164\n",
      "Training loss for batch 3235 : 0.04879587143659592\n",
      "Training loss for batch 3236 : 0.12193736433982849\n",
      "Training loss for batch 3237 : 0.004626314155757427\n",
      "Training loss for batch 3238 : 0.06993231177330017\n",
      "Training loss for batch 3239 : 0.07731686532497406\n",
      "Training loss for batch 3240 : 0.14628970623016357\n",
      "Training loss for batch 3241 : 0.08880022913217545\n",
      "Training loss for batch 3242 : 0.09454456716775894\n",
      "Training loss for batch 3243 : 0.005874752998352051\n",
      "Training loss for batch 3244 : 0.37786298990249634\n",
      "Training loss for batch 3245 : 0.0843127891421318\n",
      "Training loss for batch 3246 : 0.19806326925754547\n",
      "Training loss for batch 3247 : 0.14827539026737213\n",
      "Training loss for batch 3248 : 0.10278727859258652\n",
      "Training loss for batch 3249 : 0.3949998915195465\n",
      "Training loss for batch 3250 : 0.06419098377227783\n",
      "Training loss for batch 3251 : 0.12860161066055298\n",
      "Training loss for batch 3252 : 0.0524572916328907\n",
      "Training loss for batch 3253 : 0.07015559822320938\n",
      "Training loss for batch 3254 : 0.09490475803613663\n",
      "Training loss for batch 3255 : 0.12065045535564423\n",
      "Training loss for batch 3256 : 0.059034720063209534\n",
      "Training loss for batch 3257 : 0.24216635525226593\n",
      "Training loss for batch 3258 : 0.2586868405342102\n",
      "Training loss for batch 3259 : 0.049150675535202026\n",
      "Training loss for batch 3260 : 0.05127100646495819\n",
      "Training loss for batch 3261 : 0.03457948565483093\n",
      "Training loss for batch 3262 : 0.31473851203918457\n",
      "Training loss for batch 3263 : 0.09690447151660919\n",
      "Training loss for batch 3264 : 0.07969973981380463\n",
      "Training loss for batch 3265 : 0.5738823413848877\n",
      "Training loss for batch 3266 : 0.26665636897087097\n",
      "Training loss for batch 3267 : 0.1855977177619934\n",
      "Training loss for batch 3268 : 0.013068939559161663\n",
      "Training loss for batch 3269 : 0.05238131061196327\n",
      "Training loss for batch 3270 : 0.32049766182899475\n",
      "Training loss for batch 3271 : 0.11119696497917175\n",
      "Training loss for batch 3272 : 0.40176576375961304\n",
      "Training loss for batch 3273 : 0.49771422147750854\n",
      "Training loss for batch 3274 : 0.028184961527585983\n",
      "Training loss for batch 3275 : 0.06481966376304626\n",
      "Training loss for batch 3276 : 0.3510023355484009\n",
      "Training loss for batch 3277 : 0.21973130106925964\n",
      "Training loss for batch 3278 : 0.1623888611793518\n",
      "Training loss for batch 3279 : 0.22322505712509155\n",
      "Training loss for batch 3280 : 0.24042822420597076\n",
      "Training loss for batch 3281 : 0.31484970450401306\n",
      "Training loss for batch 3282 : 0.2894877791404724\n",
      "Training loss for batch 3283 : 0.07574732601642609\n",
      "Training loss for batch 3284 : 0.03558697924017906\n",
      "Training loss for batch 3285 : 0.45835980772972107\n",
      "Training loss for batch 3286 : 0.2021665722131729\n",
      "Training loss for batch 3287 : 0.1073254719376564\n",
      "Training loss for batch 3288 : 0.016359109431505203\n",
      "Training loss for batch 3289 : 0.15965154767036438\n",
      "Training loss for batch 3290 : 0.0115547776222229\n",
      "Training loss for batch 3291 : 0.40478891134262085\n",
      "Training loss for batch 3292 : 0.32377806305885315\n",
      "Training loss for batch 3293 : 0.4249289035797119\n",
      "Training loss for batch 3294 : 0.012773633003234863\n",
      "Training loss for batch 3295 : 0.027977708727121353\n",
      "Training loss for batch 3296 : 0.04040706530213356\n",
      "Training loss for batch 3297 : 0.11236511170864105\n",
      "Training loss for batch 3298 : 0.06816815584897995\n",
      "Training loss for batch 3299 : 0.3157464265823364\n",
      "Training loss for batch 3300 : 0.3293839693069458\n",
      "Training loss for batch 3301 : 0.15285517275333405\n",
      "Training loss for batch 3302 : 0.05287082493305206\n",
      "Training loss for batch 3303 : 0.20107616484165192\n",
      "Training loss for batch 3304 : 0.18665878474712372\n",
      "Training loss for batch 3305 : 0.10609690099954605\n",
      "Training loss for batch 3306 : 0.09991339594125748\n",
      "Training loss for batch 3307 : 0.0823005884885788\n",
      "Training loss for batch 3308 : 0.06185481324791908\n",
      "Training loss for batch 3309 : 0.13467925786972046\n",
      "Training loss for batch 3310 : 0.34854191541671753\n",
      "Training loss for batch 3311 : 0.13321034610271454\n",
      "Training loss for batch 3312 : 0.3232303261756897\n",
      "Training loss for batch 3313 : 0.3429151475429535\n",
      "Training loss for batch 3314 : 0.07066909223794937\n",
      "Training loss for batch 3315 : 0.15470045804977417\n",
      "Training loss for batch 3316 : 0.05482137203216553\n",
      "Training loss for batch 3317 : 0.5023402571678162\n",
      "Training loss for batch 3318 : 0.3012355864048004\n",
      "Training loss for batch 3319 : 0.3905847370624542\n",
      "Training loss for batch 3320 : 0.3772277235984802\n",
      "Training loss for batch 3321 : 0.29196980595588684\n",
      "Training loss for batch 3322 : 0.26535695791244507\n",
      "Training loss for batch 3323 : 0.27502384781837463\n",
      "Training loss for batch 3324 : 0.3176088035106659\n",
      "Training loss for batch 3325 : 0.05978269502520561\n",
      "Training loss for batch 3326 : 0.40262535214424133\n",
      "Training loss for batch 3327 : 0.2586396634578705\n",
      "Training loss for batch 3328 : 0.35924068093299866\n",
      "Training loss for batch 3329 : 0.310163289308548\n",
      "Training loss for batch 3330 : 0.061100710183382034\n",
      "Training loss for batch 3331 : 0.040144652128219604\n",
      "Training loss for batch 3332 : 0.2947210967540741\n",
      "Training loss for batch 3333 : 0.001979271648451686\n",
      "Training loss for batch 3334 : 0.3253861665725708\n",
      "Training loss for batch 3335 : 0.19217649102210999\n",
      "Training loss for batch 3336 : 0.1560840755701065\n",
      "Training loss for batch 3337 : 0.09155519306659698\n",
      "Training loss for batch 3338 : 0.1191975474357605\n",
      "Training loss for batch 3339 : 0.16594639420509338\n",
      "Training loss for batch 3340 : 0.060467660427093506\n",
      "Training loss for batch 3341 : 0.12644341588020325\n",
      "Training loss for batch 3342 : 0.28500881791114807\n",
      "Training loss for batch 3343 : 0.025868041440844536\n",
      "Training loss for batch 3344 : 0.10743103176355362\n",
      "Training loss for batch 3345 : 0.44865044951438904\n",
      "Training loss for batch 3346 : 0.3702920377254486\n",
      "Training loss for batch 3347 : 0.4127574563026428\n",
      "Training loss for batch 3348 : 0.2647225260734558\n",
      "Training loss for batch 3349 : 0.2959813177585602\n",
      "Training loss for batch 3350 : 0.17041411995887756\n",
      "Training loss for batch 3351 : 0.033952075988054276\n",
      "Training loss for batch 3352 : 0.40684011578559875\n",
      "Training loss for batch 3353 : 0.08835210651159286\n",
      "Training loss for batch 3354 : 0.4441736042499542\n",
      "Training loss for batch 3355 : 0.17937390506267548\n",
      "Training loss for batch 3356 : 0.18741169571876526\n",
      "Training loss for batch 3357 : 0.2318200320005417\n",
      "Training loss for batch 3358 : 0.26753658056259155\n",
      "Training loss for batch 3359 : 0.2350800633430481\n",
      "Training loss for batch 3360 : 0.11041919887065887\n",
      "Training loss for batch 3361 : 0.34831055998802185\n",
      "Training loss for batch 3362 : 0.2320057898759842\n",
      "Training loss for batch 3363 : 0.3180219531059265\n",
      "Training loss for batch 3364 : 0.11650960892438889\n",
      "Training loss for batch 3365 : 0.4627589285373688\n",
      "Training loss for batch 3366 : 0.031666915863752365\n",
      "Training loss for batch 3367 : 0.10899381339550018\n",
      "Training loss for batch 3368 : 0.0009676715126261115\n",
      "Training loss for batch 3369 : 0.12428704649209976\n",
      "Training loss for batch 3370 : 0.30225661396980286\n",
      "Training loss for batch 3371 : 0.2377542108297348\n",
      "Training loss for batch 3372 : 0.10732171684503555\n",
      "Training loss for batch 3373 : 0.0360620841383934\n",
      "Training loss for batch 3374 : 0.1319785714149475\n",
      "Training loss for batch 3375 : 0.117210254073143\n",
      "Training loss for batch 3376 : 0.3438941538333893\n",
      "Training loss for batch 3377 : 0.21870984137058258\n",
      "Training loss for batch 3378 : 0.06089486926794052\n",
      "Training loss for batch 3379 : 0.08925236016511917\n",
      "Training loss for batch 3380 : 0.19815754890441895\n",
      "Training loss for batch 3381 : 0.31805795431137085\n",
      "Training loss for batch 3382 : 0.19853092730045319\n",
      "Training loss for batch 3383 : 0.07802987098693848\n",
      "Training loss for batch 3384 : 0.16256274282932281\n",
      "Training loss for batch 3385 : 0.19404062628746033\n",
      "Training loss for batch 3386 : 0.05752517655491829\n",
      "Training loss for batch 3387 : 0.0016594963381066918\n",
      "Training loss for batch 3388 : 0.5956810712814331\n",
      "Training loss for batch 3389 : 0.22913506627082825\n",
      "Training loss for batch 3390 : 0.06111735850572586\n",
      "Training loss for batch 3391 : 0.31159812211990356\n",
      "Training loss for batch 3392 : 0.15529581904411316\n",
      "Training loss for batch 3393 : 0.22671863436698914\n",
      "Training loss for batch 3394 : 0.5064005851745605\n",
      "Training loss for batch 3395 : 0.2547946572303772\n",
      "Training loss for batch 3396 : 0.18480318784713745\n",
      "Training loss for batch 3397 : 0.1637614667415619\n",
      "Training loss for batch 3398 : 0.22862443327903748\n",
      "Training loss for batch 3399 : 0.22737431526184082\n",
      "Training loss for batch 3400 : 0.04267800226807594\n",
      "Training loss for batch 3401 : 0.41242268681526184\n",
      "Training loss for batch 3402 : 0.06861421465873718\n",
      "Training loss for batch 3403 : 0.16725359857082367\n",
      "Training loss for batch 3404 : 0.5014245510101318\n",
      "Training loss for batch 3405 : 0.05817965418100357\n",
      "Training loss for batch 3406 : 0.0780160054564476\n",
      "Training loss for batch 3407 : 0.05482953041791916\n",
      "Training loss for batch 3408 : 0.38569557666778564\n",
      "Training loss for batch 3409 : 0.15512368083000183\n",
      "Training loss for batch 3410 : 0.19264671206474304\n",
      "Training loss for batch 3411 : 0.1516580581665039\n",
      "Training loss for batch 3412 : 0.24466800689697266\n",
      "Training loss for batch 3413 : 0.7847874760627747\n",
      "Training loss for batch 3414 : 0.05143565684556961\n",
      "Training loss for batch 3415 : 0.12355583906173706\n",
      "Training loss for batch 3416 : 0.44024842977523804\n",
      "Training loss for batch 3417 : 0.2524031698703766\n",
      "Training loss for batch 3418 : 0.3405747413635254\n",
      "Training loss for batch 3419 : 0.2917080521583557\n",
      "Training loss for batch 3420 : 0.08294621855020523\n",
      "Training loss for batch 3421 : 0.04890070855617523\n",
      "Training loss for batch 3422 : 0.16995492577552795\n",
      "Training loss for batch 3423 : 0.060914795845746994\n",
      "Training loss for batch 3424 : 0.20326560735702515\n",
      "Training loss for batch 3425 : 0.09058357030153275\n",
      "Training loss for batch 3426 : 0.1278979778289795\n",
      "Training loss for batch 3427 : 0.06228339299559593\n",
      "Training loss for batch 3428 : 0.3658111095428467\n",
      "Training loss for batch 3429 : 0.317922979593277\n",
      "Training loss for batch 3430 : 0.09380699694156647\n",
      "Training loss for batch 3431 : 0.20139573514461517\n",
      "Training loss for batch 3432 : 0.08857759088277817\n",
      "Training loss for batch 3433 : 0.19139166176319122\n",
      "Training loss for batch 3434 : 0.36880651116371155\n",
      "Training loss for batch 3435 : 0.04600558057427406\n",
      "Training loss for batch 3436 : 0.08777286112308502\n",
      "Training loss for batch 3437 : 0.44884467124938965\n",
      "Training loss for batch 3438 : 0.4122278690338135\n",
      "Training loss for batch 3439 : 0.12120301276445389\n",
      "Training loss for batch 3440 : 0.26453033089637756\n",
      "Training loss for batch 3441 : 0.026297274976968765\n",
      "Training loss for batch 3442 : 0.20541134476661682\n",
      "Training loss for batch 3443 : 0.18730659782886505\n",
      "Training loss for batch 3444 : 0.02223505824804306\n",
      "Training loss for batch 3445 : 0.2830958962440491\n",
      "Training loss for batch 3446 : 0.11973478645086288\n",
      "Training loss for batch 3447 : 0.13439173996448517\n",
      "Training loss for batch 3448 : 0.018211014568805695\n",
      "Training loss for batch 3449 : 0.5720168352127075\n",
      "Training loss for batch 3450 : 0.5263902544975281\n",
      "Training loss for batch 3451 : 0.12207444757223129\n",
      "Training loss for batch 3452 : 0.0321456603705883\n",
      "Training loss for batch 3453 : 0.0\n",
      "Training loss for batch 3454 : 0.09880748391151428\n",
      "Training loss for batch 3455 : 0.059816356748342514\n",
      "Training loss for batch 3456 : 0.20878982543945312\n",
      "Training loss for batch 3457 : 0.2688188850879669\n",
      "Training loss for batch 3458 : 0.06798598915338516\n",
      "Training loss for batch 3459 : 0.14144597947597504\n",
      "Training loss for batch 3460 : 0.16543298959732056\n",
      "Training loss for batch 3461 : 0.13632754981517792\n",
      "Training loss for batch 3462 : 0.11714593321084976\n",
      "Training loss for batch 3463 : 0.17433753609657288\n",
      "Training loss for batch 3464 : 0.20612160861492157\n",
      "Training loss for batch 3465 : 0.07662872225046158\n",
      "Training loss for batch 3466 : 0.1051064059138298\n",
      "Training loss for batch 3467 : 0.1877521127462387\n",
      "Training loss for batch 3468 : 0.29086416959762573\n",
      "Training loss for batch 3469 : 0.22202853858470917\n",
      "Training loss for batch 3470 : 0.3151318430900574\n",
      "Training loss for batch 3471 : 0.28406205773353577\n",
      "Training loss for batch 3472 : 0.05181967467069626\n",
      "Training loss for batch 3473 : 0.07803378999233246\n",
      "Training loss for batch 3474 : 0.16782650351524353\n",
      "Training loss for batch 3475 : 0.5076810717582703\n",
      "Training loss for batch 3476 : 0.0958142876625061\n",
      "Training loss for batch 3477 : 0.16268184781074524\n",
      "Training loss for batch 3478 : 0.0990714430809021\n",
      "Training loss for batch 3479 : 0.038332756608724594\n",
      "Training loss for batch 3480 : 0.22628217935562134\n",
      "Training loss for batch 3481 : 0.12978892028331757\n",
      "Training loss for batch 3482 : 0.06355731934309006\n",
      "Training loss for batch 3483 : 0.24354808032512665\n",
      "Training loss for batch 3484 : 0.22977818548679352\n",
      "Training loss for batch 3485 : 0.20431651175022125\n",
      "Training loss for batch 3486 : 0.2922303080558777\n",
      "Training loss for batch 3487 : 0.007409372832626104\n",
      "Training loss for batch 3488 : 0.10749414563179016\n",
      "Training loss for batch 3489 : 0.1995103359222412\n",
      "Training loss for batch 3490 : 0.1700853705406189\n",
      "Training loss for batch 3491 : 0.2518691420555115\n",
      "Training loss for batch 3492 : 0.29882919788360596\n",
      "Training loss for batch 3493 : 0.1270797997713089\n",
      "Training loss for batch 3494 : 0.13871267437934875\n",
      "Training loss for batch 3495 : 0.024088013917207718\n",
      "Training loss for batch 3496 : 0.29042986035346985\n",
      "Training loss for batch 3497 : 0.09199853241443634\n",
      "Training loss for batch 3498 : 0.015571117401123047\n",
      "Training loss for batch 3499 : 0.1466675102710724\n",
      "Training loss for batch 3500 : 0.0007797181606292725\n",
      "Training loss for batch 3501 : 0.050180286169052124\n",
      "Training loss for batch 3502 : 0.20600810647010803\n",
      "Training loss for batch 3503 : 0.12405004352331161\n",
      "Training loss for batch 3504 : 0.1931837648153305\n",
      "Training loss for batch 3505 : 0.10282820463180542\n",
      "Training loss for batch 3506 : 0.0010684921871870756\n",
      "Training loss for batch 3507 : 0.032479360699653625\n",
      "Training loss for batch 3508 : 0.44412821531295776\n",
      "Training loss for batch 3509 : 0.1512351632118225\n",
      "Training loss for batch 3510 : 0.27501946687698364\n",
      "Training loss for batch 3511 : 0.06307247281074524\n",
      "Training loss for batch 3512 : 0.2743654251098633\n",
      "Training loss for batch 3513 : 0.31192734837532043\n",
      "Training loss for batch 3514 : 0.1634369194507599\n",
      "Training loss for batch 3515 : 0.11262793838977814\n",
      "Training loss for batch 3516 : 0.12600038945674896\n",
      "Training loss for batch 3517 : 0.6650014519691467\n",
      "Training loss for batch 3518 : 0.20580501854419708\n",
      "Training loss for batch 3519 : 0.22051560878753662\n",
      "Training loss for batch 3520 : 0.3238186538219452\n",
      "Training loss for batch 3521 : 0.14766311645507812\n",
      "Training loss for batch 3522 : 0.01811174675822258\n",
      "Training loss for batch 3523 : 0.1329912543296814\n",
      "Training loss for batch 3524 : 0.06293441355228424\n",
      "Training loss for batch 3525 : 0.046912673860788345\n",
      "Training loss for batch 3526 : 0.18580543994903564\n",
      "Training loss for batch 3527 : 0.25253331661224365\n",
      "Training loss for batch 3528 : 0.05835968255996704\n",
      "Training loss for batch 3529 : 0.11679138988256454\n",
      "Training loss for batch 3530 : 0.15900187194347382\n",
      "Training loss for batch 3531 : 0.34263062477111816\n",
      "Training loss for batch 3532 : 0.23613382875919342\n",
      "Training loss for batch 3533 : 0.11510409414768219\n",
      "Training loss for batch 3534 : 0.14894428849220276\n",
      "Training loss for batch 3535 : 0.11768113076686859\n",
      "Training loss for batch 3536 : 0.10781663656234741\n",
      "Training loss for batch 3537 : 0.1677311807870865\n",
      "Training loss for batch 3538 : 0.19172663986682892\n",
      "Training loss for batch 3539 : 0.051684293895959854\n",
      "Training loss for batch 3540 : 0.41708654165267944\n",
      "Training loss for batch 3541 : 0.07463198155164719\n",
      "Training loss for batch 3542 : 0.17088820040225983\n",
      "Training loss for batch 3543 : 0.18040549755096436\n",
      "Training loss for batch 3544 : 0.47270146012306213\n",
      "Training loss for batch 3545 : 0.10610910505056381\n",
      "Training loss for batch 3546 : 0.2671186029911041\n",
      "Training loss for batch 3547 : 0.2052939236164093\n",
      "Training loss for batch 3548 : 0.04445498064160347\n",
      "Training loss for batch 3549 : 0.720510721206665\n",
      "Training loss for batch 3550 : 0.456407755613327\n",
      "Training loss for batch 3551 : 0.10359456390142441\n",
      "Training loss for batch 3552 : 0.11459001153707504\n",
      "Training loss for batch 3553 : 0.46276623010635376\n",
      "Training loss for batch 3554 : 0.10447010397911072\n",
      "Training loss for batch 3555 : 0.5653276443481445\n",
      "Training loss for batch 3556 : 0.04691600427031517\n",
      "Training loss for batch 3557 : 0.3281305730342865\n",
      "Training loss for batch 3558 : 0.15898391604423523\n",
      "Training loss for batch 3559 : 0.20166516304016113\n",
      "Training loss for batch 3560 : 0.23478320240974426\n",
      "Training loss for batch 3561 : 0.19975796341896057\n",
      "Training loss for batch 3562 : 0.02937815524637699\n",
      "Training loss for batch 3563 : 0.33771827816963196\n",
      "Training loss for batch 3564 : 0.171009361743927\n",
      "Training loss for batch 3565 : 0.0035951954778283834\n",
      "Training loss for batch 3566 : 0.08173821121454239\n",
      "Training loss for batch 3567 : 0.20010778307914734\n",
      "Training loss for batch 3568 : 0.06571358442306519\n",
      "Training loss for batch 3569 : 0.2973899245262146\n",
      "Training loss for batch 3570 : 0.11985890567302704\n",
      "Training loss for batch 3571 : 0.46519261598587036\n",
      "Training loss for batch 3572 : 0.14954525232315063\n",
      "Training loss for batch 3573 : 0.5611739754676819\n",
      "Training loss for batch 3574 : 0.33128437399864197\n",
      "Training loss for batch 3575 : 0.12433290481567383\n",
      "Training loss for batch 3576 : 0.0\n",
      "Training loss for batch 3577 : 0.20628862082958221\n",
      "Training loss for batch 3578 : 0.293753445148468\n",
      "Training loss for batch 3579 : 0.1349334567785263\n",
      "Training loss for batch 3580 : 0.18638008832931519\n",
      "Training loss for batch 3581 : 0.21282510459423065\n",
      "Training loss for batch 3582 : 0.12237630784511566\n",
      "Training loss for batch 3583 : 0.24013569951057434\n",
      "Training loss for batch 3584 : 0.1303013563156128\n",
      "Training loss for batch 3585 : 0.01871616579592228\n",
      "Training loss for batch 3586 : 0.3302473723888397\n",
      "Training loss for batch 3587 : 0.21161842346191406\n",
      "Training loss for batch 3588 : 0.368766725063324\n",
      "Training loss for batch 3589 : 0.08798792213201523\n",
      "Training loss for batch 3590 : 0.33697500824928284\n",
      "Training loss for batch 3591 : 0.044585391879081726\n",
      "Training loss for batch 3592 : 0.0\n",
      "Training loss for batch 3593 : 0.13300096988677979\n",
      "Training loss for batch 3594 : 0.4456981122493744\n",
      "Training loss for batch 3595 : 0.20458294451236725\n",
      "Training loss for batch 3596 : 0.09862156212329865\n",
      "Training loss for batch 3597 : 0.0713830217719078\n",
      "Training loss for batch 3598 : 0.12441924214363098\n",
      "Training loss for batch 3599 : 0.2928301990032196\n",
      "Training loss for batch 3600 : 0.22473002970218658\n",
      "Training loss for batch 3601 : 0.40655335783958435\n",
      "Training loss for batch 3602 : 0.1928553581237793\n",
      "Training loss for batch 3603 : 0.30396193265914917\n",
      "Training loss for batch 3604 : 0.044630713760852814\n",
      "Training loss for batch 3605 : 0.28620395064353943\n",
      "Training loss for batch 3606 : 0.2930568754673004\n",
      "Training loss for batch 3607 : 0.2709640860557556\n",
      "Training loss for batch 3608 : 0.07623893022537231\n",
      "Training loss for batch 3609 : 0.20257355272769928\n",
      "Training loss for batch 3610 : 0.10673997551202774\n",
      "Training loss for batch 3611 : 0.13441209495067596\n",
      "Training loss for batch 3612 : 0.07835325598716736\n",
      "Training loss for batch 3613 : 0.06407836824655533\n",
      "Training loss for batch 3614 : 0.34940823912620544\n",
      "Training loss for batch 3615 : 0.024158509448170662\n",
      "Training loss for batch 3616 : 0.30307620763778687\n",
      "Training loss for batch 3617 : 0.3497319221496582\n",
      "Training loss for batch 3618 : 0.25387904047966003\n",
      "Training loss for batch 3619 : 0.33886489272117615\n",
      "Training loss for batch 3620 : 0.3378186523914337\n",
      "Training loss for batch 3621 : 0.3337988257408142\n",
      "Training loss for batch 3622 : 0.19515517354011536\n",
      "Training loss for batch 3623 : 0.00025051363627426326\n",
      "Training loss for batch 3624 : 0.1995541900396347\n",
      "Training loss for batch 3625 : 0.14440400898456573\n",
      "Training loss for batch 3626 : 0.22162029147148132\n",
      "Training loss for batch 3627 : 0.5446667671203613\n",
      "Training loss for batch 3628 : 0.156844362616539\n",
      "Training loss for batch 3629 : 0.20081204175949097\n",
      "Training loss for batch 3630 : 0.2938596308231354\n",
      "Training loss for batch 3631 : 0.07146492600440979\n",
      "Training loss for batch 3632 : 0.000756043300498277\n",
      "Training loss for batch 3633 : 0.11075863987207413\n",
      "Training loss for batch 3634 : 0.02366744540631771\n",
      "Training loss for batch 3635 : 0.013435917906463146\n",
      "Training loss for batch 3636 : 0.06606320291757584\n",
      "Training loss for batch 3637 : 0.2006610631942749\n",
      "Training loss for batch 3638 : 0.18750958144664764\n",
      "Training loss for batch 3639 : 0.0020041964016854763\n",
      "Training loss for batch 3640 : 0.02410762570798397\n",
      "Training loss for batch 3641 : 0.0977843627333641\n",
      "Training loss for batch 3642 : 0.05056080222129822\n",
      "Training loss for batch 3643 : 0.08474794030189514\n",
      "Training loss for batch 3644 : 0.19641634821891785\n",
      "Training loss for batch 3645 : 0.2465161234140396\n",
      "Training loss for batch 3646 : 0.04329301416873932\n",
      "Training loss for batch 3647 : 0.23492272198200226\n",
      "Training loss for batch 3648 : 0.07966407388448715\n",
      "Training loss for batch 3649 : 0.19978515803813934\n",
      "Training loss for batch 3650 : 0.20209211111068726\n",
      "Training loss for batch 3651 : 0.19114740192890167\n",
      "Training loss for batch 3652 : 0.3795998692512512\n",
      "Training loss for batch 3653 : 0.08934377878904343\n",
      "Training loss for batch 3654 : 0.7108874917030334\n",
      "Training loss for batch 3655 : 0.009866714477539062\n",
      "Training loss for batch 3656 : 0.03065229021012783\n",
      "Training loss for batch 3657 : 0.2510002553462982\n",
      "Training loss for batch 3658 : 0.1378479152917862\n",
      "Training loss for batch 3659 : 0.3605481684207916\n",
      "Training loss for batch 3660 : 0.2124052494764328\n",
      "Training loss for batch 3661 : 0.20687741041183472\n",
      "Training loss for batch 3662 : 0.15619927644729614\n",
      "Training loss for batch 3663 : 0.21099868416786194\n",
      "Training loss for batch 3664 : 0.02357250079512596\n",
      "Training loss for batch 3665 : 0.18093791604042053\n",
      "Training loss for batch 3666 : 0.14597012102603912\n",
      "Training loss for batch 3667 : 0.0552205964922905\n",
      "Training loss for batch 3668 : 0.07828577607870102\n",
      "Training loss for batch 3669 : 0.3501438498497009\n",
      "Training loss for batch 3670 : 0.024464385583996773\n",
      "Training loss for batch 3671 : 0.045366693288087845\n",
      "Training loss for batch 3672 : 0.4406626522541046\n",
      "Training loss for batch 3673 : 0.20641307532787323\n",
      "Training loss for batch 3674 : 0.08025341480970383\n",
      "Training loss for batch 3675 : 0.11009606719017029\n",
      "Training loss for batch 3676 : 0.26730871200561523\n",
      "Training loss for batch 3677 : 0.16226144134998322\n",
      "Training loss for batch 3678 : 0.12318702042102814\n",
      "Training loss for batch 3679 : 0.02146230638027191\n",
      "Training loss for batch 3680 : 0.012345313094556332\n",
      "Training loss for batch 3681 : 0.265460342168808\n",
      "Training loss for batch 3682 : 0.219034343957901\n",
      "Training loss for batch 3683 : 0.08923658728599548\n",
      "Training loss for batch 3684 : 0.14583346247673035\n",
      "Training loss for batch 3685 : 0.17630891501903534\n",
      "Training loss for batch 3686 : 0.17464300990104675\n",
      "Training loss for batch 3687 : 0.058133501559495926\n",
      "Training loss for batch 3688 : 0.047766610980033875\n",
      "Training loss for batch 3689 : 0.16267667710781097\n",
      "Training loss for batch 3690 : 0.24158407747745514\n",
      "Training loss for batch 3691 : 0.0992593914270401\n",
      "Training loss for batch 3692 : 0.10724177211523056\n",
      "Training loss for batch 3693 : 0.26081302762031555\n",
      "Training loss for batch 3694 : 0.1960286796092987\n",
      "Training loss for batch 3695 : 0.11347963660955429\n",
      "Training loss for batch 3696 : 0.3103581964969635\n",
      "Training loss for batch 3697 : 0.03261217474937439\n",
      "Training loss for batch 3698 : 0.23078036308288574\n",
      "Training loss for batch 3699 : 0.14971274137496948\n",
      "Training loss for batch 3700 : 0.16497933864593506\n",
      "Training loss for batch 3701 : 0.04192660003900528\n",
      "Training loss for batch 3702 : 0.41436460614204407\n",
      "Training loss for batch 3703 : 0.12395285815000534\n",
      "Training loss for batch 3704 : 0.11150578409433365\n",
      "Training loss for batch 3705 : 0.1256577968597412\n",
      "Training loss for batch 3706 : 0.1752801537513733\n",
      "Training loss for batch 3707 : 0.20658065378665924\n",
      "Training loss for batch 3708 : 0.06219060346484184\n",
      "Training loss for batch 3709 : 0.3615504205226898\n",
      "Training loss for batch 3710 : 0.27384746074676514\n",
      "Training loss for batch 3711 : 0.5323325395584106\n",
      "Training loss for batch 3712 : 0.16198189556598663\n",
      "Training loss for batch 3713 : 0.15254777669906616\n",
      "Training loss for batch 3714 : 0.0402970127761364\n",
      "Training loss for batch 3715 : 0.22518672049045563\n",
      "Training loss for batch 3716 : 0.48404157161712646\n",
      "Training loss for batch 3717 : 0.031499382108449936\n",
      "Training loss for batch 3718 : 0.0\n",
      "Training loss for batch 3719 : 0.35082152485847473\n",
      "Training loss for batch 3720 : 0.3454773724079132\n",
      "Training loss for batch 3721 : 0.03804623335599899\n",
      "Training loss for batch 3722 : 0.1322696954011917\n",
      "Training loss for batch 3723 : 0.096451535820961\n",
      "Training loss for batch 3724 : 0.1575278639793396\n",
      "Training loss for batch 3725 : 0.23709332942962646\n",
      "Training loss for batch 3726 : 0.2881144881248474\n",
      "Training loss for batch 3727 : 0.03958557918667793\n",
      "Training loss for batch 3728 : 0.16707226634025574\n",
      "Training loss for batch 3729 : 0.1429492086172104\n",
      "Training loss for batch 3730 : 0.23138676583766937\n",
      "Training loss for batch 3731 : 0.07351809740066528\n",
      "Training loss for batch 3732 : 0.47700902819633484\n",
      "Training loss for batch 3733 : 0.25114983320236206\n",
      "Training loss for batch 3734 : 0.05498768016695976\n",
      "Training loss for batch 3735 : 0.07401157915592194\n",
      "Training loss for batch 3736 : 0.09218701720237732\n",
      "Training loss for batch 3737 : 0.09873314946889877\n",
      "Training loss for batch 3738 : 0.047827109694480896\n",
      "Training loss for batch 3739 : 0.053290653973817825\n",
      "Training loss for batch 3740 : 0.22602596879005432\n",
      "Training loss for batch 3741 : 0.14609932899475098\n",
      "Training loss for batch 3742 : 0.19414393603801727\n",
      "Training loss for batch 3743 : 0.3284495770931244\n",
      "Training loss for batch 3744 : 0.027943938970565796\n",
      "Training loss for batch 3745 : 0.02626485750079155\n",
      "Training loss for batch 3746 : 0.010969310998916626\n",
      "Training loss for batch 3747 : 0.025621652603149414\n",
      "Training loss for batch 3748 : 0.15061090886592865\n",
      "Training loss for batch 3749 : 0.14629951119422913\n",
      "Training loss for batch 3750 : 0.2235792875289917\n",
      "Training loss for batch 3751 : 0.17393869161605835\n",
      "Training loss for batch 3752 : 0.0823909118771553\n",
      "Training loss for batch 3753 : 0.0\n",
      "Training loss for batch 3754 : 0.3416852355003357\n",
      "Training loss for batch 3755 : 0.16301119327545166\n",
      "Training loss for batch 3756 : 0.1705687791109085\n",
      "Training loss for batch 3757 : 0.033302031457424164\n",
      "Training loss for batch 3758 : 0.11900915950536728\n",
      "Training loss for batch 3759 : 0.15206655859947205\n",
      "Training loss for batch 3760 : 0.24463549256324768\n",
      "Training loss for batch 3761 : 0.09916315972805023\n",
      "Training loss for batch 3762 : 0.23226408660411835\n",
      "Training loss for batch 3763 : 0.20437920093536377\n",
      "Training loss for batch 3764 : 0.09457391500473022\n",
      "Training loss for batch 3765 : 0.110355444252491\n",
      "Training loss for batch 3766 : 0.2304694801568985\n",
      "Training loss for batch 3767 : 0.11755184084177017\n",
      "Training loss for batch 3768 : 0.27754178643226624\n",
      "Training loss for batch 3769 : 0.6102880239486694\n",
      "Training loss for batch 3770 : 0.28588759899139404\n",
      "Training loss for batch 3771 : 0.1284671574831009\n",
      "Training loss for batch 3772 : 0.11323414742946625\n",
      "Training loss for batch 3773 : 0.2166808694601059\n",
      "Training loss for batch 3774 : 0.18073856830596924\n",
      "Training loss for batch 3775 : 0.15087243914604187\n",
      "Training loss for batch 3776 : 0.14270320534706116\n",
      "Training loss for batch 3777 : 0.10748960822820663\n",
      "Training loss for batch 3778 : 0.20948897302150726\n",
      "Training loss for batch 3779 : 0.16577720642089844\n",
      "Training loss for batch 3780 : 0.210977241396904\n",
      "Training loss for batch 3781 : 0.5007702708244324\n",
      "Training loss for batch 3782 : 0.20276300609111786\n",
      "Training loss for batch 3783 : 0.1583070605993271\n",
      "Training loss for batch 3784 : 0.05268377810716629\n",
      "Training loss for batch 3785 : 0.044165026396512985\n",
      "Training loss for batch 3786 : 0.08907851576805115\n",
      "Training loss for batch 3787 : 0.03784700855612755\n",
      "Training loss for batch 3788 : 0.06973565369844437\n",
      "Training loss for batch 3789 : 0.15737150609493256\n",
      "Training loss for batch 3790 : 0.00480989646166563\n",
      "Training loss for batch 3791 : 0.011797871440649033\n",
      "Training loss for batch 3792 : 0.004192371387034655\n",
      "Training loss for batch 3793 : 0.49600231647491455\n",
      "Training loss for batch 3794 : 0.06258299946784973\n",
      "Training loss for batch 3795 : 0.07223566621541977\n",
      "Training loss for batch 3796 : 0.2954491078853607\n",
      "Training loss for batch 3797 : 0.47384145855903625\n",
      "Training loss for batch 3798 : 0.087041474878788\n",
      "Training loss for batch 3799 : 0.19212184846401215\n",
      "Training loss for batch 3800 : 0.023563934490084648\n",
      "Training loss for batch 3801 : 0.07758212089538574\n",
      "Training loss for batch 3802 : 0.03530541807413101\n",
      "Training loss for batch 3803 : 0.01423199474811554\n",
      "Training loss for batch 3804 : 0.1477159708738327\n",
      "Training loss for batch 3805 : 0.08206012845039368\n",
      "Training loss for batch 3806 : 0.0027731501031666994\n",
      "Training loss for batch 3807 : 0.06973756849765778\n",
      "Training loss for batch 3808 : 0.4765704572200775\n",
      "Training loss for batch 3809 : 0.19312912225723267\n",
      "Training loss for batch 3810 : 0.18073832988739014\n",
      "Training loss for batch 3811 : 0.04878625646233559\n",
      "Training loss for batch 3812 : 0.36088311672210693\n",
      "Training loss for batch 3813 : 0.11364240199327469\n",
      "Training loss for batch 3814 : 0.15502528846263885\n",
      "Training loss for batch 3815 : 0.08285942673683167\n",
      "Training loss for batch 3816 : 0.5533127784729004\n",
      "Training loss for batch 3817 : 0.24387510120868683\n",
      "Training loss for batch 3818 : 0.34271252155303955\n",
      "Training loss for batch 3819 : 0.09064671397209167\n",
      "Training loss for batch 3820 : 0.03407323732972145\n",
      "Training loss for batch 3821 : 0.22706501185894012\n",
      "Training loss for batch 3822 : 0.025670647621154785\n",
      "Training loss for batch 3823 : 0.09766525775194168\n",
      "Training loss for batch 3824 : 0.12515951693058014\n",
      "Training loss for batch 3825 : 0.5747635364532471\n",
      "Training loss for batch 3826 : 0.21769949793815613\n",
      "Training loss for batch 3827 : 0.2117292284965515\n",
      "Training loss for batch 3828 : 0.3704187273979187\n",
      "Training loss for batch 3829 : 0.17871661484241486\n",
      "Training loss for batch 3830 : 0.1465233713388443\n",
      "Training loss for batch 3831 : 0.3711705803871155\n",
      "Training loss for batch 3832 : 0.502632737159729\n",
      "Training loss for batch 3833 : 0.10923310369253159\n",
      "Training loss for batch 3834 : 0.02751290425658226\n",
      "Training loss for batch 3835 : 0.03428724408149719\n",
      "Training loss for batch 3836 : 0.2967202961444855\n",
      "Training loss for batch 3837 : 0.04370298609137535\n",
      "Training loss for batch 3838 : 0.43937647342681885\n",
      "Training loss for batch 3839 : 0.4570113718509674\n",
      "Training loss for batch 3840 : 0.514769971370697\n",
      "Training loss for batch 3841 : 0.0\n",
      "Training loss for batch 3842 : 0.2254333198070526\n",
      "Training loss for batch 3843 : 0.28465110063552856\n",
      "Training loss for batch 3844 : 0.10483444482088089\n",
      "Training loss for batch 3845 : 0.10956020653247833\n",
      "Training loss for batch 3846 : 0.38423967361450195\n",
      "Training loss for batch 3847 : 0.1875240057706833\n",
      "Training loss for batch 3848 : 0.37280935049057007\n",
      "Training loss for batch 3849 : 0.10215244442224503\n",
      "Training loss for batch 3850 : 0.05245685577392578\n",
      "Training loss for batch 3851 : 0.4272032082080841\n",
      "Training loss for batch 3852 : 0.17311407625675201\n",
      "Training loss for batch 3853 : 0.39419791102409363\n",
      "Training loss for batch 3854 : 0.3279044032096863\n",
      "Training loss for batch 3855 : 0.390162855386734\n",
      "Training loss for batch 3856 : 0.3148937523365021\n",
      "Training loss for batch 3857 : 0.28974658250808716\n",
      "Training loss for batch 3858 : 0.006877391133457422\n",
      "Training loss for batch 3859 : 0.11876916140317917\n",
      "Training loss for batch 3860 : 0.2549087703227997\n",
      "Training loss for batch 3861 : 9.03773689060472e-05\n",
      "Training loss for batch 3862 : 0.05527845770120621\n",
      "Training loss for batch 3863 : 0.13446365296840668\n",
      "Training loss for batch 3864 : 0.23245473206043243\n",
      "Training loss for batch 3865 : 0.07958406209945679\n",
      "Training loss for batch 3866 : 0.03712230548262596\n",
      "Training loss for batch 3867 : 0.11507317423820496\n",
      "Training loss for batch 3868 : 0.047144025564193726\n",
      "Training loss for batch 3869 : 0.007619229145348072\n",
      "Training loss for batch 3870 : 0.09469827264547348\n",
      "Training loss for batch 3871 : 0.03824000433087349\n",
      "Training loss for batch 3872 : 0.17891289293766022\n",
      "Training loss for batch 3873 : 0.38782402873039246\n",
      "Training loss for batch 3874 : 0.03634428232908249\n",
      "Training loss for batch 3875 : 0.004786614328622818\n",
      "Training loss for batch 3876 : 0.1268349438905716\n",
      "Training loss for batch 3877 : 0.003916964400559664\n",
      "Training loss for batch 3878 : 0.4232143461704254\n",
      "Training loss for batch 3879 : 0.33338165283203125\n",
      "Training loss for batch 3880 : 0.0007394451531581581\n",
      "Training loss for batch 3881 : 0.18468593060970306\n",
      "Training loss for batch 3882 : 0.3633248805999756\n",
      "Training loss for batch 3883 : 0.11025012284517288\n",
      "Training loss for batch 3884 : 0.01657681353390217\n",
      "Training loss for batch 3885 : 0.15160703659057617\n",
      "Training loss for batch 3886 : 0.060753270983695984\n",
      "Training loss for batch 3887 : 0.14561840891838074\n",
      "Training loss for batch 3888 : 0.5751479864120483\n",
      "Training loss for batch 3889 : 0.014743315987288952\n",
      "Training loss for batch 3890 : 0.31068217754364014\n",
      "Training loss for batch 3891 : 0.4880794286727905\n",
      "Training loss for batch 3892 : 0.09022196382284164\n",
      "Training loss for batch 3893 : 0.08886630088090897\n",
      "Training loss for batch 3894 : 0.1491333693265915\n",
      "Training loss for batch 3895 : 0.037955936044454575\n",
      "Training loss for batch 3896 : 0.07841808348894119\n",
      "Training loss for batch 3897 : 0.11352287977933884\n",
      "Training loss for batch 3898 : 0.06498495489358902\n",
      "Training loss for batch 3899 : 0.22304554283618927\n",
      "Training loss for batch 3900 : 0.11100194603204727\n",
      "Training loss for batch 3901 : 0.0\n",
      "Training loss for batch 3902 : 0.11229477822780609\n",
      "Training loss for batch 3903 : 0.35550183057785034\n",
      "Training loss for batch 3904 : 0.05707274004817009\n",
      "Training loss for batch 3905 : 0.25066423416137695\n",
      "Training loss for batch 3906 : 0.2297433614730835\n",
      "Training loss for batch 3907 : 0.1960035115480423\n",
      "Training loss for batch 3908 : 0.10401570796966553\n",
      "Training loss for batch 3909 : 0.31468120217323303\n",
      "Training loss for batch 3910 : 0.2643240690231323\n",
      "Training loss for batch 3911 : 0.04619909077882767\n",
      "Training loss for batch 3912 : 0.07947983592748642\n",
      "Training loss for batch 3913 : 0.13100183010101318\n",
      "Training loss for batch 3914 : 0.2934727370738983\n",
      "Training loss for batch 3915 : 0.16922514140605927\n",
      "Training loss for batch 3916 : 0.09249060600996017\n",
      "Training loss for batch 3917 : 0.15662598609924316\n",
      "Training loss for batch 3918 : 0.08139415830373764\n",
      "Training loss for batch 3919 : 0.15675263106822968\n",
      "Training loss for batch 3920 : 0.2959311902523041\n",
      "Training loss for batch 3921 : 0.1658715456724167\n",
      "Training loss for batch 3922 : 0.3194543123245239\n",
      "Training loss for batch 3923 : 0.27415764331817627\n",
      "Training loss for batch 3924 : 0.15443572402000427\n",
      "Training loss for batch 3925 : 0.08674575388431549\n",
      "Training loss for batch 3926 : 0.28914597630500793\n",
      "Training loss for batch 3927 : 0.05609341338276863\n",
      "Training loss for batch 3928 : 0.30614709854125977\n",
      "Training loss for batch 3929 : 0.2725095748901367\n",
      "Training loss for batch 3930 : 0.32460883259773254\n",
      "Training loss for batch 3931 : 0.1449199616909027\n",
      "Training loss for batch 3932 : 0.05331169813871384\n",
      "Training loss for batch 3933 : 0.11815643310546875\n",
      "Training loss for batch 3934 : 0.0943639799952507\n",
      "Training loss for batch 3935 : 0.24550330638885498\n",
      "Training loss for batch 3936 : 0.2671658396720886\n",
      "Training loss for batch 3937 : 0.0277653057128191\n",
      "Training loss for batch 3938 : 0.17257475852966309\n",
      "Training loss for batch 3939 : 0.20880885422229767\n",
      "Training loss for batch 3940 : 0.48344919085502625\n",
      "Training loss for batch 3941 : 0.2523963451385498\n",
      "Training loss for batch 3942 : 0.5943189859390259\n",
      "Training loss for batch 3943 : 0.29629430174827576\n",
      "Training loss for batch 3944 : 0.18206261098384857\n",
      "Training loss for batch 3945 : 0.33542516827583313\n",
      "Training loss for batch 3946 : 0.02878968045115471\n",
      "Training loss for batch 3947 : 0.03903486579656601\n",
      "Training loss for batch 3948 : 0.11145350337028503\n",
      "Training loss for batch 3949 : 0.1697344034910202\n",
      "Training loss for batch 3950 : 0.2862452268600464\n",
      "Training loss for batch 3951 : 0.009738471359014511\n",
      "Training loss for batch 3952 : 0.3364467918872833\n",
      "Training loss for batch 3953 : 0.31302523612976074\n",
      "Training loss for batch 3954 : 0.6995117664337158\n",
      "Training loss for batch 3955 : 0.444871187210083\n",
      "Training loss for batch 3956 : 0.07450789958238602\n",
      "Training loss for batch 3957 : 0.03450924530625343\n",
      "Training loss for batch 3958 : 0.10481428354978561\n",
      "Training loss for batch 3959 : 0.09606494009494781\n",
      "Training loss for batch 3960 : 0.018406076356768608\n",
      "Training loss for batch 3961 : 0.4519856870174408\n",
      "Training loss for batch 3962 : 0.3890925645828247\n",
      "Training loss for batch 3963 : 0.14214229583740234\n",
      "Training loss for batch 3964 : 0.09335275739431381\n",
      "Training loss for batch 3965 : 0.36016201972961426\n",
      "Training loss for batch 3966 : 0.14956943690776825\n",
      "Training loss for batch 3967 : 0.21892739832401276\n",
      "Training loss for batch 3968 : 0.266169011592865\n",
      "Training loss for batch 3969 : 0.055990610271692276\n",
      "Training loss for batch 3970 : 0.24092033505439758\n",
      "Training loss for batch 3971 : 0.10201661288738251\n",
      "Training loss for batch 3972 : 0.11300183832645416\n",
      "Training loss for batch 3973 : 0.020745348185300827\n",
      "Training loss for batch 3974 : 0.002642124891281128\n",
      "Training loss for batch 3975 : 0.20461691915988922\n",
      "Training loss for batch 3976 : 0.17738604545593262\n",
      "Training loss for batch 3977 : 0.046841900795698166\n",
      "Training loss for batch 3978 : 0.20510269701480865\n",
      "Training loss for batch 3979 : 0.0663958489894867\n",
      "Training loss for batch 3980 : 0.16401350498199463\n",
      "Training loss for batch 3981 : 0.3490510880947113\n",
      "Training loss for batch 3982 : 0.09700940549373627\n",
      "Training loss for batch 3983 : 0.13907775282859802\n",
      "Training loss for batch 3984 : 0.32273972034454346\n",
      "Training loss for batch 3985 : 0.5139393210411072\n",
      "Training loss for batch 3986 : 0.08760879933834076\n",
      "Training loss for batch 3987 : 0.32533150911331177\n",
      "Training loss for batch 3988 : 0.5043147206306458\n",
      "Training loss for batch 3989 : 0.056722767651081085\n",
      "Training loss for batch 3990 : 0.03242166340351105\n",
      "Training loss for batch 3991 : 0.19679343700408936\n",
      "Training loss for batch 3992 : 0.21174877882003784\n",
      "Training loss for batch 3993 : 0.41299688816070557\n",
      "Training loss for batch 3994 : 0.056261103600263596\n",
      "Training loss for batch 3995 : 0.04264000058174133\n",
      "Training loss for batch 3996 : 0.1303737312555313\n",
      "Training loss for batch 3997 : 0.06047053262591362\n",
      "Training loss for batch 3998 : 0.02460782416164875\n",
      "Training loss for batch 3999 : 0.13533228635787964\n",
      "Training loss for batch 4000 : 0.5474117398262024\n",
      "Training loss for batch 4001 : 0.0733635425567627\n",
      "Training loss for batch 4002 : 0.2441115528345108\n",
      "Training loss for batch 4003 : 0.14867427945137024\n",
      "Training loss for batch 4004 : 0.20027890801429749\n",
      "Training loss for batch 4005 : 0.2634010910987854\n",
      "Training loss for batch 4006 : 0.1600876897573471\n",
      "Training loss for batch 4007 : 0.11086057871580124\n",
      "Training loss for batch 4008 : 0.2075202912092209\n",
      "Training loss for batch 4009 : 0.17790627479553223\n",
      "Training loss for batch 4010 : 0.09590230137109756\n",
      "Training loss for batch 4011 : 0.02632146142423153\n",
      "Training loss for batch 4012 : 0.019806476309895515\n",
      "Training loss for batch 4013 : 0.4524337351322174\n",
      "Training loss for batch 4014 : 0.15826360881328583\n",
      "Training loss for batch 4015 : 0.1788811832666397\n",
      "Training loss for batch 4016 : 0.30088621377944946\n",
      "Training loss for batch 4017 : 0.028601359575986862\n",
      "Training loss for batch 4018 : 0.0020615856628865004\n",
      "Training loss for batch 4019 : 0.37016692757606506\n",
      "Training loss for batch 4020 : 0.14684899151325226\n",
      "Training loss for batch 4021 : 0.1521359086036682\n",
      "Training loss for batch 4022 : 0.22279910743236542\n",
      "Training loss for batch 4023 : 0.0\n",
      "Training loss for batch 4024 : 0.47133868932724\n",
      "Training loss for batch 4025 : 0.04335536062717438\n",
      "Training loss for batch 4026 : 0.11492372304201126\n",
      "Training loss for batch 4027 : 0.48605969548225403\n",
      "Training loss for batch 4028 : 0.15239837765693665\n",
      "Training loss for batch 4029 : 0.21925762295722961\n",
      "Training loss for batch 4030 : 0.0638885647058487\n",
      "Training loss for batch 4031 : 0.003242064267396927\n",
      "Training loss for batch 4032 : 0.37558427453041077\n",
      "Training loss for batch 4033 : 0.2308984100818634\n",
      "Training loss for batch 4034 : 0.15642346441745758\n",
      "Training loss for batch 4035 : 0.02092272974550724\n",
      "Training loss for batch 4036 : 0.05692431703209877\n",
      "Training loss for batch 4037 : 0.053695354610681534\n",
      "Training loss for batch 4038 : 0.19416916370391846\n",
      "Training loss for batch 4039 : 0.13740065693855286\n",
      "Training loss for batch 4040 : 0.04591096565127373\n",
      "Training loss for batch 4041 : 0.15881939232349396\n",
      "Training loss for batch 4042 : 0.06821583956480026\n",
      "Training loss for batch 4043 : 0.11294659972190857\n",
      "Training loss for batch 4044 : 0.356148362159729\n",
      "Training loss for batch 4045 : 0.39578646421432495\n",
      "Training loss for batch 4046 : 0.07891171425580978\n",
      "Training loss for batch 4047 : 0.19444867968559265\n",
      "Training loss for batch 4048 : 0.1450796127319336\n",
      "Training loss for batch 4049 : 0.12930084764957428\n",
      "Training loss for batch 4050 : 0.37041860818862915\n",
      "Training loss for batch 4051 : 0.08876154571771622\n",
      "Training loss for batch 4052 : 0.15237444639205933\n",
      "Training loss for batch 4053 : 0.07181524485349655\n",
      "Training loss for batch 4054 : 0.07401146739721298\n",
      "Training loss for batch 4055 : 0.430265873670578\n",
      "Training loss for batch 4056 : 1.274787450711301e-06\n",
      "Training loss for batch 4057 : 0.004521111957728863\n",
      "Training loss for batch 4058 : 0.009467687457799911\n",
      "Training loss for batch 4059 : 0.1427188217639923\n",
      "Training loss for batch 4060 : 0.26830196380615234\n",
      "Training loss for batch 4061 : 0.5430586934089661\n",
      "Training loss for batch 4062 : 0.2972104549407959\n",
      "Training loss for batch 4063 : 0.34992897510528564\n",
      "Training loss for batch 4064 : 0.04878159984946251\n",
      "Training loss for batch 4065 : 0.3213699162006378\n",
      "Training loss for batch 4066 : 0.5662358999252319\n",
      "Training loss for batch 4067 : 0.3039480447769165\n",
      "Training loss for batch 4068 : 0.15004566311836243\n",
      "Training loss for batch 4069 : 0.19734089076519012\n",
      "Training loss for batch 4070 : 0.037617605179548264\n",
      "Training loss for batch 4071 : 0.16467739641666412\n",
      "Training loss for batch 4072 : 0.30949559807777405\n",
      "Training loss for batch 4073 : 0.14222902059555054\n",
      "Training loss for batch 4074 : 0.10998541861772537\n",
      "Training loss for batch 4075 : 0.1618945300579071\n",
      "Training loss for batch 4076 : 0.275370329618454\n",
      "Training loss for batch 4077 : 0.32597899436950684\n",
      "Training loss for batch 4078 : 0.15253670513629913\n",
      "Training loss for batch 4079 : 0.20770373940467834\n",
      "Training loss for batch 4080 : 0.07455277442932129\n",
      "Training loss for batch 4081 : 0.2516823410987854\n",
      "Training loss for batch 4082 : 0.15801750123500824\n",
      "Training loss for batch 4083 : 0.11026134341955185\n",
      "Training loss for batch 4084 : 0.13232453167438507\n",
      "Training loss for batch 4085 : 0.029845627024769783\n",
      "Training loss for batch 4086 : 0.4149753153324127\n",
      "Training loss for batch 4087 : 0.15144047141075134\n",
      "Training loss for batch 4088 : 0.2111045867204666\n",
      "Training loss for batch 4089 : 0.0765160620212555\n",
      "Training loss for batch 4090 : 0.27953100204467773\n",
      "Training loss for batch 4091 : 0.3523237109184265\n",
      "Training loss for batch 4092 : 0.06977716088294983\n",
      "Training loss for batch 4093 : 0.13762754201889038\n",
      "Training loss for batch 4094 : 0.20467594265937805\n",
      "Training loss for batch 4095 : 0.4986274540424347\n",
      "Training loss for batch 4096 : 0.549964189529419\n",
      "Training loss for batch 4097 : 0.3076748847961426\n",
      "Training loss for batch 4098 : 0.12406066805124283\n",
      "Training loss for batch 4099 : 0.02896590530872345\n",
      "Training loss for batch 4100 : 0.4550701081752777\n",
      "Training loss for batch 4101 : 0.3527815043926239\n",
      "Training loss for batch 4102 : 0.2697482407093048\n",
      "Training loss for batch 4103 : 0.02067621611058712\n",
      "Training loss for batch 4104 : 0.39736807346343994\n",
      "Training loss for batch 4105 : 0.5509721636772156\n",
      "Training loss for batch 4106 : 0.3160207271575928\n",
      "Training loss for batch 4107 : 0.08359843492507935\n",
      "Training loss for batch 4108 : 0.1731853038072586\n",
      "Training loss for batch 4109 : 0.36027783155441284\n",
      "Training loss for batch 4110 : 0.3490232527256012\n",
      "Training loss for batch 4111 : 0.2046438604593277\n",
      "Training loss for batch 4112 : 0.13377375900745392\n",
      "Training loss for batch 4113 : 0.013867964968085289\n",
      "Training loss for batch 4114 : 0.0\n",
      "Training loss for batch 4115 : 0.05133942514657974\n",
      "Training loss for batch 4116 : 0.08095522969961166\n",
      "Training loss for batch 4117 : 0.039334893226623535\n",
      "Training loss for batch 4118 : 0.36163362860679626\n",
      "Training loss for batch 4119 : 0.1914409101009369\n",
      "Training loss for batch 4120 : 0.1805664300918579\n",
      "Training loss for batch 4121 : 0.034128338098526\n",
      "Training loss for batch 4122 : 0.17473390698432922\n",
      "Training loss for batch 4123 : 0.05878175050020218\n",
      "Training loss for batch 4124 : 0.12042124569416046\n",
      "Training loss for batch 4125 : 0.2791077196598053\n",
      "Training loss for batch 4126 : 0.05108480155467987\n",
      "Training loss for batch 4127 : 0.046096302568912506\n",
      "Training loss for batch 4128 : 0.03912132605910301\n",
      "Training loss for batch 4129 : 0.13694727420806885\n",
      "Training loss for batch 4130 : 0.054462190717458725\n",
      "Training loss for batch 4131 : 0.06320274621248245\n",
      "Training loss for batch 4132 : 0.10139762610197067\n",
      "Training loss for batch 4133 : 0.022949371486902237\n",
      "Training loss for batch 4134 : 0.3658227026462555\n",
      "Training loss for batch 4135 : 0.25962281227111816\n",
      "Training loss for batch 4136 : 0.03732633963227272\n",
      "Training loss for batch 4137 : 0.00712206494063139\n",
      "Training loss for batch 4138 : 0.003403487615287304\n",
      "Training loss for batch 4139 : 0.009689434431493282\n",
      "Training loss for batch 4140 : 0.1150042936205864\n",
      "Training loss for batch 4141 : 0.19466277956962585\n",
      "Training loss for batch 4142 : 0.44045746326446533\n",
      "Training loss for batch 4143 : 0.23966357111930847\n",
      "Training loss for batch 4144 : 0.17921113967895508\n",
      "Training loss for batch 4145 : 0.11614152789115906\n",
      "Training loss for batch 4146 : 0.330308735370636\n",
      "Training loss for batch 4147 : 0.0527133084833622\n",
      "Training loss for batch 4148 : 0.132509246468544\n",
      "Training loss for batch 4149 : 0.006652417127043009\n",
      "Training loss for batch 4150 : 0.11811907589435577\n",
      "Training loss for batch 4151 : 0.0749104842543602\n",
      "Training loss for batch 4152 : 0.08350232243537903\n",
      "Training loss for batch 4153 : 0.15525953471660614\n",
      "Training loss for batch 4154 : 0.04374676197767258\n",
      "Training loss for batch 4155 : 0.1209181696176529\n",
      "Training loss for batch 4156 : 0.2823789119720459\n",
      "Training loss for batch 4157 : 0.0\n",
      "Training loss for batch 4158 : 0.4829840064048767\n",
      "Training loss for batch 4159 : 0.09466661512851715\n",
      "Training loss for batch 4160 : 0.3616943955421448\n",
      "Training loss for batch 4161 : 0.10222942382097244\n",
      "Training loss for batch 4162 : 0.21031206846237183\n",
      "Training loss for batch 4163 : 0.09880157560110092\n",
      "Training loss for batch 4164 : 0.3102973699569702\n",
      "Training loss for batch 4165 : 0.010389785282313824\n",
      "Training loss for batch 4166 : 0.14663748443126678\n",
      "Training loss for batch 4167 : 0.13429513573646545\n",
      "Training loss for batch 4168 : 0.15078218281269073\n",
      "Training loss for batch 4169 : 0.10106662660837173\n",
      "Training loss for batch 4170 : 0.31685665249824524\n",
      "Training loss for batch 4171 : 0.046063173562288284\n",
      "Training loss for batch 4172 : 0.04109541326761246\n",
      "Training loss for batch 4173 : 0.07357785105705261\n",
      "Training loss for batch 4174 : 0.20588429272174835\n",
      "Training loss for batch 4175 : 0.027655232697725296\n",
      "Training loss for batch 4176 : 0.11290618032217026\n",
      "Training loss for batch 4177 : 0.07606814801692963\n",
      "Training loss for batch 4178 : 0.4146355092525482\n",
      "Training loss for batch 4179 : 0.08377426117658615\n",
      "Training loss for batch 4180 : 0.19556619226932526\n",
      "Training loss for batch 4181 : 0.17414943873882294\n",
      "Training loss for batch 4182 : 0.290767103433609\n",
      "Training loss for batch 4183 : 0.22035980224609375\n",
      "Training loss for batch 4184 : 0.2491023689508438\n",
      "Training loss for batch 4185 : 0.06940846890211105\n",
      "Training loss for batch 4186 : 0.2482132911682129\n",
      "Training loss for batch 4187 : 0.05449932441115379\n",
      "Training loss for batch 4188 : 0.0028406730853021145\n",
      "Training loss for batch 4189 : 0.17500260472297668\n",
      "Training loss for batch 4190 : 0.09384161978960037\n",
      "Training loss for batch 4191 : 0.1566239446401596\n",
      "Training loss for batch 4192 : 0.3054502606391907\n",
      "Training loss for batch 4193 : 0.036831993609666824\n",
      "Training loss for batch 4194 : 0.002792755840346217\n",
      "Training loss for batch 4195 : 0.11489922553300858\n",
      "Training loss for batch 4196 : 0.3328755795955658\n",
      "Training loss for batch 4197 : 0.2452748566865921\n",
      "Training loss for batch 4198 : 0.1159929558634758\n",
      "Training loss for batch 4199 : 0.011178996413946152\n",
      "Training loss for batch 4200 : 0.07898098975419998\n",
      "Training loss for batch 4201 : 0.005292130634188652\n",
      "Training loss for batch 4202 : 0.23230598866939545\n",
      "Training loss for batch 4203 : 0.5197710394859314\n",
      "Training loss for batch 4204 : 0.18091918528079987\n",
      "Training loss for batch 4205 : 0.07310446351766586\n",
      "Training loss for batch 4206 : 0.2048044353723526\n",
      "Training loss for batch 4207 : 0.008517731912434101\n",
      "Training loss for batch 4208 : 0.20326268672943115\n",
      "Training loss for batch 4209 : 0.1204519048333168\n",
      "Training loss for batch 4210 : 0.26974326372146606\n",
      "Training loss for batch 4211 : 0.42146605253219604\n",
      "Training loss for batch 4212 : 0.09646333754062653\n",
      "Training loss for batch 4213 : 0.07462114095687866\n",
      "Training loss for batch 4214 : 0.13275866210460663\n",
      "Training loss for batch 4215 : 0.1674502044916153\n",
      "Training loss for batch 4216 : 0.14709293842315674\n",
      "Training loss for batch 4217 : 0.07763022929430008\n",
      "Training loss for batch 4218 : 0.040718819946050644\n",
      "Training loss for batch 4219 : 0.15629248321056366\n",
      "Training loss for batch 4220 : 0.5707505941390991\n",
      "Training loss for batch 4221 : 0.06801483780145645\n",
      "Training loss for batch 4222 : 0.03600196912884712\n",
      "Training loss for batch 4223 : 0.047707926481962204\n",
      "Training loss for batch 4224 : 0.26191940903663635\n",
      "Training loss for batch 4225 : 0.05478493496775627\n",
      "Training loss for batch 4226 : 0.04525938630104065\n",
      "Training loss for batch 4227 : 0.09675300866365433\n",
      "Training loss for batch 4228 : 0.16263262927532196\n",
      "Training loss for batch 4229 : 0.09370774030685425\n",
      "Training loss for batch 4230 : 0.05813160911202431\n",
      "Training loss for batch 4231 : 0.040241509675979614\n",
      "Training loss for batch 4232 : 0.15858624875545502\n",
      "Training loss for batch 4233 : 0.07806028425693512\n",
      "Training loss for batch 4234 : 0.04967567324638367\n",
      "Training loss for batch 4235 : 0.09998764097690582\n",
      "Training loss for batch 4236 : 0.1438291221857071\n",
      "Training loss for batch 4237 : 0.024087240919470787\n",
      "Training loss for batch 4238 : 0.23207250237464905\n",
      "Training loss for batch 4239 : 0.1651405245065689\n",
      "Training loss for batch 4240 : 0.14535599946975708\n",
      "Training loss for batch 4241 : 0.09807991236448288\n",
      "Training loss for batch 4242 : 0.21907465159893036\n",
      "Training loss for batch 4243 : 0.3506520390510559\n",
      "Training loss for batch 4244 : 0.025990286841988564\n",
      "Training loss for batch 4245 : 0.04971630871295929\n",
      "Training loss for batch 4246 : 0.013350347988307476\n",
      "Training loss for batch 4247 : 0.11554483324289322\n",
      "Training loss for batch 4248 : 0.017491698265075684\n",
      "Training loss for batch 4249 : 0.05246131122112274\n",
      "Training loss for batch 4250 : 0.005300472490489483\n",
      "Training loss for batch 4251 : 0.3739738464355469\n",
      "Training loss for batch 4252 : 0.40508827567100525\n",
      "Training loss for batch 4253 : 0.026862993836402893\n",
      "Training loss for batch 4254 : 0.059548553079366684\n",
      "Training loss for batch 4255 : 0.48714733123779297\n",
      "Training loss for batch 4256 : 0.548225462436676\n",
      "Training loss for batch 4257 : 0.07479177415370941\n",
      "Training loss for batch 4258 : 0.01377604529261589\n",
      "Training loss for batch 4259 : 0.25594308972358704\n",
      "Training loss for batch 4260 : 0.14186948537826538\n",
      "Training loss for batch 4261 : 0.18259194493293762\n",
      "Training loss for batch 4262 : 0.013806125149130821\n",
      "Training loss for batch 4263 : 0.4567485749721527\n",
      "Training loss for batch 4264 : 0.06832948327064514\n",
      "Training loss for batch 4265 : 0.16579633951187134\n",
      "Training loss for batch 4266 : 0.15388444066047668\n",
      "Training loss for batch 4267 : 0.0418546125292778\n",
      "Training loss for batch 4268 : 0.19085679948329926\n",
      "Training loss for batch 4269 : 0.3006003201007843\n",
      "Training loss for batch 4270 : 0.1289350539445877\n",
      "Training loss for batch 4271 : 0.23051507771015167\n",
      "Training loss for batch 4272 : 1.1375542879104614\n",
      "Training loss for batch 4273 : 0.18270233273506165\n",
      "Training loss for batch 4274 : 0.16000626981258392\n",
      "Training loss for batch 4275 : 0.013003732077777386\n",
      "Training loss for batch 4276 : 0.3767021596431732\n",
      "Training loss for batch 4277 : 0.05074989050626755\n",
      "Training loss for batch 4278 : 0.08646934479475021\n",
      "Training loss for batch 4279 : 0.05014129728078842\n",
      "Training loss for batch 4280 : 0.0487927682697773\n",
      "Training loss for batch 4281 : 0.17493267357349396\n",
      "Training loss for batch 4282 : 0.17182029783725739\n",
      "Training loss for batch 4283 : 0.3472108542919159\n",
      "Training loss for batch 4284 : 0.05642840638756752\n",
      "Training loss for batch 4285 : 0.10265326499938965\n",
      "Training loss for batch 4286 : 0.03150516003370285\n",
      "Training loss for batch 4287 : 0.01151084154844284\n",
      "Training loss for batch 4288 : 0.33283504843711853\n",
      "Training loss for batch 4289 : 0.09895268082618713\n",
      "Training loss for batch 4290 : 0.08186633139848709\n",
      "Training loss for batch 4291 : 0.16339081525802612\n",
      "Training loss for batch 4292 : 0.2030276507139206\n",
      "Training loss for batch 4293 : 0.20199275016784668\n",
      "Training loss for batch 4294 : 0.03468361869454384\n",
      "Training loss for batch 4295 : 0.5370954275131226\n",
      "Training loss for batch 4296 : 0.2622838318347931\n",
      "Training loss for batch 4297 : 0.19251036643981934\n",
      "Training loss for batch 4298 : 0.777823805809021\n",
      "Training loss for batch 4299 : 0.34762439131736755\n",
      "Training loss for batch 4300 : 0.00819703470915556\n",
      "Training loss for batch 4301 : 0.13647198677062988\n",
      "Training loss for batch 4302 : 0.0564657524228096\n",
      "Training loss for batch 4303 : 0.29048943519592285\n",
      "Training loss for batch 4304 : 0.24862150847911835\n",
      "Training loss for batch 4305 : 0.21889576315879822\n",
      "Training loss for batch 4306 : 0.1028657853603363\n",
      "Training loss for batch 4307 : 0.37939220666885376\n",
      "Training loss for batch 4308 : 0.06893029063940048\n",
      "Training loss for batch 4309 : 0.45747625827789307\n",
      "Training loss for batch 4310 : 0.15109887719154358\n",
      "Training loss for batch 4311 : 0.10479055345058441\n",
      "Training loss for batch 4312 : 0.256719708442688\n",
      "Training loss for batch 4313 : 0.040794212371110916\n",
      "Training loss for batch 4314 : 0.18409864604473114\n",
      "Training loss for batch 4315 : 0.17178095877170563\n",
      "Training loss for batch 4316 : 0.6197526454925537\n",
      "Training loss for batch 4317 : 0.3217751681804657\n",
      "Training loss for batch 4318 : 0.03403852880001068\n",
      "Training loss for batch 4319 : 0.09824524074792862\n",
      "Training loss for batch 4320 : 0.3364369571208954\n",
      "Training loss for batch 4321 : 0.15256884694099426\n",
      "Training loss for batch 4322 : 0.26557424664497375\n",
      "Training loss for batch 4323 : 0.25008803606033325\n",
      "Training loss for batch 4324 : 0.01080815028399229\n",
      "Training loss for batch 4325 : 0.09195298701524734\n",
      "Training loss for batch 4326 : 0.23769356310367584\n",
      "Training loss for batch 4327 : 0.5551978349685669\n",
      "Training loss for batch 4328 : 0.17149478197097778\n",
      "Training loss for batch 4329 : 2.476736335665919e-05\n",
      "Training loss for batch 4330 : 0.24694839119911194\n",
      "Training loss for batch 4331 : 0.12853090465068817\n",
      "Training loss for batch 4332 : 0.1932237297296524\n",
      "Training loss for batch 4333 : 0.06381142139434814\n",
      "Training loss for batch 4334 : 0.15021201968193054\n",
      "Training loss for batch 4335 : 0.30467724800109863\n",
      "Training loss for batch 4336 : 0.0467497855424881\n",
      "Training loss for batch 4337 : 0.2742309272289276\n",
      "Training loss for batch 4338 : 0.0335535928606987\n",
      "Training loss for batch 4339 : 0.14261391758918762\n",
      "Training loss for batch 4340 : 0.303562194108963\n",
      "Training loss for batch 4341 : 0.007571429014205933\n",
      "Training loss for batch 4342 : 0.2157108187675476\n",
      "Training loss for batch 4343 : 0.08205937594175339\n",
      "Training loss for batch 4344 : 0.5793759822845459\n",
      "Training loss for batch 4345 : 0.8296072483062744\n",
      "Training loss for batch 4346 : 0.5856410264968872\n",
      "Training loss for batch 4347 : 0.33821821212768555\n",
      "Training loss for batch 4348 : 0.24473780393600464\n",
      "Training loss for batch 4349 : 0.09175422042608261\n",
      "Training loss for batch 4350 : 0.12231623381376266\n",
      "Training loss for batch 4351 : 0.520999014377594\n",
      "Training loss for batch 4352 : 0.16392049193382263\n",
      "Training loss for batch 4353 : 0.13692376017570496\n",
      "Training loss for batch 4354 : 0.38848960399627686\n",
      "Training loss for batch 4355 : 0.0021413376089185476\n",
      "Training loss for batch 4356 : 0.2566773295402527\n",
      "Training loss for batch 4357 : 0.029326142743229866\n",
      "Training loss for batch 4358 : 0.22034835815429688\n",
      "Training loss for batch 4359 : 0.20017090439796448\n",
      "Training loss for batch 4360 : 0.14063899219036102\n",
      "Training loss for batch 4361 : 0.044294729828834534\n",
      "Training loss for batch 4362 : 0.1423112452030182\n",
      "Training loss for batch 4363 : 0.09048914164304733\n",
      "Training loss for batch 4364 : 0.13213448226451874\n",
      "Training loss for batch 4365 : 0.10750731080770493\n",
      "Training loss for batch 4366 : 0.1230345144867897\n",
      "Training loss for batch 4367 : 0.016839593648910522\n",
      "Training loss for batch 4368 : 0.14878776669502258\n",
      "Training loss for batch 4369 : 0.3418027460575104\n",
      "Training loss for batch 4370 : 0.21877840161323547\n",
      "Training loss for batch 4371 : 0.1587897390127182\n",
      "Training loss for batch 4372 : 0.07913598418235779\n",
      "Training loss for batch 4373 : 0.5163686871528625\n",
      "Training loss for batch 4374 : 0.09953682124614716\n",
      "Training loss for batch 4375 : 0.12124406546354294\n",
      "Training loss for batch 4376 : 0.3314402997493744\n",
      "Training loss for batch 4377 : 0.1369180679321289\n",
      "Training loss for batch 4378 : 0.25623050332069397\n",
      "Training loss for batch 4379 : 0.0820719301700592\n",
      "Training loss for batch 4380 : 0.17187801003456116\n",
      "Training loss for batch 4381 : 0.1204427108168602\n",
      "Training loss for batch 4382 : 0.16286222636699677\n",
      "Training loss for batch 4383 : 0.33377236127853394\n",
      "Training loss for batch 4384 : 0.16619393229484558\n",
      "Training loss for batch 4385 : 0.4096943438053131\n",
      "Training loss for batch 4386 : 0.08878826349973679\n",
      "Training loss for batch 4387 : 0.15494368970394135\n",
      "Training loss for batch 4388 : 0.1758943349123001\n",
      "Training loss for batch 4389 : 0.23693062365055084\n",
      "Training loss for batch 4390 : 0.2245413362979889\n",
      "Training loss for batch 4391 : 0.36468127369880676\n",
      "Training loss for batch 4392 : 0.3706547021865845\n",
      "Training loss for batch 4393 : 0.12419936060905457\n",
      "Training loss for batch 4394 : 0.23896881937980652\n",
      "Training loss for batch 4395 : 0.08383796364068985\n",
      "Training loss for batch 4396 : 0.03182263672351837\n",
      "Training loss for batch 4397 : 0.22258388996124268\n",
      "Training loss for batch 4398 : 0.22700440883636475\n",
      "Training loss for batch 4399 : 0.061043623834848404\n",
      "Training loss for batch 4400 : 0.4423288404941559\n",
      "Training loss for batch 4401 : 0.08646364510059357\n",
      "Training loss for batch 4402 : 0.2893471419811249\n",
      "Training loss for batch 4403 : 0.05793469771742821\n",
      "Training loss for batch 4404 : 0.09005226194858551\n",
      "Training loss for batch 4405 : 0.22075814008712769\n",
      "Training loss for batch 4406 : 0.21208049356937408\n",
      "Training loss for batch 4407 : 0.04685279354453087\n",
      "Training loss for batch 4408 : 0.29139184951782227\n",
      "Training loss for batch 4409 : 0.1887926608324051\n",
      "Training loss for batch 4410 : 0.15687622129917145\n",
      "Training loss for batch 4411 : 0.09240900725126266\n",
      "Training loss for batch 4412 : 0.07776568084955215\n",
      "Training loss for batch 4413 : 0.048545170575380325\n",
      "Training loss for batch 4414 : 0.2007477730512619\n",
      "Training loss for batch 4415 : 0.05821509286761284\n",
      "Training loss for batch 4416 : 0.15055547654628754\n",
      "Training loss for batch 4417 : 0.12785738706588745\n",
      "Training loss for batch 4418 : 0.04658188298344612\n",
      "Training loss for batch 4419 : 0.09564346820116043\n",
      "Training loss for batch 4420 : 0.04252077266573906\n",
      "Training loss for batch 4421 : 0.16898229718208313\n",
      "Training loss for batch 4422 : 0.26525428891181946\n",
      "Training loss for batch 4423 : 0.1970510631799698\n",
      "Training loss for batch 4424 : 0.08431783318519592\n",
      "Training loss for batch 4425 : 0.08410852402448654\n",
      "Training loss for batch 4426 : 0.022262971848249435\n",
      "Training loss for batch 4427 : 0.262451171875\n",
      "Training loss for batch 4428 : 0.15467426180839539\n",
      "Training loss for batch 4429 : 0.08313022553920746\n",
      "Training loss for batch 4430 : 0.12196268141269684\n",
      "Training loss for batch 4431 : 0.1666806936264038\n",
      "Training loss for batch 4432 : 0.4075855016708374\n",
      "Training loss for batch 4433 : 0.05325315520167351\n",
      "Training loss for batch 4434 : 0.32680046558380127\n",
      "Training loss for batch 4435 : 0.20287787914276123\n",
      "Training loss for batch 4436 : 0.006061395164579153\n",
      "Training loss for batch 4437 : 0.09372386336326599\n",
      "Training loss for batch 4438 : 0.1383715718984604\n",
      "Training loss for batch 4439 : 0.09262531995773315\n",
      "Training loss for batch 4440 : 0.05622945353388786\n",
      "Training loss for batch 4441 : 0.0529455803334713\n",
      "Training loss for batch 4442 : 0.10992420464754105\n",
      "Training loss for batch 4443 : 0.03747393563389778\n",
      "Training loss for batch 4444 : 0.0980953499674797\n",
      "Training loss for batch 4445 : 0.051786065101623535\n",
      "Training loss for batch 4446 : 0.2907560467720032\n",
      "Training loss for batch 4447 : 0.056643757969141006\n",
      "Training loss for batch 4448 : 0.341918021440506\n",
      "Training loss for batch 4449 : 0.03516268730163574\n",
      "Training loss for batch 4450 : 0.1088700219988823\n",
      "Training loss for batch 4451 : 0.09084006398916245\n",
      "Training loss for batch 4452 : 0.04366680234670639\n",
      "Training loss for batch 4453 : 0.17122924327850342\n",
      "Training loss for batch 4454 : 0.26215484738349915\n",
      "Training loss for batch 4455 : 0.38877353072166443\n",
      "Training loss for batch 4456 : 0.166316419839859\n",
      "Training loss for batch 4457 : 0.17221547663211823\n",
      "Training loss for batch 4458 : 0.14582248032093048\n",
      "Training loss for batch 4459 : 0.055853813886642456\n",
      "Training loss for batch 4460 : 0.044459421187639236\n",
      "Training loss for batch 4461 : 0.22985993325710297\n",
      "Training loss for batch 4462 : 0.10409047454595566\n",
      "Training loss for batch 4463 : 0.028065215796232224\n",
      "Training loss for batch 4464 : 0.06641817837953568\n",
      "Training loss for batch 4465 : 0.09565666317939758\n",
      "Training loss for batch 4466 : 0.0\n",
      "Training loss for batch 4467 : 0.25342488288879395\n",
      "Training loss for batch 4468 : 0.1498095989227295\n",
      "Training loss for batch 4469 : 0.06824713200330734\n",
      "Training loss for batch 4470 : 0.22110356390476227\n",
      "Training loss for batch 4471 : 0.04150726646184921\n",
      "Training loss for batch 4472 : 0.3073958158493042\n",
      "Training loss for batch 4473 : 0.4073600172996521\n",
      "Training loss for batch 4474 : 0.1013759896159172\n",
      "Training loss for batch 4475 : 0.2747418284416199\n",
      "Training loss for batch 4476 : 0.1409539431333542\n",
      "Training loss for batch 4477 : 0.039573632180690765\n",
      "Training loss for batch 4478 : 0.002593994140625\n",
      "Training loss for batch 4479 : 0.18982622027397156\n",
      "Training loss for batch 4480 : 0.14471596479415894\n",
      "Training loss for batch 4481 : 0.07145208865404129\n",
      "Training loss for batch 4482 : 0.2401217520236969\n",
      "Training loss for batch 4483 : 0.4276981055736542\n",
      "Training loss for batch 4484 : 0.0013027365785092115\n",
      "Training loss for batch 4485 : 0.14432647824287415\n",
      "Training loss for batch 4486 : 0.562231183052063\n",
      "Training loss for batch 4487 : 0.1470159888267517\n",
      "Training loss for batch 4488 : 0.2452075183391571\n",
      "Training loss for batch 4489 : 0.03291124105453491\n",
      "Training loss for batch 4490 : 0.19104529917240143\n",
      "Training loss for batch 4491 : 0.03385289013385773\n",
      "Training loss for batch 4492 : 0.22735731303691864\n",
      "Training loss for batch 4493 : 0.048106417059898376\n",
      "Training loss for batch 4494 : 0.3712916374206543\n",
      "Training loss for batch 4495 : 0.00010779521107906476\n",
      "Training loss for batch 4496 : 0.208547905087471\n",
      "Training loss for batch 4497 : 0.5105484127998352\n",
      "Training loss for batch 4498 : 0.3278200328350067\n",
      "Training loss for batch 4499 : 0.15811091661453247\n",
      "Training loss for batch 4500 : 0.027032332494854927\n",
      "Training loss for batch 4501 : 0.24090100824832916\n",
      "Training loss for batch 4502 : 0.00969933532178402\n",
      "Training loss for batch 4503 : 0.03049459680914879\n",
      "Training loss for batch 4504 : 0.5739496350288391\n",
      "Training loss for batch 4505 : 0.29222726821899414\n",
      "Training loss for batch 4506 : 0.501511812210083\n",
      "Training loss for batch 4507 : 0.2242753803730011\n",
      "Training loss for batch 4508 : 0.006898552179336548\n",
      "Training loss for batch 4509 : 0.3471149802207947\n",
      "Training loss for batch 4510 : 0.1738288551568985\n",
      "Training loss for batch 4511 : 0.08170326799154282\n",
      "Training loss for batch 4512 : 0.34803274273872375\n",
      "Training loss for batch 4513 : 0.30940473079681396\n",
      "Training loss for batch 4514 : 0.1720646172761917\n",
      "Training loss for batch 4515 : 0.1254918873310089\n",
      "Training loss for batch 4516 : 0.2079494297504425\n",
      "Training loss for batch 4517 : 0.816805362701416\n",
      "Training loss for batch 4518 : 0.12195785343647003\n",
      "Training loss for batch 4519 : 0.0006164312362670898\n",
      "Training loss for batch 4520 : 0.011605077423155308\n",
      "Training loss for batch 4521 : 0.31798022985458374\n",
      "Training loss for batch 4522 : 0.29988893866539\n",
      "Training loss for batch 4523 : 0.40418651700019836\n",
      "Training loss for batch 4524 : 0.07449524104595184\n",
      "Training loss for batch 4525 : 0.07725601643323898\n",
      "Training loss for batch 4526 : 0.16968218982219696\n",
      "Training loss for batch 4527 : 0.07121218740940094\n",
      "Training loss for batch 4528 : 0.156779482960701\n",
      "Training loss for batch 4529 : 0.05119137093424797\n",
      "Training loss for batch 4530 : 0.15548564493656158\n",
      "Training loss for batch 4531 : 0.0034615001641213894\n",
      "Training loss for batch 4532 : 0.2972151041030884\n",
      "Training loss for batch 4533 : 0.7330179810523987\n",
      "Training loss for batch 4534 : 0.12838464975357056\n",
      "Training loss for batch 4535 : 0.11759979277849197\n",
      "Training loss for batch 4536 : 0.4608164429664612\n",
      "Training loss for batch 4537 : 0.0790630504488945\n",
      "Training loss for batch 4538 : 0.19308309257030487\n",
      "Training loss for batch 4539 : 0.136631578207016\n",
      "Training loss for batch 4540 : 0.4480379819869995\n",
      "Training loss for batch 4541 : 0.06634047627449036\n",
      "Training loss for batch 4542 : 0.26445087790489197\n",
      "Training loss for batch 4543 : 0.5585439205169678\n",
      "Training loss for batch 4544 : 0.2949404716491699\n",
      "Training loss for batch 4545 : 0.03559935837984085\n",
      "Training loss for batch 4546 : 0.5348928570747375\n",
      "Training loss for batch 4547 : 0.22166882455348969\n",
      "Training loss for batch 4548 : 0.07636100798845291\n",
      "Training loss for batch 4549 : 0.07250630855560303\n",
      "Training loss for batch 4550 : 0.252993643283844\n",
      "Training loss for batch 4551 : 0.24568822979927063\n",
      "Training loss for batch 4552 : 0.07680094987154007\n",
      "Training loss for batch 4553 : 0.28065213561058044\n",
      "Training loss for batch 4554 : 0.3203214704990387\n",
      "Training loss for batch 4555 : 0.3442474901676178\n",
      "Training loss for batch 4556 : 0.2402525693178177\n",
      "Training loss for batch 4557 : 0.07670769095420837\n",
      "Training loss for batch 4558 : 0.024005375802516937\n",
      "Training loss for batch 4559 : 0.1130460649728775\n",
      "Training loss for batch 4560 : 0.21835538744926453\n",
      "Training loss for batch 4561 : 0.3555852174758911\n",
      "Training loss for batch 4562 : 0.26539674401283264\n",
      "Training loss for batch 4563 : 0.04328592121601105\n",
      "Training loss for batch 4564 : 0.1619444638490677\n",
      "Training loss for batch 4565 : 0.014229154214262962\n",
      "Training loss for batch 4566 : 0.0837632417678833\n",
      "Training loss for batch 4567 : 0.15937532484531403\n",
      "Training loss for batch 4568 : 0.028617968782782555\n",
      "Training loss for batch 4569 : 0.13501599431037903\n",
      "Training loss for batch 4570 : 0.23547391593456268\n",
      "Training loss for batch 4571 : 0.08464330434799194\n",
      "Training loss for batch 4572 : 0.2731350064277649\n",
      "Training loss for batch 4573 : 0.18100973963737488\n",
      "Training loss for batch 4574 : 0.039486076682806015\n",
      "Training loss for batch 4575 : 0.5217450261116028\n",
      "Training loss for batch 4576 : 0.5671768188476562\n",
      "Training loss for batch 4577 : 0.026484336704015732\n",
      "Training loss for batch 4578 : 0.03870798647403717\n",
      "Training loss for batch 4579 : 0.12408529222011566\n",
      "Training loss for batch 4580 : 0.0060528842732310295\n",
      "Training loss for batch 4581 : 0.3478688895702362\n",
      "Training loss for batch 4582 : 0.11917136609554291\n",
      "Training loss for batch 4583 : 0.17614266276359558\n",
      "Training loss for batch 4584 : 0.16102835536003113\n",
      "Training loss for batch 4585 : 0.070125512778759\n",
      "Training loss for batch 4586 : 0.09756938368082047\n",
      "Training loss for batch 4587 : 0.266929030418396\n",
      "Training loss for batch 4588 : 0.29380303621292114\n",
      "Training loss for batch 4589 : 0.10044240206480026\n",
      "Training loss for batch 4590 : 0.27717944979667664\n",
      "Training loss for batch 4591 : 0.13863003253936768\n",
      "Training loss for batch 4592 : 0.2763976454734802\n",
      "Training loss for batch 4593 : 0.252739816904068\n",
      "Training loss for batch 4594 : 0.3860257863998413\n",
      "Training loss for batch 4595 : 0.18687134981155396\n",
      "Training loss for batch 4596 : 0.36536234617233276\n",
      "Training loss for batch 4597 : 0.44447603821754456\n",
      "Training loss for batch 4598 : 0.18561074137687683\n",
      "Training loss for batch 4599 : 0.04223639890551567\n",
      "Training loss for batch 4600 : 0.17050431668758392\n",
      "Training loss for batch 4601 : 0.2851708233356476\n",
      "Training loss for batch 4602 : 0.05776190012693405\n",
      "Training loss for batch 4603 : 0.07090853154659271\n",
      "Training loss for batch 4604 : 0.0018163323402404785\n",
      "Training loss for batch 4605 : 0.0682053342461586\n",
      "Training loss for batch 4606 : 0.17168454825878143\n",
      "Training loss for batch 4607 : 0.11442151665687561\n",
      "Training loss for batch 4608 : 0.18957173824310303\n",
      "Training loss for batch 4609 : 0.0024718642234802246\n",
      "Training loss for batch 4610 : 0.0\n",
      "Training loss for batch 4611 : 0.0\n",
      "Training loss for batch 4612 : 0.17711655795574188\n",
      "Training loss for batch 4613 : 0.1973709911108017\n",
      "Training loss for batch 4614 : 0.316525936126709\n",
      "Training loss for batch 4615 : 0.18778587877750397\n",
      "Training loss for batch 4616 : 0.11797915399074554\n",
      "Training loss for batch 4617 : 0.05207502096891403\n",
      "Training loss for batch 4618 : 0.06674375385046005\n",
      "Training loss for batch 4619 : 0.632326602935791\n",
      "Training loss for batch 4620 : 0.1794167309999466\n",
      "Training loss for batch 4621 : 0.2857585847377777\n",
      "Training loss for batch 4622 : 0.03168594092130661\n",
      "Training loss for batch 4623 : 0.03305734321475029\n",
      "Training loss for batch 4624 : 0.29493772983551025\n",
      "Training loss for batch 4625 : 0.15476399660110474\n",
      "Training loss for batch 4626 : 0.18389831483364105\n",
      "Training loss for batch 4627 : 0.07399086654186249\n",
      "Training loss for batch 4628 : 0.4268776774406433\n",
      "Training loss for batch 4629 : 0.11520887911319733\n",
      "Training loss for batch 4630 : 0.29484349489212036\n",
      "Training loss for batch 4631 : 0.32168900966644287\n",
      "Training loss for batch 4632 : 0.06974541395902634\n",
      "Training loss for batch 4633 : 0.13016043603420258\n",
      "Training loss for batch 4634 : 0.2909751236438751\n",
      "Training loss for batch 4635 : 0.054070305079221725\n",
      "Training loss for batch 4636 : 0.08584094047546387\n",
      "Training loss for batch 4637 : 0.1889141947031021\n",
      "Training loss for batch 4638 : 0.08729687333106995\n",
      "Training loss for batch 4639 : 0.2505408823490143\n",
      "Training loss for batch 4640 : 0.14250892400741577\n",
      "Training loss for batch 4641 : 0.18465203046798706\n",
      "Training loss for batch 4642 : 0.2912706732749939\n",
      "Training loss for batch 4643 : 0.21769744157791138\n",
      "Training loss for batch 4644 : 0.26177555322647095\n",
      "Training loss for batch 4645 : 0.28036433458328247\n",
      "Training loss for batch 4646 : 0.12311428040266037\n",
      "Training loss for batch 4647 : 0.007169207092374563\n",
      "Training loss for batch 4648 : 0.20753036439418793\n",
      "Training loss for batch 4649 : 0.1142364889383316\n",
      "Training loss for batch 4650 : 0.19841597974300385\n",
      "Training loss for batch 4651 : 0.25970709323883057\n",
      "Training loss for batch 4652 : 0.9991514682769775\n",
      "Training loss for batch 4653 : 0.15536503493785858\n",
      "Training loss for batch 4654 : 0.23805402219295502\n",
      "Training loss for batch 4655 : 0.20999231934547424\n",
      "Training loss for batch 4656 : 0.05352604016661644\n",
      "Training loss for batch 4657 : 0.027469903230667114\n",
      "Training loss for batch 4658 : 0.2632457911968231\n",
      "Training loss for batch 4659 : 0.022130176424980164\n",
      "Training loss for batch 4660 : 0.08400639146566391\n",
      "Training loss for batch 4661 : 0.14184264838695526\n",
      "Training loss for batch 4662 : 0.05711571127176285\n",
      "Training loss for batch 4663 : 0.06477206945419312\n",
      "Training loss for batch 4664 : 0.37042856216430664\n",
      "Training loss for batch 4665 : 0.007425765506923199\n",
      "Training loss for batch 4666 : 0.08443127572536469\n",
      "Training loss for batch 4667 : 0.11186445504426956\n",
      "Training loss for batch 4668 : 0.24876797199249268\n",
      "Training loss for batch 4669 : 0.0722648948431015\n",
      "Training loss for batch 4670 : 0.557367742061615\n",
      "Training loss for batch 4671 : 0.13362127542495728\n",
      "Training loss for batch 4672 : 0.12289638072252274\n",
      "Training loss for batch 4673 : 0.30208128690719604\n",
      "Training loss for batch 4674 : 0.19470496475696564\n",
      "Training loss for batch 4675 : 0.05334658548235893\n",
      "Training loss for batch 4676 : 0.08275650441646576\n",
      "Training loss for batch 4677 : 0.0\n",
      "Training loss for batch 4678 : 0.15023751556873322\n",
      "Training loss for batch 4679 : 0.12142029404640198\n",
      "Training loss for batch 4680 : 0.11830374598503113\n",
      "Training loss for batch 4681 : 0.24762709438800812\n",
      "Training loss for batch 4682 : 0.014570720493793488\n",
      "Training loss for batch 4683 : 0.240884467959404\n",
      "Training loss for batch 4684 : 0.20214200019836426\n",
      "Training loss for batch 4685 : 0.048876386135816574\n",
      "Training loss for batch 4686 : 0.13402575254440308\n",
      "Training loss for batch 4687 : 0.0\n",
      "Training loss for batch 4688 : 0.02347530983388424\n",
      "Training loss for batch 4689 : 0.053374964743852615\n",
      "Training loss for batch 4690 : 0.08183056116104126\n",
      "Training loss for batch 4691 : 0.09152354300022125\n",
      "Training loss for batch 4692 : 0.3062226176261902\n",
      "Training loss for batch 4693 : 0.46600034832954407\n",
      "Training loss for batch 4694 : 0.034150175750255585\n",
      "Training loss for batch 4695 : 0.06563093513250351\n",
      "Training loss for batch 4696 : 0.04516662657260895\n",
      "Training loss for batch 4697 : 0.09399639815092087\n",
      "Training loss for batch 4698 : 0.030713891610503197\n",
      "Training loss for batch 4699 : 0.05204535275697708\n",
      "Training loss for batch 4700 : 0.17260044813156128\n",
      "Training loss for batch 4701 : 0.308943510055542\n",
      "Training loss for batch 4702 : 0.17600207030773163\n",
      "Training loss for batch 4703 : 0.08439569920301437\n",
      "Training loss for batch 4704 : 0.06073871627449989\n",
      "Training loss for batch 4705 : 0.3835701048374176\n",
      "Training loss for batch 4706 : 0.14371563494205475\n",
      "Training loss for batch 4707 : 0.25537213683128357\n",
      "Training loss for batch 4708 : 0.09660791605710983\n",
      "Training loss for batch 4709 : 0.13838253915309906\n",
      "Training loss for batch 4710 : 0.33459746837615967\n",
      "Training loss for batch 4711 : 0.05613338574767113\n",
      "Training loss for batch 4712 : 0.20583581924438477\n",
      "Training loss for batch 4713 : 0.05582185089588165\n",
      "Training loss for batch 4714 : 0.02217724733054638\n",
      "Training loss for batch 4715 : 0.0816740095615387\n",
      "Training loss for batch 4716 : 0.120258629322052\n",
      "Training loss for batch 4717 : 0.07892132550477982\n",
      "Training loss for batch 4718 : 0.056413035839796066\n",
      "Training loss for batch 4719 : 0.19803287088871002\n",
      "Training loss for batch 4720 : 0.48431599140167236\n",
      "Training loss for batch 4721 : 0.23176102340221405\n",
      "Training loss for batch 4722 : 0.02367539145052433\n",
      "Training loss for batch 4723 : 0.008064260706305504\n",
      "Training loss for batch 4724 : 0.13066847622394562\n",
      "Training loss for batch 4725 : 0.005541936960071325\n",
      "Training loss for batch 4726 : 0.047834113240242004\n",
      "Training loss for batch 4727 : 0.35659077763557434\n",
      "Training loss for batch 4728 : 0.13235947489738464\n",
      "Training loss for batch 4729 : 0.2928404211997986\n",
      "Training loss for batch 4730 : 0.5728892087936401\n",
      "Training loss for batch 4731 : 0.004179609008133411\n",
      "Training loss for batch 4732 : 0.04493233188986778\n",
      "Training loss for batch 4733 : 0.02921777032315731\n",
      "Training loss for batch 4734 : 0.3060353696346283\n",
      "Training loss for batch 4735 : 0.27357569336891174\n",
      "Training loss for batch 4736 : 0.11447502672672272\n",
      "Training loss for batch 4737 : 0.051064275205135345\n",
      "Training loss for batch 4738 : 0.3109149932861328\n",
      "Training loss for batch 4739 : 0.001499522477388382\n",
      "Training loss for batch 4740 : 0.4305480420589447\n",
      "Training loss for batch 4741 : 0.06596487760543823\n",
      "Training loss for batch 4742 : 0.4408855140209198\n",
      "Training loss for batch 4743 : 0.027282239869236946\n",
      "Training loss for batch 4744 : 0.02521395869553089\n",
      "Training loss for batch 4745 : 0.12977567315101624\n",
      "Training loss for batch 4746 : 0.059073273092508316\n",
      "Training loss for batch 4747 : 0.20637211203575134\n",
      "Training loss for batch 4748 : 0.10470059514045715\n",
      "Training loss for batch 4749 : 0.4981484115123749\n",
      "Training loss for batch 4750 : 0.14754265546798706\n",
      "Training loss for batch 4751 : 0.026229122653603554\n",
      "Training loss for batch 4752 : 0.2721672058105469\n",
      "Training loss for batch 4753 : 0.2632516920566559\n",
      "Training loss for batch 4754 : 0.275459349155426\n",
      "Training loss for batch 4755 : 0.5643071532249451\n",
      "Training loss for batch 4756 : 0.5995693206787109\n",
      "Training loss for batch 4757 : 0.01770145818591118\n",
      "Training loss for batch 4758 : 0.17678052186965942\n",
      "Training loss for batch 4759 : 0.00532543333247304\n",
      "Training loss for batch 4760 : 0.06135188788175583\n",
      "Training loss for batch 4761 : 0.2269188016653061\n",
      "Training loss for batch 4762 : 0.1152077168226242\n",
      "Training loss for batch 4763 : 0.38362619280815125\n",
      "Training loss for batch 4764 : 0.0\n",
      "Training loss for batch 4765 : 1.0288568773830775e-05\n",
      "Training loss for batch 4766 : 0.24238331615924835\n",
      "Training loss for batch 4767 : 0.17542913556098938\n",
      "Training loss for batch 4768 : 0.12626561522483826\n",
      "Training loss for batch 4769 : 0.045495402067899704\n",
      "Training loss for batch 4770 : 0.33611977100372314\n",
      "Training loss for batch 4771 : 0.21864593029022217\n",
      "Training loss for batch 4772 : 0.06094527989625931\n",
      "Training loss for batch 4773 : 0.2113114893436432\n",
      "Training loss for batch 4774 : 0.09894997626543045\n",
      "Training loss for batch 4775 : 0.0\n",
      "Training loss for batch 4776 : 0.49154460430145264\n",
      "Training loss for batch 4777 : 0.0\n",
      "Training loss for batch 4778 : 0.05343770235776901\n",
      "Training loss for batch 4779 : 0.060131482779979706\n",
      "Training loss for batch 4780 : 0.04057867079973221\n",
      "Training loss for batch 4781 : 0.26045292615890503\n",
      "Training loss for batch 4782 : 0.06414677947759628\n",
      "Training loss for batch 4783 : 0.075802743434906\n",
      "Training loss for batch 4784 : 0.24182181060314178\n",
      "Training loss for batch 4785 : 0.15122351050376892\n",
      "Training loss for batch 4786 : 0.07554694265127182\n",
      "Training loss for batch 4787 : 0.08741757273674011\n",
      "Training loss for batch 4788 : 0.0419662706553936\n",
      "Training loss for batch 4789 : 0.3747589886188507\n",
      "Training loss for batch 4790 : 0.35719507932662964\n",
      "Training loss for batch 4791 : 0.16342198848724365\n",
      "Training loss for batch 4792 : 0.29364439845085144\n",
      "Training loss for batch 4793 : 0.06531267613172531\n",
      "Training loss for batch 4794 : 0.12397364526987076\n",
      "Training loss for batch 4795 : 0.2623111605644226\n",
      "Training loss for batch 4796 : 0.13739661872386932\n",
      "Training loss for batch 4797 : 0.13911792635917664\n",
      "Training loss for batch 4798 : 0.06500557065010071\n",
      "Training loss for batch 4799 : 0.19299913942813873\n",
      "Training loss for batch 4800 : 0.09025052934885025\n",
      "Training loss for batch 4801 : 0.16630974411964417\n",
      "Training loss for batch 4802 : 0.8271077871322632\n",
      "Training loss for batch 4803 : 0.18727931380271912\n",
      "Training loss for batch 4804 : 0.44039100408554077\n",
      "Training loss for batch 4805 : 0.04915272444486618\n",
      "Training loss for batch 4806 : 0.12370184063911438\n",
      "Training loss for batch 4807 : 0.1964622437953949\n",
      "Training loss for batch 4808 : 0.21103039383888245\n",
      "Training loss for batch 4809 : 0.11221811175346375\n",
      "Training loss for batch 4810 : 0.37412095069885254\n",
      "Training loss for batch 4811 : 0.41375282406806946\n",
      "Training loss for batch 4812 : 0.07762901484966278\n",
      "Training loss for batch 4813 : 0.1390780508518219\n",
      "Training loss for batch 4814 : 0.867967963218689\n",
      "Training loss for batch 4815 : 0.36770620942115784\n",
      "Training loss for batch 4816 : 0.06140484660863876\n",
      "Training loss for batch 4817 : 0.0979602262377739\n",
      "Training loss for batch 4818 : 0.02090485952794552\n",
      "Training loss for batch 4819 : 0.12244448065757751\n",
      "Training loss for batch 4820 : 0.06289108097553253\n",
      "Training loss for batch 4821 : 0.30754005908966064\n",
      "Training loss for batch 4822 : 0.005477677099406719\n",
      "Training loss for batch 4823 : 0.3077906668186188\n",
      "Training loss for batch 4824 : 0.33301618695259094\n",
      "Training loss for batch 4825 : 0.017923029139637947\n",
      "Training loss for batch 4826 : 0.06401106715202332\n",
      "Training loss for batch 4827 : 0.039443496614694595\n",
      "Training loss for batch 4828 : 0.25215622782707214\n",
      "Training loss for batch 4829 : 0.19974349439144135\n",
      "Training loss for batch 4830 : 0.21579629182815552\n",
      "Training loss for batch 4831 : 0.11386353522539139\n",
      "Training loss for batch 4832 : 0.24224147200584412\n",
      "Training loss for batch 4833 : 0.0006481035379692912\n",
      "Training loss for batch 4834 : 0.15229806303977966\n",
      "Training loss for batch 4835 : 0.057480283081531525\n",
      "Training loss for batch 4836 : 0.3779616057872772\n",
      "Training loss for batch 4837 : 0.3028782606124878\n",
      "Training loss for batch 4838 : 0.04918918013572693\n",
      "Training loss for batch 4839 : 0.60081946849823\n",
      "Training loss for batch 4840 : 0.09176140278577805\n",
      "Training loss for batch 4841 : 0.34607890248298645\n",
      "Training loss for batch 4842 : 0.02860354632139206\n",
      "Training loss for batch 4843 : 0.014399006962776184\n",
      "Training loss for batch 4844 : 0.1074424758553505\n",
      "Training loss for batch 4845 : 0.21379578113555908\n",
      "Training loss for batch 4846 : 0.13379183411598206\n",
      "Training loss for batch 4847 : 0.0849667638540268\n",
      "Training loss for batch 4848 : 0.06117762252688408\n",
      "Training loss for batch 4849 : 0.17776170372962952\n",
      "Training loss for batch 4850 : 0.08056419342756271\n",
      "Training loss for batch 4851 : 0.0660269558429718\n",
      "Training loss for batch 4852 : 0.1464248150587082\n",
      "Training loss for batch 4853 : 0.11441198736429214\n",
      "Training loss for batch 4854 : 0.01999032124876976\n",
      "Training loss for batch 4855 : 0.10322937369346619\n",
      "Training loss for batch 4856 : 0.0\n",
      "Training loss for batch 4857 : 0.09132573008537292\n",
      "Training loss for batch 4858 : 0.14269854128360748\n",
      "Training loss for batch 4859 : 0.13684973120689392\n",
      "Training loss for batch 4860 : 0.33216825127601624\n",
      "Training loss for batch 4861 : 0.007281854748725891\n",
      "Training loss for batch 4862 : 0.02716035209596157\n",
      "Training loss for batch 4863 : 0.24558980762958527\n",
      "Training loss for batch 4864 : 0.18530696630477905\n",
      "Training loss for batch 4865 : 0.008863657712936401\n",
      "Training loss for batch 4866 : 0.2909809350967407\n",
      "Training loss for batch 4867 : 0.0\n",
      "Training loss for batch 4868 : 0.010660847648978233\n",
      "Training loss for batch 4869 : 0.06286570429801941\n",
      "Training loss for batch 4870 : 0.05803348869085312\n",
      "Training loss for batch 4871 : 0.4216461777687073\n",
      "Training loss for batch 4872 : 0.19637686014175415\n",
      "Training loss for batch 4873 : 0.01111643947660923\n",
      "Training loss for batch 4874 : 0.03289208933711052\n",
      "Training loss for batch 4875 : 0.16085930168628693\n",
      "Training loss for batch 4876 : 0.0659823790192604\n",
      "Training loss for batch 4877 : 0.2731146812438965\n",
      "Training loss for batch 4878 : 0.6766374707221985\n",
      "Training loss for batch 4879 : 0.23423346877098083\n",
      "Training loss for batch 4880 : 0.14172549545764923\n",
      "Training loss for batch 4881 : 0.3989964723587036\n",
      "Training loss for batch 4882 : 0.003022310556843877\n",
      "Training loss for batch 4883 : 0.0464225672185421\n",
      "Training loss for batch 4884 : 0.003418539883568883\n",
      "Training loss for batch 4885 : 0.006679694168269634\n",
      "Training loss for batch 4886 : 0.16419802606105804\n",
      "Training loss for batch 4887 : 0.48595425486564636\n",
      "Training loss for batch 4888 : 0.33035847544670105\n",
      "Training loss for batch 4889 : 0.24300114810466766\n",
      "Training loss for batch 4890 : 0.2944857180118561\n",
      "Training loss for batch 4891 : 0.10819689184427261\n",
      "Training loss for batch 4892 : 0.022358575835824013\n",
      "Training loss for batch 4893 : 0.5286785364151001\n",
      "Training loss for batch 4894 : 0.4200216233730316\n",
      "Training loss for batch 4895 : 0.1620579957962036\n",
      "Training loss for batch 4896 : 0.016557913273572922\n",
      "Training loss for batch 4897 : 0.030816636979579926\n",
      "Training loss for batch 4898 : 0.04123595729470253\n",
      "Training loss for batch 4899 : 0.012550830841064453\n",
      "Training loss for batch 4900 : 0.43733733892440796\n",
      "Training loss for batch 4901 : 0.1822059452533722\n",
      "Training loss for batch 4902 : 0.1396344155073166\n",
      "Training loss for batch 4903 : 0.14186061918735504\n",
      "Training loss for batch 4904 : 0.062369927763938904\n",
      "Training loss for batch 4905 : 0.35939928889274597\n",
      "Training loss for batch 4906 : 0.0627378597855568\n",
      "Training loss for batch 4907 : 0.012726595625281334\n",
      "Training loss for batch 4908 : 0.04216722026467323\n",
      "Training loss for batch 4909 : 0.17991895973682404\n",
      "Training loss for batch 4910 : 0.21377325057983398\n",
      "Training loss for batch 4911 : 0.3330974876880646\n",
      "Training loss for batch 4912 : 0.44825202226638794\n",
      "Training loss for batch 4913 : 0.0811772421002388\n",
      "Training loss for batch 4914 : 0.1019342765212059\n",
      "Training loss for batch 4915 : 0.1114264577627182\n",
      "Training loss for batch 4916 : 0.217926025390625\n",
      "Training loss for batch 4917 : 0.1899736076593399\n",
      "Training loss for batch 4918 : 0.06736800074577332\n",
      "Training loss for batch 4919 : 0.09131105989217758\n",
      "Training loss for batch 4920 : 0.0018348743906244636\n",
      "Training loss for batch 4921 : 0.42999714612960815\n",
      "Training loss for batch 4922 : 0.2659783959388733\n",
      "Training loss for batch 4923 : 0.0033395937643945217\n",
      "Training loss for batch 4924 : 1.3130580555298366e-05\n",
      "Training loss for batch 4925 : 0.3637164831161499\n",
      "Training loss for batch 4926 : 0.322449266910553\n",
      "Training loss for batch 4927 : 0.22391656041145325\n",
      "Training loss for batch 4928 : 0.13547439873218536\n",
      "Training loss for batch 4929 : 0.31020134687423706\n",
      "Training loss for batch 4930 : 0.18378010392189026\n",
      "Training loss for batch 4931 : 0.02147291973233223\n",
      "Training loss for batch 4932 : 0.5739230513572693\n",
      "Training loss for batch 4933 : 0.08064248412847519\n",
      "Training loss for batch 4934 : 0.03202078863978386\n",
      "Training loss for batch 4935 : 0.16822916269302368\n",
      "Training loss for batch 4936 : 0.20230846107006073\n",
      "Training loss for batch 4937 : 0.5907793045043945\n",
      "Training loss for batch 4938 : 0.17568203806877136\n",
      "Training loss for batch 4939 : 0.41614314913749695\n",
      "Training loss for batch 4940 : 0.391873300075531\n",
      "Training loss for batch 4941 : 0.1825372725725174\n",
      "Training loss for batch 4942 : 0.060226920992136\n",
      "Training loss for batch 4943 : 0.01629803702235222\n",
      "Training loss for batch 4944 : 0.16104812920093536\n",
      "Training loss for batch 4945 : 0.21328383684158325\n",
      "Training loss for batch 4946 : 0.24624750018119812\n",
      "Training loss for batch 4947 : 0.13500075042247772\n",
      "Training loss for batch 4948 : 0.2713365852832794\n",
      "Training loss for batch 4949 : 0.24018077552318573\n",
      "Training loss for batch 4950 : 0.18954378366470337\n",
      "Training loss for batch 4951 : 0.08963015675544739\n",
      "Training loss for batch 4952 : 0.018739666789770126\n",
      "Training loss for batch 4953 : 0.04169202968478203\n",
      "Training loss for batch 4954 : 0.0\n",
      "Training loss for batch 4955 : 0.012156764976680279\n",
      "Training loss for batch 4956 : 0.4539053440093994\n",
      "Training loss for batch 4957 : 0.052235279232263565\n",
      "Training loss for batch 4958 : 0.31521546840667725\n",
      "Training loss for batch 4959 : 0.43879953026771545\n",
      "Training loss for batch 4960 : 0.2961294651031494\n",
      "Training loss for batch 4961 : 0.10831643640995026\n",
      "Training loss for batch 4962 : 0.3310391306877136\n",
      "Training loss for batch 4963 : 0.36067426204681396\n",
      "Training loss for batch 4964 : 0.12880341708660126\n",
      "Training loss for batch 4965 : 0.09283265471458435\n",
      "Training loss for batch 4966 : 0.10783188045024872\n",
      "Training loss for batch 4967 : 0.1714659035205841\n",
      "Training loss for batch 4968 : 0.12187155336141586\n",
      "Training loss for batch 4969 : 0.08937609940767288\n",
      "Training loss for batch 4970 : 0.07906358689069748\n",
      "Training loss for batch 4971 : 0.175584614276886\n",
      "Training loss for batch 4972 : 0.3682510554790497\n",
      "Training loss for batch 4973 : 0.07435756176710129\n",
      "Training loss for batch 4974 : 0.3381865620613098\n",
      "Training loss for batch 4975 : 0.08999841660261154\n",
      "Training loss for batch 4976 : 0.12177448719739914\n",
      "Training loss for batch 4977 : 0.0776875838637352\n",
      "Training loss for batch 4978 : 0.19812913239002228\n",
      "Training loss for batch 4979 : 0.034762900322675705\n",
      "Training loss for batch 4980 : 0.2726234793663025\n",
      "Training loss for batch 4981 : 0.4520191550254822\n",
      "Training loss for batch 4982 : 0.25964897871017456\n",
      "Training loss for batch 4983 : 0.32517126202583313\n",
      "Training loss for batch 4984 : 0.015650751069188118\n",
      "Training loss for batch 4985 : 0.3586658239364624\n",
      "Training loss for batch 4986 : 0.210390105843544\n",
      "Training loss for batch 4987 : 0.06757117807865143\n",
      "Training loss for batch 4988 : 0.29727903008461\n",
      "Training loss for batch 4989 : 0.0022498569451272488\n",
      "Training loss for batch 4990 : 0.2468595951795578\n",
      "Training loss for batch 4991 : 0.09882653504610062\n",
      "Training loss for batch 4992 : 0.19259583950042725\n",
      "Training loss for batch 4993 : 0.15859757363796234\n",
      "Training loss for batch 4994 : 0.024780556559562683\n",
      "Training loss for batch 4995 : 0.08687040954828262\n",
      "Training loss for batch 4996 : 0.3242318034172058\n",
      "Training loss for batch 4997 : 0.2524697482585907\n",
      "Training loss for batch 4998 : 0.3413464426994324\n",
      "Training loss for batch 4999 : 0.5365780591964722\n",
      "Training loss for batch 5000 : 0.07479066401720047\n",
      "Training loss for batch 5001 : 0.1796766072511673\n",
      "Training loss for batch 5002 : 0.33106064796447754\n",
      "Training loss for batch 5003 : 0.23019066452980042\n",
      "Training loss for batch 5004 : 0.04284517839550972\n",
      "Training loss for batch 5005 : 0.28118395805358887\n",
      "Training loss for batch 5006 : 0.3148549199104309\n",
      "Training loss for batch 5007 : 0.0458747074007988\n",
      "Training loss for batch 5008 : 0.30058351159095764\n",
      "Training loss for batch 5009 : 0.1627172827720642\n",
      "Training loss for batch 5010 : 0.08812791109085083\n",
      "Training loss for batch 5011 : 0.014485885389149189\n",
      "Training loss for batch 5012 : 0.28220295906066895\n",
      "Training loss for batch 5013 : 0.18251746892929077\n",
      "Training loss for batch 5014 : 0.19079214334487915\n",
      "Training loss for batch 5015 : 0.08039639890193939\n",
      "Training loss for batch 5016 : 0.2994099259376526\n",
      "Training loss for batch 5017 : 0.1328769028186798\n",
      "Training loss for batch 5018 : 0.08647079020738602\n",
      "Training loss for batch 5019 : 0.08735308796167374\n",
      "Training loss for batch 5020 : 0.5611122846603394\n",
      "Training loss for batch 5021 : 0.036421999335289\n",
      "Training loss for batch 5022 : 0.09688686579465866\n",
      "Training loss for batch 5023 : 0.19364865124225616\n",
      "Training loss for batch 5024 : 0.23509731888771057\n",
      "Training loss for batch 5025 : 0.5439160466194153\n",
      "Training loss for batch 5026 : 0.11842246353626251\n",
      "Training loss for batch 5027 : 0.05318059027194977\n",
      "Training loss for batch 5028 : 0.2256052941083908\n",
      "Training loss for batch 5029 : 0.6893582344055176\n",
      "Training loss for batch 5030 : 0.0545254722237587\n",
      "Training loss for batch 5031 : 0.02660379372537136\n",
      "Training loss for batch 5032 : 0.049648724496364594\n",
      "Training loss for batch 5033 : 0.2292025238275528\n",
      "Training loss for batch 5034 : 0.1007082536816597\n",
      "Training loss for batch 5035 : 0.0379597544670105\n",
      "Training loss for batch 5036 : 0.3227410912513733\n",
      "Training loss for batch 5037 : 0.09248975664377213\n",
      "Training loss for batch 5038 : 0.43988925218582153\n",
      "Training loss for batch 5039 : 0.31734201312065125\n",
      "Training loss for batch 5040 : 0.03440286964178085\n",
      "Training loss for batch 5041 : 0.08819066733121872\n",
      "Training loss for batch 5042 : 0.20573078095912933\n",
      "Training loss for batch 5043 : 0.27178525924682617\n",
      "Training loss for batch 5044 : 0.3809482157230377\n",
      "Training loss for batch 5045 : 0.14856016635894775\n",
      "Training loss for batch 5046 : 0.06216851994395256\n",
      "Training loss for batch 5047 : 0.31875503063201904\n",
      "Training loss for batch 5048 : 0.3175505995750427\n",
      "Training loss for batch 5049 : 0.3864651024341583\n",
      "Training loss for batch 5050 : 0.22849169373512268\n",
      "Training loss for batch 5051 : 0.45821741223335266\n",
      "Training loss for batch 5052 : 0.0047623515129089355\n",
      "Training loss for batch 5053 : 0.2938701808452606\n",
      "Training loss for batch 5054 : 0.09436658769845963\n",
      "Training loss for batch 5055 : 0.24481698870658875\n",
      "Training loss for batch 5056 : 0.12109130620956421\n",
      "Training loss for batch 5057 : 0.4828433692455292\n",
      "Training loss for batch 5058 : 0.07608982920646667\n",
      "Training loss for batch 5059 : 0.060146499425172806\n",
      "Training loss for batch 5060 : 0.12761785089969635\n",
      "Training loss for batch 5061 : 0.08596108108758926\n",
      "Training loss for batch 5062 : 0.4401535987854004\n",
      "Training loss for batch 5063 : 0.2512764036655426\n",
      "Training loss for batch 5064 : 0.10915102809667587\n",
      "Training loss for batch 5065 : 0.3065424859523773\n",
      "Training loss for batch 5066 : 0.13229835033416748\n",
      "Training loss for batch 5067 : 0.6586818099021912\n",
      "Training loss for batch 5068 : 0.22483813762664795\n",
      "Training loss for batch 5069 : 0.0\n",
      "Training loss for batch 5070 : 0.023666871711611748\n",
      "Training loss for batch 5071 : 0.09711718559265137\n",
      "Training loss for batch 5072 : 0.27298396825790405\n",
      "Training loss for batch 5073 : 0.02649417519569397\n",
      "Training loss for batch 5074 : 0.09334893524646759\n",
      "Training loss for batch 5075 : 0.09331291913986206\n",
      "Training loss for batch 5076 : 0.08004733920097351\n",
      "Training loss for batch 5077 : 0.04386039078235626\n",
      "Training loss for batch 5078 : 0.2832426428794861\n",
      "Training loss for batch 5079 : 0.3387737572193146\n",
      "Training loss for batch 5080 : 0.10282296687364578\n",
      "Training loss for batch 5081 : 0.2170330286026001\n",
      "Training loss for batch 5082 : 0.2579745650291443\n",
      "Training loss for batch 5083 : 0.1644464135169983\n",
      "Training loss for batch 5084 : 0.1102437973022461\n",
      "Training loss for batch 5085 : 0.14855271577835083\n",
      "Training loss for batch 5086 : 0.17803725600242615\n",
      "Training loss for batch 5087 : 0.06560961157083511\n",
      "Training loss for batch 5088 : 0.07423774152994156\n",
      "Training loss for batch 5089 : 0.32758626341819763\n",
      "Training loss for batch 5090 : 0.10094226896762848\n",
      "Training loss for batch 5091 : 0.35945382714271545\n",
      "Training loss for batch 5092 : 0.38673630356788635\n",
      "Training loss for batch 5093 : 0.559907853603363\n",
      "Training loss for batch 5094 : 0.31591320037841797\n",
      "Training loss for batch 5095 : 0.03540100157260895\n",
      "Training loss for batch 5096 : 0.11278446018695831\n",
      "Training loss for batch 5097 : 0.12738215923309326\n",
      "Training loss for batch 5098 : 0.10776560753583908\n",
      "Training loss for batch 5099 : 0.11148947477340698\n",
      "Training loss for batch 5100 : 0.1321331411600113\n",
      "Training loss for batch 5101 : 0.2820073068141937\n",
      "Training loss for batch 5102 : 0.2553265392780304\n",
      "Training loss for batch 5103 : 0.6668119430541992\n",
      "Training loss for batch 5104 : 0.5987321734428406\n",
      "Training loss for batch 5105 : 0.05909208953380585\n",
      "Training loss for batch 5106 : 0.043896954506635666\n",
      "Training loss for batch 5107 : 0.15041595697402954\n",
      "Training loss for batch 5108 : 0.3104262948036194\n",
      "Training loss for batch 5109 : 0.15822014212608337\n",
      "Training loss for batch 5110 : 0.3579905331134796\n",
      "Training loss for batch 5111 : 0.06693089753389359\n",
      "Training loss for batch 5112 : 0.39272621273994446\n",
      "Training loss for batch 5113 : 0.021847447380423546\n",
      "Training loss for batch 5114 : 0.6742824912071228\n",
      "Training loss for batch 5115 : 0.08439037203788757\n",
      "Training loss for batch 5116 : 0.5010818839073181\n",
      "Training loss for batch 5117 : 0.213208869099617\n",
      "Training loss for batch 5118 : 0.279159814119339\n",
      "Training loss for batch 5119 : 0.17443569004535675\n",
      "Training loss for batch 5120 : 0.1661054491996765\n",
      "Training loss for batch 5121 : 0.12625786662101746\n",
      "Training loss for batch 5122 : 0.15769125521183014\n",
      "Training loss for batch 5123 : 0.010700820945203304\n",
      "Training loss for batch 5124 : 0.187344491481781\n",
      "Training loss for batch 5125 : 1.7398000636603683e-05\n",
      "Training loss for batch 5126 : 0.07024812698364258\n",
      "Training loss for batch 5127 : 0.13792531192302704\n",
      "Training loss for batch 5128 : 0.3446597456932068\n",
      "Training loss for batch 5129 : 0.06007004901766777\n",
      "Training loss for batch 5130 : 0.3726801574230194\n",
      "Training loss for batch 5131 : 0.26742124557495117\n",
      "Training loss for batch 5132 : 0.11164136230945587\n",
      "Training loss for batch 5133 : 0.2877086102962494\n",
      "Training loss for batch 5134 : 0.29707562923431396\n",
      "Training loss for batch 5135 : 0.19592219591140747\n",
      "Training loss for batch 5136 : 0.020208239555358887\n",
      "Training loss for batch 5137 : 0.10222440212965012\n",
      "Training loss for batch 5138 : 0.11436834186315536\n",
      "Training loss for batch 5139 : 0.5411074757575989\n",
      "Training loss for batch 5140 : 0.0748511329293251\n",
      "Training loss for batch 5141 : 0.3800458610057831\n",
      "Training loss for batch 5142 : 0.049383316189050674\n",
      "Training loss for batch 5143 : 0.14556632936000824\n",
      "Training loss for batch 5144 : 0.03748480603098869\n",
      "Training loss for batch 5145 : 0.4246698319911957\n",
      "Training loss for batch 5146 : 0.19224439561367035\n",
      "Training loss for batch 5147 : 0.03922274708747864\n",
      "Training loss for batch 5148 : 0.2908506691455841\n",
      "Training loss for batch 5149 : 0.08610542863607407\n",
      "Training loss for batch 5150 : 0.1580280214548111\n",
      "Training loss for batch 5151 : 0.2973896563053131\n",
      "Training loss for batch 5152 : 0.0923929512500763\n",
      "Training loss for batch 5153 : 0.14982552826404572\n",
      "Training loss for batch 5154 : 0.07378381490707397\n",
      "Training loss for batch 5155 : 0.20901896059513092\n",
      "Training loss for batch 5156 : 0.2014172524213791\n",
      "Training loss for batch 5157 : 0.008921554312109947\n",
      "Training loss for batch 5158 : 0.35988080501556396\n",
      "Training loss for batch 5159 : 0.24573203921318054\n",
      "Training loss for batch 5160 : 0.08601760864257812\n",
      "Training loss for batch 5161 : 0.44037556648254395\n",
      "Training loss for batch 5162 : 0.03009818121790886\n",
      "Training loss for batch 5163 : 0.1756671816110611\n",
      "Training loss for batch 5164 : 0.39408987760543823\n",
      "Training loss for batch 5165 : 0.08271472901105881\n",
      "Training loss for batch 5166 : 0.09664565324783325\n",
      "Training loss for batch 5167 : 0.1353583037853241\n",
      "Training loss for batch 5168 : 0.01415905263274908\n",
      "Training loss for batch 5169 : 0.126098170876503\n",
      "Training loss for batch 5170 : 0.28525757789611816\n",
      "Training loss for batch 5171 : 0.03540704399347305\n",
      "Training loss for batch 5172 : 0.07790209352970123\n",
      "Training loss for batch 5173 : 0.28728726506233215\n",
      "Training loss for batch 5174 : 0.03510908782482147\n",
      "Training loss for batch 5175 : 0.05179917439818382\n",
      "Training loss for batch 5176 : 0.1637618988752365\n",
      "Training loss for batch 5177 : 0.12002459168434143\n",
      "Training loss for batch 5178 : 0.37204092741012573\n",
      "Training loss for batch 5179 : 0.3911184072494507\n",
      "Training loss for batch 5180 : 0.3405839800834656\n",
      "Training loss for batch 5181 : 0.005273511167615652\n",
      "Training loss for batch 5182 : 0.08995409309864044\n",
      "Training loss for batch 5183 : 0.04431503266096115\n",
      "Training loss for batch 5184 : 0.07203048467636108\n",
      "Training loss for batch 5185 : 0.0\n",
      "Training loss for batch 5186 : 0.27345186471939087\n",
      "Training loss for batch 5187 : 0.049307942390441895\n",
      "Training loss for batch 5188 : 0.04477618262171745\n",
      "Training loss for batch 5189 : 0.39578765630722046\n",
      "Training loss for batch 5190 : 0.3249455690383911\n",
      "Training loss for batch 5191 : 0.06980889290571213\n",
      "Training loss for batch 5192 : 0.05298410728573799\n",
      "Training loss for batch 5193 : 0.03305720165371895\n",
      "Training loss for batch 5194 : 0.1727735996246338\n",
      "Training loss for batch 5195 : 0.20947417616844177\n",
      "Training loss for batch 5196 : 0.07025104761123657\n",
      "Training loss for batch 5197 : 0.19417423009872437\n",
      "Training loss for batch 5198 : 0.20616933703422546\n",
      "Training loss for batch 5199 : 0.08360186964273453\n",
      "Training loss for batch 5200 : 0.03777671977877617\n",
      "Training loss for batch 5201 : 0.3897917568683624\n",
      "Training loss for batch 5202 : 0.17771349847316742\n",
      "Training loss for batch 5203 : 0.23100285232067108\n",
      "Training loss for batch 5204 : 0.21054747700691223\n",
      "Training loss for batch 5205 : 0.5296141505241394\n",
      "Training loss for batch 5206 : 0.3161487877368927\n",
      "Training loss for batch 5207 : 0.050143349915742874\n",
      "Training loss for batch 5208 : 0.03424710035324097\n",
      "Training loss for batch 5209 : 0.00829380750656128\n",
      "Training loss for batch 5210 : 0.18517792224884033\n",
      "Training loss for batch 5211 : 0.2688755393028259\n",
      "Training loss for batch 5212 : 0.01371200755238533\n",
      "Training loss for batch 5213 : 0.11437415331602097\n",
      "Training loss for batch 5214 : 0.17635107040405273\n",
      "Training loss for batch 5215 : 0.15413570404052734\n",
      "Training loss for batch 5216 : 0.09060769528150558\n",
      "Training loss for batch 5217 : 0.2034628987312317\n",
      "Training loss for batch 5218 : 0.07136718928813934\n",
      "Training loss for batch 5219 : 0.009038767777383327\n",
      "Training loss for batch 5220 : 0.34020158648490906\n",
      "Training loss for batch 5221 : 0.31706029176712036\n",
      "Training loss for batch 5222 : 0.238157719373703\n",
      "Training loss for batch 5223 : 0.31050965189933777\n",
      "Training loss for batch 5224 : 0.4522848129272461\n",
      "Training loss for batch 5225 : 0.11878117173910141\n",
      "Training loss for batch 5226 : 0.14911018311977386\n",
      "Training loss for batch 5227 : 0.34188583493232727\n",
      "Training loss for batch 5228 : 0.4035535156726837\n",
      "Training loss for batch 5229 : 0.03780248388648033\n",
      "Training loss for batch 5230 : 0.12856213748455048\n",
      "Training loss for batch 5231 : 0.23546625673770905\n",
      "Training loss for batch 5232 : 0.2397589087486267\n",
      "Training loss for batch 5233 : 0.10613022744655609\n",
      "Training loss for batch 5234 : 0.10028742253780365\n",
      "Training loss for batch 5235 : 0.2267562597990036\n",
      "Training loss for batch 5236 : 0.1279522180557251\n",
      "Training loss for batch 5237 : 0.11911265552043915\n",
      "Training loss for batch 5238 : 0.16317963600158691\n",
      "Training loss for batch 5239 : 0.44589492678642273\n",
      "Training loss for batch 5240 : 0.0833251029253006\n",
      "Training loss for batch 5241 : 0.04672512412071228\n",
      "Training loss for batch 5242 : 0.0\n",
      "Training loss for batch 5243 : 0.05397637188434601\n",
      "Training loss for batch 5244 : 0.04695277288556099\n",
      "Training loss for batch 5245 : 0.07855354249477386\n",
      "Training loss for batch 5246 : 0.04119201749563217\n",
      "Training loss for batch 5247 : 0.18185283243656158\n",
      "Training loss for batch 5248 : 0.0\n",
      "Training loss for batch 5249 : 0.20625294744968414\n",
      "Training loss for batch 5250 : 0.30230513215065\n",
      "Training loss for batch 5251 : 0.00506387185305357\n",
      "Training loss for batch 5252 : 0.15704573690891266\n",
      "Training loss for batch 5253 : 0.06334873288869858\n",
      "Training loss for batch 5254 : 0.1282004565000534\n",
      "Training loss for batch 5255 : 0.09383217245340347\n",
      "Training loss for batch 5256 : 0.2541027069091797\n",
      "Training loss for batch 5257 : 0.20694689452648163\n",
      "Training loss for batch 5258 : 0.45855656266212463\n",
      "Training loss for batch 5259 : 0.1133822649717331\n",
      "Training loss for batch 5260 : 0.3436814546585083\n",
      "Training loss for batch 5261 : 0.18920302391052246\n",
      "Training loss for batch 5262 : 0.3301730751991272\n",
      "Training loss for batch 5263 : 0.12229235470294952\n",
      "Training loss for batch 5264 : 0.0652233362197876\n",
      "Training loss for batch 5265 : 0.2036101520061493\n",
      "Training loss for batch 5266 : 0.3900170624256134\n",
      "Training loss for batch 5267 : 0.06616684794425964\n",
      "Training loss for batch 5268 : 0.3935931921005249\n",
      "Training loss for batch 5269 : 0.0\n",
      "Training loss for batch 5270 : 0.2601710557937622\n",
      "Training loss for batch 5271 : 0.3270128667354584\n",
      "Training loss for batch 5272 : 0.1505788117647171\n",
      "Training loss for batch 5273 : 0.01454476173967123\n",
      "Training loss for batch 5274 : 0.023768780753016472\n",
      "Training loss for batch 5275 : 0.17255786061286926\n",
      "Training loss for batch 5276 : 0.1710127741098404\n",
      "Training loss for batch 5277 : 0.1454591155052185\n",
      "Training loss for batch 5278 : 0.5136376023292542\n",
      "Training loss for batch 5279 : 0.4938325881958008\n",
      "Training loss for batch 5280 : 0.04607789218425751\n",
      "Training loss for batch 5281 : 0.27509328722953796\n",
      "Training loss for batch 5282 : 0.06897817552089691\n",
      "Training loss for batch 5283 : 0.27842697501182556\n",
      "Training loss for batch 5284 : 0.4782995879650116\n",
      "Training loss for batch 5285 : 0.1492476761341095\n",
      "Training loss for batch 5286 : 0.048938799649477005\n",
      "Training loss for batch 5287 : 0.16396915912628174\n",
      "Training loss for batch 5288 : 0.7040235996246338\n",
      "Training loss for batch 5289 : 0.2370067536830902\n",
      "Training loss for batch 5290 : 0.009933325462043285\n",
      "Training loss for batch 5291 : 0.10787269473075867\n",
      "Training loss for batch 5292 : 0.025116726756095886\n",
      "Training loss for batch 5293 : 0.06760025769472122\n",
      "Training loss for batch 5294 : 0.47863829135894775\n",
      "Training loss for batch 5295 : 0.14485591650009155\n",
      "Training loss for batch 5296 : 0.6564914584159851\n",
      "Training loss for batch 5297 : 0.2730671465396881\n",
      "Training loss for batch 5298 : 0.05569012835621834\n",
      "Training loss for batch 5299 : 0.018284447491168976\n",
      "Training loss for batch 5300 : 0.34898748993873596\n",
      "Training loss for batch 5301 : 0.49117687344551086\n",
      "Training loss for batch 5302 : 0.0\n",
      "Training loss for batch 5303 : 0.06202763691544533\n",
      "Training loss for batch 5304 : 0.05329637974500656\n",
      "Training loss for batch 5305 : 0.27703362703323364\n",
      "Training loss for batch 5306 : 0.04333499073982239\n",
      "Training loss for batch 5307 : 0.10744184255599976\n",
      "Training loss for batch 5308 : 0.18019333481788635\n",
      "Training loss for batch 5309 : 0.21896597743034363\n",
      "Training loss for batch 5310 : 0.248653843998909\n",
      "Training loss for batch 5311 : 0.47878479957580566\n",
      "Training loss for batch 5312 : 0.30707570910453796\n",
      "Training loss for batch 5313 : 0.0\n",
      "Training loss for batch 5314 : 0.3189201056957245\n",
      "Training loss for batch 5315 : 0.3304137587547302\n",
      "Training loss for batch 5316 : 0.0009317563963122666\n",
      "Training loss for batch 5317 : 0.020807921886444092\n",
      "Training loss for batch 5318 : 0.0785505473613739\n",
      "Training loss for batch 5319 : 0.21110975742340088\n",
      "Training loss for batch 5320 : 0.1586369425058365\n",
      "Training loss for batch 5321 : 0.25969672203063965\n",
      "Training loss for batch 5322 : 0.23065423965454102\n",
      "Training loss for batch 5323 : 0.1275615394115448\n",
      "Training loss for batch 5324 : 0.02163369581103325\n",
      "Training loss for batch 5325 : 0.6956289410591125\n",
      "Training loss for batch 5326 : 0.26879242062568665\n",
      "Training loss for batch 5327 : 0.235159769654274\n",
      "Training loss for batch 5328 : 0.07446333765983582\n",
      "Training loss for batch 5329 : 0.2238001525402069\n",
      "Training loss for batch 5330 : 0.08758857101202011\n",
      "Training loss for batch 5331 : 0.08415939658880234\n",
      "Training loss for batch 5332 : 0.17604441940784454\n",
      "Training loss for batch 5333 : 0.06849704682826996\n",
      "Training loss for batch 5334 : 0.10903763025999069\n",
      "Training loss for batch 5335 : 0.04243110492825508\n",
      "Training loss for batch 5336 : 0.0\n",
      "Training loss for batch 5337 : 0.03349258005619049\n",
      "Training loss for batch 5338 : 0.07294152677059174\n",
      "Training loss for batch 5339 : 0.011656580492854118\n",
      "Training loss for batch 5340 : 0.2529342472553253\n",
      "Training loss for batch 5341 : 0.3490513563156128\n",
      "Training loss for batch 5342 : 0.25877535343170166\n",
      "Training loss for batch 5343 : 0.2967222332954407\n",
      "Training loss for batch 5344 : 0.19236372411251068\n",
      "Training loss for batch 5345 : 0.054920800030231476\n",
      "Training loss for batch 5346 : 0.3847852945327759\n",
      "Training loss for batch 5347 : 0.02326560579240322\n",
      "Training loss for batch 5348 : 0.4273109436035156\n",
      "Training loss for batch 5349 : 0.2586077153682709\n",
      "Training loss for batch 5350 : 0.0831034705042839\n",
      "Training loss for batch 5351 : 0.3826248049736023\n",
      "Training loss for batch 5352 : 0.12634491920471191\n",
      "Training loss for batch 5353 : 0.2892068922519684\n",
      "Training loss for batch 5354 : 0.11662165820598602\n",
      "Training loss for batch 5355 : 0.0012707222485914826\n",
      "Training loss for batch 5356 : 0.021523747593164444\n",
      "Training loss for batch 5357 : 0.00815589539706707\n",
      "Training loss for batch 5358 : 0.0\n",
      "Training loss for batch 5359 : 0.1718904823064804\n",
      "Training loss for batch 5360 : 0.11484406888484955\n",
      "Training loss for batch 5361 : 0.331449419260025\n",
      "Training loss for batch 5362 : 0.039188917726278305\n",
      "Training loss for batch 5363 : 0.1751880645751953\n",
      "Training loss for batch 5364 : 0.23697707056999207\n",
      "Training loss for batch 5365 : 0.28127607703208923\n",
      "Training loss for batch 5366 : 0.12794314324855804\n",
      "Training loss for batch 5367 : 0.19927911460399628\n",
      "Training loss for batch 5368 : 0.15825170278549194\n",
      "Training loss for batch 5369 : 0.02168484590947628\n",
      "Training loss for batch 5370 : 0.1937599629163742\n",
      "Training loss for batch 5371 : 0.35204824805259705\n",
      "Training loss for batch 5372 : 0.0736810490489006\n",
      "Training loss for batch 5373 : 0.14141105115413666\n",
      "Training loss for batch 5374 : 0.1842781901359558\n",
      "Training loss for batch 5375 : 0.0041788918897509575\n",
      "Training loss for batch 5376 : 0.1716124564409256\n",
      "Training loss for batch 5377 : 0.3209472894668579\n",
      "Training loss for batch 5378 : 0.36128610372543335\n",
      "Training loss for batch 5379 : 0.39495930075645447\n",
      "Training loss for batch 5380 : 0.06619381904602051\n",
      "Training loss for batch 5381 : 0.2023714929819107\n",
      "Training loss for batch 5382 : 0.3738001585006714\n",
      "Training loss for batch 5383 : 0.2189219892024994\n",
      "Training loss for batch 5384 : 0.1035875678062439\n",
      "Training loss for batch 5385 : 0.17978526651859283\n",
      "Training loss for batch 5386 : 0.07989640533924103\n",
      "Training loss for batch 5387 : 0.3394126296043396\n",
      "Training loss for batch 5388 : 0.22624947130680084\n",
      "Training loss for batch 5389 : 0.2831774353981018\n",
      "Training loss for batch 5390 : 0.055158358067274094\n",
      "Training loss for batch 5391 : 0.18703415989875793\n",
      "Training loss for batch 5392 : 0.6152857542037964\n",
      "Training loss for batch 5393 : 0.1294502168893814\n",
      "Training loss for batch 5394 : 0.13872170448303223\n",
      "Training loss for batch 5395 : 0.136697918176651\n",
      "Training loss for batch 5396 : 0.5782755017280579\n",
      "Training loss for batch 5397 : 0.5994755625724792\n",
      "Training loss for batch 5398 : 0.12963800132274628\n",
      "Training loss for batch 5399 : 0.3461158871650696\n",
      "Training loss for batch 5400 : 0.1265799105167389\n",
      "Training loss for batch 5401 : 0.20651116967201233\n",
      "Training loss for batch 5402 : 0.2841586470603943\n",
      "Training loss for batch 5403 : 0.11352094262838364\n",
      "Training loss for batch 5404 : 0.04315728694200516\n",
      "Training loss for batch 5405 : 0.09623105078935623\n",
      "Training loss for batch 5406 : 0.15733806788921356\n",
      "Training loss for batch 5407 : 0.147652730345726\n",
      "Training loss for batch 5408 : 0.002957148477435112\n",
      "Training loss for batch 5409 : 0.02109237015247345\n",
      "Training loss for batch 5410 : 0.0\n",
      "Training loss for batch 5411 : 0.1498827040195465\n",
      "Training loss for batch 5412 : 0.03797949478030205\n",
      "Training loss for batch 5413 : 0.24609637260437012\n",
      "Training loss for batch 5414 : 0.12657035887241364\n",
      "Training loss for batch 5415 : 0.4460591971874237\n",
      "Training loss for batch 5416 : 0.12099715322256088\n",
      "Training loss for batch 5417 : 0.1936177909374237\n",
      "Training loss for batch 5418 : 0.5343295931816101\n",
      "Training loss for batch 5419 : 0.0789446160197258\n",
      "Training loss for batch 5420 : 0.019883472472429276\n",
      "Training loss for batch 5421 : 0.3591803312301636\n",
      "Training loss for batch 5422 : 0.44201943278312683\n",
      "Training loss for batch 5423 : 0.10342790931463242\n",
      "Training loss for batch 5424 : 0.37697479128837585\n",
      "Training loss for batch 5425 : 0.3655013144016266\n",
      "Training loss for batch 5426 : 0.1302170753479004\n",
      "Training loss for batch 5427 : 0.31729856133461\n",
      "Training loss for batch 5428 : 0.0\n",
      "Training loss for batch 5429 : 0.3028149902820587\n",
      "Training loss for batch 5430 : 0.02890140749514103\n",
      "Training loss for batch 5431 : 0.0973791778087616\n",
      "Training loss for batch 5432 : 0.1400182545185089\n",
      "Training loss for batch 5433 : 0.2733769118785858\n",
      "Training loss for batch 5434 : 0.14115378260612488\n",
      "Training loss for batch 5435 : 0.46348637342453003\n",
      "Training loss for batch 5436 : 0.3465740978717804\n",
      "Training loss for batch 5437 : 0.0\n",
      "Training loss for batch 5438 : 0.05457204952836037\n",
      "Training loss for batch 5439 : 0.463245153427124\n",
      "Training loss for batch 5440 : 0.22361212968826294\n",
      "Training loss for batch 5441 : 0.6167890429496765\n",
      "Training loss for batch 5442 : 0.24440380930900574\n",
      "Training loss for batch 5443 : 0.08298269659280777\n",
      "Training loss for batch 5444 : 0.04953402280807495\n",
      "Training loss for batch 5445 : 0.08452863991260529\n",
      "Training loss for batch 5446 : 0.06170513853430748\n",
      "Training loss for batch 5447 : 0.37101587653160095\n",
      "Training loss for batch 5448 : 0.07491102069616318\n",
      "Training loss for batch 5449 : 0.38888847827911377\n",
      "Training loss for batch 5450 : 0.15142162144184113\n",
      "Training loss for batch 5451 : 0.3965298533439636\n",
      "Training loss for batch 5452 : 0.700043797492981\n",
      "Training loss for batch 5453 : 0.18129777908325195\n",
      "Training loss for batch 5454 : 0.005956914741545916\n",
      "Training loss for batch 5455 : 0.21054022014141083\n",
      "Training loss for batch 5456 : -1.5882082493590133e-07\n",
      "Training loss for batch 5457 : 0.15501487255096436\n",
      "Training loss for batch 5458 : 0.06818809360265732\n",
      "Training loss for batch 5459 : 0.15015718340873718\n",
      "Training loss for batch 5460 : 0.07716342061758041\n",
      "Training loss for batch 5461 : 0.04529783874750137\n",
      "Training loss for batch 5462 : 0.16525818407535553\n",
      "Training loss for batch 5463 : 0.08648507297039032\n",
      "Training loss for batch 5464 : 0.24165348708629608\n",
      "Training loss for batch 5465 : 0.051104675978422165\n",
      "Training loss for batch 5466 : 0.07998402416706085\n",
      "Training loss for batch 5467 : 0.1527775079011917\n",
      "Training loss for batch 5468 : 0.0019701551645994186\n",
      "Training loss for batch 5469 : 0.32016628980636597\n",
      "Training loss for batch 5470 : 0.23605020344257355\n",
      "Training loss for batch 5471 : 0.21582889556884766\n",
      "Training loss for batch 5472 : 0.3491641581058502\n",
      "Training loss for batch 5473 : 0.0807444155216217\n",
      "Training loss for batch 5474 : 0.09112440794706345\n",
      "Training loss for batch 5475 : 0.2678270936012268\n",
      "Training loss for batch 5476 : 0.10406758636236191\n",
      "Training loss for batch 5477 : 0.013554975390434265\n",
      "Training loss for batch 5478 : 0.3607073724269867\n",
      "Training loss for batch 5479 : 0.049078889191150665\n",
      "Training loss for batch 5480 : 0.14567536115646362\n",
      "Training loss for batch 5481 : 0.7741069197654724\n",
      "Training loss for batch 5482 : 0.06836721301078796\n",
      "Training loss for batch 5483 : 0.20377178490161896\n",
      "Training loss for batch 5484 : 0.22170045971870422\n",
      "Training loss for batch 5485 : 0.12082158774137497\n",
      "Training loss for batch 5486 : 0.045359764248132706\n",
      "Training loss for batch 5487 : 0.282869815826416\n",
      "Training loss for batch 5488 : 0.04563352093100548\n",
      "Training loss for batch 5489 : 0.0\n",
      "Training loss for batch 5490 : 0.17255429923534393\n",
      "Training loss for batch 5491 : 0.023798102512955666\n",
      "Training loss for batch 5492 : 0.09647680073976517\n",
      "Training loss for batch 5493 : 0.3934355080127716\n",
      "Training loss for batch 5494 : 0.0860457494854927\n",
      "Training loss for batch 5495 : 0.588732898235321\n",
      "Training loss for batch 5496 : 0.20853003859519958\n",
      "Training loss for batch 5497 : 0.09213710576295853\n",
      "Training loss for batch 5498 : 0.06012475863099098\n",
      "Training loss for batch 5499 : 0.15475592017173767\n",
      "Training loss for batch 5500 : 0.3029610514640808\n",
      "Training loss for batch 5501 : 0.14327703416347504\n",
      "Training loss for batch 5502 : 0.24206063151359558\n",
      "Training loss for batch 5503 : 0.1866447627544403\n",
      "Training loss for batch 5504 : 0.12761765718460083\n",
      "Training loss for batch 5505 : 0.6610965132713318\n",
      "Training loss for batch 5506 : 0.03814234957098961\n",
      "Training loss for batch 5507 : 0.2269282042980194\n",
      "Training loss for batch 5508 : 0.21264292299747467\n",
      "Training loss for batch 5509 : 0.2689204812049866\n",
      "Training loss for batch 5510 : 0.1822926253080368\n",
      "Training loss for batch 5511 : 0.1066073551774025\n",
      "Training loss for batch 5512 : 0.06404603272676468\n",
      "Training loss for batch 5513 : 0.31473350524902344\n",
      "Training loss for batch 5514 : 0.10310503840446472\n",
      "Training loss for batch 5515 : 0.034098099917173386\n",
      "Training loss for batch 5516 : 0.46088695526123047\n",
      "Training loss for batch 5517 : 0.26148489117622375\n",
      "Training loss for batch 5518 : 0.19665363430976868\n",
      "Training loss for batch 5519 : 0.19057577848434448\n",
      "Training loss for batch 5520 : 0.18989653885364532\n",
      "Training loss for batch 5521 : 0.17877553403377533\n",
      "Training loss for batch 5522 : 0.7393333315849304\n",
      "Training loss for batch 5523 : 0.027023177593946457\n",
      "Training loss for batch 5524 : 0.19122421741485596\n",
      "Training loss for batch 5525 : 0.09741580486297607\n",
      "Training loss for batch 5526 : 0.0387105830013752\n",
      "Training loss for batch 5527 : 0.05969947576522827\n",
      "Training loss for batch 5528 : 0.0410144105553627\n",
      "Training loss for batch 5529 : 0.2583388686180115\n",
      "Training loss for batch 5530 : 0.12771765887737274\n",
      "Training loss for batch 5531 : 0.005075782537460327\n",
      "Training loss for batch 5532 : 0.23002658784389496\n",
      "Training loss for batch 5533 : 0.13129782676696777\n",
      "Training loss for batch 5534 : 0.4335867464542389\n",
      "Training loss for batch 5535 : 0.5905747413635254\n",
      "Training loss for batch 5536 : 0.1855006068944931\n",
      "Training loss for batch 5537 : 0.15075485408306122\n",
      "Training loss for batch 5538 : 0.044354841113090515\n",
      "Training loss for batch 5539 : 0.14181944727897644\n",
      "Training loss for batch 5540 : 0.25430557131767273\n",
      "Training loss for batch 5541 : 0.259446382522583\n",
      "Training loss for batch 5542 : 0.0910835862159729\n",
      "Training loss for batch 5543 : 0.08931957930326462\n",
      "Training loss for batch 5544 : 0.21789219975471497\n",
      "Training loss for batch 5545 : 0.07887648046016693\n",
      "Training loss for batch 5546 : 0.05973295494914055\n",
      "Training loss for batch 5547 : 0.06965795159339905\n",
      "Training loss for batch 5548 : 0.18171925842761993\n",
      "Training loss for batch 5549 : 0.2622544467449188\n",
      "Training loss for batch 5550 : 0.32761117815971375\n",
      "Training loss for batch 5551 : 0.048857398331165314\n",
      "Training loss for batch 5552 : 0.28486451506614685\n",
      "Training loss for batch 5553 : 0.11587654054164886\n",
      "Training loss for batch 5554 : 0.039145566523075104\n",
      "Training loss for batch 5555 : 0.21185928583145142\n",
      "Training loss for batch 5556 : 0.1988426148891449\n",
      "Training loss for batch 5557 : 0.13498347997665405\n",
      "Training loss for batch 5558 : 0.07882168889045715\n",
      "Training loss for batch 5559 : 0.25993889570236206\n",
      "Training loss for batch 5560 : 0.04763723537325859\n",
      "Training loss for batch 5561 : 0.2599708139896393\n",
      "Training loss for batch 5562 : 0.12789900600910187\n",
      "Training loss for batch 5563 : 0.29294687509536743\n",
      "Training loss for batch 5564 : 0.07598735392093658\n",
      "Training loss for batch 5565 : 0.04804772138595581\n",
      "Training loss for batch 5566 : 0.011048631742596626\n",
      "Training loss for batch 5567 : 0.08066407591104507\n",
      "Training loss for batch 5568 : 0.04812227934598923\n",
      "Training loss for batch 5569 : 0.16766810417175293\n",
      "Training loss for batch 5570 : 0.03517359867691994\n",
      "Training loss for batch 5571 : 0.09505950659513474\n",
      "Training loss for batch 5572 : 0.32508793473243713\n",
      "Training loss for batch 5573 : 0.21555612981319427\n",
      "Training loss for batch 5574 : 0.09904574602842331\n",
      "Training loss for batch 5575 : 0.27347448468208313\n",
      "Training loss for batch 5576 : 0.1525111198425293\n",
      "Training loss for batch 5577 : 0.1237252801656723\n",
      "Training loss for batch 5578 : 0.17712271213531494\n",
      "Training loss for batch 5579 : 0.010958543047308922\n",
      "Training loss for batch 5580 : 0.2549736499786377\n",
      "Training loss for batch 5581 : 0.006106540560722351\n",
      "Training loss for batch 5582 : 0.12655234336853027\n",
      "Training loss for batch 5583 : 0.17226171493530273\n",
      "Training loss for batch 5584 : 0.2699896991252899\n",
      "Training loss for batch 5585 : 0.2802160680294037\n",
      "Training loss for batch 5586 : 0.295878142118454\n",
      "Training loss for batch 5587 : 0.1699427217245102\n",
      "Training loss for batch 5588 : 0.01314285397529602\n",
      "Training loss for batch 5589 : 0.22158756852149963\n",
      "Training loss for batch 5590 : 0.14724940061569214\n",
      "Training loss for batch 5591 : 0.034170206636190414\n",
      "Training loss for batch 5592 : 0.12167127430438995\n",
      "Training loss for batch 5593 : 0.10877401381731033\n",
      "Training loss for batch 5594 : 0.2512485980987549\n",
      "Training loss for batch 5595 : 0.06745369732379913\n",
      "Training loss for batch 5596 : 0.17742304503917694\n",
      "Training loss for batch 5597 : 0.27380359172821045\n",
      "Training loss for batch 5598 : 0.08761068433523178\n",
      "Training loss for batch 5599 : 0.2305634319782257\n",
      "Training loss for batch 5600 : 0.09512321650981903\n",
      "Training loss for batch 5601 : 0.03609927371144295\n",
      "Training loss for batch 5602 : 0.23731474578380585\n",
      "Training loss for batch 5603 : 0.20130112767219543\n",
      "Training loss for batch 5604 : 0.20720820128917694\n",
      "Training loss for batch 5605 : 0.04381455481052399\n",
      "Training loss for batch 5606 : 0.1571091264486313\n",
      "Training loss for batch 5607 : 0.08686826378107071\n",
      "Training loss for batch 5608 : 0.16106024384498596\n",
      "Training loss for batch 5609 : 0.2726286053657532\n",
      "Training loss for batch 5610 : 0.1291639804840088\n",
      "Training loss for batch 5611 : 0.04579945653676987\n",
      "Training loss for batch 5612 : 0.08770722895860672\n",
      "Training loss for batch 5613 : 0.4354914724826813\n",
      "Training loss for batch 5614 : 0.06178300455212593\n",
      "Training loss for batch 5615 : 0.01727273501455784\n",
      "Training loss for batch 5616 : 0.06720923632383347\n",
      "Training loss for batch 5617 : 0.6288217306137085\n",
      "Training loss for batch 5618 : 0.18772093951702118\n",
      "Training loss for batch 5619 : 0.19376704096794128\n",
      "Training loss for batch 5620 : 0.1677560955286026\n",
      "Training loss for batch 5621 : 0.020243925973773003\n",
      "Training loss for batch 5622 : 0.09773591160774231\n",
      "Training loss for batch 5623 : 0.03282313793897629\n",
      "Training loss for batch 5624 : 0.018097126856446266\n",
      "Training loss for batch 5625 : 0.229232057929039\n",
      "Training loss for batch 5626 : 0.00834694504737854\n",
      "Training loss for batch 5627 : 0.16324608027935028\n",
      "Training loss for batch 5628 : 0.24965089559555054\n",
      "Training loss for batch 5629 : 0.04826703295111656\n",
      "Training loss for batch 5630 : 0.03805966302752495\n",
      "Training loss for batch 5631 : 0.32445839047431946\n",
      "Training loss for batch 5632 : 0.29919102787971497\n",
      "Training loss for batch 5633 : 0.17035691440105438\n",
      "Training loss for batch 5634 : 0.3144557476043701\n",
      "Training loss for batch 5635 : 0.29370778799057007\n",
      "Training loss for batch 5636 : 0.2926236689090729\n",
      "Training loss for batch 5637 : 0.3131113350391388\n",
      "Training loss for batch 5638 : 0.22209742665290833\n",
      "Training loss for batch 5639 : 0.01673066057264805\n",
      "Training loss for batch 5640 : 0.2326032519340515\n",
      "Training loss for batch 5641 : 0.334645539522171\n",
      "Training loss for batch 5642 : 0.51292484998703\n",
      "Training loss for batch 5643 : 0.0537935346364975\n",
      "Training loss for batch 5644 : 0.3300916850566864\n",
      "Training loss for batch 5645 : 0.4081334173679352\n",
      "Training loss for batch 5646 : 0.0\n",
      "Training loss for batch 5647 : 0.024525579065084457\n",
      "Training loss for batch 5648 : 0.15869347751140594\n",
      "Training loss for batch 5649 : 0.23234280943870544\n",
      "Training loss for batch 5650 : 0.210173562169075\n",
      "Training loss for batch 5651 : 0.15577135980129242\n",
      "Training loss for batch 5652 : 0.22831609845161438\n",
      "Training loss for batch 5653 : 0.5027998685836792\n",
      "Training loss for batch 5654 : 0.2779596745967865\n",
      "Training loss for batch 5655 : 0.2617623209953308\n",
      "Training loss for batch 5656 : 0.1765417605638504\n",
      "Training loss for batch 5657 : 0.09612368792295456\n",
      "Training loss for batch 5658 : 0.4527219235897064\n",
      "Training loss for batch 5659 : 0.3084304928779602\n",
      "Training loss for batch 5660 : 0.14099687337875366\n",
      "Training loss for batch 5661 : 0.028404025360941887\n",
      "Training loss for batch 5662 : 0.03030288778245449\n",
      "Training loss for batch 5663 : 0.1606568545103073\n",
      "Training loss for batch 5664 : 0.27121502161026\n",
      "Training loss for batch 5665 : 0.024424025788903236\n",
      "Training loss for batch 5666 : 0.31802472472190857\n",
      "Training loss for batch 5667 : 0.037583813071250916\n",
      "Training loss for batch 5668 : 0.03774349018931389\n",
      "Training loss for batch 5669 : 0.15015630424022675\n",
      "Training loss for batch 5670 : 0.05766137316823006\n",
      "Training loss for batch 5671 : 0.26688680052757263\n",
      "Training loss for batch 5672 : 0.10087700188159943\n",
      "Training loss for batch 5673 : 0.21694932878017426\n",
      "Training loss for batch 5674 : 0.1909017562866211\n",
      "Training loss for batch 5675 : 0.029300808906555176\n",
      "Training loss for batch 5676 : 0.02565462701022625\n",
      "Training loss for batch 5677 : 0.16120746731758118\n",
      "Training loss for batch 5678 : 0.09945569187402725\n",
      "Training loss for batch 5679 : 0.2138468474149704\n",
      "Training loss for batch 5680 : 0.08314947783946991\n",
      "Training loss for batch 5681 : 0.26375308632850647\n",
      "Training loss for batch 5682 : 0.10815591365098953\n",
      "Training loss for batch 5683 : 0.11635079234838486\n",
      "Training loss for batch 5684 : 0.14320336282253265\n",
      "Training loss for batch 5685 : 0.050093602389097214\n",
      "Training loss for batch 5686 : 0.05572495236992836\n",
      "Training loss for batch 5687 : 0.22496913373470306\n",
      "Training loss for batch 5688 : 0.17940711975097656\n",
      "Training loss for batch 5689 : 0.049134816974401474\n",
      "Training loss for batch 5690 : 0.032255303114652634\n",
      "Training loss for batch 5691 : 0.43777456879615784\n",
      "Training loss for batch 5692 : 0.00331733631901443\n",
      "Training loss for batch 5693 : 0.19409675896167755\n",
      "Training loss for batch 5694 : 0.3480995297431946\n",
      "Training loss for batch 5695 : 0.18005764484405518\n",
      "Training loss for batch 5696 : 0.15783648192882538\n",
      "Training loss for batch 5697 : 0.16629177331924438\n",
      "Training loss for batch 5698 : 0.21035374701023102\n",
      "Training loss for batch 5699 : 0.08814559131860733\n",
      "Training loss for batch 5700 : 0.0\n",
      "Training loss for batch 5701 : 0.3930220305919647\n",
      "Training loss for batch 5702 : 0.03688168525695801\n",
      "Training loss for batch 5703 : 0.019266249611973763\n",
      "Training loss for batch 5704 : 0.07590879499912262\n",
      "Training loss for batch 5705 : 0.0005595978582277894\n",
      "Training loss for batch 5706 : 0.10858145356178284\n",
      "Training loss for batch 5707 : 0.9768675565719604\n",
      "Training loss for batch 5708 : 0.05178012698888779\n",
      "Training loss for batch 5709 : 0.26817235350608826\n",
      "Training loss for batch 5710 : 0.05505102872848511\n",
      "Training loss for batch 5711 : 0.035839661955833435\n",
      "Training loss for batch 5712 : 0.40320679545402527\n",
      "Training loss for batch 5713 : 0.2861044108867645\n",
      "Training loss for batch 5714 : 0.3564210534095764\n",
      "Training loss for batch 5715 : 0.07437897473573685\n",
      "Training loss for batch 5716 : 0.0\n",
      "Training loss for batch 5717 : 0.27667948603630066\n",
      "Training loss for batch 5718 : 0.020525729283690453\n",
      "Training loss for batch 5719 : 0.34530285000801086\n",
      "Training loss for batch 5720 : 0.15174239873886108\n",
      "Training loss for batch 5721 : 0.08711940050125122\n",
      "Training loss for batch 5722 : 0.10882925987243652\n",
      "Training loss for batch 5723 : 0.07448401302099228\n",
      "Training loss for batch 5724 : 0.06533991545438766\n",
      "Training loss for batch 5725 : 0.02264500968158245\n",
      "Training loss for batch 5726 : 0.45462268590927124\n",
      "Training loss for batch 5727 : 0.22761686146259308\n",
      "Training loss for batch 5728 : 0.3166773319244385\n",
      "Training loss for batch 5729 : 0.06560497730970383\n",
      "Training loss for batch 5730 : 0.14929728209972382\n",
      "Training loss for batch 5731 : 0.22266249358654022\n",
      "Training loss for batch 5732 : 0.1004655584692955\n",
      "Training loss for batch 5733 : 0.4932301342487335\n",
      "Training loss for batch 5734 : 0.14377687871456146\n",
      "Training loss for batch 5735 : 0.2026768922805786\n",
      "Training loss for batch 5736 : 0.06537853926420212\n",
      "Training loss for batch 5737 : 0.12209634482860565\n",
      "Training loss for batch 5738 : 0.18924005329608917\n",
      "Training loss for batch 5739 : 0.456891804933548\n",
      "Training loss for batch 5740 : 0.045073747634887695\n",
      "Training loss for batch 5741 : 0.6316264867782593\n",
      "Training loss for batch 5742 : 0.2904027998447418\n",
      "Training loss for batch 5743 : 0.33147603273391724\n",
      "Training loss for batch 5744 : 0.2027408629655838\n",
      "Training loss for batch 5745 : 0.05158384516835213\n",
      "Training loss for batch 5746 : 0.18948869407176971\n",
      "Training loss for batch 5747 : 0.24635173380374908\n",
      "Training loss for batch 5748 : 0.006496239919215441\n",
      "Training loss for batch 5749 : 0.1124875545501709\n",
      "Training loss for batch 5750 : 0.007341489195823669\n",
      "Training loss for batch 5751 : 0.019041461870074272\n",
      "Training loss for batch 5752 : 0.38093101978302\n",
      "Training loss for batch 5753 : 0.2295057773590088\n",
      "Training loss for batch 5754 : 0.13144683837890625\n",
      "Training loss for batch 5755 : 0.16430607438087463\n",
      "Training loss for batch 5756 : 0.07695959508419037\n",
      "Training loss for batch 5757 : 0.13264332711696625\n",
      "Training loss for batch 5758 : 0.5526920557022095\n",
      "Training loss for batch 5759 : 0.08658682554960251\n",
      "Training loss for batch 5760 : 0.04060825705528259\n",
      "Training loss for batch 5761 : 0.03085235133767128\n",
      "Training loss for batch 5762 : 0.24785801768302917\n",
      "Training loss for batch 5763 : 0.28438133001327515\n",
      "Training loss for batch 5764 : 0.0019067759858444333\n",
      "Training loss for batch 5765 : 0.1428213268518448\n",
      "Training loss for batch 5766 : 0.1102970540523529\n",
      "Training loss for batch 5767 : 0.0005921104457229376\n",
      "Training loss for batch 5768 : 0.29188302159309387\n",
      "Training loss for batch 5769 : 0.2226322591304779\n",
      "Training loss for batch 5770 : 0.1324261873960495\n",
      "Training loss for batch 5771 : 0.10469900071620941\n",
      "Training loss for batch 5772 : 0.3398812711238861\n",
      "Training loss for batch 5773 : 0.0875784307718277\n",
      "Training loss for batch 5774 : 0.15828460454940796\n",
      "Training loss for batch 5775 : 0.012746449559926987\n",
      "Training loss for batch 5776 : 0.08594953268766403\n",
      "Training loss for batch 5777 : 0.027739468961954117\n",
      "Training loss for batch 5778 : 0.15871977806091309\n",
      "Training loss for batch 5779 : 0.28268036246299744\n",
      "Training loss for batch 5780 : 0.030678963288664818\n",
      "Training loss for batch 5781 : 0.15467678010463715\n",
      "Training loss for batch 5782 : 0.14180095493793488\n",
      "Training loss for batch 5783 : 0.09055380523204803\n",
      "Training loss for batch 5784 : 0.011831983923912048\n",
      "Training loss for batch 5785 : 0.24740764498710632\n",
      "Training loss for batch 5786 : 0.0\n",
      "Training loss for batch 5787 : 0.11944155395030975\n",
      "Training loss for batch 5788 : 0.058212097734212875\n",
      "Training loss for batch 5789 : 0.13258898258209229\n",
      "Training loss for batch 5790 : 0.15616528689861298\n",
      "Training loss for batch 5791 : 0.5303118228912354\n",
      "Training loss for batch 5792 : 0.0032998942770063877\n",
      "Training loss for batch 5793 : 0.02270066924393177\n",
      "Training loss for batch 5794 : 0.005471354350447655\n",
      "Training loss for batch 5795 : 0.01126149296760559\n",
      "Training loss for batch 5796 : 0.11830072849988937\n",
      "Training loss for batch 5797 : 0.16973328590393066\n",
      "Training loss for batch 5798 : 0.2724565267562866\n",
      "Training loss for batch 5799 : 0.27309462428092957\n",
      "Training loss for batch 5800 : 0.0004562421527225524\n",
      "Training loss for batch 5801 : 0.2200462520122528\n",
      "Training loss for batch 5802 : 0.4191194772720337\n",
      "Training loss for batch 5803 : 0.03185949847102165\n",
      "Training loss for batch 5804 : 0.016408216208219528\n",
      "Training loss for batch 5805 : 0.1814642995595932\n",
      "Training loss for batch 5806 : 0.03157560154795647\n",
      "Training loss for batch 5807 : 0.0029349748510867357\n",
      "Training loss for batch 5808 : 0.014239061623811722\n",
      "Training loss for batch 5809 : 0.3101530969142914\n",
      "Training loss for batch 5810 : 0.10848092287778854\n",
      "Training loss for batch 5811 : 0.09661269187927246\n",
      "Training loss for batch 5812 : 0.11835074424743652\n",
      "Training loss for batch 5813 : 0.07374298572540283\n",
      "Training loss for batch 5814 : 0.12218159437179565\n",
      "Training loss for batch 5815 : 0.28789088129997253\n",
      "Training loss for batch 5816 : 0.11251675337553024\n",
      "Training loss for batch 5817 : 0.08880434185266495\n",
      "Training loss for batch 5818 : 0.3216424286365509\n",
      "Training loss for batch 5819 : 0.07327345758676529\n",
      "Training loss for batch 5820 : 0.22493621706962585\n",
      "Training loss for batch 5821 : 0.08382155001163483\n",
      "Training loss for batch 5822 : 0.030993595719337463\n",
      "Training loss for batch 5823 : 0.24645844101905823\n",
      "Training loss for batch 5824 : 0.1536647081375122\n",
      "Training loss for batch 5825 : 0.046654682606458664\n",
      "Training loss for batch 5826 : 0.009291092865169048\n",
      "Training loss for batch 5827 : 0.43640801310539246\n",
      "Training loss for batch 5828 : 0.06481745839118958\n",
      "Training loss for batch 5829 : 0.16534525156021118\n",
      "Training loss for batch 5830 : 0.04471186175942421\n",
      "Training loss for batch 5831 : 0.1885228455066681\n",
      "Training loss for batch 5832 : 0.15719401836395264\n",
      "Training loss for batch 5833 : 0.04448889195919037\n",
      "Training loss for batch 5834 : 0.18324492871761322\n",
      "Training loss for batch 5835 : 0.09787455201148987\n",
      "Training loss for batch 5836 : 0.21460936963558197\n",
      "Training loss for batch 5837 : 0.03157494217157364\n",
      "Training loss for batch 5838 : 0.21519631147384644\n",
      "Training loss for batch 5839 : 0.05103538930416107\n",
      "Training loss for batch 5840 : 0.2676745653152466\n",
      "Training loss for batch 5841 : 0.4444626569747925\n",
      "Training loss for batch 5842 : 0.4076182246208191\n",
      "Training loss for batch 5843 : 0.03235095366835594\n",
      "Training loss for batch 5844 : 0.08659742027521133\n",
      "Training loss for batch 5845 : 0.482906311750412\n",
      "Training loss for batch 5846 : 0.044395238161087036\n",
      "Training loss for batch 5847 : 0.3415651023387909\n",
      "Training loss for batch 5848 : 0.12989991903305054\n",
      "Training loss for batch 5849 : 0.4155036211013794\n",
      "Training loss for batch 5850 : 0.2016056478023529\n",
      "Training loss for batch 5851 : 0.4285401999950409\n",
      "Training loss for batch 5852 : 0.32477521896362305\n",
      "Training loss for batch 5853 : 0.08801887929439545\n",
      "Training loss for batch 5854 : 0.06844653934240341\n",
      "Training loss for batch 5855 : 0.0011531567433848977\n",
      "Training loss for batch 5856 : 0.029190843924880028\n",
      "Training loss for batch 5857 : 0.12604168057441711\n",
      "Training loss for batch 5858 : 0.15015114843845367\n",
      "Training loss for batch 5859 : 0.07907681912183762\n",
      "Training loss for batch 5860 : 0.12216944247484207\n",
      "Training loss for batch 5861 : 0.0056750779040157795\n",
      "Training loss for batch 5862 : 0.040870167315006256\n",
      "Training loss for batch 5863 : 0.3239509165287018\n",
      "Training loss for batch 5864 : 0.16136357188224792\n",
      "Training loss for batch 5865 : 0.2208431214094162\n",
      "Training loss for batch 5866 : 0.2214513123035431\n",
      "Training loss for batch 5867 : 0.0432448647916317\n",
      "Training loss for batch 5868 : 0.09445299953222275\n",
      "Training loss for batch 5869 : 0.07401692867279053\n",
      "Training loss for batch 5870 : 0.022951912134885788\n",
      "Training loss for batch 5871 : 0.10349851846694946\n",
      "Training loss for batch 5872 : 0.36533722281455994\n",
      "Training loss for batch 5873 : 0.17682811617851257\n",
      "Training loss for batch 5874 : 0.06949510425329208\n",
      "Training loss for batch 5875 : 0.29496702551841736\n",
      "Training loss for batch 5876 : 0.06385017931461334\n",
      "Training loss for batch 5877 : 0.4521876871585846\n",
      "Training loss for batch 5878 : 0.31661340594291687\n",
      "Training loss for batch 5879 : 0.031571801751852036\n",
      "Training loss for batch 5880 : 0.1206861361861229\n",
      "Training loss for batch 5881 : 0.025288667529821396\n",
      "Training loss for batch 5882 : 0.44787201285362244\n",
      "Training loss for batch 5883 : 0.04472357779741287\n",
      "Training loss for batch 5884 : 0.500953197479248\n",
      "Training loss for batch 5885 : 0.16315993666648865\n",
      "Training loss for batch 5886 : 0.5892914533615112\n",
      "Training loss for batch 5887 : 0.13319475948810577\n",
      "Training loss for batch 5888 : 0.32307231426239014\n",
      "Training loss for batch 5889 : 0.19203098118305206\n",
      "Training loss for batch 5890 : 0.06481366604566574\n",
      "Training loss for batch 5891 : 0.38144078850746155\n",
      "Training loss for batch 5892 : 0.269356369972229\n",
      "Training loss for batch 5893 : 0.37847062945365906\n",
      "Training loss for batch 5894 : 1.4083189964294434\n",
      "Training loss for batch 5895 : 0.1283341348171234\n",
      "Training loss for batch 5896 : 0.041879087686538696\n",
      "Training loss for batch 5897 : 0.03390219807624817\n",
      "Training loss for batch 5898 : 0.030870944261550903\n",
      "Training loss for batch 5899 : 0.08101095259189606\n",
      "Training loss for batch 5900 : 0.07993590831756592\n",
      "Training loss for batch 5901 : 0.008808543905615807\n",
      "Training loss for batch 5902 : 0.1769394874572754\n",
      "Training loss for batch 5903 : 0.08819910883903503\n",
      "Training loss for batch 5904 : 0.3799632489681244\n",
      "Training loss for batch 5905 : 0.0015826785238459706\n",
      "Training loss for batch 5906 : 0.0418766513466835\n",
      "Training loss for batch 5907 : 0.08063562959432602\n",
      "Training loss for batch 5908 : 0.02800317294895649\n",
      "Training loss for batch 5909 : 0.038067981600761414\n",
      "Training loss for batch 5910 : 0.025411369279026985\n",
      "Training loss for batch 5911 : 0.28817546367645264\n",
      "Training loss for batch 5912 : 0.012756879441440105\n",
      "Training loss for batch 5913 : 0.09558084607124329\n",
      "Training loss for batch 5914 : 0.34278541803359985\n",
      "Training loss for batch 5915 : 0.3992443382740021\n",
      "Training loss for batch 5916 : 0.02231520041823387\n",
      "Training loss for batch 5917 : 0.2111024409532547\n",
      "Training loss for batch 5918 : 0.1828708052635193\n",
      "Training loss for batch 5919 : 0.22145043313503265\n",
      "Training loss for batch 5920 : 0.005662259180098772\n",
      "Training loss for batch 5921 : 0.22374457120895386\n",
      "Training loss for batch 5922 : 0.1936141401529312\n",
      "Training loss for batch 5923 : 0.05376853048801422\n",
      "Training loss for batch 5924 : 0.012019137851893902\n",
      "Training loss for batch 5925 : 0.035661619156599045\n",
      "Training loss for batch 5926 : 0.010151764377951622\n",
      "Training loss for batch 5927 : 0.1363220512866974\n",
      "Training loss for batch 5928 : 0.5734188556671143\n",
      "Training loss for batch 5929 : 0.12201011180877686\n",
      "Training loss for batch 5930 : 0.15617971122264862\n",
      "Training loss for batch 5931 : 0.13777196407318115\n",
      "Training loss for batch 5932 : 0.1355646252632141\n",
      "Training loss for batch 5933 : -6.123238563304767e-06\n",
      "Training loss for batch 5934 : 0.08488936722278595\n",
      "Training loss for batch 5935 : 0.061643894761800766\n",
      "Training loss for batch 5936 : -2.0667446733568795e-05\n",
      "Training loss for batch 5937 : 0.13433243334293365\n",
      "Training loss for batch 5938 : 0.00850127823650837\n",
      "Training loss for batch 5939 : 0.39416199922561646\n",
      "Training loss for batch 5940 : 0.06528255343437195\n",
      "Training loss for batch 5941 : 0.0\n",
      "Training loss for batch 5942 : 0.0649767518043518\n",
      "Training loss for batch 5943 : 0.12229806184768677\n",
      "Training loss for batch 5944 : 0.4672814607620239\n",
      "Training loss for batch 5945 : 0.44288644194602966\n",
      "Training loss for batch 5946 : 0.06566181778907776\n",
      "Training loss for batch 5947 : 0.308749794960022\n",
      "Training loss for batch 5948 : 0.3477803170681\n",
      "Training loss for batch 5949 : 0.4897992014884949\n",
      "Training loss for batch 5950 : 0.2667728662490845\n",
      "Training loss for batch 5951 : 0.2301175892353058\n",
      "Training loss for batch 5952 : 0.04629630967974663\n",
      "Training loss for batch 5953 : 0.12773102521896362\n",
      "Training loss for batch 5954 : 0.5906979441642761\n",
      "Training loss for batch 5955 : 0.3058466911315918\n",
      "Training loss for batch 5956 : 0.558607816696167\n",
      "Training loss for batch 5957 : 0.34370657801628113\n",
      "Training loss for batch 5958 : 0.3083270192146301\n",
      "Training loss for batch 5959 : 0.22776982188224792\n",
      "Training loss for batch 5960 : 0.02012559026479721\n",
      "Training loss for batch 5961 : 0.5807300209999084\n",
      "Training loss for batch 5962 : 0.1673445701599121\n",
      "Training loss for batch 5963 : 0.4529760181903839\n",
      "Training loss for batch 5964 : 0.01822878047823906\n",
      "Training loss for batch 5965 : 0.16666439175605774\n",
      "Training loss for batch 5966 : 0.011579189449548721\n",
      "Training loss for batch 5967 : 0.2813749611377716\n",
      "Training loss for batch 5968 : 0.10271777212619781\n",
      "Training loss for batch 5969 : 0.3444023132324219\n",
      "Training loss for batch 5970 : 0.0974113866686821\n",
      "Training loss for batch 5971 : 0.3507629334926605\n",
      "Training loss for batch 5972 : 0.40778306126594543\n",
      "Training loss for batch 5973 : 0.07242614030838013\n",
      "Training loss for batch 5974 : 0.11455731838941574\n",
      "Training loss for batch 5975 : 0.03328193351626396\n",
      "Training loss for batch 5976 : 0.04876111447811127\n",
      "Training loss for batch 5977 : 0.01441027782857418\n",
      "Training loss for batch 5978 : 0.0364997461438179\n",
      "Training loss for batch 5979 : 0.2517159879207611\n",
      "Training loss for batch 5980 : 0.1928034871816635\n",
      "Training loss for batch 5981 : 0.2263752967119217\n",
      "Training loss for batch 5982 : 0.11569468677043915\n",
      "Training loss for batch 5983 : 0.44797495007514954\n",
      "Training loss for batch 5984 : 0.026419872418045998\n",
      "Training loss for batch 5985 : 0.38814470171928406\n",
      "Training loss for batch 5986 : 0.010298997163772583\n",
      "Training loss for batch 5987 : 0.21137502789497375\n",
      "Training loss for batch 5988 : 0.6218484044075012\n",
      "Training loss for batch 5989 : 0.21428821980953217\n",
      "Training loss for batch 5990 : 0.15292440354824066\n",
      "Training loss for batch 5991 : 0.1231364831328392\n",
      "Training loss for batch 5992 : 0.05912385880947113\n",
      "Training loss for batch 5993 : 0.1952836513519287\n",
      "Training loss for batch 5994 : 0.0\n",
      "Training loss for batch 5995 : 0.33103665709495544\n",
      "Training loss for batch 5996 : 0.10658971220254898\n",
      "Training loss for batch 5997 : 0.09332519769668579\n",
      "Training loss for batch 5998 : 0.10892343521118164\n",
      "Training loss for batch 5999 : 0.08837797492742538\n",
      "Training loss for batch 6000 : 0.06618158519268036\n",
      "Training loss for batch 6001 : 0.19010072946548462\n",
      "Training loss for batch 6002 : 0.003046915167942643\n",
      "Training loss for batch 6003 : 0.20626398921012878\n",
      "Training loss for batch 6004 : 0.3046419322490692\n",
      "Training loss for batch 6005 : 0.003979861736297607\n",
      "Training loss for batch 6006 : 0.11801590025424957\n",
      "Training loss for batch 6007 : 0.1007978692650795\n",
      "Training loss for batch 6008 : 0.16070817410945892\n",
      "Training loss for batch 6009 : 0.015569532290101051\n",
      "Training loss for batch 6010 : 0.13264019787311554\n",
      "Training loss for batch 6011 : 0.3871425986289978\n",
      "Training loss for batch 6012 : 0.23644398152828217\n",
      "Training loss for batch 6013 : 0.0\n",
      "Training loss for batch 6014 : 0.5661894083023071\n",
      "Training loss for batch 6015 : 0.12989456951618195\n",
      "Training loss for batch 6016 : 0.06577480584383011\n",
      "Training loss for batch 6017 : 0.032624028623104095\n",
      "Training loss for batch 6018 : 0.07156499475240707\n",
      "Training loss for batch 6019 : 0.12118839472532272\n",
      "Training loss for batch 6020 : 0.0\n",
      "Training loss for batch 6021 : 0.06633144617080688\n",
      "Training loss for batch 6022 : 0.2148425132036209\n",
      "Training loss for batch 6023 : 0.06588974595069885\n",
      "Training loss for batch 6024 : 0.13855375349521637\n",
      "Training loss for batch 6025 : 0.2732235789299011\n",
      "Training loss for batch 6026 : 0.34746912121772766\n",
      "Training loss for batch 6027 : 0.06961512565612793\n",
      "Training loss for batch 6028 : 0.356832355260849\n",
      "Training loss for batch 6029 : 0.12758168578147888\n",
      "Training loss for batch 6030 : 0.01899876445531845\n",
      "Training loss for batch 6031 : 0.11296291649341583\n",
      "Training loss for batch 6032 : 0.55804044008255\n",
      "Training loss for batch 6033 : 0.09934702515602112\n",
      "Training loss for batch 6034 : 0.10716908425092697\n",
      "Training loss for batch 6035 : 0.31902453303337097\n",
      "Training loss for batch 6036 : 0.07846243679523468\n",
      "Training loss for batch 6037 : 0.19294121861457825\n",
      "Training loss for batch 6038 : 0.1176004558801651\n",
      "Training loss for batch 6039 : 0.026453828439116478\n",
      "Training loss for batch 6040 : 0.07344742119312286\n",
      "Training loss for batch 6041 : 0.0017735561123117805\n",
      "Training loss for batch 6042 : 0.1583166867494583\n",
      "Training loss for batch 6043 : 0.06150055304169655\n",
      "Training loss for batch 6044 : 0.04667927324771881\n",
      "Training loss for batch 6045 : 0.072855144739151\n",
      "Training loss for batch 6046 : 0.2715027928352356\n",
      "Training loss for batch 6047 : 0.32949450612068176\n",
      "Training loss for batch 6048 : 0.22631235420703888\n",
      "Training loss for batch 6049 : 0.23634923994541168\n",
      "Training loss for batch 6050 : 0.004471074789762497\n",
      "Training loss for batch 6051 : 0.12575016915798187\n",
      "Training loss for batch 6052 : 0.31235724687576294\n",
      "Training loss for batch 6053 : 0.20322395861148834\n",
      "Training loss for batch 6054 : 0.17566801607608795\n",
      "Training loss for batch 6055 : 0.31488820910453796\n",
      "Training loss for batch 6056 : 0.4860709309577942\n",
      "Training loss for batch 6057 : 0.17854595184326172\n",
      "Training loss for batch 6058 : 0.12167251110076904\n",
      "Training loss for batch 6059 : 0.1774970293045044\n",
      "Training loss for batch 6060 : 0.17193257808685303\n",
      "Training loss for batch 6061 : 0.46972835063934326\n",
      "Training loss for batch 6062 : 0.3577239513397217\n",
      "Training loss for batch 6063 : 0.22558782994747162\n",
      "Training loss for batch 6064 : 0.053300872445106506\n",
      "Training loss for batch 6065 : 0.017724093049764633\n",
      "Training loss for batch 6066 : 0.19249999523162842\n",
      "Training loss for batch 6067 : 0.41560521721839905\n",
      "Training loss for batch 6068 : 0.1028820052742958\n",
      "Training loss for batch 6069 : 0.17135334014892578\n",
      "Training loss for batch 6070 : 0.06465250998735428\n",
      "Training loss for batch 6071 : 0.27534762024879456\n",
      "Training loss for batch 6072 : 0.4006706774234772\n",
      "Training loss for batch 6073 : 0.1257620006799698\n",
      "Training loss for batch 6074 : 0.011326214298605919\n",
      "Training loss for batch 6075 : 0.15631015598773956\n",
      "Training loss for batch 6076 : 0.05037738010287285\n",
      "Training loss for batch 6077 : 0.23083600401878357\n",
      "Training loss for batch 6078 : 0.10228502005338669\n",
      "Training loss for batch 6079 : 0.07350713014602661\n",
      "Training loss for batch 6080 : 0.4387231171131134\n",
      "Training loss for batch 6081 : 0.09258953481912613\n",
      "Training loss for batch 6082 : 0.1160750687122345\n",
      "Training loss for batch 6083 : 0.8628098964691162\n",
      "Training loss for batch 6084 : 0.34150463342666626\n",
      "Training loss for batch 6085 : 0.38443586230278015\n",
      "Training loss for batch 6086 : 0.16607847809791565\n",
      "Training loss for batch 6087 : 0.152828186750412\n",
      "Training loss for batch 6088 : 0.11229576915502548\n",
      "Training loss for batch 6089 : 0.10773397237062454\n",
      "Training loss for batch 6090 : 0.17603127658367157\n",
      "Training loss for batch 6091 : 0.25300443172454834\n",
      "Training loss for batch 6092 : 0.3479955792427063\n",
      "Training loss for batch 6093 : 0.023158462718129158\n",
      "Training loss for batch 6094 : 0.04124489426612854\n",
      "Training loss for batch 6095 : 0.10659997165203094\n",
      "Training loss for batch 6096 : 0.010513011366128922\n",
      "Training loss for batch 6097 : 0.10347145795822144\n",
      "Training loss for batch 6098 : -1.2365469046926592e-05\n",
      "Training loss for batch 6099 : 0.23481450974941254\n",
      "Training loss for batch 6100 : 0.398317813873291\n",
      "Training loss for batch 6101 : 0.058229465037584305\n",
      "Training loss for batch 6102 : 0.06636509299278259\n",
      "Training loss for batch 6103 : 0.16957947611808777\n",
      "Training loss for batch 6104 : 0.07151690125465393\n",
      "Training loss for batch 6105 : 0.1360032707452774\n",
      "Training loss for batch 6106 : 0.003274560673162341\n",
      "Training loss for batch 6107 : 0.05679090693593025\n",
      "Training loss for batch 6108 : 0.0329398512840271\n",
      "Training loss for batch 6109 : 0.1504448652267456\n",
      "Training loss for batch 6110 : 0.20996667444705963\n",
      "Training loss for batch 6111 : 0.02213531918823719\n",
      "Training loss for batch 6112 : 0.24963651597499847\n",
      "Training loss for batch 6113 : 0.5847576856613159\n",
      "Training loss for batch 6114 : 0.0\n",
      "Training loss for batch 6115 : 0.20571379363536835\n",
      "Training loss for batch 6116 : 0.12494762241840363\n",
      "Training loss for batch 6117 : 0.06580638140439987\n",
      "Training loss for batch 6118 : 0.10246787220239639\n",
      "Training loss for batch 6119 : 0.1384921669960022\n",
      "Training loss for batch 6120 : 0.18431800603866577\n",
      "Training loss for batch 6121 : 0.37344837188720703\n",
      "Training loss for batch 6122 : 0.3342256247997284\n",
      "Training loss for batch 6123 : 0.10451505333185196\n",
      "Training loss for batch 6124 : 0.13161368668079376\n",
      "Training loss for batch 6125 : 0.24245120584964752\n",
      "Training loss for batch 6126 : 0.1014556735754013\n",
      "Training loss for batch 6127 : 0.46601659059524536\n",
      "Training loss for batch 6128 : 0.20814688503742218\n",
      "Training loss for batch 6129 : 0.060238227248191833\n",
      "Training loss for batch 6130 : 0.24609017372131348\n",
      "Training loss for batch 6131 : 0.0\n",
      "Training loss for batch 6132 : 0.13224825263023376\n",
      "Training loss for batch 6133 : 0.07720988988876343\n",
      "Training loss for batch 6134 : 0.3000200390815735\n",
      "Training loss for batch 6135 : 0.012832462787628174\n",
      "Training loss for batch 6136 : 0.06188974902033806\n",
      "Training loss for batch 6137 : 0.01980336755514145\n",
      "Training loss for batch 6138 : 0.22748282551765442\n",
      "Training loss for batch 6139 : 0.1044396460056305\n",
      "Training loss for batch 6140 : 0.21221724152565002\n",
      "Training loss for batch 6141 : 0.09939062595367432\n",
      "Training loss for batch 6142 : 0.4455636739730835\n",
      "Training loss for batch 6143 : 0.16683676838874817\n",
      "Training loss for batch 6144 : 0.1822294294834137\n",
      "Training loss for batch 6145 : 0.1354660987854004\n",
      "Training loss for batch 6146 : 0.052176497876644135\n",
      "Training loss for batch 6147 : 0.11772968620061874\n",
      "Training loss for batch 6148 : 0.3850376307964325\n",
      "Training loss for batch 6149 : 0.023980680853128433\n",
      "Training loss for batch 6150 : 0.010490949265658855\n",
      "Training loss for batch 6151 : 0.21706879138946533\n",
      "Training loss for batch 6152 : 0.08621443808078766\n",
      "Training loss for batch 6153 : 0.0\n",
      "Training loss for batch 6154 : 0.415506511926651\n",
      "Training loss for batch 6155 : 0.12194405496120453\n",
      "Training loss for batch 6156 : 0.07509812712669373\n",
      "Training loss for batch 6157 : 0.703016996383667\n",
      "Training loss for batch 6158 : 0.18336834013462067\n",
      "Training loss for batch 6159 : 0.7364122867584229\n",
      "Training loss for batch 6160 : 0.11578318476676941\n",
      "Training loss for batch 6161 : 0.08227579295635223\n",
      "Training loss for batch 6162 : 0.3071065843105316\n",
      "Training loss for batch 6163 : 0.6797769665718079\n",
      "Training loss for batch 6164 : 0.0497346967458725\n",
      "Training loss for batch 6165 : 0.5361734628677368\n",
      "Training loss for batch 6166 : 0.2812565267086029\n",
      "Training loss for batch 6167 : 0.22542373836040497\n",
      "Training loss for batch 6168 : 0.3164898753166199\n",
      "Training loss for batch 6169 : 0.3201258182525635\n",
      "Training loss for batch 6170 : 0.2820768356323242\n",
      "Training loss for batch 6171 : 0.20589952170848846\n",
      "Training loss for batch 6172 : 0.01249256357550621\n",
      "Training loss for batch 6173 : 0.033208705484867096\n",
      "Training loss for batch 6174 : 0.10193231701850891\n",
      "Training loss for batch 6175 : 0.08475808054208755\n",
      "Training loss for batch 6176 : 0.02220858260989189\n",
      "Training loss for batch 6177 : 0.45229414105415344\n",
      "Training loss for batch 6178 : 0.19154436886310577\n",
      "Training loss for batch 6179 : 0.018383607268333435\n",
      "Training loss for batch 6180 : 0.18592095375061035\n",
      "Training loss for batch 6181 : 0.6639488935470581\n",
      "Training loss for batch 6182 : 0.10420485585927963\n",
      "Training loss for batch 6183 : 0.34065210819244385\n",
      "Training loss for batch 6184 : 0.0813126415014267\n",
      "Training loss for batch 6185 : 0.2823237478733063\n",
      "Training loss for batch 6186 : 0.12631070613861084\n",
      "Training loss for batch 6187 : 0.07525719702243805\n",
      "Training loss for batch 6188 : 0.009283725172281265\n",
      "Training loss for batch 6189 : 0.23348701000213623\n",
      "Training loss for batch 6190 : 0.022045807912945747\n",
      "Training loss for batch 6191 : 0.10517633706331253\n",
      "Training loss for batch 6192 : 0.0056954799219965935\n",
      "Training loss for batch 6193 : 0.2551462948322296\n",
      "Training loss for batch 6194 : 0.09429667890071869\n",
      "Training loss for batch 6195 : 0.31198015809059143\n",
      "Training loss for batch 6196 : 0.2343093305826187\n",
      "Training loss for batch 6197 : 0.1659158319234848\n",
      "Training loss for batch 6198 : 0.1353197544813156\n",
      "Training loss for batch 6199 : 0.258667916059494\n",
      "Training loss for batch 6200 : 0.2680809795856476\n",
      "Training loss for batch 6201 : 0.1548950970172882\n",
      "Training loss for batch 6202 : 0.05199481546878815\n",
      "Training loss for batch 6203 : 0.18781937658786774\n",
      "Training loss for batch 6204 : 0.14683140814304352\n",
      "Training loss for batch 6205 : 0.03174975514411926\n",
      "Training loss for batch 6206 : 0.004285175818949938\n",
      "Training loss for batch 6207 : 0.05571606755256653\n",
      "Training loss for batch 6208 : 0.17477291822433472\n",
      "Training loss for batch 6209 : 0.6345365047454834\n",
      "Training loss for batch 6210 : 0.05186549946665764\n",
      "Training loss for batch 6211 : 0.25648167729377747\n",
      "Training loss for batch 6212 : 0.3074265718460083\n",
      "Training loss for batch 6213 : 0.08714999258518219\n",
      "Training loss for batch 6214 : 0.1581593006849289\n",
      "Training loss for batch 6215 : 0.2048095166683197\n",
      "Training loss for batch 6216 : 0.16854998469352722\n",
      "Training loss for batch 6217 : 0.2069268375635147\n",
      "Training loss for batch 6218 : 0.3665734827518463\n",
      "Training loss for batch 6219 : 0.05460593104362488\n",
      "Training loss for batch 6220 : 0.05257783457636833\n",
      "Training loss for batch 6221 : 0.5612127184867859\n",
      "Training loss for batch 6222 : 0.2563154101371765\n",
      "Training loss for batch 6223 : 0.028883175924420357\n",
      "Training loss for batch 6224 : 0.258884072303772\n",
      "Training loss for batch 6225 : 0.4898289740085602\n",
      "Training loss for batch 6226 : 0.46978917717933655\n",
      "Training loss for batch 6227 : 0.09064965695142746\n",
      "Training loss for batch 6228 : 0.11754892021417618\n",
      "Training loss for batch 6229 : 0.0635492205619812\n",
      "Training loss for batch 6230 : 0.37368300557136536\n",
      "Training loss for batch 6231 : 0.019657302647829056\n",
      "Training loss for batch 6232 : 0.30849552154541016\n",
      "Training loss for batch 6233 : 0.026223015040159225\n",
      "Training loss for batch 6234 : 0.28717586398124695\n",
      "Training loss for batch 6235 : 0.5610566735267639\n",
      "Training loss for batch 6236 : 0.15396130084991455\n",
      "Training loss for batch 6237 : 0.5782334804534912\n",
      "Training loss for batch 6238 : 0.3542923033237457\n",
      "Training loss for batch 6239 : 0.6561971306800842\n",
      "Training loss for batch 6240 : 0.11563810706138611\n",
      "Training loss for batch 6241 : 0.040969979017972946\n",
      "Training loss for batch 6242 : 0.07429111748933792\n",
      "Training loss for batch 6243 : 0.09761836379766464\n",
      "Training loss for batch 6244 : 0.37544503808021545\n",
      "Training loss for batch 6245 : 0.05122974514961243\n",
      "Training loss for batch 6246 : 0.353860467672348\n",
      "Training loss for batch 6247 : 0.15321160852909088\n",
      "Training loss for batch 6248 : 0.08840501308441162\n",
      "Training loss for batch 6249 : 0.11339233815670013\n",
      "Training loss for batch 6250 : 0.1983194649219513\n",
      "Training loss for batch 6251 : 0.09390976279973984\n",
      "Training loss for batch 6252 : 0.22361907362937927\n",
      "Training loss for batch 6253 : 0.05143638700246811\n",
      "Training loss for batch 6254 : 0.0032512943726032972\n",
      "Training loss for batch 6255 : 0.3289528787136078\n",
      "Training loss for batch 6256 : 0.031240511685609818\n",
      "Training loss for batch 6257 : 0.11037237197160721\n",
      "Training loss for batch 6258 : 0.1377321034669876\n",
      "Training loss for batch 6259 : 0.010159960016608238\n",
      "Training loss for batch 6260 : 0.11359123140573502\n",
      "Training loss for batch 6261 : 0.16716605424880981\n",
      "Training loss for batch 6262 : 0.013025522232055664\n",
      "Training loss for batch 6263 : 0.25706803798675537\n",
      "Training loss for batch 6264 : 0.24926407635211945\n",
      "Training loss for batch 6265 : 0.07345805317163467\n",
      "Training loss for batch 6266 : 0.14392977952957153\n",
      "Training loss for batch 6267 : 0.2184922844171524\n",
      "Training loss for batch 6268 : 0.22083206474781036\n",
      "Training loss for batch 6269 : 0.07059218734502792\n",
      "Training loss for batch 6270 : 0.42275798320770264\n",
      "Training loss for batch 6271 : 0.3616132438182831\n",
      "Training loss for batch 6272 : 0.1310129314661026\n",
      "Training loss for batch 6273 : 0.0243504848331213\n",
      "Training loss for batch 6274 : 0.028871595859527588\n",
      "Training loss for batch 6275 : 0.13750384747982025\n",
      "Training loss for batch 6276 : 0.3274098038673401\n",
      "Training loss for batch 6277 : 0.018006205558776855\n",
      "Training loss for batch 6278 : 0.1255030781030655\n",
      "Training loss for batch 6279 : 0.3397422432899475\n",
      "Training loss for batch 6280 : 0.04908159375190735\n",
      "Training loss for batch 6281 : 0.1637776792049408\n",
      "Training loss for batch 6282 : 0.14311577379703522\n",
      "Training loss for batch 6283 : 0.18032069504261017\n",
      "Training loss for batch 6284 : 0.1614067256450653\n",
      "Training loss for batch 6285 : 0.1659659445285797\n",
      "Training loss for batch 6286 : 0.10800936073064804\n",
      "Training loss for batch 6287 : 0.054132506251335144\n",
      "Training loss for batch 6288 : 0.15334029495716095\n",
      "Training loss for batch 6289 : 0.39490702748298645\n",
      "Training loss for batch 6290 : 0.1405053585767746\n",
      "Training loss for batch 6291 : 0.11647211760282516\n",
      "Training loss for batch 6292 : 0.0835869237780571\n",
      "Training loss for batch 6293 : 0.013088732957839966\n",
      "Training loss for batch 6294 : 0.004667162895202637\n",
      "Training loss for batch 6295 : 0.1326216757297516\n",
      "Training loss for batch 6296 : 0.1428023725748062\n",
      "Training loss for batch 6297 : 0.13802067935466766\n",
      "Training loss for batch 6298 : 0.046902596950531006\n",
      "Training loss for batch 6299 : 0.45231935381889343\n",
      "Training loss for batch 6300 : 0.3534749150276184\n",
      "Training loss for batch 6301 : 0.23997198045253754\n",
      "Training loss for batch 6302 : 0.05983152240514755\n",
      "Training loss for batch 6303 : 0.012931103818118572\n",
      "Training loss for batch 6304 : 0.07578551769256592\n",
      "Training loss for batch 6305 : 0.29365333914756775\n",
      "Training loss for batch 6306 : 0.24254538118839264\n",
      "Training loss for batch 6307 : 0.30932822823524475\n",
      "Training loss for batch 6308 : 0.28323447704315186\n",
      "Training loss for batch 6309 : 0.9409759640693665\n",
      "Training loss for batch 6310 : 0.7537547945976257\n",
      "Training loss for batch 6311 : 0.31619560718536377\n",
      "Training loss for batch 6312 : 0.01836453191936016\n",
      "Training loss for batch 6313 : 0.17399388551712036\n",
      "Training loss for batch 6314 : 0.16262267529964447\n",
      "Training loss for batch 6315 : 0.22738592326641083\n",
      "Training loss for batch 6316 : 0.027641963213682175\n",
      "Training loss for batch 6317 : 0.2612774968147278\n",
      "Training loss for batch 6318 : 0.10184979438781738\n",
      "Training loss for batch 6319 : 0.1388760805130005\n",
      "Training loss for batch 6320 : 0.4590049982070923\n",
      "Training loss for batch 6321 : 0.07761320471763611\n",
      "Training loss for batch 6322 : 0.1430581659078598\n",
      "Training loss for batch 6323 : 0.04199065640568733\n",
      "Training loss for batch 6324 : 0.005148589611053467\n",
      "Training loss for batch 6325 : 0.20139996707439423\n",
      "Training loss for batch 6326 : 0.19075238704681396\n",
      "Training loss for batch 6327 : 0.3541199266910553\n",
      "Training loss for batch 6328 : 0.29352661967277527\n",
      "Training loss for batch 6329 : 0.039887867867946625\n",
      "Training loss for batch 6330 : 0.048274364322423935\n",
      "Training loss for batch 6331 : 0.1590229868888855\n",
      "Training loss for batch 6332 : 0.22670772671699524\n",
      "Training loss for batch 6333 : 0.11143103986978531\n",
      "Training loss for batch 6334 : 0.34685033559799194\n",
      "Training loss for batch 6335 : 0.162631094455719\n",
      "Training loss for batch 6336 : 0.2637867331504822\n",
      "Training loss for batch 6337 : 0.13266204297542572\n",
      "Training loss for batch 6338 : 0.09029486030340195\n",
      "Training loss for batch 6339 : 0.3048727810382843\n",
      "Training loss for batch 6340 : 0.02416050247848034\n",
      "Training loss for batch 6341 : 0.2423376590013504\n",
      "Training loss for batch 6342 : 0.22965121269226074\n",
      "Training loss for batch 6343 : 0.022037329152226448\n",
      "Training loss for batch 6344 : 0.20906852185726166\n",
      "Training loss for batch 6345 : 0.2860313355922699\n",
      "Training loss for batch 6346 : 0.03768956661224365\n",
      "Training loss for batch 6347 : 0.05957993492484093\n",
      "Training loss for batch 6348 : 0.0\n",
      "Training loss for batch 6349 : 0.021069882437586784\n",
      "Training loss for batch 6350 : 0.1587536484003067\n",
      "Training loss for batch 6351 : 0.12207376211881638\n",
      "Training loss for batch 6352 : 0.6832008957862854\n",
      "Training loss for batch 6353 : 0.2136479616165161\n",
      "Training loss for batch 6354 : 0.0013818310108035803\n",
      "Training loss for batch 6355 : 0.006789108272641897\n",
      "Training loss for batch 6356 : 0.10989870876073837\n",
      "Training loss for batch 6357 : 0.23683597147464752\n",
      "Training loss for batch 6358 : 0.305274099111557\n",
      "Training loss for batch 6359 : 0.20600244402885437\n",
      "Training loss for batch 6360 : 0.22614380717277527\n",
      "Training loss for batch 6361 : 0.36179131269454956\n",
      "Training loss for batch 6362 : 0.36289000511169434\n",
      "Training loss for batch 6363 : 0.043638359755277634\n",
      "Training loss for batch 6364 : 0.2068493664264679\n",
      "Training loss for batch 6365 : 0.24267578125\n",
      "Training loss for batch 6366 : 0.4071885943412781\n",
      "Training loss for batch 6367 : 0.35402658581733704\n",
      "Training loss for batch 6368 : 0.10291850566864014\n",
      "Training loss for batch 6369 : 0.10724428296089172\n",
      "Training loss for batch 6370 : 0.25231316685676575\n",
      "Training loss for batch 6371 : 0.03368678689002991\n",
      "Training loss for batch 6372 : 0.12826959788799286\n",
      "Training loss for batch 6373 : 0.153264120221138\n",
      "Training loss for batch 6374 : 0.2027285248041153\n",
      "Training loss for batch 6375 : 0.3194630742073059\n",
      "Training loss for batch 6376 : 0.21600672602653503\n",
      "Training loss for batch 6377 : 0.12882164120674133\n",
      "Training loss for batch 6378 : 0.01225075963884592\n",
      "Training loss for batch 6379 : 0.10866940021514893\n",
      "Training loss for batch 6380 : 0.02153024636209011\n",
      "Training loss for batch 6381 : 0.0828050971031189\n",
      "Training loss for batch 6382 : 0.0577518492937088\n",
      "Training loss for batch 6383 : 0.4290476441383362\n",
      "Training loss for batch 6384 : 0.04186463728547096\n",
      "Training loss for batch 6385 : 0.23238930106163025\n",
      "Training loss for batch 6386 : 0.30906400084495544\n",
      "Training loss for batch 6387 : 0.1325455754995346\n",
      "Training loss for batch 6388 : -5.4601878218818456e-05\n",
      "Training loss for batch 6389 : 0.022346602752804756\n",
      "Training loss for batch 6390 : 0.233587384223938\n",
      "Training loss for batch 6391 : 0.17528127133846283\n",
      "Training loss for batch 6392 : 0.03175347298383713\n",
      "Training loss for batch 6393 : 0.12012936919927597\n",
      "Training loss for batch 6394 : 0.2796904444694519\n",
      "Training loss for batch 6395 : 0.1983221173286438\n",
      "Training loss for batch 6396 : 0.017354413866996765\n",
      "Training loss for batch 6397 : 0.39936524629592896\n",
      "Training loss for batch 6398 : 0.2563120126724243\n",
      "Training loss for batch 6399 : 0.0571841299533844\n",
      "Training loss for batch 6400 : 0.1373826563358307\n",
      "Training loss for batch 6401 : 0.05734710022807121\n",
      "Training loss for batch 6402 : 0.13070107996463776\n",
      "Training loss for batch 6403 : 0.6176431179046631\n",
      "Training loss for batch 6404 : 0.012957712635397911\n",
      "Training loss for batch 6405 : 0.04500913619995117\n",
      "Training loss for batch 6406 : 0.09353717416524887\n",
      "Training loss for batch 6407 : 0.0822678729891777\n",
      "Training loss for batch 6408 : 0.17579174041748047\n",
      "Training loss for batch 6409 : 0.1412506401538849\n",
      "Training loss for batch 6410 : 0.0677848607301712\n",
      "Training loss for batch 6411 : 0.10141083598136902\n",
      "Training loss for batch 6412 : 0.20052962005138397\n",
      "Training loss for batch 6413 : 0.10446365922689438\n",
      "Training loss for batch 6414 : 0.3552929162979126\n",
      "Training loss for batch 6415 : 0.31490135192871094\n",
      "Training loss for batch 6416 : 0.329333633184433\n",
      "Training loss for batch 6417 : 0.20519839227199554\n",
      "Training loss for batch 6418 : 0.04202425479888916\n",
      "Training loss for batch 6419 : 0.026700904592871666\n",
      "Training loss for batch 6420 : 0.08585652709007263\n",
      "Training loss for batch 6421 : 0.16339196264743805\n",
      "Training loss for batch 6422 : 0.027574598789215088\n",
      "Training loss for batch 6423 : 0.20758700370788574\n",
      "Training loss for batch 6424 : 0.0644422397017479\n",
      "Training loss for batch 6425 : 0.14273585379123688\n",
      "Training loss for batch 6426 : 0.1612609624862671\n",
      "Training loss for batch 6427 : 0.260478138923645\n",
      "Training loss for batch 6428 : 0.14306840300559998\n",
      "Training loss for batch 6429 : 0.017080122604966164\n",
      "Training loss for batch 6430 : 0.0\n",
      "Training loss for batch 6431 : 0.035358134657144547\n",
      "Training loss for batch 6432 : 0.0034411116503179073\n",
      "Training loss for batch 6433 : 0.0\n",
      "Training loss for batch 6434 : 0.2165231704711914\n",
      "Training loss for batch 6435 : 0.06251794099807739\n",
      "Training loss for batch 6436 : 0.01442168653011322\n",
      "Training loss for batch 6437 : 0.4195166826248169\n",
      "Training loss for batch 6438 : 0.038142040371894836\n",
      "Training loss for batch 6439 : 0.2646036148071289\n",
      "Training loss for batch 6440 : 0.1533241868019104\n",
      "Training loss for batch 6441 : 0.1699218600988388\n",
      "Training loss for batch 6442 : 0.10353295505046844\n",
      "Training loss for batch 6443 : 0.4511297345161438\n",
      "Training loss for batch 6444 : 0.08798353374004364\n",
      "Training loss for batch 6445 : 1.0314170122146606\n",
      "Training loss for batch 6446 : 0.07593320310115814\n",
      "Training loss for batch 6447 : 0.1431768238544464\n",
      "Training loss for batch 6448 : 0.06792598962783813\n",
      "Training loss for batch 6449 : 0.013358579948544502\n",
      "Training loss for batch 6450 : 0.44294223189353943\n",
      "Training loss for batch 6451 : 0.4374247193336487\n",
      "Training loss for batch 6452 : 0.02768526040017605\n",
      "Training loss for batch 6453 : 0.08642962574958801\n",
      "Training loss for batch 6454 : 0.06026875972747803\n",
      "Training loss for batch 6455 : 0.03800930455327034\n",
      "Training loss for batch 6456 : 0.10536988824605942\n",
      "Training loss for batch 6457 : 0.2999119460582733\n",
      "Training loss for batch 6458 : 0.03023870848119259\n",
      "Training loss for batch 6459 : 0.377381294965744\n",
      "Training loss for batch 6460 : 0.09605903178453445\n",
      "Training loss for batch 6461 : 0.10498058050870895\n",
      "Training loss for batch 6462 : 0.6196454167366028\n",
      "Training loss for batch 6463 : 0.31728696823120117\n",
      "Training loss for batch 6464 : 0.14321421086788177\n",
      "Training loss for batch 6465 : 0.39103028178215027\n",
      "Training loss for batch 6466 : 0.4118800759315491\n",
      "Training loss for batch 6467 : 0.08587457239627838\n",
      "Training loss for batch 6468 : 0.08470882475376129\n",
      "Training loss for batch 6469 : 0.3210863173007965\n",
      "Training loss for batch 6470 : 0.15120604634284973\n",
      "Training loss for batch 6471 : 0.18399208784103394\n",
      "Training loss for batch 6472 : 0.22796091437339783\n",
      "Training loss for batch 6473 : 0.06830562651157379\n",
      "Training loss for batch 6474 : 0.00021458666014950722\n",
      "Training loss for batch 6475 : 0.05155685544013977\n",
      "Training loss for batch 6476 : 0.17642058432102203\n",
      "Training loss for batch 6477 : 0.007468442432582378\n",
      "Training loss for batch 6478 : 0.0\n",
      "Training loss for batch 6479 : 0.25110524892807007\n",
      "Training loss for batch 6480 : 0.0091253025457263\n",
      "Training loss for batch 6481 : 0.03852584958076477\n",
      "Training loss for batch 6482 : 0.08927387744188309\n",
      "Training loss for batch 6483 : 0.44431576132774353\n",
      "Training loss for batch 6484 : 0.15927840769290924\n",
      "Training loss for batch 6485 : 0.37271809577941895\n",
      "Training loss for batch 6486 : 0.0040715038776397705\n",
      "Training loss for batch 6487 : 0.1321520358324051\n",
      "Training loss for batch 6488 : 0.12867788970470428\n",
      "Training loss for batch 6489 : 0.3269542455673218\n",
      "Training loss for batch 6490 : 0.064851775765419\n",
      "Training loss for batch 6491 : 0.04991195350885391\n",
      "Training loss for batch 6492 : 0.08104681968688965\n",
      "Training loss for batch 6493 : 0.37559375166893005\n",
      "Training loss for batch 6494 : 0.0499037429690361\n",
      "Training loss for batch 6495 : 0.03934202715754509\n",
      "Training loss for batch 6496 : 0.4117170572280884\n",
      "Training loss for batch 6497 : 0.11472920328378677\n",
      "Training loss for batch 6498 : 0.005051752086728811\n",
      "Training loss for batch 6499 : 0.12803681194782257\n",
      "Training loss for batch 6500 : 0.21029150485992432\n",
      "Training loss for batch 6501 : 0.03320235759019852\n",
      "Training loss for batch 6502 : 0.1483083814382553\n",
      "Training loss for batch 6503 : 0.131444051861763\n",
      "Training loss for batch 6504 : 0.01339158695191145\n",
      "Training loss for batch 6505 : 0.26932641863822937\n",
      "Training loss for batch 6506 : 0.1267176866531372\n",
      "Training loss for batch 6507 : 0.02434469386935234\n",
      "Training loss for batch 6508 : 0.087412990629673\n",
      "Training loss for batch 6509 : 0.10008114576339722\n",
      "Training loss for batch 6510 : 0.1611909568309784\n",
      "Training loss for batch 6511 : 0.04860246181488037\n",
      "Training loss for batch 6512 : 0.09121014177799225\n",
      "Training loss for batch 6513 : 0.07355266809463501\n",
      "Training loss for batch 6514 : 0.4457036852836609\n",
      "Training loss for batch 6515 : 0.2928408682346344\n",
      "Training loss for batch 6516 : 0.0756724625825882\n",
      "Training loss for batch 6517 : 0.2864520847797394\n",
      "Training loss for batch 6518 : 0.20541106164455414\n",
      "Training loss for batch 6519 : 0.1672723889350891\n",
      "Training loss for batch 6520 : 0.32062068581581116\n",
      "Training loss for batch 6521 : 0.07132505625486374\n",
      "Training loss for batch 6522 : 0.03285006061196327\n",
      "Training loss for batch 6523 : 0.3198225498199463\n",
      "Training loss for batch 6524 : 0.6084395051002502\n",
      "Training loss for batch 6525 : 0.019167235121130943\n",
      "Training loss for batch 6526 : 0.09164904803037643\n",
      "Training loss for batch 6527 : 0.08829640597105026\n",
      "Training loss for batch 6528 : 0.16978371143341064\n",
      "Training loss for batch 6529 : 0.8822704553604126\n",
      "Training loss for batch 6530 : 0.08864755928516388\n",
      "Training loss for batch 6531 : 0.09614673256874084\n",
      "Training loss for batch 6532 : 0.09723611176013947\n",
      "Training loss for batch 6533 : 0.029526134952902794\n",
      "Training loss for batch 6534 : 0.06261752545833588\n",
      "Training loss for batch 6535 : 0.3199252784252167\n",
      "Training loss for batch 6536 : 0.0621894970536232\n",
      "Training loss for batch 6537 : 0.10891608893871307\n",
      "Training loss for batch 6538 : 0.15687665343284607\n",
      "Training loss for batch 6539 : 0.127216175198555\n",
      "Training loss for batch 6540 : 0.0\n",
      "Training loss for batch 6541 : 0.3312820792198181\n",
      "Training loss for batch 6542 : 0.0\n",
      "Training loss for batch 6543 : 0.13872599601745605\n",
      "Training loss for batch 6544 : 0.04443809390068054\n",
      "Training loss for batch 6545 : 0.028509365394711494\n",
      "Training loss for batch 6546 : 0.12878191471099854\n",
      "Training loss for batch 6547 : 0.04145437479019165\n",
      "Training loss for batch 6548 : 0.19384784996509552\n",
      "Training loss for batch 6549 : 0.05908576026558876\n",
      "Training loss for batch 6550 : 0.04090075194835663\n",
      "Training loss for batch 6551 : 0.2279968559741974\n",
      "Training loss for batch 6552 : 0.3352423310279846\n",
      "Training loss for batch 6553 : 0.14366422593593597\n",
      "Training loss for batch 6554 : 0.24279551208019257\n",
      "Training loss for batch 6555 : 0.25102266669273376\n",
      "Training loss for batch 6556 : 0.043984655290842056\n",
      "Training loss for batch 6557 : 0.09673622250556946\n",
      "Training loss for batch 6558 : 0.035061292350292206\n",
      "Training loss for batch 6559 : 0.3847923278808594\n",
      "Training loss for batch 6560 : 0.013700167648494244\n",
      "Training loss for batch 6561 : 0.034670375287532806\n",
      "Training loss for batch 6562 : 0.33877861499786377\n",
      "Training loss for batch 6563 : 0.03301321342587471\n",
      "Training loss for batch 6564 : 0.07270075380802155\n",
      "Training loss for batch 6565 : 0.12052073329687119\n",
      "Training loss for batch 6566 : 0.0\n",
      "Training loss for batch 6567 : 0.302362859249115\n",
      "Training loss for batch 6568 : 0.08248494565486908\n",
      "Training loss for batch 6569 : 0.2660577893257141\n",
      "Training loss for batch 6570 : 0.18406744301319122\n",
      "Training loss for batch 6571 : 0.013858556747436523\n",
      "Training loss for batch 6572 : 0.15642639994621277\n",
      "Training loss for batch 6573 : 0.28582683205604553\n",
      "Training loss for batch 6574 : 0.3011910021305084\n",
      "Training loss for batch 6575 : 0.4317776560783386\n",
      "Training loss for batch 6576 : 0.19839036464691162\n",
      "Training loss for batch 6577 : 0.13538914918899536\n",
      "Training loss for batch 6578 : 0.27667704224586487\n",
      "Training loss for batch 6579 : 0.028335144743323326\n",
      "Training loss for batch 6580 : 0.10838815569877625\n",
      "Training loss for batch 6581 : 0.0\n",
      "Training loss for batch 6582 : 0.19632162153720856\n",
      "Training loss for batch 6583 : 0.40299487113952637\n",
      "Training loss for batch 6584 : 0.06963995099067688\n",
      "Training loss for batch 6585 : 0.3539746403694153\n",
      "Training loss for batch 6586 : 0.03611543029546738\n",
      "Training loss for batch 6587 : 0.25763583183288574\n",
      "Training loss for batch 6588 : 0.515451967716217\n",
      "Training loss for batch 6589 : 0.417732834815979\n",
      "Training loss for batch 6590 : 0.3746746778488159\n",
      "Training loss for batch 6591 : 0.028621302917599678\n",
      "Training loss for batch 6592 : 0.29453742504119873\n",
      "Training loss for batch 6593 : 0.0303211472928524\n",
      "Training loss for batch 6594 : 0.4316747486591339\n",
      "Training loss for batch 6595 : 0.15439993143081665\n",
      "Training loss for batch 6596 : 0.044839244335889816\n",
      "Training loss for batch 6597 : 0.21656844019889832\n",
      "Training loss for batch 6598 : 0.04552172124385834\n",
      "Training loss for batch 6599 : 0.1168292984366417\n",
      "Training loss for batch 6600 : 0.17442429065704346\n",
      "Training loss for batch 6601 : 0.07469943910837173\n",
      "Training loss for batch 6602 : 0.055098600685596466\n",
      "Training loss for batch 6603 : 0.26564571261405945\n",
      "Training loss for batch 6604 : 0.016286388039588928\n",
      "Training loss for batch 6605 : 0.10682877153158188\n",
      "Training loss for batch 6606 : 0.10099571198225021\n",
      "Training loss for batch 6607 : 0.1566343456506729\n",
      "Training loss for batch 6608 : 0.5715576410293579\n",
      "Training loss for batch 6609 : 0.2948154807090759\n",
      "Training loss for batch 6610 : 0.39804840087890625\n",
      "Training loss for batch 6611 : 0.0\n",
      "Training loss for batch 6612 : 0.011979726143181324\n",
      "Training loss for batch 6613 : 0.25624170899391174\n",
      "Training loss for batch 6614 : 0.07736768573522568\n",
      "Training loss for batch 6615 : 0.08122782409191132\n",
      "Training loss for batch 6616 : 0.17530207335948944\n",
      "Training loss for batch 6617 : 0.13914018869400024\n",
      "Training loss for batch 6618 : 0.4981900751590729\n",
      "Training loss for batch 6619 : 0.09567473828792572\n",
      "Training loss for batch 6620 : 0.06954558938741684\n",
      "Training loss for batch 6621 : 0.39955437183380127\n",
      "Training loss for batch 6622 : 0.13867510855197906\n",
      "Training loss for batch 6623 : 0.29920703172683716\n",
      "Training loss for batch 6624 : 0.22365537285804749\n",
      "Training loss for batch 6625 : 0.5888625979423523\n",
      "Training loss for batch 6626 : 0.10699528455734253\n",
      "Training loss for batch 6627 : 0.07178400456905365\n",
      "Training loss for batch 6628 : 0.3572412133216858\n",
      "Training loss for batch 6629 : 0.22846658527851105\n",
      "Training loss for batch 6630 : 0.2990473806858063\n",
      "Training loss for batch 6631 : 0.0\n",
      "Training loss for batch 6632 : 0.29161182045936584\n",
      "Training loss for batch 6633 : 0.004115144722163677\n",
      "Training loss for batch 6634 : 0.15159650146961212\n",
      "Training loss for batch 6635 : 0.16967038810253143\n",
      "Training loss for batch 6636 : 0.3200727701187134\n",
      "Training loss for batch 6637 : 0.0012874703388661146\n",
      "Training loss for batch 6638 : 0.13924655318260193\n",
      "Training loss for batch 6639 : 0.08588887006044388\n",
      "Training loss for batch 6640 : 0.10791083425283432\n",
      "Training loss for batch 6641 : 0.3390709161758423\n",
      "Training loss for batch 6642 : 0.0\n",
      "Training loss for batch 6643 : 0.06471946835517883\n",
      "Training loss for batch 6644 : 0.31542742252349854\n",
      "Training loss for batch 6645 : 0.29908257722854614\n",
      "Training loss for batch 6646 : 0.045029375702142715\n",
      "Training loss for batch 6647 : 0.46218156814575195\n",
      "Training loss for batch 6648 : 0.18236839771270752\n",
      "Training loss for batch 6649 : 0.02691582590341568\n",
      "Training loss for batch 6650 : 0.05442875623703003\n",
      "Training loss for batch 6651 : 0.026368986815214157\n",
      "Training loss for batch 6652 : 0.11285054683685303\n",
      "Training loss for batch 6653 : 0.4791981279850006\n",
      "Training loss for batch 6654 : 0.205428346991539\n",
      "Training loss for batch 6655 : 0.46531957387924194\n",
      "Training loss for batch 6656 : 0.01344059407711029\n",
      "Training loss for batch 6657 : -0.00012346768926363438\n",
      "Training loss for batch 6658 : 0.29430028796195984\n",
      "Training loss for batch 6659 : 0.8559275269508362\n",
      "Training loss for batch 6660 : 0.17814278602600098\n",
      "Training loss for batch 6661 : 0.5290791392326355\n",
      "Training loss for batch 6662 : 0.15609891712665558\n",
      "Training loss for batch 6663 : 0.3849618136882782\n",
      "Training loss for batch 6664 : 0.002831149147823453\n",
      "Training loss for batch 6665 : 0.11693413555622101\n",
      "Training loss for batch 6666 : 0.2503577768802643\n",
      "Training loss for batch 6667 : 0.05657590553164482\n",
      "Training loss for batch 6668 : 0.3176096975803375\n",
      "Training loss for batch 6669 : 0.1424923986196518\n",
      "Training loss for batch 6670 : 0.3182550072669983\n",
      "Training loss for batch 6671 : 0.10999452322721481\n",
      "Training loss for batch 6672 : 0.014118696562945843\n",
      "Training loss for batch 6673 : 0.17328713834285736\n",
      "Training loss for batch 6674 : 0.03022414818406105\n",
      "Training loss for batch 6675 : 0.16097813844680786\n",
      "Training loss for batch 6676 : 0.1548832803964615\n",
      "Training loss for batch 6677 : 0.018015511333942413\n",
      "Training loss for batch 6678 : 0.09369226545095444\n",
      "Training loss for batch 6679 : 0.13542920351028442\n",
      "Training loss for batch 6680 : 0.043860819190740585\n",
      "Training loss for batch 6681 : 0.127300426363945\n",
      "Training loss for batch 6682 : 0.2876308262348175\n",
      "Training loss for batch 6683 : 0.16195735335350037\n",
      "Training loss for batch 6684 : 0.0074656009674072266\n",
      "Training loss for batch 6685 : 0.3546137511730194\n",
      "Training loss for batch 6686 : 0.11321854591369629\n",
      "Training loss for batch 6687 : 0.017252733930945396\n",
      "Training loss for batch 6688 : 0.10673228651285172\n",
      "Training loss for batch 6689 : 0.0045912861824035645\n",
      "Training loss for batch 6690 : 0.0\n",
      "Training loss for batch 6691 : 0.14319974184036255\n",
      "Training loss for batch 6692 : 0.2448452264070511\n",
      "Training loss for batch 6693 : 0.020458152517676353\n",
      "Training loss for batch 6694 : 0.2920627295970917\n",
      "Training loss for batch 6695 : 0.029886934906244278\n",
      "Training loss for batch 6696 : 0.04164484888315201\n",
      "Training loss for batch 6697 : 0.2258356809616089\n",
      "Training loss for batch 6698 : 0.3329160511493683\n",
      "Training loss for batch 6699 : 0.03798075020313263\n",
      "Training loss for batch 6700 : 0.0365382581949234\n",
      "Training loss for batch 6701 : 0.11238991469144821\n",
      "Training loss for batch 6702 : 0.6203495860099792\n",
      "Training loss for batch 6703 : 0.2743561267852783\n",
      "Training loss for batch 6704 : 0.0\n",
      "Training loss for batch 6705 : 0.25892162322998047\n",
      "Training loss for batch 6706 : 0.0060244351625442505\n",
      "Training loss for batch 6707 : 0.22486403584480286\n",
      "Training loss for batch 6708 : 0.05275600776076317\n",
      "Training loss for batch 6709 : 0.29671961069107056\n",
      "Training loss for batch 6710 : 0.13353540003299713\n",
      "Training loss for batch 6711 : 0.06743036210536957\n",
      "Training loss for batch 6712 : 0.06901327520608902\n",
      "Training loss for batch 6713 : 0.43160462379455566\n",
      "Training loss for batch 6714 : 0.0\n",
      "Training loss for batch 6715 : 0.8496612906455994\n",
      "Training loss for batch 6716 : 0.3141595721244812\n",
      "Training loss for batch 6717 : 0.3204885721206665\n",
      "Training loss for batch 6718 : 0.27130651473999023\n",
      "Training loss for batch 6719 : 0.08064866811037064\n",
      "Training loss for batch 6720 : 0.03311090171337128\n",
      "Training loss for batch 6721 : 0.22334980964660645\n",
      "Training loss for batch 6722 : 0.245137020945549\n",
      "Training loss for batch 6723 : 0.1691279411315918\n",
      "Training loss for batch 6724 : 0.3556598722934723\n",
      "Training loss for batch 6725 : 0.10963423550128937\n",
      "Training loss for batch 6726 : 0.011386295780539513\n",
      "Training loss for batch 6727 : 0.21030741930007935\n",
      "Training loss for batch 6728 : 0.4025627672672272\n",
      "Training loss for batch 6729 : 0.32741114497184753\n",
      "Training loss for batch 6730 : 0.1426425278186798\n",
      "Training loss for batch 6731 : 0.2617299258708954\n",
      "Training loss for batch 6732 : 0.4027145504951477\n",
      "Training loss for batch 6733 : 0.16862604022026062\n",
      "Training loss for batch 6734 : 0.0\n",
      "Training loss for batch 6735 : 0.20085850358009338\n",
      "Training loss for batch 6736 : 0.09173646569252014\n",
      "Training loss for batch 6737 : 0.1444200575351715\n",
      "Training loss for batch 6738 : 0.0\n",
      "Training loss for batch 6739 : 0.2992531955242157\n",
      "Training loss for batch 6740 : 0.35211995244026184\n",
      "Training loss for batch 6741 : 0.2514868378639221\n",
      "Training loss for batch 6742 : 0.21700502932071686\n",
      "Training loss for batch 6743 : 0.08648723363876343\n",
      "Training loss for batch 6744 : 0.037090055644512177\n",
      "Training loss for batch 6745 : 0.16101409494876862\n",
      "Training loss for batch 6746 : 0.5318083167076111\n",
      "Training loss for batch 6747 : 0.1598183661699295\n",
      "Training loss for batch 6748 : 0.2934624254703522\n",
      "Training loss for batch 6749 : 0.3143739104270935\n",
      "Training loss for batch 6750 : 0.42775848507881165\n",
      "Training loss for batch 6751 : 0.247406467795372\n",
      "Training loss for batch 6752 : 0.4524048864841461\n",
      "Training loss for batch 6753 : 0.15874452888965607\n",
      "Training loss for batch 6754 : 0.46003758907318115\n",
      "Training loss for batch 6755 : 0.2697092890739441\n",
      "Training loss for batch 6756 : 0.1344180405139923\n",
      "Training loss for batch 6757 : 0.37566980719566345\n",
      "Training loss for batch 6758 : 0.14849753677845\n",
      "Training loss for batch 6759 : 0.20212648808956146\n",
      "Training loss for batch 6760 : 0.27077746391296387\n",
      "Training loss for batch 6761 : 0.04088252782821655\n",
      "Training loss for batch 6762 : 0.1517329066991806\n",
      "Training loss for batch 6763 : 0.030479907989501953\n",
      "Training loss for batch 6764 : 0.306544691324234\n",
      "Training loss for batch 6765 : 0.29657283425331116\n",
      "Training loss for batch 6766 : 0.22871404886245728\n",
      "Training loss for batch 6767 : 0.12362803518772125\n",
      "Training loss for batch 6768 : 0.03949487581849098\n",
      "Training loss for batch 6769 : 0.0\n",
      "Training loss for batch 6770 : 0.6011232137680054\n",
      "Training loss for batch 6771 : 0.18844908475875854\n",
      "Training loss for batch 6772 : 0.07231291383504868\n",
      "Training loss for batch 6773 : 0.5521233677864075\n",
      "Training loss for batch 6774 : 0.027097702026367188\n",
      "Training loss for batch 6775 : 0.03261682763695717\n",
      "Training loss for batch 6776 : 0.3568187654018402\n",
      "Training loss for batch 6777 : 0.2884874641895294\n",
      "Training loss for batch 6778 : 0.031974609941244125\n",
      "Training loss for batch 6779 : 0.12027928978204727\n",
      "Training loss for batch 6780 : 0.23677000403404236\n",
      "Training loss for batch 6781 : 0.05554993078112602\n",
      "Training loss for batch 6782 : 0.15813228487968445\n",
      "Training loss for batch 6783 : 0.2012854516506195\n",
      "Training loss for batch 6784 : 0.049343109130859375\n",
      "Training loss for batch 6785 : 0.17703336477279663\n",
      "Training loss for batch 6786 : 0.23602238297462463\n",
      "Training loss for batch 6787 : 0.11567032337188721\n",
      "Training loss for batch 6788 : 0.10066064447164536\n",
      "Training loss for batch 6789 : 0.2305278331041336\n",
      "Training loss for batch 6790 : 0.20092716813087463\n",
      "Training loss for batch 6791 : 0.4490370750427246\n",
      "Training loss for batch 6792 : 0.1417892575263977\n",
      "Training loss for batch 6793 : 0.35248327255249023\n",
      "Training loss for batch 6794 : 0.2424248903989792\n",
      "Training loss for batch 6795 : 0.2126457691192627\n",
      "Training loss for batch 6796 : 0.3355332911014557\n",
      "Training loss for batch 6797 : 0.3581734895706177\n",
      "Training loss for batch 6798 : 0.37257421016693115\n",
      "Training loss for batch 6799 : 0.11671507358551025\n",
      "Training loss for batch 6800 : 0.21263693273067474\n",
      "Training loss for batch 6801 : 0.05452793836593628\n",
      "Training loss for batch 6802 : 0.27391818165779114\n",
      "Training loss for batch 6803 : 0.06271328777074814\n",
      "Training loss for batch 6804 : 0.19478820264339447\n",
      "Training loss for batch 6805 : 0.015018049627542496\n",
      "Training loss for batch 6806 : 0.18619170784950256\n",
      "Training loss for batch 6807 : 0.1616620570421219\n",
      "Training loss for batch 6808 : 0.22965529561042786\n",
      "Training loss for batch 6809 : 0.3454459309577942\n",
      "Training loss for batch 6810 : 0.44348934292793274\n",
      "Training loss for batch 6811 : 0.2442345768213272\n",
      "Training loss for batch 6812 : 0.11061958223581314\n",
      "Training loss for batch 6813 : 0.2636719346046448\n",
      "Training loss for batch 6814 : 0.021999409422278404\n",
      "Training loss for batch 6815 : 0.2952709496021271\n",
      "Training loss for batch 6816 : 0.17398057878017426\n",
      "Training loss for batch 6817 : 0.1814267337322235\n",
      "Training loss for batch 6818 : 0.5962367653846741\n",
      "Training loss for batch 6819 : 0.34650638699531555\n",
      "Training loss for batch 6820 : 0.059786513447761536\n",
      "Training loss for batch 6821 : 0.26082053780555725\n",
      "Training loss for batch 6822 : 0.3629787266254425\n",
      "Training loss for batch 6823 : 0.274457722902298\n",
      "Training loss for batch 6824 : 0.044943030923604965\n",
      "Training loss for batch 6825 : 0.11027532070875168\n",
      "Training loss for batch 6826 : 0.06955841183662415\n",
      "Training loss for batch 6827 : 0.1314178854227066\n",
      "Training loss for batch 6828 : 0.1732248216867447\n",
      "Training loss for batch 6829 : 0.03636835142970085\n",
      "Training loss for batch 6830 : 0.17575936019420624\n",
      "Training loss for batch 6831 : 0.2719264626502991\n",
      "Training loss for batch 6832 : 0.09392888098955154\n",
      "Training loss for batch 6833 : 0.07667040079832077\n",
      "Training loss for batch 6834 : 0.23050236701965332\n",
      "Training loss for batch 6835 : 0.43849050998687744\n",
      "Training loss for batch 6836 : 0.4274650514125824\n",
      "Training loss for batch 6837 : 0.15261290967464447\n",
      "Training loss for batch 6838 : 0.04577549919486046\n",
      "Training loss for batch 6839 : 0.36928874254226685\n",
      "Training loss for batch 6840 : 0.4367397725582123\n",
      "Training loss for batch 6841 : 0.1460179090499878\n",
      "Training loss for batch 6842 : 0.09667607396841049\n",
      "Training loss for batch 6843 : 0.1335568130016327\n",
      "Training loss for batch 6844 : 0.5233224034309387\n",
      "Training loss for batch 6845 : 0.008069908246397972\n",
      "Training loss for batch 6846 : 0.282659649848938\n",
      "Training loss for batch 6847 : 0.08731709420681\n",
      "Training loss for batch 6848 : 0.1763501763343811\n",
      "Training loss for batch 6849 : 0.17299258708953857\n",
      "Training loss for batch 6850 : 0.035415247082710266\n",
      "Training loss for batch 6851 : 0.21894800662994385\n",
      "Training loss for batch 6852 : 0.029623879119753838\n",
      "Training loss for batch 6853 : 0.0223418977111578\n",
      "Training loss for batch 6854 : 0.3671700358390808\n",
      "Training loss for batch 6855 : 0.32019370794296265\n",
      "Training loss for batch 6856 : 0.009509861469268799\n",
      "Training loss for batch 6857 : 0.45050281286239624\n",
      "Training loss for batch 6858 : 0.014874917455017567\n",
      "Training loss for batch 6859 : 0.02551404759287834\n",
      "Training loss for batch 6860 : 0.38670918345451355\n",
      "Training loss for batch 6861 : 0.4062556326389313\n",
      "Training loss for batch 6862 : 0.38080090284347534\n",
      "Training loss for batch 6863 : 0.18222357332706451\n",
      "Training loss for batch 6864 : 0.3385119140148163\n",
      "Training loss for batch 6865 : 0.14589093625545502\n",
      "Training loss for batch 6866 : 0.1120375320315361\n",
      "Training loss for batch 6867 : 0.1165388822555542\n",
      "Training loss for batch 6868 : 0.10631150007247925\n",
      "Training loss for batch 6869 : 0.1683715432882309\n",
      "Training loss for batch 6870 : 0.26395219564437866\n",
      "Training loss for batch 6871 : 0.2465154081583023\n",
      "Training loss for batch 6872 : 0.19525383412837982\n",
      "Training loss for batch 6873 : -0.00015043126768432558\n",
      "Training loss for batch 6874 : 0.1412242352962494\n",
      "Training loss for batch 6875 : 0.32916581630706787\n",
      "Training loss for batch 6876 : 0.03238873928785324\n",
      "Training loss for batch 6877 : 0.02282704971730709\n",
      "Training loss for batch 6878 : 0.39608079195022583\n",
      "Training loss for batch 6879 : 0.27407681941986084\n",
      "Training loss for batch 6880 : 0.22346098721027374\n",
      "Training loss for batch 6881 : 0.43993547558784485\n",
      "Training loss for batch 6882 : 0.18523453176021576\n",
      "Training loss for batch 6883 : 0.27019423246383667\n",
      "Training loss for batch 6884 : 0.6789303421974182\n",
      "Training loss for batch 6885 : 0.20670633018016815\n",
      "Training loss for batch 6886 : 0.33125704526901245\n",
      "Training loss for batch 6887 : 0.019606774672865868\n",
      "Training loss for batch 6888 : 0.1705814003944397\n",
      "Training loss for batch 6889 : 0.11176455765962601\n",
      "Training loss for batch 6890 : 0.16512353718280792\n",
      "Training loss for batch 6891 : 0.023500623181462288\n",
      "Training loss for batch 6892 : 0.2803766131401062\n",
      "Training loss for batch 6893 : 0.17390593886375427\n",
      "Training loss for batch 6894 : 0.7822651267051697\n",
      "Training loss for batch 6895 : 0.230158269405365\n",
      "Training loss for batch 6896 : 0.05163247510790825\n",
      "Training loss for batch 6897 : 0.07970874011516571\n",
      "Training loss for batch 6898 : 0.37838101387023926\n",
      "Training loss for batch 6899 : 0.02029515616595745\n",
      "Training loss for batch 6900 : 0.0936606377363205\n",
      "Training loss for batch 6901 : 0.8396024107933044\n",
      "Training loss for batch 6902 : 0.09820525348186493\n",
      "Training loss for batch 6903 : 0.13117465376853943\n",
      "Training loss for batch 6904 : 0.34425950050354004\n",
      "Training loss for batch 6905 : 0.22913511097431183\n",
      "Training loss for batch 6906 : 0.0013749649515375495\n",
      "Training loss for batch 6907 : 0.0658855065703392\n",
      "Training loss for batch 6908 : 0.4907057285308838\n",
      "Training loss for batch 6909 : 0.20189261436462402\n",
      "Training loss for batch 6910 : 0.43737560510635376\n",
      "Training loss for batch 6911 : 0.4324219226837158\n",
      "Training loss for batch 6912 : 0.06373345851898193\n",
      "Training loss for batch 6913 : 0.08074367046356201\n",
      "Training loss for batch 6914 : 0.18459288775920868\n",
      "Training loss for batch 6915 : 0.10947897285223007\n",
      "Training loss for batch 6916 : 0.19520212709903717\n",
      "Training loss for batch 6917 : 0.1890363246202469\n",
      "Training loss for batch 6918 : 0.42484140396118164\n",
      "Training loss for batch 6919 : 0.13299107551574707\n",
      "Training loss for batch 6920 : 0.36705392599105835\n",
      "Training loss for batch 6921 : 0.5834825038909912\n",
      "Training loss for batch 6922 : 0.02120226062834263\n",
      "Training loss for batch 6923 : 0.17287695407867432\n",
      "Training loss for batch 6924 : 0.017562180757522583\n",
      "Training loss for batch 6925 : 0.1049618348479271\n",
      "Training loss for batch 6926 : 0.1598946750164032\n",
      "Training loss for batch 6927 : 0.037441905587911606\n",
      "Training loss for batch 6928 : 0.5703442692756653\n",
      "Training loss for batch 6929 : 0.29732683300971985\n",
      "Training loss for batch 6930 : 0.08186379075050354\n",
      "Training loss for batch 6931 : 0.2493973672389984\n",
      "Training loss for batch 6932 : 0.0961441919207573\n",
      "Training loss for batch 6933 : 0.30431294441223145\n",
      "Training loss for batch 6934 : 0.14818766713142395\n",
      "Training loss for batch 6935 : 0.4785619080066681\n",
      "Training loss for batch 6936 : 0.15883663296699524\n",
      "Training loss for batch 6937 : 0.0033504695165902376\n",
      "Training loss for batch 6938 : 0.10221198201179504\n",
      "Training loss for batch 6939 : 0.2321777045726776\n",
      "Training loss for batch 6940 : 0.8876806497573853\n",
      "Training loss for batch 6941 : 0.03610874339938164\n",
      "Training loss for batch 6942 : 0.23037414252758026\n",
      "Training loss for batch 6943 : 0.2669202983379364\n",
      "Training loss for batch 6944 : 0.08572197705507278\n",
      "Training loss for batch 6945 : 0.16722829639911652\n",
      "Training loss for batch 6946 : 0.3595017194747925\n",
      "Training loss for batch 6947 : 0.37630876898765564\n",
      "Training loss for batch 6948 : 0.11308590322732925\n",
      "Training loss for batch 6949 : 0.1697426736354828\n",
      "Training loss for batch 6950 : 0.0\n",
      "Training loss for batch 6951 : 0.18379993736743927\n",
      "Training loss for batch 6952 : 0.11741594970226288\n",
      "Training loss for batch 6953 : 0.13342025876045227\n",
      "Training loss for batch 6954 : 0.028898324817419052\n",
      "Training loss for batch 6955 : 0.07720176130533218\n",
      "Training loss for batch 6956 : 0.05529072508215904\n",
      "Training loss for batch 6957 : 0.024580642580986023\n",
      "Training loss for batch 6958 : 0.3274261951446533\n",
      "Training loss for batch 6959 : 0.6323709487915039\n",
      "Training loss for batch 6960 : 0.029774339869618416\n",
      "Training loss for batch 6961 : 0.2089381366968155\n",
      "Training loss for batch 6962 : 0.2537732422351837\n",
      "Training loss for batch 6963 : 0.29950156807899475\n",
      "Training loss for batch 6964 : 0.20878365635871887\n",
      "Training loss for batch 6965 : 0.5469138026237488\n",
      "Training loss for batch 6966 : 0.2508638799190521\n",
      "Training loss for batch 6967 : 0.37703248858451843\n",
      "Training loss for batch 6968 : 0.0034432709217071533\n",
      "Training loss for batch 6969 : 0.20394429564476013\n",
      "Training loss for batch 6970 : 0.06087726727128029\n",
      "Training loss for batch 6971 : 0.08583948016166687\n",
      "Training loss for batch 6972 : 0.2602889835834503\n",
      "Training loss for batch 6973 : 0.12450315058231354\n",
      "Training loss for batch 6974 : 0.27207833528518677\n",
      "Training loss for batch 6975 : 0.06495632231235504\n",
      "Training loss for batch 6976 : 0.4173251986503601\n",
      "Training loss for batch 6977 : 0.5644227862358093\n",
      "Training loss for batch 6978 : 0.372742235660553\n",
      "Training loss for batch 6979 : 0.11290759593248367\n",
      "Training loss for batch 6980 : 0.15584056079387665\n",
      "Training loss for batch 6981 : 0.3013385534286499\n",
      "Training loss for batch 6982 : 0.5590186715126038\n",
      "Training loss for batch 6983 : 0.10093449801206589\n",
      "Training loss for batch 6984 : 0.1708010882139206\n",
      "Training loss for batch 6985 : 0.05524345859885216\n",
      "Training loss for batch 6986 : 0.2885657548904419\n",
      "Training loss for batch 6987 : 0.10865962505340576\n",
      "Training loss for batch 6988 : 0.3495699465274811\n",
      "Training loss for batch 6989 : 0.05421372503042221\n",
      "Training loss for batch 6990 : 0.4161946475505829\n",
      "Training loss for batch 6991 : 0.13272631168365479\n",
      "Training loss for batch 6992 : 0.30675753951072693\n",
      "Training loss for batch 6993 : 0.35230201482772827\n",
      "Training loss for batch 6994 : 0.2599901854991913\n",
      "Training loss for batch 6995 : 0.4215877652168274\n",
      "Training loss for batch 6996 : 0.11893530935049057\n",
      "Training loss for batch 6997 : 0.5227446556091309\n",
      "Training loss for batch 6998 : 0.01115734688937664\n",
      "Training loss for batch 6999 : 0.08375618606805801\n",
      "Training loss for batch 7000 : 0.13307294249534607\n",
      "Training loss for batch 7001 : 0.04399121180176735\n",
      "Training loss for batch 7002 : 0.12766724824905396\n",
      "Training loss for batch 7003 : 0.3941972255706787\n",
      "Training loss for batch 7004 : 0.12872929871082306\n",
      "Training loss for batch 7005 : 0.22875696420669556\n",
      "Training loss for batch 7006 : 0.08668647706508636\n",
      "Training loss for batch 7007 : 0.25500378012657166\n",
      "Training loss for batch 7008 : 0.20415864884853363\n",
      "Training loss for batch 7009 : 0.0\n",
      "Training loss for batch 7010 : 0.9054084420204163\n",
      "Training loss for batch 7011 : 0.032373737543821335\n",
      "Training loss for batch 7012 : 0.19855907559394836\n",
      "Training loss for batch 7013 : 0.1060144305229187\n",
      "Training loss for batch 7014 : 0.20803117752075195\n",
      "Training loss for batch 7015 : 0.38628464937210083\n",
      "Training loss for batch 7016 : 0.35774046182632446\n",
      "Training loss for batch 7017 : 0.14099657535552979\n",
      "Training loss for batch 7018 : 0.18773838877677917\n",
      "Training loss for batch 7019 : 0.25277724862098694\n",
      "Training loss for batch 7020 : 0.3459179401397705\n",
      "Training loss for batch 7021 : 0.1387169510126114\n",
      "Training loss for batch 7022 : 0.2464209645986557\n",
      "Training loss for batch 7023 : 0.6481531858444214\n",
      "Training loss for batch 7024 : 0.09002957493066788\n",
      "Training loss for batch 7025 : 0.2884281277656555\n",
      "Training loss for batch 7026 : 0.2053077518939972\n",
      "Training loss for batch 7027 : 0.016991162672638893\n",
      "Training loss for batch 7028 : 0.2546536326408386\n",
      "Training loss for batch 7029 : 0.23106350004673004\n",
      "Training loss for batch 7030 : 0.1047297939658165\n",
      "Training loss for batch 7031 : 0.3747013509273529\n",
      "Training loss for batch 7032 : 0.14538291096687317\n",
      "Training loss for batch 7033 : 0.1621355265378952\n",
      "Training loss for batch 7034 : 0.065750353038311\n",
      "Training loss for batch 7035 : 0.12624050676822662\n",
      "Training loss for batch 7036 : 0.5066515207290649\n",
      "Training loss for batch 7037 : 0.41587597131729126\n",
      "Training loss for batch 7038 : 0.17398150265216827\n",
      "Training loss for batch 7039 : 0.3887650966644287\n",
      "Training loss for batch 7040 : 0.18841353058815002\n",
      "Training loss for batch 7041 : 0.024330710992217064\n",
      "Training loss for batch 7042 : 0.474287748336792\n",
      "Training loss for batch 7043 : 0.22217196226119995\n",
      "Training loss for batch 7044 : 0.12916535139083862\n",
      "Training loss for batch 7045 : 0.08553534746170044\n",
      "Training loss for batch 7046 : 0.21092060208320618\n",
      "Training loss for batch 7047 : 0.19297927618026733\n",
      "Training loss for batch 7048 : 0.40842780470848083\n",
      "Training loss for batch 7049 : 0.30073437094688416\n",
      "Training loss for batch 7050 : 0.38977542519569397\n",
      "Training loss for batch 7051 : 0.18335501849651337\n",
      "Training loss for batch 7052 : 0.2349463850259781\n",
      "Training loss for batch 7053 : 0.24346157908439636\n",
      "Training loss for batch 7054 : 0.26126036047935486\n",
      "Training loss for batch 7055 : 0.34157735109329224\n",
      "Training loss for batch 7056 : 0.09415490180253983\n",
      "Training loss for batch 7057 : 0.10974519699811935\n",
      "Training loss for batch 7058 : 0.08593697845935822\n",
      "Training loss for batch 7059 : 0.08935829252004623\n",
      "Training loss for batch 7060 : 0.36372873187065125\n",
      "Training loss for batch 7061 : 0.05260800942778587\n",
      "Training loss for batch 7062 : 0.04661673307418823\n",
      "Training loss for batch 7063 : 0.059192758053541183\n",
      "Training loss for batch 7064 : 0.01326949242502451\n",
      "Training loss for batch 7065 : 0.029513103887438774\n",
      "Training loss for batch 7066 : 0.02584412693977356\n",
      "Training loss for batch 7067 : 0.021725280210375786\n",
      "Training loss for batch 7068 : 0.04450654610991478\n",
      "Training loss for batch 7069 : 0.08585848659276962\n",
      "Training loss for batch 7070 : 0.27475258708000183\n",
      "Training loss for batch 7071 : 0.02479608729481697\n",
      "Training loss for batch 7072 : 0.45462557673454285\n",
      "Training loss for batch 7073 : 0.00851907767355442\n",
      "Training loss for batch 7074 : 0.14012999832630157\n",
      "Training loss for batch 7075 : 0.1450742930173874\n",
      "Training loss for batch 7076 : 0.0913817435503006\n",
      "Training loss for batch 7077 : 0.19557160139083862\n",
      "Training loss for batch 7078 : 0.15532349050045013\n",
      "Training loss for batch 7079 : 0.01971135288476944\n",
      "Training loss for batch 7080 : 0.24210867285728455\n",
      "Training loss for batch 7081 : 0.05523001030087471\n",
      "Training loss for batch 7082 : 0.2104068249464035\n",
      "Training loss for batch 7083 : 0.3665255010128021\n",
      "Training loss for batch 7084 : 0.00015295545745175332\n",
      "Training loss for batch 7085 : 0.1452731341123581\n",
      "Training loss for batch 7086 : 0.045379020273685455\n",
      "Training loss for batch 7087 : 0.22795535624027252\n",
      "Training loss for batch 7088 : 0.04497583955526352\n",
      "Training loss for batch 7089 : 0.04801957681775093\n",
      "Training loss for batch 7090 : 0.17798995971679688\n",
      "Training loss for batch 7091 : 0.14439447224140167\n",
      "Training loss for batch 7092 : 0.4155595302581787\n",
      "Training loss for batch 7093 : 0.048708461225032806\n",
      "Training loss for batch 7094 : 0.3239160478115082\n",
      "Training loss for batch 7095 : 0.12796802818775177\n",
      "Training loss for batch 7096 : 0.03960277512669563\n",
      "Training loss for batch 7097 : 0.0\n",
      "Training loss for batch 7098 : 0.2265908420085907\n",
      "Training loss for batch 7099 : 0.23752938210964203\n",
      "Training loss for batch 7100 : 0.06393106281757355\n",
      "Training loss for batch 7101 : 0.0874921977519989\n",
      "Training loss for batch 7102 : 0.3990509510040283\n",
      "Training loss for batch 7103 : 0.04750223085284233\n",
      "Training loss for batch 7104 : 0.13986551761627197\n",
      "Training loss for batch 7105 : 0.2779946029186249\n",
      "Training loss for batch 7106 : 0.2947615087032318\n",
      "Training loss for batch 7107 : 0.16102440655231476\n",
      "Training loss for batch 7108 : 0.3690130114555359\n",
      "Training loss for batch 7109 : 0.1666475236415863\n",
      "Training loss for batch 7110 : 0.3944280743598938\n",
      "Training loss for batch 7111 : 0.1701718419790268\n",
      "Training loss for batch 7112 : 0.27337080240249634\n",
      "Training loss for batch 7113 : 0.19335433840751648\n",
      "Training loss for batch 7114 : 0.12632617354393005\n",
      "Training loss for batch 7115 : 0.13913939893245697\n",
      "Training loss for batch 7116 : 0.05877402052283287\n",
      "Training loss for batch 7117 : 0.1738039255142212\n",
      "Training loss for batch 7118 : 0.027507450431585312\n",
      "Training loss for batch 7119 : 0.38901448249816895\n",
      "Training loss for batch 7120 : 0.03107081726193428\n",
      "Training loss for batch 7121 : 0.5281004309654236\n",
      "Training loss for batch 7122 : 0.4841561019420624\n",
      "Training loss for batch 7123 : 0.09991644322872162\n",
      "Training loss for batch 7124 : 0.05887194722890854\n",
      "Training loss for batch 7125 : 0.04842311516404152\n",
      "Training loss for batch 7126 : 0.19652482867240906\n",
      "Training loss for batch 7127 : 0.09986847639083862\n",
      "Training loss for batch 7128 : 0.15822239220142365\n",
      "Training loss for batch 7129 : 0.02700936421751976\n",
      "Training loss for batch 7130 : 0.1365247517824173\n",
      "Training loss for batch 7131 : 0.2140914797782898\n",
      "Training loss for batch 7132 : 0.34881871938705444\n",
      "Training loss for batch 7133 : 0.6389725804328918\n",
      "Training loss for batch 7134 : 0.07483495771884918\n",
      "Training loss for batch 7135 : 0.476349413394928\n",
      "Training loss for batch 7136 : 0.06901602447032928\n",
      "Training loss for batch 7137 : 0.1995546668767929\n",
      "Training loss for batch 7138 : 0.1995510309934616\n",
      "Training loss for batch 7139 : 0.1009485051035881\n",
      "Training loss for batch 7140 : 0.18665781617164612\n",
      "Training loss for batch 7141 : 0.3607623279094696\n",
      "Training loss for batch 7142 : 0.009424639865756035\n",
      "Training loss for batch 7143 : 0.2716878354549408\n",
      "Training loss for batch 7144 : 0.06295057386159897\n",
      "Training loss for batch 7145 : 0.2083675116300583\n",
      "Training loss for batch 7146 : 0.04622827097773552\n",
      "Training loss for batch 7147 : 0.6715391278266907\n",
      "Training loss for batch 7148 : 0.2659686505794525\n",
      "Training loss for batch 7149 : 0.22497521340847015\n",
      "Training loss for batch 7150 : 0.007955601438879967\n",
      "Training loss for batch 7151 : 0.07735355943441391\n",
      "Training loss for batch 7152 : 0.21851351857185364\n",
      "Training loss for batch 7153 : 0.23190061748027802\n",
      "Training loss for batch 7154 : 0.09860765188932419\n",
      "Training loss for batch 7155 : 0.007480292581021786\n",
      "Training loss for batch 7156 : 0.04804905503988266\n",
      "Training loss for batch 7157 : 0.29335278272628784\n",
      "Training loss for batch 7158 : 0.2142907679080963\n",
      "Training loss for batch 7159 : 0.11645650118589401\n",
      "Training loss for batch 7160 : 0.20023201406002045\n",
      "Training loss for batch 7161 : 0.06640379875898361\n",
      "Training loss for batch 7162 : 0.00048073133802972734\n",
      "Training loss for batch 7163 : 0.1625545769929886\n",
      "Training loss for batch 7164 : 0.08207497745752335\n",
      "Training loss for batch 7165 : 0.2270990014076233\n",
      "Training loss for batch 7166 : 0.220748633146286\n",
      "Training loss for batch 7167 : 0.040103085339069366\n",
      "Training loss for batch 7168 : 0.15577317774295807\n",
      "Training loss for batch 7169 : 0.043367426842451096\n",
      "Training loss for batch 7170 : 0.06811220943927765\n",
      "Training loss for batch 7171 : 0.06639063358306885\n",
      "Training loss for batch 7172 : 0.1971428096294403\n",
      "Training loss for batch 7173 : 0.29120346903800964\n",
      "Training loss for batch 7174 : 0.385231614112854\n",
      "Training loss for batch 7175 : 0.20262973010540009\n",
      "Training loss for batch 7176 : 0.09700214117765427\n",
      "Training loss for batch 7177 : 0.14244268834590912\n",
      "Training loss for batch 7178 : 0.12956051528453827\n",
      "Training loss for batch 7179 : 0.15674063563346863\n",
      "Training loss for batch 7180 : 0.01635577343404293\n",
      "Training loss for batch 7181 : 0.09534169733524323\n",
      "Training loss for batch 7182 : 0.49916884303092957\n",
      "Training loss for batch 7183 : 0.21551905572414398\n",
      "Training loss for batch 7184 : 0.1353214681148529\n",
      "Training loss for batch 7185 : 0.07971728593111038\n",
      "Training loss for batch 7186 : 0.208462193608284\n",
      "Training loss for batch 7187 : 0.0346536822617054\n",
      "Training loss for batch 7188 : 0.30450189113616943\n",
      "Training loss for batch 7189 : 0.07432752847671509\n",
      "Training loss for batch 7190 : 0.04889768362045288\n",
      "Training loss for batch 7191 : 0.0\n",
      "Training loss for batch 7192 : 0.2638948857784271\n",
      "Training loss for batch 7193 : 0.24130485951900482\n",
      "Training loss for batch 7194 : 0.21417789161205292\n",
      "Training loss for batch 7195 : 0.039363786578178406\n",
      "Training loss for batch 7196 : 0.12516167759895325\n",
      "Training loss for batch 7197 : 0.6050450801849365\n",
      "Training loss for batch 7198 : -9.329201566288248e-05\n",
      "Training loss for batch 7199 : 0.0378396175801754\n",
      "Training loss for batch 7200 : 0.1517789512872696\n",
      "Training loss for batch 7201 : 0.7078814506530762\n",
      "Training loss for batch 7202 : 0.021902551874518394\n",
      "Training loss for batch 7203 : 0.03804492950439453\n",
      "Training loss for batch 7204 : 0.25123387575149536\n",
      "Training loss for batch 7205 : 0.1893482357263565\n",
      "Training loss for batch 7206 : 0.054565634578466415\n",
      "Training loss for batch 7207 : 0.021629709750413895\n",
      "Training loss for batch 7208 : 0.03997164964675903\n",
      "Training loss for batch 7209 : 0.24262818694114685\n",
      "Training loss for batch 7210 : 0.15255378186702728\n",
      "Training loss for batch 7211 : 0.11571943014860153\n",
      "Training loss for batch 7212 : 0.11312410235404968\n",
      "Training loss for batch 7213 : 0.29042986035346985\n",
      "Training loss for batch 7214 : 0.10741455852985382\n",
      "Training loss for batch 7215 : 0.11148538440465927\n",
      "Training loss for batch 7216 : 0.10639677196741104\n",
      "Training loss for batch 7217 : 0.053839053958654404\n",
      "Training loss for batch 7218 : 0.16161049902439117\n",
      "Training loss for batch 7219 : 0.37529289722442627\n",
      "Training loss for batch 7220 : 0.038333166390657425\n",
      "Training loss for batch 7221 : 0.30497056245803833\n",
      "Training loss for batch 7222 : 0.2371038943529129\n",
      "Training loss for batch 7223 : 0.07738956809043884\n",
      "Training loss for batch 7224 : 0.33781373500823975\n",
      "Training loss for batch 7225 : 0.2897489368915558\n",
      "Training loss for batch 7226 : 0.1932886242866516\n",
      "Training loss for batch 7227 : 0.16488215327262878\n",
      "Training loss for batch 7228 : 0.3346192240715027\n",
      "Training loss for batch 7229 : 0.1766212284564972\n",
      "Training loss for batch 7230 : 0.0805768072605133\n",
      "Training loss for batch 7231 : 0.024297326803207397\n",
      "Training loss for batch 7232 : 0.44196754693984985\n",
      "Training loss for batch 7233 : 0.0775875672698021\n",
      "Training loss for batch 7234 : 0.4720366597175598\n",
      "Training loss for batch 7235 : 0.4806760847568512\n",
      "Training loss for batch 7236 : 0.06564031541347504\n",
      "Training loss for batch 7237 : 0.11950081586837769\n",
      "Training loss for batch 7238 : 0.1686670184135437\n",
      "Training loss for batch 7239 : 0.012487553060054779\n",
      "Training loss for batch 7240 : 0.22269399464130402\n",
      "Training loss for batch 7241 : 0.2984376847743988\n",
      "Training loss for batch 7242 : 0.07195276021957397\n",
      "Training loss for batch 7243 : 0.4137536287307739\n",
      "Training loss for batch 7244 : 0.14349251985549927\n",
      "Training loss for batch 7245 : 0.11995647102594376\n",
      "Training loss for batch 7246 : 0.027621781453490257\n",
      "Training loss for batch 7247 : 0.2823238670825958\n",
      "Training loss for batch 7248 : 0.13300581276416779\n",
      "Training loss for batch 7249 : 0.3189627230167389\n",
      "Training loss for batch 7250 : 0.04293855279684067\n",
      "Training loss for batch 7251 : 0.03127995878458023\n",
      "Training loss for batch 7252 : 0.23986005783081055\n",
      "Training loss for batch 7253 : 0.07241766899824142\n",
      "Training loss for batch 7254 : 0.37206777930259705\n",
      "Training loss for batch 7255 : 0.0819777324795723\n",
      "Training loss for batch 7256 : 0.0\n",
      "Training loss for batch 7257 : 0.09563848376274109\n",
      "Training loss for batch 7258 : 0.09049205482006073\n",
      "Training loss for batch 7259 : 0.08342277258634567\n",
      "Training loss for batch 7260 : 0.13948680460453033\n",
      "Training loss for batch 7261 : 0.15503434836864471\n",
      "Training loss for batch 7262 : 0.18096868693828583\n",
      "Training loss for batch 7263 : 0.1274937391281128\n",
      "Training loss for batch 7264 : 0.5928884744644165\n",
      "Training loss for batch 7265 : 0.005683586001396179\n",
      "Training loss for batch 7266 : 0.25900572538375854\n",
      "Training loss for batch 7267 : 0.46693316102027893\n",
      "Training loss for batch 7268 : 0.004452357068657875\n",
      "Training loss for batch 7269 : 0.0360293947160244\n",
      "Training loss for batch 7270 : 0.02845502458512783\n",
      "Training loss for batch 7271 : 0.3049188256263733\n",
      "Training loss for batch 7272 : 0.06378848850727081\n",
      "Training loss for batch 7273 : 0.12975165247917175\n",
      "Training loss for batch 7274 : 0.06687920540571213\n",
      "Training loss for batch 7275 : 0.023863190785050392\n",
      "Training loss for batch 7276 : 0.0023830931168049574\n",
      "Training loss for batch 7277 : 0.01352718099951744\n",
      "Training loss for batch 7278 : 0.20406563580036163\n",
      "Training loss for batch 7279 : 0.4239278733730316\n",
      "Training loss for batch 7280 : 0.39172273874282837\n",
      "Training loss for batch 7281 : 0.05546480417251587\n",
      "Training loss for batch 7282 : 0.2980740964412689\n",
      "Training loss for batch 7283 : 0.06687425822019577\n",
      "Training loss for batch 7284 : 0.06148125231266022\n",
      "Training loss for batch 7285 : 0.14503151178359985\n",
      "Training loss for batch 7286 : 0.09295905381441116\n",
      "Training loss for batch 7287 : 0.38319748640060425\n",
      "Training loss for batch 7288 : 0.06079963594675064\n",
      "Training loss for batch 7289 : 0.1341986060142517\n",
      "Training loss for batch 7290 : 0.3401220738887787\n",
      "Training loss for batch 7291 : 0.5114216208457947\n",
      "Training loss for batch 7292 : 0.11649660766124725\n",
      "Training loss for batch 7293 : 0.03807130083441734\n",
      "Training loss for batch 7294 : 0.26469120383262634\n",
      "Training loss for batch 7295 : 0.2958470582962036\n",
      "Training loss for batch 7296 : 0.06074073910713196\n",
      "Training loss for batch 7297 : 0.4401927888393402\n",
      "Training loss for batch 7298 : 0.3282140791416168\n",
      "Training loss for batch 7299 : 0.010207153856754303\n",
      "Training loss for batch 7300 : 0.049671124666929245\n",
      "Training loss for batch 7301 : 0.009125173091888428\n",
      "Training loss for batch 7302 : 0.0293908528983593\n",
      "Training loss for batch 7303 : 0.21725603938102722\n",
      "Training loss for batch 7304 : 0.11784946918487549\n",
      "Training loss for batch 7305 : 0.05568921938538551\n",
      "Training loss for batch 7306 : 0.18220971524715424\n",
      "Training loss for batch 7307 : 0.03887808322906494\n",
      "Training loss for batch 7308 : 0.024648087099194527\n",
      "Training loss for batch 7309 : 0.010112900286912918\n",
      "Training loss for batch 7310 : 0.3397194445133209\n",
      "Training loss for batch 7311 : 0.04602075740695\n",
      "Training loss for batch 7312 : 0.16485778987407684\n",
      "Training loss for batch 7313 : 0.08915968239307404\n",
      "Training loss for batch 7314 : 0.02066976949572563\n",
      "Training loss for batch 7315 : 0.3826368451118469\n",
      "Training loss for batch 7316 : 0.011168019846081734\n",
      "Training loss for batch 7317 : 0.24074921011924744\n",
      "Training loss for batch 7318 : 0.05034559965133667\n",
      "Training loss for batch 7319 : 0.07207109034061432\n",
      "Training loss for batch 7320 : 0.15320003032684326\n",
      "Training loss for batch 7321 : 0.12036289274692535\n",
      "Training loss for batch 7322 : 0.09544722735881805\n",
      "Training loss for batch 7323 : 0.19920215010643005\n",
      "Training loss for batch 7324 : 0.49765968322753906\n",
      "Training loss for batch 7325 : 0.2951633930206299\n",
      "Training loss for batch 7326 : 0.415060430765152\n",
      "Training loss for batch 7327 : 0.14819802343845367\n",
      "Training loss for batch 7328 : 0.1593940109014511\n",
      "Training loss for batch 7329 : 0.07044761627912521\n",
      "Training loss for batch 7330 : 0.11333180218935013\n",
      "Training loss for batch 7331 : 0.3537428081035614\n",
      "Training loss for batch 7332 : 0.08303854614496231\n",
      "Training loss for batch 7333 : 0.0972900390625\n",
      "Training loss for batch 7334 : 0.5505799651145935\n",
      "Training loss for batch 7335 : 0.0849267765879631\n",
      "Training loss for batch 7336 : 0.08216911554336548\n",
      "Training loss for batch 7337 : 0.1953050047159195\n",
      "Training loss for batch 7338 : 0.09455085545778275\n",
      "Training loss for batch 7339 : 0.25486162304878235\n",
      "Training loss for batch 7340 : 0.11101457476615906\n",
      "Training loss for batch 7341 : 0.39209961891174316\n",
      "Training loss for batch 7342 : 0.12410172075033188\n",
      "Training loss for batch 7343 : 0.07636339962482452\n",
      "Training loss for batch 7344 : 0.3602237403392792\n",
      "Training loss for batch 7345 : 0.08852991461753845\n",
      "Training loss for batch 7346 : 0.2853222191333771\n",
      "Training loss for batch 7347 : 0.23376941680908203\n",
      "Training loss for batch 7348 : 0.017679020762443542\n",
      "Training loss for batch 7349 : 0.17530480027198792\n",
      "Training loss for batch 7350 : 0.38189321756362915\n",
      "Training loss for batch 7351 : 0.15258459746837616\n",
      "Training loss for batch 7352 : 0.18595094978809357\n",
      "Training loss for batch 7353 : 0.5179088115692139\n",
      "Training loss for batch 7354 : 0.04453294351696968\n",
      "Training loss for batch 7355 : 0.409302681684494\n",
      "Training loss for batch 7356 : 0.3551831543445587\n",
      "Training loss for batch 7357 : 0.16545794904232025\n",
      "Training loss for batch 7358 : 0.0059185028076171875\n",
      "Training loss for batch 7359 : 0.06210435926914215\n",
      "Training loss for batch 7360 : 0.10684224218130112\n",
      "Training loss for batch 7361 : 0.2634694576263428\n",
      "Training loss for batch 7362 : 0.08185520768165588\n",
      "Training loss for batch 7363 : 0.002820484573021531\n",
      "Training loss for batch 7364 : 0.20396919548511505\n",
      "Training loss for batch 7365 : 0.10978478938341141\n",
      "Training loss for batch 7366 : 0.014071719720959663\n",
      "Training loss for batch 7367 : 0.22883184254169464\n",
      "Training loss for batch 7368 : 0.40180566906929016\n",
      "Training loss for batch 7369 : 0.0012353956699371338\n",
      "Training loss for batch 7370 : 0.3416427671909332\n",
      "Training loss for batch 7371 : 0.04958011955022812\n",
      "Training loss for batch 7372 : 0.6224920153617859\n",
      "Training loss for batch 7373 : 0.02339503914117813\n",
      "Training loss for batch 7374 : 0.2691543996334076\n",
      "Training loss for batch 7375 : 0.11723427474498749\n",
      "Training loss for batch 7376 : 0.26166456937789917\n",
      "Training loss for batch 7377 : 0.1860831379890442\n",
      "Training loss for batch 7378 : 0.632722020149231\n",
      "Training loss for batch 7379 : 0.279374361038208\n",
      "Training loss for batch 7380 : 0.7538285851478577\n",
      "Training loss for batch 7381 : 0.22921091318130493\n",
      "Training loss for batch 7382 : 0.08731409162282944\n",
      "Training loss for batch 7383 : 0.1833198219537735\n",
      "Training loss for batch 7384 : 0.2549174129962921\n",
      "Training loss for batch 7385 : 0.01981908082962036\n",
      "Training loss for batch 7386 : 0.0016371409874409437\n",
      "Training loss for batch 7387 : 0.0523761510848999\n",
      "Training loss for batch 7388 : 0.4677741527557373\n",
      "Training loss for batch 7389 : 0.06276431679725647\n",
      "Training loss for batch 7390 : 0.1870240867137909\n",
      "Training loss for batch 7391 : 0.05980285257101059\n",
      "Training loss for batch 7392 : 0.21814565360546112\n",
      "Training loss for batch 7393 : 0.5069303512573242\n",
      "Training loss for batch 7394 : 0.3091830015182495\n",
      "Training loss for batch 7395 : 0.011091340333223343\n",
      "Training loss for batch 7396 : 0.3902297616004944\n",
      "Training loss for batch 7397 : 0.03270134702324867\n",
      "Training loss for batch 7398 : 0.0\n",
      "Training loss for batch 7399 : 0.013208459131419659\n",
      "Training loss for batch 7400 : 0.31738340854644775\n",
      "Training loss for batch 7401 : 0.04737266153097153\n",
      "Training loss for batch 7402 : 0.10991866141557693\n",
      "Training loss for batch 7403 : 0.0\n",
      "Training loss for batch 7404 : 0.200387641787529\n",
      "Training loss for batch 7405 : 0.14220662415027618\n",
      "Training loss for batch 7406 : 0.14293600618839264\n",
      "Training loss for batch 7407 : 0.0653877705335617\n",
      "Training loss for batch 7408 : 0.0014851185260340571\n",
      "Training loss for batch 7409 : 0.04128377139568329\n",
      "Training loss for batch 7410 : 0.10225128382444382\n",
      "Training loss for batch 7411 : 0.1321878284215927\n",
      "Training loss for batch 7412 : 0.25646090507507324\n",
      "Training loss for batch 7413 : 0.17099249362945557\n",
      "Training loss for batch 7414 : 0.1749480813741684\n",
      "Training loss for batch 7415 : 0.048765867948532104\n",
      "Training loss for batch 7416 : 0.1762998104095459\n",
      "Training loss for batch 7417 : 0.04755249619483948\n",
      "Training loss for batch 7418 : 0.00666278600692749\n",
      "Training loss for batch 7419 : 0.6576666235923767\n",
      "Training loss for batch 7420 : 0.22627434134483337\n",
      "Training loss for batch 7421 : 0.11039268225431442\n",
      "Training loss for batch 7422 : 0.3531782627105713\n",
      "Training loss for batch 7423 : 0.011341741308569908\n",
      "Training loss for batch 7424 : 0.14726339280605316\n",
      "Training loss for batch 7425 : 0.2570618689060211\n",
      "Training loss for batch 7426 : 0.5183228850364685\n",
      "Training loss for batch 7427 : 0.2495940923690796\n",
      "Training loss for batch 7428 : 0.7226456999778748\n",
      "Training loss for batch 7429 : 0.14876338839530945\n",
      "Training loss for batch 7430 : 0.21981947124004364\n",
      "Training loss for batch 7431 : 0.21649041771888733\n",
      "Training loss for batch 7432 : 0.14189323782920837\n",
      "Training loss for batch 7433 : 0.013978133909404278\n",
      "Training loss for batch 7434 : 0.05082772299647331\n",
      "Training loss for batch 7435 : 0.5384223461151123\n",
      "Training loss for batch 7436 : 0.026621341705322266\n",
      "Training loss for batch 7437 : 0.02138938009738922\n",
      "Training loss for batch 7438 : 0.07976595312356949\n",
      "Training loss for batch 7439 : 0.0666084960103035\n",
      "Training loss for batch 7440 : 0.2018355429172516\n",
      "Training loss for batch 7441 : 0.09874480217695236\n",
      "Training loss for batch 7442 : 0.0\n",
      "Training loss for batch 7443 : 0.15511853992938995\n",
      "Training loss for batch 7444 : 0.20966149866580963\n",
      "Training loss for batch 7445 : 0.027284596115350723\n",
      "Training loss for batch 7446 : 0.2274339646100998\n",
      "Training loss for batch 7447 : 0.2437255084514618\n",
      "Training loss for batch 7448 : 0.020842833444476128\n",
      "Training loss for batch 7449 : 0.25740206241607666\n",
      "Training loss for batch 7450 : 0.3798271119594574\n",
      "Training loss for batch 7451 : 0.2856682240962982\n",
      "Training loss for batch 7452 : 0.07568784058094025\n",
      "Training loss for batch 7453 : 0.4383467733860016\n",
      "Training loss for batch 7454 : 0.3557381331920624\n",
      "Training loss for batch 7455 : 0.18602769076824188\n",
      "Training loss for batch 7456 : 0.06632161140441895\n",
      "Training loss for batch 7457 : 0.3810373544692993\n",
      "Training loss for batch 7458 : 0.2267870157957077\n",
      "Training loss for batch 7459 : 0.1577012538909912\n",
      "Training loss for batch 7460 : 0.22359584271907806\n",
      "Training loss for batch 7461 : 0.033382222056388855\n",
      "Training loss for batch 7462 : 0.013221471570432186\n",
      "Training loss for batch 7463 : 0.4374690055847168\n",
      "Training loss for batch 7464 : 0.3812253177165985\n",
      "Training loss for batch 7465 : 0.06198453903198242\n",
      "Training loss for batch 7466 : 0.09662613272666931\n",
      "Training loss for batch 7467 : 0.15452100336551666\n",
      "Training loss for batch 7468 : 0.1800549477338791\n",
      "Training loss for batch 7469 : 0.04699718952178955\n",
      "Training loss for batch 7470 : 0.15549156069755554\n",
      "Training loss for batch 7471 : 0.2805245518684387\n",
      "Training loss for batch 7472 : 0.3050026595592499\n",
      "Training loss for batch 7473 : 0.20943742990493774\n",
      "Training loss for batch 7474 : 0.14422741532325745\n",
      "Training loss for batch 7475 : 0.03875122591853142\n",
      "Training loss for batch 7476 : 0.008853466250002384\n",
      "Training loss for batch 7477 : 0.13987167179584503\n",
      "Training loss for batch 7478 : 0.13517048954963684\n",
      "Training loss for batch 7479 : 0.07063926011323929\n",
      "Training loss for batch 7480 : 0.0019965795800089836\n",
      "Training loss for batch 7481 : 0.1800820529460907\n",
      "Training loss for batch 7482 : 0.2773580551147461\n",
      "Training loss for batch 7483 : 0.04268616810441017\n",
      "Training loss for batch 7484 : 0.15360699594020844\n",
      "Training loss for batch 7485 : 0.1559385061264038\n",
      "Training loss for batch 7486 : 0.051088396459817886\n",
      "Training loss for batch 7487 : 0.0\n",
      "Training loss for batch 7488 : 0.24450504779815674\n",
      "Training loss for batch 7489 : 0.13906101882457733\n",
      "Training loss for batch 7490 : 0.20011118054389954\n",
      "Training loss for batch 7491 : 0.15011659264564514\n",
      "Training loss for batch 7492 : 0.5477680563926697\n",
      "Training loss for batch 7493 : 0.1595306694507599\n",
      "Training loss for batch 7494 : 0.007773423567414284\n",
      "Training loss for batch 7495 : 0.043287161737680435\n",
      "Training loss for batch 7496 : 0.10866915434598923\n",
      "Training loss for batch 7497 : 0.16375000774860382\n",
      "Training loss for batch 7498 : 0.393616646528244\n",
      "Training loss for batch 7499 : 0.016427088528871536\n",
      "Training loss for batch 7500 : 0.20021222531795502\n",
      "Training loss for batch 7501 : 0.24568933248519897\n",
      "Training loss for batch 7502 : 0.37636706233024597\n",
      "Training loss for batch 7503 : 0.33156269788742065\n",
      "Training loss for batch 7504 : 0.21005281805992126\n",
      "Training loss for batch 7505 : 0.18963435292243958\n",
      "Training loss for batch 7506 : 0.12167059630155563\n",
      "Training loss for batch 7507 : 0.18568561971187592\n",
      "Training loss for batch 7508 : 0.0069095296785235405\n",
      "Training loss for batch 7509 : 0.6204004883766174\n",
      "Training loss for batch 7510 : 0.13097792863845825\n",
      "Training loss for batch 7511 : 0.07608858495950699\n",
      "Training loss for batch 7512 : 0.015068048611283302\n",
      "Training loss for batch 7513 : 0.5556984543800354\n",
      "Training loss for batch 7514 : 0.0963161289691925\n",
      "Training loss for batch 7515 : 0.026060618460178375\n",
      "Training loss for batch 7516 : 0.12244863063097\n",
      "Training loss for batch 7517 : 0.32181528210639954\n",
      "Training loss for batch 7518 : 0.14988334476947784\n",
      "Training loss for batch 7519 : 0.2818329334259033\n",
      "Training loss for batch 7520 : 0.12918917834758759\n",
      "Training loss for batch 7521 : 0.02098362147808075\n",
      "Training loss for batch 7522 : 0.23472093045711517\n",
      "Training loss for batch 7523 : 0.6748319864273071\n",
      "Training loss for batch 7524 : 0.022022167220711708\n",
      "Training loss for batch 7525 : 0.20765124261379242\n",
      "Training loss for batch 7526 : 0.06199241802096367\n",
      "Training loss for batch 7527 : 0.3046337366104126\n",
      "Training loss for batch 7528 : 0.06512990593910217\n",
      "Training loss for batch 7529 : 0.11481630802154541\n",
      "Training loss for batch 7530 : 0.015606284141540527\n",
      "Training loss for batch 7531 : 0.22409772872924805\n",
      "Training loss for batch 7532 : 0.1545242965221405\n",
      "Training loss for batch 7533 : 0.17226536571979523\n",
      "Training loss for batch 7534 : 0.04298730939626694\n",
      "Training loss for batch 7535 : 0.6013390421867371\n",
      "Training loss for batch 7536 : 0.4352444112300873\n",
      "Training loss for batch 7537 : 0.004638166632503271\n",
      "Training loss for batch 7538 : 0.1478441208600998\n",
      "Training loss for batch 7539 : 0.1372317671775818\n",
      "Training loss for batch 7540 : 0.2766532897949219\n",
      "Training loss for batch 7541 : 0.08376545459032059\n",
      "Training loss for batch 7542 : 0.16337056457996368\n",
      "Training loss for batch 7543 : 0.19416038691997528\n",
      "Training loss for batch 7544 : 0.039141759276390076\n",
      "Training loss for batch 7545 : 0.15259583294391632\n",
      "Training loss for batch 7546 : 0.07258632034063339\n",
      "Training loss for batch 7547 : 0.06033821031451225\n",
      "Training loss for batch 7548 : 0.488880455493927\n",
      "Training loss for batch 7549 : 0.03406130149960518\n",
      "Training loss for batch 7550 : 0.03251378983259201\n",
      "Training loss for batch 7551 : 0.23044393956661224\n",
      "Training loss for batch 7552 : 0.03222912549972534\n",
      "Training loss for batch 7553 : 0.21774423122406006\n",
      "Training loss for batch 7554 : 0.27645808458328247\n",
      "Training loss for batch 7555 : 0.4517710208892822\n",
      "Training loss for batch 7556 : 0.10956825315952301\n",
      "Training loss for batch 7557 : 0.2252376526594162\n",
      "Training loss for batch 7558 : 0.27896687388420105\n",
      "Training loss for batch 7559 : 0.20047268271446228\n",
      "Training loss for batch 7560 : 0.10398583114147186\n",
      "Training loss for batch 7561 : 0.18429134786128998\n",
      "Training loss for batch 7562 : 0.16682763397693634\n",
      "Training loss for batch 7563 : 0.40160948038101196\n",
      "Training loss for batch 7564 : 0.20418080687522888\n",
      "Training loss for batch 7565 : 0.18957392871379852\n",
      "Training loss for batch 7566 : 0.13095076382160187\n",
      "Training loss for batch 7567 : 0.1357959806919098\n",
      "Training loss for batch 7568 : 0.1505468636751175\n",
      "Training loss for batch 7569 : 0.08361225575208664\n",
      "Training loss for batch 7570 : 0.3582834005355835\n",
      "Training loss for batch 7571 : 0.0\n",
      "Training loss for batch 7572 : 0.18452541530132294\n",
      "Training loss for batch 7573 : 0.14415304362773895\n",
      "Training loss for batch 7574 : 0.27853867411613464\n",
      "Training loss for batch 7575 : 0.2667771875858307\n",
      "Training loss for batch 7576 : 0.012618793174624443\n",
      "Training loss for batch 7577 : 0.25361916422843933\n",
      "Training loss for batch 7578 : 0.30022498965263367\n",
      "Training loss for batch 7579 : 0.372415691614151\n",
      "Training loss for batch 7580 : 0.11059728264808655\n",
      "Training loss for batch 7581 : 0.23932935297489166\n",
      "Training loss for batch 7582 : 0.03888849914073944\n",
      "Training loss for batch 7583 : 0.14897628128528595\n",
      "Training loss for batch 7584 : 0.031099112704396248\n",
      "Training loss for batch 7585 : 0.016130762174725533\n",
      "Training loss for batch 7586 : 0.00897073745727539\n",
      "Training loss for batch 7587 : 0.3679444193840027\n",
      "Training loss for batch 7588 : 0.11602362245321274\n",
      "Training loss for batch 7589 : 0.17872145771980286\n",
      "Training loss for batch 7590 : 0.38663750886917114\n",
      "Training loss for batch 7591 : 0.09725677222013474\n",
      "Training loss for batch 7592 : 0.15094566345214844\n",
      "Training loss for batch 7593 : 0.054212622344493866\n",
      "Training loss for batch 7594 : 0.14332525432109833\n",
      "Training loss for batch 7595 : 0.02261371538043022\n",
      "Training loss for batch 7596 : 0.44722262024879456\n",
      "Training loss for batch 7597 : 0.42067837715148926\n",
      "Training loss for batch 7598 : 0.11107704788446426\n",
      "Training loss for batch 7599 : 0.0984707847237587\n",
      "Training loss for batch 7600 : 0.10499808192253113\n",
      "Training loss for batch 7601 : 0.3586474657058716\n",
      "Training loss for batch 7602 : 0.16501784324645996\n",
      "Training loss for batch 7603 : 0.0944259762763977\n",
      "Training loss for batch 7604 : 0.14000548422336578\n",
      "Training loss for batch 7605 : 0.04481712356209755\n",
      "Training loss for batch 7606 : 0.1846901923418045\n",
      "Training loss for batch 7607 : 0.10952380299568176\n",
      "Training loss for batch 7608 : 0.37173908948898315\n",
      "Training loss for batch 7609 : 0.04456234350800514\n",
      "Training loss for batch 7610 : 0.00850229524075985\n",
      "Training loss for batch 7611 : 0.1674119383096695\n",
      "Training loss for batch 7612 : 0.030597183853387833\n",
      "Training loss for batch 7613 : 0.0032476484775543213\n",
      "Training loss for batch 7614 : 0.13875746726989746\n",
      "Training loss for batch 7615 : 0.12685534358024597\n",
      "Training loss for batch 7616 : 0.4923514425754547\n",
      "Training loss for batch 7617 : 0.05321095138788223\n",
      "Training loss for batch 7618 : 0.0809442549943924\n",
      "Training loss for batch 7619 : 0.308431476354599\n",
      "Training loss for batch 7620 : 0.39280518889427185\n",
      "Training loss for batch 7621 : 0.28094643354415894\n",
      "Training loss for batch 7622 : 0.2773216962814331\n",
      "Training loss for batch 7623 : 0.06762757897377014\n",
      "Training loss for batch 7624 : 0.2074626237154007\n",
      "Training loss for batch 7625 : 0.0006947219371795654\n",
      "Training loss for batch 7626 : 0.27328920364379883\n",
      "Training loss for batch 7627 : 0.32276543974876404\n",
      "Training loss for batch 7628 : 0.011440996080636978\n",
      "Training loss for batch 7629 : 0.09087605029344559\n",
      "Training loss for batch 7630 : 0.13815423846244812\n",
      "Training loss for batch 7631 : 0.13343028724193573\n",
      "Training loss for batch 7632 : 0.2472175806760788\n",
      "Training loss for batch 7633 : 0.024507034569978714\n",
      "Training loss for batch 7634 : 0.14620086550712585\n",
      "Training loss for batch 7635 : 0.04734032601118088\n",
      "Training loss for batch 7636 : 0.12103623151779175\n",
      "Training loss for batch 7637 : 0.2936289310455322\n",
      "Training loss for batch 7638 : 0.25319963693618774\n",
      "Training loss for batch 7639 : 0.45449313521385193\n",
      "Training loss for batch 7640 : 0.06918904185295105\n",
      "Training loss for batch 7641 : 0.21712349355220795\n",
      "Training loss for batch 7642 : 0.18050113320350647\n",
      "Training loss for batch 7643 : 0.4358569085597992\n",
      "Training loss for batch 7644 : 0.17807482182979584\n",
      "Training loss for batch 7645 : 0.1743811070919037\n",
      "Training loss for batch 7646 : 0.05565348640084267\n",
      "Training loss for batch 7647 : 0.15967433154582977\n",
      "Training loss for batch 7648 : 0.16174323856830597\n",
      "Training loss for batch 7649 : 0.10833317786455154\n",
      "Training loss for batch 7650 : 0.6924105882644653\n",
      "Training loss for batch 7651 : 0.3172155022621155\n",
      "Training loss for batch 7652 : 0.24046942591667175\n",
      "Training loss for batch 7653 : 0.171029731631279\n",
      "Training loss for batch 7654 : 0.2324119210243225\n",
      "Training loss for batch 7655 : 0.12051334232091904\n",
      "Training loss for batch 7656 : 0.024290326982736588\n",
      "Training loss for batch 7657 : 0.4155389964580536\n",
      "Training loss for batch 7658 : 0.017874814569950104\n",
      "Training loss for batch 7659 : 0.29947906732559204\n",
      "Training loss for batch 7660 : 0.5271800756454468\n",
      "Training loss for batch 7661 : 0.0856078714132309\n",
      "Training loss for batch 7662 : 0.31519603729248047\n",
      "Training loss for batch 7663 : 0.2131592035293579\n",
      "Training loss for batch 7664 : 0.5123283863067627\n",
      "Training loss for batch 7665 : 0.19209304451942444\n",
      "Training loss for batch 7666 : 0.13255496323108673\n",
      "Training loss for batch 7667 : 0.2773502469062805\n",
      "Training loss for batch 7668 : 0.22243352234363556\n",
      "Training loss for batch 7669 : 0.023729071021080017\n",
      "Training loss for batch 7670 : 0.32395634055137634\n",
      "Training loss for batch 7671 : 0.029281964525580406\n",
      "Training loss for batch 7672 : 0.47588589787483215\n",
      "Training loss for batch 7673 : 0.471244215965271\n",
      "Training loss for batch 7674 : 0.2917740046977997\n",
      "Training loss for batch 7675 : 0.16895899176597595\n",
      "Training loss for batch 7676 : 0.31584084033966064\n",
      "Training loss for batch 7677 : 0.07590869814157486\n",
      "Training loss for batch 7678 : 0.14177465438842773\n",
      "Training loss for batch 7679 : 0.31132516264915466\n",
      "Training loss for batch 7680 : 0.33369508385658264\n",
      "Training loss for batch 7681 : 0.548602283000946\n",
      "Training loss for batch 7682 : 0.03219567984342575\n",
      "Training loss for batch 7683 : 0.09764143079519272\n",
      "Training loss for batch 7684 : 0.08932923525571823\n",
      "Training loss for batch 7685 : 0.0026416711043566465\n",
      "Training loss for batch 7686 : 0.17431141436100006\n",
      "Training loss for batch 7687 : 0.030942577868700027\n",
      "Training loss for batch 7688 : 0.22361215949058533\n",
      "Training loss for batch 7689 : 0.09958960860967636\n",
      "Training loss for batch 7690 : 0.18588034808635712\n",
      "Training loss for batch 7691 : 0.14434576034545898\n",
      "Training loss for batch 7692 : 0.06590777635574341\n",
      "Training loss for batch 7693 : 0.28069931268692017\n",
      "Training loss for batch 7694 : 0.14214354753494263\n",
      "Training loss for batch 7695 : 0.11066281050443649\n",
      "Training loss for batch 7696 : 0.13779333233833313\n",
      "Training loss for batch 7697 : 0.20859478414058685\n",
      "Training loss for batch 7698 : 0.2507735788822174\n",
      "Training loss for batch 7699 : 0.19165807962417603\n",
      "Training loss for batch 7700 : 0.1462174504995346\n",
      "Training loss for batch 7701 : 0.07444155216217041\n",
      "Training loss for batch 7702 : 0.07835255563259125\n",
      "Training loss for batch 7703 : 0.040902864187955856\n",
      "Training loss for batch 7704 : 0.1432061493396759\n",
      "Training loss for batch 7705 : 0.3504781723022461\n",
      "Training loss for batch 7706 : 0.06457317620515823\n",
      "Training loss for batch 7707 : 0.604414701461792\n",
      "Training loss for batch 7708 : 0.19323790073394775\n",
      "Training loss for batch 7709 : 0.19419902563095093\n",
      "Training loss for batch 7710 : 0.0\n",
      "Training loss for batch 7711 : 0.16863347589969635\n",
      "Training loss for batch 7712 : 0.09499197453260422\n",
      "Training loss for batch 7713 : 0.1784217208623886\n",
      "Training loss for batch 7714 : 0.07209804654121399\n",
      "Training loss for batch 7715 : 0.23677703738212585\n",
      "Training loss for batch 7716 : 0.03504077345132828\n",
      "Training loss for batch 7717 : 0.10841857641935349\n",
      "Training loss for batch 7718 : 0.05870480462908745\n",
      "Training loss for batch 7719 : 0.1547698825597763\n",
      "Training loss for batch 7720 : 0.07379534840583801\n",
      "Training loss for batch 7721 : 0.4129295349121094\n",
      "Training loss for batch 7722 : 0.0\n",
      "Training loss for batch 7723 : 0.1396797001361847\n",
      "Training loss for batch 7724 : 0.04725894331932068\n",
      "Training loss for batch 7725 : 0.3702346086502075\n",
      "Training loss for batch 7726 : 0.017979050055146217\n",
      "Training loss for batch 7727 : 0.11923427134752274\n",
      "Training loss for batch 7728 : 0.02246064692735672\n",
      "Training loss for batch 7729 : 0.05597112327814102\n",
      "Training loss for batch 7730 : 0.07177130877971649\n",
      "Training loss for batch 7731 : 0.16115380823612213\n",
      "Training loss for batch 7732 : 0.10001648217439651\n",
      "Training loss for batch 7733 : 0.03471869230270386\n",
      "Training loss for batch 7734 : 0.20489995181560516\n",
      "Training loss for batch 7735 : 0.020570112392306328\n",
      "Training loss for batch 7736 : 0.22196544706821442\n",
      "Training loss for batch 7737 : 0.1624426394701004\n",
      "Training loss for batch 7738 : 0.23831376433372498\n",
      "Training loss for batch 7739 : 0.03468261659145355\n",
      "Training loss for batch 7740 : 0.1780479997396469\n",
      "Training loss for batch 7741 : 0.20483651757240295\n",
      "Training loss for batch 7742 : 0.4013412594795227\n",
      "Training loss for batch 7743 : 0.1860862374305725\n",
      "Training loss for batch 7744 : 0.13107608258724213\n",
      "Training loss for batch 7745 : 0.12243093550205231\n",
      "Training loss for batch 7746 : 0.20522938668727875\n",
      "Training loss for batch 7747 : 0.18053504824638367\n",
      "Training loss for batch 7748 : 0.3821268677711487\n",
      "Training loss for batch 7749 : 0.008910557255148888\n",
      "Training loss for batch 7750 : 0.06239764019846916\n",
      "Training loss for batch 7751 : 0.2695043981075287\n",
      "Training loss for batch 7752 : 0.21272650361061096\n",
      "Training loss for batch 7753 : 0.09321554750204086\n",
      "Training loss for batch 7754 : 0.23624758422374725\n",
      "Training loss for batch 7755 : 0.2633352279663086\n",
      "Training loss for batch 7756 : 0.22174407541751862\n",
      "Training loss for batch 7757 : 0.19326841831207275\n",
      "Training loss for batch 7758 : 0.07375220954418182\n",
      "Training loss for batch 7759 : 0.19487795233726501\n",
      "Training loss for batch 7760 : 0.009997976012527943\n",
      "Training loss for batch 7761 : 0.1627340018749237\n",
      "Training loss for batch 7762 : 0.2101672887802124\n",
      "Training loss for batch 7763 : 0.11976649612188339\n",
      "Training loss for batch 7764 : 0.15140800178050995\n",
      "Training loss for batch 7765 : 0.012309739366173744\n",
      "Training loss for batch 7766 : 0.12338234484195709\n",
      "Training loss for batch 7767 : 0.2707909643650055\n",
      "Training loss for batch 7768 : 0.33788594603538513\n",
      "Training loss for batch 7769 : 0.17887850105762482\n",
      "Training loss for batch 7770 : 0.4867165684700012\n",
      "Training loss for batch 7771 : 0.021251434460282326\n",
      "Training loss for batch 7772 : 0.2924500107765198\n",
      "Training loss for batch 7773 : 0.1408613622188568\n",
      "Training loss for batch 7774 : 0.16778425872325897\n",
      "Training loss for batch 7775 : 0.2933068871498108\n",
      "Training loss for batch 7776 : 0.33032286167144775\n",
      "Training loss for batch 7777 : 0.07883249968290329\n",
      "Training loss for batch 7778 : 0.5090725421905518\n",
      "Training loss for batch 7779 : 0.15770366787910461\n",
      "Training loss for batch 7780 : 0.10321609675884247\n",
      "Training loss for batch 7781 : 0.06901746988296509\n",
      "Training loss for batch 7782 : 0.18639962375164032\n",
      "Training loss for batch 7783 : 0.09119950234889984\n",
      "Training loss for batch 7784 : 0.05594590678811073\n",
      "Training loss for batch 7785 : 0.04805319011211395\n",
      "Training loss for batch 7786 : 0.12246972322463989\n",
      "Training loss for batch 7787 : 0.19964726269245148\n",
      "Training loss for batch 7788 : 0.07649212330579758\n",
      "Training loss for batch 7789 : 0.08811509609222412\n",
      "Training loss for batch 7790 : 0.1002713069319725\n",
      "Training loss for batch 7791 : 0.040420398116111755\n",
      "Training loss for batch 7792 : 0.35216501355171204\n",
      "Training loss for batch 7793 : 0.11246830970048904\n",
      "Training loss for batch 7794 : 0.2095841020345688\n",
      "Training loss for batch 7795 : 0.03686029464006424\n",
      "Training loss for batch 7796 : 0.030172664672136307\n",
      "Training loss for batch 7797 : 0.2601739764213562\n",
      "Training loss for batch 7798 : 0.08915024250745773\n",
      "Training loss for batch 7799 : -0.00014810873835813254\n",
      "Training loss for batch 7800 : 0.1660953015089035\n",
      "Training loss for batch 7801 : 0.32937195897102356\n",
      "Training loss for batch 7802 : 0.0\n",
      "Training loss for batch 7803 : 0.48815396428108215\n",
      "Training loss for batch 7804 : 0.2334064245223999\n",
      "Training loss for batch 7805 : 0.3057232201099396\n",
      "Training loss for batch 7806 : 0.17685914039611816\n",
      "Training loss for batch 7807 : 0.2629771828651428\n",
      "Training loss for batch 7808 : 0.2254965752363205\n",
      "Training loss for batch 7809 : 0.10779747366905212\n",
      "Training loss for batch 7810 : 0.3769586980342865\n",
      "Training loss for batch 7811 : 0.18519413471221924\n",
      "Training loss for batch 7812 : 0.2988535761833191\n",
      "Training loss for batch 7813 : 0.0575699508190155\n",
      "Training loss for batch 7814 : 0.0\n",
      "Training loss for batch 7815 : 0.09477867186069489\n",
      "Training loss for batch 7816 : 0.09227820485830307\n",
      "Training loss for batch 7817 : 0.016259532421827316\n",
      "Training loss for batch 7818 : 0.05097746104001999\n",
      "Training loss for batch 7819 : 0.0715041309595108\n",
      "Training loss for batch 7820 : 0.21648728847503662\n",
      "Training loss for batch 7821 : 0.044575806707143784\n",
      "Training loss for batch 7822 : 0.2982131540775299\n",
      "Training loss for batch 7823 : 0.10622776299715042\n",
      "Training loss for batch 7824 : 0.05814842879772186\n",
      "Training loss for batch 7825 : 0.45177730917930603\n",
      "Training loss for batch 7826 : 0.17452161014080048\n",
      "Training loss for batch 7827 : 0.1656089723110199\n",
      "Training loss for batch 7828 : 0.36009493470191956\n",
      "Training loss for batch 7829 : 0.004777949303388596\n",
      "Training loss for batch 7830 : 0.03272613137960434\n",
      "Training loss for batch 7831 : 0.1030052974820137\n",
      "Training loss for batch 7832 : 0.14985518157482147\n",
      "Training loss for batch 7833 : 0.14938560128211975\n",
      "Training loss for batch 7834 : 0.189504474401474\n",
      "Training loss for batch 7835 : 0.439035564661026\n",
      "Training loss for batch 7836 : 0.16440607607364655\n",
      "Training loss for batch 7837 : 0.1652819961309433\n",
      "Training loss for batch 7838 : 0.0356147401034832\n",
      "Training loss for batch 7839 : 0.306966632604599\n",
      "Training loss for batch 7840 : 0.18990901112556458\n",
      "Training loss for batch 7841 : 0.09631744772195816\n",
      "Training loss for batch 7842 : 0.10723792761564255\n",
      "Training loss for batch 7843 : 0.2721894085407257\n",
      "Training loss for batch 7844 : 0.07192913442850113\n",
      "Training loss for batch 7845 : 0.053655676543712616\n",
      "Training loss for batch 7846 : 0.1326582133769989\n",
      "Training loss for batch 7847 : 0.293218195438385\n",
      "Training loss for batch 7848 : 0.24308893084526062\n",
      "Training loss for batch 7849 : 0.021962229162454605\n",
      "Training loss for batch 7850 : 0.16524465382099152\n",
      "Training loss for batch 7851 : 0.023747878149151802\n",
      "Training loss for batch 7852 : 0.10010279715061188\n",
      "Training loss for batch 7853 : 0.16836987435817719\n",
      "Training loss for batch 7854 : 0.22740712761878967\n",
      "Training loss for batch 7855 : 0.02705167792737484\n",
      "Training loss for batch 7856 : 0.08872818946838379\n",
      "Training loss for batch 7857 : 0.2111070603132248\n",
      "Training loss for batch 7858 : 0.049783848226070404\n",
      "Training loss for batch 7859 : 0.18430423736572266\n",
      "Training loss for batch 7860 : 0.08515716344118118\n",
      "Training loss for batch 7861 : 0.12385955452919006\n",
      "Training loss for batch 7862 : 0.2883104979991913\n",
      "Training loss for batch 7863 : 0.20996838808059692\n",
      "Training loss for batch 7864 : 0.03570020571351051\n",
      "Training loss for batch 7865 : 0.3169997036457062\n",
      "Training loss for batch 7866 : 0.038077399134635925\n",
      "Training loss for batch 7867 : 0.03990007936954498\n",
      "Training loss for batch 7868 : 0.11634302884340286\n",
      "Training loss for batch 7869 : 0.11837846040725708\n",
      "Training loss for batch 7870 : 0.27348023653030396\n",
      "Training loss for batch 7871 : 0.0574130117893219\n",
      "Training loss for batch 7872 : 0.25116634368896484\n",
      "Training loss for batch 7873 : 0.3049561679363251\n",
      "Training loss for batch 7874 : 0.15644066035747528\n",
      "Training loss for batch 7875 : 0.20047836005687714\n",
      "Training loss for batch 7876 : 0.026961423456668854\n",
      "Training loss for batch 7877 : 0.06575310230255127\n",
      "Training loss for batch 7878 : 0.18313904106616974\n",
      "Training loss for batch 7879 : 0.4372038245201111\n",
      "Training loss for batch 7880 : 0.11044436693191528\n",
      "Training loss for batch 7881 : 0.16470332443714142\n",
      "Training loss for batch 7882 : 0.18995782732963562\n",
      "Training loss for batch 7883 : 0.09003165364265442\n",
      "Training loss for batch 7884 : 0.19221557676792145\n",
      "Training loss for batch 7885 : 0.11845213919878006\n",
      "Training loss for batch 7886 : 0.2894735038280487\n",
      "Training loss for batch 7887 : 0.1063171848654747\n",
      "Training loss for batch 7888 : 0.12298181653022766\n",
      "Training loss for batch 7889 : 0.06692341715097427\n",
      "Training loss for batch 7890 : 0.3954184949398041\n",
      "Training loss for batch 7891 : 0.007896488532423973\n",
      "Training loss for batch 7892 : 0.19142121076583862\n",
      "Training loss for batch 7893 : 0.3015817105770111\n",
      "Training loss for batch 7894 : 0.3690504729747772\n",
      "Training loss for batch 7895 : 0.09123514592647552\n",
      "Training loss for batch 7896 : 0.0\n",
      "Training loss for batch 7897 : 0.04616343602538109\n",
      "Training loss for batch 7898 : 0.2129705399274826\n",
      "Training loss for batch 7899 : 0.34262093901634216\n",
      "Training loss for batch 7900 : 0.16064020991325378\n",
      "Training loss for batch 7901 : 0.0981002002954483\n",
      "Training loss for batch 7902 : 0.6012409925460815\n",
      "Training loss for batch 7903 : 0.03609609231352806\n",
      "Training loss for batch 7904 : 0.12250496447086334\n",
      "Training loss for batch 7905 : 0.35792624950408936\n",
      "Training loss for batch 7906 : 0.04262414947152138\n",
      "Training loss for batch 7907 : 0.08367504924535751\n",
      "Training loss for batch 7908 : 0.07085689157247543\n",
      "Training loss for batch 7909 : 0.08733241260051727\n",
      "Training loss for batch 7910 : 0.06082557886838913\n",
      "Training loss for batch 7911 : 0.3796273469924927\n",
      "Training loss for batch 7912 : 0.12627659738063812\n",
      "Training loss for batch 7913 : 0.01248985342681408\n",
      "Training loss for batch 7914 : 0.5564643740653992\n",
      "Training loss for batch 7915 : 0.1399574726819992\n",
      "Training loss for batch 7916 : 0.16643136739730835\n",
      "Training loss for batch 7917 : 0.08684268593788147\n",
      "Training loss for batch 7918 : 0.1292072981595993\n",
      "Training loss for batch 7919 : 0.15994739532470703\n",
      "Training loss for batch 7920 : 0.1166844293475151\n",
      "Training loss for batch 7921 : 0.15141834318637848\n",
      "Training loss for batch 7922 : -2.0749923805851722e-06\n",
      "Training loss for batch 7923 : 0.16665557026863098\n",
      "Training loss for batch 7924 : 0.03786787763237953\n",
      "Training loss for batch 7925 : 0.08433689922094345\n",
      "Training loss for batch 7926 : 0.23049500584602356\n",
      "Training loss for batch 7927 : 0.29289668798446655\n",
      "Training loss for batch 7928 : 0.17552883923053741\n",
      "Training loss for batch 7929 : 0.22432532906532288\n",
      "Training loss for batch 7930 : 0.18915411829948425\n",
      "Training loss for batch 7931 : 0.2720996141433716\n",
      "Training loss for batch 7932 : 0.07174155861139297\n",
      "Training loss for batch 7933 : 0.1131589338183403\n",
      "Training loss for batch 7934 : 0.3264659643173218\n",
      "Training loss for batch 7935 : 0.07316538691520691\n",
      "Training loss for batch 7936 : 0.15815401077270508\n",
      "Training loss for batch 7937 : 0.07633931189775467\n",
      "Training loss for batch 7938 : 0.13233359158039093\n",
      "Training loss for batch 7939 : 0.07088006287813187\n",
      "Training loss for batch 7940 : 0.32766634225845337\n",
      "Training loss for batch 7941 : 0.010850450024008751\n",
      "Training loss for batch 7942 : 0.309050977230072\n",
      "Training loss for batch 7943 : 0.4731963574886322\n",
      "Training loss for batch 7944 : 0.12048962712287903\n",
      "Training loss for batch 7945 : -8.323415386257693e-06\n",
      "Training loss for batch 7946 : 0.05051163583993912\n",
      "Training loss for batch 7947 : 0.23525726795196533\n",
      "Training loss for batch 7948 : 0.060023512691259384\n",
      "Training loss for batch 7949 : 0.2601091265678406\n",
      "Training loss for batch 7950 : 0.23253369331359863\n",
      "Training loss for batch 7951 : 0.6053688526153564\n",
      "Training loss for batch 7952 : 0.35633403062820435\n",
      "Training loss for batch 7953 : 0.07397381216287613\n",
      "Training loss for batch 7954 : 0.14256565272808075\n",
      "Training loss for batch 7955 : -0.00011230734526179731\n",
      "Training loss for batch 7956 : 0.0006298971129581332\n",
      "Training loss for batch 7957 : 0.2497192770242691\n",
      "Training loss for batch 7958 : 0.07586367428302765\n",
      "Training loss for batch 7959 : 0.15250375866889954\n",
      "Training loss for batch 7960 : 0.3533861041069031\n",
      "Training loss for batch 7961 : 0.02755490504205227\n",
      "Training loss for batch 7962 : 0.14942052960395813\n",
      "Training loss for batch 7963 : 0.1268724799156189\n",
      "Training loss for batch 7964 : 0.0751175805926323\n",
      "Training loss for batch 7965 : 0.2461995631456375\n",
      "Training loss for batch 7966 : 0.2640005946159363\n",
      "Training loss for batch 7967 : 0.08172895759344101\n",
      "Training loss for batch 7968 : 0.00963814090937376\n",
      "Training loss for batch 7969 : 0.1980896145105362\n",
      "Training loss for batch 7970 : 0.21837030351161957\n",
      "Training loss for batch 7971 : 0.22415858507156372\n",
      "Training loss for batch 7972 : 0.4131663739681244\n",
      "Training loss for batch 7973 : 0.21304449439048767\n",
      "Training loss for batch 7974 : 0.16820840537548065\n",
      "Training loss for batch 7975 : 0.2550976574420929\n",
      "Training loss for batch 7976 : 0.15968824923038483\n",
      "Training loss for batch 7977 : 0.558506965637207\n",
      "Training loss for batch 7978 : 0.013106117956340313\n",
      "Training loss for batch 7979 : 0.17845673859119415\n",
      "Training loss for batch 7980 : 0.23747830092906952\n",
      "Training loss for batch 7981 : 0.18830853700637817\n",
      "Training loss for batch 7982 : 0.6680989861488342\n",
      "Training loss for batch 7983 : 0.20307481288909912\n",
      "Training loss for batch 7984 : 0.09096236526966095\n",
      "Training loss for batch 7985 : 0.3608955144882202\n",
      "Training loss for batch 7986 : 0.14603154361248016\n",
      "Training loss for batch 7987 : 0.16639883816242218\n",
      "Training loss for batch 7988 : 0.07280635833740234\n",
      "Training loss for batch 7989 : 0.35327818989753723\n",
      "Training loss for batch 7990 : 0.1268528401851654\n",
      "Training loss for batch 7991 : 0.2892460227012634\n",
      "Training loss for batch 7992 : 0.24276234209537506\n",
      "Training loss for batch 7993 : 0.12765611708164215\n",
      "Training loss for batch 7994 : 0.2162005454301834\n",
      "Training loss for batch 7995 : 0.006575240753591061\n",
      "Training loss for batch 7996 : 0.1406433880329132\n",
      "Training loss for batch 7997 : 0.049395546317100525\n",
      "Training loss for batch 7998 : 0.14378061890602112\n",
      "Training loss for batch 7999 : 0.25322991609573364\n",
      "Training loss for batch 8000 : 0.5655860900878906\n",
      "Training loss for batch 8001 : 0.059443626552820206\n",
      "Training loss for batch 8002 : 0.10904515534639359\n",
      "Training loss for batch 8003 : 0.02752239629626274\n",
      "Training loss for batch 8004 : 0.16493535041809082\n",
      "Training loss for batch 8005 : 0.28709617257118225\n",
      "Training loss for batch 8006 : 0.1765185296535492\n",
      "Training loss for batch 8007 : 0.4116628170013428\n",
      "Training loss for batch 8008 : 0.07808681577444077\n",
      "Training loss for batch 8009 : 0.002864889334887266\n",
      "Training loss for batch 8010 : 0.40521499514579773\n",
      "Training loss for batch 8011 : 0.1279871165752411\n",
      "Training loss for batch 8012 : 0.0638609230518341\n",
      "Training loss for batch 8013 : 0.16245923936367035\n",
      "Training loss for batch 8014 : 0.16799578070640564\n",
      "Training loss for batch 8015 : 0.08183068782091141\n",
      "Training loss for batch 8016 : 0.018268059939146042\n",
      "Training loss for batch 8017 : 0.017665987834334373\n",
      "Training loss for batch 8018 : 0.38787978887557983\n",
      "Training loss for batch 8019 : 0.23482349514961243\n",
      "Training loss for batch 8020 : 0.14656949043273926\n",
      "Training loss for batch 8021 : 0.004717241041362286\n",
      "Training loss for batch 8022 : 0.2664942443370819\n",
      "Training loss for batch 8023 : 0.3810094892978668\n",
      "Training loss for batch 8024 : 0.14288966357707977\n",
      "Training loss for batch 8025 : 0.022726815193891525\n",
      "Training loss for batch 8026 : 0.14043909311294556\n",
      "Training loss for batch 8027 : 0.48999878764152527\n",
      "Training loss for batch 8028 : 0.22579266130924225\n",
      "Training loss for batch 8029 : 0.2341870665550232\n",
      "Training loss for batch 8030 : 0.11663155257701874\n",
      "Training loss for batch 8031 : 0.04704706743359566\n",
      "Training loss for batch 8032 : 0.16839568316936493\n",
      "Training loss for batch 8033 : 0.09782598912715912\n",
      "Training loss for batch 8034 : 0.05816512554883957\n",
      "Training loss for batch 8035 : 0.13800029456615448\n",
      "Training loss for batch 8036 : 0.18418502807617188\n",
      "Training loss for batch 8037 : 0.6209232211112976\n",
      "Training loss for batch 8038 : 0.09216863662004471\n",
      "Training loss for batch 8039 : 0.002917345380410552\n",
      "Training loss for batch 8040 : 0.2575390934944153\n",
      "Training loss for batch 8041 : 0.12639327347278595\n",
      "Training loss for batch 8042 : 0.4477544128894806\n",
      "Training loss for batch 8043 : 0.022175408899784088\n",
      "Training loss for batch 8044 : 0.19476881623268127\n",
      "Training loss for batch 8045 : 0.05053522437810898\n",
      "Training loss for batch 8046 : 0.0\n",
      "Training loss for batch 8047 : 0.03063586726784706\n",
      "Training loss for batch 8048 : 0.34991204738616943\n",
      "Training loss for batch 8049 : 0.03643137216567993\n",
      "Training loss for batch 8050 : 0.03992997482419014\n",
      "Training loss for batch 8051 : 0.34728315472602844\n",
      "Training loss for batch 8052 : 0.218405619263649\n",
      "Training loss for batch 8053 : 0.10264404863119125\n",
      "Training loss for batch 8054 : 0.27830109000205994\n",
      "Training loss for batch 8055 : 0.08361440896987915\n",
      "Training loss for batch 8056 : 0.15730677545070648\n",
      "Training loss for batch 8057 : 0.1803562492132187\n",
      "Training loss for batch 8058 : 0.2203899323940277\n",
      "Training loss for batch 8059 : 0.1567593365907669\n",
      "Training loss for batch 8060 : 0.03552531450986862\n",
      "Training loss for batch 8061 : 0.10524921119213104\n",
      "Training loss for batch 8062 : 0.07590805739164352\n",
      "Training loss for batch 8063 : 0.0005762875080108643\n",
      "Training loss for batch 8064 : 0.11579232662916183\n",
      "Training loss for batch 8065 : 0.1904209405183792\n",
      "Training loss for batch 8066 : 0.5295438170433044\n",
      "Training loss for batch 8067 : 0.06247583031654358\n",
      "Training loss for batch 8068 : 0.0768752321600914\n",
      "Training loss for batch 8069 : 0.13536490499973297\n",
      "Training loss for batch 8070 : 0.008670042268931866\n",
      "Training loss for batch 8071 : 0.1268911063671112\n",
      "Training loss for batch 8072 : 0.3617345094680786\n",
      "Training loss for batch 8073 : 0.16770116984844208\n",
      "Training loss for batch 8074 : 0.0878182053565979\n",
      "Training loss for batch 8075 : 0.005163659807294607\n",
      "Training loss for batch 8076 : 0.1655517816543579\n",
      "Training loss for batch 8077 : 0.0\n",
      "Training loss for batch 8078 : 0.6586955189704895\n",
      "Training loss for batch 8079 : 0.051312245428562164\n",
      "Training loss for batch 8080 : 0.30541399121284485\n",
      "Training loss for batch 8081 : 0.23256945610046387\n",
      "Training loss for batch 8082 : 0.23846939206123352\n",
      "Training loss for batch 8083 : 0.6876561045646667\n",
      "Training loss for batch 8084 : 0.021998489275574684\n",
      "Training loss for batch 8085 : 0.10942593961954117\n",
      "Training loss for batch 8086 : 0.01215300988405943\n",
      "Training loss for batch 8087 : 0.06141909211874008\n",
      "Training loss for batch 8088 : 0.04709552228450775\n",
      "Training loss for batch 8089 : 0.11669106781482697\n",
      "Training loss for batch 8090 : 0.004649053327739239\n",
      "Training loss for batch 8091 : 0.3768921494483948\n",
      "Training loss for batch 8092 : 0.23572948575019836\n",
      "Training loss for batch 8093 : 0.153701514005661\n",
      "Training loss for batch 8094 : 0.009891713038086891\n",
      "Training loss for batch 8095 : 0.23560672998428345\n",
      "Training loss for batch 8096 : 0.06223580241203308\n",
      "Training loss for batch 8097 : 0.1421646624803543\n",
      "Training loss for batch 8098 : 0.11708071827888489\n",
      "Training loss for batch 8099 : 0.16042298078536987\n",
      "Training loss for batch 8100 : 0.2633858025074005\n",
      "Training loss for batch 8101 : 0.13909633457660675\n",
      "Training loss for batch 8102 : 0.22863104939460754\n",
      "Training loss for batch 8103 : 0.10414150357246399\n",
      "Training loss for batch 8104 : 0.16804668307304382\n",
      "Training loss for batch 8105 : 0.17832991480827332\n",
      "Training loss for batch 8106 : -3.168051262036897e-05\n",
      "Training loss for batch 8107 : 0.11636582762002945\n",
      "Training loss for batch 8108 : 0.1798265427350998\n",
      "Training loss for batch 8109 : 0.012535746209323406\n",
      "Training loss for batch 8110 : 0.05238480493426323\n",
      "Training loss for batch 8111 : 0.19692906737327576\n",
      "Training loss for batch 8112 : 0.007423813454806805\n",
      "Training loss for batch 8113 : 0.014380560256540775\n",
      "Training loss for batch 8114 : 0.34565308690071106\n",
      "Training loss for batch 8115 : 0.25389668345451355\n",
      "Training loss for batch 8116 : 0.09654828161001205\n",
      "Training loss for batch 8117 : 0.345630019903183\n",
      "Training loss for batch 8118 : 0.2331291288137436\n",
      "Training loss for batch 8119 : 0.17989489436149597\n",
      "Training loss for batch 8120 : 0.12394127994775772\n",
      "Training loss for batch 8121 : 0.3891257643699646\n",
      "Training loss for batch 8122 : 0.0852774977684021\n",
      "Training loss for batch 8123 : 0.039040565490722656\n",
      "Training loss for batch 8124 : 0.11119509488344193\n",
      "Training loss for batch 8125 : 0.08485175669193268\n",
      "Training loss for batch 8126 : 0.5431680083274841\n",
      "Training loss for batch 8127 : 0.2836427688598633\n",
      "Training loss for batch 8128 : 0.38442957401275635\n",
      "Training loss for batch 8129 : 0.427783727645874\n",
      "Training loss for batch 8130 : 0.22704392671585083\n",
      "Training loss for batch 8131 : 0.015702275559306145\n",
      "Training loss for batch 8132 : 0.37501415610313416\n",
      "Training loss for batch 8133 : 0.11215578019618988\n",
      "Training loss for batch 8134 : 0.243317648768425\n",
      "Training loss for batch 8135 : 0.030380358919501305\n",
      "Training loss for batch 8136 : 0.29155728220939636\n",
      "Training loss for batch 8137 : 0.4594913721084595\n",
      "Training loss for batch 8138 : 0.1958957463502884\n",
      "Training loss for batch 8139 : 0.12861314415931702\n",
      "Training loss for batch 8140 : 0.3042425513267517\n",
      "Training loss for batch 8141 : 0.0027577828150242567\n",
      "Training loss for batch 8142 : 0.03740663453936577\n",
      "Training loss for batch 8143 : 0.06673014163970947\n",
      "Training loss for batch 8144 : 0.15792906284332275\n",
      "Training loss for batch 8145 : 0.26542115211486816\n",
      "Training loss for batch 8146 : 0.13155168294906616\n",
      "Training loss for batch 8147 : 0.08637358993291855\n",
      "Training loss for batch 8148 : 0.3319616913795471\n",
      "Training loss for batch 8149 : 0.003763671265915036\n",
      "Training loss for batch 8150 : 0.10748955607414246\n",
      "Training loss for batch 8151 : 0.17189925909042358\n",
      "Training loss for batch 8152 : 0.13747508823871613\n",
      "Training loss for batch 8153 : 0.21460174024105072\n",
      "Training loss for batch 8154 : 0.19716878235340118\n",
      "Training loss for batch 8155 : 0.21113181114196777\n",
      "Training loss for batch 8156 : 0.3957533836364746\n",
      "Training loss for batch 8157 : 0.0683448314666748\n",
      "Training loss for batch 8158 : 0.18900221586227417\n",
      "Training loss for batch 8159 : 0.11360922455787659\n",
      "Training loss for batch 8160 : 0.11832559108734131\n",
      "Training loss for batch 8161 : 0.10720707476139069\n",
      "Training loss for batch 8162 : 0.34238460659980774\n",
      "Training loss for batch 8163 : 0.0019386808853596449\n",
      "Training loss for batch 8164 : 0.22441387176513672\n",
      "Training loss for batch 8165 : 0.11922543495893478\n",
      "Training loss for batch 8166 : 0.13518762588500977\n",
      "Training loss for batch 8167 : 0.1234966516494751\n",
      "Training loss for batch 8168 : 0.0886351466178894\n",
      "Training loss for batch 8169 : 0.09779693186283112\n",
      "Training loss for batch 8170 : 0.33551180362701416\n",
      "Training loss for batch 8171 : 0.004415621515363455\n",
      "Training loss for batch 8172 : 0.2716015577316284\n",
      "Training loss for batch 8173 : 0.49122169613838196\n",
      "Training loss for batch 8174 : 0.027932267636060715\n",
      "Training loss for batch 8175 : 0.37960612773895264\n",
      "Training loss for batch 8176 : 0.0980471670627594\n",
      "Training loss for batch 8177 : 0.05521108955144882\n",
      "Training loss for batch 8178 : 0.02842656895518303\n",
      "Training loss for batch 8179 : 0.25228384137153625\n",
      "Training loss for batch 8180 : 0.18521127104759216\n",
      "Training loss for batch 8181 : 0.20323707163333893\n",
      "Training loss for batch 8182 : 0.1597507745027542\n",
      "Training loss for batch 8183 : 0.14749549329280853\n",
      "Training loss for batch 8184 : 0.03944848105311394\n",
      "Training loss for batch 8185 : 0.018732022494077682\n",
      "Training loss for batch 8186 : 0.49465909600257874\n",
      "Training loss for batch 8187 : 0.2797001898288727\n",
      "Training loss for batch 8188 : 0.2188098430633545\n",
      "Training loss for batch 8189 : 0.4881843030452728\n",
      "Training loss for batch 8190 : 0.30369997024536133\n",
      "Training loss for batch 8191 : 0.20899704098701477\n",
      "Training loss for batch 8192 : 0.01607701927423477\n",
      "Training loss for batch 8193 : 0.0725998654961586\n",
      "Training loss for batch 8194 : 0.0863180160522461\n",
      "Training loss for batch 8195 : 0.22772347927093506\n",
      "Training loss for batch 8196 : 0.4059752821922302\n",
      "Training loss for batch 8197 : 0.0\n",
      "Training loss for batch 8198 : 0.3419935703277588\n",
      "Training loss for batch 8199 : 0.3900200128555298\n",
      "Training loss for batch 8200 : 0.27349764108657837\n",
      "Training loss for batch 8201 : 0.12513120472431183\n",
      "Training loss for batch 8202 : 0.5543274879455566\n",
      "Training loss for batch 8203 : 0.018528521060943604\n",
      "Training loss for batch 8204 : 0.12997685372829437\n",
      "Training loss for batch 8205 : 0.1518734097480774\n",
      "Training loss for batch 8206 : 0.11638272553682327\n",
      "Training loss for batch 8207 : 0.040359169244766235\n",
      "Training loss for batch 8208 : 0.4869224727153778\n",
      "Training loss for batch 8209 : 0.09805463999509811\n",
      "Training loss for batch 8210 : -0.0001365561765851453\n",
      "Training loss for batch 8211 : 0.3064368665218353\n",
      "Training loss for batch 8212 : 0.06320025026798248\n",
      "Training loss for batch 8213 : 0.09021744132041931\n",
      "Training loss for batch 8214 : 0.2713841199874878\n",
      "Training loss for batch 8215 : 0.08909179270267487\n",
      "Training loss for batch 8216 : 0.016458045691251755\n",
      "Training loss for batch 8217 : 0.1990278959274292\n",
      "Training loss for batch 8218 : 0.07171991467475891\n",
      "Training loss for batch 8219 : -0.0002036040386883542\n",
      "Training loss for batch 8220 : 0.38624247908592224\n",
      "Training loss for batch 8221 : 0.13636372983455658\n",
      "Training loss for batch 8222 : 0.09774923324584961\n",
      "Training loss for batch 8223 : 0.2713189721107483\n",
      "Training loss for batch 8224 : 0.15871669352054596\n",
      "Training loss for batch 8225 : 0.08425328135490417\n",
      "Training loss for batch 8226 : 0.17322447896003723\n",
      "Training loss for batch 8227 : 0.2639756500720978\n",
      "Training loss for batch 8228 : 0.15125589072704315\n",
      "Training loss for batch 8229 : 0.025855395942926407\n",
      "Training loss for batch 8230 : 0.005900240503251553\n",
      "Training loss for batch 8231 : 0.160001739859581\n",
      "Training loss for batch 8232 : 0.20870032906532288\n",
      "Training loss for batch 8233 : 0.15049731731414795\n",
      "Training loss for batch 8234 : 0.06414259225130081\n",
      "Training loss for batch 8235 : 0.012526353821158409\n",
      "Training loss for batch 8236 : 0.09309917688369751\n",
      "Training loss for batch 8237 : 0.1637989580631256\n",
      "Training loss for batch 8238 : 0.09037443995475769\n",
      "Training loss for batch 8239 : 0.21343454718589783\n",
      "Training loss for batch 8240 : 0.21465256810188293\n",
      "Training loss for batch 8241 : 0.2170686274766922\n",
      "Training loss for batch 8242 : 0.384386271238327\n",
      "Training loss for batch 8243 : 0.17915336787700653\n",
      "Training loss for batch 8244 : 0.20836085081100464\n",
      "Training loss for batch 8245 : 0.06654880940914154\n",
      "Training loss for batch 8246 : 0.2530434727668762\n",
      "Training loss for batch 8247 : 0.029720177873969078\n",
      "Training loss for batch 8248 : 0.19733932614326477\n",
      "Training loss for batch 8249 : 0.06579899787902832\n",
      "Training loss for batch 8250 : 0.46065661311149597\n",
      "Training loss for batch 8251 : 0.0990968644618988\n",
      "Training loss for batch 8252 : 0.325035035610199\n",
      "Training loss for batch 8253 : 0.2612922787666321\n",
      "Training loss for batch 8254 : 0.4606906473636627\n",
      "Training loss for batch 8255 : 0.13686606287956238\n",
      "Training loss for batch 8256 : 0.10035550594329834\n",
      "Training loss for batch 8257 : 0.5142419338226318\n",
      "Training loss for batch 8258 : 0.21588633954524994\n",
      "Training loss for batch 8259 : 0.02876262180507183\n",
      "Training loss for batch 8260 : 0.02368525229394436\n",
      "Training loss for batch 8261 : 0.32268208265304565\n",
      "Training loss for batch 8262 : 0.4029659032821655\n",
      "Training loss for batch 8263 : 0.7582994699478149\n",
      "Training loss for batch 8264 : 0.1932348906993866\n",
      "Training loss for batch 8265 : 0.17574626207351685\n",
      "Training loss for batch 8266 : 0.1818498820066452\n",
      "Training loss for batch 8267 : 0.07969716191291809\n",
      "Training loss for batch 8268 : 0.7709461450576782\n",
      "Training loss for batch 8269 : 0.30545079708099365\n",
      "Training loss for batch 8270 : 0.6282574534416199\n",
      "Training loss for batch 8271 : 0.26030847430229187\n",
      "Training loss for batch 8272 : 0.20819760859012604\n",
      "Training loss for batch 8273 : 0.3967059552669525\n",
      "Training loss for batch 8274 : 0.010948616079986095\n",
      "Training loss for batch 8275 : 0.31678342819213867\n",
      "Training loss for batch 8276 : 0.11699121445417404\n",
      "Training loss for batch 8277 : 0.3423229455947876\n",
      "Training loss for batch 8278 : 0.2658277153968811\n",
      "Training loss for batch 8279 : 0.18743981420993805\n",
      "Training loss for batch 8280 : 0.24731819331645966\n",
      "Training loss for batch 8281 : 0.2802696228027344\n",
      "Training loss for batch 8282 : 0.05856094881892204\n",
      "Training loss for batch 8283 : 0.08097129315137863\n",
      "Training loss for batch 8284 : 0.28660815954208374\n",
      "Training loss for batch 8285 : 0.09993702173233032\n",
      "Training loss for batch 8286 : 0.05701731517910957\n",
      "Training loss for batch 8287 : 0.02352721057832241\n",
      "Training loss for batch 8288 : 0.06224227696657181\n",
      "Training loss for batch 8289 : 0.042225684970617294\n",
      "Training loss for batch 8290 : 0.37733107805252075\n",
      "Training loss for batch 8291 : 0.051651883870363235\n",
      "Training loss for batch 8292 : 0.168030247092247\n",
      "Training loss for batch 8293 : 0.10879050940275192\n",
      "Training loss for batch 8294 : 0.05464029312133789\n",
      "Training loss for batch 8295 : 0.08734557777643204\n",
      "Training loss for batch 8296 : 0.1071704551577568\n",
      "Training loss for batch 8297 : 0.20178265869617462\n",
      "Training loss for batch 8298 : 0.4522630572319031\n",
      "Training loss for batch 8299 : 0.13828504085540771\n",
      "Training loss for batch 8300 : 0.008006302639842033\n",
      "Training loss for batch 8301 : 0.08664044737815857\n",
      "Training loss for batch 8302 : 0.11126785725355148\n",
      "Training loss for batch 8303 : 0.2756728529930115\n",
      "Training loss for batch 8304 : 0.42569100856781006\n",
      "Training loss for batch 8305 : 0.2606234848499298\n",
      "Training loss for batch 8306 : 0.1118774339556694\n",
      "Training loss for batch 8307 : 0.9800529479980469\n",
      "Parameter containing:\n",
      "tensor(-0.0200, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 0.9800397753715515\n",
      "Training loss for batch 1 : 0.9800216555595398\n",
      "Training loss for batch 2 : 0.9799991846084595\n",
      "Training loss for batch 3 : 0.9799731373786926\n",
      "Training loss for batch 4 : 0.9799440503120422\n",
      "Training loss for batch 5 : 0.9799121618270874\n",
      "Training loss for batch 6 : 0.9798780679702759\n",
      "Training loss for batch 7 : 0.9798420667648315\n",
      "Training loss for batch 8 : 0.9798043370246887\n",
      "Training loss for batch 9 : 0.9797653555870056\n",
      "Training loss for batch 10 : 0.9797252416610718\n",
      "Training loss for batch 11 : 0.9796841144561768\n",
      "Training loss for batch 12 : 0.017702152952551842\n",
      "Training loss for batch 13 : 0.2414298802614212\n",
      "Training loss for batch 14 : 0.10151328146457672\n",
      "Training loss for batch 15 : 0.0\n",
      "Training loss for batch 16 : 0.03973405435681343\n",
      "Training loss for batch 17 : 0.08543644845485687\n",
      "Training loss for batch 18 : 0.02833329513669014\n",
      "Training loss for batch 19 : 0.1413191854953766\n",
      "Training loss for batch 20 : 0.11761261522769928\n",
      "Training loss for batch 21 : 0.1005157083272934\n",
      "Training loss for batch 22 : 0.04067619889974594\n",
      "Training loss for batch 23 : 0.06802508234977722\n",
      "Training loss for batch 24 : 0.08850641548633575\n",
      "Training loss for batch 25 : 0.09304620325565338\n",
      "Training loss for batch 26 : 0.141876220703125\n",
      "Training loss for batch 27 : 0.14648398756980896\n",
      "Training loss for batch 28 : 0.48769769072532654\n",
      "Training loss for batch 29 : 0.39467206597328186\n",
      "Training loss for batch 30 : 0.2996182143688202\n",
      "Training loss for batch 31 : 0.35944756865501404\n",
      "Training loss for batch 32 : 0.16438722610473633\n",
      "Training loss for batch 33 : 0.2389860302209854\n",
      "Training loss for batch 34 : 0.4877474308013916\n",
      "Training loss for batch 35 : 0.03369775786995888\n",
      "Training loss for batch 36 : 0.26543939113616943\n",
      "Training loss for batch 37 : 0.15004606544971466\n",
      "Training loss for batch 38 : 0.09809012711048126\n",
      "Training loss for batch 39 : 0.18287907540798187\n",
      "Training loss for batch 40 : 0.08877436071634293\n",
      "Training loss for batch 41 : 0.2986673414707184\n",
      "Training loss for batch 42 : 0.5183398723602295\n",
      "Training loss for batch 43 : 0.07348595559597015\n",
      "Training loss for batch 44 : 0.07310401648283005\n",
      "Training loss for batch 45 : 0.0839705690741539\n",
      "Training loss for batch 46 : 0.30445602536201477\n",
      "Training loss for batch 47 : 0.06876198947429657\n",
      "Training loss for batch 48 : 0.29321882128715515\n",
      "Training loss for batch 49 : 0.11237698793411255\n",
      "Training loss for batch 50 : 0.06229373812675476\n",
      "Training loss for batch 51 : 0.00011565723980311304\n",
      "Training loss for batch 52 : 0.7831907272338867\n",
      "Training loss for batch 53 : 0.09011854976415634\n",
      "Training loss for batch 54 : 0.033518433570861816\n",
      "Training loss for batch 55 : 0.18936118483543396\n",
      "Training loss for batch 56 : 0.23660467565059662\n",
      "Training loss for batch 57 : 0.40803012251853943\n",
      "Training loss for batch 58 : 0.3241753578186035\n",
      "Training loss for batch 59 : 0.0\n",
      "Training loss for batch 60 : 0.3114153742790222\n",
      "Training loss for batch 61 : 0.19884280860424042\n",
      "Training loss for batch 62 : 0.08446025103330612\n",
      "Training loss for batch 63 : 0.17944000661373138\n",
      "Training loss for batch 64 : 0.10597549378871918\n",
      "Training loss for batch 65 : 0.37599852681159973\n",
      "Training loss for batch 66 : 0.2790425717830658\n",
      "Training loss for batch 67 : 0.06021751090884209\n",
      "Training loss for batch 68 : 0.29884618520736694\n",
      "Training loss for batch 69 : 0.38421475887298584\n",
      "Training loss for batch 70 : 0.17604508996009827\n",
      "Training loss for batch 71 : 0.28405481576919556\n",
      "Training loss for batch 72 : 0.2355591356754303\n",
      "Training loss for batch 73 : 0.3465181291103363\n",
      "Training loss for batch 74 : 0.27921757102012634\n",
      "Training loss for batch 75 : 0.011543208733201027\n",
      "Training loss for batch 76 : 0.42482101917266846\n",
      "Training loss for batch 77 : 0.26181653141975403\n",
      "Training loss for batch 78 : 0.15057826042175293\n",
      "Training loss for batch 79 : 0.06975268572568893\n",
      "Training loss for batch 80 : 0.19293761253356934\n",
      "Training loss for batch 81 : 0.07482685893774033\n",
      "Training loss for batch 82 : 0.11956972628831863\n",
      "Training loss for batch 83 : 0.09463365375995636\n",
      "Training loss for batch 84 : 0.1404501348733902\n",
      "Training loss for batch 85 : 0.31855905055999756\n",
      "Training loss for batch 86 : 0.2575806677341461\n",
      "Training loss for batch 87 : 0.3789480924606323\n",
      "Training loss for batch 88 : 0.17400772869586945\n",
      "Training loss for batch 89 : 0.1250533163547516\n",
      "Training loss for batch 90 : 0.32602688670158386\n",
      "Training loss for batch 91 : 0.16039183735847473\n",
      "Training loss for batch 92 : 0.08527834713459015\n",
      "Training loss for batch 93 : 0.316229909658432\n",
      "Training loss for batch 94 : 0.2760739326477051\n",
      "Training loss for batch 95 : 0.04130863398313522\n",
      "Training loss for batch 96 : 0.13155317306518555\n",
      "Training loss for batch 97 : 0.07076595723628998\n",
      "Training loss for batch 98 : 0.3942823112010956\n",
      "Training loss for batch 99 : 0.001956760883331299\n",
      "Training loss for batch 100 : 0.00392189621925354\n",
      "Training loss for batch 101 : 0.6466284990310669\n",
      "Training loss for batch 102 : 0.3041161000728607\n",
      "Training loss for batch 103 : 0.0036857116501778364\n",
      "Training loss for batch 104 : 0.12101560086011887\n",
      "Training loss for batch 105 : 0.07184388488531113\n",
      "Training loss for batch 106 : 0.21416635811328888\n",
      "Training loss for batch 107 : 0.04894715175032616\n",
      "Training loss for batch 108 : 0.3425672650337219\n",
      "Training loss for batch 109 : 0.20365406572818756\n",
      "Training loss for batch 110 : 0.035401541739702225\n",
      "Training loss for batch 111 : 0.2749708294868469\n",
      "Training loss for batch 112 : 0.47465780377388\n",
      "Training loss for batch 113 : 0.2724702060222626\n",
      "Training loss for batch 114 : 0.17630717158317566\n",
      "Training loss for batch 115 : 0.031081130728125572\n",
      "Training loss for batch 116 : 0.10639483481645584\n",
      "Training loss for batch 117 : 0.10260073840618134\n",
      "Training loss for batch 118 : 0.11211642622947693\n",
      "Training loss for batch 119 : 0.030401241034269333\n",
      "Training loss for batch 120 : 0.1132417619228363\n",
      "Training loss for batch 121 : 0.08619308471679688\n",
      "Training loss for batch 122 : 0.3033478260040283\n",
      "Training loss for batch 123 : 0.2849757969379425\n",
      "Training loss for batch 124 : 0.06198723614215851\n",
      "Training loss for batch 125 : 0.036863263696432114\n",
      "Training loss for batch 126 : 0.09934309870004654\n",
      "Training loss for batch 127 : 0.0014118056278675795\n",
      "Training loss for batch 128 : 0.17450912296772003\n",
      "Training loss for batch 129 : 0.029683589935302734\n",
      "Training loss for batch 130 : 0.27520251274108887\n",
      "Training loss for batch 131 : 0.24366435408592224\n",
      "Training loss for batch 132 : 0.03977987915277481\n",
      "Training loss for batch 133 : 0.1841418296098709\n",
      "Training loss for batch 134 : 0.2628943622112274\n",
      "Training loss for batch 135 : 0.24347147345542908\n",
      "Training loss for batch 136 : 0.3551632761955261\n",
      "Training loss for batch 137 : 0.13517244160175323\n",
      "Training loss for batch 138 : 0.47206807136535645\n",
      "Training loss for batch 139 : 0.15738053619861603\n",
      "Training loss for batch 140 : 0.25412923097610474\n",
      "Training loss for batch 141 : 0.09480485320091248\n",
      "Training loss for batch 142 : 0.0415787473320961\n",
      "Training loss for batch 143 : 0.059793345630168915\n",
      "Training loss for batch 144 : 0.2807222902774811\n",
      "Training loss for batch 145 : 0.18608920276165009\n",
      "Training loss for batch 146 : 0.060400836169719696\n",
      "Training loss for batch 147 : 0.1846570074558258\n",
      "Training loss for batch 148 : 0.2256821244955063\n",
      "Training loss for batch 149 : 0.03199265897274017\n",
      "Training loss for batch 150 : 0.09917975962162018\n",
      "Training loss for batch 151 : 0.4407322108745575\n",
      "Training loss for batch 152 : 0.273033082485199\n",
      "Training loss for batch 153 : 0.21566052734851837\n",
      "Training loss for batch 154 : 0.3233993649482727\n",
      "Training loss for batch 155 : 0.12698166072368622\n",
      "Training loss for batch 156 : 0.061092767864465714\n",
      "Training loss for batch 157 : 0.25496309995651245\n",
      "Training loss for batch 158 : 0.2910830080509186\n",
      "Training loss for batch 159 : 0.004898965358734131\n",
      "Training loss for batch 160 : 0.11691247671842575\n",
      "Training loss for batch 161 : 0.1670130491256714\n",
      "Training loss for batch 162 : 0.057435136288404465\n",
      "Training loss for batch 163 : 0.05550075322389603\n",
      "Training loss for batch 164 : 0.0022972922306507826\n",
      "Training loss for batch 165 : 0.19823922216892242\n",
      "Training loss for batch 166 : 0.04756111651659012\n",
      "Training loss for batch 167 : 0.22432537376880646\n",
      "Training loss for batch 168 : 0.028673429042100906\n",
      "Training loss for batch 169 : 0.004258224740624428\n",
      "Training loss for batch 170 : 0.4719220697879791\n",
      "Training loss for batch 171 : 0.3125334084033966\n",
      "Training loss for batch 172 : 0.052889976650476456\n",
      "Training loss for batch 173 : 0.33567014336586\n",
      "Training loss for batch 174 : 0.4225861430168152\n",
      "Training loss for batch 175 : 0.13324858248233795\n",
      "Training loss for batch 176 : 0.008096461184322834\n",
      "Training loss for batch 177 : 0.1059267446398735\n",
      "Training loss for batch 178 : 0.17555956542491913\n",
      "Training loss for batch 179 : 0.0\n",
      "Training loss for batch 180 : 0.167475163936615\n",
      "Training loss for batch 181 : 0.2499549388885498\n",
      "Training loss for batch 182 : 0.14038912951946259\n",
      "Training loss for batch 183 : 0.12939831614494324\n",
      "Training loss for batch 184 : 0.12293921411037445\n",
      "Training loss for batch 185 : 0.27587825059890747\n",
      "Training loss for batch 186 : 0.15975145995616913\n",
      "Training loss for batch 187 : 0.15008507668972015\n",
      "Training loss for batch 188 : 0.21313925087451935\n",
      "Training loss for batch 189 : 0.39275142550468445\n",
      "Training loss for batch 190 : 0.2660418748855591\n",
      "Training loss for batch 191 : 0.2960633635520935\n",
      "Training loss for batch 192 : 0.15340149402618408\n",
      "Training loss for batch 193 : 0.2848775386810303\n",
      "Training loss for batch 194 : 0.5582420229911804\n",
      "Training loss for batch 195 : 0.0579846166074276\n",
      "Training loss for batch 196 : 0.2596903145313263\n",
      "Training loss for batch 197 : 0.05907103419303894\n",
      "Training loss for batch 198 : 0.034683216363191605\n",
      "Training loss for batch 199 : 0.17491576075553894\n",
      "Training loss for batch 200 : 0.06446928530931473\n",
      "Training loss for batch 201 : 0.01616733707487583\n",
      "Training loss for batch 202 : 0.22437651455402374\n",
      "Training loss for batch 203 : 0.17497554421424866\n",
      "Training loss for batch 204 : 0.015146918594837189\n",
      "Training loss for batch 205 : 0.13918110728263855\n",
      "Training loss for batch 206 : 0.33196377754211426\n",
      "Training loss for batch 207 : -0.0002655997814144939\n",
      "Training loss for batch 208 : 0.05121965333819389\n",
      "Training loss for batch 209 : 0.14690987765789032\n",
      "Training loss for batch 210 : 0.14952901005744934\n",
      "Training loss for batch 211 : 0.04378442093729973\n",
      "Training loss for batch 212 : 0.19495859742164612\n",
      "Training loss for batch 213 : 0.030478887259960175\n",
      "Training loss for batch 214 : 0.03957337886095047\n",
      "Training loss for batch 215 : 0.059415917843580246\n",
      "Training loss for batch 216 : 0.5515410900115967\n",
      "Training loss for batch 217 : 0.20780637860298157\n",
      "Training loss for batch 218 : 0.2303657978773117\n",
      "Training loss for batch 219 : 0.2944670021533966\n",
      "Training loss for batch 220 : 0.19304564595222473\n",
      "Training loss for batch 221 : 0.09679629653692245\n",
      "Training loss for batch 222 : 0.029449889436364174\n",
      "Training loss for batch 223 : 0.03561204671859741\n",
      "Training loss for batch 224 : 0.28125056624412537\n",
      "Training loss for batch 225 : 0.303947776556015\n",
      "Training loss for batch 226 : 0.0999276265501976\n",
      "Training loss for batch 227 : 0.43959689140319824\n",
      "Training loss for batch 228 : 0.07043582201004028\n",
      "Training loss for batch 229 : 0.02096111886203289\n",
      "Training loss for batch 230 : 0.1832401156425476\n",
      "Training loss for batch 231 : 0.28681686520576477\n",
      "Training loss for batch 232 : 0.06266749650239944\n",
      "Training loss for batch 233 : 0.08543703705072403\n",
      "Training loss for batch 234 : 0.33600854873657227\n",
      "Training loss for batch 235 : 0.2114487737417221\n",
      "Training loss for batch 236 : 0.15534871816635132\n",
      "Training loss for batch 237 : 0.10603707283735275\n",
      "Training loss for batch 238 : 0.024028748273849487\n",
      "Training loss for batch 239 : 0.11670345813035965\n",
      "Training loss for batch 240 : 0.4787687659263611\n",
      "Training loss for batch 241 : 0.16934385895729065\n",
      "Training loss for batch 242 : 0.31471580266952515\n",
      "Training loss for batch 243 : 0.12364600598812103\n",
      "Training loss for batch 244 : 0.36579516530036926\n",
      "Training loss for batch 245 : 0.20636412501335144\n",
      "Training loss for batch 246 : 0.2519356608390808\n",
      "Training loss for batch 247 : 0.1366899013519287\n",
      "Training loss for batch 248 : -3.961742186220363e-05\n",
      "Training loss for batch 249 : 0.09347625821828842\n",
      "Training loss for batch 250 : 0.1970551460981369\n",
      "Training loss for batch 251 : 0.40790578722953796\n",
      "Training loss for batch 252 : 0.13599827885627747\n",
      "Training loss for batch 253 : 0.22074908018112183\n",
      "Training loss for batch 254 : 0.22633813321590424\n",
      "Training loss for batch 255 : 0.1581752449274063\n",
      "Training loss for batch 256 : 0.09608816355466843\n",
      "Training loss for batch 257 : 0.143397256731987\n",
      "Training loss for batch 258 : 0.136644184589386\n",
      "Training loss for batch 259 : 0.1602642983198166\n",
      "Training loss for batch 260 : 0.1604747772216797\n",
      "Training loss for batch 261 : 0.5343813300132751\n",
      "Training loss for batch 262 : 0.0\n",
      "Training loss for batch 263 : 0.07554648071527481\n",
      "Training loss for batch 264 : 0.19975313544273376\n",
      "Training loss for batch 265 : 0.06248122453689575\n",
      "Training loss for batch 266 : 0.18982797861099243\n",
      "Training loss for batch 267 : 0.21001185476779938\n",
      "Training loss for batch 268 : 0.4101825952529907\n",
      "Training loss for batch 269 : 0.15435902774333954\n",
      "Training loss for batch 270 : 0.002338893711566925\n",
      "Training loss for batch 271 : 0.28227153420448303\n",
      "Training loss for batch 272 : 0.025978384539484978\n",
      "Training loss for batch 273 : 0.22588549554347992\n",
      "Training loss for batch 274 : 0.09117500483989716\n",
      "Training loss for batch 275 : 0.36483633518218994\n",
      "Training loss for batch 276 : 0.07442070543766022\n",
      "Training loss for batch 277 : 0.47915661334991455\n",
      "Training loss for batch 278 : 0.5514382719993591\n",
      "Training loss for batch 279 : 0.19426769018173218\n",
      "Training loss for batch 280 : 0.09945321828126907\n",
      "Training loss for batch 281 : 0.0857611820101738\n",
      "Training loss for batch 282 : 0.14719295501708984\n",
      "Training loss for batch 283 : 0.2605619430541992\n",
      "Training loss for batch 284 : 0.2825556695461273\n",
      "Training loss for batch 285 : 0.19365017116069794\n",
      "Training loss for batch 286 : 0.25514087080955505\n",
      "Training loss for batch 287 : 0.5137035250663757\n",
      "Training loss for batch 288 : -0.00015813560457900167\n",
      "Training loss for batch 289 : 0.11192058026790619\n",
      "Training loss for batch 290 : 0.28854504227638245\n",
      "Training loss for batch 291 : 0.0\n",
      "Training loss for batch 292 : 0.3060718774795532\n",
      "Training loss for batch 293 : 0.23347114026546478\n",
      "Training loss for batch 294 : 0.26976966857910156\n",
      "Training loss for batch 295 : 0.25390496850013733\n",
      "Training loss for batch 296 : 0.11529236286878586\n",
      "Training loss for batch 297 : 0.014666120521724224\n",
      "Training loss for batch 298 : 0.17509284615516663\n",
      "Training loss for batch 299 : 0.10645779967308044\n",
      "Training loss for batch 300 : 0.41507071256637573\n",
      "Training loss for batch 301 : 0.053518008440732956\n",
      "Training loss for batch 302 : 0.25361624360084534\n",
      "Training loss for batch 303 : 0.12171238660812378\n",
      "Training loss for batch 304 : 0.22237692773342133\n",
      "Training loss for batch 305 : 0.06443876773118973\n",
      "Training loss for batch 306 : 0.26071837544441223\n",
      "Training loss for batch 307 : 0.2568104565143585\n",
      "Training loss for batch 308 : 0.03705978021025658\n",
      "Training loss for batch 309 : 0.3066313862800598\n",
      "Training loss for batch 310 : 0.052186913788318634\n",
      "Training loss for batch 311 : 0.3271230459213257\n",
      "Training loss for batch 312 : 0.09045536816120148\n",
      "Training loss for batch 313 : 0.17784319818019867\n",
      "Training loss for batch 314 : 0.1489218771457672\n",
      "Training loss for batch 315 : 0.017150556668639183\n",
      "Training loss for batch 316 : 0.15155813097953796\n",
      "Training loss for batch 317 : 0.3502059280872345\n",
      "Training loss for batch 318 : 0.030699770897626877\n",
      "Training loss for batch 319 : 0.09959134459495544\n",
      "Training loss for batch 320 : 0.27299246191978455\n",
      "Training loss for batch 321 : 0.27037256956100464\n",
      "Training loss for batch 322 : 0.24224910140037537\n",
      "Training loss for batch 323 : 0.19668392837047577\n",
      "Training loss for batch 324 : 0.21068088710308075\n",
      "Training loss for batch 325 : 0.09988673776388168\n",
      "Training loss for batch 326 : 0.3374585509300232\n",
      "Training loss for batch 327 : 0.3626581132411957\n",
      "Training loss for batch 328 : 0.34485572576522827\n",
      "Training loss for batch 329 : 0.09571226686239243\n",
      "Training loss for batch 330 : 0.10610028356313705\n",
      "Training loss for batch 331 : 0.5592744946479797\n",
      "Training loss for batch 332 : 0.2549790143966675\n",
      "Training loss for batch 333 : 0.27453914284706116\n",
      "Training loss for batch 334 : 0.19179533421993256\n",
      "Training loss for batch 335 : 0.04797524958848953\n",
      "Training loss for batch 336 : 0.08376765996217728\n",
      "Training loss for batch 337 : 0.12983494997024536\n",
      "Training loss for batch 338 : 0.15654568374156952\n",
      "Training loss for batch 339 : 0.1515294909477234\n",
      "Training loss for batch 340 : 0.3249438405036926\n",
      "Training loss for batch 341 : 0.19386252760887146\n",
      "Training loss for batch 342 : 0.15954916179180145\n",
      "Training loss for batch 343 : 0.0173361673951149\n",
      "Training loss for batch 344 : 0.34393084049224854\n",
      "Training loss for batch 345 : 0.30302900075912476\n",
      "Training loss for batch 346 : 0.2440871149301529\n",
      "Training loss for batch 347 : 0.1740311235189438\n",
      "Training loss for batch 348 : 0.33226433396339417\n",
      "Training loss for batch 349 : 0.12120140343904495\n",
      "Training loss for batch 350 : 0.1557326465845108\n",
      "Training loss for batch 351 : 0.06081371009349823\n",
      "Training loss for batch 352 : 0.08625201880931854\n",
      "Training loss for batch 353 : 0.09267789870500565\n",
      "Training loss for batch 354 : 0.2977657914161682\n",
      "Training loss for batch 355 : 0.23243719339370728\n",
      "Training loss for batch 356 : 0.24057206511497498\n",
      "Training loss for batch 357 : 0.2904805839061737\n",
      "Training loss for batch 358 : 0.06202760338783264\n",
      "Training loss for batch 359 : 0.45835191011428833\n",
      "Training loss for batch 360 : 0.032867129892110825\n",
      "Training loss for batch 361 : 0.3014337718486786\n",
      "Training loss for batch 362 : 0.17565810680389404\n",
      "Training loss for batch 363 : 0.10487833619117737\n",
      "Training loss for batch 364 : 0.21003323793411255\n",
      "Training loss for batch 365 : 0.3140281140804291\n",
      "Training loss for batch 366 : 0.02960178442299366\n",
      "Training loss for batch 367 : 0.07416633516550064\n",
      "Training loss for batch 368 : 0.15016786754131317\n",
      "Training loss for batch 369 : 0.13434910774230957\n",
      "Training loss for batch 370 : 0.07215683907270432\n",
      "Training loss for batch 371 : 0.0986471176147461\n",
      "Training loss for batch 372 : 0.13536453247070312\n",
      "Training loss for batch 373 : 0.07872661203145981\n",
      "Training loss for batch 374 : 0.2731345295906067\n",
      "Training loss for batch 375 : 0.4526884853839874\n",
      "Training loss for batch 376 : 0.11066554486751556\n",
      "Training loss for batch 377 : 0.2262040376663208\n",
      "Training loss for batch 378 : 0.0495268814265728\n",
      "Training loss for batch 379 : 0.1508009284734726\n",
      "Training loss for batch 380 : 0.09341397881507874\n",
      "Training loss for batch 381 : 0.1925426721572876\n",
      "Training loss for batch 382 : 0.10721242427825928\n",
      "Training loss for batch 383 : 0.10736770927906036\n",
      "Training loss for batch 384 : 0.08148667961359024\n",
      "Training loss for batch 385 : 0.20764152705669403\n",
      "Training loss for batch 386 : 0.23837491869926453\n",
      "Training loss for batch 387 : 0.33979490399360657\n",
      "Training loss for batch 388 : 0.16234758496284485\n",
      "Training loss for batch 389 : 0.13767892122268677\n",
      "Training loss for batch 390 : 0.24963907897472382\n",
      "Training loss for batch 391 : 0.09183850884437561\n",
      "Training loss for batch 392 : 0.3415404260158539\n",
      "Training loss for batch 393 : 0.20904241502285004\n",
      "Training loss for batch 394 : 0.0922367125749588\n",
      "Training loss for batch 395 : 0.24419574439525604\n",
      "Training loss for batch 396 : 0.18048028647899628\n",
      "Training loss for batch 397 : 0.24668066203594208\n",
      "Training loss for batch 398 : 0.1721593737602234\n",
      "Training loss for batch 399 : 0.044116076081991196\n",
      "Training loss for batch 400 : 0.16955359280109406\n",
      "Training loss for batch 401 : 0.06878204643726349\n",
      "Training loss for batch 402 : 0.22645814716815948\n",
      "Training loss for batch 403 : 0.14415760338306427\n",
      "Training loss for batch 404 : 0.3487666845321655\n",
      "Training loss for batch 405 : 0.18569140136241913\n",
      "Training loss for batch 406 : 0.41189029812812805\n",
      "Training loss for batch 407 : 0.08185306191444397\n",
      "Training loss for batch 408 : 0.04110619053244591\n",
      "Training loss for batch 409 : 0.0058094775304198265\n",
      "Training loss for batch 410 : 0.4022562801837921\n",
      "Training loss for batch 411 : 0.12197843194007874\n",
      "Training loss for batch 412 : 0.2931525707244873\n",
      "Training loss for batch 413 : 0.30649739503860474\n",
      "Training loss for batch 414 : 0.31718748807907104\n",
      "Training loss for batch 415 : 0.1042054295539856\n",
      "Training loss for batch 416 : 0.04591534286737442\n",
      "Training loss for batch 417 : 0.22531914710998535\n",
      "Training loss for batch 418 : 0.28075501322746277\n",
      "Training loss for batch 419 : 0.020121216773986816\n",
      "Training loss for batch 420 : 0.3221648931503296\n",
      "Training loss for batch 421 : 0.2940174341201782\n",
      "Training loss for batch 422 : 0.0792371928691864\n",
      "Training loss for batch 423 : 0.25908365845680237\n",
      "Training loss for batch 424 : 0.3481521010398865\n",
      "Training loss for batch 425 : 0.1374913901090622\n",
      "Training loss for batch 426 : 0.09607679396867752\n",
      "Training loss for batch 427 : 0.00420322734862566\n",
      "Training loss for batch 428 : 0.024398384615778923\n",
      "Training loss for batch 429 : 0.11797527223825455\n",
      "Training loss for batch 430 : 0.19864560663700104\n",
      "Training loss for batch 431 : 0.29309216141700745\n",
      "Training loss for batch 432 : 0.3070659935474396\n",
      "Training loss for batch 433 : 0.26499587297439575\n",
      "Training loss for batch 434 : 0.09832238405942917\n",
      "Training loss for batch 435 : 0.13102158904075623\n",
      "Training loss for batch 436 : 0.12081681191921234\n",
      "Training loss for batch 437 : 0.3144496977329254\n",
      "Training loss for batch 438 : 0.059805139899253845\n",
      "Training loss for batch 439 : 0.12926137447357178\n",
      "Training loss for batch 440 : 0.07723872363567352\n",
      "Training loss for batch 441 : 0.031267791986465454\n",
      "Training loss for batch 442 : 0.3010866045951843\n",
      "Training loss for batch 443 : 0.32272011041641235\n",
      "Training loss for batch 444 : 0.03332957997918129\n",
      "Training loss for batch 445 : 0.18743698298931122\n",
      "Training loss for batch 446 : 0.11522098630666733\n",
      "Training loss for batch 447 : 0.11295817047357559\n",
      "Training loss for batch 448 : 0.588140606880188\n",
      "Training loss for batch 449 : 0.07543615251779556\n",
      "Training loss for batch 450 : 0.045759331434965134\n",
      "Training loss for batch 451 : 0.07748987525701523\n",
      "Training loss for batch 452 : 0.4698637127876282\n",
      "Training loss for batch 453 : 0.0539361834526062\n",
      "Training loss for batch 454 : 0.48220908641815186\n",
      "Training loss for batch 455 : 0.27001839876174927\n",
      "Training loss for batch 456 : 0.2946336269378662\n",
      "Training loss for batch 457 : 0.0889001190662384\n",
      "Training loss for batch 458 : 0.06367289274930954\n",
      "Training loss for batch 459 : 0.29874730110168457\n",
      "Training loss for batch 460 : 0.1630563586950302\n",
      "Training loss for batch 461 : 0.2528359293937683\n",
      "Training loss for batch 462 : 0.07737304270267487\n",
      "Training loss for batch 463 : 0.03695610910654068\n",
      "Training loss for batch 464 : 0.06373845040798187\n",
      "Training loss for batch 465 : 0.2071208655834198\n",
      "Training loss for batch 466 : 0.10485570132732391\n",
      "Training loss for batch 467 : 0.23682019114494324\n",
      "Training loss for batch 468 : 0.30212974548339844\n",
      "Training loss for batch 469 : 0.573573112487793\n",
      "Training loss for batch 470 : 0.05512772873044014\n",
      "Training loss for batch 471 : 0.4724010229110718\n",
      "Training loss for batch 472 : 0.009000926278531551\n",
      "Training loss for batch 473 : 0.2336442768573761\n",
      "Training loss for batch 474 : 0.17331825196743011\n",
      "Training loss for batch 475 : 0.2025570422410965\n",
      "Training loss for batch 476 : 0.23738032579421997\n",
      "Training loss for batch 477 : 0.5462460517883301\n",
      "Training loss for batch 478 : 0.43485596776008606\n",
      "Training loss for batch 479 : 0.17120012640953064\n",
      "Training loss for batch 480 : 0.3641417920589447\n",
      "Training loss for batch 481 : 0.1214827224612236\n",
      "Training loss for batch 482 : 0.23825420439243317\n",
      "Training loss for batch 483 : 0.28811532258987427\n",
      "Training loss for batch 484 : 0.04670102521777153\n",
      "Training loss for batch 485 : 0.1719745397567749\n",
      "Training loss for batch 486 : 0.2810809910297394\n",
      "Training loss for batch 487 : 0.1836189180612564\n",
      "Training loss for batch 488 : 0.024761339649558067\n",
      "Training loss for batch 489 : 0.2616063356399536\n",
      "Training loss for batch 490 : 0.1823330521583557\n",
      "Training loss for batch 491 : 0.027241798117756844\n",
      "Training loss for batch 492 : 0.07848116010427475\n",
      "Training loss for batch 493 : 0.011777706444263458\n",
      "Training loss for batch 494 : 0.3693208694458008\n",
      "Training loss for batch 495 : 0.13714145123958588\n",
      "Training loss for batch 496 : 0.12116850912570953\n",
      "Training loss for batch 497 : 0.03616074472665787\n",
      "Training loss for batch 498 : 0.10013257712125778\n",
      "Training loss for batch 499 : 0.14683455228805542\n",
      "Training loss for batch 500 : 0.013856391422450542\n",
      "Training loss for batch 501 : 0.26960402727127075\n",
      "Training loss for batch 502 : 0.24769634008407593\n",
      "Training loss for batch 503 : 0.19024038314819336\n",
      "Training loss for batch 504 : 0.12361498177051544\n",
      "Training loss for batch 505 : 0.05166798084974289\n",
      "Training loss for batch 506 : 0.18687985837459564\n",
      "Training loss for batch 507 : 0.20147450268268585\n",
      "Training loss for batch 508 : 0.09016072005033493\n",
      "Training loss for batch 509 : 0.11339669674634933\n",
      "Training loss for batch 510 : 0.04323902353644371\n",
      "Training loss for batch 511 : 0.03130155801773071\n",
      "Training loss for batch 512 : 0.07819248735904694\n",
      "Training loss for batch 513 : 0.26914864778518677\n",
      "Training loss for batch 514 : 0.5067081451416016\n",
      "Training loss for batch 515 : 0.07413232326507568\n",
      "Training loss for batch 516 : 0.06118559092283249\n",
      "Training loss for batch 517 : 0.23716150224208832\n",
      "Training loss for batch 518 : 0.08159606158733368\n",
      "Training loss for batch 519 : 0.11196190863847733\n",
      "Training loss for batch 520 : 0.027619045227766037\n",
      "Training loss for batch 521 : 0.3350239396095276\n",
      "Training loss for batch 522 : 0.3616596758365631\n",
      "Training loss for batch 523 : 0.16390937566757202\n",
      "Training loss for batch 524 : 0.14438261091709137\n",
      "Training loss for batch 525 : 0.0419282428920269\n",
      "Training loss for batch 526 : 0.16859054565429688\n",
      "Training loss for batch 527 : 0.02675234153866768\n",
      "Training loss for batch 528 : 0.31389519572257996\n",
      "Training loss for batch 529 : 0.5518665909767151\n",
      "Training loss for batch 530 : 0.44664838910102844\n",
      "Training loss for batch 531 : 0.3805137872695923\n",
      "Training loss for batch 532 : 0.3738754093647003\n",
      "Training loss for batch 533 : 0.2297041118144989\n",
      "Training loss for batch 534 : 0.3384273052215576\n",
      "Training loss for batch 535 : 0.3472810387611389\n",
      "Training loss for batch 536 : 0.043568048626184464\n",
      "Training loss for batch 537 : 0.28703221678733826\n",
      "Training loss for batch 538 : 0.10621783137321472\n",
      "Training loss for batch 539 : 0.17539693415164948\n",
      "Training loss for batch 540 : 0.28105637431144714\n",
      "Training loss for batch 541 : 0.20124366879463196\n",
      "Training loss for batch 542 : 0.43028542399406433\n",
      "Training loss for batch 543 : 0.03560461848974228\n",
      "Training loss for batch 544 : 0.33027780055999756\n",
      "Training loss for batch 545 : 0.046475015580654144\n",
      "Training loss for batch 546 : 0.08902434259653091\n",
      "Training loss for batch 547 : 0.05104720592498779\n",
      "Training loss for batch 548 : 0.053812842816114426\n",
      "Training loss for batch 549 : 0.22123701870441437\n",
      "Training loss for batch 550 : 0.28775832056999207\n",
      "Training loss for batch 551 : 0.316001296043396\n",
      "Training loss for batch 552 : 0.08454018086194992\n",
      "Training loss for batch 553 : 0.23122373223304749\n",
      "Training loss for batch 554 : 0.3270584046840668\n",
      "Training loss for batch 555 : 0.23804473876953125\n",
      "Training loss for batch 556 : 0.21167632937431335\n",
      "Training loss for batch 557 : 0.4982842206954956\n",
      "Training loss for batch 558 : 0.08917321264743805\n",
      "Training loss for batch 559 : 0.21823830902576447\n",
      "Training loss for batch 560 : 0.0439075231552124\n",
      "Training loss for batch 561 : 0.02605113759636879\n",
      "Training loss for batch 562 : 0.22613216936588287\n",
      "Training loss for batch 563 : 0.2909373641014099\n",
      "Training loss for batch 564 : 0.32036489248275757\n",
      "Training loss for batch 565 : 0.4325585663318634\n",
      "Training loss for batch 566 : 0.18110023438930511\n",
      "Training loss for batch 567 : 0.1732199341058731\n",
      "Training loss for batch 568 : 0.34508436918258667\n",
      "Training loss for batch 569 : 0.1842561513185501\n",
      "Training loss for batch 570 : 0.15017229318618774\n",
      "Training loss for batch 571 : 0.1514720618724823\n",
      "Training loss for batch 572 : 0.5222284197807312\n",
      "Training loss for batch 573 : 0.0\n",
      "Training loss for batch 574 : 0.49747025966644287\n",
      "Training loss for batch 575 : 0.41817665100097656\n",
      "Training loss for batch 576 : 0.0028750975616276264\n",
      "Training loss for batch 577 : 0.3545909523963928\n",
      "Training loss for batch 578 : 0.545629620552063\n",
      "Training loss for batch 579 : 0.019570868462324142\n",
      "Training loss for batch 580 : 0.24177785217761993\n",
      "Training loss for batch 581 : 0.22712799906730652\n",
      "Training loss for batch 582 : 0.14753559231758118\n",
      "Training loss for batch 583 : 0.15392160415649414\n",
      "Training loss for batch 584 : 0.041038814932107925\n",
      "Training loss for batch 585 : 0.37875014543533325\n",
      "Training loss for batch 586 : 0.06173902377486229\n",
      "Training loss for batch 587 : 0.1249387338757515\n",
      "Training loss for batch 588 : 0.2272236943244934\n",
      "Training loss for batch 589 : 0.3113327920436859\n",
      "Training loss for batch 590 : 0.314066082239151\n",
      "Training loss for batch 591 : 0.07211624830961227\n",
      "Training loss for batch 592 : 0.3236358165740967\n",
      "Training loss for batch 593 : 0.4782261252403259\n",
      "Training loss for batch 594 : 0.34485694766044617\n",
      "Training loss for batch 595 : 0.3293164074420929\n",
      "Training loss for batch 596 : 0.13396970927715302\n",
      "Training loss for batch 597 : 0.26220765709877014\n",
      "Training loss for batch 598 : 0.10661584138870239\n",
      "Training loss for batch 599 : 0.13165533542633057\n",
      "Training loss for batch 600 : 0.24907176196575165\n",
      "Training loss for batch 601 : 0.14505940675735474\n",
      "Training loss for batch 602 : 0.03952658176422119\n",
      "Training loss for batch 603 : 0.17947164177894592\n",
      "Training loss for batch 604 : 0.29041510820388794\n",
      "Training loss for batch 605 : 0.1850738674402237\n",
      "Training loss for batch 606 : 0.32611289620399475\n",
      "Training loss for batch 607 : 0.011308373883366585\n",
      "Training loss for batch 608 : 0.08632887154817581\n",
      "Training loss for batch 609 : 0.1965186595916748\n",
      "Training loss for batch 610 : 0.2024032473564148\n",
      "Training loss for batch 611 : 0.116568423807621\n",
      "Training loss for batch 612 : 0.1546519696712494\n",
      "Training loss for batch 613 : 0.029380623251199722\n",
      "Training loss for batch 614 : 0.2130945920944214\n",
      "Training loss for batch 615 : 0.24842435121536255\n",
      "Training loss for batch 616 : 0.11618291586637497\n",
      "Training loss for batch 617 : 0.2729794979095459\n",
      "Training loss for batch 618 : 0.12415380775928497\n",
      "Training loss for batch 619 : 0.11819629371166229\n",
      "Training loss for batch 620 : 0.0703158974647522\n",
      "Training loss for batch 621 : 0.21975834667682648\n",
      "Training loss for batch 622 : 0.3205280900001526\n",
      "Training loss for batch 623 : 0.1658739149570465\n",
      "Training loss for batch 624 : 0.538323163986206\n",
      "Training loss for batch 625 : 0.24358569085597992\n",
      "Training loss for batch 626 : 0.06781189888715744\n",
      "Training loss for batch 627 : 0.21288995444774628\n",
      "Training loss for batch 628 : 0.052630797028541565\n",
      "Training loss for batch 629 : 0.3021427392959595\n",
      "Training loss for batch 630 : 0.11717821657657623\n",
      "Training loss for batch 631 : 0.14184921979904175\n",
      "Training loss for batch 632 : 0.051147349178791046\n",
      "Training loss for batch 633 : 0.08087700605392456\n",
      "Training loss for batch 634 : 0.07596710324287415\n",
      "Training loss for batch 635 : 0.15257874131202698\n",
      "Training loss for batch 636 : 0.06799806654453278\n",
      "Training loss for batch 637 : 0.1233692616224289\n",
      "Training loss for batch 638 : 0.13231870532035828\n",
      "Training loss for batch 639 : 0.12759314477443695\n",
      "Training loss for batch 640 : 0.5610750913619995\n",
      "Training loss for batch 641 : 0.14789226651191711\n",
      "Training loss for batch 642 : 0.17978185415267944\n",
      "Training loss for batch 643 : 0.01043610367923975\n",
      "Training loss for batch 644 : 0.10771419107913971\n",
      "Training loss for batch 645 : 0.20489174127578735\n",
      "Training loss for batch 646 : 0.408526748418808\n",
      "Training loss for batch 647 : 0.14820781350135803\n",
      "Training loss for batch 648 : 0.09082172065973282\n",
      "Training loss for batch 649 : 0.3789532482624054\n",
      "Training loss for batch 650 : 0.4578455686569214\n",
      "Training loss for batch 651 : 0.19794680178165436\n",
      "Training loss for batch 652 : 0.15520955622196198\n",
      "Training loss for batch 653 : 0.027278732508420944\n",
      "Training loss for batch 654 : 0.5322988629341125\n",
      "Training loss for batch 655 : 0.01589568518102169\n",
      "Training loss for batch 656 : 0.275418221950531\n",
      "Training loss for batch 657 : 0.05774315819144249\n",
      "Training loss for batch 658 : 0.3216908872127533\n",
      "Training loss for batch 659 : 0.4628176987171173\n",
      "Training loss for batch 660 : 0.13193705677986145\n",
      "Training loss for batch 661 : 0.040627531707286835\n",
      "Training loss for batch 662 : 0.16676193475723267\n",
      "Training loss for batch 663 : 0.14268755912780762\n",
      "Training loss for batch 664 : 0.3291991651058197\n",
      "Training loss for batch 665 : 0.5839337110519409\n",
      "Training loss for batch 666 : 0.07359113544225693\n",
      "Training loss for batch 667 : 0.1936713457107544\n",
      "Training loss for batch 668 : 0.20189538598060608\n",
      "Training loss for batch 669 : 0.09808073192834854\n",
      "Training loss for batch 670 : 0.09528140723705292\n",
      "Training loss for batch 671 : 0.14159579575061798\n",
      "Training loss for batch 672 : 0.12797237932682037\n",
      "Training loss for batch 673 : 0.1253892481327057\n",
      "Training loss for batch 674 : 0.06282339245080948\n",
      "Training loss for batch 675 : 0.024324459955096245\n",
      "Training loss for batch 676 : 0.2892352044582367\n",
      "Training loss for batch 677 : 0.3235120475292206\n",
      "Training loss for batch 678 : 0.2220413237810135\n",
      "Training loss for batch 679 : 0.040109843015670776\n",
      "Training loss for batch 680 : 0.2581627368927002\n",
      "Training loss for batch 681 : 0.08685961365699768\n",
      "Training loss for batch 682 : 0.06246829405426979\n",
      "Training loss for batch 683 : 0.28421640396118164\n",
      "Training loss for batch 684 : 0.09777440130710602\n",
      "Training loss for batch 685 : 0.04158574342727661\n",
      "Training loss for batch 686 : 0.2779853343963623\n",
      "Training loss for batch 687 : 0.2701915502548218\n",
      "Training loss for batch 688 : 0.06258434057235718\n",
      "Training loss for batch 689 : 0.08474399149417877\n",
      "Training loss for batch 690 : 0.1419907808303833\n",
      "Training loss for batch 691 : 0.15242110192775726\n",
      "Training loss for batch 692 : 0.3849039673805237\n",
      "Training loss for batch 693 : 0.01783321052789688\n",
      "Training loss for batch 694 : 0.04055517166852951\n",
      "Training loss for batch 695 : 0.23547303676605225\n",
      "Training loss for batch 696 : 0.027201861143112183\n",
      "Training loss for batch 697 : 0.16431817412376404\n",
      "Training loss for batch 698 : 0.18481236696243286\n",
      "Training loss for batch 699 : 0.04076573997735977\n",
      "Training loss for batch 700 : 0.22776207327842712\n",
      "Training loss for batch 701 : 0.2132565677165985\n",
      "Training loss for batch 702 : 0.3063044548034668\n",
      "Training loss for batch 703 : 0.1758188158273697\n",
      "Training loss for batch 704 : 0.13804814219474792\n",
      "Training loss for batch 705 : 0.12422772496938705\n",
      "Training loss for batch 706 : 0.22029462456703186\n",
      "Training loss for batch 707 : 0.07980584353208542\n",
      "Training loss for batch 708 : 0.2997385561466217\n",
      "Training loss for batch 709 : 0.12078816443681717\n",
      "Training loss for batch 710 : 0.0120060034096241\n",
      "Training loss for batch 711 : 0.3981553316116333\n",
      "Training loss for batch 712 : 0.3561573624610901\n",
      "Training loss for batch 713 : 0.0033178129233419895\n",
      "Training loss for batch 714 : 0.48526546359062195\n",
      "Training loss for batch 715 : 0.24276162683963776\n",
      "Training loss for batch 716 : 0.04423299431800842\n",
      "Training loss for batch 717 : 0.2379171997308731\n",
      "Training loss for batch 718 : 0.20520256459712982\n",
      "Training loss for batch 719 : 0.19759367406368256\n",
      "Training loss for batch 720 : 0.19093139469623566\n",
      "Training loss for batch 721 : 0.0036241819616407156\n",
      "Training loss for batch 722 : 0.28027141094207764\n",
      "Training loss for batch 723 : 0.2840389907360077\n",
      "Training loss for batch 724 : 0.02381124161183834\n",
      "Training loss for batch 725 : 0.2768702805042267\n",
      "Training loss for batch 726 : 0.08329163491725922\n",
      "Training loss for batch 727 : 0.1486406773328781\n",
      "Training loss for batch 728 : 0.2910042703151703\n",
      "Training loss for batch 729 : 0.39054518938064575\n",
      "Training loss for batch 730 : 0.09976457059383392\n",
      "Training loss for batch 731 : 0.30077171325683594\n",
      "Training loss for batch 732 : 0.1663917750120163\n",
      "Training loss for batch 733 : 0.24876520037651062\n",
      "Training loss for batch 734 : 0.35201603174209595\n",
      "Training loss for batch 735 : 0.36205726861953735\n",
      "Training loss for batch 736 : 0.0\n",
      "Training loss for batch 737 : 0.43953588604927063\n",
      "Training loss for batch 738 : 0.0568205900490284\n",
      "Training loss for batch 739 : 0.17944224178791046\n",
      "Training loss for batch 740 : 0.06352870166301727\n",
      "Training loss for batch 741 : 0.06848721206188202\n",
      "Training loss for batch 742 : 0.07202205061912537\n",
      "Training loss for batch 743 : 0.14598147571086884\n",
      "Training loss for batch 744 : 0.0693979561328888\n",
      "Training loss for batch 745 : 0.13855190575122833\n",
      "Training loss for batch 746 : 0.3849509358406067\n",
      "Training loss for batch 747 : 0.17460395395755768\n",
      "Training loss for batch 748 : 0.05045246332883835\n",
      "Training loss for batch 749 : 0.21388928592205048\n",
      "Training loss for batch 750 : 0.1511101871728897\n",
      "Training loss for batch 751 : 0.06890644133090973\n",
      "Training loss for batch 752 : 0.12602029740810394\n",
      "Training loss for batch 753 : 0.15559209883213043\n",
      "Training loss for batch 754 : 0.34676095843315125\n",
      "Training loss for batch 755 : 0.2825334668159485\n",
      "Training loss for batch 756 : 0.10156971961259842\n",
      "Training loss for batch 757 : 0.19498758018016815\n",
      "Training loss for batch 758 : 0.2991808354854584\n",
      "Training loss for batch 759 : 0.018105626106262207\n",
      "Training loss for batch 760 : 0.3263150751590729\n",
      "Training loss for batch 761 : 0.15025338530540466\n",
      "Training loss for batch 762 : 0.6224163770675659\n",
      "Training loss for batch 763 : 0.09938839077949524\n",
      "Training loss for batch 764 : 0.22226707637310028\n",
      "Training loss for batch 765 : 0.259161114692688\n",
      "Training loss for batch 766 : 0.034740597009658813\n",
      "Training loss for batch 767 : 0.022115303203463554\n",
      "Training loss for batch 768 : 0.19250668585300446\n",
      "Training loss for batch 769 : 0.2828298509120941\n",
      "Training loss for batch 770 : 0.022630073130130768\n",
      "Training loss for batch 771 : 0.4096875488758087\n",
      "Training loss for batch 772 : 0.13662628829479218\n",
      "Training loss for batch 773 : 0.05915173143148422\n",
      "Training loss for batch 774 : 0.12156639248132706\n",
      "Training loss for batch 775 : 0.0720876082777977\n",
      "Training loss for batch 776 : 0.1534741222858429\n",
      "Training loss for batch 777 : 0.01399310864508152\n",
      "Training loss for batch 778 : 0.08714235574007034\n",
      "Training loss for batch 779 : 0.26740899682044983\n",
      "Training loss for batch 780 : 0.2510223686695099\n",
      "Training loss for batch 781 : 0.09028385579586029\n",
      "Training loss for batch 782 : 0.040757887065410614\n",
      "Training loss for batch 783 : 0.22888554632663727\n",
      "Training loss for batch 784 : 0.07100193202495575\n",
      "Training loss for batch 785 : 0.026507532224059105\n",
      "Training loss for batch 786 : 0.05641843006014824\n",
      "Training loss for batch 787 : 0.09778350591659546\n",
      "Training loss for batch 788 : 0.1682979315519333\n",
      "Training loss for batch 789 : 0.27918726205825806\n",
      "Training loss for batch 790 : 0.07029392570257187\n",
      "Training loss for batch 791 : 0.3184064030647278\n",
      "Training loss for batch 792 : 0.13117153942584991\n",
      "Training loss for batch 793 : 0.2119048833847046\n",
      "Training loss for batch 794 : 0.3619861304759979\n",
      "Training loss for batch 795 : 0.36593344807624817\n",
      "Training loss for batch 796 : 0.14070086181163788\n",
      "Training loss for batch 797 : 0.18602031469345093\n",
      "Training loss for batch 798 : 0.27697354555130005\n",
      "Training loss for batch 799 : 0.1459534764289856\n",
      "Training loss for batch 800 : 0.41021424531936646\n",
      "Training loss for batch 801 : 0.10357488691806793\n",
      "Training loss for batch 802 : 0.2873876392841339\n",
      "Training loss for batch 803 : 0.24681515991687775\n",
      "Training loss for batch 804 : 0.1769942045211792\n",
      "Training loss for batch 805 : 0.060985516756772995\n",
      "Training loss for batch 806 : 0.15020114183425903\n",
      "Training loss for batch 807 : 0.13600219786167145\n",
      "Training loss for batch 808 : 0.20255406200885773\n",
      "Training loss for batch 809 : 0.2517275810241699\n",
      "Training loss for batch 810 : 0.04145444557070732\n",
      "Training loss for batch 811 : 0.3118191957473755\n",
      "Training loss for batch 812 : 0.12525328993797302\n",
      "Training loss for batch 813 : 0.00398989487439394\n",
      "Training loss for batch 814 : 0.3029190003871918\n",
      "Training loss for batch 815 : 0.22092992067337036\n",
      "Training loss for batch 816 : 0.14163237810134888\n",
      "Training loss for batch 817 : 0.0788031592965126\n",
      "Training loss for batch 818 : 0.11793220788240433\n",
      "Training loss for batch 819 : 0.10673405230045319\n",
      "Training loss for batch 820 : 0.3920857012271881\n",
      "Training loss for batch 821 : 0.048131316900253296\n",
      "Training loss for batch 822 : 0.018261466175317764\n",
      "Training loss for batch 823 : 0.24230727553367615\n",
      "Training loss for batch 824 : 0.2907438278198242\n",
      "Training loss for batch 825 : 0.034808892756700516\n",
      "Training loss for batch 826 : 0.15529951453208923\n",
      "Training loss for batch 827 : 0.012368778698146343\n",
      "Training loss for batch 828 : 0.005803038831800222\n",
      "Training loss for batch 829 : 0.24615617096424103\n",
      "Training loss for batch 830 : 0.17694829404354095\n",
      "Training loss for batch 831 : 0.019906237721443176\n",
      "Training loss for batch 832 : 0.03025081753730774\n",
      "Training loss for batch 833 : 0.15394902229309082\n",
      "Training loss for batch 834 : 0.05902435630559921\n",
      "Training loss for batch 835 : 0.287657231092453\n",
      "Training loss for batch 836 : 0.08924550563097\n",
      "Training loss for batch 837 : 0.08524582535028458\n",
      "Training loss for batch 838 : 0.13067907094955444\n",
      "Training loss for batch 839 : 0.0183937456458807\n",
      "Training loss for batch 840 : 0.08268017321825027\n",
      "Training loss for batch 841 : 0.26049017906188965\n",
      "Training loss for batch 842 : 0.35903722047805786\n",
      "Training loss for batch 843 : 0.11096477508544922\n",
      "Training loss for batch 844 : 0.2422424852848053\n",
      "Training loss for batch 845 : 0.11798658221960068\n",
      "Training loss for batch 846 : 0.4523875117301941\n",
      "Training loss for batch 847 : 0.4127993881702423\n",
      "Training loss for batch 848 : 0.2533145844936371\n",
      "Training loss for batch 849 : 0.004374300129711628\n",
      "Training loss for batch 850 : 0.3175663352012634\n",
      "Training loss for batch 851 : 0.29572346806526184\n",
      "Training loss for batch 852 : 0.04477587342262268\n",
      "Training loss for batch 853 : 0.37975794076919556\n",
      "Training loss for batch 854 : 0.1924317479133606\n",
      "Training loss for batch 855 : 0.2741260230541229\n",
      "Training loss for batch 856 : 0.07555435597896576\n",
      "Training loss for batch 857 : 0.2897806167602539\n",
      "Training loss for batch 858 : 0.23958028852939606\n",
      "Training loss for batch 859 : 0.18479762971401215\n",
      "Training loss for batch 860 : 0.00024053454399108887\n",
      "Training loss for batch 861 : 0.23006571829319\n",
      "Training loss for batch 862 : 0.058239083737134933\n",
      "Training loss for batch 863 : 0.08691888302564621\n",
      "Training loss for batch 864 : 0.2648938298225403\n",
      "Training loss for batch 865 : 0.3165658712387085\n",
      "Training loss for batch 866 : 0.1996416598558426\n",
      "Training loss for batch 867 : 0.01701817288994789\n",
      "Training loss for batch 868 : 0.0\n",
      "Training loss for batch 869 : 0.465934157371521\n",
      "Training loss for batch 870 : 0.04741951450705528\n",
      "Training loss for batch 871 : 0.22613635659217834\n",
      "Training loss for batch 872 : 0.37435001134872437\n",
      "Training loss for batch 873 : 0.22002370655536652\n",
      "Training loss for batch 874 : 0.17399725317955017\n",
      "Training loss for batch 875 : 0.11052785813808441\n",
      "Training loss for batch 876 : 0.04833404719829559\n",
      "Training loss for batch 877 : 0.1681230366230011\n",
      "Training loss for batch 878 : 0.14224795997142792\n",
      "Training loss for batch 879 : 0.15191617608070374\n",
      "Training loss for batch 880 : 0.12746380269527435\n",
      "Training loss for batch 881 : 0.07119794934988022\n",
      "Training loss for batch 882 : 0.08612512052059174\n",
      "Training loss for batch 883 : 0.2974325120449066\n",
      "Training loss for batch 884 : 0.208482027053833\n",
      "Training loss for batch 885 : 0.10975350439548492\n",
      "Training loss for batch 886 : 0.0622263178229332\n",
      "Training loss for batch 887 : 0.03228655457496643\n",
      "Training loss for batch 888 : 0.2522963881492615\n",
      "Training loss for batch 889 : 0.20860254764556885\n",
      "Training loss for batch 890 : 0.028375085443258286\n",
      "Training loss for batch 891 : 0.1859985589981079\n",
      "Training loss for batch 892 : 0.18801862001419067\n",
      "Training loss for batch 893 : 0.322618693113327\n",
      "Training loss for batch 894 : 0.08274640142917633\n",
      "Training loss for batch 895 : 0.012213586829602718\n",
      "Training loss for batch 896 : 0.18327049911022186\n",
      "Training loss for batch 897 : 0.4628879725933075\n",
      "Training loss for batch 898 : 0.2502024173736572\n",
      "Training loss for batch 899 : 0.05062255263328552\n",
      "Training loss for batch 900 : 0.04626001790165901\n",
      "Training loss for batch 901 : 0.635270357131958\n",
      "Training loss for batch 902 : 0.05228099226951599\n",
      "Training loss for batch 903 : 0.16464020311832428\n",
      "Training loss for batch 904 : 0.24441680312156677\n",
      "Training loss for batch 905 : 0.22644485533237457\n",
      "Training loss for batch 906 : 0.08759813010692596\n",
      "Training loss for batch 907 : 0.2272392064332962\n",
      "Training loss for batch 908 : 0.2060454934835434\n",
      "Training loss for batch 909 : 0.2614818513393402\n",
      "Training loss for batch 910 : 0.1496829241514206\n",
      "Training loss for batch 911 : 0.12314046174287796\n",
      "Training loss for batch 912 : 0.006886729504913092\n",
      "Training loss for batch 913 : 0.12945178151130676\n",
      "Training loss for batch 914 : 0.12716913223266602\n",
      "Training loss for batch 915 : 0.09008314460515976\n",
      "Training loss for batch 916 : 0.21684634685516357\n",
      "Training loss for batch 917 : 0.2109510749578476\n",
      "Training loss for batch 918 : 0.011279779486358166\n",
      "Training loss for batch 919 : 0.2817804217338562\n",
      "Training loss for batch 920 : 0.08847612887620926\n",
      "Training loss for batch 921 : 0.046350594609975815\n",
      "Training loss for batch 922 : 0.2942313253879547\n",
      "Training loss for batch 923 : 0.1977631002664566\n",
      "Training loss for batch 924 : 0.28454405069351196\n",
      "Training loss for batch 925 : 0.0196548942476511\n",
      "Training loss for batch 926 : 0.27795130014419556\n",
      "Training loss for batch 927 : 0.11528841406106949\n",
      "Training loss for batch 928 : 0.31808945536613464\n",
      "Training loss for batch 929 : 0.31129375100135803\n",
      "Training loss for batch 930 : 0.113865926861763\n",
      "Training loss for batch 931 : 0.2658190131187439\n",
      "Training loss for batch 932 : 0.1754770129919052\n",
      "Training loss for batch 933 : 0.016005706042051315\n",
      "Training loss for batch 934 : 0.21393398940563202\n",
      "Training loss for batch 935 : 0.09879902750253677\n",
      "Training loss for batch 936 : 0.04140133783221245\n",
      "Training loss for batch 937 : 0.07566384226083755\n",
      "Training loss for batch 938 : 0.004609205760061741\n",
      "Training loss for batch 939 : 0.07946346700191498\n",
      "Training loss for batch 940 : 0.09844975173473358\n",
      "Training loss for batch 941 : 0.1057169958949089\n",
      "Training loss for batch 942 : 0.21779711544513702\n",
      "Training loss for batch 943 : 0.22289115190505981\n",
      "Training loss for batch 944 : 0.23325064778327942\n",
      "Training loss for batch 945 : 0.24040387570858002\n",
      "Training loss for batch 946 : 0.20969992876052856\n",
      "Training loss for batch 947 : 0.3326976001262665\n",
      "Training loss for batch 948 : 0.07146910578012466\n",
      "Training loss for batch 949 : 0.3972613215446472\n",
      "Training loss for batch 950 : 0.2680248022079468\n",
      "Training loss for batch 951 : 0.13686099648475647\n",
      "Training loss for batch 952 : 0.12960495054721832\n",
      "Training loss for batch 953 : 0.13990356028079987\n",
      "Training loss for batch 954 : 0.347970575094223\n",
      "Training loss for batch 955 : 0.0\n",
      "Training loss for batch 956 : 0.045660048723220825\n",
      "Training loss for batch 957 : 0.13178850710391998\n",
      "Training loss for batch 958 : 0.11349286139011383\n",
      "Training loss for batch 959 : 0.25552111864089966\n",
      "Training loss for batch 960 : 0.19899100065231323\n",
      "Training loss for batch 961 : 0.06879167258739471\n",
      "Training loss for batch 962 : 0.1706153005361557\n",
      "Training loss for batch 963 : 0.38471606373786926\n",
      "Training loss for batch 964 : 0.5149807929992676\n",
      "Training loss for batch 965 : 0.1679849624633789\n",
      "Training loss for batch 966 : 0.3872416019439697\n",
      "Training loss for batch 967 : 0.1798153519630432\n",
      "Training loss for batch 968 : 0.05765215680003166\n",
      "Training loss for batch 969 : 0.28115198016166687\n",
      "Training loss for batch 970 : 0.1892133504152298\n",
      "Training loss for batch 971 : 0.13028068840503693\n",
      "Training loss for batch 972 : 0.16584697365760803\n",
      "Training loss for batch 973 : 0.0185951367020607\n",
      "Training loss for batch 974 : 0.34060654044151306\n",
      "Training loss for batch 975 : 0.05936902388930321\n",
      "Training loss for batch 976 : 0.4780654311180115\n",
      "Training loss for batch 977 : 0.2741071283817291\n",
      "Training loss for batch 978 : 0.2750992476940155\n",
      "Training loss for batch 979 : 0.11171162873506546\n",
      "Training loss for batch 980 : 0.1736333966255188\n",
      "Training loss for batch 981 : 0.07718910276889801\n",
      "Training loss for batch 982 : 0.24085818231105804\n",
      "Training loss for batch 983 : 0.18211059272289276\n",
      "Training loss for batch 984 : 0.11301372945308685\n",
      "Training loss for batch 985 : 0.01885990984737873\n",
      "Training loss for batch 986 : 0.15462340414524078\n",
      "Training loss for batch 987 : 0.04518794268369675\n",
      "Training loss for batch 988 : 0.02903713844716549\n",
      "Training loss for batch 989 : 0.31449320912361145\n",
      "Training loss for batch 990 : 0.14742925763130188\n",
      "Training loss for batch 991 : 0.09594976902008057\n",
      "Training loss for batch 992 : 0.05647465959191322\n",
      "Training loss for batch 993 : 0.127729594707489\n",
      "Training loss for batch 994 : 0.32048991322517395\n",
      "Training loss for batch 995 : 0.3257609009742737\n",
      "Training loss for batch 996 : 0.009471246972680092\n",
      "Training loss for batch 997 : 0.0041201612912118435\n",
      "Training loss for batch 998 : 0.0787416398525238\n",
      "Training loss for batch 999 : 0.378361314535141\n",
      "Training loss for batch 1000 : 0.012779898010194302\n",
      "Training loss for batch 1001 : 0.2154354751110077\n",
      "Training loss for batch 1002 : 0.07126688212156296\n",
      "Training loss for batch 1003 : 0.11821570247411728\n",
      "Training loss for batch 1004 : 0.2028678059577942\n",
      "Training loss for batch 1005 : 0.18058092892169952\n",
      "Training loss for batch 1006 : 0.16250690817832947\n",
      "Training loss for batch 1007 : 0.2818291485309601\n",
      "Training loss for batch 1008 : 0.2958614230155945\n",
      "Training loss for batch 1009 : 0.1674109697341919\n",
      "Training loss for batch 1010 : 0.012898619286715984\n",
      "Training loss for batch 1011 : 0.08827367424964905\n",
      "Training loss for batch 1012 : 0.13665267825126648\n",
      "Training loss for batch 1013 : 0.0763288363814354\n",
      "Training loss for batch 1014 : 0.33921435475349426\n",
      "Training loss for batch 1015 : 0.08305910229682922\n",
      "Training loss for batch 1016 : 0.17611545324325562\n",
      "Training loss for batch 1017 : 0.011323834769427776\n",
      "Training loss for batch 1018 : 0.03623523935675621\n",
      "Training loss for batch 1019 : 0.043944381177425385\n",
      "Training loss for batch 1020 : 0.020968478173017502\n",
      "Training loss for batch 1021 : 0.08711724728345871\n",
      "Training loss for batch 1022 : 0.14432576298713684\n",
      "Training loss for batch 1023 : 0.15294690430164337\n",
      "Training loss for batch 1024 : 0.05223716050386429\n",
      "Training loss for batch 1025 : 0.054638780653476715\n",
      "Training loss for batch 1026 : 0.28642261028289795\n",
      "Training loss for batch 1027 : 0.09381353855133057\n",
      "Training loss for batch 1028 : 0.04058229550719261\n",
      "Training loss for batch 1029 : 0.02074974589049816\n",
      "Training loss for batch 1030 : 0.15068715810775757\n",
      "Training loss for batch 1031 : 0.1183442547917366\n",
      "Training loss for batch 1032 : 0.0010324220638722181\n",
      "Training loss for batch 1033 : 0.3093164265155792\n",
      "Training loss for batch 1034 : 0.12445252388715744\n",
      "Training loss for batch 1035 : 0.18429388105869293\n",
      "Training loss for batch 1036 : 0.0\n",
      "Training loss for batch 1037 : 0.23191159963607788\n",
      "Training loss for batch 1038 : 0.1540600210428238\n",
      "Training loss for batch 1039 : 0.15326140820980072\n",
      "Training loss for batch 1040 : 0.23417934775352478\n",
      "Training loss for batch 1041 : 0.018817109987139702\n",
      "Training loss for batch 1042 : 0.20030701160430908\n",
      "Training loss for batch 1043 : 0.3284666836261749\n",
      "Training loss for batch 1044 : 0.13755881786346436\n",
      "Training loss for batch 1045 : 0.0319213941693306\n",
      "Training loss for batch 1046 : 0.20370858907699585\n",
      "Training loss for batch 1047 : 0.07623185962438583\n",
      "Training loss for batch 1048 : 0.009466966614127159\n",
      "Training loss for batch 1049 : 0.17121492326259613\n",
      "Training loss for batch 1050 : 0.3854651153087616\n",
      "Training loss for batch 1051 : 0.23083128035068512\n",
      "Training loss for batch 1052 : 0.22584833204746246\n",
      "Training loss for batch 1053 : 0.12935376167297363\n",
      "Training loss for batch 1054 : 0.2166101485490799\n",
      "Training loss for batch 1055 : 0.15581616759300232\n",
      "Training loss for batch 1056 : 0.2960875630378723\n",
      "Training loss for batch 1057 : -0.0002379268844379112\n",
      "Training loss for batch 1058 : 0.28765925765037537\n",
      "Training loss for batch 1059 : 0.3646780252456665\n",
      "Training loss for batch 1060 : 0.4962802231311798\n",
      "Training loss for batch 1061 : 0.1955140233039856\n",
      "Training loss for batch 1062 : 0.3757557272911072\n",
      "Training loss for batch 1063 : 0.2813880145549774\n",
      "Training loss for batch 1064 : 0.3186718225479126\n",
      "Training loss for batch 1065 : 0.4374701678752899\n",
      "Training loss for batch 1066 : 0.09245828539133072\n",
      "Training loss for batch 1067 : 0.3356480300426483\n",
      "Training loss for batch 1068 : 0.05600821599364281\n",
      "Training loss for batch 1069 : 0.0\n",
      "Training loss for batch 1070 : 0.24604205787181854\n",
      "Training loss for batch 1071 : 0.3920578062534332\n",
      "Training loss for batch 1072 : 0.09897401183843613\n",
      "Training loss for batch 1073 : 0.17851601541042328\n",
      "Training loss for batch 1074 : 0.031988319009542465\n",
      "Training loss for batch 1075 : 0.011840621940791607\n",
      "Training loss for batch 1076 : 0.23004701733589172\n",
      "Training loss for batch 1077 : 0.46822020411491394\n",
      "Training loss for batch 1078 : 0.015542499721050262\n",
      "Training loss for batch 1079 : 0.07563161849975586\n",
      "Training loss for batch 1080 : 0.04864659160375595\n",
      "Training loss for batch 1081 : 0.0017187256598845124\n",
      "Training loss for batch 1082 : 0.15237027406692505\n",
      "Training loss for batch 1083 : 0.6136832237243652\n",
      "Training loss for batch 1084 : 0.1629507839679718\n",
      "Training loss for batch 1085 : 0.1580885499715805\n",
      "Training loss for batch 1086 : 0.5214370489120483\n",
      "Training loss for batch 1087 : 0.16211473941802979\n",
      "Training loss for batch 1088 : 0.2695939540863037\n",
      "Training loss for batch 1089 : 0.22672241926193237\n",
      "Training loss for batch 1090 : 0.5045299530029297\n",
      "Training loss for batch 1091 : 0.10557780414819717\n",
      "Training loss for batch 1092 : 0.2526516318321228\n",
      "Training loss for batch 1093 : 0.38816916942596436\n",
      "Training loss for batch 1094 : 0.1851060390472412\n",
      "Training loss for batch 1095 : 0.2774966061115265\n",
      "Training loss for batch 1096 : 0.057910121977329254\n",
      "Training loss for batch 1097 : 0.07292582094669342\n",
      "Training loss for batch 1098 : 0.1165742501616478\n",
      "Training loss for batch 1099 : 0.13724003732204437\n",
      "Training loss for batch 1100 : 0.03864157572388649\n",
      "Training loss for batch 1101 : 0.10260651260614395\n",
      "Training loss for batch 1102 : 0.030977213755249977\n",
      "Training loss for batch 1103 : 0.1239587664604187\n",
      "Training loss for batch 1104 : 0.06892150640487671\n",
      "Training loss for batch 1105 : 0.07730492204427719\n",
      "Training loss for batch 1106 : 0.11396545171737671\n",
      "Training loss for batch 1107 : 0.1490095853805542\n",
      "Training loss for batch 1108 : 0.04269896447658539\n",
      "Training loss for batch 1109 : 0.038597222417593\n",
      "Training loss for batch 1110 : 0.14098882675170898\n",
      "Training loss for batch 1111 : 0.33841681480407715\n",
      "Training loss for batch 1112 : 0.2920995056629181\n",
      "Training loss for batch 1113 : -0.00023447976855095476\n",
      "Training loss for batch 1114 : 0.06235606223344803\n",
      "Training loss for batch 1115 : 0.19860698282718658\n",
      "Training loss for batch 1116 : 0.38975784182548523\n",
      "Training loss for batch 1117 : 0.6158886551856995\n",
      "Training loss for batch 1118 : 0.11117739975452423\n",
      "Training loss for batch 1119 : 0.11612241715192795\n",
      "Training loss for batch 1120 : 0.0634194165468216\n",
      "Training loss for batch 1121 : 0.20570504665374756\n",
      "Training loss for batch 1122 : 0.320780485868454\n",
      "Training loss for batch 1123 : 0.3382732570171356\n",
      "Training loss for batch 1124 : 0.3593277931213379\n",
      "Training loss for batch 1125 : 0.03138678893446922\n",
      "Training loss for batch 1126 : 0.029288262128829956\n",
      "Training loss for batch 1127 : 0.01711903139948845\n",
      "Training loss for batch 1128 : 0.2378794103860855\n",
      "Training loss for batch 1129 : 0.06811220198869705\n",
      "Training loss for batch 1130 : 0.07264130562543869\n",
      "Training loss for batch 1131 : 0.1735237091779709\n",
      "Training loss for batch 1132 : 0.0428784042596817\n",
      "Training loss for batch 1133 : 0.007762053515762091\n",
      "Training loss for batch 1134 : 0.029531287029385567\n",
      "Training loss for batch 1135 : 0.2706904411315918\n",
      "Training loss for batch 1136 : 0.237097829580307\n",
      "Training loss for batch 1137 : 0.07898390293121338\n",
      "Training loss for batch 1138 : 0.2105555236339569\n",
      "Training loss for batch 1139 : 0.19115106761455536\n",
      "Training loss for batch 1140 : 0.48832011222839355\n",
      "Training loss for batch 1141 : 0.021103037521243095\n",
      "Training loss for batch 1142 : 0.05348891019821167\n",
      "Training loss for batch 1143 : 0.2149815410375595\n",
      "Training loss for batch 1144 : 0.004522006493061781\n",
      "Training loss for batch 1145 : 0.2594464123249054\n",
      "Training loss for batch 1146 : 0.21700063347816467\n",
      "Training loss for batch 1147 : 0.20292989909648895\n",
      "Training loss for batch 1148 : 0.1328245848417282\n",
      "Training loss for batch 1149 : 0.23952625691890717\n",
      "Training loss for batch 1150 : 0.06132655218243599\n",
      "Training loss for batch 1151 : 0.19011206924915314\n",
      "Training loss for batch 1152 : 0.22221826016902924\n",
      "Training loss for batch 1153 : 0.3352964520454407\n",
      "Training loss for batch 1154 : 0.0\n",
      "Training loss for batch 1155 : 0.02478538267314434\n",
      "Training loss for batch 1156 : 0.06733280420303345\n",
      "Training loss for batch 1157 : 0.02837824448943138\n",
      "Training loss for batch 1158 : 0.08593716472387314\n",
      "Training loss for batch 1159 : 0.37856927514076233\n",
      "Training loss for batch 1160 : 0.16252577304840088\n",
      "Training loss for batch 1161 : 0.00484223198145628\n",
      "Training loss for batch 1162 : 0.18034057319164276\n",
      "Training loss for batch 1163 : 0.21858222782611847\n",
      "Training loss for batch 1164 : 0.31843042373657227\n",
      "Training loss for batch 1165 : 0.09014493972063065\n",
      "Training loss for batch 1166 : 0.08380880206823349\n",
      "Training loss for batch 1167 : 0.44271722435951233\n",
      "Training loss for batch 1168 : 0.06808847188949585\n",
      "Training loss for batch 1169 : 0.11132282763719559\n",
      "Training loss for batch 1170 : 0.11353139579296112\n",
      "Training loss for batch 1171 : 0.04829925298690796\n",
      "Training loss for batch 1172 : 0.0026797608006745577\n",
      "Training loss for batch 1173 : 0.006353046279400587\n",
      "Training loss for batch 1174 : 0.00809355266392231\n",
      "Training loss for batch 1175 : 0.03830842301249504\n",
      "Training loss for batch 1176 : 0.26587995886802673\n",
      "Training loss for batch 1177 : 0.031256601214408875\n",
      "Training loss for batch 1178 : 0.09546642005443573\n",
      "Training loss for batch 1179 : 0.0\n",
      "Training loss for batch 1180 : 0.24681183695793152\n",
      "Training loss for batch 1181 : 0.03262846916913986\n",
      "Training loss for batch 1182 : 0.34130859375\n",
      "Training loss for batch 1183 : 0.2946625351905823\n",
      "Training loss for batch 1184 : 0.09364619851112366\n",
      "Training loss for batch 1185 : 0.23470593988895416\n",
      "Training loss for batch 1186 : 1.301680326461792\n",
      "Training loss for batch 1187 : 0.22141939401626587\n",
      "Training loss for batch 1188 : 0.03788987919688225\n",
      "Training loss for batch 1189 : 0.011242306791245937\n",
      "Training loss for batch 1190 : 0.2768215835094452\n",
      "Training loss for batch 1191 : 0.0006404221057891846\n",
      "Training loss for batch 1192 : 0.024391334503889084\n",
      "Training loss for batch 1193 : 0.48879629373550415\n",
      "Training loss for batch 1194 : 0.0\n",
      "Training loss for batch 1195 : 0.18549659848213196\n",
      "Training loss for batch 1196 : 0.25471919775009155\n",
      "Training loss for batch 1197 : 0.3528432548046112\n",
      "Training loss for batch 1198 : 0.06472228467464447\n",
      "Training loss for batch 1199 : 0.06904294341802597\n",
      "Training loss for batch 1200 : 0.19626353681087494\n",
      "Training loss for batch 1201 : 0.22612231969833374\n",
      "Training loss for batch 1202 : 0.4454538822174072\n",
      "Training loss for batch 1203 : 0.17601682245731354\n",
      "Training loss for batch 1204 : 0.2819564640522003\n",
      "Training loss for batch 1205 : 0.2144429087638855\n",
      "Training loss for batch 1206 : 0.39971667528152466\n",
      "Training loss for batch 1207 : 0.8423428535461426\n",
      "Training loss for batch 1208 : 0.016158852726221085\n",
      "Training loss for batch 1209 : 0.363620787858963\n",
      "Training loss for batch 1210 : 0.17183394730091095\n",
      "Training loss for batch 1211 : 0.08902794867753983\n",
      "Training loss for batch 1212 : 0.1950361579656601\n",
      "Training loss for batch 1213 : 0.20632319152355194\n",
      "Training loss for batch 1214 : 0.14261722564697266\n",
      "Training loss for batch 1215 : 0.04771394282579422\n",
      "Training loss for batch 1216 : 0.0497119165956974\n",
      "Training loss for batch 1217 : 0.31277403235435486\n",
      "Training loss for batch 1218 : 0.21382266283035278\n",
      "Training loss for batch 1219 : 0.18129833042621613\n",
      "Training loss for batch 1220 : 0.2849455177783966\n",
      "Training loss for batch 1221 : 0.3844180107116699\n",
      "Training loss for batch 1222 : 0.2806469202041626\n",
      "Training loss for batch 1223 : 0.1511705070734024\n",
      "Training loss for batch 1224 : 0.20365697145462036\n",
      "Training loss for batch 1225 : 0.18856899440288544\n",
      "Training loss for batch 1226 : 0.0\n",
      "Training loss for batch 1227 : 0.06485220044851303\n",
      "Training loss for batch 1228 : 0.23375748097896576\n",
      "Training loss for batch 1229 : 0.01642438769340515\n",
      "Training loss for batch 1230 : 0.7711952328681946\n",
      "Training loss for batch 1231 : 0.21849754452705383\n",
      "Training loss for batch 1232 : 0.02805515192449093\n",
      "Training loss for batch 1233 : 0.1476951688528061\n",
      "Training loss for batch 1234 : 0.18088041245937347\n",
      "Training loss for batch 1235 : 0.41012826561927795\n",
      "Training loss for batch 1236 : 0.11326877027750015\n",
      "Training loss for batch 1237 : 0.12936970591545105\n",
      "Training loss for batch 1238 : 0.22889076173305511\n",
      "Training loss for batch 1239 : 0.41255202889442444\n",
      "Training loss for batch 1240 : 0.13395357131958008\n",
      "Training loss for batch 1241 : 0.33778226375579834\n",
      "Training loss for batch 1242 : 0.13441921770572662\n",
      "Training loss for batch 1243 : 0.15713447332382202\n",
      "Training loss for batch 1244 : 0.2759071886539459\n",
      "Training loss for batch 1245 : 0.0395607128739357\n",
      "Training loss for batch 1246 : 0.2335694283246994\n",
      "Training loss for batch 1247 : 0.2714099884033203\n",
      "Training loss for batch 1248 : 0.072740338742733\n",
      "Training loss for batch 1249 : 0.3895275890827179\n",
      "Training loss for batch 1250 : 0.12880922853946686\n",
      "Training loss for batch 1251 : 0.2109016627073288\n",
      "Training loss for batch 1252 : 0.13669295608997345\n",
      "Training loss for batch 1253 : 0.1716272234916687\n",
      "Training loss for batch 1254 : 0.28930461406707764\n",
      "Training loss for batch 1255 : 0.1315174102783203\n",
      "Training loss for batch 1256 : 0.12809890508651733\n",
      "Training loss for batch 1257 : 0.06447956711053848\n",
      "Training loss for batch 1258 : 0.0\n",
      "Training loss for batch 1259 : 0.14487676322460175\n",
      "Training loss for batch 1260 : 0.09022314846515656\n",
      "Training loss for batch 1261 : 0.10223127901554108\n",
      "Training loss for batch 1262 : 0.25478753447532654\n",
      "Training loss for batch 1263 : 0.27733302116394043\n",
      "Training loss for batch 1264 : 0.020290430635213852\n",
      "Training loss for batch 1265 : 0.3063042163848877\n",
      "Training loss for batch 1266 : 0.06273528188467026\n",
      "Training loss for batch 1267 : 0.15149365365505219\n",
      "Training loss for batch 1268 : 0.033059049397706985\n",
      "Training loss for batch 1269 : 0.15043871104717255\n",
      "Training loss for batch 1270 : 0.05553177744150162\n",
      "Training loss for batch 1271 : 0.19953413307666779\n",
      "Training loss for batch 1272 : 0.0681074783205986\n",
      "Training loss for batch 1273 : 0.15811268985271454\n",
      "Training loss for batch 1274 : 0.06658624857664108\n",
      "Training loss for batch 1275 : 0.3252415359020233\n",
      "Training loss for batch 1276 : 0.030282724648714066\n",
      "Training loss for batch 1277 : 0.08498216420412064\n",
      "Training loss for batch 1278 : 0.2697451710700989\n",
      "Training loss for batch 1279 : 0.011372891254723072\n",
      "Training loss for batch 1280 : 0.25776490569114685\n",
      "Training loss for batch 1281 : 0.14685004949569702\n",
      "Training loss for batch 1282 : 0.24720148742198944\n",
      "Training loss for batch 1283 : 0.31104519963264465\n",
      "Training loss for batch 1284 : 0.11495645344257355\n",
      "Training loss for batch 1285 : 0.2097001075744629\n",
      "Training loss for batch 1286 : 0.02675493061542511\n",
      "Training loss for batch 1287 : 0.022517602890729904\n",
      "Training loss for batch 1288 : 0.1696682721376419\n",
      "Training loss for batch 1289 : 0.046532612293958664\n",
      "Training loss for batch 1290 : 0.09523378312587738\n",
      "Training loss for batch 1291 : 0.04674308001995087\n",
      "Training loss for batch 1292 : 0.03148868680000305\n",
      "Training loss for batch 1293 : 0.046221066266298294\n",
      "Training loss for batch 1294 : 0.05156736448407173\n",
      "Training loss for batch 1295 : 0.04791836068034172\n",
      "Training loss for batch 1296 : 0.05417909845709801\n",
      "Training loss for batch 1297 : 0.3615027070045471\n",
      "Training loss for batch 1298 : 0.1063193827867508\n",
      "Training loss for batch 1299 : 0.12619610130786896\n",
      "Training loss for batch 1300 : 0.30125564336776733\n",
      "Training loss for batch 1301 : 0.040084511041641235\n",
      "Training loss for batch 1302 : 0.2139027714729309\n",
      "Training loss for batch 1303 : 0.047803934663534164\n",
      "Training loss for batch 1304 : 0.34600090980529785\n",
      "Training loss for batch 1305 : 0.1530550867319107\n",
      "Training loss for batch 1306 : 0.4099774956703186\n",
      "Training loss for batch 1307 : 0.07942686975002289\n",
      "Training loss for batch 1308 : 0.0\n",
      "Training loss for batch 1309 : 0.2613638639450073\n",
      "Training loss for batch 1310 : 0.2694498598575592\n",
      "Training loss for batch 1311 : 0.17813004553318024\n",
      "Training loss for batch 1312 : 0.11547861248254776\n",
      "Training loss for batch 1313 : 0.20873618125915527\n",
      "Training loss for batch 1314 : 0.40291282534599304\n",
      "Training loss for batch 1315 : 0.3254072368144989\n",
      "Training loss for batch 1316 : 0.10274354368448257\n",
      "Training loss for batch 1317 : 0.17895281314849854\n",
      "Training loss for batch 1318 : 0.5345302224159241\n",
      "Training loss for batch 1319 : 0.3698105216026306\n",
      "Training loss for batch 1320 : 0.00036648911191150546\n",
      "Training loss for batch 1321 : 0.316639244556427\n",
      "Training loss for batch 1322 : 0.02758614905178547\n",
      "Training loss for batch 1323 : 0.14582373201847076\n",
      "Training loss for batch 1324 : 0.12659065425395966\n",
      "Training loss for batch 1325 : 0.06822206825017929\n",
      "Training loss for batch 1326 : 0.1380806863307953\n",
      "Training loss for batch 1327 : 0.14063584804534912\n",
      "Training loss for batch 1328 : 0.04120239242911339\n",
      "Training loss for batch 1329 : 0.09681526571512222\n",
      "Training loss for batch 1330 : 0.12341771274805069\n",
      "Training loss for batch 1331 : 0.11366371810436249\n",
      "Training loss for batch 1332 : 0.1028839573264122\n",
      "Training loss for batch 1333 : 0.3782089650630951\n",
      "Training loss for batch 1334 : 0.49688228964805603\n",
      "Training loss for batch 1335 : 0.24864548444747925\n",
      "Training loss for batch 1336 : 0.07144290953874588\n",
      "Training loss for batch 1337 : 0.0010270909406244755\n",
      "Training loss for batch 1338 : 0.10828359425067902\n",
      "Training loss for batch 1339 : 0.11489701271057129\n",
      "Training loss for batch 1340 : 0.047407057136297226\n",
      "Training loss for batch 1341 : 0.22163990139961243\n",
      "Training loss for batch 1342 : 0.5646089911460876\n",
      "Training loss for batch 1343 : 0.03890455886721611\n",
      "Training loss for batch 1344 : 0.11042527109384537\n",
      "Training loss for batch 1345 : 0.05782951042056084\n",
      "Training loss for batch 1346 : 0.09654301404953003\n",
      "Training loss for batch 1347 : 0.16844411194324493\n",
      "Training loss for batch 1348 : 0.17836134135723114\n",
      "Training loss for batch 1349 : 0.323403537273407\n",
      "Training loss for batch 1350 : 0.16411878168582916\n",
      "Training loss for batch 1351 : 0.1829453557729721\n",
      "Training loss for batch 1352 : 0.6422824859619141\n",
      "Training loss for batch 1353 : 0.08723530918359756\n",
      "Training loss for batch 1354 : 0.15534088015556335\n",
      "Training loss for batch 1355 : 0.4040261209011078\n",
      "Training loss for batch 1356 : 0.4135902523994446\n",
      "Training loss for batch 1357 : 0.028719712048768997\n",
      "Training loss for batch 1358 : 0.09458727389574051\n",
      "Training loss for batch 1359 : 0.18262524902820587\n",
      "Training loss for batch 1360 : 0.5815528631210327\n",
      "Training loss for batch 1361 : 0.24864692986011505\n",
      "Training loss for batch 1362 : 0.10723305493593216\n",
      "Training loss for batch 1363 : 0.2441905289888382\n",
      "Training loss for batch 1364 : 0.19077810645103455\n",
      "Training loss for batch 1365 : 0.42562398314476013\n",
      "Training loss for batch 1366 : 0.14765232801437378\n",
      "Training loss for batch 1367 : 0.30240440368652344\n",
      "Training loss for batch 1368 : 0.29262709617614746\n",
      "Training loss for batch 1369 : 0.06774326413869858\n",
      "Training loss for batch 1370 : 0.6464360952377319\n",
      "Training loss for batch 1371 : 0.01225840114057064\n",
      "Training loss for batch 1372 : 0.3144585192203522\n",
      "Training loss for batch 1373 : 0.08119763433933258\n",
      "Training loss for batch 1374 : 0.0354645662009716\n",
      "Training loss for batch 1375 : 0.24033351242542267\n",
      "Training loss for batch 1376 : 0.1304149478673935\n",
      "Training loss for batch 1377 : 0.020451854914426804\n",
      "Training loss for batch 1378 : 0.16347233951091766\n",
      "Training loss for batch 1379 : 0.2168073207139969\n",
      "Training loss for batch 1380 : 0.1425008475780487\n",
      "Training loss for batch 1381 : 0.28601741790771484\n",
      "Training loss for batch 1382 : 0.10642009973526001\n",
      "Training loss for batch 1383 : 0.3708321154117584\n",
      "Training loss for batch 1384 : 0.25198227167129517\n",
      "Training loss for batch 1385 : 0.19285830855369568\n",
      "Training loss for batch 1386 : 0.08294768631458282\n",
      "Training loss for batch 1387 : 0.23322945833206177\n",
      "Training loss for batch 1388 : 0.31135934591293335\n",
      "Training loss for batch 1389 : 0.20378828048706055\n",
      "Training loss for batch 1390 : 0.12620322406291962\n",
      "Training loss for batch 1391 : 0.17511489987373352\n",
      "Training loss for batch 1392 : 0.33654096722602844\n",
      "Training loss for batch 1393 : 0.2268347144126892\n",
      "Training loss for batch 1394 : 0.1574113816022873\n",
      "Training loss for batch 1395 : 0.25812965631484985\n",
      "Training loss for batch 1396 : 0.15557339787483215\n",
      "Training loss for batch 1397 : 0.1833072006702423\n",
      "Training loss for batch 1398 : 0.16008761525154114\n",
      "Training loss for batch 1399 : 0.05329451337456703\n",
      "Training loss for batch 1400 : 0.36864185333251953\n",
      "Training loss for batch 1401 : 0.0040672169998288155\n",
      "Training loss for batch 1402 : 0.18355432152748108\n",
      "Training loss for batch 1403 : 0.414288729429245\n",
      "Training loss for batch 1404 : 0.03742732107639313\n",
      "Training loss for batch 1405 : 0.4206787943840027\n",
      "Training loss for batch 1406 : 0.19646313786506653\n",
      "Training loss for batch 1407 : 0.0956871509552002\n",
      "Training loss for batch 1408 : 0.22495390474796295\n",
      "Training loss for batch 1409 : 0.014118400402367115\n",
      "Training loss for batch 1410 : 0.2771918773651123\n",
      "Training loss for batch 1411 : 0.16847556829452515\n",
      "Training loss for batch 1412 : 0.09513179957866669\n",
      "Training loss for batch 1413 : 0.1039428859949112\n",
      "Training loss for batch 1414 : 0.15696631371974945\n",
      "Training loss for batch 1415 : 0.18814927339553833\n",
      "Training loss for batch 1416 : 0.08862543851137161\n",
      "Training loss for batch 1417 : 0.13686493039131165\n",
      "Training loss for batch 1418 : 0.31089162826538086\n",
      "Training loss for batch 1419 : 0.022660434246063232\n",
      "Training loss for batch 1420 : 0.21442919969558716\n",
      "Training loss for batch 1421 : 0.03167980536818504\n",
      "Training loss for batch 1422 : 0.21331538259983063\n",
      "Training loss for batch 1423 : 0.12937697768211365\n",
      "Training loss for batch 1424 : 0.17646901309490204\n",
      "Training loss for batch 1425 : 0.16048792004585266\n",
      "Training loss for batch 1426 : 0.3400157392024994\n",
      "Training loss for batch 1427 : 0.21332041919231415\n",
      "Training loss for batch 1428 : 0.26673680543899536\n",
      "Training loss for batch 1429 : 0.23198524117469788\n",
      "Training loss for batch 1430 : 0.1999133974313736\n",
      "Training loss for batch 1431 : 0.4752301871776581\n",
      "Training loss for batch 1432 : 0.27120015025138855\n",
      "Training loss for batch 1433 : 0.4287724792957306\n",
      "Training loss for batch 1434 : 0.10995512455701828\n",
      "Training loss for batch 1435 : 0.14069560170173645\n",
      "Training loss for batch 1436 : 0.1761121153831482\n",
      "Training loss for batch 1437 : 0.4189997613430023\n",
      "Training loss for batch 1438 : 0.016761144623160362\n",
      "Training loss for batch 1439 : 0.14273202419281006\n",
      "Training loss for batch 1440 : 0.03841593489050865\n",
      "Training loss for batch 1441 : 0.03959917277097702\n",
      "Training loss for batch 1442 : 0.03617681562900543\n",
      "Training loss for batch 1443 : 0.2034008800983429\n",
      "Training loss for batch 1444 : 0.2897275686264038\n",
      "Training loss for batch 1445 : 0.09797734767198563\n",
      "Training loss for batch 1446 : 0.30949583649635315\n",
      "Training loss for batch 1447 : 0.20337291061878204\n",
      "Training loss for batch 1448 : 0.04414508864283562\n",
      "Training loss for batch 1449 : 0.04030713811516762\n",
      "Training loss for batch 1450 : 0.1970507949590683\n",
      "Training loss for batch 1451 : 0.4547403156757355\n",
      "Training loss for batch 1452 : 0.12963563203811646\n",
      "Training loss for batch 1453 : 0.09962719678878784\n",
      "Training loss for batch 1454 : 0.016218531876802444\n",
      "Training loss for batch 1455 : 0.31490278244018555\n",
      "Training loss for batch 1456 : 0.35223713517189026\n",
      "Training loss for batch 1457 : 0.2843153774738312\n",
      "Training loss for batch 1458 : 0.11228573322296143\n",
      "Training loss for batch 1459 : 0.07233323156833649\n",
      "Training loss for batch 1460 : 0.32287415862083435\n",
      "Training loss for batch 1461 : 0.054641641676425934\n",
      "Training loss for batch 1462 : 0.06907271593809128\n",
      "Training loss for batch 1463 : 0.18376889824867249\n",
      "Training loss for batch 1464 : 0.12225005775690079\n",
      "Training loss for batch 1465 : 0.01902041584253311\n",
      "Training loss for batch 1466 : 0.16039888560771942\n",
      "Training loss for batch 1467 : 0.22900724411010742\n",
      "Training loss for batch 1468 : 0.18136975169181824\n",
      "Training loss for batch 1469 : 0.1440717577934265\n",
      "Training loss for batch 1470 : 0.2477034479379654\n",
      "Training loss for batch 1471 : 0.4338507652282715\n",
      "Training loss for batch 1472 : 0.32784321904182434\n",
      "Training loss for batch 1473 : 0.34242621064186096\n",
      "Training loss for batch 1474 : 0.4022406041622162\n",
      "Training loss for batch 1475 : 0.17277592420578003\n",
      "Training loss for batch 1476 : 0.0549444705247879\n",
      "Training loss for batch 1477 : 0.009713253006339073\n",
      "Training loss for batch 1478 : 0.22667887806892395\n",
      "Training loss for batch 1479 : 0.006117572542279959\n",
      "Training loss for batch 1480 : 0.35418954491615295\n",
      "Training loss for batch 1481 : 0.048477791249752045\n",
      "Training loss for batch 1482 : 0.16346322000026703\n",
      "Training loss for batch 1483 : 0.08318232744932175\n",
      "Training loss for batch 1484 : 0.007754074409604073\n",
      "Training loss for batch 1485 : 0.08096690475940704\n",
      "Training loss for batch 1486 : 0.22521580755710602\n",
      "Training loss for batch 1487 : 0.12088526040315628\n",
      "Training loss for batch 1488 : 0.23718193173408508\n",
      "Training loss for batch 1489 : 0.056786879897117615\n",
      "Training loss for batch 1490 : 0.33147454261779785\n",
      "Training loss for batch 1491 : 0.07984228432178497\n",
      "Training loss for batch 1492 : 0.2831196188926697\n",
      "Training loss for batch 1493 : 0.12558679282665253\n",
      "Training loss for batch 1494 : 0.32246214151382446\n",
      "Training loss for batch 1495 : 0.13654513657093048\n",
      "Training loss for batch 1496 : 0.150905579328537\n",
      "Training loss for batch 1497 : 0.013156972825527191\n",
      "Training loss for batch 1498 : 0.054875027388334274\n",
      "Training loss for batch 1499 : 0.1337725967168808\n",
      "Training loss for batch 1500 : 0.294950008392334\n",
      "Training loss for batch 1501 : 0.04529755562543869\n",
      "Training loss for batch 1502 : 0.08350255340337753\n",
      "Training loss for batch 1503 : 0.04397858306765556\n",
      "Training loss for batch 1504 : 0.08525054901838303\n",
      "Training loss for batch 1505 : 0.4517306685447693\n",
      "Training loss for batch 1506 : 0.45034629106521606\n",
      "Training loss for batch 1507 : 0.1785873919725418\n",
      "Training loss for batch 1508 : 0.11321384459733963\n",
      "Training loss for batch 1509 : 0.29983657598495483\n",
      "Training loss for batch 1510 : 0.22660690546035767\n",
      "Training loss for batch 1511 : 0.2050449103116989\n",
      "Training loss for batch 1512 : 0.43312159180641174\n",
      "Training loss for batch 1513 : 0.3601337969303131\n",
      "Training loss for batch 1514 : 0.1811419427394867\n",
      "Training loss for batch 1515 : 0.061629921197891235\n",
      "Training loss for batch 1516 : 0.1854063719511032\n",
      "Training loss for batch 1517 : 0.2695087790489197\n",
      "Training loss for batch 1518 : 0.2869345247745514\n",
      "Training loss for batch 1519 : 0.20615936815738678\n",
      "Training loss for batch 1520 : 0.617198646068573\n",
      "Training loss for batch 1521 : 0.2562061846256256\n",
      "Training loss for batch 1522 : 0.2194933295249939\n",
      "Training loss for batch 1523 : 0.013367146253585815\n",
      "Training loss for batch 1524 : 0.3600751757621765\n",
      "Training loss for batch 1525 : 0.14775098860263824\n",
      "Training loss for batch 1526 : 0.05462915822863579\n",
      "Training loss for batch 1527 : 0.3836113214492798\n",
      "Training loss for batch 1528 : 0.15835529565811157\n",
      "Training loss for batch 1529 : 0.1791546493768692\n",
      "Training loss for batch 1530 : 0.04280292987823486\n",
      "Training loss for batch 1531 : 0.07856325060129166\n",
      "Training loss for batch 1532 : 0.42368221282958984\n",
      "Training loss for batch 1533 : 0.12030573934316635\n",
      "Training loss for batch 1534 : 0.31107279658317566\n",
      "Training loss for batch 1535 : 0.19467793405056\n",
      "Training loss for batch 1536 : 0.0628858432173729\n",
      "Training loss for batch 1537 : 0.43957144021987915\n",
      "Training loss for batch 1538 : 0.23760347068309784\n",
      "Training loss for batch 1539 : 0.2153242975473404\n",
      "Training loss for batch 1540 : 0.05357836186885834\n",
      "Training loss for batch 1541 : 0.03689226135611534\n",
      "Training loss for batch 1542 : 0.03205086663365364\n",
      "Training loss for batch 1543 : 0.07324626296758652\n",
      "Training loss for batch 1544 : 0.22929437458515167\n",
      "Training loss for batch 1545 : 0.07820270210504532\n",
      "Training loss for batch 1546 : 0.20545612275600433\n",
      "Training loss for batch 1547 : 0.2765917181968689\n",
      "Training loss for batch 1548 : 0.19185635447502136\n",
      "Training loss for batch 1549 : 0.3179108500480652\n",
      "Training loss for batch 1550 : 0.4198952913284302\n",
      "Training loss for batch 1551 : 0.03038100339472294\n",
      "Training loss for batch 1552 : 0.1750141680240631\n",
      "Training loss for batch 1553 : 0.11406024545431137\n",
      "Training loss for batch 1554 : 0.42430341243743896\n",
      "Training loss for batch 1555 : 0.04411899670958519\n",
      "Training loss for batch 1556 : 0.1972637176513672\n",
      "Training loss for batch 1557 : 0.40620294213294983\n",
      "Training loss for batch 1558 : 0.1484925001859665\n",
      "Training loss for batch 1559 : 0.07280488312244415\n",
      "Training loss for batch 1560 : 0.23863086104393005\n",
      "Training loss for batch 1561 : 0.021552393212914467\n",
      "Training loss for batch 1562 : 0.5183712244033813\n",
      "Training loss for batch 1563 : 0.09537066519260406\n",
      "Training loss for batch 1564 : 0.12617352604866028\n",
      "Training loss for batch 1565 : 0.025212805718183517\n",
      "Training loss for batch 1566 : 0.052046455442905426\n",
      "Training loss for batch 1567 : 0.06826306134462357\n",
      "Training loss for batch 1568 : 0.3621498644351959\n",
      "Training loss for batch 1569 : 0.2488727867603302\n",
      "Training loss for batch 1570 : 0.08613452315330505\n",
      "Training loss for batch 1571 : 0.10730226337909698\n",
      "Training loss for batch 1572 : 0.404342383146286\n",
      "Training loss for batch 1573 : 0.10328356176614761\n",
      "Training loss for batch 1574 : 0.06014289706945419\n",
      "Training loss for batch 1575 : 0.47729548811912537\n",
      "Training loss for batch 1576 : 0.11080725491046906\n",
      "Training loss for batch 1577 : 0.0750996470451355\n",
      "Training loss for batch 1578 : 0.06309648603200912\n",
      "Training loss for batch 1579 : 0.6358128786087036\n",
      "Training loss for batch 1580 : 0.40542536973953247\n",
      "Training loss for batch 1581 : 0.4573010206222534\n",
      "Training loss for batch 1582 : 0.03412841260433197\n",
      "Training loss for batch 1583 : 0.17427347600460052\n",
      "Training loss for batch 1584 : 0.43276074528694153\n",
      "Training loss for batch 1585 : 0.3006536662578583\n",
      "Training loss for batch 1586 : 0.09541302919387817\n",
      "Training loss for batch 1587 : 0.07599931210279465\n",
      "Training loss for batch 1588 : 0.22855670750141144\n",
      "Training loss for batch 1589 : 0.16878077387809753\n",
      "Training loss for batch 1590 : 0.12793049216270447\n",
      "Training loss for batch 1591 : 0.005603782832622528\n",
      "Training loss for batch 1592 : 0.22184741497039795\n",
      "Training loss for batch 1593 : 0.22039368748664856\n",
      "Training loss for batch 1594 : 0.20804378390312195\n",
      "Training loss for batch 1595 : 0.10651909559965134\n",
      "Training loss for batch 1596 : 0.0777512937784195\n",
      "Training loss for batch 1597 : 0.08146877586841583\n",
      "Training loss for batch 1598 : 0.23457539081573486\n",
      "Training loss for batch 1599 : 0.11574573069810867\n",
      "Training loss for batch 1600 : 0.06299371272325516\n",
      "Training loss for batch 1601 : 0.09040754288434982\n",
      "Training loss for batch 1602 : 0.31745511293411255\n",
      "Training loss for batch 1603 : 0.02039225399494171\n",
      "Training loss for batch 1604 : 0.12082551419734955\n",
      "Training loss for batch 1605 : 0.12104830890893936\n",
      "Training loss for batch 1606 : 0.4150381088256836\n",
      "Training loss for batch 1607 : 0.17414817214012146\n",
      "Training loss for batch 1608 : 0.10450632125139236\n",
      "Training loss for batch 1609 : 0.04810316860675812\n",
      "Training loss for batch 1610 : 0.3184759318828583\n",
      "Training loss for batch 1611 : 0.018628984689712524\n",
      "Training loss for batch 1612 : 0.08326593041419983\n",
      "Training loss for batch 1613 : 0.2688278257846832\n",
      "Training loss for batch 1614 : 0.776106059551239\n",
      "Training loss for batch 1615 : 0.16116924583911896\n",
      "Training loss for batch 1616 : 0.3677479028701782\n",
      "Training loss for batch 1617 : 0.3853617310523987\n",
      "Training loss for batch 1618 : 0.27702072262763977\n",
      "Training loss for batch 1619 : 0.12849824130535126\n",
      "Training loss for batch 1620 : 0.3437654972076416\n",
      "Training loss for batch 1621 : 0.15581052005290985\n",
      "Training loss for batch 1622 : 0.3730417788028717\n",
      "Training loss for batch 1623 : 0.4695645868778229\n",
      "Training loss for batch 1624 : 0.44046393036842346\n",
      "Training loss for batch 1625 : 0.37735384702682495\n",
      "Training loss for batch 1626 : 0.07047992199659348\n",
      "Training loss for batch 1627 : 0.07729575783014297\n",
      "Training loss for batch 1628 : 0.048583023250103\n",
      "Training loss for batch 1629 : 0.38269028067588806\n",
      "Training loss for batch 1630 : 0.193259596824646\n",
      "Training loss for batch 1631 : 0.05442463234066963\n",
      "Training loss for batch 1632 : 0.22935666143894196\n",
      "Training loss for batch 1633 : 0.11650515347719193\n",
      "Training loss for batch 1634 : 0.17245888710021973\n",
      "Training loss for batch 1635 : 0.2973145842552185\n",
      "Training loss for batch 1636 : 0.12722055613994598\n",
      "Training loss for batch 1637 : 0.03327304869890213\n",
      "Training loss for batch 1638 : 0.4070631265640259\n",
      "Training loss for batch 1639 : 0.2525968551635742\n",
      "Training loss for batch 1640 : 0.40927210450172424\n",
      "Training loss for batch 1641 : 0.0823558121919632\n",
      "Training loss for batch 1642 : 0.1472933143377304\n",
      "Training loss for batch 1643 : 0.3155035376548767\n",
      "Training loss for batch 1644 : 0.3627586364746094\n",
      "Training loss for batch 1645 : 0.2555384635925293\n",
      "Training loss for batch 1646 : 0.08222728222608566\n",
      "Training loss for batch 1647 : 0.17569845914840698\n",
      "Training loss for batch 1648 : 0.2487904578447342\n",
      "Training loss for batch 1649 : 0.05151274427771568\n",
      "Training loss for batch 1650 : 0.19886644184589386\n",
      "Training loss for batch 1651 : 0.1827319860458374\n",
      "Training loss for batch 1652 : 0.469958633184433\n",
      "Training loss for batch 1653 : 0.015251066535711288\n",
      "Training loss for batch 1654 : 0.10478845238685608\n",
      "Training loss for batch 1655 : 0.09933929890394211\n",
      "Training loss for batch 1656 : 0.2171722650527954\n",
      "Training loss for batch 1657 : 0.14402467012405396\n",
      "Training loss for batch 1658 : 0.21588309109210968\n",
      "Training loss for batch 1659 : 0.21955005824565887\n",
      "Training loss for batch 1660 : 0.3073253333568573\n",
      "Training loss for batch 1661 : 0.22323714196681976\n",
      "Training loss for batch 1662 : 0.4199901223182678\n",
      "Training loss for batch 1663 : 0.10018956661224365\n",
      "Training loss for batch 1664 : 0.044639088213443756\n",
      "Training loss for batch 1665 : 0.03552277386188507\n",
      "Training loss for batch 1666 : 0.025888266041874886\n",
      "Training loss for batch 1667 : 0.36793118715286255\n",
      "Training loss for batch 1668 : 0.276562362909317\n",
      "Training loss for batch 1669 : 0.018580202013254166\n",
      "Training loss for batch 1670 : 0.029426120221614838\n",
      "Training loss for batch 1671 : 0.0628303810954094\n",
      "Training loss for batch 1672 : 0.23771896958351135\n",
      "Training loss for batch 1673 : 0.13534709811210632\n",
      "Training loss for batch 1674 : 0.45136234164237976\n",
      "Training loss for batch 1675 : 0.11713884770870209\n",
      "Training loss for batch 1676 : 0.0846630111336708\n",
      "Training loss for batch 1677 : 0.03696393966674805\n",
      "Training loss for batch 1678 : 0.4281587600708008\n",
      "Training loss for batch 1679 : 0.07211510092020035\n",
      "Training loss for batch 1680 : 0.15484634041786194\n",
      "Training loss for batch 1681 : 0.21317178010940552\n",
      "Training loss for batch 1682 : 0.52969890832901\n",
      "Training loss for batch 1683 : 0.3502311110496521\n",
      "Training loss for batch 1684 : 0.24154886603355408\n",
      "Training loss for batch 1685 : 0.2855718731880188\n",
      "Training loss for batch 1686 : 0.5776796340942383\n",
      "Training loss for batch 1687 : 0.3397511839866638\n",
      "Training loss for batch 1688 : 0.13304170966148376\n",
      "Training loss for batch 1689 : 0.09989519417285919\n",
      "Training loss for batch 1690 : 0.4650997519493103\n",
      "Training loss for batch 1691 : 0.21133172512054443\n",
      "Training loss for batch 1692 : 0.08984396606683731\n",
      "Training loss for batch 1693 : 0.2999892234802246\n",
      "Training loss for batch 1694 : 0.15133564174175262\n",
      "Training loss for batch 1695 : 0.04875049367547035\n",
      "Training loss for batch 1696 : 0.3409394323825836\n",
      "Training loss for batch 1697 : 0.013174502179026604\n",
      "Training loss for batch 1698 : 0.14766548573970795\n",
      "Training loss for batch 1699 : 0.09684350341558456\n",
      "Training loss for batch 1700 : 0.10969864577054977\n",
      "Training loss for batch 1701 : 0.4688693583011627\n",
      "Training loss for batch 1702 : 0.34629395604133606\n",
      "Training loss for batch 1703 : 0.25975459814071655\n",
      "Training loss for batch 1704 : 0.1284174919128418\n",
      "Training loss for batch 1705 : 0.14697381854057312\n",
      "Training loss for batch 1706 : 0.05995034798979759\n",
      "Training loss for batch 1707 : 0.25341925024986267\n",
      "Training loss for batch 1708 : 0.06658909469842911\n",
      "Training loss for batch 1709 : 0.2263544648885727\n",
      "Training loss for batch 1710 : 0.17571642994880676\n",
      "Training loss for batch 1711 : 0.2757880687713623\n",
      "Training loss for batch 1712 : 0.059178635478019714\n",
      "Training loss for batch 1713 : 0.0799403041601181\n",
      "Training loss for batch 1714 : 0.2144361436367035\n",
      "Training loss for batch 1715 : 0.2872735559940338\n",
      "Training loss for batch 1716 : 0.12281371653079987\n",
      "Training loss for batch 1717 : 0.12347864359617233\n",
      "Training loss for batch 1718 : 0.11680859327316284\n",
      "Training loss for batch 1719 : 0.046418290585279465\n",
      "Training loss for batch 1720 : 0.05683520436286926\n",
      "Training loss for batch 1721 : 0.08215188980102539\n",
      "Training loss for batch 1722 : 0.1999155730009079\n",
      "Training loss for batch 1723 : 0.04907931387424469\n",
      "Training loss for batch 1724 : 0.1700482964515686\n",
      "Training loss for batch 1725 : 0.2953115403652191\n",
      "Training loss for batch 1726 : 0.340928852558136\n",
      "Training loss for batch 1727 : 0.2710855007171631\n",
      "Training loss for batch 1728 : 0.4270924925804138\n",
      "Training loss for batch 1729 : 0.25823915004730225\n",
      "Training loss for batch 1730 : 0.07398191094398499\n",
      "Training loss for batch 1731 : 0.058107830584049225\n",
      "Training loss for batch 1732 : 0.2461978793144226\n",
      "Training loss for batch 1733 : 0.2072157859802246\n",
      "Training loss for batch 1734 : 0.13767383992671967\n",
      "Training loss for batch 1735 : 0.3416598439216614\n",
      "Training loss for batch 1736 : 0.23627543449401855\n",
      "Training loss for batch 1737 : 0.09156137704849243\n",
      "Training loss for batch 1738 : 0.14499865472316742\n",
      "Training loss for batch 1739 : 0.05997864529490471\n",
      "Training loss for batch 1740 : 0.11192011833190918\n",
      "Training loss for batch 1741 : 0.09235763549804688\n",
      "Training loss for batch 1742 : 0.3908367156982422\n",
      "Training loss for batch 1743 : 0.23254580795764923\n",
      "Training loss for batch 1744 : 0.1663202941417694\n",
      "Training loss for batch 1745 : 0.07156678289175034\n",
      "Training loss for batch 1746 : 0.8379166126251221\n",
      "Training loss for batch 1747 : 0.00030741095542907715\n",
      "Training loss for batch 1748 : 0.1113801971077919\n",
      "Training loss for batch 1749 : 0.3237239420413971\n",
      "Training loss for batch 1750 : 0.3129903972148895\n",
      "Training loss for batch 1751 : 0.16652651131153107\n",
      "Training loss for batch 1752 : 0.001475852681323886\n",
      "Training loss for batch 1753 : 0.14991416037082672\n",
      "Training loss for batch 1754 : 0.060396745800971985\n",
      "Training loss for batch 1755 : 0.3470507860183716\n",
      "Training loss for batch 1756 : 0.2757832407951355\n",
      "Training loss for batch 1757 : 0.14866819977760315\n",
      "Training loss for batch 1758 : 0.0\n",
      "Training loss for batch 1759 : 0.05324745178222656\n",
      "Training loss for batch 1760 : 0.020197194069623947\n",
      "Training loss for batch 1761 : 0.22088302671909332\n",
      "Training loss for batch 1762 : 0.2706441879272461\n",
      "Training loss for batch 1763 : 0.14159920811653137\n",
      "Training loss for batch 1764 : 0.17571204900741577\n",
      "Training loss for batch 1765 : 0.3225317597389221\n",
      "Training loss for batch 1766 : 0.46018552780151367\n",
      "Training loss for batch 1767 : 0.12109934538602829\n",
      "Training loss for batch 1768 : 0.26818642020225525\n",
      "Training loss for batch 1769 : 0.07643349468708038\n",
      "Training loss for batch 1770 : 0.21793217957019806\n",
      "Training loss for batch 1771 : 0.13000404834747314\n",
      "Training loss for batch 1772 : 0.23940104246139526\n",
      "Training loss for batch 1773 : 0.014439506456255913\n",
      "Training loss for batch 1774 : 0.08931620419025421\n",
      "Training loss for batch 1775 : 0.04528222233057022\n",
      "Training loss for batch 1776 : 0.150946244597435\n",
      "Training loss for batch 1777 : 0.0\n",
      "Training loss for batch 1778 : 0.1990518569946289\n",
      "Training loss for batch 1779 : 0.0\n",
      "Training loss for batch 1780 : 0.07059173285961151\n",
      "Training loss for batch 1781 : 0.33507001399993896\n",
      "Training loss for batch 1782 : 0.30131402611732483\n",
      "Training loss for batch 1783 : 0.22020097076892853\n",
      "Training loss for batch 1784 : 0.07594774663448334\n",
      "Training loss for batch 1785 : 0.12597976624965668\n",
      "Training loss for batch 1786 : 0.18153773248195648\n",
      "Training loss for batch 1787 : 0.08874444663524628\n",
      "Training loss for batch 1788 : 0.05631522089242935\n",
      "Training loss for batch 1789 : 0.4678139090538025\n",
      "Training loss for batch 1790 : 0.012473019771277905\n",
      "Training loss for batch 1791 : 0.1822170615196228\n",
      "Training loss for batch 1792 : 0.08566119521856308\n",
      "Training loss for batch 1793 : 0.231236070394516\n",
      "Training loss for batch 1794 : 0.07065773755311966\n",
      "Training loss for batch 1795 : 0.03952353075146675\n",
      "Training loss for batch 1796 : 0.09577877074480057\n",
      "Training loss for batch 1797 : 0.23859761655330658\n",
      "Training loss for batch 1798 : 0.0644102618098259\n",
      "Training loss for batch 1799 : 0.06552865356206894\n",
      "Training loss for batch 1800 : 0.21533730626106262\n",
      "Training loss for batch 1801 : 0.03952749818563461\n",
      "Training loss for batch 1802 : 0.36004364490509033\n",
      "Training loss for batch 1803 : 0.022707121446728706\n",
      "Training loss for batch 1804 : 0.0602530837059021\n",
      "Training loss for batch 1805 : 0.2644442319869995\n",
      "Training loss for batch 1806 : 0.12897025048732758\n",
      "Training loss for batch 1807 : 0.05098395794630051\n",
      "Training loss for batch 1808 : 0.06941162794828415\n",
      "Training loss for batch 1809 : 0.2025526612997055\n",
      "Training loss for batch 1810 : 0.06596483290195465\n",
      "Training loss for batch 1811 : 0.19218674302101135\n",
      "Training loss for batch 1812 : 0.3768552839756012\n",
      "Training loss for batch 1813 : 0.2569473385810852\n",
      "Training loss for batch 1814 : 0.009428970515727997\n",
      "Training loss for batch 1815 : 0.11811894178390503\n",
      "Training loss for batch 1816 : 0.1053212583065033\n",
      "Training loss for batch 1817 : 0.04951944574713707\n",
      "Training loss for batch 1818 : 0.08698751032352448\n",
      "Training loss for batch 1819 : 0.26535865664482117\n",
      "Training loss for batch 1820 : 0.21311287581920624\n",
      "Training loss for batch 1821 : 0.15110689401626587\n",
      "Training loss for batch 1822 : 0.3820733428001404\n",
      "Training loss for batch 1823 : 0.1102096289396286\n",
      "Training loss for batch 1824 : 0.5026975870132446\n",
      "Training loss for batch 1825 : 0.046732593327760696\n",
      "Training loss for batch 1826 : 0.15624494850635529\n",
      "Training loss for batch 1827 : 0.07064604014158249\n",
      "Training loss for batch 1828 : 0.17187775671482086\n",
      "Training loss for batch 1829 : 0.20553191006183624\n",
      "Training loss for batch 1830 : 0.01802639663219452\n",
      "Training loss for batch 1831 : 0.00830349326133728\n",
      "Training loss for batch 1832 : 0.2631833553314209\n",
      "Training loss for batch 1833 : 0.09103010594844818\n",
      "Training loss for batch 1834 : 0.2801482081413269\n",
      "Training loss for batch 1835 : 0.14847995340824127\n",
      "Training loss for batch 1836 : 0.35182300209999084\n",
      "Training loss for batch 1837 : 0.2230292707681656\n",
      "Training loss for batch 1838 : 0.12387000769376755\n",
      "Training loss for batch 1839 : 0.4078983664512634\n",
      "Training loss for batch 1840 : 0.42648255825042725\n",
      "Training loss for batch 1841 : 0.06246223673224449\n",
      "Training loss for batch 1842 : 0.25606462359428406\n",
      "Training loss for batch 1843 : 0.08707873523235321\n",
      "Training loss for batch 1844 : 0.24907134473323822\n",
      "Training loss for batch 1845 : 0.13304917514324188\n",
      "Training loss for batch 1846 : 0.13059678673744202\n",
      "Training loss for batch 1847 : 0.15154394507408142\n",
      "Training loss for batch 1848 : 0.12960399687290192\n",
      "Training loss for batch 1849 : 0.15173888206481934\n",
      "Training loss for batch 1850 : 0.0558004267513752\n",
      "Training loss for batch 1851 : 0.2709684669971466\n",
      "Training loss for batch 1852 : 0.19318513572216034\n",
      "Training loss for batch 1853 : 0.4900009334087372\n",
      "Training loss for batch 1854 : 0.24926596879959106\n",
      "Training loss for batch 1855 : 0.07310571521520615\n",
      "Training loss for batch 1856 : 0.4599669277667999\n",
      "Training loss for batch 1857 : 0.19385361671447754\n",
      "Training loss for batch 1858 : 0.04630893096327782\n",
      "Training loss for batch 1859 : 0.06205075979232788\n",
      "Training loss for batch 1860 : 0.017238019034266472\n",
      "Training loss for batch 1861 : 0.14424572885036469\n",
      "Training loss for batch 1862 : 0.286598265171051\n",
      "Training loss for batch 1863 : 0.027594618499279022\n",
      "Training loss for batch 1864 : 0.16069649159908295\n",
      "Training loss for batch 1865 : 0.11251518130302429\n",
      "Training loss for batch 1866 : 0.0\n",
      "Training loss for batch 1867 : 0.4197262227535248\n",
      "Training loss for batch 1868 : 0.43719685077667236\n",
      "Training loss for batch 1869 : 0.0508829765021801\n",
      "Training loss for batch 1870 : 0.025632565841078758\n",
      "Training loss for batch 1871 : 0.14291873574256897\n",
      "Training loss for batch 1872 : 0.0900236964225769\n",
      "Training loss for batch 1873 : 0.12856027483940125\n",
      "Training loss for batch 1874 : 0.15114165842533112\n",
      "Training loss for batch 1875 : 0.03497227653861046\n",
      "Training loss for batch 1876 : 0.13619737327098846\n",
      "Training loss for batch 1877 : 0.20557482540607452\n",
      "Training loss for batch 1878 : 0.17424872517585754\n",
      "Training loss for batch 1879 : 0.2548779249191284\n",
      "Training loss for batch 1880 : 0.24005091190338135\n",
      "Training loss for batch 1881 : 0.0613451823592186\n",
      "Training loss for batch 1882 : 0.27250391244888306\n",
      "Training loss for batch 1883 : 0.13499869406223297\n",
      "Training loss for batch 1884 : 0.0945630669593811\n",
      "Training loss for batch 1885 : 0.2009732723236084\n",
      "Training loss for batch 1886 : 0.17029540240764618\n",
      "Training loss for batch 1887 : 0.11260228604078293\n",
      "Training loss for batch 1888 : 0.22459961473941803\n",
      "Training loss for batch 1889 : 0.012785783968865871\n",
      "Training loss for batch 1890 : 0.08839564770460129\n",
      "Training loss for batch 1891 : 0.2765266001224518\n",
      "Training loss for batch 1892 : 0.047953277826309204\n",
      "Training loss for batch 1893 : 0.09358081221580505\n",
      "Training loss for batch 1894 : 0.07909239828586578\n",
      "Training loss for batch 1895 : 0.2504163980484009\n",
      "Training loss for batch 1896 : 0.2957397699356079\n",
      "Training loss for batch 1897 : 0.016384856775403023\n",
      "Training loss for batch 1898 : 0.19992132484912872\n",
      "Training loss for batch 1899 : 0.09133248776197433\n",
      "Training loss for batch 1900 : 0.9776994585990906\n",
      "Training loss for batch 1901 : 0.3776070475578308\n",
      "Training loss for batch 1902 : 0.31137463450431824\n",
      "Training loss for batch 1903 : 0.08431614935398102\n",
      "Training loss for batch 1904 : 0.2132049947977066\n",
      "Training loss for batch 1905 : 0.10324637591838837\n",
      "Training loss for batch 1906 : 0.343696653842926\n",
      "Training loss for batch 1907 : 0.18965689837932587\n",
      "Training loss for batch 1908 : 0.04391728341579437\n",
      "Training loss for batch 1909 : 0.020351389423012733\n",
      "Training loss for batch 1910 : 0.20660577714443207\n",
      "Training loss for batch 1911 : 0.029914170503616333\n",
      "Training loss for batch 1912 : 0.39912620186805725\n",
      "Training loss for batch 1913 : 0.05096675828099251\n",
      "Training loss for batch 1914 : 0.3929433822631836\n",
      "Training loss for batch 1915 : 0.44553378224372864\n",
      "Training loss for batch 1916 : 0.05860759690403938\n",
      "Training loss for batch 1917 : 0.012841762974858284\n",
      "Training loss for batch 1918 : 0.15012133121490479\n",
      "Training loss for batch 1919 : 0.2009134590625763\n",
      "Training loss for batch 1920 : 0.20806066691875458\n",
      "Training loss for batch 1921 : 0.1261168122291565\n",
      "Training loss for batch 1922 : 0.06882386654615402\n",
      "Training loss for batch 1923 : 0.2408420294523239\n",
      "Training loss for batch 1924 : 0.31035587191581726\n",
      "Training loss for batch 1925 : 0.05062646418809891\n",
      "Training loss for batch 1926 : 0.12243878841400146\n",
      "Training loss for batch 1927 : 0.4439815878868103\n",
      "Training loss for batch 1928 : 0.17610986530780792\n",
      "Training loss for batch 1929 : 0.03358173370361328\n",
      "Training loss for batch 1930 : 0.1516367495059967\n",
      "Training loss for batch 1931 : 0.10398996621370316\n",
      "Training loss for batch 1932 : 0.11083533614873886\n",
      "Training loss for batch 1933 : 0.0\n",
      "Training loss for batch 1934 : 0.3344547748565674\n",
      "Training loss for batch 1935 : 0.17155055701732635\n",
      "Training loss for batch 1936 : 0.0023190476931631565\n",
      "Training loss for batch 1937 : 0.045479584485292435\n",
      "Training loss for batch 1938 : 0.10928363353013992\n",
      "Training loss for batch 1939 : 0.2251664102077484\n",
      "Training loss for batch 1940 : 0.33032798767089844\n",
      "Training loss for batch 1941 : 0.23209603130817413\n",
      "Training loss for batch 1942 : 0.5134711861610413\n",
      "Training loss for batch 1943 : 0.19221043586730957\n",
      "Training loss for batch 1944 : 0.02214846946299076\n",
      "Training loss for batch 1945 : 0.0427732840180397\n",
      "Training loss for batch 1946 : 0.21682536602020264\n",
      "Training loss for batch 1947 : 0.04111729934811592\n",
      "Training loss for batch 1948 : 0.31870147585868835\n",
      "Training loss for batch 1949 : 0.175917848944664\n",
      "Training loss for batch 1950 : 0.2604970932006836\n",
      "Training loss for batch 1951 : 0.06363317370414734\n",
      "Training loss for batch 1952 : 0.4592822790145874\n",
      "Training loss for batch 1953 : 0.011186381801962852\n",
      "Training loss for batch 1954 : 0.13864083588123322\n",
      "Training loss for batch 1955 : 0.2618192434310913\n",
      "Training loss for batch 1956 : 0.0026244311593472958\n",
      "Training loss for batch 1957 : 0.3610304892063141\n",
      "Training loss for batch 1958 : 0.33356568217277527\n",
      "Training loss for batch 1959 : 0.07446062564849854\n",
      "Training loss for batch 1960 : 0.14700496196746826\n",
      "Training loss for batch 1961 : 0.12194155901670456\n",
      "Training loss for batch 1962 : 0.22923240065574646\n",
      "Training loss for batch 1963 : 0.06263018399477005\n",
      "Training loss for batch 1964 : 0.08160419762134552\n",
      "Training loss for batch 1965 : 0.34998080134391785\n",
      "Training loss for batch 1966 : 0.011617467738687992\n",
      "Training loss for batch 1967 : 0.37591663002967834\n",
      "Training loss for batch 1968 : 0.3985847234725952\n",
      "Training loss for batch 1969 : 0.18467490375041962\n",
      "Training loss for batch 1970 : 0.16192395985126495\n",
      "Training loss for batch 1971 : 0.008932839147746563\n",
      "Training loss for batch 1972 : 0.11445727944374084\n",
      "Training loss for batch 1973 : 0.09437946230173111\n",
      "Training loss for batch 1974 : 0.20390836894512177\n",
      "Training loss for batch 1975 : 0.2808597981929779\n",
      "Training loss for batch 1976 : 0.04335242137312889\n",
      "Training loss for batch 1977 : 0.06439239531755447\n",
      "Training loss for batch 1978 : 0.14037400484085083\n",
      "Training loss for batch 1979 : 0.5643267035484314\n",
      "Training loss for batch 1980 : 0.11922115087509155\n",
      "Training loss for batch 1981 : 0.1436719447374344\n",
      "Training loss for batch 1982 : 0.12162341177463531\n",
      "Training loss for batch 1983 : 0.24974137544631958\n",
      "Training loss for batch 1984 : 0.20220373570919037\n",
      "Training loss for batch 1985 : 0.020090194419026375\n",
      "Training loss for batch 1986 : 0.08243151009082794\n",
      "Training loss for batch 1987 : 0.0035980078391730785\n",
      "Training loss for batch 1988 : 0.2563644349575043\n",
      "Training loss for batch 1989 : 0.24565932154655457\n",
      "Training loss for batch 1990 : 0.1754496544599533\n",
      "Training loss for batch 1991 : 0.10553909838199615\n",
      "Training loss for batch 1992 : 0.0\n",
      "Training loss for batch 1993 : 0.277859091758728\n",
      "Training loss for batch 1994 : 0.2681010663509369\n",
      "Training loss for batch 1995 : 0.0631701648235321\n",
      "Training loss for batch 1996 : 0.25417205691337585\n",
      "Training loss for batch 1997 : 0.1480785757303238\n",
      "Training loss for batch 1998 : 0.09141912311315536\n",
      "Training loss for batch 1999 : 0.2938164174556732\n",
      "Training loss for batch 2000 : 0.5174914598464966\n",
      "Training loss for batch 2001 : 0.5426083207130432\n",
      "Training loss for batch 2002 : 0.021609509363770485\n",
      "Training loss for batch 2003 : 0.8103502988815308\n",
      "Training loss for batch 2004 : 0.29172518849372864\n",
      "Training loss for batch 2005 : 0.04007496312260628\n",
      "Training loss for batch 2006 : 0.16509035229682922\n",
      "Training loss for batch 2007 : 0.030338935554027557\n",
      "Training loss for batch 2008 : 0.0874360203742981\n",
      "Training loss for batch 2009 : 0.3301492929458618\n",
      "Training loss for batch 2010 : 0.07129526138305664\n",
      "Training loss for batch 2011 : 0.2813035547733307\n",
      "Training loss for batch 2012 : 0.10774777084589005\n",
      "Training loss for batch 2013 : 0.11718589812517166\n",
      "Training loss for batch 2014 : 0.2572927474975586\n",
      "Training loss for batch 2015 : 0.34910330176353455\n",
      "Training loss for batch 2016 : 0.05116201937198639\n",
      "Training loss for batch 2017 : 0.27651435136795044\n",
      "Training loss for batch 2018 : 0.21145141124725342\n",
      "Training loss for batch 2019 : 0.1698051244020462\n",
      "Training loss for batch 2020 : 0.02705361507833004\n",
      "Training loss for batch 2021 : 0.014654777944087982\n",
      "Training loss for batch 2022 : 0.4007851481437683\n",
      "Training loss for batch 2023 : 0.28737449645996094\n",
      "Training loss for batch 2024 : 0.3003561794757843\n",
      "Training loss for batch 2025 : 0.02706918865442276\n",
      "Training loss for batch 2026 : 0.0024387738667428493\n",
      "Training loss for batch 2027 : 0.3117125630378723\n",
      "Training loss for batch 2028 : 0.005682468414306641\n",
      "Training loss for batch 2029 : 0.23939824104309082\n",
      "Training loss for batch 2030 : 0.27525049448013306\n",
      "Training loss for batch 2031 : 0.38843244314193726\n",
      "Training loss for batch 2032 : 0.3336324691772461\n",
      "Training loss for batch 2033 : 0.07368331402540207\n",
      "Training loss for batch 2034 : 0.13665485382080078\n",
      "Training loss for batch 2035 : 0.12813834846019745\n",
      "Training loss for batch 2036 : 0.21543028950691223\n",
      "Training loss for batch 2037 : 0.4832330048084259\n",
      "Training loss for batch 2038 : 0.1940900832414627\n",
      "Training loss for batch 2039 : 0.06872078776359558\n",
      "Training loss for batch 2040 : 0.20602890849113464\n",
      "Training loss for batch 2041 : 0.29316070675849915\n",
      "Training loss for batch 2042 : 0.19187331199645996\n",
      "Training loss for batch 2043 : 0.2934453785419464\n",
      "Training loss for batch 2044 : 0.08087998628616333\n",
      "Training loss for batch 2045 : 0.11894361674785614\n",
      "Training loss for batch 2046 : 0.20199958980083466\n",
      "Training loss for batch 2047 : 0.14311347901821136\n",
      "Training loss for batch 2048 : 0.22499510645866394\n",
      "Training loss for batch 2049 : 0.2824682891368866\n",
      "Training loss for batch 2050 : 0.05729559436440468\n",
      "Training loss for batch 2051 : 0.47521287202835083\n",
      "Training loss for batch 2052 : 0.14926347136497498\n",
      "Training loss for batch 2053 : 0.1418069452047348\n",
      "Training loss for batch 2054 : 0.051510293036699295\n",
      "Training loss for batch 2055 : 0.03654735907912254\n",
      "Training loss for batch 2056 : 0.32226258516311646\n",
      "Training loss for batch 2057 : 0.16412325203418732\n",
      "Training loss for batch 2058 : 0.5729082226753235\n",
      "Training loss for batch 2059 : 0.057366110384464264\n",
      "Training loss for batch 2060 : 0.3025926351547241\n",
      "Training loss for batch 2061 : 0.16197729110717773\n",
      "Training loss for batch 2062 : 0.6465555429458618\n",
      "Training loss for batch 2063 : 0.3516780138015747\n",
      "Training loss for batch 2064 : 0.25170204043388367\n",
      "Training loss for batch 2065 : 0.08608805388212204\n",
      "Training loss for batch 2066 : 0.22215501964092255\n",
      "Training loss for batch 2067 : 0.06859078258275986\n",
      "Training loss for batch 2068 : 0.23110704123973846\n",
      "Training loss for batch 2069 : 0.020056722685694695\n",
      "Training loss for batch 2070 : 0.12976062297821045\n",
      "Training loss for batch 2071 : 0.2271927148103714\n",
      "Training loss for batch 2072 : 0.058110710233449936\n",
      "Training loss for batch 2073 : 0.00890154205262661\n",
      "Training loss for batch 2074 : 0.1671256572008133\n",
      "Training loss for batch 2075 : 0.08283191919326782\n",
      "Training loss for batch 2076 : 0.1880243718624115\n",
      "Training loss for batch 2077 : 0.41584888100624084\n",
      "Training loss for batch 2078 : 0.37241724133491516\n",
      "Training loss for batch 2079 : 0.44218793511390686\n",
      "Training loss for batch 2080 : 0.05518243461847305\n",
      "Training loss for batch 2081 : 0.22032780945301056\n",
      "Training loss for batch 2082 : 0.10870862752199173\n",
      "Training loss for batch 2083 : 0.5374714136123657\n",
      "Training loss for batch 2084 : 0.282633513212204\n",
      "Training loss for batch 2085 : 0.1854841560125351\n",
      "Training loss for batch 2086 : 0.5338875651359558\n",
      "Training loss for batch 2087 : 0.05618671327829361\n",
      "Training loss for batch 2088 : 0.2739509642124176\n",
      "Training loss for batch 2089 : 0.126360222697258\n",
      "Training loss for batch 2090 : 0.06036420539021492\n",
      "Training loss for batch 2091 : 0.4401542544364929\n",
      "Training loss for batch 2092 : 0.09070375561714172\n",
      "Training loss for batch 2093 : 0.4754331111907959\n",
      "Training loss for batch 2094 : 0.28325155377388\n",
      "Training loss for batch 2095 : 0.3318459987640381\n",
      "Training loss for batch 2096 : 0.1066410169005394\n",
      "Training loss for batch 2097 : 0.4017834961414337\n",
      "Training loss for batch 2098 : 0.0\n",
      "Training loss for batch 2099 : 0.19631551206111908\n",
      "Training loss for batch 2100 : 0.24034620821475983\n",
      "Training loss for batch 2101 : 0.1321796327829361\n",
      "Training loss for batch 2102 : 0.3175484240055084\n",
      "Training loss for batch 2103 : 0.0\n",
      "Training loss for batch 2104 : 0.13916274905204773\n",
      "Training loss for batch 2105 : 0.14631550014019012\n",
      "Training loss for batch 2106 : 0.08467452973127365\n",
      "Training loss for batch 2107 : 0.19191224873065948\n",
      "Training loss for batch 2108 : 0.15364649891853333\n",
      "Training loss for batch 2109 : 0.196786567568779\n",
      "Training loss for batch 2110 : 0.6164056658744812\n",
      "Training loss for batch 2111 : 0.06823592633008957\n",
      "Training loss for batch 2112 : 0.0\n",
      "Training loss for batch 2113 : 0.2191549390554428\n",
      "Training loss for batch 2114 : 0.15179462730884552\n",
      "Training loss for batch 2115 : 0.2583586871623993\n",
      "Training loss for batch 2116 : 0.1125991940498352\n",
      "Training loss for batch 2117 : 0.6057297587394714\n",
      "Training loss for batch 2118 : 0.1565197855234146\n",
      "Training loss for batch 2119 : 0.18639115989208221\n",
      "Training loss for batch 2120 : 0.2762998044490814\n",
      "Training loss for batch 2121 : 0.23192986845970154\n",
      "Training loss for batch 2122 : 0.2965432405471802\n",
      "Training loss for batch 2123 : 0.04198841378092766\n",
      "Training loss for batch 2124 : 0.1411145180463791\n",
      "Training loss for batch 2125 : 0.11895298957824707\n",
      "Training loss for batch 2126 : 0.06236938759684563\n",
      "Training loss for batch 2127 : 0.01402241736650467\n",
      "Training loss for batch 2128 : 0.20489072799682617\n",
      "Training loss for batch 2129 : 0.17480137944221497\n",
      "Training loss for batch 2130 : 0.16749469935894012\n",
      "Training loss for batch 2131 : 0.03492895886301994\n",
      "Training loss for batch 2132 : 0.11239031702280045\n",
      "Training loss for batch 2133 : 0.24233655631542206\n",
      "Training loss for batch 2134 : -0.0004677611286751926\n",
      "Training loss for batch 2135 : 0.28546902537345886\n",
      "Training loss for batch 2136 : 0.04966118931770325\n",
      "Training loss for batch 2137 : 0.1508309692144394\n",
      "Training loss for batch 2138 : 0.7022870779037476\n",
      "Training loss for batch 2139 : 0.16055238246917725\n",
      "Training loss for batch 2140 : 0.2831118106842041\n",
      "Training loss for batch 2141 : 0.02520987018942833\n",
      "Training loss for batch 2142 : 0.0671975240111351\n",
      "Training loss for batch 2143 : 0.5381453037261963\n",
      "Training loss for batch 2144 : 0.1353822499513626\n",
      "Training loss for batch 2145 : 0.135704904794693\n",
      "Training loss for batch 2146 : 0.1951715648174286\n",
      "Training loss for batch 2147 : 0.1992640346288681\n",
      "Training loss for batch 2148 : 0.04974627122282982\n",
      "Training loss for batch 2149 : 0.03572933003306389\n",
      "Training loss for batch 2150 : 0.09282334893941879\n",
      "Training loss for batch 2151 : 0.08993814885616302\n",
      "Training loss for batch 2152 : 0.2876417934894562\n",
      "Training loss for batch 2153 : 0.029985269531607628\n",
      "Training loss for batch 2154 : 0.3924313485622406\n",
      "Training loss for batch 2155 : 0.07744120806455612\n",
      "Training loss for batch 2156 : 0.3061385154724121\n",
      "Training loss for batch 2157 : 0.008588996715843678\n",
      "Training loss for batch 2158 : 0.3511630594730377\n",
      "Training loss for batch 2159 : 0.10265325009822845\n",
      "Training loss for batch 2160 : 0.3056372106075287\n",
      "Training loss for batch 2161 : 0.28737494349479675\n",
      "Training loss for batch 2162 : 0.13647763431072235\n",
      "Training loss for batch 2163 : 0.286413311958313\n",
      "Training loss for batch 2164 : 0.1545385718345642\n",
      "Training loss for batch 2165 : 0.06554580479860306\n",
      "Training loss for batch 2166 : 0.06725464761257172\n",
      "Training loss for batch 2167 : 0.13099950551986694\n",
      "Training loss for batch 2168 : 0.09848398715257645\n",
      "Training loss for batch 2169 : 0.093505859375\n",
      "Training loss for batch 2170 : 0.5486690402030945\n",
      "Training loss for batch 2171 : 0.16520479321479797\n",
      "Training loss for batch 2172 : 0.1383795589208603\n",
      "Training loss for batch 2173 : 0.4934670329093933\n",
      "Training loss for batch 2174 : 0.2877401113510132\n",
      "Training loss for batch 2175 : 0.04991934448480606\n",
      "Training loss for batch 2176 : 0.0022558271884918213\n",
      "Training loss for batch 2177 : 0.30694520473480225\n",
      "Training loss for batch 2178 : 0.3472309410572052\n",
      "Training loss for batch 2179 : 0.21530097723007202\n",
      "Training loss for batch 2180 : 0.09221292287111282\n",
      "Training loss for batch 2181 : 0.14217016100883484\n",
      "Training loss for batch 2182 : 0.11349241435527802\n",
      "Training loss for batch 2183 : 0.24155564606189728\n",
      "Training loss for batch 2184 : 0.3974425494670868\n",
      "Training loss for batch 2185 : 0.1761607974767685\n",
      "Training loss for batch 2186 : 0.1559145152568817\n",
      "Training loss for batch 2187 : 0.00011090285261161625\n",
      "Training loss for batch 2188 : 0.005865112878382206\n",
      "Training loss for batch 2189 : -7.68314566812478e-06\n",
      "Training loss for batch 2190 : 0.07459091395139694\n",
      "Training loss for batch 2191 : 0.07740842550992966\n",
      "Training loss for batch 2192 : 0.08382673561573029\n",
      "Training loss for batch 2193 : 0.002876767423003912\n",
      "Training loss for batch 2194 : 0.23776891827583313\n",
      "Training loss for batch 2195 : 0.10953358560800552\n",
      "Training loss for batch 2196 : 0.008270672522485256\n",
      "Training loss for batch 2197 : 0.3817223906517029\n",
      "Training loss for batch 2198 : 0.007929712533950806\n",
      "Training loss for batch 2199 : 0.1706225574016571\n",
      "Training loss for batch 2200 : 0.21414850652217865\n",
      "Training loss for batch 2201 : 0.10848219692707062\n",
      "Training loss for batch 2202 : 0.08982932567596436\n",
      "Training loss for batch 2203 : 0.2417488396167755\n",
      "Training loss for batch 2204 : 0.06091512739658356\n",
      "Training loss for batch 2205 : 0.27425098419189453\n",
      "Training loss for batch 2206 : 0.4848770499229431\n",
      "Training loss for batch 2207 : 0.0531499981880188\n",
      "Training loss for batch 2208 : 0.10752309113740921\n",
      "Training loss for batch 2209 : 0.11415541917085648\n",
      "Training loss for batch 2210 : 0.09190037846565247\n",
      "Training loss for batch 2211 : 0.30669721961021423\n",
      "Training loss for batch 2212 : 0.12498390674591064\n",
      "Training loss for batch 2213 : 0.20329144597053528\n",
      "Training loss for batch 2214 : 0.3406883180141449\n",
      "Training loss for batch 2215 : 0.24564270675182343\n",
      "Training loss for batch 2216 : 0.1381450593471527\n",
      "Training loss for batch 2217 : 0.18924644589424133\n",
      "Training loss for batch 2218 : 0.3427875339984894\n",
      "Training loss for batch 2219 : 0.10611897706985474\n",
      "Training loss for batch 2220 : 0.08518598973751068\n",
      "Training loss for batch 2221 : 0.11249509453773499\n",
      "Training loss for batch 2222 : 0.5691231489181519\n",
      "Training loss for batch 2223 : 0.3125014007091522\n",
      "Training loss for batch 2224 : 0.18426384031772614\n",
      "Training loss for batch 2225 : 0.2029171735048294\n",
      "Training loss for batch 2226 : 0.03794405981898308\n",
      "Training loss for batch 2227 : 0.15073570609092712\n",
      "Training loss for batch 2228 : 0.34317195415496826\n",
      "Training loss for batch 2229 : 0.2319006621837616\n",
      "Training loss for batch 2230 : 0.03906988725066185\n",
      "Training loss for batch 2231 : 0.04343515262007713\n",
      "Training loss for batch 2232 : 0.5105123519897461\n",
      "Training loss for batch 2233 : 0.19169239699840546\n",
      "Training loss for batch 2234 : 0.12163560092449188\n",
      "Training loss for batch 2235 : 0.029263287782669067\n",
      "Training loss for batch 2236 : 0.37791773676872253\n",
      "Training loss for batch 2237 : 0.028106866404414177\n",
      "Training loss for batch 2238 : 0.1955142468214035\n",
      "Training loss for batch 2239 : 0.17383526265621185\n",
      "Training loss for batch 2240 : 0.10665375739336014\n",
      "Training loss for batch 2241 : 0.10253126919269562\n",
      "Training loss for batch 2242 : 0.3062093257904053\n",
      "Training loss for batch 2243 : 0.23013170063495636\n",
      "Training loss for batch 2244 : 0.1362953633069992\n",
      "Training loss for batch 2245 : 0.06834723055362701\n",
      "Training loss for batch 2246 : 0.018591927364468575\n",
      "Training loss for batch 2247 : 0.056859247386455536\n",
      "Training loss for batch 2248 : 0.025153672322630882\n",
      "Training loss for batch 2249 : 0.39877739548683167\n",
      "Training loss for batch 2250 : 0.3111441135406494\n",
      "Training loss for batch 2251 : 0.10654139518737793\n",
      "Training loss for batch 2252 : 0.21930523216724396\n",
      "Training loss for batch 2253 : 0.028209863230586052\n",
      "Training loss for batch 2254 : 0.15027005970478058\n",
      "Training loss for batch 2255 : 0.41291260719299316\n",
      "Training loss for batch 2256 : 0.22929680347442627\n",
      "Training loss for batch 2257 : 0.050560250878334045\n",
      "Training loss for batch 2258 : 0.06997068226337433\n",
      "Training loss for batch 2259 : 0.42840152978897095\n",
      "Training loss for batch 2260 : 0.0\n",
      "Training loss for batch 2261 : 0.050037089735269547\n",
      "Training loss for batch 2262 : 0.37952688336372375\n",
      "Training loss for batch 2263 : 0.25862425565719604\n",
      "Training loss for batch 2264 : 0.18150563538074493\n",
      "Training loss for batch 2265 : 0.01551454421132803\n",
      "Training loss for batch 2266 : 0.4215555489063263\n",
      "Training loss for batch 2267 : 0.007460752502083778\n",
      "Training loss for batch 2268 : 0.16325467824935913\n",
      "Training loss for batch 2269 : 0.2459404468536377\n",
      "Training loss for batch 2270 : 0.0993715301156044\n",
      "Training loss for batch 2271 : 0.328372985124588\n",
      "Training loss for batch 2272 : 0.0052733090706169605\n",
      "Training loss for batch 2273 : 0.05463024228811264\n",
      "Training loss for batch 2274 : 0.03505149111151695\n",
      "Training loss for batch 2275 : 0.1167702004313469\n",
      "Training loss for batch 2276 : 0.221649631857872\n",
      "Training loss for batch 2277 : 0.08007946610450745\n",
      "Training loss for batch 2278 : 0.2873929738998413\n",
      "Training loss for batch 2279 : 0.3222484886646271\n",
      "Training loss for batch 2280 : 0.19291242957115173\n",
      "Training loss for batch 2281 : 0.055246010422706604\n",
      "Training loss for batch 2282 : 0.03620149567723274\n",
      "Training loss for batch 2283 : 0.36282768845558167\n",
      "Training loss for batch 2284 : 0.06667286157608032\n",
      "Training loss for batch 2285 : 0.29054969549179077\n",
      "Training loss for batch 2286 : 0.1521337330341339\n",
      "Training loss for batch 2287 : 0.0\n",
      "Training loss for batch 2288 : 0.059460386633872986\n",
      "Training loss for batch 2289 : 0.16979210078716278\n",
      "Training loss for batch 2290 : 0.10586155951023102\n",
      "Training loss for batch 2291 : 0.15498733520507812\n",
      "Training loss for batch 2292 : 0.39977166056632996\n",
      "Training loss for batch 2293 : 0.041159067302942276\n",
      "Training loss for batch 2294 : 0.08407605439424515\n",
      "Training loss for batch 2295 : 0.06064683198928833\n",
      "Training loss for batch 2296 : 0.24135881662368774\n",
      "Training loss for batch 2297 : 0.3568515181541443\n",
      "Training loss for batch 2298 : 0.22622868418693542\n",
      "Training loss for batch 2299 : 0.06339268386363983\n",
      "Training loss for batch 2300 : 0.1809312105178833\n",
      "Training loss for batch 2301 : 0.06577353924512863\n",
      "Training loss for batch 2302 : 0.030684027820825577\n",
      "Training loss for batch 2303 : 0.009204510599374771\n",
      "Training loss for batch 2304 : 0.22321459650993347\n",
      "Training loss for batch 2305 : 0.019959663972258568\n",
      "Training loss for batch 2306 : 0.2639610469341278\n",
      "Training loss for batch 2307 : 0.18370908498764038\n",
      "Training loss for batch 2308 : 0.17728324234485626\n",
      "Training loss for batch 2309 : 0.09534721821546555\n",
      "Training loss for batch 2310 : 0.16650110483169556\n",
      "Training loss for batch 2311 : 0.03090752847492695\n",
      "Training loss for batch 2312 : 0.006815909408032894\n",
      "Training loss for batch 2313 : 0.7317997813224792\n",
      "Training loss for batch 2314 : 0.15131530165672302\n",
      "Training loss for batch 2315 : 0.16720598936080933\n",
      "Training loss for batch 2316 : 0.1288619339466095\n",
      "Training loss for batch 2317 : 0.13045211136341095\n",
      "Training loss for batch 2318 : 0.15945596992969513\n",
      "Training loss for batch 2319 : 0.16987282037734985\n",
      "Training loss for batch 2320 : 0.17912960052490234\n",
      "Training loss for batch 2321 : 0.14131854474544525\n",
      "Training loss for batch 2322 : 0.24436526000499725\n",
      "Training loss for batch 2323 : 0.025245659053325653\n",
      "Training loss for batch 2324 : 0.357075035572052\n",
      "Training loss for batch 2325 : 0.139237642288208\n",
      "Training loss for batch 2326 : 0.025391539558768272\n",
      "Training loss for batch 2327 : 0.16442881524562836\n",
      "Training loss for batch 2328 : 0.10677262395620346\n",
      "Training loss for batch 2329 : 0.32851555943489075\n",
      "Training loss for batch 2330 : 0.16904132068157196\n",
      "Training loss for batch 2331 : 0.24792297184467316\n",
      "Training loss for batch 2332 : 0.1764230728149414\n",
      "Training loss for batch 2333 : 0.06930079311132431\n",
      "Training loss for batch 2334 : 0.09007981419563293\n",
      "Training loss for batch 2335 : 0.19042626023292542\n",
      "Training loss for batch 2336 : 0.09578317403793335\n",
      "Training loss for batch 2337 : 0.43273603916168213\n",
      "Training loss for batch 2338 : 0.10274814069271088\n",
      "Training loss for batch 2339 : 0.022294174879789352\n",
      "Training loss for batch 2340 : 0.08744256943464279\n",
      "Training loss for batch 2341 : 0.3151397109031677\n",
      "Training loss for batch 2342 : 0.03972021862864494\n",
      "Training loss for batch 2343 : 0.018576251342892647\n",
      "Training loss for batch 2344 : -0.00035511405440047383\n",
      "Training loss for batch 2345 : 0.06971276551485062\n",
      "Training loss for batch 2346 : 0.0630359873175621\n",
      "Training loss for batch 2347 : 0.20271700620651245\n",
      "Training loss for batch 2348 : 0.038312919437885284\n",
      "Training loss for batch 2349 : 0.10295648127794266\n",
      "Training loss for batch 2350 : 0.3028585910797119\n",
      "Training loss for batch 2351 : 0.30082592368125916\n",
      "Training loss for batch 2352 : 0.08134177327156067\n",
      "Training loss for batch 2353 : 0.4123821258544922\n",
      "Training loss for batch 2354 : 0.21680374443531036\n",
      "Training loss for batch 2355 : 0.2621670365333557\n",
      "Training loss for batch 2356 : 0.12929615378379822\n",
      "Training loss for batch 2357 : 0.10257598757743835\n",
      "Training loss for batch 2358 : 0.025499802082777023\n",
      "Training loss for batch 2359 : 0.1864224374294281\n",
      "Training loss for batch 2360 : 0.1320970505475998\n",
      "Training loss for batch 2361 : 0.019413676112890244\n",
      "Training loss for batch 2362 : 0.17604805529117584\n",
      "Training loss for batch 2363 : 0.7188666462898254\n",
      "Training loss for batch 2364 : 0.2984734773635864\n",
      "Training loss for batch 2365 : 0.13094966113567352\n",
      "Training loss for batch 2366 : 0.22267037630081177\n",
      "Training loss for batch 2367 : 0.06190712749958038\n",
      "Training loss for batch 2368 : 0.4200611710548401\n",
      "Training loss for batch 2369 : 0.10723727196455002\n",
      "Training loss for batch 2370 : 0.2891618013381958\n",
      "Training loss for batch 2371 : 0.13218624889850616\n",
      "Training loss for batch 2372 : 0.04210207238793373\n",
      "Training loss for batch 2373 : 0.14046503603458405\n",
      "Training loss for batch 2374 : 0.3202240765094757\n",
      "Training loss for batch 2375 : 0.3405555188655853\n",
      "Training loss for batch 2376 : 0.21444019675254822\n",
      "Training loss for batch 2377 : 0.18438312411308289\n",
      "Training loss for batch 2378 : 0.3906197249889374\n",
      "Training loss for batch 2379 : 0.4656071066856384\n",
      "Training loss for batch 2380 : 0.013984139077365398\n",
      "Training loss for batch 2381 : 0.16242437064647675\n",
      "Training loss for batch 2382 : 0.15443646907806396\n",
      "Training loss for batch 2383 : 0.2714715003967285\n",
      "Training loss for batch 2384 : 0.047065239399671555\n",
      "Training loss for batch 2385 : 0.23220446705818176\n",
      "Training loss for batch 2386 : 1.0641273260116577\n",
      "Training loss for batch 2387 : 0.3607082664966583\n",
      "Training loss for batch 2388 : 0.4440253973007202\n",
      "Training loss for batch 2389 : 0.5353456735610962\n",
      "Training loss for batch 2390 : 0.05859272554516792\n",
      "Training loss for batch 2391 : 0.06561499089002609\n",
      "Training loss for batch 2392 : 0.16749809682369232\n",
      "Training loss for batch 2393 : 0.20036225020885468\n",
      "Training loss for batch 2394 : 0.4414258897304535\n",
      "Training loss for batch 2395 : 0.09673482924699783\n",
      "Training loss for batch 2396 : 0.4777015745639801\n",
      "Training loss for batch 2397 : 0.3798792064189911\n",
      "Training loss for batch 2398 : 0.11256871372461319\n",
      "Training loss for batch 2399 : 0.0\n",
      "Training loss for batch 2400 : 0.03335632383823395\n",
      "Training loss for batch 2401 : 0.060853518545627594\n",
      "Training loss for batch 2402 : 0.48697352409362793\n",
      "Training loss for batch 2403 : 0.032225340604782104\n",
      "Training loss for batch 2404 : 0.022276435047388077\n",
      "Training loss for batch 2405 : 0.3329835534095764\n",
      "Training loss for batch 2406 : 0.15160655975341797\n",
      "Training loss for batch 2407 : 0.05036136507987976\n",
      "Training loss for batch 2408 : 0.0669218972325325\n",
      "Training loss for batch 2409 : 0.2210194617509842\n",
      "Training loss for batch 2410 : 0.24668611586093903\n",
      "Training loss for batch 2411 : 0.44452065229415894\n",
      "Training loss for batch 2412 : 0.07780737429857254\n",
      "Training loss for batch 2413 : 0.3816428482532501\n",
      "Training loss for batch 2414 : 0.25223127007484436\n",
      "Training loss for batch 2415 : 0.059878673404455185\n",
      "Training loss for batch 2416 : 0.20915764570236206\n",
      "Training loss for batch 2417 : 0.05418623238801956\n",
      "Training loss for batch 2418 : 0.09990954399108887\n",
      "Training loss for batch 2419 : 0.031988587230443954\n",
      "Training loss for batch 2420 : 0.22001905739307404\n",
      "Training loss for batch 2421 : 0.0065666972659528255\n",
      "Training loss for batch 2422 : 0.10647372156381607\n",
      "Training loss for batch 2423 : 0.06157815828919411\n",
      "Training loss for batch 2424 : 0.05347407981753349\n",
      "Training loss for batch 2425 : 0.3136793076992035\n",
      "Training loss for batch 2426 : 0.1679067611694336\n",
      "Training loss for batch 2427 : 0.09480619430541992\n",
      "Training loss for batch 2428 : 0.0633627399802208\n",
      "Training loss for batch 2429 : 0.2311345487833023\n",
      "Training loss for batch 2430 : 0.13360731303691864\n",
      "Training loss for batch 2431 : 0.14734594523906708\n",
      "Training loss for batch 2432 : 0.5136942267417908\n",
      "Training loss for batch 2433 : 0.0821702629327774\n",
      "Training loss for batch 2434 : 0.05658167600631714\n",
      "Training loss for batch 2435 : 0.24214869737625122\n",
      "Training loss for batch 2436 : 0.34980419278144836\n",
      "Training loss for batch 2437 : 0.23655937612056732\n",
      "Training loss for batch 2438 : 0.11637018620967865\n",
      "Training loss for batch 2439 : 0.26829108595848083\n",
      "Training loss for batch 2440 : 0.2650153636932373\n",
      "Training loss for batch 2441 : 0.011137631721794605\n",
      "Training loss for batch 2442 : -5.17991284141317e-05\n",
      "Training loss for batch 2443 : 0.22868704795837402\n",
      "Training loss for batch 2444 : 0.11461517214775085\n",
      "Training loss for batch 2445 : 0.21915410459041595\n",
      "Training loss for batch 2446 : 0.4824294447898865\n",
      "Training loss for batch 2447 : 0.4307127892971039\n",
      "Training loss for batch 2448 : 0.5576545596122742\n",
      "Training loss for batch 2449 : 0.0008947494789026678\n",
      "Training loss for batch 2450 : 0.04689644277095795\n",
      "Training loss for batch 2451 : 0.09451619535684586\n",
      "Training loss for batch 2452 : 0.11575597524642944\n",
      "Training loss for batch 2453 : 0.28841432929039\n",
      "Training loss for batch 2454 : 0.20655636489391327\n",
      "Training loss for batch 2455 : 0.02814513072371483\n",
      "Training loss for batch 2456 : 0.8949782848358154\n",
      "Training loss for batch 2457 : 0.017944328486919403\n",
      "Training loss for batch 2458 : 0.06761258095502853\n",
      "Training loss for batch 2459 : 0.07878793030977249\n",
      "Training loss for batch 2460 : 0.16795165836811066\n",
      "Training loss for batch 2461 : 0.28337860107421875\n",
      "Training loss for batch 2462 : 0.21482408046722412\n",
      "Training loss for batch 2463 : 0.3168959617614746\n",
      "Training loss for batch 2464 : 0.02992076799273491\n",
      "Training loss for batch 2465 : 0.09352542459964752\n",
      "Training loss for batch 2466 : 0.0256099384278059\n",
      "Training loss for batch 2467 : 0.3986729681491852\n",
      "Training loss for batch 2468 : 0.05504974350333214\n",
      "Training loss for batch 2469 : 0.18962232768535614\n",
      "Training loss for batch 2470 : 0.6592210531234741\n",
      "Training loss for batch 2471 : 0.04010344669222832\n",
      "Training loss for batch 2472 : 0.17051565647125244\n",
      "Training loss for batch 2473 : 0.05974811315536499\n",
      "Training loss for batch 2474 : 0.37447434663772583\n",
      "Training loss for batch 2475 : 0.07436587661504745\n",
      "Training loss for batch 2476 : 0.15659859776496887\n",
      "Training loss for batch 2477 : 0.49067622423171997\n",
      "Training loss for batch 2478 : 0.30619698762893677\n",
      "Training loss for batch 2479 : 0.1280667781829834\n",
      "Training loss for batch 2480 : 0.02774151973426342\n",
      "Training loss for batch 2481 : 0.1040019765496254\n",
      "Training loss for batch 2482 : 0.18973056972026825\n",
      "Training loss for batch 2483 : 0.2655012011528015\n",
      "Training loss for batch 2484 : 0.24444659054279327\n",
      "Training loss for batch 2485 : 0.1352931559085846\n",
      "Training loss for batch 2486 : 0.17896580696105957\n",
      "Training loss for batch 2487 : 0.20119012892246246\n",
      "Training loss for batch 2488 : 0.05625728517770767\n",
      "Training loss for batch 2489 : 0.35710853338241577\n",
      "Training loss for batch 2490 : 0.38518717885017395\n",
      "Training loss for batch 2491 : 0.11394217610359192\n",
      "Training loss for batch 2492 : 0.057331304997205734\n",
      "Training loss for batch 2493 : 0.16365088522434235\n",
      "Training loss for batch 2494 : 0.2335098683834076\n",
      "Training loss for batch 2495 : 0.1881243884563446\n",
      "Training loss for batch 2496 : 0.2542649507522583\n",
      "Training loss for batch 2497 : 0.0291737150400877\n",
      "Training loss for batch 2498 : 0.20331937074661255\n",
      "Training loss for batch 2499 : 0.27904012799263\n",
      "Training loss for batch 2500 : 0.3564569354057312\n",
      "Training loss for batch 2501 : 0.2153310775756836\n",
      "Training loss for batch 2502 : 0.2525225281715393\n",
      "Training loss for batch 2503 : 0.07409118115901947\n",
      "Training loss for batch 2504 : 0.19648246467113495\n",
      "Training loss for batch 2505 : 0.025568267330527306\n",
      "Training loss for batch 2506 : 0.05287645757198334\n",
      "Training loss for batch 2507 : 0.29037365317344666\n",
      "Training loss for batch 2508 : 0.37898507714271545\n",
      "Training loss for batch 2509 : 0.22929786145687103\n",
      "Training loss for batch 2510 : 0.08832184225320816\n",
      "Training loss for batch 2511 : 0.3017626702785492\n",
      "Training loss for batch 2512 : 0.19685523211956024\n",
      "Training loss for batch 2513 : 0.1529518961906433\n",
      "Training loss for batch 2514 : 0.17160135507583618\n",
      "Training loss for batch 2515 : 0.31516894698143005\n",
      "Training loss for batch 2516 : 0.18368485569953918\n",
      "Training loss for batch 2517 : 0.14359675347805023\n",
      "Training loss for batch 2518 : 0.2058841735124588\n",
      "Training loss for batch 2519 : 0.007405984681099653\n",
      "Training loss for batch 2520 : 0.10206267237663269\n",
      "Training loss for batch 2521 : 0.13189373910427094\n",
      "Training loss for batch 2522 : 0.1835745871067047\n",
      "Training loss for batch 2523 : 0.18275517225265503\n",
      "Training loss for batch 2524 : 0.4920521378517151\n",
      "Training loss for batch 2525 : 0.4865530729293823\n",
      "Training loss for batch 2526 : 0.1453254222869873\n",
      "Training loss for batch 2527 : 0.24653436243534088\n",
      "Training loss for batch 2528 : 0.18784567713737488\n",
      "Training loss for batch 2529 : 0.13901710510253906\n",
      "Training loss for batch 2530 : 0.12597225606441498\n",
      "Training loss for batch 2531 : 0.027245625853538513\n",
      "Training loss for batch 2532 : 0.03291185200214386\n",
      "Training loss for batch 2533 : 0.11492150276899338\n",
      "Training loss for batch 2534 : 0.1878148913383484\n",
      "Training loss for batch 2535 : 0.18099555373191833\n",
      "Training loss for batch 2536 : 0.1374766230583191\n",
      "Training loss for batch 2537 : 0.6500592827796936\n",
      "Training loss for batch 2538 : 0.1599997878074646\n",
      "Training loss for batch 2539 : 0.2730054557323456\n",
      "Training loss for batch 2540 : 0.18815092742443085\n",
      "Training loss for batch 2541 : 0.07176592200994492\n",
      "Training loss for batch 2542 : 0.0650506541132927\n",
      "Training loss for batch 2543 : 0.11742379516363144\n",
      "Training loss for batch 2544 : 0.033749885857105255\n",
      "Training loss for batch 2545 : 0.3741351366043091\n",
      "Training loss for batch 2546 : 0.16554483771324158\n",
      "Training loss for batch 2547 : 0.410115122795105\n",
      "Training loss for batch 2548 : 0.3286001980304718\n",
      "Training loss for batch 2549 : 0.26892897486686707\n",
      "Training loss for batch 2550 : 0.03351601958274841\n",
      "Training loss for batch 2551 : 0.137344628572464\n",
      "Training loss for batch 2552 : 0.12853951752185822\n",
      "Training loss for batch 2553 : 0.23832283914089203\n",
      "Training loss for batch 2554 : 0.20724992454051971\n",
      "Training loss for batch 2555 : 0.07832716405391693\n",
      "Training loss for batch 2556 : 0.030615486204624176\n",
      "Training loss for batch 2557 : 0.2112599015235901\n",
      "Training loss for batch 2558 : 0.04311951994895935\n",
      "Training loss for batch 2559 : 0.32442840933799744\n",
      "Training loss for batch 2560 : 0.2659257650375366\n",
      "Training loss for batch 2561 : 0.15487317740917206\n",
      "Training loss for batch 2562 : 0.2631049156188965\n",
      "Training loss for batch 2563 : 0.41834211349487305\n",
      "Training loss for batch 2564 : 0.2918892204761505\n",
      "Training loss for batch 2565 : 0.05106502026319504\n",
      "Training loss for batch 2566 : 0.03423652797937393\n",
      "Training loss for batch 2567 : 0.00020232796669006348\n",
      "Training loss for batch 2568 : 0.08338303864002228\n",
      "Training loss for batch 2569 : 0.19062219560146332\n",
      "Training loss for batch 2570 : 0.12616024911403656\n",
      "Training loss for batch 2571 : 0.34653398394584656\n",
      "Training loss for batch 2572 : 0.17085810005664825\n",
      "Training loss for batch 2573 : 0.16740579903125763\n",
      "Training loss for batch 2574 : 0.09123991429805756\n",
      "Training loss for batch 2575 : 0.22849780321121216\n",
      "Training loss for batch 2576 : 0.08988115191459656\n",
      "Training loss for batch 2577 : 0.03472740575671196\n",
      "Training loss for batch 2578 : 0.042164865881204605\n",
      "Training loss for batch 2579 : 0.2331007719039917\n",
      "Training loss for batch 2580 : 0.4388001263141632\n",
      "Training loss for batch 2581 : 0.12065502256155014\n",
      "Training loss for batch 2582 : 0.20386679470539093\n",
      "Training loss for batch 2583 : 0.05286678299307823\n",
      "Training loss for batch 2584 : 0.08260080963373184\n",
      "Training loss for batch 2585 : 0.4546439051628113\n",
      "Training loss for batch 2586 : 0.037748001515865326\n",
      "Training loss for batch 2587 : 0.08527809381484985\n",
      "Training loss for batch 2588 : 0.3092572093009949\n",
      "Training loss for batch 2589 : 0.00640625786036253\n",
      "Training loss for batch 2590 : 0.16623805463314056\n",
      "Training loss for batch 2591 : 0.11904305964708328\n",
      "Training loss for batch 2592 : 0.20137910544872284\n",
      "Training loss for batch 2593 : 1.1006214618682861\n",
      "Training loss for batch 2594 : 0.17121490836143494\n",
      "Training loss for batch 2595 : 0.14063258469104767\n",
      "Training loss for batch 2596 : 0.3213355541229248\n",
      "Training loss for batch 2597 : 0.2205682396888733\n",
      "Training loss for batch 2598 : 0.10584307461977005\n",
      "Training loss for batch 2599 : 0.09518368542194366\n",
      "Training loss for batch 2600 : 0.48383858799934387\n",
      "Training loss for batch 2601 : 0.2136193960905075\n",
      "Training loss for batch 2602 : 0.07107105106115341\n",
      "Training loss for batch 2603 : 0.0684080645442009\n",
      "Training loss for batch 2604 : 0.21724644303321838\n",
      "Training loss for batch 2605 : 0.10144364833831787\n",
      "Training loss for batch 2606 : 0.3233356773853302\n",
      "Training loss for batch 2607 : 0.3595120310783386\n",
      "Training loss for batch 2608 : 0.04831381142139435\n",
      "Training loss for batch 2609 : 0.06401002407073975\n",
      "Training loss for batch 2610 : 0.03005622886121273\n",
      "Training loss for batch 2611 : 0.08600636571645737\n",
      "Training loss for batch 2612 : 0.13467246294021606\n",
      "Training loss for batch 2613 : 0.3437586724758148\n",
      "Training loss for batch 2614 : 0.3725746273994446\n",
      "Training loss for batch 2615 : 0.20127035677433014\n",
      "Training loss for batch 2616 : 0.2770867943763733\n",
      "Training loss for batch 2617 : 0.02599470689892769\n",
      "Training loss for batch 2618 : 0.18358929455280304\n",
      "Training loss for batch 2619 : 0.26277029514312744\n",
      "Training loss for batch 2620 : 0.08161156624555588\n",
      "Training loss for batch 2621 : 0.38695618510246277\n",
      "Training loss for batch 2622 : 0.12807971239089966\n",
      "Training loss for batch 2623 : 0.509424090385437\n",
      "Training loss for batch 2624 : 0.002040617633610964\n",
      "Training loss for batch 2625 : 0.08246596157550812\n",
      "Training loss for batch 2626 : 0.042671725153923035\n",
      "Training loss for batch 2627 : 0.14900127053260803\n",
      "Training loss for batch 2628 : 0.18270012736320496\n",
      "Training loss for batch 2629 : 0.3630659878253937\n",
      "Training loss for batch 2630 : 0.08360818028450012\n",
      "Training loss for batch 2631 : 0.007170121185481548\n",
      "Training loss for batch 2632 : 0.372483491897583\n",
      "Training loss for batch 2633 : 0.2098545879125595\n",
      "Training loss for batch 2634 : 0.2526870667934418\n",
      "Training loss for batch 2635 : 0.474965363740921\n",
      "Training loss for batch 2636 : 0.19082404673099518\n",
      "Training loss for batch 2637 : 0.11939284205436707\n",
      "Training loss for batch 2638 : 0.1476580649614334\n",
      "Training loss for batch 2639 : 0.1451844871044159\n",
      "Training loss for batch 2640 : 0.08802850544452667\n",
      "Training loss for batch 2641 : 0.00014990204363130033\n",
      "Training loss for batch 2642 : 0.3066839873790741\n",
      "Training loss for batch 2643 : 0.18726639449596405\n",
      "Training loss for batch 2644 : 0.0884845107793808\n",
      "Training loss for batch 2645 : 0.29758012294769287\n",
      "Training loss for batch 2646 : 0.48113393783569336\n",
      "Training loss for batch 2647 : 0.1751621961593628\n",
      "Training loss for batch 2648 : 0.02089030109345913\n",
      "Training loss for batch 2649 : 0.19857972860336304\n",
      "Training loss for batch 2650 : 0.020355692133307457\n",
      "Training loss for batch 2651 : 0.12646254897117615\n",
      "Training loss for batch 2652 : 0.13418737053871155\n",
      "Training loss for batch 2653 : 0.16190862655639648\n",
      "Training loss for batch 2654 : 0.14993157982826233\n",
      "Training loss for batch 2655 : 0.1418536752462387\n",
      "Training loss for batch 2656 : 0.2731064260005951\n",
      "Training loss for batch 2657 : 0.047488026320934296\n",
      "Training loss for batch 2658 : 0.3864102065563202\n",
      "Training loss for batch 2659 : 0.17045068740844727\n",
      "Training loss for batch 2660 : 0.13695918023586273\n",
      "Training loss for batch 2661 : 0.11979255080223083\n",
      "Training loss for batch 2662 : 0.35681501030921936\n",
      "Training loss for batch 2663 : 0.2641388475894928\n",
      "Training loss for batch 2664 : 0.40274739265441895\n",
      "Training loss for batch 2665 : 0.2947486340999603\n",
      "Training loss for batch 2666 : 0.1250852793455124\n",
      "Training loss for batch 2667 : 0.13105784356594086\n",
      "Training loss for batch 2668 : 0.06544508785009384\n",
      "Training loss for batch 2669 : 0.2505813241004944\n",
      "Training loss for batch 2670 : 0.33482229709625244\n",
      "Training loss for batch 2671 : 0.19464734196662903\n",
      "Training loss for batch 2672 : 0.2755819261074066\n",
      "Training loss for batch 2673 : 0.40566128492355347\n",
      "Training loss for batch 2674 : 0.26581889390945435\n",
      "Training loss for batch 2675 : 0.27923884987831116\n",
      "Training loss for batch 2676 : 0.13176831603050232\n",
      "Training loss for batch 2677 : 0.0932995080947876\n",
      "Training loss for batch 2678 : 0.11398112028837204\n",
      "Training loss for batch 2679 : 0.12765854597091675\n",
      "Training loss for batch 2680 : 0.55817711353302\n",
      "Training loss for batch 2681 : 0.14922837913036346\n",
      "Training loss for batch 2682 : 0.1969517320394516\n",
      "Training loss for batch 2683 : 0.1594204157590866\n",
      "Training loss for batch 2684 : 0.2639269232749939\n",
      "Training loss for batch 2685 : 0.009731973521411419\n",
      "Training loss for batch 2686 : 0.06909028440713882\n",
      "Training loss for batch 2687 : 0.04390636086463928\n",
      "Training loss for batch 2688 : 0.04613872990012169\n",
      "Training loss for batch 2689 : 0.18676039576530457\n",
      "Training loss for batch 2690 : 0.5561183094978333\n",
      "Training loss for batch 2691 : 0.15072381496429443\n",
      "Training loss for batch 2692 : 0.44473353028297424\n",
      "Training loss for batch 2693 : 0.20234930515289307\n",
      "Training loss for batch 2694 : 0.10927236080169678\n",
      "Training loss for batch 2695 : 0.04122872278094292\n",
      "Training loss for batch 2696 : 0.4357356131076813\n",
      "Training loss for batch 2697 : 0.2937087416648865\n",
      "Training loss for batch 2698 : 0.12607751786708832\n",
      "Training loss for batch 2699 : 0.3927237093448639\n",
      "Training loss for batch 2700 : 0.268810898065567\n",
      "Training loss for batch 2701 : 0.10036185383796692\n",
      "Training loss for batch 2702 : 0.2388271987438202\n",
      "Training loss for batch 2703 : 0.08057144284248352\n",
      "Training loss for batch 2704 : 0.3641164004802704\n",
      "Training loss for batch 2705 : 0.09965948015451431\n",
      "Training loss for batch 2706 : 0.14397282898426056\n",
      "Training loss for batch 2707 : 0.06717969477176666\n",
      "Training loss for batch 2708 : 0.2870110869407654\n",
      "Training loss for batch 2709 : 0.08130714297294617\n",
      "Training loss for batch 2710 : 0.03925762698054314\n",
      "Training loss for batch 2711 : 0.04942121356725693\n",
      "Training loss for batch 2712 : 0.47493329644203186\n",
      "Training loss for batch 2713 : 0.13015536963939667\n",
      "Training loss for batch 2714 : 0.38749194145202637\n",
      "Training loss for batch 2715 : 0.04455000162124634\n",
      "Training loss for batch 2716 : 0.4381785988807678\n",
      "Training loss for batch 2717 : 0.07486691325902939\n",
      "Training loss for batch 2718 : 0.354832261800766\n",
      "Training loss for batch 2719 : 0.0632191076874733\n",
      "Training loss for batch 2720 : 0.28086259961128235\n",
      "Training loss for batch 2721 : 0.026924047619104385\n",
      "Training loss for batch 2722 : 0.16174447536468506\n",
      "Training loss for batch 2723 : 0.17136549949645996\n",
      "Training loss for batch 2724 : 0.2632301449775696\n",
      "Training loss for batch 2725 : 0.07557258009910583\n",
      "Training loss for batch 2726 : 0.36682260036468506\n",
      "Training loss for batch 2727 : 0.0\n",
      "Training loss for batch 2728 : 0.2381443977355957\n",
      "Training loss for batch 2729 : 0.4119030833244324\n",
      "Training loss for batch 2730 : 0.33977535367012024\n",
      "Training loss for batch 2731 : 0.03369314968585968\n",
      "Training loss for batch 2732 : 0.2389955222606659\n",
      "Training loss for batch 2733 : 0.05821645259857178\n",
      "Training loss for batch 2734 : 0.27723971009254456\n",
      "Training loss for batch 2735 : 0.05298161879181862\n",
      "Training loss for batch 2736 : 0.25718048214912415\n",
      "Training loss for batch 2737 : 0.2698456645011902\n",
      "Training loss for batch 2738 : 0.19277341663837433\n",
      "Training loss for batch 2739 : 0.19815771281719208\n",
      "Training loss for batch 2740 : 0.05632321164011955\n",
      "Training loss for batch 2741 : 0.3966631293296814\n",
      "Training loss for batch 2742 : 0.3756272494792938\n",
      "Training loss for batch 2743 : 0.29356151819229126\n",
      "Training loss for batch 2744 : 0.16281954944133759\n",
      "Training loss for batch 2745 : 0.10432344675064087\n",
      "Training loss for batch 2746 : 0.1397095024585724\n",
      "Training loss for batch 2747 : 0.009287983179092407\n",
      "Training loss for batch 2748 : 0.13178324699401855\n",
      "Training loss for batch 2749 : 0.7517597079277039\n",
      "Training loss for batch 2750 : 0.12933193147182465\n",
      "Training loss for batch 2751 : 0.09767521917819977\n",
      "Training loss for batch 2752 : 0.24972540140151978\n",
      "Training loss for batch 2753 : 0.2598395049571991\n",
      "Training loss for batch 2754 : 0.24223002791404724\n",
      "Training loss for batch 2755 : 0.4126317501068115\n",
      "Training loss for batch 2756 : 0.2151600569486618\n",
      "Training loss for batch 2757 : 0.13692958652973175\n",
      "Training loss for batch 2758 : 0.14902184903621674\n",
      "Training loss for batch 2759 : 0.12788091599941254\n",
      "Training loss for batch 2760 : 0.06808506697416306\n",
      "Training loss for batch 2761 : 0.10827655345201492\n",
      "Training loss for batch 2762 : 0.10738258808851242\n",
      "Training loss for batch 2763 : 0.20136658847332\n",
      "Training loss for batch 2764 : 0.39385852217674255\n",
      "Training loss for batch 2765 : 0.25794267654418945\n",
      "Training loss for batch 2766 : 0.5202709436416626\n",
      "Training loss for batch 2767 : 0.32022207975387573\n",
      "Training loss for batch 2768 : 0.567947268486023\n",
      "Training loss for batch 2769 : 0.034511227160692215\n",
      "Training loss for batch 2770 : 0.07024817168712616\n",
      "Training loss for batch 2771 : 0.11476989835500717\n",
      "Training loss for batch 2772 : 0.2088405042886734\n",
      "Training loss for batch 2773 : 0.16980580985546112\n",
      "Training loss for batch 2774 : 0.16263660788536072\n",
      "Training loss for batch 2775 : 0.11558416485786438\n",
      "Training loss for batch 2776 : 0.33605051040649414\n",
      "Training loss for batch 2777 : 0.2324489802122116\n",
      "Training loss for batch 2778 : 0.06890164315700531\n",
      "Training loss for batch 2779 : 0.1394324153661728\n",
      "Training loss for batch 2780 : 0.046283215284347534\n",
      "Training loss for batch 2781 : 0.2518698275089264\n",
      "Training loss for batch 2782 : 0.10091263055801392\n",
      "Training loss for batch 2783 : 0.3024075925350189\n",
      "Training loss for batch 2784 : 0.08247826993465424\n",
      "Training loss for batch 2785 : 0.00797552801668644\n",
      "Training loss for batch 2786 : 0.1105845719575882\n",
      "Training loss for batch 2787 : 0.07242967933416367\n",
      "Training loss for batch 2788 : 0.24791909754276276\n",
      "Training loss for batch 2789 : 0.3725186586380005\n",
      "Training loss for batch 2790 : 0.4480287432670593\n",
      "Training loss for batch 2791 : 0.1914563924074173\n",
      "Training loss for batch 2792 : 0.811163067817688\n",
      "Training loss for batch 2793 : 0.08865310251712799\n",
      "Training loss for batch 2794 : 0.16486863791942596\n",
      "Training loss for batch 2795 : 0.06463904678821564\n",
      "Training loss for batch 2796 : 0.05155546963214874\n",
      "Training loss for batch 2797 : 0.12344484031200409\n",
      "Training loss for batch 2798 : 0.06536754220724106\n",
      "Training loss for batch 2799 : 0.42630118131637573\n",
      "Training loss for batch 2800 : 0.1163146048784256\n",
      "Training loss for batch 2801 : 0.02115560509264469\n",
      "Training loss for batch 2802 : 0.24365045130252838\n",
      "Training loss for batch 2803 : 0.27677837014198303\n",
      "Training loss for batch 2804 : 0.11071522533893585\n",
      "Training loss for batch 2805 : 0.11917442828416824\n",
      "Training loss for batch 2806 : 0.13291040062904358\n",
      "Training loss for batch 2807 : 0.27297118306159973\n",
      "Training loss for batch 2808 : 0.1028711274266243\n",
      "Training loss for batch 2809 : 0.27133238315582275\n",
      "Training loss for batch 2810 : 0.2093624621629715\n",
      "Training loss for batch 2811 : 0.24380037188529968\n",
      "Training loss for batch 2812 : 0.2894207537174225\n",
      "Training loss for batch 2813 : 0.2786305844783783\n",
      "Training loss for batch 2814 : 0.13219018280506134\n",
      "Training loss for batch 2815 : 0.2510101795196533\n",
      "Training loss for batch 2816 : 0.21317154169082642\n",
      "Training loss for batch 2817 : 0.230600506067276\n",
      "Training loss for batch 2818 : 0.45855632424354553\n",
      "Training loss for batch 2819 : 0.22998547554016113\n",
      "Training loss for batch 2820 : 0.2661146819591522\n",
      "Training loss for batch 2821 : 0.0762447863817215\n",
      "Training loss for batch 2822 : 0.12063481658697128\n",
      "Training loss for batch 2823 : 0.05971415340900421\n",
      "Training loss for batch 2824 : 0.16120310127735138\n",
      "Training loss for batch 2825 : 0.04674563184380531\n",
      "Training loss for batch 2826 : 0.20624957978725433\n",
      "Training loss for batch 2827 : 0.3646222651004791\n",
      "Training loss for batch 2828 : 0.13019175827503204\n",
      "Training loss for batch 2829 : 0.2792833149433136\n",
      "Training loss for batch 2830 : 0.22836992144584656\n",
      "Training loss for batch 2831 : 0.011517728678882122\n",
      "Training loss for batch 2832 : 0.22697225213050842\n",
      "Training loss for batch 2833 : 0.47074803709983826\n",
      "Training loss for batch 2834 : 0.12937484681606293\n",
      "Training loss for batch 2835 : 0.27314984798431396\n",
      "Training loss for batch 2836 : 0.0718458965420723\n",
      "Training loss for batch 2837 : 0.1552087515592575\n",
      "Training loss for batch 2838 : 0.09420689195394516\n",
      "Training loss for batch 2839 : 0.18208405375480652\n",
      "Training loss for batch 2840 : 0.10429812967777252\n",
      "Training loss for batch 2841 : 0.1844821721315384\n",
      "Training loss for batch 2842 : 0.12766119837760925\n",
      "Training loss for batch 2843 : 0.07128655910491943\n",
      "Training loss for batch 2844 : 0.14029115438461304\n",
      "Training loss for batch 2845 : 0.07143951952457428\n",
      "Training loss for batch 2846 : 0.12672649323940277\n",
      "Training loss for batch 2847 : 0.18880701065063477\n",
      "Training loss for batch 2848 : 0.24759216606616974\n",
      "Training loss for batch 2849 : 0.03366934880614281\n",
      "Training loss for batch 2850 : 0.41266778111457825\n",
      "Training loss for batch 2851 : 0.31464675068855286\n",
      "Training loss for batch 2852 : 0.05823642760515213\n",
      "Training loss for batch 2853 : 0.10207463800907135\n",
      "Training loss for batch 2854 : -0.0002828757278621197\n",
      "Training loss for batch 2855 : 0.15365761518478394\n",
      "Training loss for batch 2856 : 0.23547016084194183\n",
      "Training loss for batch 2857 : 0.0847364068031311\n",
      "Training loss for batch 2858 : 0.41102755069732666\n",
      "Training loss for batch 2859 : 0.13131938874721527\n",
      "Training loss for batch 2860 : 0.09318726509809494\n",
      "Training loss for batch 2861 : 0.333042711019516\n",
      "Training loss for batch 2862 : 0.20493105053901672\n",
      "Training loss for batch 2863 : 0.18648090958595276\n",
      "Training loss for batch 2864 : 0.47945642471313477\n",
      "Training loss for batch 2865 : 0.16838854551315308\n",
      "Training loss for batch 2866 : 0.11439257115125656\n",
      "Training loss for batch 2867 : 0.12054610252380371\n",
      "Training loss for batch 2868 : 0.4641439616680145\n",
      "Training loss for batch 2869 : 0.4699627757072449\n",
      "Training loss for batch 2870 : 0.2729479968547821\n",
      "Training loss for batch 2871 : 0.31124305725097656\n",
      "Training loss for batch 2872 : 0.12518242001533508\n",
      "Training loss for batch 2873 : 0.23588623106479645\n",
      "Training loss for batch 2874 : 0.04546867683529854\n",
      "Training loss for batch 2875 : 0.10370911657810211\n",
      "Training loss for batch 2876 : 0.2735166847705841\n",
      "Training loss for batch 2877 : 0.324888676404953\n",
      "Training loss for batch 2878 : 0.2887248992919922\n",
      "Training loss for batch 2879 : 0.05275329202413559\n",
      "Training loss for batch 2880 : 0.11111506074666977\n",
      "Training loss for batch 2881 : 0.2597562074661255\n",
      "Training loss for batch 2882 : 0.1170448437333107\n",
      "Training loss for batch 2883 : 0.08220918476581573\n",
      "Training loss for batch 2884 : 0.2055073380470276\n",
      "Training loss for batch 2885 : 0.3543146848678589\n",
      "Training loss for batch 2886 : 0.16330042481422424\n",
      "Training loss for batch 2887 : 0.049004025757312775\n",
      "Training loss for batch 2888 : 0.17528097331523895\n",
      "Training loss for batch 2889 : 0.12183354049921036\n",
      "Training loss for batch 2890 : 0.17579419910907745\n",
      "Training loss for batch 2891 : 0.062219250947237015\n",
      "Training loss for batch 2892 : 0.026593198999762535\n",
      "Training loss for batch 2893 : 0.19243478775024414\n",
      "Training loss for batch 2894 : 0.09209416061639786\n",
      "Training loss for batch 2895 : 0.3232765197753906\n",
      "Training loss for batch 2896 : 0.5114898681640625\n",
      "Training loss for batch 2897 : 0.27466168999671936\n",
      "Training loss for batch 2898 : -0.00011903968697879463\n",
      "Training loss for batch 2899 : 0.22987693548202515\n",
      "Training loss for batch 2900 : 0.11364567279815674\n",
      "Training loss for batch 2901 : 0.1998750865459442\n",
      "Training loss for batch 2902 : 0.048272110521793365\n",
      "Training loss for batch 2903 : 0.21129347383975983\n",
      "Training loss for batch 2904 : 0.06712528318166733\n",
      "Training loss for batch 2905 : 0.06621675193309784\n",
      "Training loss for batch 2906 : 0.017915189266204834\n",
      "Training loss for batch 2907 : 0.16120943427085876\n",
      "Training loss for batch 2908 : -4.8162881284952164e-05\n",
      "Training loss for batch 2909 : 0.08872480690479279\n",
      "Training loss for batch 2910 : 0.03576861694455147\n",
      "Training loss for batch 2911 : 0.15227816998958588\n",
      "Training loss for batch 2912 : 0.24452605843544006\n",
      "Training loss for batch 2913 : 0.22658081352710724\n",
      "Training loss for batch 2914 : 0.062167711555957794\n",
      "Training loss for batch 2915 : 0.17371883988380432\n",
      "Training loss for batch 2916 : 0.2621398866176605\n",
      "Training loss for batch 2917 : 0.13743902742862701\n",
      "Training loss for batch 2918 : 0.07947836816310883\n",
      "Training loss for batch 2919 : 0.329485148191452\n",
      "Training loss for batch 2920 : 0.10612688958644867\n",
      "Training loss for batch 2921 : 0.1657143384218216\n",
      "Training loss for batch 2922 : 0.05833863466978073\n",
      "Training loss for batch 2923 : 0.08221536129713058\n",
      "Training loss for batch 2924 : 0.030278479680418968\n",
      "Training loss for batch 2925 : 0.2559036314487457\n",
      "Training loss for batch 2926 : 0.18281809985637665\n",
      "Training loss for batch 2927 : 0.4943540692329407\n",
      "Training loss for batch 2928 : 0.029046129435300827\n",
      "Training loss for batch 2929 : 0.05215778574347496\n",
      "Training loss for batch 2930 : 0.2755439579486847\n",
      "Training loss for batch 2931 : 0.007974615320563316\n",
      "Training loss for batch 2932 : 0.08857586979866028\n",
      "Training loss for batch 2933 : 0.3923879563808441\n",
      "Training loss for batch 2934 : 0.13643726706504822\n",
      "Training loss for batch 2935 : 0.0950358435511589\n",
      "Training loss for batch 2936 : 0.2341116964817047\n",
      "Training loss for batch 2937 : 0.13676732778549194\n",
      "Training loss for batch 2938 : 0.21408119797706604\n",
      "Training loss for batch 2939 : 0.5099436640739441\n",
      "Training loss for batch 2940 : 0.39664918184280396\n",
      "Training loss for batch 2941 : 0.3078906238079071\n",
      "Training loss for batch 2942 : 0.5019786953926086\n",
      "Training loss for batch 2943 : 0.10377337038516998\n",
      "Training loss for batch 2944 : 0.2660817503929138\n",
      "Training loss for batch 2945 : 0.100426584482193\n",
      "Training loss for batch 2946 : 0.08069426566362381\n",
      "Training loss for batch 2947 : 0.26272448897361755\n",
      "Training loss for batch 2948 : 0.30910348892211914\n",
      "Training loss for batch 2949 : 0.0890708863735199\n",
      "Training loss for batch 2950 : 0.2715304493904114\n",
      "Training loss for batch 2951 : 0.12119880318641663\n",
      "Training loss for batch 2952 : 0.20075000822544098\n",
      "Training loss for batch 2953 : 0.20620889961719513\n",
      "Training loss for batch 2954 : 0.3894180953502655\n",
      "Training loss for batch 2955 : 0.2525196969509125\n",
      "Training loss for batch 2956 : 0.25449487566947937\n",
      "Training loss for batch 2957 : 0.14414367079734802\n",
      "Training loss for batch 2958 : 0.4226974546909332\n",
      "Training loss for batch 2959 : 0.3523281216621399\n",
      "Training loss for batch 2960 : 0.06804870814085007\n",
      "Training loss for batch 2961 : 0.36850661039352417\n",
      "Training loss for batch 2962 : 0.06397049129009247\n",
      "Training loss for batch 2963 : 0.07191497087478638\n",
      "Training loss for batch 2964 : 0.23767152428627014\n",
      "Training loss for batch 2965 : 0.19308030605316162\n",
      "Training loss for batch 2966 : 0.1314687728881836\n",
      "Training loss for batch 2967 : 0.07295068353414536\n",
      "Training loss for batch 2968 : 0.2166120857000351\n",
      "Training loss for batch 2969 : 0.4384247362613678\n",
      "Training loss for batch 2970 : 0.04504619538784027\n",
      "Training loss for batch 2971 : 0.2775767147541046\n",
      "Training loss for batch 2972 : 0.046655651181936264\n",
      "Training loss for batch 2973 : 0.10596244037151337\n",
      "Training loss for batch 2974 : 0.5124892592430115\n",
      "Training loss for batch 2975 : -0.0001493047020630911\n",
      "Training loss for batch 2976 : 0.43837806582450867\n",
      "Training loss for batch 2977 : 0.31301555037498474\n",
      "Training loss for batch 2978 : 0.08055219799280167\n",
      "Training loss for batch 2979 : 0.26017147302627563\n",
      "Training loss for batch 2980 : 0.5583482384681702\n",
      "Training loss for batch 2981 : 0.18789054453372955\n",
      "Training loss for batch 2982 : 0.27330052852630615\n",
      "Training loss for batch 2983 : 0.0449550598859787\n",
      "Training loss for batch 2984 : 0.1392120122909546\n",
      "Training loss for batch 2985 : 0.1954105943441391\n",
      "Training loss for batch 2986 : 0.09881986677646637\n",
      "Training loss for batch 2987 : 0.17527295649051666\n",
      "Training loss for batch 2988 : 0.13803476095199585\n",
      "Training loss for batch 2989 : 0.4497449994087219\n",
      "Training loss for batch 2990 : 0.5659801959991455\n",
      "Training loss for batch 2991 : 0.24854914844036102\n",
      "Training loss for batch 2992 : 0.0759655311703682\n",
      "Training loss for batch 2993 : 0.12816840410232544\n",
      "Training loss for batch 2994 : 0.04063446447253227\n",
      "Training loss for batch 2995 : 0.0\n",
      "Training loss for batch 2996 : 0.4355180263519287\n",
      "Training loss for batch 2997 : 0.0028013389091938734\n",
      "Training loss for batch 2998 : 0.5167669653892517\n",
      "Training loss for batch 2999 : 0.21264740824699402\n",
      "Training loss for batch 3000 : 0.0016190656460821629\n",
      "Training loss for batch 3001 : 0.028308359906077385\n",
      "Training loss for batch 3002 : 0.01756194978952408\n",
      "Training loss for batch 3003 : 0.18641191720962524\n",
      "Training loss for batch 3004 : 0.211892768740654\n",
      "Training loss for batch 3005 : 0.7040603160858154\n",
      "Training loss for batch 3006 : 0.10316311568021774\n",
      "Training loss for batch 3007 : 0.01426211278885603\n",
      "Training loss for batch 3008 : 0.053058791905641556\n",
      "Training loss for batch 3009 : 0.31470295786857605\n",
      "Training loss for batch 3010 : 0.2716679573059082\n",
      "Training loss for batch 3011 : 0.06388309597969055\n",
      "Training loss for batch 3012 : 0.10149283707141876\n",
      "Training loss for batch 3013 : 0.049060381948947906\n",
      "Training loss for batch 3014 : 0.06433220952749252\n",
      "Training loss for batch 3015 : 0.0737479031085968\n",
      "Training loss for batch 3016 : 0.1508716195821762\n",
      "Training loss for batch 3017 : 0.0\n",
      "Training loss for batch 3018 : 0.1221909373998642\n",
      "Training loss for batch 3019 : 0.20275083184242249\n",
      "Training loss for batch 3020 : 0.3929385840892792\n",
      "Training loss for batch 3021 : 0.19466790556907654\n",
      "Training loss for batch 3022 : 0.36687079071998596\n",
      "Training loss for batch 3023 : 0.1481315940618515\n",
      "Training loss for batch 3024 : 0.2821086049079895\n",
      "Training loss for batch 3025 : 0.13484513759613037\n",
      "Training loss for batch 3026 : 0.008455169387161732\n",
      "Training loss for batch 3027 : 0.1959075927734375\n",
      "Training loss for batch 3028 : 0.010202513076364994\n",
      "Training loss for batch 3029 : 0.1044355109333992\n",
      "Training loss for batch 3030 : 0.07762017846107483\n",
      "Training loss for batch 3031 : 0.33053505420684814\n",
      "Training loss for batch 3032 : 0.25109291076660156\n",
      "Training loss for batch 3033 : 0.3061387538909912\n",
      "Training loss for batch 3034 : 0.18527071177959442\n",
      "Training loss for batch 3035 : 0.49180057644844055\n",
      "Training loss for batch 3036 : 0.12148568034172058\n",
      "Training loss for batch 3037 : 0.394293874502182\n",
      "Training loss for batch 3038 : 0.06635507941246033\n",
      "Training loss for batch 3039 : 0.38269898295402527\n",
      "Training loss for batch 3040 : 0.11257865279912949\n",
      "Training loss for batch 3041 : 0.26454776525497437\n",
      "Training loss for batch 3042 : 0.027926553040742874\n",
      "Training loss for batch 3043 : 0.2441709190607071\n",
      "Training loss for batch 3044 : 0.2757948040962219\n",
      "Training loss for batch 3045 : 0.16177736222743988\n",
      "Training loss for batch 3046 : 0.16162581741809845\n",
      "Training loss for batch 3047 : 0.24505652487277985\n",
      "Training loss for batch 3048 : 0.13600094616413116\n",
      "Training loss for batch 3049 : 0.0\n",
      "Training loss for batch 3050 : 0.11840825527906418\n",
      "Training loss for batch 3051 : 0.21824640035629272\n",
      "Training loss for batch 3052 : 0.23602111637592316\n",
      "Training loss for batch 3053 : 0.02368241734802723\n",
      "Training loss for batch 3054 : 0.0031308531761169434\n",
      "Training loss for batch 3055 : 0.11294122040271759\n",
      "Training loss for batch 3056 : 0.48183560371398926\n",
      "Training loss for batch 3057 : 0.3789990544319153\n",
      "Training loss for batch 3058 : 0.015238563530147076\n",
      "Training loss for batch 3059 : 0.10470100492238998\n",
      "Training loss for batch 3060 : 0.03782966732978821\n",
      "Training loss for batch 3061 : 0.05016244947910309\n",
      "Training loss for batch 3062 : 0.5613760948181152\n",
      "Training loss for batch 3063 : 0.10324692726135254\n",
      "Training loss for batch 3064 : 0.15846951305866241\n",
      "Training loss for batch 3065 : 0.09299052506685257\n",
      "Training loss for batch 3066 : 0.09734445810317993\n",
      "Training loss for batch 3067 : 0.04822035878896713\n",
      "Training loss for batch 3068 : 0.07481574267148972\n",
      "Training loss for batch 3069 : 0.12796778976917267\n",
      "Training loss for batch 3070 : 0.1947045773267746\n",
      "Training loss for batch 3071 : 0.07233016192913055\n",
      "Training loss for batch 3072 : 0.0233499426394701\n",
      "Training loss for batch 3073 : 0.3477918207645416\n",
      "Training loss for batch 3074 : 0.26389357447624207\n",
      "Training loss for batch 3075 : 0.3231689929962158\n",
      "Training loss for batch 3076 : 0.023170582950115204\n",
      "Training loss for batch 3077 : 0.4386850595474243\n",
      "Training loss for batch 3078 : 0.3148524761199951\n",
      "Training loss for batch 3079 : 0.013246635906398296\n",
      "Training loss for batch 3080 : 0.3639873266220093\n",
      "Training loss for batch 3081 : 0.19923348724842072\n",
      "Training loss for batch 3082 : 0.20781901478767395\n",
      "Training loss for batch 3083 : 0.04086862877011299\n",
      "Training loss for batch 3084 : 0.37463808059692383\n",
      "Training loss for batch 3085 : 0.040755823254585266\n",
      "Training loss for batch 3086 : 0.15355290472507477\n",
      "Training loss for batch 3087 : 0.003812151262536645\n",
      "Training loss for batch 3088 : 0.09660995751619339\n",
      "Training loss for batch 3089 : 0.18877586722373962\n",
      "Training loss for batch 3090 : 0.32760298252105713\n",
      "Training loss for batch 3091 : 0.41852042078971863\n",
      "Training loss for batch 3092 : 0.5181097984313965\n",
      "Training loss for batch 3093 : 0.04705387353897095\n",
      "Training loss for batch 3094 : 0.07486910372972488\n",
      "Training loss for batch 3095 : 0.9807729125022888\n",
      "Training loss for batch 3096 : 0.044462334364652634\n",
      "Training loss for batch 3097 : 0.23770734667778015\n",
      "Training loss for batch 3098 : 0.035097599029541016\n",
      "Training loss for batch 3099 : 0.10299757868051529\n",
      "Training loss for batch 3100 : 0.12376010417938232\n",
      "Training loss for batch 3101 : 0.4642319977283478\n",
      "Training loss for batch 3102 : 0.10188080370426178\n",
      "Training loss for batch 3103 : 0.0009870430221781135\n",
      "Training loss for batch 3104 : 0.07717539370059967\n",
      "Training loss for batch 3105 : 0.05885801464319229\n",
      "Training loss for batch 3106 : 0.06775587052106857\n",
      "Training loss for batch 3107 : 0.16664764285087585\n",
      "Training loss for batch 3108 : 0.043931543827056885\n",
      "Training loss for batch 3109 : 0.04975994676351547\n",
      "Training loss for batch 3110 : 0.008336811326444149\n",
      "Training loss for batch 3111 : 0.44474342465400696\n",
      "Training loss for batch 3112 : 0.20286455750465393\n",
      "Training loss for batch 3113 : 0.24845018982887268\n",
      "Training loss for batch 3114 : 0.23573800921440125\n",
      "Training loss for batch 3115 : 0.31214651465415955\n",
      "Training loss for batch 3116 : 0.12941974401474\n",
      "Training loss for batch 3117 : 0.06682530045509338\n",
      "Training loss for batch 3118 : 0.07570288330316544\n",
      "Training loss for batch 3119 : 0.1503027081489563\n",
      "Training loss for batch 3120 : 0.13549397885799408\n",
      "Training loss for batch 3121 : 0.03907429426908493\n",
      "Training loss for batch 3122 : 0.12172701209783554\n",
      "Training loss for batch 3123 : 0.021866530179977417\n",
      "Training loss for batch 3124 : 0.17674945294857025\n",
      "Training loss for batch 3125 : 0.19414062798023224\n",
      "Training loss for batch 3126 : 0.1541805863380432\n",
      "Training loss for batch 3127 : 0.08565889298915863\n",
      "Training loss for batch 3128 : 0.2360127568244934\n",
      "Training loss for batch 3129 : 0.2497408390045166\n",
      "Training loss for batch 3130 : 0.04836268723011017\n",
      "Training loss for batch 3131 : 0.21523357927799225\n",
      "Training loss for batch 3132 : 0.16264137625694275\n",
      "Training loss for batch 3133 : 0.13581418991088867\n",
      "Training loss for batch 3134 : 0.4175870418548584\n",
      "Training loss for batch 3135 : 0.1883872002363205\n",
      "Training loss for batch 3136 : 0.185574471950531\n",
      "Training loss for batch 3137 : 0.2610858976840973\n",
      "Training loss for batch 3138 : 0.32737165689468384\n",
      "Training loss for batch 3139 : 0.19745998084545135\n",
      "Training loss for batch 3140 : 0.1944117397069931\n",
      "Training loss for batch 3141 : 0.12946997582912445\n",
      "Training loss for batch 3142 : 0.23395118117332458\n",
      "Training loss for batch 3143 : 0.1414272040128708\n",
      "Training loss for batch 3144 : 0.19373489916324615\n",
      "Training loss for batch 3145 : 0.07185389846563339\n",
      "Training loss for batch 3146 : 0.08845599740743637\n",
      "Training loss for batch 3147 : 0.4009828269481659\n",
      "Training loss for batch 3148 : 0.10137531161308289\n",
      "Training loss for batch 3149 : 0.04842746630311012\n",
      "Training loss for batch 3150 : 0.0\n",
      "Training loss for batch 3151 : 0.19963225722312927\n",
      "Training loss for batch 3152 : 0.17247822880744934\n",
      "Training loss for batch 3153 : 0.2656095027923584\n",
      "Training loss for batch 3154 : 0.16701745986938477\n",
      "Training loss for batch 3155 : 0.1683788299560547\n",
      "Training loss for batch 3156 : 0.2562597095966339\n",
      "Training loss for batch 3157 : 0.31319254636764526\n",
      "Training loss for batch 3158 : 0.013061881065368652\n",
      "Training loss for batch 3159 : 0.041178956627845764\n",
      "Training loss for batch 3160 : 0.20963282883167267\n",
      "Training loss for batch 3161 : 0.06833762675523758\n",
      "Training loss for batch 3162 : 0.04295143485069275\n",
      "Training loss for batch 3163 : 0.14789259433746338\n",
      "Training loss for batch 3164 : 0.4967653155326843\n",
      "Training loss for batch 3165 : 0.16215968132019043\n",
      "Training loss for batch 3166 : 0.2472846955060959\n",
      "Training loss for batch 3167 : 0.21858660876750946\n",
      "Training loss for batch 3168 : 0.39719948172569275\n",
      "Training loss for batch 3169 : 0.20516282320022583\n",
      "Training loss for batch 3170 : 0.09247305244207382\n",
      "Training loss for batch 3171 : 0.008654644712805748\n",
      "Training loss for batch 3172 : 0.002694070339202881\n",
      "Training loss for batch 3173 : 0.021886102855205536\n",
      "Training loss for batch 3174 : 0.2622825801372528\n",
      "Training loss for batch 3175 : 0.2395588457584381\n",
      "Training loss for batch 3176 : 0.18578380346298218\n",
      "Training loss for batch 3177 : 0.37240517139434814\n",
      "Training loss for batch 3178 : 0.4142017960548401\n",
      "Training loss for batch 3179 : 0.01558404229581356\n",
      "Training loss for batch 3180 : 0.05743667110800743\n",
      "Training loss for batch 3181 : 0.22779056429862976\n",
      "Training loss for batch 3182 : 0.23192046582698822\n",
      "Training loss for batch 3183 : 0.5229800343513489\n",
      "Training loss for batch 3184 : 0.09778103977441788\n",
      "Training loss for batch 3185 : 0.1005408838391304\n",
      "Training loss for batch 3186 : 0.19760461151599884\n",
      "Training loss for batch 3187 : 0.05696767941117287\n",
      "Training loss for batch 3188 : 0.1491541862487793\n",
      "Training loss for batch 3189 : 0.1496143788099289\n",
      "Training loss for batch 3190 : 0.2254447191953659\n",
      "Training loss for batch 3191 : 0.16270427405834198\n",
      "Training loss for batch 3192 : 0.026734625920653343\n",
      "Training loss for batch 3193 : 0.18504013121128082\n",
      "Training loss for batch 3194 : 0.019557606428861618\n",
      "Training loss for batch 3195 : 0.5230321288108826\n",
      "Training loss for batch 3196 : 0.2355670928955078\n",
      "Training loss for batch 3197 : 0.29746413230895996\n",
      "Training loss for batch 3198 : 0.08456716686487198\n",
      "Training loss for batch 3199 : 0.28222915530204773\n",
      "Training loss for batch 3200 : 0.22325009107589722\n",
      "Training loss for batch 3201 : 0.2030692845582962\n",
      "Training loss for batch 3202 : 0.012391060590744019\n",
      "Training loss for batch 3203 : 0.2366076558828354\n",
      "Training loss for batch 3204 : 0.1336265206336975\n",
      "Training loss for batch 3205 : 0.15335534512996674\n",
      "Training loss for batch 3206 : 0.02960074692964554\n",
      "Training loss for batch 3207 : 0.29023635387420654\n",
      "Training loss for batch 3208 : 0.07193519175052643\n",
      "Training loss for batch 3209 : 0.11520631611347198\n",
      "Training loss for batch 3210 : 0.03376523777842522\n",
      "Training loss for batch 3211 : 0.33516985177993774\n",
      "Training loss for batch 3212 : 0.13670162856578827\n",
      "Training loss for batch 3213 : 0.03892120346426964\n",
      "Training loss for batch 3214 : 0.09086489677429199\n",
      "Training loss for batch 3215 : 0.0004252294893376529\n",
      "Training loss for batch 3216 : 0.21892371773719788\n",
      "Training loss for batch 3217 : 0.04414232447743416\n",
      "Training loss for batch 3218 : 0.3949909806251526\n",
      "Training loss for batch 3219 : 0.048267919570207596\n",
      "Training loss for batch 3220 : 0.40537387132644653\n",
      "Training loss for batch 3221 : 0.25037646293640137\n",
      "Training loss for batch 3222 : 0.010229440405964851\n",
      "Training loss for batch 3223 : 0.0626949593424797\n",
      "Training loss for batch 3224 : 0.06577285379171371\n",
      "Training loss for batch 3225 : 0.19694340229034424\n",
      "Training loss for batch 3226 : 0.2076907902956009\n",
      "Training loss for batch 3227 : 0.17904378473758698\n",
      "Training loss for batch 3228 : 0.21917390823364258\n",
      "Training loss for batch 3229 : 0.04993373900651932\n",
      "Training loss for batch 3230 : 0.4699830114841461\n",
      "Training loss for batch 3231 : 0.10594544559717178\n",
      "Training loss for batch 3232 : 0.10209356993436813\n",
      "Training loss for batch 3233 : 0.1857675164937973\n",
      "Training loss for batch 3234 : 0.0948716327548027\n",
      "Training loss for batch 3235 : 0.31423208117485046\n",
      "Training loss for batch 3236 : 0.20121553540229797\n",
      "Training loss for batch 3237 : 0.19531875848770142\n",
      "Training loss for batch 3238 : 0.162823885679245\n",
      "Training loss for batch 3239 : 0.34955736994743347\n",
      "Training loss for batch 3240 : 0.2564113438129425\n",
      "Training loss for batch 3241 : 0.16266325116157532\n",
      "Training loss for batch 3242 : 0.23668867349624634\n",
      "Training loss for batch 3243 : 0.03983325883746147\n",
      "Training loss for batch 3244 : 0.20525385439395905\n",
      "Training loss for batch 3245 : 0.08054409921169281\n",
      "Training loss for batch 3246 : 0.07064534723758698\n",
      "Training loss for batch 3247 : 0.0674191489815712\n",
      "Training loss for batch 3248 : 0.08595900237560272\n",
      "Training loss for batch 3249 : 0.30176639556884766\n",
      "Training loss for batch 3250 : 0.05341731011867523\n",
      "Training loss for batch 3251 : 0.11950349807739258\n",
      "Training loss for batch 3252 : 0.481596976518631\n",
      "Training loss for batch 3253 : 0.048494018614292145\n",
      "Training loss for batch 3254 : 0.3454224765300751\n",
      "Training loss for batch 3255 : 0.22339282929897308\n",
      "Training loss for batch 3256 : 0.44510114192962646\n",
      "Training loss for batch 3257 : 0.24758893251419067\n",
      "Training loss for batch 3258 : 0.20590339601039886\n",
      "Training loss for batch 3259 : 0.05843775346875191\n",
      "Training loss for batch 3260 : 0.03108314424753189\n",
      "Training loss for batch 3261 : 0.07117878645658493\n",
      "Training loss for batch 3262 : 0.0676610916852951\n",
      "Training loss for batch 3263 : 0.09521650522947311\n",
      "Training loss for batch 3264 : 0.060954008251428604\n",
      "Training loss for batch 3265 : 0.25367313623428345\n",
      "Training loss for batch 3266 : 0.026080342009663582\n",
      "Training loss for batch 3267 : 0.09415480494499207\n",
      "Training loss for batch 3268 : 0.0035513630136847496\n",
      "Training loss for batch 3269 : 0.17186670005321503\n",
      "Training loss for batch 3270 : 0.4561541676521301\n",
      "Training loss for batch 3271 : 0.11147793382406235\n",
      "Training loss for batch 3272 : 0.35691267251968384\n",
      "Training loss for batch 3273 : 0.10313740372657776\n",
      "Training loss for batch 3274 : 0.416797012090683\n",
      "Training loss for batch 3275 : 0.5528742074966431\n",
      "Training loss for batch 3276 : 0.028912678360939026\n",
      "Training loss for batch 3277 : 0.29236528277397156\n",
      "Training loss for batch 3278 : 0.09075851738452911\n",
      "Training loss for batch 3279 : 0.07826664298772812\n",
      "Training loss for batch 3280 : 0.12157291918992996\n",
      "Training loss for batch 3281 : 0.21540959179401398\n",
      "Training loss for batch 3282 : 0.3971007764339447\n",
      "Training loss for batch 3283 : 0.06389745324850082\n",
      "Training loss for batch 3284 : 0.17793451249599457\n",
      "Training loss for batch 3285 : 0.34215715527534485\n",
      "Training loss for batch 3286 : 0.0\n",
      "Training loss for batch 3287 : 0.20380157232284546\n",
      "Training loss for batch 3288 : 0.054948899894952774\n",
      "Training loss for batch 3289 : 0.7630486488342285\n",
      "Training loss for batch 3290 : 0.2873648405075073\n",
      "Training loss for batch 3291 : 0.005795006640255451\n",
      "Training loss for batch 3292 : 0.0033623874187469482\n",
      "Training loss for batch 3293 : 0.298953652381897\n",
      "Training loss for batch 3294 : 0.007233153097331524\n",
      "Training loss for batch 3295 : 0.34572896361351013\n",
      "Training loss for batch 3296 : 0.05974707007408142\n",
      "Training loss for batch 3297 : 0.038387708365917206\n",
      "Training loss for batch 3298 : 0.26775655150413513\n",
      "Training loss for batch 3299 : 0.3863292336463928\n",
      "Training loss for batch 3300 : 0.19402503967285156\n",
      "Training loss for batch 3301 : 0.07098918408155441\n",
      "Training loss for batch 3302 : 0.21771104633808136\n",
      "Training loss for batch 3303 : 0.09393960982561111\n",
      "Training loss for batch 3304 : 0.1310831606388092\n",
      "Training loss for batch 3305 : 0.1584179401397705\n",
      "Training loss for batch 3306 : 0.4251379072666168\n",
      "Training loss for batch 3307 : 0.014188548550009727\n",
      "Training loss for batch 3308 : 0.14017772674560547\n",
      "Training loss for batch 3309 : 0.39230597019195557\n",
      "Training loss for batch 3310 : 0.07458806037902832\n",
      "Training loss for batch 3311 : 0.34541627764701843\n",
      "Training loss for batch 3312 : 0.21786919236183167\n",
      "Training loss for batch 3313 : 0.3977789282798767\n",
      "Training loss for batch 3314 : 0.11149235814809799\n",
      "Training loss for batch 3315 : 0.06090486794710159\n",
      "Training loss for batch 3316 : 0.24381212890148163\n",
      "Training loss for batch 3317 : 0.1587745100259781\n",
      "Training loss for batch 3318 : 0.35715699195861816\n",
      "Training loss for batch 3319 : 0.3533532917499542\n",
      "Training loss for batch 3320 : 0.18200372159481049\n",
      "Training loss for batch 3321 : 0.06455543637275696\n",
      "Training loss for batch 3322 : 0.01627509854733944\n",
      "Training loss for batch 3323 : 0.23883679509162903\n",
      "Training loss for batch 3324 : 0.190291166305542\n",
      "Training loss for batch 3325 : 0.04955438897013664\n",
      "Training loss for batch 3326 : 0.14415358006954193\n",
      "Training loss for batch 3327 : 0.28911107778549194\n",
      "Training loss for batch 3328 : -0.00028530933195725083\n",
      "Training loss for batch 3329 : 0.3467985689640045\n",
      "Training loss for batch 3330 : 0.08409230411052704\n",
      "Training loss for batch 3331 : 0.4385780096054077\n",
      "Training loss for batch 3332 : 0.5009589791297913\n",
      "Training loss for batch 3333 : 0.18182918429374695\n",
      "Training loss for batch 3334 : 0.06020202860236168\n",
      "Training loss for batch 3335 : 0.015484845265746117\n",
      "Training loss for batch 3336 : 0.15250959992408752\n",
      "Training loss for batch 3337 : 0.1313096433877945\n",
      "Training loss for batch 3338 : 0.24975934624671936\n",
      "Training loss for batch 3339 : 0.22477445006370544\n",
      "Training loss for batch 3340 : 0.003750791307538748\n",
      "Training loss for batch 3341 : 0.4496765434741974\n",
      "Training loss for batch 3342 : 0.2133064568042755\n",
      "Training loss for batch 3343 : 0.514019787311554\n",
      "Training loss for batch 3344 : 0.23744812607765198\n",
      "Training loss for batch 3345 : 0.5055606961250305\n",
      "Training loss for batch 3346 : 0.2540036141872406\n",
      "Training loss for batch 3347 : 0.026626015082001686\n",
      "Training loss for batch 3348 : 0.32081523537635803\n",
      "Training loss for batch 3349 : 0.27014878392219543\n",
      "Training loss for batch 3350 : 0.37700629234313965\n",
      "Training loss for batch 3351 : 0.017124317586421967\n",
      "Training loss for batch 3352 : 0.09623740613460541\n",
      "Training loss for batch 3353 : 0.09360110014677048\n",
      "Training loss for batch 3354 : 0.1550316959619522\n",
      "Training loss for batch 3355 : 0.06427571922540665\n",
      "Training loss for batch 3356 : 0.23292113840579987\n",
      "Training loss for batch 3357 : 0.3220788240432739\n",
      "Training loss for batch 3358 : 0.11012718081474304\n",
      "Training loss for batch 3359 : 0.5010372400283813\n",
      "Training loss for batch 3360 : 0.3947419822216034\n",
      "Training loss for batch 3361 : 0.17432034015655518\n",
      "Training loss for batch 3362 : 0.0959867462515831\n",
      "Training loss for batch 3363 : 0.0612480528652668\n",
      "Training loss for batch 3364 : 0.1467406004667282\n",
      "Training loss for batch 3365 : 0.16092664003372192\n",
      "Training loss for batch 3366 : 0.2763267159461975\n",
      "Training loss for batch 3367 : 0.17761142551898956\n",
      "Training loss for batch 3368 : 0.1480034738779068\n",
      "Training loss for batch 3369 : 0.2511169910430908\n",
      "Training loss for batch 3370 : 0.3044486939907074\n",
      "Training loss for batch 3371 : 0.17917387187480927\n",
      "Training loss for batch 3372 : 0.1617748737335205\n",
      "Training loss for batch 3373 : 0.007230088114738464\n",
      "Training loss for batch 3374 : 0.07608352601528168\n",
      "Training loss for batch 3375 : 0.026887238025665283\n",
      "Training loss for batch 3376 : 0.13298436999320984\n",
      "Training loss for batch 3377 : 0.1545744687318802\n",
      "Training loss for batch 3378 : 0.061459820717573166\n",
      "Training loss for batch 3379 : 0.19667406380176544\n",
      "Training loss for batch 3380 : 0.1766568422317505\n",
      "Training loss for batch 3381 : 0.12130646407604218\n",
      "Training loss for batch 3382 : 0.07171999663114548\n",
      "Training loss for batch 3383 : 0.20083528757095337\n",
      "Training loss for batch 3384 : 0.04362885281443596\n",
      "Training loss for batch 3385 : 0.28588977456092834\n",
      "Training loss for batch 3386 : 0.12118140608072281\n",
      "Training loss for batch 3387 : 0.4528687000274658\n",
      "Training loss for batch 3388 : 0.31473326683044434\n",
      "Training loss for batch 3389 : 0.3373139202594757\n",
      "Training loss for batch 3390 : 0.22995702922344208\n",
      "Training loss for batch 3391 : 0.212018221616745\n",
      "Training loss for batch 3392 : 0.003857394214719534\n",
      "Training loss for batch 3393 : 0.1842639148235321\n",
      "Training loss for batch 3394 : 0.33572918176651\n",
      "Training loss for batch 3395 : 0.17272916436195374\n",
      "Training loss for batch 3396 : 0.02924872562289238\n",
      "Training loss for batch 3397 : 0.4340643584728241\n",
      "Training loss for batch 3398 : 0.14821504056453705\n",
      "Training loss for batch 3399 : 0.12937875092029572\n",
      "Training loss for batch 3400 : 0.057150062173604965\n",
      "Training loss for batch 3401 : 0.28658488392829895\n",
      "Training loss for batch 3402 : 0.06640176475048065\n",
      "Training loss for batch 3403 : 0.1497018039226532\n",
      "Training loss for batch 3404 : 0.32927176356315613\n",
      "Training loss for batch 3405 : 0.22195258736610413\n",
      "Training loss for batch 3406 : 0.28039124608039856\n",
      "Training loss for batch 3407 : 0.04974883049726486\n",
      "Training loss for batch 3408 : 0.06324947625398636\n",
      "Training loss for batch 3409 : 0.043993234634399414\n",
      "Training loss for batch 3410 : 0.3850821852684021\n",
      "Training loss for batch 3411 : 0.43955284357070923\n",
      "Training loss for batch 3412 : 0.2621634900569916\n",
      "Training loss for batch 3413 : 0.3040255606174469\n",
      "Training loss for batch 3414 : 0.21415644884109497\n",
      "Training loss for batch 3415 : 0.07323874533176422\n",
      "Training loss for batch 3416 : 0.15629658102989197\n",
      "Training loss for batch 3417 : 0.22429592907428741\n",
      "Training loss for batch 3418 : 0.09022800624370575\n",
      "Training loss for batch 3419 : 0.1660662591457367\n",
      "Training loss for batch 3420 : 0.3265584111213684\n",
      "Training loss for batch 3421 : 0.14769423007965088\n",
      "Training loss for batch 3422 : 0.061398498713970184\n",
      "Training loss for batch 3423 : 0.13300906121730804\n",
      "Training loss for batch 3424 : 0.14359311759471893\n",
      "Training loss for batch 3425 : 0.08627097308635712\n",
      "Training loss for batch 3426 : 0.33803144097328186\n",
      "Training loss for batch 3427 : 0.24124422669410706\n",
      "Training loss for batch 3428 : 0.2699788808822632\n",
      "Training loss for batch 3429 : 0.0855439230799675\n",
      "Training loss for batch 3430 : 0.005243493709713221\n",
      "Training loss for batch 3431 : 0.3635117709636688\n",
      "Training loss for batch 3432 : 0.2688782513141632\n",
      "Training loss for batch 3433 : 0.05101881921291351\n",
      "Training loss for batch 3434 : 0.1364409327507019\n",
      "Training loss for batch 3435 : 0.2600824236869812\n",
      "Training loss for batch 3436 : 0.06916829943656921\n",
      "Training loss for batch 3437 : 0.5290777683258057\n",
      "Training loss for batch 3438 : 0.04729254171252251\n",
      "Training loss for batch 3439 : 0.2941139042377472\n",
      "Training loss for batch 3440 : 0.23720145225524902\n",
      "Training loss for batch 3441 : 0.12312057614326477\n",
      "Training loss for batch 3442 : 0.3172104060649872\n",
      "Training loss for batch 3443 : 0.051207564771175385\n",
      "Training loss for batch 3444 : 0.2726304829120636\n",
      "Training loss for batch 3445 : 0.10272637754678726\n",
      "Training loss for batch 3446 : 0.05996035784482956\n",
      "Training loss for batch 3447 : 0.038609981536865234\n",
      "Training loss for batch 3448 : 0.044149383902549744\n",
      "Training loss for batch 3449 : 0.24261796474456787\n",
      "Training loss for batch 3450 : 0.007844021543860435\n",
      "Training loss for batch 3451 : 0.21599754691123962\n",
      "Training loss for batch 3452 : 0.05178350210189819\n",
      "Training loss for batch 3453 : 0.15357881784439087\n",
      "Training loss for batch 3454 : 0.005617600400000811\n",
      "Training loss for batch 3455 : 0.4903067648410797\n",
      "Training loss for batch 3456 : 0.22279663383960724\n",
      "Training loss for batch 3457 : 0.00034249824238941073\n",
      "Training loss for batch 3458 : 0.1283334195613861\n",
      "Training loss for batch 3459 : 0.18864287436008453\n",
      "Training loss for batch 3460 : 0.41011446714401245\n",
      "Training loss for batch 3461 : 0.18904177844524384\n",
      "Training loss for batch 3462 : 0.3223299980163574\n",
      "Training loss for batch 3463 : 0.15984420478343964\n",
      "Training loss for batch 3464 : 0.9520341157913208\n",
      "Training loss for batch 3465 : 0.6084502935409546\n",
      "Training loss for batch 3466 : 0.31070730090141296\n",
      "Training loss for batch 3467 : 0.050803594291210175\n",
      "Training loss for batch 3468 : 0.03735644370317459\n",
      "Training loss for batch 3469 : 0.08396940678358078\n",
      "Training loss for batch 3470 : 0.10870987176895142\n",
      "Training loss for batch 3471 : 0.0027591511607170105\n",
      "Training loss for batch 3472 : 0.215952530503273\n",
      "Training loss for batch 3473 : 0.4408416152000427\n",
      "Training loss for batch 3474 : 0.0924450159072876\n",
      "Training loss for batch 3475 : 0.3348681926727295\n",
      "Training loss for batch 3476 : 0.2989594638347626\n",
      "Training loss for batch 3477 : 0.17341403663158417\n",
      "Training loss for batch 3478 : 0.22901341319084167\n",
      "Training loss for batch 3479 : 0.08151072263717651\n",
      "Training loss for batch 3480 : 0.014105197973549366\n",
      "Training loss for batch 3481 : 0.3899412751197815\n",
      "Training loss for batch 3482 : 0.2209557145833969\n",
      "Training loss for batch 3483 : 0.12482263147830963\n",
      "Training loss for batch 3484 : 0.06363712251186371\n",
      "Training loss for batch 3485 : 0.24941986799240112\n",
      "Training loss for batch 3486 : 0.17390254139900208\n",
      "Training loss for batch 3487 : 0.06991337239742279\n",
      "Training loss for batch 3488 : 0.12367702275514603\n",
      "Training loss for batch 3489 : 0.13269999623298645\n",
      "Training loss for batch 3490 : 0.42022836208343506\n",
      "Training loss for batch 3491 : 0.2403153032064438\n",
      "Training loss for batch 3492 : 0.2878738045692444\n",
      "Training loss for batch 3493 : 0.3351399302482605\n",
      "Training loss for batch 3494 : 0.2539805769920349\n",
      "Training loss for batch 3495 : 0.1783210039138794\n",
      "Training loss for batch 3496 : 0.3623422384262085\n",
      "Training loss for batch 3497 : 0.4296588897705078\n",
      "Training loss for batch 3498 : 0.24549560248851776\n",
      "Training loss for batch 3499 : 0.40092769265174866\n",
      "Training loss for batch 3500 : 0.2506401836872101\n",
      "Training loss for batch 3501 : 0.053749557584524155\n",
      "Training loss for batch 3502 : 0.1853569895029068\n",
      "Training loss for batch 3503 : 0.004133333917707205\n",
      "Training loss for batch 3504 : 0.20678672194480896\n",
      "Training loss for batch 3505 : 0.4820840358734131\n",
      "Training loss for batch 3506 : 0.10211014002561569\n",
      "Training loss for batch 3507 : 0.13361160457134247\n",
      "Training loss for batch 3508 : 0.21552519500255585\n",
      "Training loss for batch 3509 : 0.2640090584754944\n",
      "Training loss for batch 3510 : 0.43022000789642334\n",
      "Training loss for batch 3511 : 0.38372811675071716\n",
      "Training loss for batch 3512 : 0.13806578516960144\n",
      "Training loss for batch 3513 : 0.08855380862951279\n",
      "Training loss for batch 3514 : 0.319185733795166\n",
      "Training loss for batch 3515 : 0.32057705521583557\n",
      "Training loss for batch 3516 : 0.06063035875558853\n",
      "Training loss for batch 3517 : 0.25394657254219055\n",
      "Training loss for batch 3518 : 0.3057607114315033\n",
      "Training loss for batch 3519 : 0.32107046246528625\n",
      "Training loss for batch 3520 : 0.030253976583480835\n",
      "Training loss for batch 3521 : 0.4263198673725128\n",
      "Training loss for batch 3522 : 0.18527477979660034\n",
      "Training loss for batch 3523 : 0.24670784175395966\n",
      "Training loss for batch 3524 : 0.23681163787841797\n",
      "Training loss for batch 3525 : 0.18709248304367065\n",
      "Training loss for batch 3526 : 0.329401433467865\n",
      "Training loss for batch 3527 : 0.3949105441570282\n",
      "Training loss for batch 3528 : 0.10503404587507248\n",
      "Training loss for batch 3529 : 0.4202598035335541\n",
      "Training loss for batch 3530 : 0.18496425449848175\n",
      "Training loss for batch 3531 : 0.06788595020771027\n",
      "Training loss for batch 3532 : 0.4684735834598541\n",
      "Training loss for batch 3533 : 0.08134006708860397\n",
      "Training loss for batch 3534 : 0.14201019704341888\n",
      "Training loss for batch 3535 : 0.21530844271183014\n",
      "Training loss for batch 3536 : 0.3435337543487549\n",
      "Training loss for batch 3537 : 0.2524747848510742\n",
      "Training loss for batch 3538 : 0.2685002088546753\n",
      "Training loss for batch 3539 : 0.15947334468364716\n",
      "Training loss for batch 3540 : 0.34445449709892273\n",
      "Training loss for batch 3541 : 0.19944089651107788\n",
      "Training loss for batch 3542 : 0.13114723563194275\n",
      "Training loss for batch 3543 : 0.16226045787334442\n",
      "Training loss for batch 3544 : 0.027452588081359863\n",
      "Training loss for batch 3545 : 0.24261607229709625\n",
      "Training loss for batch 3546 : 0.032186128199100494\n",
      "Training loss for batch 3547 : 0.3389679789543152\n",
      "Training loss for batch 3548 : 0.09762874245643616\n",
      "Training loss for batch 3549 : 0.24626849591732025\n",
      "Training loss for batch 3550 : 0.031779635697603226\n",
      "Training loss for batch 3551 : 0.1690981686115265\n",
      "Training loss for batch 3552 : 0.1070612296462059\n",
      "Training loss for batch 3553 : 0.002884825225919485\n",
      "Training loss for batch 3554 : 0.09985149651765823\n",
      "Training loss for batch 3555 : 0.03395902365446091\n",
      "Training loss for batch 3556 : 0.38821470737457275\n",
      "Training loss for batch 3557 : 0.18398775160312653\n",
      "Training loss for batch 3558 : 0.17652347683906555\n",
      "Training loss for batch 3559 : 0.3292335867881775\n",
      "Training loss for batch 3560 : 0.31086280941963196\n",
      "Training loss for batch 3561 : 0.02735564112663269\n",
      "Training loss for batch 3562 : 0.2063349485397339\n",
      "Training loss for batch 3563 : 0.2428773194551468\n",
      "Training loss for batch 3564 : 0.08665057271718979\n",
      "Training loss for batch 3565 : 0.19572313129901886\n",
      "Training loss for batch 3566 : 0.2596336901187897\n",
      "Training loss for batch 3567 : 0.010637612082064152\n",
      "Training loss for batch 3568 : 0.0920683890581131\n",
      "Training loss for batch 3569 : 0.09659631550312042\n",
      "Training loss for batch 3570 : 0.06815199553966522\n",
      "Training loss for batch 3571 : 0.3764286935329437\n",
      "Training loss for batch 3572 : 0.23072704672813416\n",
      "Training loss for batch 3573 : 0.39873990416526794\n",
      "Training loss for batch 3574 : 0.1292465627193451\n",
      "Training loss for batch 3575 : 0.13670141994953156\n",
      "Training loss for batch 3576 : 0.06836210191249847\n",
      "Training loss for batch 3577 : 0.060995426028966904\n",
      "Training loss for batch 3578 : 0.10857955366373062\n",
      "Training loss for batch 3579 : 0.22063693404197693\n",
      "Training loss for batch 3580 : 0.14518029987812042\n",
      "Training loss for batch 3581 : 0.2748270630836487\n",
      "Training loss for batch 3582 : 0.1527821272611618\n",
      "Training loss for batch 3583 : 0.27784284949302673\n",
      "Training loss for batch 3584 : 0.21737253665924072\n",
      "Training loss for batch 3585 : 0.7652145028114319\n",
      "Training loss for batch 3586 : 0.32954657077789307\n",
      "Training loss for batch 3587 : 0.36871179938316345\n",
      "Training loss for batch 3588 : 0.6463022828102112\n",
      "Training loss for batch 3589 : 0.1240086778998375\n",
      "Training loss for batch 3590 : 0.1869751662015915\n",
      "Training loss for batch 3591 : 0.16985571384429932\n",
      "Training loss for batch 3592 : 0.024481385946273804\n",
      "Training loss for batch 3593 : 0.1373540759086609\n",
      "Training loss for batch 3594 : 0.199494868516922\n",
      "Training loss for batch 3595 : 0.1089828759431839\n",
      "Training loss for batch 3596 : 0.17404070496559143\n",
      "Training loss for batch 3597 : 0.04116411134600639\n",
      "Training loss for batch 3598 : 0.1614302545785904\n",
      "Training loss for batch 3599 : 0.28701499104499817\n",
      "Training loss for batch 3600 : 0.3466363251209259\n",
      "Training loss for batch 3601 : 0.011217594146728516\n",
      "Training loss for batch 3602 : 0.2476460486650467\n",
      "Training loss for batch 3603 : 0.16878890991210938\n",
      "Training loss for batch 3604 : 0.10798881202936172\n",
      "Training loss for batch 3605 : 0.6491718888282776\n",
      "Training loss for batch 3606 : 0.10904090106487274\n",
      "Training loss for batch 3607 : 0.19292223453521729\n",
      "Training loss for batch 3608 : 0.17685489356517792\n",
      "Training loss for batch 3609 : 0.06125572696328163\n",
      "Training loss for batch 3610 : 0.12921586632728577\n",
      "Training loss for batch 3611 : 0.0031426928471773863\n",
      "Training loss for batch 3612 : 0.21331475675106049\n",
      "Training loss for batch 3613 : 0.25347280502319336\n",
      "Training loss for batch 3614 : 0.20026706159114838\n",
      "Training loss for batch 3615 : 0.1799405962228775\n",
      "Training loss for batch 3616 : 0.2335081249475479\n",
      "Training loss for batch 3617 : 0.0691380575299263\n",
      "Training loss for batch 3618 : 0.15204133093357086\n",
      "Training loss for batch 3619 : 0.09944309294223785\n",
      "Training loss for batch 3620 : 0.2666076719760895\n",
      "Training loss for batch 3621 : 0.06413143873214722\n",
      "Training loss for batch 3622 : 0.05067252367734909\n",
      "Training loss for batch 3623 : 0.10924364626407623\n",
      "Training loss for batch 3624 : 0.08890338987112045\n",
      "Training loss for batch 3625 : 0.1241670548915863\n",
      "Training loss for batch 3626 : 0.1500280201435089\n",
      "Training loss for batch 3627 : 0.2675188183784485\n",
      "Training loss for batch 3628 : 0.5036525726318359\n",
      "Training loss for batch 3629 : 0.06299319118261337\n",
      "Training loss for batch 3630 : 0.14213719964027405\n",
      "Training loss for batch 3631 : 0.128072589635849\n",
      "Training loss for batch 3632 : 0.22488448023796082\n",
      "Training loss for batch 3633 : 0.06623676419258118\n",
      "Training loss for batch 3634 : 0.11684011667966843\n",
      "Training loss for batch 3635 : 0.06927825510501862\n",
      "Training loss for batch 3636 : 0.289340078830719\n",
      "Training loss for batch 3637 : 0.016536682844161987\n",
      "Training loss for batch 3638 : 0.079920195043087\n",
      "Training loss for batch 3639 : 0.09693235903978348\n",
      "Training loss for batch 3640 : 0.04546060785651207\n",
      "Training loss for batch 3641 : 0.25122126936912537\n",
      "Training loss for batch 3642 : 0.15411512553691864\n",
      "Training loss for batch 3643 : -0.0006231932202354074\n",
      "Training loss for batch 3644 : 0.17007037997245789\n",
      "Training loss for batch 3645 : 0.06699974834918976\n",
      "Training loss for batch 3646 : 0.3724782466888428\n",
      "Training loss for batch 3647 : 0.12198082357645035\n",
      "Training loss for batch 3648 : 0.40283143520355225\n",
      "Training loss for batch 3649 : 0.061199188232421875\n",
      "Training loss for batch 3650 : 0.1584114134311676\n",
      "Training loss for batch 3651 : 0.21386674046516418\n",
      "Training loss for batch 3652 : 0.23370802402496338\n",
      "Training loss for batch 3653 : 0.08124013990163803\n",
      "Training loss for batch 3654 : 0.024862168356776237\n",
      "Training loss for batch 3655 : 0.29200175404548645\n",
      "Training loss for batch 3656 : 0.05701203644275665\n",
      "Training loss for batch 3657 : 0.17837849259376526\n",
      "Training loss for batch 3658 : 0.18928474187850952\n",
      "Training loss for batch 3659 : 0.05051359906792641\n",
      "Training loss for batch 3660 : 0.06262250244617462\n",
      "Training loss for batch 3661 : 0.029254987835884094\n",
      "Training loss for batch 3662 : 0.2705604135990143\n",
      "Training loss for batch 3663 : 0.3136384189128876\n",
      "Training loss for batch 3664 : 0.14384739100933075\n",
      "Training loss for batch 3665 : 0.20375216007232666\n",
      "Training loss for batch 3666 : 0.42174383997917175\n",
      "Training loss for batch 3667 : 0.2625916302204132\n",
      "Training loss for batch 3668 : 0.25264057517051697\n",
      "Training loss for batch 3669 : 0.12143119424581528\n",
      "Training loss for batch 3670 : 0.09676507860422134\n",
      "Training loss for batch 3671 : 0.20289470255374908\n",
      "Training loss for batch 3672 : 0.11192934215068817\n",
      "Training loss for batch 3673 : 0.014868563041090965\n",
      "Training loss for batch 3674 : 0.1416168361902237\n",
      "Training loss for batch 3675 : 0.20850276947021484\n",
      "Training loss for batch 3676 : 0.03255680575966835\n",
      "Training loss for batch 3677 : 0.2984653115272522\n",
      "Training loss for batch 3678 : 0.427535742521286\n",
      "Training loss for batch 3679 : 0.08859393000602722\n",
      "Training loss for batch 3680 : 0.24975356459617615\n",
      "Training loss for batch 3681 : 0.20518112182617188\n",
      "Training loss for batch 3682 : 0.3234401047229767\n",
      "Training loss for batch 3683 : 0.19065503776073456\n",
      "Training loss for batch 3684 : 0.24184611439704895\n",
      "Training loss for batch 3685 : 0.13873784244060516\n",
      "Training loss for batch 3686 : 0.08423119783401489\n",
      "Training loss for batch 3687 : 0.24073675274848938\n",
      "Training loss for batch 3688 : 0.06401460617780685\n",
      "Training loss for batch 3689 : 0.22258076071739197\n",
      "Training loss for batch 3690 : 0.3031651973724365\n",
      "Training loss for batch 3691 : 0.07419028133153915\n",
      "Training loss for batch 3692 : 0.27822446823120117\n",
      "Training loss for batch 3693 : 0.03666418418288231\n",
      "Training loss for batch 3694 : 0.281955748796463\n",
      "Training loss for batch 3695 : 0.37033215165138245\n",
      "Training loss for batch 3696 : 0.3113105297088623\n",
      "Training loss for batch 3697 : 0.15143056213855743\n",
      "Training loss for batch 3698 : 0.17520001530647278\n",
      "Training loss for batch 3699 : 0.0\n",
      "Training loss for batch 3700 : 0.07407598942518234\n",
      "Training loss for batch 3701 : 0.045400720089673996\n",
      "Training loss for batch 3702 : 0.4268493056297302\n",
      "Training loss for batch 3703 : 0.08195219933986664\n",
      "Training loss for batch 3704 : 0.17724353075027466\n",
      "Training loss for batch 3705 : 0.2643943727016449\n",
      "Training loss for batch 3706 : 0.4419509470462799\n",
      "Training loss for batch 3707 : 0.19848088920116425\n",
      "Training loss for batch 3708 : 0.08776384592056274\n",
      "Training loss for batch 3709 : 0.27715393900871277\n",
      "Training loss for batch 3710 : 0.038940511643886566\n",
      "Training loss for batch 3711 : 0.08659284561872482\n",
      "Training loss for batch 3712 : 0.2836950719356537\n",
      "Training loss for batch 3713 : 0.03875037655234337\n",
      "Training loss for batch 3714 : 0.09842663258314133\n",
      "Training loss for batch 3715 : 0.07123061269521713\n",
      "Training loss for batch 3716 : 0.10873700678348541\n",
      "Training loss for batch 3717 : 0.32602977752685547\n",
      "Training loss for batch 3718 : 0.024662703275680542\n",
      "Training loss for batch 3719 : 0.24177898466587067\n",
      "Training loss for batch 3720 : 0.0402262918651104\n",
      "Training loss for batch 3721 : 0.25645911693573\n",
      "Training loss for batch 3722 : 0.04680851101875305\n",
      "Training loss for batch 3723 : 0.05427985638380051\n",
      "Training loss for batch 3724 : 0.0\n",
      "Training loss for batch 3725 : 0.10737942159175873\n",
      "Training loss for batch 3726 : 0.1617247611284256\n",
      "Training loss for batch 3727 : 0.0902274027466774\n",
      "Training loss for batch 3728 : 0.560285210609436\n",
      "Training loss for batch 3729 : 0.10279092192649841\n",
      "Training loss for batch 3730 : 0.07493934035301208\n",
      "Training loss for batch 3731 : 0.11232584714889526\n",
      "Training loss for batch 3732 : 0.22889776527881622\n",
      "Training loss for batch 3733 : 0.05266953259706497\n",
      "Training loss for batch 3734 : 0.08564236760139465\n",
      "Training loss for batch 3735 : 0.09333541989326477\n",
      "Training loss for batch 3736 : 0.0018386265728622675\n",
      "Training loss for batch 3737 : 0.03812273591756821\n",
      "Training loss for batch 3738 : 0.12483392655849457\n",
      "Training loss for batch 3739 : 0.028479140251874924\n",
      "Training loss for batch 3740 : 0.21874377131462097\n",
      "Training loss for batch 3741 : 0.0\n",
      "Training loss for batch 3742 : 0.2402081936597824\n",
      "Training loss for batch 3743 : 0.499784380197525\n",
      "Training loss for batch 3744 : 0.20325039327144623\n",
      "Training loss for batch 3745 : 0.15949159860610962\n",
      "Training loss for batch 3746 : 0.09622038155794144\n",
      "Training loss for batch 3747 : 0.320058673620224\n",
      "Training loss for batch 3748 : 0.042074255645275116\n",
      "Training loss for batch 3749 : 0.2865004241466522\n",
      "Training loss for batch 3750 : 0.0323493629693985\n",
      "Training loss for batch 3751 : 0.13867536187171936\n",
      "Training loss for batch 3752 : 0.16464655101299286\n",
      "Training loss for batch 3753 : 0.11540813744068146\n",
      "Training loss for batch 3754 : 0.3407134413719177\n",
      "Training loss for batch 3755 : 0.3590855896472931\n",
      "Training loss for batch 3756 : 0.3604969084262848\n",
      "Training loss for batch 3757 : 0.32067832350730896\n",
      "Training loss for batch 3758 : 0.13094837963581085\n",
      "Training loss for batch 3759 : 0.02511981874704361\n",
      "Training loss for batch 3760 : 0.3149915933609009\n",
      "Training loss for batch 3761 : 0.11806405335664749\n",
      "Training loss for batch 3762 : 0.24102744460105896\n",
      "Training loss for batch 3763 : 0.06794776767492294\n",
      "Training loss for batch 3764 : 0.05840282514691353\n",
      "Training loss for batch 3765 : 0.12827858328819275\n",
      "Training loss for batch 3766 : 0.05213560163974762\n",
      "Training loss for batch 3767 : 0.06802915781736374\n",
      "Training loss for batch 3768 : 0.35891780257225037\n",
      "Training loss for batch 3769 : 0.19765818119049072\n",
      "Training loss for batch 3770 : 0.252949595451355\n",
      "Training loss for batch 3771 : 0.0047698658891022205\n",
      "Training loss for batch 3772 : 0.042348217219114304\n",
      "Training loss for batch 3773 : 0.17828674614429474\n",
      "Training loss for batch 3774 : 0.12693333625793457\n",
      "Training loss for batch 3775 : 0.3073027431964874\n",
      "Training loss for batch 3776 : 0.168657124042511\n",
      "Training loss for batch 3777 : 0.14086726307868958\n",
      "Training loss for batch 3778 : 0.3442225754261017\n",
      "Training loss for batch 3779 : 0.2942439019680023\n",
      "Training loss for batch 3780 : 0.4215914309024811\n",
      "Training loss for batch 3781 : 0.13109947741031647\n",
      "Training loss for batch 3782 : 0.002399842021986842\n",
      "Training loss for batch 3783 : 0.4406169056892395\n",
      "Training loss for batch 3784 : 0.10789330303668976\n",
      "Training loss for batch 3785 : 0.0016416391590610147\n",
      "Training loss for batch 3786 : 0.05461963638663292\n",
      "Training loss for batch 3787 : 0.2970714867115021\n",
      "Training loss for batch 3788 : 0.33224111795425415\n",
      "Training loss for batch 3789 : 0.2564942240715027\n",
      "Training loss for batch 3790 : 0.0010258376132696867\n",
      "Training loss for batch 3791 : 0.009638786315917969\n",
      "Training loss for batch 3792 : 0.13018937408924103\n",
      "Training loss for batch 3793 : 0.49057525396347046\n",
      "Training loss for batch 3794 : 0.13612587749958038\n",
      "Training loss for batch 3795 : 0.11066984385251999\n",
      "Training loss for batch 3796 : 0.03856369107961655\n",
      "Training loss for batch 3797 : 0.3925699293613434\n",
      "Training loss for batch 3798 : 0.3538289964199066\n",
      "Training loss for batch 3799 : 0.09730320423841476\n",
      "Training loss for batch 3800 : 0.06356925517320633\n",
      "Training loss for batch 3801 : 0.12384646385908127\n",
      "Training loss for batch 3802 : 0.023981580510735512\n",
      "Training loss for batch 3803 : 0.18258748948574066\n",
      "Training loss for batch 3804 : 0.2230464369058609\n",
      "Training loss for batch 3805 : 0.17836911976337433\n",
      "Training loss for batch 3806 : 0.1306743621826172\n",
      "Training loss for batch 3807 : 0.3028462827205658\n",
      "Training loss for batch 3808 : 0.17275385558605194\n",
      "Training loss for batch 3809 : 0.5987079739570618\n",
      "Training loss for batch 3810 : 0.2186824083328247\n",
      "Training loss for batch 3811 : 0.37502843141555786\n",
      "Training loss for batch 3812 : 0.08591525256633759\n",
      "Training loss for batch 3813 : 0.24556253850460052\n",
      "Training loss for batch 3814 : 0.402068167924881\n",
      "Training loss for batch 3815 : 0.27486786246299744\n",
      "Training loss for batch 3816 : 0.07679329812526703\n",
      "Training loss for batch 3817 : 0.039358071982860565\n",
      "Training loss for batch 3818 : 0.09756843745708466\n",
      "Training loss for batch 3819 : 0.03989167883992195\n",
      "Training loss for batch 3820 : 0.10520096868276596\n",
      "Training loss for batch 3821 : 0.14274142682552338\n",
      "Training loss for batch 3822 : 0.625312328338623\n",
      "Training loss for batch 3823 : 0.4324883222579956\n",
      "Training loss for batch 3824 : 0.22316597402095795\n",
      "Training loss for batch 3825 : 0.04595920816063881\n",
      "Training loss for batch 3826 : 0.24450649321079254\n",
      "Training loss for batch 3827 : 0.14717327058315277\n",
      "Training loss for batch 3828 : 0.13177050650119781\n",
      "Training loss for batch 3829 : 0.07256580889225006\n",
      "Training loss for batch 3830 : 0.208921879529953\n",
      "Training loss for batch 3831 : 0.026201175525784492\n",
      "Training loss for batch 3832 : 0.23864470422267914\n",
      "Training loss for batch 3833 : 0.043395012617111206\n",
      "Training loss for batch 3834 : 0.2950930893421173\n",
      "Training loss for batch 3835 : 0.1102333590388298\n",
      "Training loss for batch 3836 : 0.25481805205345154\n",
      "Training loss for batch 3837 : 0.0\n",
      "Training loss for batch 3838 : 0.10914099216461182\n",
      "Training loss for batch 3839 : 0.1192278191447258\n",
      "Training loss for batch 3840 : 0.12137309461832047\n",
      "Training loss for batch 3841 : 0.3062104284763336\n",
      "Training loss for batch 3842 : 0.0784946158528328\n",
      "Training loss for batch 3843 : 0.10821177810430527\n",
      "Training loss for batch 3844 : 0.09081816673278809\n",
      "Training loss for batch 3845 : 0.14098621904850006\n",
      "Training loss for batch 3846 : 0.24004410207271576\n",
      "Training loss for batch 3847 : 0.21599383652210236\n",
      "Training loss for batch 3848 : 0.21922561526298523\n",
      "Training loss for batch 3849 : 0.35782521963119507\n",
      "Training loss for batch 3850 : 0.020710822194814682\n",
      "Training loss for batch 3851 : 0.267673522233963\n",
      "Training loss for batch 3852 : 0.15382583439350128\n",
      "Training loss for batch 3853 : 0.217369943857193\n",
      "Training loss for batch 3854 : 0.09338628500699997\n",
      "Training loss for batch 3855 : 0.29805147647857666\n",
      "Training loss for batch 3856 : 0.04688189923763275\n",
      "Training loss for batch 3857 : 0.3414612412452698\n",
      "Training loss for batch 3858 : 0.11984463036060333\n",
      "Training loss for batch 3859 : 0.21999579668045044\n",
      "Training loss for batch 3860 : 0.2836642563343048\n",
      "Training loss for batch 3861 : 0.2675752639770508\n",
      "Training loss for batch 3862 : 0.2772483825683594\n",
      "Training loss for batch 3863 : 0.22591063380241394\n",
      "Training loss for batch 3864 : 0.14628759026527405\n",
      "Training loss for batch 3865 : 0.20690804719924927\n",
      "Training loss for batch 3866 : 0.05563846603035927\n",
      "Training loss for batch 3867 : 0.22234061360359192\n",
      "Training loss for batch 3868 : 0.338420033454895\n",
      "Training loss for batch 3869 : 0.3550817370414734\n",
      "Training loss for batch 3870 : 0.16521604359149933\n",
      "Training loss for batch 3871 : 0.28308817744255066\n",
      "Training loss for batch 3872 : 0.2237180471420288\n",
      "Training loss for batch 3873 : 0.29924601316452026\n",
      "Training loss for batch 3874 : 0.0534580759704113\n",
      "Training loss for batch 3875 : 0.3029539883136749\n",
      "Training loss for batch 3876 : 0.07585754990577698\n",
      "Training loss for batch 3877 : 0.293019562959671\n",
      "Training loss for batch 3878 : 0.020382549613714218\n",
      "Training loss for batch 3879 : 0.014072248712182045\n",
      "Training loss for batch 3880 : 0.3946947753429413\n",
      "Training loss for batch 3881 : 0.22486089169979095\n",
      "Training loss for batch 3882 : 0.25942903757095337\n",
      "Training loss for batch 3883 : 0.016787119209766388\n",
      "Training loss for batch 3884 : 0.1844303160905838\n",
      "Training loss for batch 3885 : 0.04269362613558769\n",
      "Training loss for batch 3886 : 0.12019658088684082\n",
      "Training loss for batch 3887 : 0.3144362270832062\n",
      "Training loss for batch 3888 : 0.11438077688217163\n",
      "Training loss for batch 3889 : 0.4616737961769104\n",
      "Training loss for batch 3890 : 0.11453010141849518\n",
      "Training loss for batch 3891 : 0.008809327147901058\n",
      "Training loss for batch 3892 : 0.05546332523226738\n",
      "Training loss for batch 3893 : 0.5357818603515625\n",
      "Training loss for batch 3894 : 0.29647961258888245\n",
      "Training loss for batch 3895 : 0.19611991941928864\n",
      "Training loss for batch 3896 : 0.26343825459480286\n",
      "Training loss for batch 3897 : 0.1764603555202484\n",
      "Training loss for batch 3898 : 0.4098452925682068\n",
      "Training loss for batch 3899 : 0.3857736885547638\n",
      "Training loss for batch 3900 : 0.054140202701091766\n",
      "Training loss for batch 3901 : 0.16904786229133606\n",
      "Training loss for batch 3902 : 0.22102801501750946\n",
      "Training loss for batch 3903 : 0.11662592738866806\n",
      "Training loss for batch 3904 : 0.08363958448171616\n",
      "Training loss for batch 3905 : 0.05735761299729347\n",
      "Training loss for batch 3906 : 0.19051511585712433\n",
      "Training loss for batch 3907 : 0.45595040917396545\n",
      "Training loss for batch 3908 : 0.26205629110336304\n",
      "Training loss for batch 3909 : 0.17046038806438446\n",
      "Training loss for batch 3910 : 0.38575929403305054\n",
      "Training loss for batch 3911 : 0.5476532578468323\n",
      "Training loss for batch 3912 : 0.0110801812261343\n",
      "Training loss for batch 3913 : 0.22362326085567474\n",
      "Training loss for batch 3914 : 0.16433072090148926\n",
      "Training loss for batch 3915 : 0.3677647113800049\n",
      "Training loss for batch 3916 : 0.43267059326171875\n",
      "Training loss for batch 3917 : 0.09522789716720581\n",
      "Training loss for batch 3918 : 0.5215552449226379\n",
      "Training loss for batch 3919 : 0.3226405382156372\n",
      "Training loss for batch 3920 : 0.07030246406793594\n",
      "Training loss for batch 3921 : 0.05213029682636261\n",
      "Training loss for batch 3922 : 0.05330492928624153\n",
      "Training loss for batch 3923 : 0.26932111382484436\n",
      "Training loss for batch 3924 : 0.2532156705856323\n",
      "Training loss for batch 3925 : 0.23226596415042877\n",
      "Training loss for batch 3926 : 0.12429096549749374\n",
      "Training loss for batch 3927 : 0.5159979462623596\n",
      "Training loss for batch 3928 : 0.2547059953212738\n",
      "Training loss for batch 3929 : 0.12928661704063416\n",
      "Training loss for batch 3930 : 0.24362726509571075\n",
      "Training loss for batch 3931 : 0.001965393777936697\n",
      "Training loss for batch 3932 : 0.264161080121994\n",
      "Training loss for batch 3933 : 0.2902490496635437\n",
      "Training loss for batch 3934 : 0.0\n",
      "Training loss for batch 3935 : 0.030663099139928818\n",
      "Training loss for batch 3936 : -3.444857429713011e-05\n",
      "Training loss for batch 3937 : 0.16395686566829681\n",
      "Training loss for batch 3938 : 0.18774332106113434\n",
      "Training loss for batch 3939 : 0.30913981795310974\n",
      "Training loss for batch 3940 : 0.21639978885650635\n",
      "Training loss for batch 3941 : 0.267956018447876\n",
      "Training loss for batch 3942 : 0.05871027708053589\n",
      "Training loss for batch 3943 : 0.1444794088602066\n",
      "Training loss for batch 3944 : 0.03515816479921341\n",
      "Training loss for batch 3945 : 0.17280720174312592\n",
      "Training loss for batch 3946 : 0.0\n",
      "Training loss for batch 3947 : 0.19015946984291077\n",
      "Training loss for batch 3948 : 0.03218376636505127\n",
      "Training loss for batch 3949 : 0.30937328934669495\n",
      "Training loss for batch 3950 : 0.2128235548734665\n",
      "Training loss for batch 3951 : 0.40479806065559387\n",
      "Training loss for batch 3952 : 0.12052896618843079\n",
      "Training loss for batch 3953 : 0.026590028777718544\n",
      "Training loss for batch 3954 : 0.8146588206291199\n",
      "Training loss for batch 3955 : 0.09475342184305191\n",
      "Training loss for batch 3956 : 0.07728495448827744\n",
      "Training loss for batch 3957 : 0.2374110370874405\n",
      "Training loss for batch 3958 : 0.10616346448659897\n",
      "Training loss for batch 3959 : 0.27473652362823486\n",
      "Training loss for batch 3960 : 0.46807757019996643\n",
      "Training loss for batch 3961 : 0.026930009946227074\n",
      "Training loss for batch 3962 : 0.11350846290588379\n",
      "Training loss for batch 3963 : 0.34278222918510437\n",
      "Training loss for batch 3964 : 0.06282485276460648\n",
      "Training loss for batch 3965 : 0.2000618875026703\n",
      "Training loss for batch 3966 : 0.06338086724281311\n",
      "Training loss for batch 3967 : 0.17361684143543243\n",
      "Training loss for batch 3968 : 0.22933343052864075\n",
      "Training loss for batch 3969 : 0.10313605517148972\n",
      "Training loss for batch 3970 : 0.039587728679180145\n",
      "Training loss for batch 3971 : 0.15542541444301605\n",
      "Training loss for batch 3972 : 0.07402994483709335\n",
      "Training loss for batch 3973 : 0.1982661634683609\n",
      "Training loss for batch 3974 : 0.226124569773674\n",
      "Training loss for batch 3975 : 0.005054850596934557\n",
      "Training loss for batch 3976 : 0.04235947132110596\n",
      "Training loss for batch 3977 : 0.32879745960235596\n",
      "Training loss for batch 3978 : 0.2566557228565216\n",
      "Training loss for batch 3979 : 0.33015763759613037\n",
      "Training loss for batch 3980 : 0.022090129554271698\n",
      "Training loss for batch 3981 : 0.17056721448898315\n",
      "Training loss for batch 3982 : 0.2584950923919678\n",
      "Training loss for batch 3983 : 0.3527524173259735\n",
      "Training loss for batch 3984 : 0.07257786393165588\n",
      "Training loss for batch 3985 : 0.46942606568336487\n",
      "Training loss for batch 3986 : 0.09053559601306915\n",
      "Training loss for batch 3987 : 0.045796286314725876\n",
      "Training loss for batch 3988 : 0.08780673146247864\n",
      "Training loss for batch 3989 : 0.3572291135787964\n",
      "Training loss for batch 3990 : 0.04575983062386513\n",
      "Training loss for batch 3991 : 0.19469647109508514\n",
      "Training loss for batch 3992 : 0.18743208050727844\n",
      "Training loss for batch 3993 : 0.356073796749115\n",
      "Training loss for batch 3994 : 0.4903378486633301\n",
      "Training loss for batch 3995 : 0.3549206256866455\n",
      "Training loss for batch 3996 : 0.11391506344079971\n",
      "Training loss for batch 3997 : 0.4373122751712799\n",
      "Training loss for batch 3998 : 0.11815272271633148\n",
      "Training loss for batch 3999 : 0.014316042885184288\n",
      "Training loss for batch 4000 : 0.13911384344100952\n",
      "Training loss for batch 4001 : 0.13979296386241913\n",
      "Training loss for batch 4002 : 0.2512526214122772\n",
      "Training loss for batch 4003 : 0.24300043284893036\n",
      "Training loss for batch 4004 : 0.04126887023448944\n",
      "Training loss for batch 4005 : 0.11316502094268799\n",
      "Training loss for batch 4006 : 0.3452974557876587\n",
      "Training loss for batch 4007 : 0.04216821491718292\n",
      "Training loss for batch 4008 : 0.0701201856136322\n",
      "Training loss for batch 4009 : 0.07680101692676544\n",
      "Training loss for batch 4010 : 0.3694896996021271\n",
      "Training loss for batch 4011 : 0.05585019290447235\n",
      "Training loss for batch 4012 : 0.10830388218164444\n",
      "Training loss for batch 4013 : 0.1455513834953308\n",
      "Training loss for batch 4014 : 0.2216305285692215\n",
      "Training loss for batch 4015 : 0.4462428092956543\n",
      "Training loss for batch 4016 : 0.34236228466033936\n",
      "Training loss for batch 4017 : 0.2843702435493469\n",
      "Training loss for batch 4018 : 0.006521125324070454\n",
      "Training loss for batch 4019 : 0.27558884024620056\n",
      "Training loss for batch 4020 : 0.11642913520336151\n",
      "Training loss for batch 4021 : 0.16171929240226746\n",
      "Training loss for batch 4022 : 0.09501869231462479\n",
      "Training loss for batch 4023 : 0.440467894077301\n",
      "Training loss for batch 4024 : 0.34811633825302124\n",
      "Training loss for batch 4025 : 0.2268802672624588\n",
      "Training loss for batch 4026 : 0.0879315584897995\n",
      "Training loss for batch 4027 : 0.2753760516643524\n",
      "Training loss for batch 4028 : 0.09209489077329636\n",
      "Training loss for batch 4029 : 0.14948119223117828\n",
      "Training loss for batch 4030 : 0.008457744494080544\n",
      "Training loss for batch 4031 : 0.121701680123806\n",
      "Training loss for batch 4032 : 0.4083402752876282\n",
      "Training loss for batch 4033 : 0.4096940755844116\n",
      "Training loss for batch 4034 : 0.46439045667648315\n",
      "Training loss for batch 4035 : 0.2797718644142151\n",
      "Training loss for batch 4036 : 0.311264306306839\n",
      "Training loss for batch 4037 : 0.26278188824653625\n",
      "Training loss for batch 4038 : 0.17406632006168365\n",
      "Training loss for batch 4039 : 0.14620384573936462\n",
      "Training loss for batch 4040 : 0.2934620678424835\n",
      "Training loss for batch 4041 : 0.0003545555518940091\n",
      "Training loss for batch 4042 : 0.41998085379600525\n",
      "Training loss for batch 4043 : 0.12368229031562805\n",
      "Training loss for batch 4044 : 0.18657685816287994\n",
      "Training loss for batch 4045 : 0.2424030750989914\n",
      "Training loss for batch 4046 : 0.2538694739341736\n",
      "Training loss for batch 4047 : 0.20783530175685883\n",
      "Training loss for batch 4048 : -0.0007230238406918943\n",
      "Training loss for batch 4049 : 0.0635499432682991\n",
      "Training loss for batch 4050 : 0.3011596202850342\n",
      "Training loss for batch 4051 : 0.19243106245994568\n",
      "Training loss for batch 4052 : 0.04478895664215088\n",
      "Training loss for batch 4053 : 0.10254761576652527\n",
      "Training loss for batch 4054 : 0.002057064324617386\n",
      "Training loss for batch 4055 : 0.04040278121829033\n",
      "Training loss for batch 4056 : 0.10856818407773972\n",
      "Training loss for batch 4057 : 0.4474624693393707\n",
      "Training loss for batch 4058 : 0.13204804062843323\n",
      "Training loss for batch 4059 : 0.10945019870996475\n",
      "Training loss for batch 4060 : 0.13227413594722748\n",
      "Training loss for batch 4061 : 0.09625612944364548\n",
      "Training loss for batch 4062 : 0.00923621840775013\n",
      "Training loss for batch 4063 : 0.08006493002176285\n",
      "Training loss for batch 4064 : 0.2883111238479614\n",
      "Training loss for batch 4065 : 0.005296574905514717\n",
      "Training loss for batch 4066 : 0.29402151703834534\n",
      "Training loss for batch 4067 : 0.07936081290245056\n",
      "Training loss for batch 4068 : 0.22575753927230835\n",
      "Training loss for batch 4069 : 0.35939571261405945\n",
      "Training loss for batch 4070 : 0.04256230965256691\n",
      "Training loss for batch 4071 : 0.016677871346473694\n",
      "Training loss for batch 4072 : 0.2388448566198349\n",
      "Training loss for batch 4073 : 0.07631439715623856\n",
      "Training loss for batch 4074 : 0.0740630030632019\n",
      "Training loss for batch 4075 : 0.11823131144046783\n",
      "Training loss for batch 4076 : 0.07995882630348206\n",
      "Training loss for batch 4077 : 0.5508095026016235\n",
      "Training loss for batch 4078 : 0.016094785183668137\n",
      "Training loss for batch 4079 : 0.303077757358551\n",
      "Training loss for batch 4080 : 0.06944914907217026\n",
      "Training loss for batch 4081 : 0.23999598622322083\n",
      "Training loss for batch 4082 : 0.2840580344200134\n",
      "Training loss for batch 4083 : 0.10342849045991898\n",
      "Training loss for batch 4084 : 0.36419767141342163\n",
      "Training loss for batch 4085 : 0.2199043482542038\n",
      "Training loss for batch 4086 : 0.3706948757171631\n",
      "Training loss for batch 4087 : 0.260497123003006\n",
      "Training loss for batch 4088 : 0.005310195032507181\n",
      "Training loss for batch 4089 : 0.4136901795864105\n",
      "Training loss for batch 4090 : 0.10835212469100952\n",
      "Training loss for batch 4091 : 0.3158186376094818\n",
      "Training loss for batch 4092 : 0.23955297470092773\n",
      "Training loss for batch 4093 : 0.05284195393323898\n",
      "Training loss for batch 4094 : 0.03348737210035324\n",
      "Training loss for batch 4095 : 0.15880687534809113\n",
      "Training loss for batch 4096 : 0.006172428838908672\n",
      "Training loss for batch 4097 : 0.13170011341571808\n",
      "Training loss for batch 4098 : 0.011305243708193302\n",
      "Training loss for batch 4099 : 0.1687442660331726\n",
      "Training loss for batch 4100 : 0.37092339992523193\n",
      "Training loss for batch 4101 : 0.14815238118171692\n",
      "Training loss for batch 4102 : 0.4105140268802643\n",
      "Training loss for batch 4103 : 0.07384540140628815\n",
      "Training loss for batch 4104 : 0.17175619304180145\n",
      "Training loss for batch 4105 : 0.006911754608154297\n",
      "Training loss for batch 4106 : 0.09424787014722824\n",
      "Training loss for batch 4107 : 0.12720218300819397\n",
      "Training loss for batch 4108 : 0.1842382252216339\n",
      "Training loss for batch 4109 : 0.14264018833637238\n",
      "Training loss for batch 4110 : 0.4899466633796692\n",
      "Training loss for batch 4111 : 0.10389362275600433\n",
      "Training loss for batch 4112 : 0.3083043098449707\n",
      "Training loss for batch 4113 : 0.08521395176649094\n",
      "Training loss for batch 4114 : 0.07273207604885101\n",
      "Training loss for batch 4115 : 0.0906917154788971\n",
      "Training loss for batch 4116 : 0.07721707224845886\n",
      "Training loss for batch 4117 : 0.12558980286121368\n",
      "Training loss for batch 4118 : 0.04548308253288269\n",
      "Training loss for batch 4119 : 0.11533722281455994\n",
      "Training loss for batch 4120 : 0.025707267224788666\n",
      "Training loss for batch 4121 : 0.030275719240307808\n",
      "Training loss for batch 4122 : 0.05222056433558464\n",
      "Training loss for batch 4123 : 0.5466026663780212\n",
      "Training loss for batch 4124 : 0.12275265157222748\n",
      "Training loss for batch 4125 : 0.14275331795215607\n",
      "Training loss for batch 4126 : 0.09800180047750473\n",
      "Training loss for batch 4127 : 0.1553938239812851\n",
      "Training loss for batch 4128 : 0.24289987981319427\n",
      "Training loss for batch 4129 : 0.3850638270378113\n",
      "Training loss for batch 4130 : 0.3422669470310211\n",
      "Training loss for batch 4131 : 0.14245963096618652\n",
      "Training loss for batch 4132 : 0.06548337638378143\n",
      "Training loss for batch 4133 : 0.22662483155727386\n",
      "Training loss for batch 4134 : 0.028865326195955276\n",
      "Training loss for batch 4135 : 0.21018557250499725\n",
      "Training loss for batch 4136 : 0.3192483186721802\n",
      "Training loss for batch 4137 : 0.11045031994581223\n",
      "Training loss for batch 4138 : 0.27915218472480774\n",
      "Training loss for batch 4139 : 0.07255691289901733\n",
      "Training loss for batch 4140 : 0.1286526471376419\n",
      "Training loss for batch 4141 : 0.14915521442890167\n",
      "Training loss for batch 4142 : 0.06365546584129333\n",
      "Training loss for batch 4143 : 0.0068994322791695595\n",
      "Training loss for batch 4144 : 0.29071488976478577\n",
      "Training loss for batch 4145 : 0.4642457365989685\n",
      "Training loss for batch 4146 : 0.010976157151162624\n",
      "Training loss for batch 4147 : 0.08055570721626282\n",
      "Training loss for batch 4148 : 0.042364995926618576\n",
      "Training loss for batch 4149 : 0.0\n",
      "Training loss for batch 4150 : 0.20133431255817413\n",
      "Training loss for batch 4151 : 0.1566254049539566\n",
      "Training loss for batch 4152 : 0.09294676035642624\n",
      "Training loss for batch 4153 : 0.03358225151896477\n",
      "Training loss for batch 4154 : 0.18646332621574402\n",
      "Training loss for batch 4155 : 0.033258721232414246\n",
      "Training loss for batch 4156 : 0.1953120231628418\n",
      "Training loss for batch 4157 : 0.32352522015571594\n",
      "Training loss for batch 4158 : 0.08027581870555878\n",
      "Training loss for batch 4159 : 0.08045150339603424\n",
      "Training loss for batch 4160 : 0.24742965400218964\n",
      "Training loss for batch 4161 : 0.39169177412986755\n",
      "Training loss for batch 4162 : 0.25635114312171936\n",
      "Training loss for batch 4163 : 0.06666930764913559\n",
      "Training loss for batch 4164 : 0.011042392812669277\n",
      "Training loss for batch 4165 : -0.0005063678254373372\n",
      "Training loss for batch 4166 : 0.13038691878318787\n",
      "Training loss for batch 4167 : 0.42175284028053284\n",
      "Training loss for batch 4168 : 0.2519523501396179\n",
      "Training loss for batch 4169 : 0.15630118548870087\n",
      "Training loss for batch 4170 : 0.19974887371063232\n",
      "Training loss for batch 4171 : 0.015304028056561947\n",
      "Training loss for batch 4172 : 0.07386183738708496\n",
      "Training loss for batch 4173 : 0.10147857666015625\n",
      "Training loss for batch 4174 : 0.0\n",
      "Training loss for batch 4175 : 0.00528721883893013\n",
      "Training loss for batch 4176 : 0.2473747730255127\n",
      "Training loss for batch 4177 : 0.26782599091529846\n",
      "Training loss for batch 4178 : 0.20062589645385742\n",
      "Training loss for batch 4179 : 0.03400305658578873\n",
      "Training loss for batch 4180 : 0.0013456471497192979\n",
      "Training loss for batch 4181 : 0.2577304244041443\n",
      "Training loss for batch 4182 : 0.10458581894636154\n",
      "Training loss for batch 4183 : 0.1560554802417755\n",
      "Training loss for batch 4184 : 0.29687055945396423\n",
      "Training loss for batch 4185 : 0.1518602818250656\n",
      "Training loss for batch 4186 : 0.20187732577323914\n",
      "Training loss for batch 4187 : 0.015130494721233845\n",
      "Training loss for batch 4188 : 0.09142962098121643\n",
      "Training loss for batch 4189 : 0.3220651149749756\n",
      "Training loss for batch 4190 : 0.1229579746723175\n",
      "Training loss for batch 4191 : 0.32131022214889526\n",
      "Training loss for batch 4192 : 0.27922213077545166\n",
      "Training loss for batch 4193 : 0.04509979858994484\n",
      "Training loss for batch 4194 : 0.27469900250434875\n",
      "Training loss for batch 4195 : 0.3585880696773529\n",
      "Training loss for batch 4196 : 0.3349134624004364\n",
      "Training loss for batch 4197 : 0.16719950735569\n",
      "Training loss for batch 4198 : 0.3544120490550995\n",
      "Training loss for batch 4199 : 0.008283154107630253\n",
      "Training loss for batch 4200 : 0.25686362385749817\n",
      "Training loss for batch 4201 : 0.16007661819458008\n",
      "Training loss for batch 4202 : 0.5996604561805725\n",
      "Training loss for batch 4203 : 0.0831732302904129\n",
      "Training loss for batch 4204 : 0.05379719287157059\n",
      "Training loss for batch 4205 : 0.28420713543891907\n",
      "Training loss for batch 4206 : 0.11193566024303436\n",
      "Training loss for batch 4207 : 0.11326396465301514\n",
      "Training loss for batch 4208 : 0.23556503653526306\n",
      "Training loss for batch 4209 : 0.141819566488266\n",
      "Training loss for batch 4210 : 0.17010048031806946\n",
      "Training loss for batch 4211 : 0.09071469306945801\n",
      "Training loss for batch 4212 : 0.3949618637561798\n",
      "Training loss for batch 4213 : 0.10897330194711685\n",
      "Training loss for batch 4214 : 0.27465128898620605\n",
      "Training loss for batch 4215 : 0.35099783539772034\n",
      "Training loss for batch 4216 : 0.21838431060314178\n",
      "Training loss for batch 4217 : 0.020734300836920738\n",
      "Training loss for batch 4218 : 0.09082801640033722\n",
      "Training loss for batch 4219 : 0.22606445848941803\n",
      "Training loss for batch 4220 : 0.24814215302467346\n",
      "Training loss for batch 4221 : 0.30321455001831055\n",
      "Training loss for batch 4222 : 0.30998530983924866\n",
      "Training loss for batch 4223 : 0.3329908549785614\n",
      "Training loss for batch 4224 : 0.20920883119106293\n",
      "Training loss for batch 4225 : 0.20771658420562744\n",
      "Training loss for batch 4226 : 0.2386002391576767\n",
      "Training loss for batch 4227 : 0.25007107853889465\n",
      "Training loss for batch 4228 : 0.17867517471313477\n",
      "Training loss for batch 4229 : 0.08158864080905914\n",
      "Training loss for batch 4230 : 0.027910487726330757\n",
      "Training loss for batch 4231 : 0.19809725880622864\n",
      "Training loss for batch 4232 : 0.03736609220504761\n",
      "Training loss for batch 4233 : 0.10662107169628143\n",
      "Training loss for batch 4234 : 0.033289793878793716\n",
      "Training loss for batch 4235 : 0.3964211642742157\n",
      "Training loss for batch 4236 : 0.12060578167438507\n",
      "Training loss for batch 4237 : 0.16742970049381256\n",
      "Training loss for batch 4238 : 0.027523882687091827\n",
      "Training loss for batch 4239 : 0.4057961106300354\n",
      "Training loss for batch 4240 : 0.0694652870297432\n",
      "Training loss for batch 4241 : 0.41340306401252747\n",
      "Training loss for batch 4242 : 0.37251561880111694\n",
      "Training loss for batch 4243 : 0.14906780421733856\n",
      "Training loss for batch 4244 : 0.1813763678073883\n",
      "Training loss for batch 4245 : 0.3677135407924652\n",
      "Training loss for batch 4246 : 0.23146262764930725\n",
      "Training loss for batch 4247 : 0.42452383041381836\n",
      "Training loss for batch 4248 : 0.0484425313770771\n",
      "Training loss for batch 4249 : 0.17885586619377136\n",
      "Training loss for batch 4250 : 0.07265502214431763\n",
      "Training loss for batch 4251 : 0.2801172733306885\n",
      "Training loss for batch 4252 : 0.02620145119726658\n",
      "Training loss for batch 4253 : 0.09608752280473709\n",
      "Training loss for batch 4254 : 0.10802596062421799\n",
      "Training loss for batch 4255 : 0.03032444790005684\n",
      "Training loss for batch 4256 : 0.14153656363487244\n",
      "Training loss for batch 4257 : 0.1370992511510849\n",
      "Training loss for batch 4258 : 0.5287858843803406\n",
      "Training loss for batch 4259 : 0.06851445138454437\n",
      "Training loss for batch 4260 : 0.1871960610151291\n",
      "Training loss for batch 4261 : 0.023463666439056396\n",
      "Training loss for batch 4262 : 0.08652909100055695\n",
      "Training loss for batch 4263 : 0.2252308428287506\n",
      "Training loss for batch 4264 : 0.1432139128446579\n",
      "Training loss for batch 4265 : 0.5309145450592041\n",
      "Training loss for batch 4266 : 0.08251524716615677\n",
      "Training loss for batch 4267 : 0.21661148965358734\n",
      "Training loss for batch 4268 : 0.5103937387466431\n",
      "Training loss for batch 4269 : 0.06143467500805855\n",
      "Training loss for batch 4270 : 0.24340438842773438\n",
      "Training loss for batch 4271 : 0.14266285300254822\n",
      "Training loss for batch 4272 : 0.1525951474905014\n",
      "Training loss for batch 4273 : 0.2426285445690155\n",
      "Training loss for batch 4274 : 0.21442826092243195\n",
      "Training loss for batch 4275 : 0.31429049372673035\n",
      "Training loss for batch 4276 : 0.16666890680789948\n",
      "Training loss for batch 4277 : 0.14207836985588074\n",
      "Training loss for batch 4278 : 0.1070941686630249\n",
      "Training loss for batch 4279 : 0.21537558734416962\n",
      "Training loss for batch 4280 : 0.3316953182220459\n",
      "Training loss for batch 4281 : 0.09167172014713287\n",
      "Training loss for batch 4282 : 0.08747904747724533\n",
      "Training loss for batch 4283 : 0.302633672952652\n",
      "Training loss for batch 4284 : 0.05329451709985733\n",
      "Training loss for batch 4285 : 0.2151910960674286\n",
      "Training loss for batch 4286 : 0.33178582787513733\n",
      "Training loss for batch 4287 : 0.27108830213546753\n",
      "Training loss for batch 4288 : 0.007075881585478783\n",
      "Training loss for batch 4289 : 0.050043873488903046\n",
      "Training loss for batch 4290 : 0.3215687572956085\n",
      "Training loss for batch 4291 : 0.1974484622478485\n",
      "Training loss for batch 4292 : 0.32562994956970215\n",
      "Training loss for batch 4293 : 0.28064975142478943\n",
      "Training loss for batch 4294 : 0.2764390707015991\n",
      "Training loss for batch 4295 : 0.4431445896625519\n",
      "Training loss for batch 4296 : 0.17204870283603668\n",
      "Training loss for batch 4297 : 0.4832466244697571\n",
      "Training loss for batch 4298 : 0.18245680630207062\n",
      "Training loss for batch 4299 : 0.025933707132935524\n",
      "Training loss for batch 4300 : 0.07905050367116928\n",
      "Training loss for batch 4301 : 0.24985525012016296\n",
      "Training loss for batch 4302 : 0.15375806391239166\n",
      "Training loss for batch 4303 : 0.22525930404663086\n",
      "Training loss for batch 4304 : 0.4161788821220398\n",
      "Training loss for batch 4305 : 0.08474215865135193\n",
      "Training loss for batch 4306 : 0.1469624936580658\n",
      "Training loss for batch 4307 : 0.07886959612369537\n",
      "Training loss for batch 4308 : 0.1823895424604416\n",
      "Training loss for batch 4309 : 0.5315039157867432\n",
      "Training loss for batch 4310 : 0.6815523505210876\n",
      "Training loss for batch 4311 : 0.0021465024910867214\n",
      "Training loss for batch 4312 : 0.3058633804321289\n",
      "Training loss for batch 4313 : 0.4895037114620209\n",
      "Training loss for batch 4314 : 0.06023513525724411\n",
      "Training loss for batch 4315 : 0.40732255578041077\n",
      "Training loss for batch 4316 : 0.10844533145427704\n",
      "Training loss for batch 4317 : 0.05910045653581619\n",
      "Training loss for batch 4318 : 0.34607765078544617\n",
      "Training loss for batch 4319 : 0.2736667990684509\n",
      "Training loss for batch 4320 : 0.06114865466952324\n",
      "Training loss for batch 4321 : 0.213098406791687\n",
      "Training loss for batch 4322 : 0.17596641182899475\n",
      "Training loss for batch 4323 : 0.05244523286819458\n",
      "Training loss for batch 4324 : 0.2388114184141159\n",
      "Training loss for batch 4325 : 0.3080345094203949\n",
      "Training loss for batch 4326 : 0.2087537795305252\n",
      "Training loss for batch 4327 : 0.03673456981778145\n",
      "Training loss for batch 4328 : 0.17387011647224426\n",
      "Training loss for batch 4329 : 0.3074003756046295\n",
      "Training loss for batch 4330 : 0.05127917230129242\n",
      "Training loss for batch 4331 : 0.019779201596975327\n",
      "Training loss for batch 4332 : 0.29116225242614746\n",
      "Training loss for batch 4333 : 0.17429275810718536\n",
      "Training loss for batch 4334 : 0.5052091479301453\n",
      "Training loss for batch 4335 : 0.02884129248559475\n",
      "Training loss for batch 4336 : 0.03238465636968613\n",
      "Training loss for batch 4337 : 0.22772729396820068\n",
      "Training loss for batch 4338 : 0.1582057774066925\n",
      "Training loss for batch 4339 : 0.06444564461708069\n",
      "Training loss for batch 4340 : 0.09496705234050751\n",
      "Training loss for batch 4341 : 0.43396392464637756\n",
      "Training loss for batch 4342 : 0.24962833523750305\n",
      "Training loss for batch 4343 : 0.14945124089717865\n",
      "Training loss for batch 4344 : 0.27513614296913147\n",
      "Training loss for batch 4345 : 0.19719573855400085\n",
      "Training loss for batch 4346 : 0.08684182167053223\n",
      "Training loss for batch 4347 : 0.21908842027187347\n",
      "Training loss for batch 4348 : 0.19531935453414917\n",
      "Training loss for batch 4349 : 0.13963431119918823\n",
      "Training loss for batch 4350 : 0.08188604563474655\n",
      "Training loss for batch 4351 : 0.11327776312828064\n",
      "Training loss for batch 4352 : 0.17094698548316956\n",
      "Training loss for batch 4353 : 0.0002847477444447577\n",
      "Training loss for batch 4354 : 0.3648809790611267\n",
      "Training loss for batch 4355 : 0.2013595849275589\n",
      "Training loss for batch 4356 : 0.2717370092868805\n",
      "Training loss for batch 4357 : 0.18497733771800995\n",
      "Training loss for batch 4358 : 0.06376966089010239\n",
      "Training loss for batch 4359 : 0.06190171092748642\n",
      "Training loss for batch 4360 : 0.4849734306335449\n",
      "Training loss for batch 4361 : 0.17063675820827484\n",
      "Training loss for batch 4362 : 0.11452388763427734\n",
      "Training loss for batch 4363 : 0.007141421549022198\n",
      "Training loss for batch 4364 : 0.06613381206989288\n",
      "Training loss for batch 4365 : 0.25155600905418396\n",
      "Training loss for batch 4366 : 0.10988331586122513\n",
      "Training loss for batch 4367 : 0.0596480555832386\n",
      "Training loss for batch 4368 : 0.09828732162714005\n",
      "Training loss for batch 4369 : 0.5534458160400391\n",
      "Training loss for batch 4370 : 0.1345239281654358\n",
      "Training loss for batch 4371 : 0.3278712034225464\n",
      "Training loss for batch 4372 : 0.21223583817481995\n",
      "Training loss for batch 4373 : 0.05212026461958885\n",
      "Training loss for batch 4374 : 0.039219919592142105\n",
      "Training loss for batch 4375 : 0.16600675880908966\n",
      "Training loss for batch 4376 : 0.3804459273815155\n",
      "Training loss for batch 4377 : 0.2061995565891266\n",
      "Training loss for batch 4378 : 0.12236379086971283\n",
      "Training loss for batch 4379 : 0.000801384449005127\n",
      "Training loss for batch 4380 : 0.16818557679653168\n",
      "Training loss for batch 4381 : 0.09076584875583649\n",
      "Training loss for batch 4382 : 0.006545156240463257\n",
      "Training loss for batch 4383 : 0.020464349538087845\n",
      "Training loss for batch 4384 : 0.2668651342391968\n",
      "Training loss for batch 4385 : 0.06396555155515671\n",
      "Training loss for batch 4386 : 0.361386775970459\n",
      "Training loss for batch 4387 : 0.10728688538074493\n",
      "Training loss for batch 4388 : 0.4221534729003906\n",
      "Training loss for batch 4389 : 0.2103586196899414\n",
      "Training loss for batch 4390 : 0.05161815136671066\n",
      "Training loss for batch 4391 : 0.0\n",
      "Training loss for batch 4392 : 0.0025516252499073744\n",
      "Training loss for batch 4393 : 0.2659251391887665\n",
      "Training loss for batch 4394 : 0.5954834818840027\n",
      "Training loss for batch 4395 : 0.24203623831272125\n",
      "Training loss for batch 4396 : 0.22380326688289642\n",
      "Training loss for batch 4397 : 0.4472886621952057\n",
      "Training loss for batch 4398 : 0.41016948223114014\n",
      "Training loss for batch 4399 : 0.09475703537464142\n",
      "Training loss for batch 4400 : 0.02636605314910412\n",
      "Training loss for batch 4401 : 0.3032447099685669\n",
      "Training loss for batch 4402 : 0.0\n",
      "Training loss for batch 4403 : 0.40247586369514465\n",
      "Training loss for batch 4404 : 0.0319378487765789\n",
      "Training loss for batch 4405 : 0.18887114524841309\n",
      "Training loss for batch 4406 : 0.32692837715148926\n",
      "Training loss for batch 4407 : 0.14609326422214508\n",
      "Training loss for batch 4408 : 0.5381289124488831\n",
      "Training loss for batch 4409 : 0.2532775402069092\n",
      "Training loss for batch 4410 : 0.0653473436832428\n",
      "Training loss for batch 4411 : 0.04836563766002655\n",
      "Training loss for batch 4412 : 0.05577867850661278\n",
      "Training loss for batch 4413 : 0.17281289398670197\n",
      "Training loss for batch 4414 : 0.05942400172352791\n",
      "Training loss for batch 4415 : 0.42074185609817505\n",
      "Training loss for batch 4416 : 0.03593972697854042\n",
      "Training loss for batch 4417 : 0.12649135291576385\n",
      "Training loss for batch 4418 : 0.1480107307434082\n",
      "Training loss for batch 4419 : 0.027945568785071373\n",
      "Training loss for batch 4420 : 0.2073373645544052\n",
      "Training loss for batch 4421 : 0.0\n",
      "Training loss for batch 4422 : 0.15104804933071136\n",
      "Training loss for batch 4423 : 0.39535027742385864\n",
      "Training loss for batch 4424 : 0.039176277816295624\n",
      "Training loss for batch 4425 : 0.07610100507736206\n",
      "Training loss for batch 4426 : 0.10703892260789871\n",
      "Training loss for batch 4427 : 0.08188080787658691\n",
      "Training loss for batch 4428 : 0.06214204803109169\n",
      "Training loss for batch 4429 : 0.43485426902770996\n",
      "Training loss for batch 4430 : 0.10776105523109436\n",
      "Training loss for batch 4431 : 0.3721497356891632\n",
      "Training loss for batch 4432 : 0.011250395327806473\n",
      "Training loss for batch 4433 : 0.08092834055423737\n",
      "Training loss for batch 4434 : 0.18591301143169403\n",
      "Training loss for batch 4435 : 0.1271410435438156\n",
      "Training loss for batch 4436 : 0.2549085319042206\n",
      "Training loss for batch 4437 : 0.10710068047046661\n",
      "Training loss for batch 4438 : 0.13854528963565826\n",
      "Training loss for batch 4439 : 0.41165265440940857\n",
      "Training loss for batch 4440 : 0.23685790598392487\n",
      "Training loss for batch 4441 : 0.07866572588682175\n",
      "Training loss for batch 4442 : 0.5676823854446411\n",
      "Training loss for batch 4443 : 0.029136400669813156\n",
      "Training loss for batch 4444 : 0.13446034491062164\n",
      "Training loss for batch 4445 : 0.22919778525829315\n",
      "Training loss for batch 4446 : 0.0\n",
      "Training loss for batch 4447 : 0.08361934870481491\n",
      "Training loss for batch 4448 : 0.3072143793106079\n",
      "Training loss for batch 4449 : -4.3479732994455844e-05\n",
      "Training loss for batch 4450 : 0.11239766329526901\n",
      "Training loss for batch 4451 : 0.03295743465423584\n",
      "Training loss for batch 4452 : 0.4491194784641266\n",
      "Training loss for batch 4453 : 0.35564935207366943\n",
      "Training loss for batch 4454 : 0.4559207856655121\n",
      "Training loss for batch 4455 : 0.039338018745183945\n",
      "Training loss for batch 4456 : 0.3881830871105194\n",
      "Training loss for batch 4457 : 0.24159246683120728\n",
      "Training loss for batch 4458 : 0.17627274990081787\n",
      "Training loss for batch 4459 : 0.12578915059566498\n",
      "Training loss for batch 4460 : 0.34796473383903503\n",
      "Training loss for batch 4461 : 0.03894587606191635\n",
      "Training loss for batch 4462 : 0.10034874826669693\n",
      "Training loss for batch 4463 : 0.19377557933330536\n",
      "Training loss for batch 4464 : 0.3537346124649048\n",
      "Training loss for batch 4465 : 0.04261671379208565\n",
      "Training loss for batch 4466 : 0.11181528866291046\n",
      "Training loss for batch 4467 : 0.045286037027835846\n",
      "Training loss for batch 4468 : 0.10259406268596649\n",
      "Training loss for batch 4469 : 0.3406177759170532\n",
      "Training loss for batch 4470 : 0.0\n",
      "Training loss for batch 4471 : 0.2824684977531433\n",
      "Training loss for batch 4472 : 0.05972326546907425\n",
      "Training loss for batch 4473 : 0.16990453004837036\n",
      "Training loss for batch 4474 : 0.08299089968204498\n",
      "Training loss for batch 4475 : 0.13772809505462646\n",
      "Training loss for batch 4476 : 0.1251009851694107\n",
      "Training loss for batch 4477 : 0.29610806703567505\n",
      "Training loss for batch 4478 : 0.14753910899162292\n",
      "Training loss for batch 4479 : 0.02678777649998665\n",
      "Training loss for batch 4480 : 0.17503300309181213\n",
      "Training loss for batch 4481 : 0.22731368243694305\n",
      "Training loss for batch 4482 : 0.34629711508750916\n",
      "Training loss for batch 4483 : 0.2698134779930115\n",
      "Training loss for batch 4484 : 0.13381299376487732\n",
      "Training loss for batch 4485 : 0.2404605895280838\n",
      "Training loss for batch 4486 : 0.10570148378610611\n",
      "Training loss for batch 4487 : 0.08244897425174713\n",
      "Training loss for batch 4488 : 0.31601595878601074\n",
      "Training loss for batch 4489 : 0.06652027368545532\n",
      "Training loss for batch 4490 : 0.09881383925676346\n",
      "Training loss for batch 4491 : 0.02235374040901661\n",
      "Training loss for batch 4492 : 0.23742015659809113\n",
      "Training loss for batch 4493 : 0.045564331114292145\n",
      "Training loss for batch 4494 : 0.08168298006057739\n",
      "Training loss for batch 4495 : 0.09966170787811279\n",
      "Training loss for batch 4496 : 0.02599300816655159\n",
      "Training loss for batch 4497 : 0.08885571360588074\n",
      "Training loss for batch 4498 : 0.09223807603120804\n",
      "Training loss for batch 4499 : 0.2146032601594925\n",
      "Training loss for batch 4500 : 0.11446820199489594\n",
      "Training loss for batch 4501 : 0.3426211178302765\n",
      "Training loss for batch 4502 : 0.27334272861480713\n",
      "Training loss for batch 4503 : 0.18229593336582184\n",
      "Training loss for batch 4504 : 0.2036895751953125\n",
      "Training loss for batch 4505 : 0.10180963575839996\n",
      "Training loss for batch 4506 : 0.1937887966632843\n",
      "Training loss for batch 4507 : 0.09855066239833832\n",
      "Training loss for batch 4508 : 0.3246736228466034\n",
      "Training loss for batch 4509 : 0.6430325508117676\n",
      "Training loss for batch 4510 : 0.0192781500518322\n",
      "Training loss for batch 4511 : 0.0\n",
      "Training loss for batch 4512 : 0.3058185577392578\n",
      "Training loss for batch 4513 : 0.03166232258081436\n",
      "Training loss for batch 4514 : 0.002616043435409665\n",
      "Training loss for batch 4515 : 0.03805394470691681\n",
      "Training loss for batch 4516 : 0.21819396317005157\n",
      "Training loss for batch 4517 : 0.13785986602306366\n",
      "Training loss for batch 4518 : 0.22749236226081848\n",
      "Training loss for batch 4519 : 0.22994863986968994\n",
      "Training loss for batch 4520 : 0.17324374616146088\n",
      "Training loss for batch 4521 : 0.1657988280057907\n",
      "Training loss for batch 4522 : 0.23110820353031158\n",
      "Training loss for batch 4523 : 0.1627236008644104\n",
      "Training loss for batch 4524 : 0.2882433533668518\n",
      "Training loss for batch 4525 : 0.06349270045757294\n",
      "Training loss for batch 4526 : 0.4499484896659851\n",
      "Training loss for batch 4527 : 0.550533652305603\n",
      "Training loss for batch 4528 : 0.08006422966718674\n",
      "Training loss for batch 4529 : 0.12070357799530029\n",
      "Training loss for batch 4530 : 0.04650697112083435\n",
      "Training loss for batch 4531 : 0.4757445454597473\n",
      "Training loss for batch 4532 : 0.1330622285604477\n",
      "Training loss for batch 4533 : 0.15560489892959595\n",
      "Training loss for batch 4534 : 0.139565110206604\n",
      "Training loss for batch 4535 : 0.09569462388753891\n",
      "Training loss for batch 4536 : 0.28982728719711304\n",
      "Training loss for batch 4537 : 0.0208335779607296\n",
      "Training loss for batch 4538 : 0.26249396800994873\n",
      "Training loss for batch 4539 : 0.42336833477020264\n",
      "Training loss for batch 4540 : 0.1396743357181549\n",
      "Training loss for batch 4541 : 0.10581277310848236\n",
      "Training loss for batch 4542 : 0.04989219456911087\n",
      "Training loss for batch 4543 : 0.5566328763961792\n",
      "Training loss for batch 4544 : 0.06341318041086197\n",
      "Training loss for batch 4545 : 0.24267733097076416\n",
      "Training loss for batch 4546 : 0.38120216131210327\n",
      "Training loss for batch 4547 : 0.3452674448490143\n",
      "Training loss for batch 4548 : 0.20990855991840363\n",
      "Training loss for batch 4549 : 0.0014312290586531162\n",
      "Training loss for batch 4550 : 0.11724402010440826\n",
      "Training loss for batch 4551 : 0.2840428352355957\n",
      "Training loss for batch 4552 : 0.0\n",
      "Training loss for batch 4553 : 0.29204872250556946\n",
      "Training loss for batch 4554 : 0.7525102496147156\n",
      "Training loss for batch 4555 : 0.08270931988954544\n",
      "Training loss for batch 4556 : 0.3358718752861023\n",
      "Training loss for batch 4557 : 0.26415178179740906\n",
      "Training loss for batch 4558 : 0.0018427869072183967\n",
      "Training loss for batch 4559 : 0.14151862263679504\n",
      "Training loss for batch 4560 : 0.14513683319091797\n",
      "Training loss for batch 4561 : 0.165257066488266\n",
      "Training loss for batch 4562 : 0.11572873592376709\n",
      "Training loss for batch 4563 : 0.06705980002880096\n",
      "Training loss for batch 4564 : 0.14752228558063507\n",
      "Training loss for batch 4565 : 0.14663198590278625\n",
      "Training loss for batch 4566 : 0.3375566005706787\n",
      "Training loss for batch 4567 : 0.22731944918632507\n",
      "Training loss for batch 4568 : 0.22924412786960602\n",
      "Training loss for batch 4569 : 0.03894360736012459\n",
      "Training loss for batch 4570 : 0.09335050731897354\n",
      "Training loss for batch 4571 : 0.2816379964351654\n",
      "Training loss for batch 4572 : 0.664273202419281\n",
      "Training loss for batch 4573 : 0.1075110137462616\n",
      "Training loss for batch 4574 : 0.33870941400527954\n",
      "Training loss for batch 4575 : 0.08547894656658173\n",
      "Training loss for batch 4576 : 0.08207304030656815\n",
      "Training loss for batch 4577 : 0.3023495674133301\n",
      "Training loss for batch 4578 : 0.3964086174964905\n",
      "Training loss for batch 4579 : 0.40035194158554077\n",
      "Training loss for batch 4580 : 0.20850317180156708\n",
      "Training loss for batch 4581 : 0.2541653513908386\n",
      "Training loss for batch 4582 : 0.11695270985364914\n",
      "Training loss for batch 4583 : 0.28000974655151367\n",
      "Training loss for batch 4584 : 0.019061526283621788\n",
      "Training loss for batch 4585 : 0.037204593420028687\n",
      "Training loss for batch 4586 : 0.6334505081176758\n",
      "Training loss for batch 4587 : 0.12103639543056488\n",
      "Training loss for batch 4588 : 0.23026591539382935\n",
      "Training loss for batch 4589 : 0.14566779136657715\n",
      "Training loss for batch 4590 : 0.48631060123443604\n",
      "Training loss for batch 4591 : 0.021117424592375755\n",
      "Training loss for batch 4592 : 0.0500696562230587\n",
      "Training loss for batch 4593 : 0.21602563560009003\n",
      "Training loss for batch 4594 : 0.3742392659187317\n",
      "Training loss for batch 4595 : 0.21153484284877777\n",
      "Training loss for batch 4596 : 0.33566969633102417\n",
      "Training loss for batch 4597 : 0.3972183167934418\n",
      "Training loss for batch 4598 : 0.20480820536613464\n",
      "Training loss for batch 4599 : 0.3006787896156311\n",
      "Training loss for batch 4600 : 0.22397130727767944\n",
      "Training loss for batch 4601 : 0.13980117440223694\n",
      "Training loss for batch 4602 : 0.3120002746582031\n",
      "Training loss for batch 4603 : 0.2758404016494751\n",
      "Training loss for batch 4604 : 0.06736597418785095\n",
      "Training loss for batch 4605 : 0.10576322674751282\n",
      "Training loss for batch 4606 : 0.27695977687835693\n",
      "Training loss for batch 4607 : 0.21985624730587006\n",
      "Training loss for batch 4608 : 0.12160146981477737\n",
      "Training loss for batch 4609 : 0.31562554836273193\n",
      "Training loss for batch 4610 : 0.03441140055656433\n",
      "Training loss for batch 4611 : 0.08077798783779144\n",
      "Training loss for batch 4612 : 0.35337647795677185\n",
      "Training loss for batch 4613 : 0.16481220722198486\n",
      "Training loss for batch 4614 : 0.07292395830154419\n",
      "Training loss for batch 4615 : 0.14432834088802338\n",
      "Training loss for batch 4616 : 0.0568573884665966\n",
      "Training loss for batch 4617 : 0.08076895028352737\n",
      "Training loss for batch 4618 : 0.05249454826116562\n",
      "Training loss for batch 4619 : 0.464730441570282\n",
      "Training loss for batch 4620 : 0.09668289870023727\n",
      "Training loss for batch 4621 : 0.06717733293771744\n",
      "Training loss for batch 4622 : 0.16681692004203796\n",
      "Training loss for batch 4623 : 0.0838862881064415\n",
      "Training loss for batch 4624 : 0.03499068692326546\n",
      "Training loss for batch 4625 : 0.21094796061515808\n",
      "Training loss for batch 4626 : 0.051845695823431015\n",
      "Training loss for batch 4627 : 0.2918383777141571\n",
      "Training loss for batch 4628 : 0.048968445509672165\n",
      "Training loss for batch 4629 : 0.05627080425620079\n",
      "Training loss for batch 4630 : 0.0194215290248394\n",
      "Training loss for batch 4631 : 0.06156177818775177\n",
      "Training loss for batch 4632 : 0.041595593094825745\n",
      "Training loss for batch 4633 : 0.3770660161972046\n",
      "Training loss for batch 4634 : 0.2004498988389969\n",
      "Training loss for batch 4635 : 0.06887313723564148\n",
      "Training loss for batch 4636 : 0.17992250621318817\n",
      "Training loss for batch 4637 : 0.5500260591506958\n",
      "Training loss for batch 4638 : 0.13163012266159058\n",
      "Training loss for batch 4639 : 0.07584288716316223\n",
      "Training loss for batch 4640 : 0.5710461735725403\n",
      "Training loss for batch 4641 : 0.04936967045068741\n",
      "Training loss for batch 4642 : 0.396293580532074\n",
      "Training loss for batch 4643 : 0.20358587801456451\n",
      "Training loss for batch 4644 : 0.16176484525203705\n",
      "Training loss for batch 4645 : 0.250545859336853\n",
      "Training loss for batch 4646 : 0.05713995546102524\n",
      "Training loss for batch 4647 : 0.020405631512403488\n",
      "Training loss for batch 4648 : 0.18962113559246063\n",
      "Training loss for batch 4649 : 0.1660856455564499\n",
      "Training loss for batch 4650 : 0.09719914197921753\n",
      "Training loss for batch 4651 : 0.1619003862142563\n",
      "Training loss for batch 4652 : 0.07727716118097305\n",
      "Training loss for batch 4653 : 0.0774441808462143\n",
      "Training loss for batch 4654 : 0.16026222705841064\n",
      "Training loss for batch 4655 : 0.06716448813676834\n",
      "Training loss for batch 4656 : 0.33432433009147644\n",
      "Training loss for batch 4657 : 0.13313798606395721\n",
      "Training loss for batch 4658 : 0.22573797404766083\n",
      "Training loss for batch 4659 : 0.2702154517173767\n",
      "Training loss for batch 4660 : 0.17520672082901\n",
      "Training loss for batch 4661 : 0.16287969052791595\n",
      "Training loss for batch 4662 : 0.10190800577402115\n",
      "Training loss for batch 4663 : 0.1847083568572998\n",
      "Training loss for batch 4664 : 0.3013623058795929\n",
      "Training loss for batch 4665 : 0.5504328608512878\n",
      "Training loss for batch 4666 : 0.12328243255615234\n",
      "Training loss for batch 4667 : 0.10318925976753235\n",
      "Training loss for batch 4668 : 0.052778210490942\n",
      "Training loss for batch 4669 : 0.047853514552116394\n",
      "Training loss for batch 4670 : 0.09336801618337631\n",
      "Training loss for batch 4671 : 0.024429747834801674\n",
      "Training loss for batch 4672 : 0.04342232644557953\n",
      "Training loss for batch 4673 : 0.12859146296977997\n",
      "Training loss for batch 4674 : 0.5156140923500061\n",
      "Training loss for batch 4675 : 0.036138854920864105\n",
      "Training loss for batch 4676 : 0.06726886332035065\n",
      "Training loss for batch 4677 : 0.34007692337036133\n",
      "Training loss for batch 4678 : 0.20417790114879608\n",
      "Training loss for batch 4679 : 0.06682943552732468\n",
      "Training loss for batch 4680 : 0.13901732861995697\n",
      "Training loss for batch 4681 : 0.30367669463157654\n",
      "Training loss for batch 4682 : 0.23238860070705414\n",
      "Training loss for batch 4683 : 0.14994467794895172\n",
      "Training loss for batch 4684 : 0.04350548982620239\n",
      "Training loss for batch 4685 : 0.10157070308923721\n",
      "Training loss for batch 4686 : 0.031037768349051476\n",
      "Training loss for batch 4687 : 0.34652793407440186\n",
      "Training loss for batch 4688 : 0.13812384009361267\n",
      "Training loss for batch 4689 : 0.3078809678554535\n",
      "Training loss for batch 4690 : 0.2671707570552826\n",
      "Training loss for batch 4691 : 0.1460714489221573\n",
      "Training loss for batch 4692 : 0.12605926394462585\n",
      "Training loss for batch 4693 : 0.036780066788196564\n",
      "Training loss for batch 4694 : 0.07972396910190582\n",
      "Training loss for batch 4695 : 0.07274270057678223\n",
      "Training loss for batch 4696 : 0.06432482600212097\n",
      "Training loss for batch 4697 : 0.17740443348884583\n",
      "Training loss for batch 4698 : 0.07282404601573944\n",
      "Training loss for batch 4699 : 0.43251821398735046\n",
      "Training loss for batch 4700 : 0.11250157654285431\n",
      "Training loss for batch 4701 : 0.22948412597179413\n",
      "Training loss for batch 4702 : 0.11940092593431473\n",
      "Training loss for batch 4703 : 0.18502607941627502\n",
      "Training loss for batch 4704 : 0.07427376508712769\n",
      "Training loss for batch 4705 : 0.6086999177932739\n",
      "Training loss for batch 4706 : 0.07203880697488785\n",
      "Training loss for batch 4707 : 0.06723377853631973\n",
      "Training loss for batch 4708 : 0.0013806422939524055\n",
      "Training loss for batch 4709 : 0.21897384524345398\n",
      "Training loss for batch 4710 : 0.32780492305755615\n",
      "Training loss for batch 4711 : 0.033527035266160965\n",
      "Training loss for batch 4712 : 0.12382822483778\n",
      "Training loss for batch 4713 : 0.4391668438911438\n",
      "Training loss for batch 4714 : 0.454123854637146\n",
      "Training loss for batch 4715 : 0.06809407472610474\n",
      "Training loss for batch 4716 : 0.17455846071243286\n",
      "Training loss for batch 4717 : 0.4274618625640869\n",
      "Training loss for batch 4718 : 0.22748920321464539\n",
      "Training loss for batch 4719 : 0.26558810472488403\n",
      "Training loss for batch 4720 : 0.11925039440393448\n",
      "Training loss for batch 4721 : 0.19917941093444824\n",
      "Training loss for batch 4722 : 0.15269483625888824\n",
      "Training loss for batch 4723 : 0.2857707142829895\n",
      "Training loss for batch 4724 : 0.1542622148990631\n",
      "Training loss for batch 4725 : 0.0857902318239212\n",
      "Training loss for batch 4726 : 0.09711404144763947\n",
      "Training loss for batch 4727 : 0.13013450801372528\n",
      "Training loss for batch 4728 : 0.2713618576526642\n",
      "Training loss for batch 4729 : 0.2595568597316742\n",
      "Training loss for batch 4730 : 0.32011768221855164\n",
      "Training loss for batch 4731 : 0.005186812952160835\n",
      "Training loss for batch 4732 : 0.10577645152807236\n",
      "Training loss for batch 4733 : 0.04107820242643356\n",
      "Training loss for batch 4734 : 0.3919406831264496\n",
      "Training loss for batch 4735 : 0.12120500952005386\n",
      "Training loss for batch 4736 : 0.278604656457901\n",
      "Training loss for batch 4737 : 0.02736032009124756\n",
      "Training loss for batch 4738 : 0.016649506986141205\n",
      "Training loss for batch 4739 : 0.3490198254585266\n",
      "Training loss for batch 4740 : 0.20751307904720306\n",
      "Training loss for batch 4741 : 0.07955695688724518\n",
      "Training loss for batch 4742 : 0.6078150272369385\n",
      "Training loss for batch 4743 : 0.42480626702308655\n",
      "Training loss for batch 4744 : 0.4400876462459564\n",
      "Training loss for batch 4745 : 0.21076688170433044\n",
      "Training loss for batch 4746 : 0.0960695743560791\n",
      "Training loss for batch 4747 : 0.13162896037101746\n",
      "Training loss for batch 4748 : 0.44061607122421265\n",
      "Training loss for batch 4749 : 0.06481112539768219\n",
      "Training loss for batch 4750 : 0.07807046175003052\n",
      "Training loss for batch 4751 : 0.13285742700099945\n",
      "Training loss for batch 4752 : 0.1313202828168869\n",
      "Training loss for batch 4753 : 0.23608221113681793\n",
      "Training loss for batch 4754 : 0.20723748207092285\n",
      "Training loss for batch 4755 : 0.087696872651577\n",
      "Training loss for batch 4756 : 0.07457400858402252\n",
      "Training loss for batch 4757 : 0.21351826190948486\n",
      "Training loss for batch 4758 : 0.19412310421466827\n",
      "Training loss for batch 4759 : 0.17883040010929108\n",
      "Training loss for batch 4760 : 0.07548278570175171\n",
      "Training loss for batch 4761 : 0.3789370656013489\n",
      "Training loss for batch 4762 : 0.14006854593753815\n",
      "Training loss for batch 4763 : 0.07966539263725281\n",
      "Training loss for batch 4764 : 0.10830433666706085\n",
      "Training loss for batch 4765 : 0.17698615789413452\n",
      "Training loss for batch 4766 : 0.19322550296783447\n",
      "Training loss for batch 4767 : 0.13885316252708435\n",
      "Training loss for batch 4768 : 0.28109896183013916\n",
      "Training loss for batch 4769 : 0.14642776548862457\n",
      "Training loss for batch 4770 : 0.09581610560417175\n",
      "Training loss for batch 4771 : 0.2077525556087494\n",
      "Training loss for batch 4772 : 0.06805887818336487\n",
      "Training loss for batch 4773 : 0.35872745513916016\n",
      "Training loss for batch 4774 : 0.26745930314064026\n",
      "Training loss for batch 4775 : 0.13151979446411133\n",
      "Training loss for batch 4776 : 0.09131414443254471\n",
      "Training loss for batch 4777 : 0.32957351207733154\n",
      "Training loss for batch 4778 : 0.5391904711723328\n",
      "Training loss for batch 4779 : 0.08586325496435165\n",
      "Training loss for batch 4780 : 0.14373239874839783\n",
      "Training loss for batch 4781 : 0.033540695905685425\n",
      "Training loss for batch 4782 : 0.0008709430694580078\n",
      "Training loss for batch 4783 : 0.3683406710624695\n",
      "Training loss for batch 4784 : 0.31726333498954773\n",
      "Training loss for batch 4785 : 0.33668845891952515\n",
      "Training loss for batch 4786 : 0.2054755687713623\n",
      "Training loss for batch 4787 : 0.4353451430797577\n",
      "Training loss for batch 4788 : 0.364179402589798\n",
      "Training loss for batch 4789 : 0.13039164245128632\n",
      "Training loss for batch 4790 : 0.22997288405895233\n",
      "Training loss for batch 4791 : 0.4306211769580841\n",
      "Training loss for batch 4792 : 0.06941502541303635\n",
      "Training loss for batch 4793 : 0.1299125850200653\n",
      "Training loss for batch 4794 : 0.1023654043674469\n",
      "Training loss for batch 4795 : 0.10429244488477707\n",
      "Training loss for batch 4796 : 0.07625190168619156\n",
      "Training loss for batch 4797 : 0.09999533742666245\n",
      "Training loss for batch 4798 : 0.07846806943416595\n",
      "Training loss for batch 4799 : 0.1870223879814148\n",
      "Training loss for batch 4800 : 0.18988463282585144\n",
      "Training loss for batch 4801 : 0.05309640243649483\n",
      "Training loss for batch 4802 : 0.05024898424744606\n",
      "Training loss for batch 4803 : 0.39449700713157654\n",
      "Training loss for batch 4804 : 0.11176446825265884\n",
      "Training loss for batch 4805 : 0.14097833633422852\n",
      "Training loss for batch 4806 : 0.03615712746977806\n",
      "Training loss for batch 4807 : 0.0478394478559494\n",
      "Training loss for batch 4808 : 0.3385050892829895\n",
      "Training loss for batch 4809 : 0.35358601808547974\n",
      "Training loss for batch 4810 : 0.031168997287750244\n",
      "Training loss for batch 4811 : 0.22651873528957367\n",
      "Training loss for batch 4812 : 0.1342269778251648\n",
      "Training loss for batch 4813 : 0.06744303554296494\n",
      "Training loss for batch 4814 : 0.31308528780937195\n",
      "Training loss for batch 4815 : 0.3308888077735901\n",
      "Training loss for batch 4816 : 0.09431017190217972\n",
      "Training loss for batch 4817 : 0.13755469024181366\n",
      "Training loss for batch 4818 : 0.3892606794834137\n",
      "Training loss for batch 4819 : 0.28744983673095703\n",
      "Training loss for batch 4820 : 0.11864039301872253\n",
      "Training loss for batch 4821 : 0.2902059257030487\n",
      "Training loss for batch 4822 : 0.04482368007302284\n",
      "Training loss for batch 4823 : 0.20333613455295563\n",
      "Training loss for batch 4824 : 0.19559887051582336\n",
      "Training loss for batch 4825 : 0.3303994834423065\n",
      "Training loss for batch 4826 : 0.4863700866699219\n",
      "Training loss for batch 4827 : 0.023806210607290268\n",
      "Training loss for batch 4828 : 0.33719533681869507\n",
      "Training loss for batch 4829 : 0.21473242342472076\n",
      "Training loss for batch 4830 : 0.17880509793758392\n",
      "Training loss for batch 4831 : 0.17264190316200256\n",
      "Training loss for batch 4832 : 0.6411545872688293\n",
      "Training loss for batch 4833 : 0.19625258445739746\n",
      "Training loss for batch 4834 : 0.21932022273540497\n",
      "Training loss for batch 4835 : 0.018156666308641434\n",
      "Training loss for batch 4836 : 0.08796379715204239\n",
      "Training loss for batch 4837 : 0.08757217228412628\n",
      "Training loss for batch 4838 : 0.04410749301314354\n",
      "Training loss for batch 4839 : 0.10149930417537689\n",
      "Training loss for batch 4840 : 0.2474004626274109\n",
      "Training loss for batch 4841 : 0.08088499307632446\n",
      "Training loss for batch 4842 : 0.0543937087059021\n",
      "Training loss for batch 4843 : 0.05305204167962074\n",
      "Training loss for batch 4844 : 0.005650143139064312\n",
      "Training loss for batch 4845 : 0.13104665279388428\n",
      "Training loss for batch 4846 : 0.05049150437116623\n",
      "Training loss for batch 4847 : 0.38534384965896606\n",
      "Training loss for batch 4848 : -2.638774094521068e-05\n",
      "Training loss for batch 4849 : 0.2031095027923584\n",
      "Training loss for batch 4850 : 0.4694844186306\n",
      "Training loss for batch 4851 : 0.27029022574424744\n",
      "Training loss for batch 4852 : 0.16828075051307678\n",
      "Training loss for batch 4853 : 0.4634004235267639\n",
      "Training loss for batch 4854 : 0.0027660331688821316\n",
      "Training loss for batch 4855 : 0.026154180988669395\n",
      "Training loss for batch 4856 : 0.25934407114982605\n",
      "Training loss for batch 4857 : 0.24014176428318024\n",
      "Training loss for batch 4858 : 0.35030752420425415\n",
      "Training loss for batch 4859 : 0.32476159930229187\n",
      "Training loss for batch 4860 : 0.3903869390487671\n",
      "Training loss for batch 4861 : 0.051351290196180344\n",
      "Training loss for batch 4862 : 0.03597317636013031\n",
      "Training loss for batch 4863 : 0.24078093469142914\n",
      "Training loss for batch 4864 : 0.30538758635520935\n",
      "Training loss for batch 4865 : 0.2222742736339569\n",
      "Training loss for batch 4866 : 0.11262930184602737\n",
      "Training loss for batch 4867 : 0.213332861661911\n",
      "Training loss for batch 4868 : 0.36156269907951355\n",
      "Training loss for batch 4869 : 0.507520854473114\n",
      "Training loss for batch 4870 : 0.23196549713611603\n",
      "Training loss for batch 4871 : 0.1072126105427742\n",
      "Training loss for batch 4872 : 0.24928024411201477\n",
      "Training loss for batch 4873 : 0.16343237459659576\n",
      "Training loss for batch 4874 : 0.21103964745998383\n",
      "Training loss for batch 4875 : 0.22374629974365234\n",
      "Training loss for batch 4876 : 0.23885560035705566\n",
      "Training loss for batch 4877 : 0.22276872396469116\n",
      "Training loss for batch 4878 : 0.09820704907178879\n",
      "Training loss for batch 4879 : 0.00453938078135252\n",
      "Training loss for batch 4880 : 0.21030054986476898\n",
      "Training loss for batch 4881 : 0.1006377637386322\n",
      "Training loss for batch 4882 : 0.13642776012420654\n",
      "Training loss for batch 4883 : 0.02068389765918255\n",
      "Training loss for batch 4884 : 0.025285568088293076\n",
      "Training loss for batch 4885 : 0.007821550592780113\n",
      "Training loss for batch 4886 : 0.3444479703903198\n",
      "Training loss for batch 4887 : 0.3062128722667694\n",
      "Training loss for batch 4888 : 0.10801007598638535\n",
      "Training loss for batch 4889 : 0.4784414768218994\n",
      "Training loss for batch 4890 : 0.23473834991455078\n",
      "Training loss for batch 4891 : 0.02477775327861309\n",
      "Training loss for batch 4892 : 0.40177932381629944\n",
      "Training loss for batch 4893 : 0.42223355174064636\n",
      "Training loss for batch 4894 : 0.28509998321533203\n",
      "Training loss for batch 4895 : 0.05934712290763855\n",
      "Training loss for batch 4896 : 0.17951104044914246\n",
      "Training loss for batch 4897 : 0.07561633735895157\n",
      "Training loss for batch 4898 : 0.07956850528717041\n",
      "Training loss for batch 4899 : 0.48449620604515076\n",
      "Training loss for batch 4900 : 0.21199318766593933\n",
      "Training loss for batch 4901 : 0.1005784273147583\n",
      "Training loss for batch 4902 : 0.2754485309123993\n",
      "Training loss for batch 4903 : 0.34311965107917786\n",
      "Training loss for batch 4904 : 0.2572857141494751\n",
      "Training loss for batch 4905 : 0.12127259373664856\n",
      "Training loss for batch 4906 : 0.1341598778963089\n",
      "Training loss for batch 4907 : 0.23432183265686035\n",
      "Training loss for batch 4908 : 0.6141406297683716\n",
      "Training loss for batch 4909 : 0.20154131948947906\n",
      "Training loss for batch 4910 : 0.15335676074028015\n",
      "Training loss for batch 4911 : 0.22139661014080048\n",
      "Training loss for batch 4912 : 0.16073782742023468\n",
      "Training loss for batch 4913 : 0.1501062661409378\n",
      "Training loss for batch 4914 : 0.023656954988837242\n",
      "Training loss for batch 4915 : 0.0534362867474556\n",
      "Training loss for batch 4916 : 0.08079316467046738\n",
      "Training loss for batch 4917 : 0.16940686106681824\n",
      "Training loss for batch 4918 : 0.06909974664449692\n",
      "Training loss for batch 4919 : 0.30297690629959106\n",
      "Training loss for batch 4920 : 0.27383339405059814\n",
      "Training loss for batch 4921 : 0.379108726978302\n",
      "Training loss for batch 4922 : 0.17350700497627258\n",
      "Training loss for batch 4923 : 0.646981954574585\n",
      "Training loss for batch 4924 : 0.5283626914024353\n",
      "Training loss for batch 4925 : 0.4019888937473297\n",
      "Training loss for batch 4926 : 0.09847208112478256\n",
      "Training loss for batch 4927 : 0.050367455929517746\n",
      "Training loss for batch 4928 : 0.16031478345394135\n",
      "Training loss for batch 4929 : 0.11740842461585999\n",
      "Training loss for batch 4930 : 0.21734033524990082\n",
      "Training loss for batch 4931 : 0.023086586967110634\n",
      "Training loss for batch 4932 : 0.345163494348526\n",
      "Training loss for batch 4933 : 0.2048923522233963\n",
      "Training loss for batch 4934 : 0.6124663949012756\n",
      "Training loss for batch 4935 : 0.11710160970687866\n",
      "Training loss for batch 4936 : 0.13011237978935242\n",
      "Training loss for batch 4937 : 0.16827508807182312\n",
      "Training loss for batch 4938 : 0.19424359500408173\n",
      "Training loss for batch 4939 : 0.07912332564592361\n",
      "Training loss for batch 4940 : 0.5950419902801514\n",
      "Training loss for batch 4941 : 0.2736423909664154\n",
      "Training loss for batch 4942 : 0.18268369138240814\n",
      "Training loss for batch 4943 : 0.118987075984478\n",
      "Training loss for batch 4944 : 0.22823680937290192\n",
      "Training loss for batch 4945 : 0.009910523891448975\n",
      "Training loss for batch 4946 : 0.05576639249920845\n",
      "Training loss for batch 4947 : 0.3901616930961609\n",
      "Training loss for batch 4948 : 0.07842569053173065\n",
      "Training loss for batch 4949 : 0.23118652403354645\n",
      "Training loss for batch 4950 : 0.06105976179242134\n",
      "Training loss for batch 4951 : 0.2741474211215973\n",
      "Training loss for batch 4952 : 0.17112872004508972\n",
      "Training loss for batch 4953 : 0.033476799726486206\n",
      "Training loss for batch 4954 : 0.2334432303905487\n",
      "Training loss for batch 4955 : 0.17349599301815033\n",
      "Training loss for batch 4956 : 0.16820277273654938\n",
      "Training loss for batch 4957 : 0.2895645499229431\n",
      "Training loss for batch 4958 : 0.27308136224746704\n",
      "Training loss for batch 4959 : 0.263335645198822\n",
      "Training loss for batch 4960 : 0.08857156336307526\n",
      "Training loss for batch 4961 : 0.19212500751018524\n",
      "Training loss for batch 4962 : 0.08642139285802841\n",
      "Training loss for batch 4963 : 0.41689208149909973\n",
      "Training loss for batch 4964 : 0.04390096664428711\n",
      "Training loss for batch 4965 : 0.1182062104344368\n",
      "Training loss for batch 4966 : 0.03931095078587532\n",
      "Training loss for batch 4967 : 0.06007685139775276\n",
      "Training loss for batch 4968 : 0.3539442718029022\n",
      "Training loss for batch 4969 : 0.13557249307632446\n",
      "Training loss for batch 4970 : 0.08007513731718063\n",
      "Training loss for batch 4971 : 0.16253289580345154\n",
      "Training loss for batch 4972 : 0.25048571825027466\n",
      "Training loss for batch 4973 : 0.3540685474872589\n",
      "Training loss for batch 4974 : 0.3012197017669678\n",
      "Training loss for batch 4975 : 0.3875236213207245\n",
      "Training loss for batch 4976 : 0.30644547939300537\n",
      "Training loss for batch 4977 : 0.400820255279541\n",
      "Training loss for batch 4978 : 0.06752772629261017\n",
      "Training loss for batch 4979 : 0.16372893750667572\n",
      "Training loss for batch 4980 : 0.09143234044313431\n",
      "Training loss for batch 4981 : 0.20438514649868011\n",
      "Training loss for batch 4982 : 0.15809766948223114\n",
      "Training loss for batch 4983 : 0.08460540324449539\n",
      "Training loss for batch 4984 : 0.30998772382736206\n",
      "Training loss for batch 4985 : 0.21929705142974854\n",
      "Training loss for batch 4986 : 0.01294713281095028\n",
      "Training loss for batch 4987 : 0.15618367493152618\n",
      "Training loss for batch 4988 : 0.1366630345582962\n",
      "Training loss for batch 4989 : 0.017373112961649895\n",
      "Training loss for batch 4990 : 0.1180090606212616\n",
      "Training loss for batch 4991 : 0.17570333182811737\n",
      "Training loss for batch 4992 : 0.628639817237854\n",
      "Training loss for batch 4993 : 0.23552246391773224\n",
      "Training loss for batch 4994 : 0.20868663489818573\n",
      "Training loss for batch 4995 : 0.10956398397684097\n",
      "Training loss for batch 4996 : 0.1476772278547287\n",
      "Training loss for batch 4997 : 0.0\n",
      "Training loss for batch 4998 : 0.22564324736595154\n",
      "Training loss for batch 4999 : 0.056353457272052765\n",
      "Training loss for batch 5000 : 0.18639719486236572\n",
      "Training loss for batch 5001 : 0.08421056717634201\n",
      "Training loss for batch 5002 : 0.1675347536802292\n",
      "Training loss for batch 5003 : 0.2326703518629074\n",
      "Training loss for batch 5004 : 0.20394152402877808\n",
      "Training loss for batch 5005 : 0.25049325823783875\n",
      "Training loss for batch 5006 : 0.12424271553754807\n",
      "Training loss for batch 5007 : 0.0\n",
      "Training loss for batch 5008 : 0.2100161761045456\n",
      "Training loss for batch 5009 : 0.145583376288414\n",
      "Training loss for batch 5010 : 0.16387568414211273\n",
      "Training loss for batch 5011 : 0.21743348240852356\n",
      "Training loss for batch 5012 : 0.3019734025001526\n",
      "Training loss for batch 5013 : 0.2464570552110672\n",
      "Training loss for batch 5014 : 0.00699325418099761\n",
      "Training loss for batch 5015 : 0.13092267513275146\n",
      "Training loss for batch 5016 : 0.1805344820022583\n",
      "Training loss for batch 5017 : 0.2175333946943283\n",
      "Training loss for batch 5018 : 0.029013555496931076\n",
      "Training loss for batch 5019 : 0.15492163598537445\n",
      "Training loss for batch 5020 : 0.23716281354427338\n",
      "Training loss for batch 5021 : 0.31715792417526245\n",
      "Training loss for batch 5022 : 0.15740790963172913\n",
      "Training loss for batch 5023 : 0.07791423797607422\n",
      "Training loss for batch 5024 : 0.4053184390068054\n",
      "Training loss for batch 5025 : 0.06768044084310532\n",
      "Training loss for batch 5026 : 0.13221417367458344\n",
      "Training loss for batch 5027 : 0.029818041250109673\n",
      "Training loss for batch 5028 : 0.002303232904523611\n",
      "Training loss for batch 5029 : 0.1964312046766281\n",
      "Training loss for batch 5030 : 0.23601531982421875\n",
      "Training loss for batch 5031 : 0.14774709939956665\n",
      "Training loss for batch 5032 : 0.09321146458387375\n",
      "Training loss for batch 5033 : 0.43102365732192993\n",
      "Training loss for batch 5034 : 0.10246807336807251\n",
      "Training loss for batch 5035 : 0.16541902720928192\n",
      "Training loss for batch 5036 : 0.09709057211875916\n",
      "Training loss for batch 5037 : 0.08529333025217056\n",
      "Training loss for batch 5038 : 0.003993779420852661\n",
      "Training loss for batch 5039 : 0.20815280079841614\n",
      "Training loss for batch 5040 : 0.10360117256641388\n",
      "Training loss for batch 5041 : 0.14965814352035522\n",
      "Training loss for batch 5042 : 0.36253783106803894\n",
      "Training loss for batch 5043 : 0.04497699439525604\n",
      "Training loss for batch 5044 : 0.08857180178165436\n",
      "Training loss for batch 5045 : 0.047873929142951965\n",
      "Training loss for batch 5046 : 0.18981437385082245\n",
      "Training loss for batch 5047 : 0.01412121020257473\n",
      "Training loss for batch 5048 : 0.04879007115960121\n",
      "Training loss for batch 5049 : 0.05494898557662964\n",
      "Training loss for batch 5050 : 0.18181666731834412\n",
      "Training loss for batch 5051 : 0.051172394305467606\n",
      "Training loss for batch 5052 : 0.09315621852874756\n",
      "Training loss for batch 5053 : 0.0913982093334198\n",
      "Training loss for batch 5054 : 0.012873153202235699\n",
      "Training loss for batch 5055 : 0.1704433262348175\n",
      "Training loss for batch 5056 : 0.8554027676582336\n",
      "Training loss for batch 5057 : 0.11069364100694656\n",
      "Training loss for batch 5058 : 0.16200882196426392\n",
      "Training loss for batch 5059 : 0.3320576548576355\n",
      "Training loss for batch 5060 : 0.04012386128306389\n",
      "Training loss for batch 5061 : 0.11794950813055038\n",
      "Training loss for batch 5062 : 0.1956387460231781\n",
      "Training loss for batch 5063 : 0.32890698313713074\n",
      "Training loss for batch 5064 : 0.19665995240211487\n",
      "Training loss for batch 5065 : 0.07424134761095047\n",
      "Training loss for batch 5066 : 0.005897790193557739\n",
      "Training loss for batch 5067 : 0.06403189152479172\n",
      "Training loss for batch 5068 : 0.31201067566871643\n",
      "Training loss for batch 5069 : 0.020096957683563232\n",
      "Training loss for batch 5070 : 0.0\n",
      "Training loss for batch 5071 : 0.05078208073973656\n",
      "Training loss for batch 5072 : 0.13940344750881195\n",
      "Training loss for batch 5073 : 0.12720482051372528\n",
      "Training loss for batch 5074 : 0.35940971970558167\n",
      "Training loss for batch 5075 : 0.4860025942325592\n",
      "Training loss for batch 5076 : 0.07252311706542969\n",
      "Training loss for batch 5077 : 0.2055184245109558\n",
      "Training loss for batch 5078 : 0.1364223062992096\n",
      "Training loss for batch 5079 : 0.20737822353839874\n",
      "Training loss for batch 5080 : 0.29567304253578186\n",
      "Training loss for batch 5081 : 0.3944966495037079\n",
      "Training loss for batch 5082 : 0.020983871072530746\n",
      "Training loss for batch 5083 : 0.03644351288676262\n",
      "Training loss for batch 5084 : 0.04589952528476715\n",
      "Training loss for batch 5085 : 0.9689019322395325\n",
      "Training loss for batch 5086 : 0.050211820751428604\n",
      "Training loss for batch 5087 : 0.13452668488025665\n",
      "Training loss for batch 5088 : 0.2132621854543686\n",
      "Training loss for batch 5089 : 0.21960429847240448\n",
      "Training loss for batch 5090 : 0.05524415522813797\n",
      "Training loss for batch 5091 : 0.20770364999771118\n",
      "Training loss for batch 5092 : 0.3209226727485657\n",
      "Training loss for batch 5093 : 0.3577209711074829\n",
      "Training loss for batch 5094 : 0.026868676766753197\n",
      "Training loss for batch 5095 : 0.0967073142528534\n",
      "Training loss for batch 5096 : 0.3628390431404114\n",
      "Training loss for batch 5097 : 0.37898722290992737\n",
      "Training loss for batch 5098 : 0.04959382116794586\n",
      "Training loss for batch 5099 : 0.15231460332870483\n",
      "Training loss for batch 5100 : 0.13495513796806335\n",
      "Training loss for batch 5101 : 0.1922457218170166\n",
      "Training loss for batch 5102 : 0.09665972739458084\n",
      "Training loss for batch 5103 : 0.03628797084093094\n",
      "Training loss for batch 5104 : 0.30239197611808777\n",
      "Training loss for batch 5105 : 0.1354614496231079\n",
      "Training loss for batch 5106 : 0.3293052315711975\n",
      "Training loss for batch 5107 : 0.08033658564090729\n",
      "Training loss for batch 5108 : 0.17542597651481628\n",
      "Training loss for batch 5109 : 0.059706054627895355\n",
      "Training loss for batch 5110 : 0.5180550813674927\n",
      "Training loss for batch 5111 : 0.22554324567317963\n",
      "Training loss for batch 5112 : 0.08164792507886887\n",
      "Training loss for batch 5113 : 0.16197337210178375\n",
      "Training loss for batch 5114 : 0.04923954978585243\n",
      "Training loss for batch 5115 : 0.04249504208564758\n",
      "Training loss for batch 5116 : 0.07155639678239822\n",
      "Training loss for batch 5117 : 0.022218327969312668\n",
      "Training loss for batch 5118 : 0.17517106235027313\n",
      "Training loss for batch 5119 : 0.23716984689235687\n",
      "Training loss for batch 5120 : 0.0512283593416214\n",
      "Training loss for batch 5121 : 0.5744450688362122\n",
      "Training loss for batch 5122 : 0.13586397469043732\n",
      "Training loss for batch 5123 : 0.3235262930393219\n",
      "Training loss for batch 5124 : 0.19747477769851685\n",
      "Training loss for batch 5125 : 0.2216116487979889\n",
      "Training loss for batch 5126 : 0.07410982251167297\n",
      "Training loss for batch 5127 : 0.05592885985970497\n",
      "Training loss for batch 5128 : 0.3665105402469635\n",
      "Training loss for batch 5129 : 0.11259936541318893\n",
      "Training loss for batch 5130 : 0.08503463864326477\n",
      "Training loss for batch 5131 : 0.2753770351409912\n",
      "Training loss for batch 5132 : 0.11821639537811279\n",
      "Training loss for batch 5133 : 0.028212687000632286\n",
      "Training loss for batch 5134 : 0.29206037521362305\n",
      "Training loss for batch 5135 : 0.014814739115536213\n",
      "Training loss for batch 5136 : 0.10196680575609207\n",
      "Training loss for batch 5137 : 0.0091452207416296\n",
      "Training loss for batch 5138 : 0.06461081653833389\n",
      "Training loss for batch 5139 : 0.15034447610378265\n",
      "Training loss for batch 5140 : 0.030936483293771744\n",
      "Training loss for batch 5141 : 0.08588790893554688\n",
      "Training loss for batch 5142 : 0.00463634729385376\n",
      "Training loss for batch 5143 : 0.09969295561313629\n",
      "Training loss for batch 5144 : 0.08397301286458969\n",
      "Training loss for batch 5145 : 0.17609021067619324\n",
      "Training loss for batch 5146 : 0.24081167578697205\n",
      "Training loss for batch 5147 : 0.08389679342508316\n",
      "Training loss for batch 5148 : 0.1730547845363617\n",
      "Training loss for batch 5149 : 0.1799379289150238\n",
      "Training loss for batch 5150 : 0.023195475339889526\n",
      "Training loss for batch 5151 : 0.20011328160762787\n",
      "Training loss for batch 5152 : 0.39617446064949036\n",
      "Training loss for batch 5153 : 0.2575989365577698\n",
      "Training loss for batch 5154 : 0.2815595865249634\n",
      "Training loss for batch 5155 : 0.054004404693841934\n",
      "Training loss for batch 5156 : 0.3073827624320984\n",
      "Training loss for batch 5157 : 0.33197298645973206\n",
      "Training loss for batch 5158 : 0.1071372777223587\n",
      "Training loss for batch 5159 : 0.15277765691280365\n",
      "Training loss for batch 5160 : 0.0\n",
      "Training loss for batch 5161 : 0.1184847429394722\n",
      "Training loss for batch 5162 : 0.10495290905237198\n",
      "Training loss for batch 5163 : 0.1375945657491684\n",
      "Training loss for batch 5164 : 0.19563227891921997\n",
      "Training loss for batch 5165 : 0.20008806884288788\n",
      "Training loss for batch 5166 : 0.3272172212600708\n",
      "Training loss for batch 5167 : 0.0\n",
      "Training loss for batch 5168 : 0.0776035413146019\n",
      "Training loss for batch 5169 : 0.412503719329834\n",
      "Training loss for batch 5170 : 0.12693168222904205\n",
      "Training loss for batch 5171 : 0.08405181020498276\n",
      "Training loss for batch 5172 : 0.3221576511859894\n",
      "Training loss for batch 5173 : 0.20916226506233215\n",
      "Training loss for batch 5174 : 0.07213684171438217\n",
      "Training loss for batch 5175 : 0.4979412257671356\n",
      "Training loss for batch 5176 : 0.340451717376709\n",
      "Training loss for batch 5177 : 0.40131354331970215\n",
      "Training loss for batch 5178 : 0.0734640508890152\n",
      "Training loss for batch 5179 : 0.11966098099946976\n",
      "Training loss for batch 5180 : 0.3004375994205475\n",
      "Training loss for batch 5181 : 0.1170748695731163\n",
      "Training loss for batch 5182 : 0.13655231893062592\n",
      "Training loss for batch 5183 : 0.10033830255270004\n",
      "Training loss for batch 5184 : 0.15587857365608215\n",
      "Training loss for batch 5185 : 0.4758540987968445\n",
      "Training loss for batch 5186 : -0.0005625896737910807\n",
      "Training loss for batch 5187 : 0.15980979800224304\n",
      "Training loss for batch 5188 : 0.08086755871772766\n",
      "Training loss for batch 5189 : 0.0\n",
      "Training loss for batch 5190 : 0.058361463248729706\n",
      "Training loss for batch 5191 : 0.02776464633643627\n",
      "Training loss for batch 5192 : 0.2070539891719818\n",
      "Training loss for batch 5193 : 0.21351072192192078\n",
      "Training loss for batch 5194 : 0.1726740300655365\n",
      "Training loss for batch 5195 : 0.29928910732269287\n",
      "Training loss for batch 5196 : 0.01590353436768055\n",
      "Training loss for batch 5197 : 0.41644516587257385\n",
      "Training loss for batch 5198 : 0.24912326037883759\n",
      "Training loss for batch 5199 : 0.19430266320705414\n",
      "Training loss for batch 5200 : 0.2422609180212021\n",
      "Training loss for batch 5201 : 0.29770955443382263\n",
      "Training loss for batch 5202 : 0.07821031659841537\n",
      "Training loss for batch 5203 : 0.09186750650405884\n",
      "Training loss for batch 5204 : 0.12406355887651443\n",
      "Training loss for batch 5205 : 0.09733843803405762\n",
      "Training loss for batch 5206 : 0.1278667151927948\n",
      "Training loss for batch 5207 : 0.08775386959314346\n",
      "Training loss for batch 5208 : 0.09334346652030945\n",
      "Training loss for batch 5209 : 0.05986238643527031\n",
      "Training loss for batch 5210 : 0.5545321702957153\n",
      "Training loss for batch 5211 : 0.22127626836299896\n",
      "Training loss for batch 5212 : 0.08484309911727905\n",
      "Training loss for batch 5213 : 0.12334620207548141\n",
      "Training loss for batch 5214 : 0.30864864587783813\n",
      "Training loss for batch 5215 : 0.19162173569202423\n",
      "Training loss for batch 5216 : 0.055199652910232544\n",
      "Training loss for batch 5217 : 0.028498578816652298\n",
      "Training loss for batch 5218 : 0.04859991744160652\n",
      "Training loss for batch 5219 : 0.018496358767151833\n",
      "Training loss for batch 5220 : 0.01160435937345028\n",
      "Training loss for batch 5221 : 0.13474228978157043\n",
      "Training loss for batch 5222 : 0.014736250974237919\n",
      "Training loss for batch 5223 : 0.19790466129779816\n",
      "Training loss for batch 5224 : 0.09574101865291595\n",
      "Training loss for batch 5225 : 0.05331976339221001\n",
      "Training loss for batch 5226 : 0.2604656219482422\n",
      "Training loss for batch 5227 : 0.30799373984336853\n",
      "Training loss for batch 5228 : 0.08235771209001541\n",
      "Training loss for batch 5229 : 0.31941497325897217\n",
      "Training loss for batch 5230 : 0.053121618926525116\n",
      "Training loss for batch 5231 : 0.2296067625284195\n",
      "Training loss for batch 5232 : 0.09017060697078705\n",
      "Training loss for batch 5233 : 0.3468446731567383\n",
      "Training loss for batch 5234 : 0.0818358063697815\n",
      "Training loss for batch 5235 : 0.0555286630988121\n",
      "Training loss for batch 5236 : 0.31775662302970886\n",
      "Training loss for batch 5237 : 0.20574012398719788\n",
      "Training loss for batch 5238 : 0.3126150965690613\n",
      "Training loss for batch 5239 : 0.39896437525749207\n",
      "Training loss for batch 5240 : 0.033928096294403076\n",
      "Training loss for batch 5241 : 0.48026832938194275\n",
      "Training loss for batch 5242 : 0.09598900377750397\n",
      "Training loss for batch 5243 : 0.14826014637947083\n",
      "Training loss for batch 5244 : 0.07651373744010925\n",
      "Training loss for batch 5245 : 1.0133378505706787\n",
      "Training loss for batch 5246 : 0.032873332500457764\n",
      "Training loss for batch 5247 : 0.39239931106567383\n",
      "Training loss for batch 5248 : 0.4189704954624176\n",
      "Training loss for batch 5249 : 0.28961265087127686\n",
      "Training loss for batch 5250 : 0.12294169515371323\n",
      "Training loss for batch 5251 : 0.060604505240917206\n",
      "Training loss for batch 5252 : 0.23317569494247437\n",
      "Training loss for batch 5253 : 0.15894867479801178\n",
      "Training loss for batch 5254 : 0.22428177297115326\n",
      "Training loss for batch 5255 : 0.23697134852409363\n",
      "Training loss for batch 5256 : 0.3296458423137665\n",
      "Training loss for batch 5257 : 0.23742705583572388\n",
      "Training loss for batch 5258 : 0.37843289971351624\n",
      "Training loss for batch 5259 : 0.22428768873214722\n",
      "Training loss for batch 5260 : 0.08385169506072998\n",
      "Training loss for batch 5261 : 0.21270397305488586\n",
      "Training loss for batch 5262 : 0.13982072472572327\n",
      "Training loss for batch 5263 : 0.3741740584373474\n",
      "Training loss for batch 5264 : 0.22620314359664917\n",
      "Training loss for batch 5265 : 0.1952364295721054\n",
      "Training loss for batch 5266 : 0.04800143092870712\n",
      "Training loss for batch 5267 : 0.13892315328121185\n",
      "Training loss for batch 5268 : 0.1461559236049652\n",
      "Training loss for batch 5269 : 0.35792604088783264\n",
      "Training loss for batch 5270 : 0.20157253742218018\n",
      "Training loss for batch 5271 : 0.044452935457229614\n",
      "Training loss for batch 5272 : 0.19678042829036713\n",
      "Training loss for batch 5273 : 0.05881926044821739\n",
      "Training loss for batch 5274 : 0.026564886793494225\n",
      "Training loss for batch 5275 : 0.18606644868850708\n",
      "Training loss for batch 5276 : 0.18809723854064941\n",
      "Training loss for batch 5277 : 0.26400870084762573\n",
      "Training loss for batch 5278 : 0.43627187609672546\n",
      "Training loss for batch 5279 : 0.21403813362121582\n",
      "Training loss for batch 5280 : 0.36382386088371277\n",
      "Training loss for batch 5281 : 0.10901477932929993\n",
      "Training loss for batch 5282 : 0.34377166628837585\n",
      "Training loss for batch 5283 : 0.03854823857545853\n",
      "Training loss for batch 5284 : 0.1508045643568039\n",
      "Training loss for batch 5285 : 0.4456004798412323\n",
      "Training loss for batch 5286 : 0.3157044053077698\n",
      "Training loss for batch 5287 : 0.023199869319796562\n",
      "Training loss for batch 5288 : 0.09344048798084259\n",
      "Training loss for batch 5289 : 0.15812863409519196\n",
      "Training loss for batch 5290 : 0.012644585222005844\n",
      "Training loss for batch 5291 : 0.19260075688362122\n",
      "Training loss for batch 5292 : 0.38314810395240784\n",
      "Training loss for batch 5293 : 0.28827717900276184\n",
      "Training loss for batch 5294 : 0.038567960262298584\n",
      "Training loss for batch 5295 : 0.29131588339805603\n",
      "Training loss for batch 5296 : 0.21315966546535492\n",
      "Training loss for batch 5297 : 0.09169034659862518\n",
      "Training loss for batch 5298 : 0.20671778917312622\n",
      "Training loss for batch 5299 : 0.08975080400705338\n",
      "Training loss for batch 5300 : 0.01915746182203293\n",
      "Training loss for batch 5301 : 0.39631369709968567\n",
      "Training loss for batch 5302 : 0.4726753234863281\n",
      "Training loss for batch 5303 : 0.4133022725582123\n",
      "Training loss for batch 5304 : 0.1890469193458557\n",
      "Training loss for batch 5305 : 0.03310393914580345\n",
      "Training loss for batch 5306 : 0.08587489277124405\n",
      "Training loss for batch 5307 : 0.398695170879364\n",
      "Training loss for batch 5308 : 0.05923711135983467\n",
      "Training loss for batch 5309 : 0.00678330147638917\n",
      "Training loss for batch 5310 : 0.07442479580640793\n",
      "Training loss for batch 5311 : 0.7151828408241272\n",
      "Training loss for batch 5312 : 0.40964752435684204\n",
      "Training loss for batch 5313 : 0.08228596299886703\n",
      "Training loss for batch 5314 : 0.24101224541664124\n",
      "Training loss for batch 5315 : 0.045163705945014954\n",
      "Training loss for batch 5316 : 0.4100430905818939\n",
      "Training loss for batch 5317 : 0.06769527494907379\n",
      "Training loss for batch 5318 : 0.10572770237922668\n",
      "Training loss for batch 5319 : 0.12656961381435394\n",
      "Training loss for batch 5320 : 0.20470185577869415\n",
      "Training loss for batch 5321 : 0.1336669921875\n",
      "Training loss for batch 5322 : 0.1178959533572197\n",
      "Training loss for batch 5323 : 0.2183809131383896\n",
      "Training loss for batch 5324 : 0.09974995255470276\n",
      "Training loss for batch 5325 : 0.09870167076587677\n",
      "Training loss for batch 5326 : 0.3149449825286865\n",
      "Training loss for batch 5327 : 0.04209015890955925\n",
      "Training loss for batch 5328 : 0.47022131085395813\n",
      "Training loss for batch 5329 : 0.13387131690979004\n",
      "Training loss for batch 5330 : 0.009179066866636276\n",
      "Training loss for batch 5331 : 0.006981975864619017\n",
      "Training loss for batch 5332 : 0.31061288714408875\n",
      "Training loss for batch 5333 : 0.015026911161839962\n",
      "Training loss for batch 5334 : 0.2883836328983307\n",
      "Training loss for batch 5335 : 0.16729111969470978\n",
      "Training loss for batch 5336 : 0.3589683175086975\n",
      "Training loss for batch 5337 : 0.1045672819018364\n",
      "Training loss for batch 5338 : 0.23291634023189545\n",
      "Training loss for batch 5339 : 0.27480435371398926\n",
      "Training loss for batch 5340 : 0.0359085313975811\n",
      "Training loss for batch 5341 : 0.08055818825960159\n",
      "Training loss for batch 5342 : 0.07785278558731079\n",
      "Training loss for batch 5343 : 0.2233036756515503\n",
      "Training loss for batch 5344 : 0.17176347970962524\n",
      "Training loss for batch 5345 : 0.07321383059024811\n",
      "Training loss for batch 5346 : 0.2818352282047272\n",
      "Training loss for batch 5347 : 0.06033618375658989\n",
      "Training loss for batch 5348 : 0.15165942907333374\n",
      "Training loss for batch 5349 : 0.18426565825939178\n",
      "Training loss for batch 5350 : 0.4234665632247925\n",
      "Training loss for batch 5351 : 0.3040085434913635\n",
      "Training loss for batch 5352 : 0.27181100845336914\n",
      "Training loss for batch 5353 : 0.24263708293437958\n",
      "Training loss for batch 5354 : 0.1397194117307663\n",
      "Training loss for batch 5355 : 0.08145949989557266\n",
      "Training loss for batch 5356 : 0.09018075466156006\n",
      "Training loss for batch 5357 : 0.3480437397956848\n",
      "Training loss for batch 5358 : 0.0\n",
      "Training loss for batch 5359 : 0.13402989506721497\n",
      "Training loss for batch 5360 : 0.20892567932605743\n",
      "Training loss for batch 5361 : 0.22201138734817505\n",
      "Training loss for batch 5362 : 0.1396632343530655\n",
      "Training loss for batch 5363 : 0.29016920924186707\n",
      "Training loss for batch 5364 : 0.11434763669967651\n",
      "Training loss for batch 5365 : 0.06712830811738968\n",
      "Training loss for batch 5366 : 0.4358968436717987\n",
      "Training loss for batch 5367 : 0.016716644167900085\n",
      "Training loss for batch 5368 : 0.12040524184703827\n",
      "Training loss for batch 5369 : 0.09943342208862305\n",
      "Training loss for batch 5370 : 0.05913390964269638\n",
      "Training loss for batch 5371 : 0.09372103214263916\n",
      "Training loss for batch 5372 : 0.1620127260684967\n",
      "Training loss for batch 5373 : 0.05497395619750023\n",
      "Training loss for batch 5374 : 0.06014527380466461\n",
      "Training loss for batch 5375 : 0.23650217056274414\n",
      "Training loss for batch 5376 : 0.05798141658306122\n",
      "Training loss for batch 5377 : 0.49373647570610046\n",
      "Training loss for batch 5378 : 0.1843397170305252\n",
      "Training loss for batch 5379 : 0.32861828804016113\n",
      "Training loss for batch 5380 : 0.15915220975875854\n",
      "Training loss for batch 5381 : 0.4344977140426636\n",
      "Training loss for batch 5382 : 0.3008391559123993\n",
      "Training loss for batch 5383 : 0.2503485083580017\n",
      "Training loss for batch 5384 : 0.08422594517469406\n",
      "Training loss for batch 5385 : 0.03294576704502106\n",
      "Training loss for batch 5386 : 0.648739218711853\n",
      "Training loss for batch 5387 : 0.14934027194976807\n",
      "Training loss for batch 5388 : 0.09492552280426025\n",
      "Training loss for batch 5389 : 0.35279184579849243\n",
      "Training loss for batch 5390 : 0.012651367112994194\n",
      "Training loss for batch 5391 : 0.07561001181602478\n",
      "Training loss for batch 5392 : 0.06832966208457947\n",
      "Training loss for batch 5393 : 0.08392377942800522\n",
      "Training loss for batch 5394 : 0.07727757841348648\n",
      "Training loss for batch 5395 : 0.07493843883275986\n",
      "Training loss for batch 5396 : 0.0798804759979248\n",
      "Training loss for batch 5397 : 0.020878281444311142\n",
      "Training loss for batch 5398 : 0.47878584265708923\n",
      "Training loss for batch 5399 : 0.09236784279346466\n",
      "Training loss for batch 5400 : 0.1776212453842163\n",
      "Training loss for batch 5401 : 0.1605306714773178\n",
      "Training loss for batch 5402 : 0.12436042726039886\n",
      "Training loss for batch 5403 : 0.09499788284301758\n",
      "Training loss for batch 5404 : 0.21151435375213623\n",
      "Training loss for batch 5405 : 0.03936586529016495\n",
      "Training loss for batch 5406 : 0.18933315575122833\n",
      "Training loss for batch 5407 : 0.24255286157131195\n",
      "Training loss for batch 5408 : 0.40268462896347046\n",
      "Training loss for batch 5409 : 0.17797532677650452\n",
      "Training loss for batch 5410 : 0.13806255161762238\n",
      "Training loss for batch 5411 : 0.059453874826431274\n",
      "Training loss for batch 5412 : 0.09645145386457443\n",
      "Training loss for batch 5413 : 0.27316775918006897\n",
      "Training loss for batch 5414 : 0.018874993547797203\n",
      "Training loss for batch 5415 : 0.24056163430213928\n",
      "Training loss for batch 5416 : 0.13021840155124664\n",
      "Training loss for batch 5417 : -0.00015765021089464426\n",
      "Training loss for batch 5418 : 0.15047582983970642\n",
      "Training loss for batch 5419 : 0.3799303472042084\n",
      "Training loss for batch 5420 : 0.3073391616344452\n",
      "Training loss for batch 5421 : 0.08816800266504288\n",
      "Training loss for batch 5422 : 0.10253676027059555\n",
      "Training loss for batch 5423 : 0.22823311388492584\n",
      "Training loss for batch 5424 : 0.17377448081970215\n",
      "Training loss for batch 5425 : 0.1446721851825714\n",
      "Training loss for batch 5426 : 0.32817015051841736\n",
      "Training loss for batch 5427 : 0.14149940013885498\n",
      "Training loss for batch 5428 : 0.07649195939302444\n",
      "Training loss for batch 5429 : 0.27300068736076355\n",
      "Training loss for batch 5430 : 0.18049198389053345\n",
      "Training loss for batch 5431 : 0.3843230605125427\n",
      "Training loss for batch 5432 : 0.7319176197052002\n",
      "Training loss for batch 5433 : 0.007689297199249268\n",
      "Training loss for batch 5434 : 0.10963677614927292\n",
      "Training loss for batch 5435 : 0.2087312936782837\n",
      "Training loss for batch 5436 : 0.38863837718963623\n",
      "Training loss for batch 5437 : 0.17889882624149323\n",
      "Training loss for batch 5438 : 0.4052827060222626\n",
      "Training loss for batch 5439 : 0.05154791846871376\n",
      "Training loss for batch 5440 : 0.4421040415763855\n",
      "Training loss for batch 5441 : 0.010284497402608395\n",
      "Training loss for batch 5442 : 0.039556216448545456\n",
      "Training loss for batch 5443 : 0.30930840969085693\n",
      "Training loss for batch 5444 : 0.10737545788288116\n",
      "Training loss for batch 5445 : 0.3730630874633789\n",
      "Training loss for batch 5446 : 0.0002577727136667818\n",
      "Training loss for batch 5447 : 0.10580594092607498\n",
      "Training loss for batch 5448 : 0.21740968525409698\n",
      "Training loss for batch 5449 : 0.18380595743656158\n",
      "Training loss for batch 5450 : 0.1918717473745346\n",
      "Training loss for batch 5451 : 0.1682232916355133\n",
      "Training loss for batch 5452 : 0.0765557736158371\n",
      "Training loss for batch 5453 : 0.1045764833688736\n",
      "Training loss for batch 5454 : 0.485302597284317\n",
      "Training loss for batch 5455 : 0.10011832416057587\n",
      "Training loss for batch 5456 : 0.10912838578224182\n",
      "Training loss for batch 5457 : 0.2349889874458313\n",
      "Training loss for batch 5458 : 0.579394519329071\n",
      "Training loss for batch 5459 : 0.24987570941448212\n",
      "Training loss for batch 5460 : 0.19030408561229706\n",
      "Training loss for batch 5461 : 0.3420194685459137\n",
      "Training loss for batch 5462 : 0.009456011466681957\n",
      "Training loss for batch 5463 : 0.2290256917476654\n",
      "Training loss for batch 5464 : 0.14507418870925903\n",
      "Training loss for batch 5465 : 0.06952811032533646\n",
      "Training loss for batch 5466 : 0.07401982694864273\n",
      "Training loss for batch 5467 : 0.26918500661849976\n",
      "Training loss for batch 5468 : 0.01611831970512867\n",
      "Training loss for batch 5469 : 0.05417714640498161\n",
      "Training loss for batch 5470 : 0.21638604998588562\n",
      "Training loss for batch 5471 : 0.20195168256759644\n",
      "Training loss for batch 5472 : 0.4390515387058258\n",
      "Training loss for batch 5473 : 0.048925310373306274\n",
      "Training loss for batch 5474 : 0.9369407892227173\n",
      "Training loss for batch 5475 : 0.23610499501228333\n",
      "Training loss for batch 5476 : 0.2101791501045227\n",
      "Training loss for batch 5477 : 0.1279921680688858\n",
      "Training loss for batch 5478 : 0.3479346036911011\n",
      "Training loss for batch 5479 : 0.01703750342130661\n",
      "Training loss for batch 5480 : 0.44687050580978394\n",
      "Training loss for batch 5481 : 0.15661975741386414\n",
      "Training loss for batch 5482 : 0.09564283490180969\n",
      "Training loss for batch 5483 : 0.13585145771503448\n",
      "Training loss for batch 5484 : 0.05237444117665291\n",
      "Training loss for batch 5485 : 0.24639441072940826\n",
      "Training loss for batch 5486 : 0.2419678270816803\n",
      "Training loss for batch 5487 : 0.28324463963508606\n",
      "Training loss for batch 5488 : 0.10762849450111389\n",
      "Training loss for batch 5489 : 0.1855829507112503\n",
      "Training loss for batch 5490 : 0.2273135930299759\n",
      "Training loss for batch 5491 : 0.13398773968219757\n",
      "Training loss for batch 5492 : 0.2538873851299286\n",
      "Training loss for batch 5493 : 0.17240814864635468\n",
      "Training loss for batch 5494 : 0.023523088544607162\n",
      "Training loss for batch 5495 : 0.29693737626075745\n",
      "Training loss for batch 5496 : 0.33011192083358765\n",
      "Training loss for batch 5497 : 0.1519794911146164\n",
      "Training loss for batch 5498 : 0.12630170583724976\n",
      "Training loss for batch 5499 : 0.1339198797941208\n",
      "Training loss for batch 5500 : 0.38899919390678406\n",
      "Training loss for batch 5501 : 0.038706038147211075\n",
      "Training loss for batch 5502 : 0.16665245592594147\n",
      "Training loss for batch 5503 : 0.29350554943084717\n",
      "Training loss for batch 5504 : 0.18412165343761444\n",
      "Training loss for batch 5505 : 0.10689724236726761\n",
      "Training loss for batch 5506 : 0.08665786683559418\n",
      "Training loss for batch 5507 : 0.338544100522995\n",
      "Training loss for batch 5508 : 0.4053206145763397\n",
      "Training loss for batch 5509 : 0.371021568775177\n",
      "Training loss for batch 5510 : 0.058578427881002426\n",
      "Training loss for batch 5511 : 0.22968918085098267\n",
      "Training loss for batch 5512 : 0.11526530236005783\n",
      "Training loss for batch 5513 : 0.23431093990802765\n",
      "Training loss for batch 5514 : 0.5623680353164673\n",
      "Training loss for batch 5515 : 0.18276521563529968\n",
      "Training loss for batch 5516 : 0.19874703884124756\n",
      "Training loss for batch 5517 : 0.05100388452410698\n",
      "Training loss for batch 5518 : 0.11720050871372223\n",
      "Training loss for batch 5519 : 0.21404926478862762\n",
      "Training loss for batch 5520 : 0.017003634944558144\n",
      "Training loss for batch 5521 : 0.13144062459468842\n",
      "Training loss for batch 5522 : 0.0689469426870346\n",
      "Training loss for batch 5523 : 0.09002401679754257\n",
      "Training loss for batch 5524 : 0.02056543342769146\n",
      "Training loss for batch 5525 : 0.06179002299904823\n",
      "Training loss for batch 5526 : 0.5036818385124207\n",
      "Training loss for batch 5527 : 0.25704455375671387\n",
      "Training loss for batch 5528 : 0.35246041417121887\n",
      "Training loss for batch 5529 : 0.11350490897893906\n",
      "Training loss for batch 5530 : 0.16770713031291962\n",
      "Training loss for batch 5531 : 0.060238394886255264\n",
      "Training loss for batch 5532 : 0.028387270867824554\n",
      "Training loss for batch 5533 : 0.1744324266910553\n",
      "Training loss for batch 5534 : 0.058932870626449585\n",
      "Training loss for batch 5535 : 0.023662835359573364\n",
      "Training loss for batch 5536 : 0.3648132085800171\n",
      "Training loss for batch 5537 : 0.2035883069038391\n",
      "Training loss for batch 5538 : 0.14350418746471405\n",
      "Training loss for batch 5539 : 0.06278689205646515\n",
      "Training loss for batch 5540 : 0.11952584981918335\n",
      "Training loss for batch 5541 : 0.3624807894229889\n",
      "Training loss for batch 5542 : 0.13591057062149048\n",
      "Training loss for batch 5543 : 0.334488183259964\n",
      "Training loss for batch 5544 : 0.3233284056186676\n",
      "Training loss for batch 5545 : 0.07357040047645569\n",
      "Training loss for batch 5546 : 0.3202665448188782\n",
      "Training loss for batch 5547 : 0.043227359652519226\n",
      "Training loss for batch 5548 : 0.12959574162960052\n",
      "Training loss for batch 5549 : 0.16006161272525787\n",
      "Training loss for batch 5550 : 0.08809465914964676\n",
      "Training loss for batch 5551 : 0.09257101267576218\n",
      "Training loss for batch 5552 : 0.03178545460104942\n",
      "Training loss for batch 5553 : 0.2270035743713379\n",
      "Training loss for batch 5554 : 0.03156743571162224\n",
      "Training loss for batch 5555 : 0.11248348653316498\n",
      "Training loss for batch 5556 : 0.48307138681411743\n",
      "Training loss for batch 5557 : 0.12213790416717529\n",
      "Training loss for batch 5558 : 0.20924043655395508\n",
      "Training loss for batch 5559 : 0.5675919651985168\n",
      "Training loss for batch 5560 : 0.275615394115448\n",
      "Training loss for batch 5561 : 0.32247719168663025\n",
      "Training loss for batch 5562 : 0.01875404641032219\n",
      "Training loss for batch 5563 : 0.23677726089954376\n",
      "Training loss for batch 5564 : 0.4594818353652954\n",
      "Training loss for batch 5565 : 0.20572806894779205\n",
      "Training loss for batch 5566 : 0.4590361416339874\n",
      "Training loss for batch 5567 : 0.026873689144849777\n",
      "Training loss for batch 5568 : 0.12239936739206314\n",
      "Training loss for batch 5569 : 0.4112783372402191\n",
      "Training loss for batch 5570 : 0.17768597602844238\n",
      "Training loss for batch 5571 : 0.21615231037139893\n",
      "Training loss for batch 5572 : 0.1241319328546524\n",
      "Training loss for batch 5573 : 0.0723031759262085\n",
      "Training loss for batch 5574 : 0.3748833239078522\n",
      "Training loss for batch 5575 : 0.04292440414428711\n",
      "Training loss for batch 5576 : 0.10739918798208237\n",
      "Training loss for batch 5577 : 0.3137272596359253\n",
      "Training loss for batch 5578 : 0.10205764323472977\n",
      "Training loss for batch 5579 : 0.014183173887431622\n",
      "Training loss for batch 5580 : 0.06482461839914322\n",
      "Training loss for batch 5581 : 0.3678549826145172\n",
      "Training loss for batch 5582 : 0.17309650778770447\n",
      "Training loss for batch 5583 : 0.04467329382896423\n",
      "Training loss for batch 5584 : 0.09744936972856522\n",
      "Training loss for batch 5585 : 0.2409883439540863\n",
      "Training loss for batch 5586 : 0.2793233096599579\n",
      "Training loss for batch 5587 : 0.19138121604919434\n",
      "Training loss for batch 5588 : 0.02824483811855316\n",
      "Training loss for batch 5589 : 0.31502586603164673\n",
      "Training loss for batch 5590 : 0.31256330013275146\n",
      "Training loss for batch 5591 : 0.49501824378967285\n",
      "Training loss for batch 5592 : 0.18577632308006287\n",
      "Training loss for batch 5593 : 0.030289988964796066\n",
      "Training loss for batch 5594 : 0.2118256390094757\n",
      "Training loss for batch 5595 : 0.03246781975030899\n",
      "Training loss for batch 5596 : 0.1894598752260208\n",
      "Training loss for batch 5597 : 0.2740439474582672\n",
      "Training loss for batch 5598 : 0.11064595729112625\n",
      "Training loss for batch 5599 : 0.03363037109375\n",
      "Training loss for batch 5600 : 0.5873472690582275\n",
      "Training loss for batch 5601 : 0.20148885250091553\n",
      "Training loss for batch 5602 : 0.32097893953323364\n",
      "Training loss for batch 5603 : 0.0585564523935318\n",
      "Training loss for batch 5604 : 0.2196551114320755\n",
      "Training loss for batch 5605 : 0.17088504135608673\n",
      "Training loss for batch 5606 : 0.14767411351203918\n",
      "Training loss for batch 5607 : 0.23530909419059753\n",
      "Training loss for batch 5608 : 0.1012611910700798\n",
      "Training loss for batch 5609 : 0.21439561247825623\n",
      "Training loss for batch 5610 : 0.3010844588279724\n",
      "Training loss for batch 5611 : 0.02814910002052784\n",
      "Training loss for batch 5612 : 0.13272631168365479\n",
      "Training loss for batch 5613 : 0.7683765888214111\n",
      "Training loss for batch 5614 : 0.024752628058195114\n",
      "Training loss for batch 5615 : 0.03684648498892784\n",
      "Training loss for batch 5616 : 0.09287240356206894\n",
      "Training loss for batch 5617 : 0.10624408721923828\n",
      "Training loss for batch 5618 : 0.2995564043521881\n",
      "Training loss for batch 5619 : 0.1416727602481842\n",
      "Training loss for batch 5620 : 0.10419721901416779\n",
      "Training loss for batch 5621 : 0.14266133308410645\n",
      "Training loss for batch 5622 : 0.3335312604904175\n",
      "Training loss for batch 5623 : 0.3689030706882477\n",
      "Training loss for batch 5624 : 0.11701396107673645\n",
      "Training loss for batch 5625 : 0.5256751179695129\n",
      "Training loss for batch 5626 : 0.10116980969905853\n",
      "Training loss for batch 5627 : 0.10627258569002151\n",
      "Training loss for batch 5628 : 0.15797923505306244\n",
      "Training loss for batch 5629 : 0.22184748947620392\n",
      "Training loss for batch 5630 : 0.2021370828151703\n",
      "Training loss for batch 5631 : 0.20340847969055176\n",
      "Training loss for batch 5632 : 0.30936241149902344\n",
      "Training loss for batch 5633 : 0.34600383043289185\n",
      "Training loss for batch 5634 : 0.05364958941936493\n",
      "Training loss for batch 5635 : 0.0963854044675827\n",
      "Training loss for batch 5636 : 0.015236535109579563\n",
      "Training loss for batch 5637 : 0.5345269441604614\n",
      "Training loss for batch 5638 : 0.03820506110787392\n",
      "Training loss for batch 5639 : 0.057742223143577576\n",
      "Training loss for batch 5640 : 0.176056906580925\n",
      "Training loss for batch 5641 : 0.054410628974437714\n",
      "Training loss for batch 5642 : 0.46731460094451904\n",
      "Training loss for batch 5643 : 0.35271787643432617\n",
      "Training loss for batch 5644 : 0.2149195522069931\n",
      "Training loss for batch 5645 : 0.38840752840042114\n",
      "Training loss for batch 5646 : 0.1803334355354309\n",
      "Training loss for batch 5647 : 0.27975279092788696\n",
      "Training loss for batch 5648 : 0.2591001093387604\n",
      "Training loss for batch 5649 : 0.32264500856399536\n",
      "Training loss for batch 5650 : 0.2562429904937744\n",
      "Training loss for batch 5651 : 0.4345477521419525\n",
      "Training loss for batch 5652 : 0.20450356602668762\n",
      "Training loss for batch 5653 : 0.25711461901664734\n",
      "Training loss for batch 5654 : 0.017467359080910683\n",
      "Training loss for batch 5655 : 0.038572777062654495\n",
      "Training loss for batch 5656 : 0.17285874485969543\n",
      "Training loss for batch 5657 : 0.19716456532478333\n",
      "Training loss for batch 5658 : 0.19826321303844452\n",
      "Training loss for batch 5659 : 0.4233098328113556\n",
      "Training loss for batch 5660 : 0.0663469135761261\n",
      "Training loss for batch 5661 : 0.032354891300201416\n",
      "Training loss for batch 5662 : 0.11226488649845123\n",
      "Training loss for batch 5663 : 0.15430794656276703\n",
      "Training loss for batch 5664 : 0.05766559764742851\n",
      "Training loss for batch 5665 : 0.168589785695076\n",
      "Training loss for batch 5666 : 0.18121348321437836\n",
      "Training loss for batch 5667 : 0.30579307675361633\n",
      "Training loss for batch 5668 : 0.285514235496521\n",
      "Training loss for batch 5669 : 0.1984788179397583\n",
      "Training loss for batch 5670 : 0.24235203862190247\n",
      "Training loss for batch 5671 : 0.08825398236513138\n",
      "Training loss for batch 5672 : 0.3922589421272278\n",
      "Training loss for batch 5673 : 0.004399818833917379\n",
      "Training loss for batch 5674 : 0.05821888521313667\n",
      "Training loss for batch 5675 : 0.26662495732307434\n",
      "Training loss for batch 5676 : 0.2893328070640564\n",
      "Training loss for batch 5677 : 0.4201835095882416\n",
      "Training loss for batch 5678 : 0.09044096618890762\n",
      "Training loss for batch 5679 : 0.0\n",
      "Training loss for batch 5680 : 0.06320612132549286\n",
      "Training loss for batch 5681 : 0.38516610860824585\n",
      "Training loss for batch 5682 : 0.057131871581077576\n",
      "Training loss for batch 5683 : 0.4938566982746124\n",
      "Training loss for batch 5684 : 0.29395872354507446\n",
      "Training loss for batch 5685 : 0.0384972058236599\n",
      "Training loss for batch 5686 : -7.288296183105558e-05\n",
      "Training loss for batch 5687 : 0.3769122064113617\n",
      "Training loss for batch 5688 : 0.09767108410596848\n",
      "Training loss for batch 5689 : 0.14343635737895966\n",
      "Training loss for batch 5690 : 0.29331713914871216\n",
      "Training loss for batch 5691 : 0.17874476313591003\n",
      "Training loss for batch 5692 : 0.08765991777181625\n",
      "Training loss for batch 5693 : 0.005344092845916748\n",
      "Training loss for batch 5694 : 0.1448507457971573\n",
      "Training loss for batch 5695 : 0.14477340877056122\n",
      "Training loss for batch 5696 : 0.12527796626091003\n",
      "Training loss for batch 5697 : 0.17373201251029968\n",
      "Training loss for batch 5698 : 0.3888276219367981\n",
      "Training loss for batch 5699 : 0.028331061825156212\n",
      "Training loss for batch 5700 : 0.10847466439008713\n",
      "Training loss for batch 5701 : 0.3265157639980316\n",
      "Training loss for batch 5702 : 0.1826470047235489\n",
      "Training loss for batch 5703 : 0.23783455789089203\n",
      "Training loss for batch 5704 : 0.19124795496463776\n",
      "Training loss for batch 5705 : 0.35388097167015076\n",
      "Training loss for batch 5706 : 0.13953761756420135\n",
      "Training loss for batch 5707 : 0.12806172668933868\n",
      "Training loss for batch 5708 : 0.30481693148612976\n",
      "Training loss for batch 5709 : 0.0112498439848423\n",
      "Training loss for batch 5710 : 0.09195472300052643\n",
      "Training loss for batch 5711 : 0.4700809419155121\n",
      "Training loss for batch 5712 : 0.2800922989845276\n",
      "Training loss for batch 5713 : 0.07956935465335846\n",
      "Training loss for batch 5714 : 0.21296453475952148\n",
      "Training loss for batch 5715 : 0.11809249967336655\n",
      "Training loss for batch 5716 : 0.3501950800418854\n",
      "Training loss for batch 5717 : 0.3140000104904175\n",
      "Training loss for batch 5718 : 0.11453983187675476\n",
      "Training loss for batch 5719 : 0.21250085532665253\n",
      "Training loss for batch 5720 : 0.17563597857952118\n",
      "Training loss for batch 5721 : 0.030154263600707054\n",
      "Training loss for batch 5722 : 0.21665039658546448\n",
      "Training loss for batch 5723 : 0.29949450492858887\n",
      "Training loss for batch 5724 : 0.20090433955192566\n",
      "Training loss for batch 5725 : 0.03852878883481026\n",
      "Training loss for batch 5726 : 0.19969560205936432\n",
      "Training loss for batch 5727 : 0.06833416223526001\n",
      "Training loss for batch 5728 : 0.4743850529193878\n",
      "Training loss for batch 5729 : 0.5575600862503052\n",
      "Training loss for batch 5730 : 0.4250500500202179\n",
      "Training loss for batch 5731 : 0.1724814772605896\n",
      "Training loss for batch 5732 : 0.2910913825035095\n",
      "Training loss for batch 5733 : 0.11832407861948013\n",
      "Training loss for batch 5734 : 0.40460023283958435\n",
      "Training loss for batch 5735 : 0.008363651111721992\n",
      "Training loss for batch 5736 : 0.24579505622386932\n",
      "Training loss for batch 5737 : 0.13913559913635254\n",
      "Training loss for batch 5738 : 0.3550165593624115\n",
      "Training loss for batch 5739 : 0.07755396515130997\n",
      "Training loss for batch 5740 : 0.08105640858411789\n",
      "Training loss for batch 5741 : 0.05916832387447357\n",
      "Training loss for batch 5742 : 0.21003566682338715\n",
      "Training loss for batch 5743 : 0.21675628423690796\n",
      "Training loss for batch 5744 : 0.27026548981666565\n",
      "Training loss for batch 5745 : 0.06410693377256393\n",
      "Training loss for batch 5746 : 0.0028774540405720472\n",
      "Training loss for batch 5747 : 0.045912761241197586\n",
      "Training loss for batch 5748 : 0.2547287344932556\n",
      "Training loss for batch 5749 : 0.03644802048802376\n",
      "Training loss for batch 5750 : 0.09865697473287582\n",
      "Training loss for batch 5751 : 0.2618553042411804\n",
      "Training loss for batch 5752 : 0.1793530434370041\n",
      "Training loss for batch 5753 : 0.02080671489238739\n",
      "Training loss for batch 5754 : 0.3413490653038025\n",
      "Training loss for batch 5755 : 0.3022421598434448\n",
      "Training loss for batch 5756 : 0.05506429448723793\n",
      "Training loss for batch 5757 : 0.10314306616783142\n",
      "Training loss for batch 5758 : 0.22834715247154236\n",
      "Training loss for batch 5759 : 0.4172302782535553\n",
      "Training loss for batch 5760 : 0.3224315345287323\n",
      "Training loss for batch 5761 : 0.28183266520500183\n",
      "Training loss for batch 5762 : 0.2304535061120987\n",
      "Training loss for batch 5763 : 0.37338167428970337\n",
      "Training loss for batch 5764 : 0.16996321082115173\n",
      "Training loss for batch 5765 : 0.3695957660675049\n",
      "Training loss for batch 5766 : 0.5473856925964355\n",
      "Training loss for batch 5767 : 0.2154082953929901\n",
      "Training loss for batch 5768 : 0.04258357360959053\n",
      "Training loss for batch 5769 : 0.09896168112754822\n",
      "Training loss for batch 5770 : 0.14249762892723083\n",
      "Training loss for batch 5771 : 0.2344227284193039\n",
      "Training loss for batch 5772 : 0.09541621059179306\n",
      "Training loss for batch 5773 : 0.32163476943969727\n",
      "Training loss for batch 5774 : 0.23648187518119812\n",
      "Training loss for batch 5775 : 0.2520029544830322\n",
      "Training loss for batch 5776 : 0.04166686534881592\n",
      "Training loss for batch 5777 : 0.022588985040783882\n",
      "Training loss for batch 5778 : 0.1173778548836708\n",
      "Training loss for batch 5779 : 0.27058225870132446\n",
      "Training loss for batch 5780 : 0.05839947983622551\n",
      "Training loss for batch 5781 : 0.05687655508518219\n",
      "Training loss for batch 5782 : 0.12986083328723907\n",
      "Training loss for batch 5783 : 0.2706068754196167\n",
      "Training loss for batch 5784 : 0.0228306632488966\n",
      "Training loss for batch 5785 : 0.15622755885124207\n",
      "Training loss for batch 5786 : 0.07285679876804352\n",
      "Training loss for batch 5787 : 0.04835156723856926\n",
      "Training loss for batch 5788 : 0.16093605756759644\n",
      "Training loss for batch 5789 : 0.14840303361415863\n",
      "Training loss for batch 5790 : 0.009539729915559292\n",
      "Training loss for batch 5791 : 0.07257789373397827\n",
      "Training loss for batch 5792 : 0.3170885145664215\n",
      "Training loss for batch 5793 : 0.32203564047813416\n",
      "Training loss for batch 5794 : 0.04084313288331032\n",
      "Training loss for batch 5795 : 0.11298375576734543\n",
      "Training loss for batch 5796 : 0.11042051017284393\n",
      "Training loss for batch 5797 : 0.09305017441511154\n",
      "Training loss for batch 5798 : 0.07801070064306259\n",
      "Training loss for batch 5799 : 0.646327018737793\n",
      "Training loss for batch 5800 : 0.1600533127784729\n",
      "Training loss for batch 5801 : 0.10524098575115204\n",
      "Training loss for batch 5802 : 0.22728221118450165\n",
      "Training loss for batch 5803 : 0.02966046705842018\n",
      "Training loss for batch 5804 : 0.4613903760910034\n",
      "Training loss for batch 5805 : 0.2497251033782959\n",
      "Training loss for batch 5806 : 0.21198715269565582\n",
      "Training loss for batch 5807 : 0.19641831517219543\n",
      "Training loss for batch 5808 : 0.7972990870475769\n",
      "Training loss for batch 5809 : 0.06680969893932343\n",
      "Training loss for batch 5810 : 0.22866225242614746\n",
      "Training loss for batch 5811 : 0.09968902170658112\n",
      "Training loss for batch 5812 : 0.21534328162670135\n",
      "Training loss for batch 5813 : 0.34911805391311646\n",
      "Training loss for batch 5814 : 0.14913500845432281\n",
      "Training loss for batch 5815 : 0.15009449422359467\n",
      "Training loss for batch 5816 : 0.2542039155960083\n",
      "Training loss for batch 5817 : 0.07874680310487747\n",
      "Training loss for batch 5818 : 0.23604853451251984\n",
      "Training loss for batch 5819 : 0.00026480306405574083\n",
      "Training loss for batch 5820 : 0.0502629391849041\n",
      "Training loss for batch 5821 : 0.3305966258049011\n",
      "Training loss for batch 5822 : 0.34922850131988525\n",
      "Training loss for batch 5823 : 0.10756122320890427\n",
      "Training loss for batch 5824 : 0.01190032809972763\n",
      "Training loss for batch 5825 : 0.24891547858715057\n",
      "Training loss for batch 5826 : 0.0\n",
      "Training loss for batch 5827 : 0.37846943736076355\n",
      "Training loss for batch 5828 : 0.25348442792892456\n",
      "Training loss for batch 5829 : 0.2452366203069687\n",
      "Training loss for batch 5830 : 0.15409940481185913\n",
      "Training loss for batch 5831 : 0.4493849277496338\n",
      "Training loss for batch 5832 : 0.2777605354785919\n",
      "Training loss for batch 5833 : 0.30958208441734314\n",
      "Training loss for batch 5834 : 0.1853400468826294\n",
      "Training loss for batch 5835 : 0.054222747683525085\n",
      "Training loss for batch 5836 : 0.0027257066685706377\n",
      "Training loss for batch 5837 : 0.20624153316020966\n",
      "Training loss for batch 5838 : 0.1318373829126358\n",
      "Training loss for batch 5839 : 0.4590456485748291\n",
      "Training loss for batch 5840 : 0.11745715141296387\n",
      "Training loss for batch 5841 : 0.25236475467681885\n",
      "Training loss for batch 5842 : 0.1263723373413086\n",
      "Training loss for batch 5843 : 0.3435182571411133\n",
      "Training loss for batch 5844 : 0.2432270050048828\n",
      "Training loss for batch 5845 : 0.3211897611618042\n",
      "Training loss for batch 5846 : 0.26024022698402405\n",
      "Training loss for batch 5847 : 0.5728058815002441\n",
      "Training loss for batch 5848 : 0.13474658131599426\n",
      "Training loss for batch 5849 : 0.09915775805711746\n",
      "Training loss for batch 5850 : 0.03791772201657295\n",
      "Training loss for batch 5851 : 0.4140574634075165\n",
      "Training loss for batch 5852 : 0.07469956576824188\n",
      "Training loss for batch 5853 : 0.14663727581501007\n",
      "Training loss for batch 5854 : 0.013805719092488289\n",
      "Training loss for batch 5855 : 0.2437959611415863\n",
      "Training loss for batch 5856 : 0.20538492500782013\n",
      "Training loss for batch 5857 : 0.10351160913705826\n",
      "Training loss for batch 5858 : 0.06588049232959747\n",
      "Training loss for batch 5859 : 0.07343563437461853\n",
      "Training loss for batch 5860 : 0.10051435232162476\n",
      "Training loss for batch 5861 : 0.34182262420654297\n",
      "Training loss for batch 5862 : 0.18789218366146088\n",
      "Training loss for batch 5863 : 0.21281254291534424\n",
      "Training loss for batch 5864 : 0.10176163166761398\n",
      "Training loss for batch 5865 : 0.2722756862640381\n",
      "Training loss for batch 5866 : 0.0808059349656105\n",
      "Training loss for batch 5867 : 0.1295463591814041\n",
      "Training loss for batch 5868 : 0.09792138636112213\n",
      "Training loss for batch 5869 : 0.06492284685373306\n",
      "Training loss for batch 5870 : 0.3883555829524994\n",
      "Training loss for batch 5871 : 0.033624883741140366\n",
      "Training loss for batch 5872 : 0.23801609873771667\n",
      "Training loss for batch 5873 : 0.27727317810058594\n",
      "Training loss for batch 5874 : 0.256193608045578\n",
      "Training loss for batch 5875 : 0.31135645508766174\n",
      "Training loss for batch 5876 : 0.12926025688648224\n",
      "Training loss for batch 5877 : 0.3665541708469391\n",
      "Training loss for batch 5878 : 0.22419938445091248\n",
      "Training loss for batch 5879 : 0.1214381530880928\n",
      "Training loss for batch 5880 : 0.09838444739580154\n",
      "Training loss for batch 5881 : 0.2529832720756531\n",
      "Training loss for batch 5882 : 0.1714131087064743\n",
      "Training loss for batch 5883 : 0.13892525434494019\n",
      "Training loss for batch 5884 : 0.016398360952734947\n",
      "Training loss for batch 5885 : 0.03295045346021652\n",
      "Training loss for batch 5886 : 0.369393527507782\n",
      "Training loss for batch 5887 : 0.287701815366745\n",
      "Training loss for batch 5888 : 0.10896844416856766\n",
      "Training loss for batch 5889 : 0.6440355181694031\n",
      "Training loss for batch 5890 : 0.35934191942214966\n",
      "Training loss for batch 5891 : 0.20821842551231384\n",
      "Training loss for batch 5892 : 0.3080856502056122\n",
      "Training loss for batch 5893 : 0.13702484965324402\n",
      "Training loss for batch 5894 : 0.18713048100471497\n",
      "Training loss for batch 5895 : 0.3595483899116516\n",
      "Training loss for batch 5896 : 0.11532770842313766\n",
      "Training loss for batch 5897 : 0.3517927825450897\n",
      "Training loss for batch 5898 : 0.31767216324806213\n",
      "Training loss for batch 5899 : 0.1535375416278839\n",
      "Training loss for batch 5900 : 0.08996446430683136\n",
      "Training loss for batch 5901 : 0.016818929463624954\n",
      "Training loss for batch 5902 : 0.1907455176115036\n",
      "Training loss for batch 5903 : 0.01713358424603939\n",
      "Training loss for batch 5904 : 0.33730682730674744\n",
      "Training loss for batch 5905 : 0.08317922055721283\n",
      "Training loss for batch 5906 : 0.10697796940803528\n",
      "Training loss for batch 5907 : 0.15722930431365967\n",
      "Training loss for batch 5908 : 0.27912360429763794\n",
      "Training loss for batch 5909 : 0.10019620507955551\n",
      "Training loss for batch 5910 : 0.8476958870887756\n",
      "Training loss for batch 5911 : 0.2569738030433655\n",
      "Training loss for batch 5912 : 0.11205647885799408\n",
      "Training loss for batch 5913 : 0.13734737038612366\n",
      "Training loss for batch 5914 : 0.2836315333843231\n",
      "Training loss for batch 5915 : 0.314401775598526\n",
      "Training loss for batch 5916 : 0.3285467326641083\n",
      "Training loss for batch 5917 : 0.28023114800453186\n",
      "Training loss for batch 5918 : 0.3728884160518646\n",
      "Training loss for batch 5919 : 0.3118039667606354\n",
      "Training loss for batch 5920 : 0.33116722106933594\n",
      "Training loss for batch 5921 : 0.08525773137807846\n",
      "Training loss for batch 5922 : 0.16189411282539368\n",
      "Training loss for batch 5923 : 0.19450163841247559\n",
      "Training loss for batch 5924 : 0.1513795554637909\n",
      "Training loss for batch 5925 : 0.09141058474779129\n",
      "Training loss for batch 5926 : 0.02214914560317993\n",
      "Training loss for batch 5927 : 0.10391724109649658\n",
      "Training loss for batch 5928 : 0.06448042392730713\n",
      "Training loss for batch 5929 : 0.08997971564531326\n",
      "Training loss for batch 5930 : 0.36095133423805237\n",
      "Training loss for batch 5931 : 0.07961065322160721\n",
      "Training loss for batch 5932 : 0.1533135622739792\n",
      "Training loss for batch 5933 : 0.14728839695453644\n",
      "Training loss for batch 5934 : 0.1480417102575302\n",
      "Training loss for batch 5935 : 0.33501067757606506\n",
      "Training loss for batch 5936 : 0.28366950154304504\n",
      "Training loss for batch 5937 : 0.31445035338401794\n",
      "Training loss for batch 5938 : 0.2655657231807709\n",
      "Training loss for batch 5939 : 0.2480572611093521\n",
      "Training loss for batch 5940 : 0.06952457875013351\n",
      "Training loss for batch 5941 : 0.013795505277812481\n",
      "Training loss for batch 5942 : 0.22728030383586884\n",
      "Training loss for batch 5943 : 0.18456269800662994\n",
      "Training loss for batch 5944 : 0.3031483292579651\n",
      "Training loss for batch 5945 : 0.08580261468887329\n",
      "Training loss for batch 5946 : 0.2234710454940796\n",
      "Training loss for batch 5947 : 0.11289195716381073\n",
      "Training loss for batch 5948 : 0.06936728209257126\n",
      "Training loss for batch 5949 : 0.41847237944602966\n",
      "Training loss for batch 5950 : 0.09257267415523529\n",
      "Training loss for batch 5951 : 0.20033134520053864\n",
      "Training loss for batch 5952 : 0.3694447875022888\n",
      "Training loss for batch 5953 : 0.11983644217252731\n",
      "Training loss for batch 5954 : 0.2568376660346985\n",
      "Training loss for batch 5955 : 0.2264765501022339\n",
      "Training loss for batch 5956 : 0.46516942977905273\n",
      "Training loss for batch 5957 : 0.23698420822620392\n",
      "Training loss for batch 5958 : 0.3488360643386841\n",
      "Training loss for batch 5959 : 0.21864348649978638\n",
      "Training loss for batch 5960 : 0.3617531359195709\n",
      "Training loss for batch 5961 : 0.26646292209625244\n",
      "Training loss for batch 5962 : 0.07963607460260391\n",
      "Training loss for batch 5963 : 0.10792140662670135\n",
      "Training loss for batch 5964 : 0.049256227910518646\n",
      "Training loss for batch 5965 : 0.21596476435661316\n",
      "Training loss for batch 5966 : 0.1202395111322403\n",
      "Training loss for batch 5967 : 0.04778854548931122\n",
      "Training loss for batch 5968 : 0.24858495593070984\n",
      "Training loss for batch 5969 : 0.17873352766036987\n",
      "Training loss for batch 5970 : 0.2312665581703186\n",
      "Training loss for batch 5971 : 0.03600461408495903\n",
      "Training loss for batch 5972 : 0.28469958901405334\n",
      "Training loss for batch 5973 : 0.06896577030420303\n",
      "Training loss for batch 5974 : 0.20733268558979034\n",
      "Training loss for batch 5975 : 0.13645124435424805\n",
      "Training loss for batch 5976 : 0.035771213471889496\n",
      "Training loss for batch 5977 : 0.10249481350183487\n",
      "Training loss for batch 5978 : 0.051635876297950745\n",
      "Training loss for batch 5979 : 0.03166729584336281\n",
      "Training loss for batch 5980 : 0.0800861045718193\n",
      "Training loss for batch 5981 : 0.10742074996232986\n",
      "Training loss for batch 5982 : 0.22692802548408508\n",
      "Training loss for batch 5983 : 0.16269303858280182\n",
      "Training loss for batch 5984 : 0.10701534897089005\n",
      "Training loss for batch 5985 : 0.01058538630604744\n",
      "Training loss for batch 5986 : 0.015223821625113487\n",
      "Training loss for batch 5987 : 0.12154316902160645\n",
      "Training loss for batch 5988 : 0.4438777565956116\n",
      "Training loss for batch 5989 : 0.40881168842315674\n",
      "Training loss for batch 5990 : 0.1883263736963272\n",
      "Training loss for batch 5991 : 0.11599747836589813\n",
      "Training loss for batch 5992 : 0.2163231521844864\n",
      "Training loss for batch 5993 : 0.258443146944046\n",
      "Training loss for batch 5994 : 0.23555444180965424\n",
      "Training loss for batch 5995 : 0.18187516927719116\n",
      "Training loss for batch 5996 : 0.15061628818511963\n",
      "Training loss for batch 5997 : 0.3012027442455292\n",
      "Training loss for batch 5998 : 0.4209817051887512\n",
      "Training loss for batch 5999 : 0.18898919224739075\n",
      "Training loss for batch 6000 : 0.029712537303566933\n",
      "Training loss for batch 6001 : 0.0739138275384903\n",
      "Training loss for batch 6002 : 0.02082815393805504\n",
      "Training loss for batch 6003 : 0.2111312747001648\n",
      "Training loss for batch 6004 : 0.14986677467823029\n",
      "Training loss for batch 6005 : 0.33883360028266907\n",
      "Training loss for batch 6006 : 0.3016492426395416\n",
      "Training loss for batch 6007 : 0.013851187191903591\n",
      "Training loss for batch 6008 : 0.4259627163410187\n",
      "Training loss for batch 6009 : 0.20305320620536804\n",
      "Training loss for batch 6010 : 0.000665942847263068\n",
      "Training loss for batch 6011 : 0.09718646854162216\n",
      "Training loss for batch 6012 : 0.014919938519597054\n",
      "Training loss for batch 6013 : 0.1890314370393753\n",
      "Training loss for batch 6014 : 0.07327847927808762\n",
      "Training loss for batch 6015 : 0.27468016743659973\n",
      "Training loss for batch 6016 : 0.13005979359149933\n",
      "Training loss for batch 6017 : 0.2076917588710785\n",
      "Training loss for batch 6018 : 0.36139971017837524\n",
      "Training loss for batch 6019 : 0.2588711977005005\n",
      "Training loss for batch 6020 : 0.04043582081794739\n",
      "Training loss for batch 6021 : 0.11319930851459503\n",
      "Training loss for batch 6022 : 0.5310639142990112\n",
      "Training loss for batch 6023 : 0.18366631865501404\n",
      "Training loss for batch 6024 : 0.28602132201194763\n",
      "Training loss for batch 6025 : 0.14622431993484497\n",
      "Training loss for batch 6026 : 0.10005517303943634\n",
      "Training loss for batch 6027 : 1.0155980587005615\n",
      "Training loss for batch 6028 : 0.01367002073675394\n",
      "Training loss for batch 6029 : 0.14557994902133942\n",
      "Training loss for batch 6030 : 0.0029307231307029724\n",
      "Training loss for batch 6031 : 0.020726725459098816\n",
      "Training loss for batch 6032 : 0.20894289016723633\n",
      "Training loss for batch 6033 : 0.37463152408599854\n",
      "Training loss for batch 6034 : 0.0\n",
      "Training loss for batch 6035 : 0.22953036427497864\n",
      "Training loss for batch 6036 : 0.06709995120763779\n",
      "Training loss for batch 6037 : 0.028776973485946655\n",
      "Training loss for batch 6038 : 0.4825994670391083\n",
      "Training loss for batch 6039 : 0.06074574962258339\n",
      "Training loss for batch 6040 : 0.2522585690021515\n",
      "Training loss for batch 6041 : 0.11026555299758911\n",
      "Training loss for batch 6042 : 0.09955278784036636\n",
      "Training loss for batch 6043 : 0.0012869935017079115\n",
      "Training loss for batch 6044 : 0.041874367743730545\n",
      "Training loss for batch 6045 : 0.6357231140136719\n",
      "Training loss for batch 6046 : 0.18363423645496368\n",
      "Training loss for batch 6047 : 0.48798564076423645\n",
      "Training loss for batch 6048 : 0.37588319182395935\n",
      "Training loss for batch 6049 : 0.32542991638183594\n",
      "Training loss for batch 6050 : 0.016112327575683594\n",
      "Training loss for batch 6051 : 0.3077930808067322\n",
      "Training loss for batch 6052 : 0.18239209055900574\n",
      "Training loss for batch 6053 : 0.028025157749652863\n",
      "Training loss for batch 6054 : 0.017418980598449707\n",
      "Training loss for batch 6055 : 0.20906329154968262\n",
      "Training loss for batch 6056 : 0.34059497714042664\n",
      "Training loss for batch 6057 : 0.17100034654140472\n",
      "Training loss for batch 6058 : 0.12621724605560303\n",
      "Training loss for batch 6059 : 0.04131956771016121\n",
      "Training loss for batch 6060 : 0.4147595465183258\n",
      "Training loss for batch 6061 : 0.10044796764850616\n",
      "Training loss for batch 6062 : 0.3369448184967041\n",
      "Training loss for batch 6063 : 0.10131414979696274\n",
      "Training loss for batch 6064 : 0.12961110472679138\n",
      "Training loss for batch 6065 : 0.4709424376487732\n",
      "Training loss for batch 6066 : 0.40098056197166443\n",
      "Training loss for batch 6067 : 0.7144492864608765\n",
      "Training loss for batch 6068 : 0.32283005118370056\n",
      "Training loss for batch 6069 : 0.07913002371788025\n",
      "Training loss for batch 6070 : 0.4986392557621002\n",
      "Training loss for batch 6071 : 0.016865825280547142\n",
      "Training loss for batch 6072 : 0.4039323031902313\n",
      "Training loss for batch 6073 : 0.32359421253204346\n",
      "Training loss for batch 6074 : 0.3153524696826935\n",
      "Training loss for batch 6075 : 0.019901882857084274\n",
      "Training loss for batch 6076 : 0.5237411856651306\n",
      "Training loss for batch 6077 : 0.28893017768859863\n",
      "Training loss for batch 6078 : 0.23727762699127197\n",
      "Training loss for batch 6079 : 0.09139609336853027\n",
      "Training loss for batch 6080 : 0.2554263174533844\n",
      "Training loss for batch 6081 : 0.013450682163238525\n",
      "Training loss for batch 6082 : 1.1071606874465942\n",
      "Training loss for batch 6083 : 0.16382518410682678\n",
      "Training loss for batch 6084 : 0.38183122873306274\n",
      "Training loss for batch 6085 : 0.11179123818874359\n",
      "Training loss for batch 6086 : 0.06403370201587677\n",
      "Training loss for batch 6087 : 0.33244559168815613\n",
      "Training loss for batch 6088 : 0.08299801498651505\n",
      "Training loss for batch 6089 : 0.26901042461395264\n",
      "Training loss for batch 6090 : 0.3324783146381378\n",
      "Training loss for batch 6091 : 0.16802430152893066\n",
      "Training loss for batch 6092 : 0.005581557750701904\n",
      "Training loss for batch 6093 : 0.28800228238105774\n",
      "Training loss for batch 6094 : 0.11957618594169617\n",
      "Training loss for batch 6095 : 0.8645973205566406\n",
      "Training loss for batch 6096 : 0.37021058797836304\n",
      "Training loss for batch 6097 : 0.269500195980072\n",
      "Training loss for batch 6098 : 0.001818110584281385\n",
      "Training loss for batch 6099 : 0.16511790454387665\n",
      "Training loss for batch 6100 : 0.1150609701871872\n",
      "Training loss for batch 6101 : 0.14437130093574524\n",
      "Training loss for batch 6102 : 0.15165023505687714\n",
      "Training loss for batch 6103 : 0.18359725177288055\n",
      "Training loss for batch 6104 : 0.0887996181845665\n",
      "Training loss for batch 6105 : 0.19853205978870392\n",
      "Training loss for batch 6106 : 0.06449397653341293\n",
      "Training loss for batch 6107 : 0.28477346897125244\n",
      "Training loss for batch 6108 : 0.10154726356267929\n",
      "Training loss for batch 6109 : 0.1533367931842804\n",
      "Training loss for batch 6110 : 0.10865148901939392\n",
      "Training loss for batch 6111 : 0.11023183912038803\n",
      "Training loss for batch 6112 : 0.18152853846549988\n",
      "Training loss for batch 6113 : 0.11307718604803085\n",
      "Training loss for batch 6114 : 0.17707858979701996\n",
      "Training loss for batch 6115 : 0.17683231830596924\n",
      "Training loss for batch 6116 : 0.06192990764975548\n",
      "Training loss for batch 6117 : 0.12947142124176025\n",
      "Training loss for batch 6118 : 0.10966508835554123\n",
      "Training loss for batch 6119 : 0.14058127999305725\n",
      "Training loss for batch 6120 : 0.16816343367099762\n",
      "Training loss for batch 6121 : 0.22434347867965698\n",
      "Training loss for batch 6122 : 0.19701135158538818\n",
      "Training loss for batch 6123 : 0.07074607163667679\n",
      "Training loss for batch 6124 : 0.10566237568855286\n",
      "Training loss for batch 6125 : 0.4998268187046051\n",
      "Training loss for batch 6126 : 0.3871983587741852\n",
      "Training loss for batch 6127 : 0.06976783275604248\n",
      "Training loss for batch 6128 : 0.1903236359357834\n",
      "Training loss for batch 6129 : 0.1679679900407791\n",
      "Training loss for batch 6130 : 0.06435225158929825\n",
      "Training loss for batch 6131 : 0.085282102227211\n",
      "Training loss for batch 6132 : 0.4958082437515259\n",
      "Training loss for batch 6133 : 0.10779942572116852\n",
      "Training loss for batch 6134 : 0.1639309674501419\n",
      "Training loss for batch 6135 : 0.08767186105251312\n",
      "Training loss for batch 6136 : 0.2802889943122864\n",
      "Training loss for batch 6137 : 0.19133172929286957\n",
      "Training loss for batch 6138 : 0.002233197446912527\n",
      "Training loss for batch 6139 : 0.39868468046188354\n",
      "Training loss for batch 6140 : 0.09684322774410248\n",
      "Training loss for batch 6141 : 0.12197220325469971\n",
      "Training loss for batch 6142 : 0.015204926952719688\n",
      "Training loss for batch 6143 : 0.42937207221984863\n",
      "Training loss for batch 6144 : 0.048623912036418915\n",
      "Training loss for batch 6145 : 0.07609066367149353\n",
      "Training loss for batch 6146 : 0.09162025898694992\n",
      "Training loss for batch 6147 : 0.18632884323596954\n",
      "Training loss for batch 6148 : 0.27877864241600037\n",
      "Training loss for batch 6149 : 0.16191977262496948\n",
      "Training loss for batch 6150 : 0.3102472722530365\n",
      "Training loss for batch 6151 : 0.1776202917098999\n",
      "Training loss for batch 6152 : 0.1807568520307541\n",
      "Training loss for batch 6153 : 0.38305309414863586\n",
      "Training loss for batch 6154 : 0.21150289475917816\n",
      "Training loss for batch 6155 : 0.018722759559750557\n",
      "Training loss for batch 6156 : 0.010417194105684757\n",
      "Training loss for batch 6157 : 0.26860514283180237\n",
      "Training loss for batch 6158 : 0.4143046736717224\n",
      "Training loss for batch 6159 : 0.18251003324985504\n",
      "Training loss for batch 6160 : 0.2067808359861374\n",
      "Training loss for batch 6161 : 0.40060746669769287\n",
      "Training loss for batch 6162 : 0.18174998462200165\n",
      "Training loss for batch 6163 : 0.2754364013671875\n",
      "Training loss for batch 6164 : 0.4603535532951355\n",
      "Training loss for batch 6165 : 0.0668107122182846\n",
      "Training loss for batch 6166 : 0.6287702322006226\n",
      "Training loss for batch 6167 : 0.09798550605773926\n",
      "Training loss for batch 6168 : 0.07322952151298523\n",
      "Training loss for batch 6169 : 0.06835059821605682\n",
      "Training loss for batch 6170 : 0.2545102536678314\n",
      "Training loss for batch 6171 : 0.17012228071689606\n",
      "Training loss for batch 6172 : 0.11213990300893784\n",
      "Training loss for batch 6173 : 0.10093622654676437\n",
      "Training loss for batch 6174 : 0.12222001701593399\n",
      "Training loss for batch 6175 : 0.41936200857162476\n",
      "Training loss for batch 6176 : 0.430288165807724\n",
      "Training loss for batch 6177 : 0.06488506495952606\n",
      "Training loss for batch 6178 : 0.08574096858501434\n",
      "Training loss for batch 6179 : 0.05988729000091553\n",
      "Training loss for batch 6180 : 0.14731228351593018\n",
      "Training loss for batch 6181 : 0.12508490681648254\n",
      "Training loss for batch 6182 : 0.1687198430299759\n",
      "Training loss for batch 6183 : 0.375955194234848\n",
      "Training loss for batch 6184 : 0.4513302743434906\n",
      "Training loss for batch 6185 : 0.11717119812965393\n",
      "Training loss for batch 6186 : 0.13324669003486633\n",
      "Training loss for batch 6187 : 0.07946716248989105\n",
      "Training loss for batch 6188 : 0.16104274988174438\n",
      "Training loss for batch 6189 : 0.09655918926000595\n",
      "Training loss for batch 6190 : 0.059830982238054276\n",
      "Training loss for batch 6191 : 0.3783346712589264\n",
      "Training loss for batch 6192 : 0.29159459471702576\n",
      "Training loss for batch 6193 : 0.3324161767959595\n",
      "Training loss for batch 6194 : 0.26043960452079773\n",
      "Training loss for batch 6195 : 0.04838082194328308\n",
      "Training loss for batch 6196 : 0.18043217062950134\n",
      "Training loss for batch 6197 : 0.04450700059533119\n",
      "Training loss for batch 6198 : 0.1098090261220932\n",
      "Training loss for batch 6199 : 0.11691826581954956\n",
      "Training loss for batch 6200 : 0.11164404451847076\n",
      "Training loss for batch 6201 : 0.28857991099357605\n",
      "Training loss for batch 6202 : 0.49442848563194275\n",
      "Training loss for batch 6203 : 0.39750340580940247\n",
      "Training loss for batch 6204 : 0.12526994943618774\n",
      "Training loss for batch 6205 : 0.4327808618545532\n",
      "Training loss for batch 6206 : 0.11862196773290634\n",
      "Training loss for batch 6207 : 0.1195363700389862\n",
      "Training loss for batch 6208 : 0.3200487792491913\n",
      "Training loss for batch 6209 : 0.11808531731367111\n",
      "Training loss for batch 6210 : 0.07192981988191605\n",
      "Training loss for batch 6211 : 0.33919844031333923\n",
      "Training loss for batch 6212 : 0.32468533515930176\n",
      "Training loss for batch 6213 : 0.1631714552640915\n",
      "Training loss for batch 6214 : 0.1683862954378128\n",
      "Training loss for batch 6215 : 0.0443338006734848\n",
      "Training loss for batch 6216 : 0.09224805235862732\n",
      "Training loss for batch 6217 : 0.14359799027442932\n",
      "Training loss for batch 6218 : 0.19021223485469818\n",
      "Training loss for batch 6219 : 0.20093348622322083\n",
      "Training loss for batch 6220 : 0.1300690770149231\n",
      "Training loss for batch 6221 : 0.009480416774749756\n",
      "Training loss for batch 6222 : 0.16148123145103455\n",
      "Training loss for batch 6223 : 0.10691387951374054\n",
      "Training loss for batch 6224 : 0.23069293797016144\n",
      "Training loss for batch 6225 : 0.29772529006004333\n",
      "Training loss for batch 6226 : 0.3363303542137146\n",
      "Training loss for batch 6227 : 0.1817174106836319\n",
      "Training loss for batch 6228 : 0.22895567119121552\n",
      "Training loss for batch 6229 : 0.09043647348880768\n",
      "Training loss for batch 6230 : 0.15239375829696655\n",
      "Training loss for batch 6231 : 0.10476236045360565\n",
      "Training loss for batch 6232 : 0.07662397623062134\n",
      "Training loss for batch 6233 : 0.17056606709957123\n",
      "Training loss for batch 6234 : 0.35813790559768677\n",
      "Training loss for batch 6235 : 0.23797668516635895\n",
      "Training loss for batch 6236 : 0.12584805488586426\n",
      "Training loss for batch 6237 : 0.21341195702552795\n",
      "Training loss for batch 6238 : 0.28109675645828247\n",
      "Training loss for batch 6239 : 0.08451611548662186\n",
      "Training loss for batch 6240 : 0.3363144099712372\n",
      "Training loss for batch 6241 : 0.23475901782512665\n",
      "Training loss for batch 6242 : 0.23364005982875824\n",
      "Training loss for batch 6243 : 0.12553058564662933\n",
      "Training loss for batch 6244 : 0.06738568842411041\n",
      "Training loss for batch 6245 : 0.42094311118125916\n",
      "Training loss for batch 6246 : 0.3785504996776581\n",
      "Training loss for batch 6247 : 0.081303671002388\n",
      "Training loss for batch 6248 : 0.060834817588329315\n",
      "Training loss for batch 6249 : 0.11361990869045258\n",
      "Training loss for batch 6250 : 0.08988358080387115\n",
      "Training loss for batch 6251 : 0.21857325732707977\n",
      "Training loss for batch 6252 : 0.43891483545303345\n",
      "Training loss for batch 6253 : 0.2229357361793518\n",
      "Training loss for batch 6254 : 0.441082239151001\n",
      "Training loss for batch 6255 : 0.1480051875114441\n",
      "Training loss for batch 6256 : 0.11992687731981277\n",
      "Training loss for batch 6257 : 0.3834877908229828\n",
      "Training loss for batch 6258 : 0.24165789783000946\n",
      "Training loss for batch 6259 : 0.08466183394193649\n",
      "Training loss for batch 6260 : 0.30003419518470764\n",
      "Training loss for batch 6261 : 0.17908063530921936\n",
      "Training loss for batch 6262 : 0.17593877017498016\n",
      "Training loss for batch 6263 : 0.09713517129421234\n",
      "Training loss for batch 6264 : 0.14695952832698822\n",
      "Training loss for batch 6265 : 0.05802418664097786\n",
      "Training loss for batch 6266 : 0.02157047763466835\n",
      "Training loss for batch 6267 : 0.14250676333904266\n",
      "Training loss for batch 6268 : 0.20139256119728088\n",
      "Training loss for batch 6269 : 0.022325020283460617\n",
      "Training loss for batch 6270 : 0.3159225285053253\n",
      "Training loss for batch 6271 : 0.0899280533194542\n",
      "Training loss for batch 6272 : 0.17718961834907532\n",
      "Training loss for batch 6273 : 0.08072871714830399\n",
      "Training loss for batch 6274 : 0.13928285241127014\n",
      "Training loss for batch 6275 : 0.17954576015472412\n",
      "Training loss for batch 6276 : 0.18405380845069885\n",
      "Training loss for batch 6277 : 0.31573134660720825\n",
      "Training loss for batch 6278 : 0.27544668316841125\n",
      "Training loss for batch 6279 : 0.20263735949993134\n",
      "Training loss for batch 6280 : 0.08696043491363525\n",
      "Training loss for batch 6281 : 0.10855500400066376\n",
      "Training loss for batch 6282 : 0.1918887495994568\n",
      "Training loss for batch 6283 : 0.24815203249454498\n",
      "Training loss for batch 6284 : 0.21369659900665283\n",
      "Training loss for batch 6285 : 0.21845318377017975\n",
      "Training loss for batch 6286 : 0.3444637060165405\n",
      "Training loss for batch 6287 : 0.16361181437969208\n",
      "Training loss for batch 6288 : 0.19622935354709625\n",
      "Training loss for batch 6289 : 0.0748550295829773\n",
      "Training loss for batch 6290 : 0.2990966737270355\n",
      "Training loss for batch 6291 : 0.17254383862018585\n",
      "Training loss for batch 6292 : 0.2018827646970749\n",
      "Training loss for batch 6293 : 0.32461678981781006\n",
      "Training loss for batch 6294 : 0.08226723968982697\n",
      "Training loss for batch 6295 : 0.3355291187763214\n",
      "Training loss for batch 6296 : 0.06711068749427795\n",
      "Training loss for batch 6297 : 0.14529359340667725\n",
      "Training loss for batch 6298 : 0.21970221400260925\n",
      "Training loss for batch 6299 : 0.2567150592803955\n",
      "Training loss for batch 6300 : 0.08109688758850098\n",
      "Training loss for batch 6301 : 0.22645798325538635\n",
      "Training loss for batch 6302 : 0.17467474937438965\n",
      "Training loss for batch 6303 : 0.43773192167282104\n",
      "Training loss for batch 6304 : 0.02758963592350483\n",
      "Training loss for batch 6305 : 0.15684473514556885\n",
      "Training loss for batch 6306 : 0.2698134779930115\n",
      "Training loss for batch 6307 : 0.35004565119743347\n",
      "Training loss for batch 6308 : 0.10030225664377213\n",
      "Training loss for batch 6309 : 0.08862002193927765\n",
      "Training loss for batch 6310 : 0.1777961552143097\n",
      "Training loss for batch 6311 : 0.28410640358924866\n",
      "Training loss for batch 6312 : 0.006989042274653912\n",
      "Training loss for batch 6313 : 0.278296560049057\n",
      "Training loss for batch 6314 : 0.03280957043170929\n",
      "Training loss for batch 6315 : 0.013844192028045654\n",
      "Training loss for batch 6316 : 0.2207615077495575\n",
      "Training loss for batch 6317 : 0.038905899971723557\n",
      "Training loss for batch 6318 : 0.05586492642760277\n",
      "Training loss for batch 6319 : 0.07293768227100372\n",
      "Training loss for batch 6320 : 0.38366979360580444\n",
      "Training loss for batch 6321 : 0.16402888298034668\n",
      "Training loss for batch 6322 : 0.2011048048734665\n",
      "Training loss for batch 6323 : 0.27951931953430176\n",
      "Training loss for batch 6324 : 0.09457793086767197\n",
      "Training loss for batch 6325 : 0.24229620397090912\n",
      "Training loss for batch 6326 : 0.11901365220546722\n",
      "Training loss for batch 6327 : 0.2281826138496399\n",
      "Training loss for batch 6328 : 0.3310485780239105\n",
      "Training loss for batch 6329 : 0.23807507753372192\n",
      "Training loss for batch 6330 : 0.15624427795410156\n",
      "Training loss for batch 6331 : 0.10484427213668823\n",
      "Training loss for batch 6332 : 0.2984442412853241\n",
      "Training loss for batch 6333 : 0.5009095668792725\n",
      "Training loss for batch 6334 : 0.2458149641752243\n",
      "Training loss for batch 6335 : 0.1278720498085022\n",
      "Training loss for batch 6336 : 0.15090340375900269\n",
      "Training loss for batch 6337 : 0.48824742436408997\n",
      "Training loss for batch 6338 : 0.05001845210790634\n",
      "Training loss for batch 6339 : 0.46593958139419556\n",
      "Training loss for batch 6340 : 0.018904831260442734\n",
      "Training loss for batch 6341 : 0.14083196222782135\n",
      "Training loss for batch 6342 : 0.4346371591091156\n",
      "Training loss for batch 6343 : 0.03907819092273712\n",
      "Training loss for batch 6344 : 0.2813713848590851\n",
      "Training loss for batch 6345 : 0.14992308616638184\n",
      "Training loss for batch 6346 : 0.3567369282245636\n",
      "Training loss for batch 6347 : -0.0006106736254878342\n",
      "Training loss for batch 6348 : 0.4641580581665039\n",
      "Training loss for batch 6349 : 0.38594451546669006\n",
      "Training loss for batch 6350 : 0.13439930975437164\n",
      "Training loss for batch 6351 : 0.18446490168571472\n",
      "Training loss for batch 6352 : 0.33917149901390076\n",
      "Training loss for batch 6353 : 0.2541123330593109\n",
      "Training loss for batch 6354 : 0.14089500904083252\n",
      "Training loss for batch 6355 : 0.1887088567018509\n",
      "Training loss for batch 6356 : 0.12480141967535019\n",
      "Training loss for batch 6357 : 0.46582773327827454\n",
      "Training loss for batch 6358 : 0.027260133996605873\n",
      "Training loss for batch 6359 : 0.10508101433515549\n",
      "Training loss for batch 6360 : 0.20136110484600067\n",
      "Training loss for batch 6361 : 0.030800698325037956\n",
      "Training loss for batch 6362 : 0.1999032348394394\n",
      "Training loss for batch 6363 : 0.455786794424057\n",
      "Training loss for batch 6364 : 0.06826555728912354\n",
      "Training loss for batch 6365 : 0.08086905628442764\n",
      "Training loss for batch 6366 : 0.12882357835769653\n",
      "Training loss for batch 6367 : 0.376936137676239\n",
      "Training loss for batch 6368 : 0.06454213708639145\n",
      "Training loss for batch 6369 : 0.010387679561972618\n",
      "Training loss for batch 6370 : 0.2834096848964691\n",
      "Training loss for batch 6371 : 0.05755750462412834\n",
      "Training loss for batch 6372 : 0.09684640914201736\n",
      "Training loss for batch 6373 : 0.056238483637571335\n",
      "Training loss for batch 6374 : 0.0933188870549202\n",
      "Training loss for batch 6375 : 0.02399037405848503\n",
      "Training loss for batch 6376 : 0.15752705931663513\n",
      "Training loss for batch 6377 : 0.28984197974205017\n",
      "Training loss for batch 6378 : 0.022974850609898567\n",
      "Training loss for batch 6379 : 0.17792059481143951\n",
      "Training loss for batch 6380 : 0.09929568320512772\n",
      "Training loss for batch 6381 : 0.3633022904396057\n",
      "Training loss for batch 6382 : 0.13782548904418945\n",
      "Training loss for batch 6383 : 0.4861040413379669\n",
      "Training loss for batch 6384 : 0.0723276287317276\n",
      "Training loss for batch 6385 : 0.21162399649620056\n",
      "Training loss for batch 6386 : 0.12319016456604004\n",
      "Training loss for batch 6387 : 0.023928511887788773\n",
      "Training loss for batch 6388 : 0.03823437914252281\n",
      "Training loss for batch 6389 : 0.28500062227249146\n",
      "Training loss for batch 6390 : 0.32290077209472656\n",
      "Training loss for batch 6391 : 0.16252800822257996\n",
      "Training loss for batch 6392 : 0.5316295623779297\n",
      "Training loss for batch 6393 : 0.04328927397727966\n",
      "Training loss for batch 6394 : 0.19248716533184052\n",
      "Training loss for batch 6395 : 0.0057349903509020805\n",
      "Training loss for batch 6396 : 0.22138597071170807\n",
      "Training loss for batch 6397 : 0.007596026174724102\n",
      "Training loss for batch 6398 : 0.017195463180541992\n",
      "Training loss for batch 6399 : 0.12686757743358612\n",
      "Training loss for batch 6400 : 0.1302165985107422\n",
      "Training loss for batch 6401 : 0.40696266293525696\n",
      "Training loss for batch 6402 : 0.07828966528177261\n",
      "Training loss for batch 6403 : 0.13449883460998535\n",
      "Training loss for batch 6404 : 0.200381338596344\n",
      "Training loss for batch 6405 : 0.20440667867660522\n",
      "Training loss for batch 6406 : 0.06964600831270218\n",
      "Training loss for batch 6407 : 0.031307000666856766\n",
      "Training loss for batch 6408 : 0.06964638829231262\n",
      "Training loss for batch 6409 : 0.26381027698516846\n",
      "Training loss for batch 6410 : 0.07298088818788528\n",
      "Training loss for batch 6411 : 0.30148234963417053\n",
      "Training loss for batch 6412 : 0.21899043023586273\n",
      "Training loss for batch 6413 : 0.209438756108284\n",
      "Training loss for batch 6414 : 0.32802316546440125\n",
      "Training loss for batch 6415 : 0.3498651087284088\n",
      "Training loss for batch 6416 : 0.08664361387491226\n",
      "Training loss for batch 6417 : 0.15442045032978058\n",
      "Training loss for batch 6418 : 0.36217066645622253\n",
      "Training loss for batch 6419 : 0.03424181789159775\n",
      "Training loss for batch 6420 : 0.17676448822021484\n",
      "Training loss for batch 6421 : 0.32996466755867004\n",
      "Training loss for batch 6422 : 0.2152508795261383\n",
      "Training loss for batch 6423 : 0.489027738571167\n",
      "Training loss for batch 6424 : 0.09469597786664963\n",
      "Training loss for batch 6425 : 0.08574365079402924\n",
      "Training loss for batch 6426 : 0.21863165497779846\n",
      "Training loss for batch 6427 : 0.06257403641939163\n",
      "Training loss for batch 6428 : 0.0437927171587944\n",
      "Training loss for batch 6429 : 0.11836564540863037\n",
      "Training loss for batch 6430 : 0.23378384113311768\n",
      "Training loss for batch 6431 : 0.26297464966773987\n",
      "Training loss for batch 6432 : 0.10081995278596878\n",
      "Training loss for batch 6433 : 0.4108709394931793\n",
      "Training loss for batch 6434 : 0.02085570991039276\n",
      "Training loss for batch 6435 : 0.08560498058795929\n",
      "Training loss for batch 6436 : 0.30391713976860046\n",
      "Training loss for batch 6437 : 0.00472878385335207\n",
      "Training loss for batch 6438 : 0.10078660398721695\n",
      "Training loss for batch 6439 : 0.152857705950737\n",
      "Training loss for batch 6440 : 0.05035732313990593\n",
      "Training loss for batch 6441 : 0.31874921917915344\n",
      "Training loss for batch 6442 : 0.2798851728439331\n",
      "Training loss for batch 6443 : 0.11936566978693008\n",
      "Training loss for batch 6444 : 0.1916762888431549\n",
      "Training loss for batch 6445 : 0.3359248638153076\n",
      "Training loss for batch 6446 : 0.008514229208230972\n",
      "Training loss for batch 6447 : 0.0\n",
      "Training loss for batch 6448 : 0.3338351547718048\n",
      "Training loss for batch 6449 : 0.22540070116519928\n",
      "Training loss for batch 6450 : 0.22690153121948242\n",
      "Training loss for batch 6451 : 0.26271289587020874\n",
      "Training loss for batch 6452 : 0.1475655436515808\n",
      "Training loss for batch 6453 : 0.37223371863365173\n",
      "Training loss for batch 6454 : 0.08542288094758987\n",
      "Training loss for batch 6455 : 0.3637922704219818\n",
      "Training loss for batch 6456 : 0.3903278112411499\n",
      "Training loss for batch 6457 : 0.1581140160560608\n",
      "Training loss for batch 6458 : 0.3003917932510376\n",
      "Training loss for batch 6459 : 0.004758050199598074\n",
      "Training loss for batch 6460 : 0.009321861900389194\n",
      "Training loss for batch 6461 : 0.2725078761577606\n",
      "Training loss for batch 6462 : 0.12389348447322845\n",
      "Training loss for batch 6463 : 0.15812425315380096\n",
      "Training loss for batch 6464 : 0.12849944829940796\n",
      "Training loss for batch 6465 : 0.4141654968261719\n",
      "Training loss for batch 6466 : 0.030307691544294357\n",
      "Training loss for batch 6467 : 0.057073306292295456\n",
      "Training loss for batch 6468 : 0.02887771651148796\n",
      "Training loss for batch 6469 : 0.16462475061416626\n",
      "Training loss for batch 6470 : 0.24422064423561096\n",
      "Training loss for batch 6471 : 0.10196200013160706\n",
      "Training loss for batch 6472 : 0.04682398959994316\n",
      "Training loss for batch 6473 : 0.3961164057254791\n",
      "Training loss for batch 6474 : 0.2360733449459076\n",
      "Training loss for batch 6475 : 0.24899618327617645\n",
      "Training loss for batch 6476 : 0.029961660504341125\n",
      "Training loss for batch 6477 : 0.7119609713554382\n",
      "Training loss for batch 6478 : 0.36223670840263367\n",
      "Training loss for batch 6479 : 0.08647069334983826\n",
      "Training loss for batch 6480 : 0.04833907634019852\n",
      "Training loss for batch 6481 : 0.1632726788520813\n",
      "Training loss for batch 6482 : 0.3784274458885193\n",
      "Training loss for batch 6483 : 0.17176492512226105\n",
      "Training loss for batch 6484 : 0.09476975351572037\n",
      "Training loss for batch 6485 : 0.19126616418361664\n",
      "Training loss for batch 6486 : 0.03371071442961693\n",
      "Training loss for batch 6487 : 0.043337132781744\n",
      "Training loss for batch 6488 : 0.2088523805141449\n",
      "Training loss for batch 6489 : 0.006123940460383892\n",
      "Training loss for batch 6490 : 0.36155712604522705\n",
      "Training loss for batch 6491 : 0.17136573791503906\n",
      "Training loss for batch 6492 : 0.09621235728263855\n",
      "Training loss for batch 6493 : 0.01656031794846058\n",
      "Training loss for batch 6494 : 0.036745473742485046\n",
      "Training loss for batch 6495 : 0.052563030272722244\n",
      "Training loss for batch 6496 : 0.10799673944711685\n",
      "Training loss for batch 6497 : 0.6089766621589661\n",
      "Training loss for batch 6498 : 0.1032281443476677\n",
      "Training loss for batch 6499 : 0.055035900324583054\n",
      "Training loss for batch 6500 : 0.21495278179645538\n",
      "Training loss for batch 6501 : 0.35046711564064026\n",
      "Training loss for batch 6502 : 0.07131744921207428\n",
      "Training loss for batch 6503 : 0.20244188606739044\n",
      "Training loss for batch 6504 : 0.01759522221982479\n",
      "Training loss for batch 6505 : 0.324432373046875\n",
      "Training loss for batch 6506 : 0.13964292407035828\n",
      "Training loss for batch 6507 : 0.033763933926820755\n",
      "Training loss for batch 6508 : 0.29069527983665466\n",
      "Training loss for batch 6509 : 0.004608686547726393\n",
      "Training loss for batch 6510 : 0.17320981621742249\n",
      "Training loss for batch 6511 : 0.326937198638916\n",
      "Training loss for batch 6512 : 0.018170177936553955\n",
      "Training loss for batch 6513 : 0.17107023298740387\n",
      "Training loss for batch 6514 : 0.806391716003418\n",
      "Training loss for batch 6515 : 0.044323619455099106\n",
      "Training loss for batch 6516 : 0.38004598021507263\n",
      "Training loss for batch 6517 : 0.13729846477508545\n",
      "Training loss for batch 6518 : 0.028190260753035545\n",
      "Training loss for batch 6519 : 0.038465749472379684\n",
      "Training loss for batch 6520 : 0.014237701892852783\n",
      "Training loss for batch 6521 : 0.13662974536418915\n",
      "Training loss for batch 6522 : 0.3597533702850342\n",
      "Training loss for batch 6523 : 0.07477486878633499\n",
      "Training loss for batch 6524 : 0.2486664354801178\n",
      "Training loss for batch 6525 : 0.48577237129211426\n",
      "Training loss for batch 6526 : 0.28106415271759033\n",
      "Training loss for batch 6527 : 0.06994350999593735\n",
      "Training loss for batch 6528 : 0.1529814749956131\n",
      "Training loss for batch 6529 : 0.309476763010025\n",
      "Training loss for batch 6530 : 0.2502697706222534\n",
      "Training loss for batch 6531 : 0.07217061519622803\n",
      "Training loss for batch 6532 : 0.0331130251288414\n",
      "Training loss for batch 6533 : 0.4860284626483917\n",
      "Training loss for batch 6534 : 0.13338634371757507\n",
      "Training loss for batch 6535 : 0.13787373900413513\n",
      "Training loss for batch 6536 : 0.16890844702720642\n",
      "Training loss for batch 6537 : 0.35961291193962097\n",
      "Training loss for batch 6538 : 0.0\n",
      "Training loss for batch 6539 : 0.20031245052814484\n",
      "Training loss for batch 6540 : 0.10970966517925262\n",
      "Training loss for batch 6541 : 0.14350588619709015\n",
      "Training loss for batch 6542 : 0.11230185627937317\n",
      "Training loss for batch 6543 : 0.24923859536647797\n",
      "Training loss for batch 6544 : 0.41415777802467346\n",
      "Training loss for batch 6545 : 0.20422838628292084\n",
      "Training loss for batch 6546 : 0.13344864547252655\n",
      "Training loss for batch 6547 : 0.28670766949653625\n",
      "Training loss for batch 6548 : 0.34293198585510254\n",
      "Training loss for batch 6549 : 0.673793613910675\n",
      "Training loss for batch 6550 : 0.02852766588330269\n",
      "Training loss for batch 6551 : 0.31367021799087524\n",
      "Training loss for batch 6552 : 0.19255389273166656\n",
      "Training loss for batch 6553 : 0.24375690519809723\n",
      "Training loss for batch 6554 : 0.11967466026544571\n",
      "Training loss for batch 6555 : 0.14308515191078186\n",
      "Training loss for batch 6556 : 0.11626904457807541\n",
      "Training loss for batch 6557 : 0.15434831380844116\n",
      "Training loss for batch 6558 : 0.1647171974182129\n",
      "Training loss for batch 6559 : 0.5102303624153137\n",
      "Training loss for batch 6560 : 0.14991557598114014\n",
      "Training loss for batch 6561 : -0.00021176273003220558\n",
      "Training loss for batch 6562 : 0.5926150679588318\n",
      "Training loss for batch 6563 : 0.06453657895326614\n",
      "Training loss for batch 6564 : 0.24548247456550598\n",
      "Training loss for batch 6565 : 0.10742950439453125\n",
      "Training loss for batch 6566 : 0.3564315736293793\n",
      "Training loss for batch 6567 : 0.30841556191444397\n",
      "Training loss for batch 6568 : 0.30288368463516235\n",
      "Training loss for batch 6569 : 0.30349022150039673\n",
      "Training loss for batch 6570 : 0.3015174865722656\n",
      "Training loss for batch 6571 : 0.036604903638362885\n",
      "Training loss for batch 6572 : 0.2701209783554077\n",
      "Training loss for batch 6573 : 0.09485875815153122\n",
      "Training loss for batch 6574 : 0.10383473336696625\n",
      "Training loss for batch 6575 : 0.25796639919281006\n",
      "Training loss for batch 6576 : 0.056031033396720886\n",
      "Training loss for batch 6577 : 0.10463511943817139\n",
      "Training loss for batch 6578 : 0.20036372542381287\n",
      "Training loss for batch 6579 : 0.00831811223179102\n",
      "Training loss for batch 6580 : 0.08237574249505997\n",
      "Training loss for batch 6581 : 0.04389956593513489\n",
      "Training loss for batch 6582 : 0.1482517421245575\n",
      "Training loss for batch 6583 : 0.0721014142036438\n",
      "Training loss for batch 6584 : 0.2068266123533249\n",
      "Training loss for batch 6585 : 0.11306721717119217\n",
      "Training loss for batch 6586 : 0.2017616629600525\n",
      "Training loss for batch 6587 : 0.027984093874692917\n",
      "Training loss for batch 6588 : 0.08659359812736511\n",
      "Training loss for batch 6589 : 0.29483291506767273\n",
      "Training loss for batch 6590 : 0.0444754995405674\n",
      "Training loss for batch 6591 : 0.3525870144367218\n",
      "Training loss for batch 6592 : 0.07163137197494507\n",
      "Training loss for batch 6593 : 0.23884235322475433\n",
      "Training loss for batch 6594 : 0.12363159656524658\n",
      "Training loss for batch 6595 : 0.023379474878311157\n",
      "Training loss for batch 6596 : 0.41262736916542053\n",
      "Training loss for batch 6597 : 0.32955417037010193\n",
      "Training loss for batch 6598 : 0.2609359323978424\n",
      "Training loss for batch 6599 : 0.7634043097496033\n",
      "Training loss for batch 6600 : 0.038464564830064774\n",
      "Training loss for batch 6601 : 0.2002495378255844\n",
      "Training loss for batch 6602 : 0.08424428105354309\n",
      "Training loss for batch 6603 : 0.1088390052318573\n",
      "Training loss for batch 6604 : 0.23952467739582062\n",
      "Training loss for batch 6605 : 0.0\n",
      "Training loss for batch 6606 : 0.017960790544748306\n",
      "Training loss for batch 6607 : 0.23092737793922424\n",
      "Training loss for batch 6608 : 0.21590793132781982\n",
      "Training loss for batch 6609 : 0.2400335967540741\n",
      "Training loss for batch 6610 : 0.5083695650100708\n",
      "Training loss for batch 6611 : 0.2918926477432251\n",
      "Training loss for batch 6612 : 0.218479186296463\n",
      "Training loss for batch 6613 : 0.1361282467842102\n",
      "Training loss for batch 6614 : 0.13557833433151245\n",
      "Training loss for batch 6615 : 0.081401027739048\n",
      "Training loss for batch 6616 : 0.1246216744184494\n",
      "Training loss for batch 6617 : 0.13673989474773407\n",
      "Training loss for batch 6618 : 0.3006049394607544\n",
      "Training loss for batch 6619 : 0.30196088552474976\n",
      "Training loss for batch 6620 : 0.029163897037506104\n",
      "Training loss for batch 6621 : 0.05928421765565872\n",
      "Training loss for batch 6622 : 0.19581174850463867\n",
      "Training loss for batch 6623 : 0.5435379147529602\n",
      "Training loss for batch 6624 : 0.2860634922981262\n",
      "Training loss for batch 6625 : 0.19271168112754822\n",
      "Training loss for batch 6626 : 0.012183168902993202\n",
      "Training loss for batch 6627 : 0.27772560715675354\n",
      "Training loss for batch 6628 : 0.08633061498403549\n",
      "Training loss for batch 6629 : 0.08437780290842056\n",
      "Training loss for batch 6630 : 0.09977288544178009\n",
      "Training loss for batch 6631 : 0.25425300002098083\n",
      "Training loss for batch 6632 : 0.08654433488845825\n",
      "Training loss for batch 6633 : 0.1519448608160019\n",
      "Training loss for batch 6634 : 0.03254619613289833\n",
      "Training loss for batch 6635 : 0.25679779052734375\n",
      "Training loss for batch 6636 : 0.3029431998729706\n",
      "Training loss for batch 6637 : 0.2490396648645401\n",
      "Training loss for batch 6638 : 0.013347744941711426\n",
      "Training loss for batch 6639 : 0.41582053899765015\n",
      "Training loss for batch 6640 : 0.1491355448961258\n",
      "Training loss for batch 6641 : 0.2518395185470581\n",
      "Training loss for batch 6642 : 0.11257310956716537\n",
      "Training loss for batch 6643 : 0.07878328859806061\n",
      "Training loss for batch 6644 : 0.37458378076553345\n",
      "Training loss for batch 6645 : 0.1881694495677948\n",
      "Training loss for batch 6646 : 0.15162062644958496\n",
      "Training loss for batch 6647 : 0.06505182385444641\n",
      "Training loss for batch 6648 : 0.04059121012687683\n",
      "Training loss for batch 6649 : 0.1615598201751709\n",
      "Training loss for batch 6650 : 0.29571840167045593\n",
      "Training loss for batch 6651 : 0.2997762858867645\n",
      "Training loss for batch 6652 : 0.44404101371765137\n",
      "Training loss for batch 6653 : 0.1157940998673439\n",
      "Training loss for batch 6654 : 0.42520108819007874\n",
      "Training loss for batch 6655 : 0.36367157101631165\n",
      "Training loss for batch 6656 : 0.2469063550233841\n",
      "Training loss for batch 6657 : 0.36110079288482666\n",
      "Training loss for batch 6658 : 0.06041669845581055\n",
      "Training loss for batch 6659 : 0.1770552545785904\n",
      "Training loss for batch 6660 : 0.02002290077507496\n",
      "Training loss for batch 6661 : 0.2034684270620346\n",
      "Training loss for batch 6662 : 0.10851447284221649\n",
      "Training loss for batch 6663 : 0.07937794178724289\n",
      "Training loss for batch 6664 : 0.38363224267959595\n",
      "Training loss for batch 6665 : 0.1467166244983673\n",
      "Training loss for batch 6666 : 0.07900860160589218\n",
      "Training loss for batch 6667 : 0.03644351288676262\n",
      "Training loss for batch 6668 : 0.19043852388858795\n",
      "Training loss for batch 6669 : 0.36280006170272827\n",
      "Training loss for batch 6670 : 0.08919287472963333\n",
      "Training loss for batch 6671 : 0.05268007516860962\n",
      "Training loss for batch 6672 : 0.1711527705192566\n",
      "Training loss for batch 6673 : 0.28873327374458313\n",
      "Training loss for batch 6674 : 0.1036340519785881\n",
      "Training loss for batch 6675 : 0.03209303319454193\n",
      "Training loss for batch 6676 : 0.23917771875858307\n",
      "Training loss for batch 6677 : 0.07679926604032516\n",
      "Training loss for batch 6678 : 0.027540026232600212\n",
      "Training loss for batch 6679 : 0.03858531638979912\n",
      "Training loss for batch 6680 : 0.050799351185560226\n",
      "Training loss for batch 6681 : 0.33325302600860596\n",
      "Training loss for batch 6682 : 0.31664320826530457\n",
      "Training loss for batch 6683 : 0.16869312524795532\n",
      "Training loss for batch 6684 : 0.2664444148540497\n",
      "Training loss for batch 6685 : 0.1568252146244049\n",
      "Training loss for batch 6686 : 0.14118902385234833\n",
      "Training loss for batch 6687 : 0.0909288302063942\n",
      "Training loss for batch 6688 : 0.15537160634994507\n",
      "Training loss for batch 6689 : 0.460391640663147\n",
      "Training loss for batch 6690 : 0.05598815158009529\n",
      "Training loss for batch 6691 : 0.2947113811969757\n",
      "Training loss for batch 6692 : 0.33454737067222595\n",
      "Training loss for batch 6693 : 0.11782548576593399\n",
      "Training loss for batch 6694 : 0.17229996621608734\n",
      "Training loss for batch 6695 : 0.27451345324516296\n",
      "Training loss for batch 6696 : 0.1915035843849182\n",
      "Training loss for batch 6697 : 0.12071166932582855\n",
      "Training loss for batch 6698 : 0.20123834908008575\n",
      "Training loss for batch 6699 : 0.12366701662540436\n",
      "Training loss for batch 6700 : 0.37917083501815796\n",
      "Training loss for batch 6701 : 0.3922669291496277\n",
      "Training loss for batch 6702 : 0.07787448167800903\n",
      "Training loss for batch 6703 : 0.3121525049209595\n",
      "Training loss for batch 6704 : 0.05042129009962082\n",
      "Training loss for batch 6705 : 0.02527763694524765\n",
      "Training loss for batch 6706 : 0.19592630863189697\n",
      "Training loss for batch 6707 : 0.1433020383119583\n",
      "Training loss for batch 6708 : 0.1696995496749878\n",
      "Training loss for batch 6709 : 0.12703672051429749\n",
      "Training loss for batch 6710 : 0.2613801956176758\n",
      "Training loss for batch 6711 : 0.5151515603065491\n",
      "Training loss for batch 6712 : 0.069518081843853\n",
      "Training loss for batch 6713 : 0.2388998419046402\n",
      "Training loss for batch 6714 : 0.3221204876899719\n",
      "Training loss for batch 6715 : 0.11134923994541168\n",
      "Training loss for batch 6716 : 0.17673780024051666\n",
      "Training loss for batch 6717 : 0.04717351868748665\n",
      "Training loss for batch 6718 : 0.13863816857337952\n",
      "Training loss for batch 6719 : 0.13513970375061035\n",
      "Training loss for batch 6720 : 0.01894637942314148\n",
      "Training loss for batch 6721 : 0.1391485631465912\n",
      "Training loss for batch 6722 : 0.028378605842590332\n",
      "Training loss for batch 6723 : 0.20911160111427307\n",
      "Training loss for batch 6724 : 0.41946014761924744\n",
      "Training loss for batch 6725 : 0.03618396818637848\n",
      "Training loss for batch 6726 : 0.1070292741060257\n",
      "Training loss for batch 6727 : 0.25555121898651123\n",
      "Training loss for batch 6728 : 0.32970044016838074\n",
      "Training loss for batch 6729 : 0.018631132319569588\n",
      "Training loss for batch 6730 : 0.12361470609903336\n",
      "Training loss for batch 6731 : 0.1913461685180664\n",
      "Training loss for batch 6732 : 0.17387302219867706\n",
      "Training loss for batch 6733 : 0.04307321831583977\n",
      "Training loss for batch 6734 : 0.1274450421333313\n",
      "Training loss for batch 6735 : 0.026300635188817978\n",
      "Training loss for batch 6736 : 0.22077816724777222\n",
      "Training loss for batch 6737 : 0.0024715762119740248\n",
      "Training loss for batch 6738 : 0.19780080020427704\n",
      "Training loss for batch 6739 : 0.08495435118675232\n",
      "Training loss for batch 6740 : 0.44899624586105347\n",
      "Training loss for batch 6741 : 0.1546398401260376\n",
      "Training loss for batch 6742 : 0.45145824551582336\n",
      "Training loss for batch 6743 : 0.3842540383338928\n",
      "Training loss for batch 6744 : 0.0207773856818676\n",
      "Training loss for batch 6745 : 0.1490909904241562\n",
      "Training loss for batch 6746 : 0.12242293357849121\n",
      "Training loss for batch 6747 : 0.6306114196777344\n",
      "Training loss for batch 6748 : 0.004107087850570679\n",
      "Training loss for batch 6749 : 0.1775071918964386\n",
      "Training loss for batch 6750 : 0.09021053463220596\n",
      "Training loss for batch 6751 : 0.2074001133441925\n",
      "Training loss for batch 6752 : 0.19056765735149384\n",
      "Training loss for batch 6753 : 0.5103858113288879\n",
      "Training loss for batch 6754 : 0.06836511194705963\n",
      "Training loss for batch 6755 : 0.02771422080695629\n",
      "Training loss for batch 6756 : 0.28229668736457825\n",
      "Training loss for batch 6757 : 0.364841490983963\n",
      "Training loss for batch 6758 : 0.22669263184070587\n",
      "Training loss for batch 6759 : 0.33461740612983704\n",
      "Training loss for batch 6760 : 0.10947908461093903\n",
      "Training loss for batch 6761 : 0.2212553322315216\n",
      "Training loss for batch 6762 : 0.3315105736255646\n",
      "Training loss for batch 6763 : 0.0695716068148613\n",
      "Training loss for batch 6764 : 0.18292880058288574\n",
      "Training loss for batch 6765 : 0.14333072304725647\n",
      "Training loss for batch 6766 : 0.19638140499591827\n",
      "Training loss for batch 6767 : 0.09532307833433151\n",
      "Training loss for batch 6768 : 0.13611914217472076\n",
      "Training loss for batch 6769 : 0.1366908699274063\n",
      "Training loss for batch 6770 : 0.14944685995578766\n",
      "Training loss for batch 6771 : 0.2371618002653122\n",
      "Training loss for batch 6772 : 0.2173210084438324\n",
      "Training loss for batch 6773 : 0.04420437663793564\n",
      "Training loss for batch 6774 : 0.16070814430713654\n",
      "Training loss for batch 6775 : 0.22077031433582306\n",
      "Training loss for batch 6776 : 0.20380869507789612\n",
      "Training loss for batch 6777 : 0.4245779514312744\n",
      "Training loss for batch 6778 : 0.3016165792942047\n",
      "Training loss for batch 6779 : 0.02783956378698349\n",
      "Training loss for batch 6780 : 0.342063844203949\n",
      "Training loss for batch 6781 : 0.2554561495780945\n",
      "Training loss for batch 6782 : 0.12421056628227234\n",
      "Training loss for batch 6783 : 0.36329689621925354\n",
      "Training loss for batch 6784 : 0.230347141623497\n",
      "Training loss for batch 6785 : 0.0316288061439991\n",
      "Training loss for batch 6786 : 0.34329813718795776\n",
      "Training loss for batch 6787 : 0.03609690070152283\n",
      "Training loss for batch 6788 : 0.29797154664993286\n",
      "Training loss for batch 6789 : 0.2570355534553528\n",
      "Training loss for batch 6790 : 0.09050315618515015\n",
      "Training loss for batch 6791 : 0.1651492714881897\n",
      "Training loss for batch 6792 : 0.2890259921550751\n",
      "Training loss for batch 6793 : 0.32505595684051514\n",
      "Training loss for batch 6794 : 0.43423768877983093\n",
      "Training loss for batch 6795 : 0.07944206148386002\n",
      "Training loss for batch 6796 : 0.23509226739406586\n",
      "Training loss for batch 6797 : 0.13839353621006012\n",
      "Training loss for batch 6798 : 0.29206886887550354\n",
      "Training loss for batch 6799 : 0.24805928766727448\n",
      "Training loss for batch 6800 : 0.0835324376821518\n",
      "Training loss for batch 6801 : 0.05490264296531677\n",
      "Training loss for batch 6802 : 0.07874373346567154\n",
      "Training loss for batch 6803 : 0.03441447764635086\n",
      "Training loss for batch 6804 : 0.08504603058099747\n",
      "Training loss for batch 6805 : 0.26600125432014465\n",
      "Training loss for batch 6806 : 0.05736129358410835\n",
      "Training loss for batch 6807 : 0.2065366953611374\n",
      "Training loss for batch 6808 : 0.11650346219539642\n",
      "Training loss for batch 6809 : 0.07021515816450119\n",
      "Training loss for batch 6810 : 0.15200190246105194\n",
      "Training loss for batch 6811 : 0.10847895592451096\n",
      "Training loss for batch 6812 : 0.160062775015831\n",
      "Training loss for batch 6813 : 0.06041090562939644\n",
      "Training loss for batch 6814 : 0.18692651391029358\n",
      "Training loss for batch 6815 : 0.18213096261024475\n",
      "Training loss for batch 6816 : 0.0019118143245577812\n",
      "Training loss for batch 6817 : 0.26187604665756226\n",
      "Training loss for batch 6818 : 0.4375033974647522\n",
      "Training loss for batch 6819 : 0.30555179715156555\n",
      "Training loss for batch 6820 : 0.418220192193985\n",
      "Training loss for batch 6821 : 0.23879145085811615\n",
      "Training loss for batch 6822 : 0.06378357857465744\n",
      "Training loss for batch 6823 : 0.37775683403015137\n",
      "Training loss for batch 6824 : 0.11007512360811234\n",
      "Training loss for batch 6825 : 0.03214159607887268\n",
      "Training loss for batch 6826 : 0.04572085663676262\n",
      "Training loss for batch 6827 : 0.24816381931304932\n",
      "Training loss for batch 6828 : 0.08548185229301453\n",
      "Training loss for batch 6829 : 0.6050164103507996\n",
      "Training loss for batch 6830 : 0.0006467103958129883\n",
      "Training loss for batch 6831 : 0.13849909603595734\n",
      "Training loss for batch 6832 : 0.23158705234527588\n",
      "Training loss for batch 6833 : 0.0049241385422647\n",
      "Training loss for batch 6834 : 0.2991645336151123\n",
      "Training loss for batch 6835 : 0.4756897985935211\n",
      "Training loss for batch 6836 : 0.22968007624149323\n",
      "Training loss for batch 6837 : 0.08962051570415497\n",
      "Training loss for batch 6838 : 0.0019385616760700941\n",
      "Training loss for batch 6839 : 0.40388375520706177\n",
      "Training loss for batch 6840 : 0.31884992122650146\n",
      "Training loss for batch 6841 : 0.0605912059545517\n",
      "Training loss for batch 6842 : 0.13494998216629028\n",
      "Training loss for batch 6843 : 0.34606868028640747\n",
      "Training loss for batch 6844 : 0.2017374485731125\n",
      "Training loss for batch 6845 : 0.2016199827194214\n",
      "Training loss for batch 6846 : 0.3092289865016937\n",
      "Training loss for batch 6847 : 0.29432404041290283\n",
      "Training loss for batch 6848 : 0.3921321630477905\n",
      "Training loss for batch 6849 : 0.05209733173251152\n",
      "Training loss for batch 6850 : 0.31159481406211853\n",
      "Training loss for batch 6851 : 0.1828351467847824\n",
      "Training loss for batch 6852 : 0.13279812037944794\n",
      "Training loss for batch 6853 : 0.010930514894425869\n",
      "Training loss for batch 6854 : 0.2425074279308319\n",
      "Training loss for batch 6855 : 0.1329619139432907\n",
      "Training loss for batch 6856 : 0.14821623265743256\n",
      "Training loss for batch 6857 : 0.24316583573818207\n",
      "Training loss for batch 6858 : 0.19435110688209534\n",
      "Training loss for batch 6859 : 0.2346976101398468\n",
      "Training loss for batch 6860 : 0.037977296859025955\n",
      "Training loss for batch 6861 : 0.6659526228904724\n",
      "Training loss for batch 6862 : 0.12345612049102783\n",
      "Training loss for batch 6863 : 0.08457361161708832\n",
      "Training loss for batch 6864 : 0.27803948521614075\n",
      "Training loss for batch 6865 : 0.2490653395652771\n",
      "Training loss for batch 6866 : 0.1400647759437561\n",
      "Training loss for batch 6867 : 0.09144032746553421\n",
      "Training loss for batch 6868 : 0.028799444437026978\n",
      "Training loss for batch 6869 : 0.2890475392341614\n",
      "Training loss for batch 6870 : 0.1434468924999237\n",
      "Training loss for batch 6871 : 0.5971939563751221\n",
      "Training loss for batch 6872 : 0.17142900824546814\n",
      "Training loss for batch 6873 : 0.19734008610248566\n",
      "Training loss for batch 6874 : 0.0433366484940052\n",
      "Training loss for batch 6875 : 0.0\n",
      "Training loss for batch 6876 : 0.3860470652580261\n",
      "Training loss for batch 6877 : 0.23989054560661316\n",
      "Training loss for batch 6878 : 0.06775549054145813\n",
      "Training loss for batch 6879 : 0.2951093912124634\n",
      "Training loss for batch 6880 : 0.48065653443336487\n",
      "Training loss for batch 6881 : 0.18357272446155548\n",
      "Training loss for batch 6882 : 0.10045327246189117\n",
      "Training loss for batch 6883 : 0.22171324491500854\n",
      "Training loss for batch 6884 : 0.1507725566625595\n",
      "Training loss for batch 6885 : 0.07818778604269028\n",
      "Training loss for batch 6886 : 0.21344688534736633\n",
      "Training loss for batch 6887 : 0.16143731772899628\n",
      "Training loss for batch 6888 : 0.025022845715284348\n",
      "Training loss for batch 6889 : 0.23977996408939362\n",
      "Training loss for batch 6890 : 0.3456805348396301\n",
      "Training loss for batch 6891 : 0.3442946970462799\n",
      "Training loss for batch 6892 : 0.05402568727731705\n",
      "Training loss for batch 6893 : 0.20646215975284576\n",
      "Training loss for batch 6894 : 0.1398048996925354\n",
      "Training loss for batch 6895 : 0.23518559336662292\n",
      "Training loss for batch 6896 : 0.07815787196159363\n",
      "Training loss for batch 6897 : 0.146319180727005\n",
      "Training loss for batch 6898 : 0.3367692530155182\n",
      "Training loss for batch 6899 : -0.0004008219111710787\n",
      "Training loss for batch 6900 : 0.05492810532450676\n",
      "Training loss for batch 6901 : 0.3499959111213684\n",
      "Training loss for batch 6902 : 0.28192654252052307\n",
      "Training loss for batch 6903 : 0.07531557977199554\n",
      "Training loss for batch 6904 : 0.03048083186149597\n",
      "Training loss for batch 6905 : 0.07560478150844574\n",
      "Training loss for batch 6906 : 0.09244873374700546\n",
      "Training loss for batch 6907 : 0.08473315089941025\n",
      "Training loss for batch 6908 : 0.1760604828596115\n",
      "Training loss for batch 6909 : 0.28712230920791626\n",
      "Training loss for batch 6910 : 0.23343856632709503\n",
      "Training loss for batch 6911 : 0.33957695960998535\n",
      "Training loss for batch 6912 : 0.24744030833244324\n",
      "Training loss for batch 6913 : 0.5049432516098022\n",
      "Training loss for batch 6914 : -0.00030367058934643865\n",
      "Training loss for batch 6915 : 0.3057769536972046\n",
      "Training loss for batch 6916 : 0.09494102746248245\n",
      "Training loss for batch 6917 : 0.5802723169326782\n",
      "Training loss for batch 6918 : 0.02305944450199604\n",
      "Training loss for batch 6919 : 0.5981125235557556\n",
      "Training loss for batch 6920 : 0.15884970128536224\n",
      "Training loss for batch 6921 : 0.08847405016422272\n",
      "Training loss for batch 6922 : 0.20520590245723724\n",
      "Training loss for batch 6923 : 0.058854371309280396\n",
      "Training loss for batch 6924 : 0.1626712679862976\n",
      "Training loss for batch 6925 : 0.10393369942903519\n",
      "Training loss for batch 6926 : 0.14826956391334534\n",
      "Training loss for batch 6927 : 0.028911855071783066\n",
      "Training loss for batch 6928 : 0.3930269479751587\n",
      "Training loss for batch 6929 : 0.23569509387016296\n",
      "Training loss for batch 6930 : 0.4110741913318634\n",
      "Training loss for batch 6931 : 0.17002275586128235\n",
      "Training loss for batch 6932 : 0.17901359498500824\n",
      "Training loss for batch 6933 : 0.04117655009031296\n",
      "Training loss for batch 6934 : 0.17582637071609497\n",
      "Training loss for batch 6935 : 0.36672165989875793\n",
      "Training loss for batch 6936 : 0.05671493336558342\n",
      "Training loss for batch 6937 : 0.004984140396118164\n",
      "Training loss for batch 6938 : 0.21825996041297913\n",
      "Training loss for batch 6939 : 0.1814221292734146\n",
      "Training loss for batch 6940 : 0.5058283805847168\n",
      "Training loss for batch 6941 : 0.27301573753356934\n",
      "Training loss for batch 6942 : 0.25868141651153564\n",
      "Training loss for batch 6943 : 0.15510442852973938\n",
      "Training loss for batch 6944 : 0.07742144167423248\n",
      "Training loss for batch 6945 : 0.1205071359872818\n",
      "Training loss for batch 6946 : 0.15851199626922607\n",
      "Training loss for batch 6947 : 0.24678052961826324\n",
      "Training loss for batch 6948 : 0.49959638714790344\n",
      "Training loss for batch 6949 : 0.1841963231563568\n",
      "Training loss for batch 6950 : 0.12029194086790085\n",
      "Training loss for batch 6951 : 0.3481751084327698\n",
      "Training loss for batch 6952 : 0.012571331113576889\n",
      "Training loss for batch 6953 : 0.16576772928237915\n",
      "Training loss for batch 6954 : 0.471077561378479\n",
      "Training loss for batch 6955 : 0.06344959139823914\n",
      "Training loss for batch 6956 : 0.16896551847457886\n",
      "Training loss for batch 6957 : 0.09110455960035324\n",
      "Training loss for batch 6958 : 0.030038852244615555\n",
      "Training loss for batch 6959 : 0.37464427947998047\n",
      "Training loss for batch 6960 : 0.1729380488395691\n",
      "Training loss for batch 6961 : 0.23579983413219452\n",
      "Training loss for batch 6962 : 0.008372246287763119\n",
      "Training loss for batch 6963 : 0.06250271946191788\n",
      "Training loss for batch 6964 : 0.06876179575920105\n",
      "Training loss for batch 6965 : 0.2858404815196991\n",
      "Training loss for batch 6966 : 0.09097611904144287\n",
      "Training loss for batch 6967 : 0.2872040271759033\n",
      "Training loss for batch 6968 : 0.24758976697921753\n",
      "Training loss for batch 6969 : 0.35670801997184753\n",
      "Training loss for batch 6970 : 0.14093154668807983\n",
      "Training loss for batch 6971 : 0.36852917075157166\n",
      "Training loss for batch 6972 : 0.2804189920425415\n",
      "Training loss for batch 6973 : 0.11283721029758453\n",
      "Training loss for batch 6974 : 0.4866698980331421\n",
      "Training loss for batch 6975 : 0.09898672997951508\n",
      "Training loss for batch 6976 : 0.2139376848936081\n",
      "Training loss for batch 6977 : 0.12211793661117554\n",
      "Training loss for batch 6978 : 0.5100459456443787\n",
      "Training loss for batch 6979 : 0.219665989279747\n",
      "Training loss for batch 6980 : 0.2932213544845581\n",
      "Training loss for batch 6981 : 0.4078768193721771\n",
      "Training loss for batch 6982 : 0.021483246237039566\n",
      "Training loss for batch 6983 : 0.11795160919427872\n",
      "Training loss for batch 6984 : 0.09934414178133011\n",
      "Training loss for batch 6985 : 0.13978922367095947\n",
      "Training loss for batch 6986 : 0.026660658419132233\n",
      "Training loss for batch 6987 : 0.07492805272340775\n",
      "Training loss for batch 6988 : 0.15313875675201416\n",
      "Training loss for batch 6989 : 0.10918227583169937\n",
      "Training loss for batch 6990 : 0.2618758976459503\n",
      "Training loss for batch 6991 : 0.011942881159484386\n",
      "Training loss for batch 6992 : 0.06995927542448044\n",
      "Training loss for batch 6993 : 0.1345454901456833\n",
      "Training loss for batch 6994 : 0.07827109843492508\n",
      "Training loss for batch 6995 : 0.08804962038993835\n",
      "Training loss for batch 6996 : 0.10765647888183594\n",
      "Training loss for batch 6997 : 0.36795774102211\n",
      "Training loss for batch 6998 : 0.2402297705411911\n",
      "Training loss for batch 6999 : 0.4896283447742462\n",
      "Training loss for batch 7000 : 0.13480623066425323\n",
      "Training loss for batch 7001 : 0.046018101274967194\n",
      "Training loss for batch 7002 : 0.1704925000667572\n",
      "Training loss for batch 7003 : 0.2753404676914215\n",
      "Training loss for batch 7004 : 0.36420711874961853\n",
      "Training loss for batch 7005 : 0.356364905834198\n",
      "Training loss for batch 7006 : 0.12386953830718994\n",
      "Training loss for batch 7007 : 0.4051474630832672\n",
      "Training loss for batch 7008 : 0.14588108658790588\n",
      "Training loss for batch 7009 : 0.3766815960407257\n",
      "Training loss for batch 7010 : 0.29913631081581116\n",
      "Training loss for batch 7011 : 0.25332391262054443\n",
      "Training loss for batch 7012 : 0.3218124806880951\n",
      "Training loss for batch 7013 : 0.10100487619638443\n",
      "Training loss for batch 7014 : 0.025793056935071945\n",
      "Training loss for batch 7015 : 0.058929603546857834\n",
      "Training loss for batch 7016 : 0.039915841072797775\n",
      "Training loss for batch 7017 : 0.25879570841789246\n",
      "Training loss for batch 7018 : 0.07336835563182831\n",
      "Training loss for batch 7019 : 0.2473895251750946\n",
      "Training loss for batch 7020 : 0.41938304901123047\n",
      "Training loss for batch 7021 : 0.28668904304504395\n",
      "Training loss for batch 7022 : 0.021849364042282104\n",
      "Training loss for batch 7023 : 0.3138371706008911\n",
      "Training loss for batch 7024 : 0.4840845763683319\n",
      "Training loss for batch 7025 : 0.43951499462127686\n",
      "Training loss for batch 7026 : 0.35592353343963623\n",
      "Training loss for batch 7027 : 0.26641547679901123\n",
      "Training loss for batch 7028 : 0.10329239815473557\n",
      "Training loss for batch 7029 : 0.06330841034650803\n",
      "Training loss for batch 7030 : 0.10834656655788422\n",
      "Training loss for batch 7031 : 0.3307170867919922\n",
      "Training loss for batch 7032 : 0.0676547959446907\n",
      "Training loss for batch 7033 : 0.3317713439464569\n",
      "Training loss for batch 7034 : 0.12974494695663452\n",
      "Training loss for batch 7035 : 0.07552625238895416\n",
      "Training loss for batch 7036 : 0.016931751742959023\n",
      "Training loss for batch 7037 : 0.4550025761127472\n",
      "Training loss for batch 7038 : 0.30510568618774414\n",
      "Training loss for batch 7039 : 0.35417699813842773\n",
      "Training loss for batch 7040 : 0.49028074741363525\n",
      "Training loss for batch 7041 : 0.1831701099872589\n",
      "Training loss for batch 7042 : 0.032370056957006454\n",
      "Training loss for batch 7043 : 0.5421481728553772\n",
      "Training loss for batch 7044 : 0.0\n",
      "Training loss for batch 7045 : 0.07302674651145935\n",
      "Training loss for batch 7046 : 0.06094453111290932\n",
      "Training loss for batch 7047 : 0.13633409142494202\n",
      "Training loss for batch 7048 : 0.06304716318845749\n",
      "Training loss for batch 7049 : 0.15635530650615692\n",
      "Training loss for batch 7050 : 0.09944869577884674\n",
      "Training loss for batch 7051 : 0.06120078265666962\n",
      "Training loss for batch 7052 : 0.10891105234622955\n",
      "Training loss for batch 7053 : 0.1850762665271759\n",
      "Training loss for batch 7054 : 1.0490411520004272\n",
      "Training loss for batch 7055 : 0.03386784717440605\n",
      "Training loss for batch 7056 : 0.14711441099643707\n",
      "Training loss for batch 7057 : 0.2516237199306488\n",
      "Training loss for batch 7058 : 0.18321529030799866\n",
      "Training loss for batch 7059 : 0.15665821731090546\n",
      "Training loss for batch 7060 : 0.24501590430736542\n",
      "Training loss for batch 7061 : 0.16514502465724945\n",
      "Training loss for batch 7062 : 0.10207611322402954\n",
      "Training loss for batch 7063 : 0.22432154417037964\n",
      "Training loss for batch 7064 : 0.3177826404571533\n",
      "Training loss for batch 7065 : 0.1398290991783142\n",
      "Training loss for batch 7066 : 0.05360361933708191\n",
      "Training loss for batch 7067 : 0.2788945734500885\n",
      "Training loss for batch 7068 : 0.1772342175245285\n",
      "Training loss for batch 7069 : 0.1807742714881897\n",
      "Training loss for batch 7070 : 0.2688829004764557\n",
      "Training loss for batch 7071 : 0.32948461174964905\n",
      "Training loss for batch 7072 : 0.14107023179531097\n",
      "Training loss for batch 7073 : 0.4787599742412567\n",
      "Training loss for batch 7074 : 0.08556170761585236\n",
      "Training loss for batch 7075 : 0.11534249037504196\n",
      "Training loss for batch 7076 : 0.11242686957120895\n",
      "Training loss for batch 7077 : 0.1094699278473854\n",
      "Training loss for batch 7078 : 0.06300805509090424\n",
      "Training loss for batch 7079 : 0.24950550496578217\n",
      "Training loss for batch 7080 : 0.2839975655078888\n",
      "Training loss for batch 7081 : 0.1678650826215744\n",
      "Training loss for batch 7082 : 0.27628159523010254\n",
      "Training loss for batch 7083 : 0.3416104316711426\n",
      "Training loss for batch 7084 : 0.09789076447486877\n",
      "Training loss for batch 7085 : 0.10069948434829712\n",
      "Training loss for batch 7086 : 0.11413635313510895\n",
      "Training loss for batch 7087 : 0.08531614392995834\n",
      "Training loss for batch 7088 : 0.16500557959079742\n",
      "Training loss for batch 7089 : 0.0026854972820729017\n",
      "Training loss for batch 7090 : 0.13012602925300598\n",
      "Training loss for batch 7091 : 0.19072839617729187\n",
      "Training loss for batch 7092 : 0.017390286549925804\n",
      "Training loss for batch 7093 : 0.07811177521944046\n",
      "Training loss for batch 7094 : 0.13206784427165985\n",
      "Training loss for batch 7095 : 0.1770877242088318\n",
      "Training loss for batch 7096 : 0.20848935842514038\n",
      "Training loss for batch 7097 : 0.0782657042145729\n",
      "Training loss for batch 7098 : 0.13657641410827637\n",
      "Training loss for batch 7099 : 0.07709524780511856\n",
      "Training loss for batch 7100 : 0.08741886168718338\n",
      "Training loss for batch 7101 : 0.08603394031524658\n",
      "Training loss for batch 7102 : 0.06519431620836258\n",
      "Training loss for batch 7103 : 0.1824827343225479\n",
      "Training loss for batch 7104 : 0.0348830372095108\n",
      "Training loss for batch 7105 : 0.20224350690841675\n",
      "Training loss for batch 7106 : 0.10226035118103027\n",
      "Training loss for batch 7107 : 0.010256552137434483\n",
      "Training loss for batch 7108 : 0.12011843919754028\n",
      "Training loss for batch 7109 : 0.37100711464881897\n",
      "Training loss for batch 7110 : 0.26692336797714233\n",
      "Training loss for batch 7111 : 0.15917474031448364\n",
      "Training loss for batch 7112 : 0.1876816749572754\n",
      "Training loss for batch 7113 : 0.07465898990631104\n",
      "Training loss for batch 7114 : 0.057708028703927994\n",
      "Training loss for batch 7115 : 0.01279341708868742\n",
      "Training loss for batch 7116 : 0.0005797719932161272\n",
      "Training loss for batch 7117 : 0.13525345921516418\n",
      "Training loss for batch 7118 : 0.15537376701831818\n",
      "Training loss for batch 7119 : 0.03675740957260132\n",
      "Training loss for batch 7120 : 0.11983447521924973\n",
      "Training loss for batch 7121 : 0.12839150428771973\n",
      "Training loss for batch 7122 : 0.06902903318405151\n",
      "Training loss for batch 7123 : 0.2641652822494507\n",
      "Training loss for batch 7124 : 0.10019112378358841\n",
      "Training loss for batch 7125 : 0.03912092745304108\n",
      "Training loss for batch 7126 : 0.01887388527393341\n",
      "Training loss for batch 7127 : 0.05174916237592697\n",
      "Training loss for batch 7128 : 0.2275366336107254\n",
      "Training loss for batch 7129 : 0.37471574544906616\n",
      "Training loss for batch 7130 : 0.03719213977456093\n",
      "Training loss for batch 7131 : 0.08330336213111877\n",
      "Training loss for batch 7132 : 0.22232216596603394\n",
      "Training loss for batch 7133 : 0.0\n",
      "Training loss for batch 7134 : 0.355692982673645\n",
      "Training loss for batch 7135 : 0.1484118551015854\n",
      "Training loss for batch 7136 : 0.4224699139595032\n",
      "Training loss for batch 7137 : 0.37223291397094727\n",
      "Training loss for batch 7138 : 0.0006851103389635682\n",
      "Training loss for batch 7139 : 0.3933431804180145\n",
      "Training loss for batch 7140 : 0.04351155459880829\n",
      "Training loss for batch 7141 : 0.04186678305268288\n",
      "Training loss for batch 7142 : 0.011617571115493774\n",
      "Training loss for batch 7143 : 0.0\n",
      "Training loss for batch 7144 : 0.10151751339435577\n",
      "Training loss for batch 7145 : 0.14736205339431763\n",
      "Training loss for batch 7146 : 0.29742199182510376\n",
      "Training loss for batch 7147 : 0.28841209411621094\n",
      "Training loss for batch 7148 : 0.35682734847068787\n",
      "Training loss for batch 7149 : 0.5215451717376709\n",
      "Training loss for batch 7150 : 0.3219963014125824\n",
      "Training loss for batch 7151 : 0.262186735868454\n",
      "Training loss for batch 7152 : 0.04773770272731781\n",
      "Training loss for batch 7153 : 0.2495180368423462\n",
      "Training loss for batch 7154 : 0.1762627363204956\n",
      "Training loss for batch 7155 : 0.6137294769287109\n",
      "Training loss for batch 7156 : 0.13892099261283875\n",
      "Training loss for batch 7157 : 0.11857029795646667\n",
      "Training loss for batch 7158 : 0.07924239337444305\n",
      "Training loss for batch 7159 : 0.1453597992658615\n",
      "Training loss for batch 7160 : 0.11889657378196716\n",
      "Training loss for batch 7161 : 0.041918348520994186\n",
      "Training loss for batch 7162 : 0.16075216233730316\n",
      "Training loss for batch 7163 : 0.064621202647686\n",
      "Training loss for batch 7164 : 0.10108526796102524\n",
      "Training loss for batch 7165 : 0.045775774866342545\n",
      "Training loss for batch 7166 : 0.17640145123004913\n",
      "Training loss for batch 7167 : 0.1290113776922226\n",
      "Training loss for batch 7168 : 0.19302783906459808\n",
      "Training loss for batch 7169 : 0.16474762558937073\n",
      "Training loss for batch 7170 : 0.5147618055343628\n",
      "Training loss for batch 7171 : 0.1309119164943695\n",
      "Training loss for batch 7172 : 0.12617257237434387\n",
      "Training loss for batch 7173 : 0.16277866065502167\n",
      "Training loss for batch 7174 : 0.09002230316400528\n",
      "Training loss for batch 7175 : 0.24204400181770325\n",
      "Training loss for batch 7176 : 0.1338430494070053\n",
      "Training loss for batch 7177 : 0.13191211223602295\n",
      "Training loss for batch 7178 : 0.43598803877830505\n",
      "Training loss for batch 7179 : 0.39747947454452515\n",
      "Training loss for batch 7180 : 0.0960903912782669\n",
      "Training loss for batch 7181 : 0.2511427700519562\n",
      "Training loss for batch 7182 : 0.03127817064523697\n",
      "Training loss for batch 7183 : 0.13665665686130524\n",
      "Training loss for batch 7184 : 0.3201686143875122\n",
      "Training loss for batch 7185 : 0.27695411443710327\n",
      "Training loss for batch 7186 : 0.42225751280784607\n",
      "Training loss for batch 7187 : 0.0023032824974507093\n",
      "Training loss for batch 7188 : 0.43193233013153076\n",
      "Training loss for batch 7189 : 0.08285750448703766\n",
      "Training loss for batch 7190 : 0.15841826796531677\n",
      "Training loss for batch 7191 : 0.18379461765289307\n",
      "Training loss for batch 7192 : 0.4881700873374939\n",
      "Training loss for batch 7193 : 0.008225232362747192\n",
      "Training loss for batch 7194 : 0.14613625407218933\n",
      "Training loss for batch 7195 : 0.07931327074766159\n",
      "Training loss for batch 7196 : 0.1306082159280777\n",
      "Training loss for batch 7197 : 0.07279033213853836\n",
      "Training loss for batch 7198 : 0.02125781774520874\n",
      "Training loss for batch 7199 : 0.22256645560264587\n",
      "Training loss for batch 7200 : 0.3530881106853485\n",
      "Training loss for batch 7201 : 0.3303821086883545\n",
      "Training loss for batch 7202 : 0.11809337139129639\n",
      "Training loss for batch 7203 : 0.292533814907074\n",
      "Training loss for batch 7204 : 0.0018543502083048224\n",
      "Training loss for batch 7205 : 0.16983096301555634\n",
      "Training loss for batch 7206 : 0.1696140468120575\n",
      "Training loss for batch 7207 : 0.16299623250961304\n",
      "Training loss for batch 7208 : 0.46699514985084534\n",
      "Training loss for batch 7209 : 0.15186187624931335\n",
      "Training loss for batch 7210 : 0.2765165865421295\n",
      "Training loss for batch 7211 : 0.212415412068367\n",
      "Training loss for batch 7212 : 0.07943470776081085\n",
      "Training loss for batch 7213 : 0.2336900681257248\n",
      "Training loss for batch 7214 : 0.3307720720767975\n",
      "Training loss for batch 7215 : 0.1538657695055008\n",
      "Training loss for batch 7216 : 0.36295169591903687\n",
      "Training loss for batch 7217 : 0.3804374635219574\n",
      "Training loss for batch 7218 : 0.12059208750724792\n",
      "Training loss for batch 7219 : 0.06265740096569061\n",
      "Training loss for batch 7220 : 0.053075000643730164\n",
      "Training loss for batch 7221 : 0.09617889672517776\n",
      "Training loss for batch 7222 : 0.25925910472869873\n",
      "Training loss for batch 7223 : 0.2081049680709839\n",
      "Training loss for batch 7224 : 0.12866441905498505\n",
      "Training loss for batch 7225 : 0.022752918303012848\n",
      "Training loss for batch 7226 : 0.7732114195823669\n",
      "Training loss for batch 7227 : 0.062097128480672836\n",
      "Training loss for batch 7228 : 0.1717967838048935\n",
      "Training loss for batch 7229 : 0.18146774172782898\n",
      "Training loss for batch 7230 : 0.1014191061258316\n",
      "Training loss for batch 7231 : 0.015083983540534973\n",
      "Training loss for batch 7232 : 0.5103110074996948\n",
      "Training loss for batch 7233 : 0.0\n",
      "Training loss for batch 7234 : 0.1076589822769165\n",
      "Training loss for batch 7235 : 0.06751386821269989\n",
      "Training loss for batch 7236 : 0.22413091361522675\n",
      "Training loss for batch 7237 : 0.08703508973121643\n",
      "Training loss for batch 7238 : 0.01905471831560135\n",
      "Training loss for batch 7239 : 0.0\n",
      "Training loss for batch 7240 : 0.06726253032684326\n",
      "Training loss for batch 7241 : 0.09108775854110718\n",
      "Training loss for batch 7242 : 0.058236703276634216\n",
      "Training loss for batch 7243 : 0.2714526653289795\n",
      "Training loss for batch 7244 : 0.0\n",
      "Training loss for batch 7245 : 0.08826220035552979\n",
      "Training loss for batch 7246 : 0.26244795322418213\n",
      "Training loss for batch 7247 : 0.137180894613266\n",
      "Training loss for batch 7248 : 0.091382697224617\n",
      "Training loss for batch 7249 : 0.24517366290092468\n",
      "Training loss for batch 7250 : 0.0\n",
      "Training loss for batch 7251 : 0.3467324674129486\n",
      "Training loss for batch 7252 : 0.14493294060230255\n",
      "Training loss for batch 7253 : 0.13099586963653564\n",
      "Training loss for batch 7254 : 0.44500359892845154\n",
      "Training loss for batch 7255 : 0.054209526628255844\n",
      "Training loss for batch 7256 : 0.02573006972670555\n",
      "Training loss for batch 7257 : 0.033438790589571\n",
      "Training loss for batch 7258 : 0.2972661852836609\n",
      "Training loss for batch 7259 : 0.18475687503814697\n",
      "Training loss for batch 7260 : 0.4077596664428711\n",
      "Training loss for batch 7261 : 0.04510054364800453\n",
      "Training loss for batch 7262 : 0.19689150154590607\n",
      "Training loss for batch 7263 : 0.143519788980484\n",
      "Training loss for batch 7264 : 0.03824806958436966\n",
      "Training loss for batch 7265 : 0.30680495500564575\n",
      "Training loss for batch 7266 : 0.0\n",
      "Training loss for batch 7267 : 0.3757748305797577\n",
      "Training loss for batch 7268 : 0.35061097145080566\n",
      "Training loss for batch 7269 : 0.33763808012008667\n",
      "Training loss for batch 7270 : 0.01092596910893917\n",
      "Training loss for batch 7271 : 0.08646508306264877\n",
      "Training loss for batch 7272 : 0.1938486546278\n",
      "Training loss for batch 7273 : 0.0015451511135324836\n",
      "Training loss for batch 7274 : 0.28094759583473206\n",
      "Training loss for batch 7275 : 0.4417169988155365\n",
      "Training loss for batch 7276 : 0.017013192176818848\n",
      "Training loss for batch 7277 : 0.03545317053794861\n",
      "Training loss for batch 7278 : 0.12045560777187347\n",
      "Training loss for batch 7279 : 0.25688275694847107\n",
      "Training loss for batch 7280 : 0.10770614445209503\n",
      "Training loss for batch 7281 : 0.25546795129776\n",
      "Training loss for batch 7282 : 0.07873787730932236\n",
      "Training loss for batch 7283 : 0.3031814396381378\n",
      "Training loss for batch 7284 : 0.003218024969100952\n",
      "Training loss for batch 7285 : 0.33363330364227295\n",
      "Training loss for batch 7286 : 0.22765250504016876\n",
      "Training loss for batch 7287 : 0.007814502343535423\n",
      "Training loss for batch 7288 : 0.04812931641936302\n",
      "Training loss for batch 7289 : 0.06431248784065247\n",
      "Training loss for batch 7290 : 0.09058064967393875\n",
      "Training loss for batch 7291 : 0.11853200942277908\n",
      "Training loss for batch 7292 : 0.14598093926906586\n",
      "Training loss for batch 7293 : -2.416746792732738e-05\n",
      "Training loss for batch 7294 : 0.43772682547569275\n",
      "Training loss for batch 7295 : 0.2669399380683899\n",
      "Training loss for batch 7296 : 0.1002485603094101\n",
      "Training loss for batch 7297 : 0.2406519204378128\n",
      "Training loss for batch 7298 : 0.22601652145385742\n",
      "Training loss for batch 7299 : 0.131667822599411\n",
      "Training loss for batch 7300 : 0.12378641217947006\n",
      "Training loss for batch 7301 : 0.18548105657100677\n",
      "Training loss for batch 7302 : 0.014917043969035149\n",
      "Training loss for batch 7303 : 0.16326843202114105\n",
      "Training loss for batch 7304 : 0.20122456550598145\n",
      "Training loss for batch 7305 : 0.07503146678209305\n",
      "Training loss for batch 7306 : 0.24674533307552338\n",
      "Training loss for batch 7307 : 0.42847150564193726\n",
      "Training loss for batch 7308 : 0.007769265212118626\n",
      "Training loss for batch 7309 : 0.3303733170032501\n",
      "Training loss for batch 7310 : 0.20951896905899048\n",
      "Training loss for batch 7311 : 0.046950601041316986\n",
      "Training loss for batch 7312 : 0.13783109188079834\n",
      "Training loss for batch 7313 : 0.14172276854515076\n",
      "Training loss for batch 7314 : 0.12695865333080292\n",
      "Training loss for batch 7315 : 0.011332310736179352\n",
      "Training loss for batch 7316 : 0.0559215284883976\n",
      "Training loss for batch 7317 : 0.0698053166270256\n",
      "Training loss for batch 7318 : 0.1086633950471878\n",
      "Training loss for batch 7319 : 0.24110278487205505\n",
      "Training loss for batch 7320 : 0.31739312410354614\n",
      "Training loss for batch 7321 : 0.5658236742019653\n",
      "Training loss for batch 7322 : 0.019279876723885536\n",
      "Training loss for batch 7323 : 0.18513056635856628\n",
      "Training loss for batch 7324 : 0.15814109146595\n",
      "Training loss for batch 7325 : 0.22874274849891663\n",
      "Training loss for batch 7326 : 0.40876686573028564\n",
      "Training loss for batch 7327 : 0.3372504413127899\n",
      "Training loss for batch 7328 : 0.05255720764398575\n",
      "Training loss for batch 7329 : 0.5280324816703796\n",
      "Training loss for batch 7330 : 0.26308655738830566\n",
      "Training loss for batch 7331 : 0.08450101315975189\n",
      "Training loss for batch 7332 : 0.08611583709716797\n",
      "Training loss for batch 7333 : 0.2407083809375763\n",
      "Training loss for batch 7334 : 0.009260965511202812\n",
      "Training loss for batch 7335 : 0.2200644165277481\n",
      "Training loss for batch 7336 : 0.011039813980460167\n",
      "Training loss for batch 7337 : 0.17185382544994354\n",
      "Training loss for batch 7338 : 0.1347423791885376\n",
      "Training loss for batch 7339 : 0.0\n",
      "Training loss for batch 7340 : 0.01749398186802864\n",
      "Training loss for batch 7341 : 0.3568398356437683\n",
      "Training loss for batch 7342 : 0.01008649729192257\n",
      "Training loss for batch 7343 : 0.5826143026351929\n",
      "Training loss for batch 7344 : 0.1362304836511612\n",
      "Training loss for batch 7345 : 0.09417618066072464\n",
      "Training loss for batch 7346 : 0.3360831141471863\n",
      "Training loss for batch 7347 : 0.09073752909898758\n",
      "Training loss for batch 7348 : 0.2695207893848419\n",
      "Training loss for batch 7349 : 0.05665239691734314\n",
      "Training loss for batch 7350 : 0.5281278491020203\n",
      "Training loss for batch 7351 : 0.20147286355495453\n",
      "Training loss for batch 7352 : 0.06125013530254364\n",
      "Training loss for batch 7353 : 0.13170412182807922\n",
      "Training loss for batch 7354 : 0.01336001418530941\n",
      "Training loss for batch 7355 : 0.34822630882263184\n",
      "Training loss for batch 7356 : 0.15431667864322662\n",
      "Training loss for batch 7357 : 0.4559851586818695\n",
      "Training loss for batch 7358 : 0.08919604867696762\n",
      "Training loss for batch 7359 : 0.025819316506385803\n",
      "Training loss for batch 7360 : 0.29073888063430786\n",
      "Training loss for batch 7361 : 0.35788804292678833\n",
      "Training loss for batch 7362 : -0.0007013081922195852\n",
      "Training loss for batch 7363 : 0.16026636958122253\n",
      "Training loss for batch 7364 : 0.14780528843402863\n",
      "Training loss for batch 7365 : 0.017463520169258118\n",
      "Training loss for batch 7366 : 0.04750490561127663\n",
      "Training loss for batch 7367 : 0.0014366547111421824\n",
      "Training loss for batch 7368 : 0.2589815557003021\n",
      "Training loss for batch 7369 : 0.28234735131263733\n",
      "Training loss for batch 7370 : 0.041106466203927994\n",
      "Training loss for batch 7371 : 0.19143371284008026\n",
      "Training loss for batch 7372 : 0.3547617197036743\n",
      "Training loss for batch 7373 : 0.4104658365249634\n",
      "Training loss for batch 7374 : 0.1666897088289261\n",
      "Training loss for batch 7375 : 0.018413567915558815\n",
      "Training loss for batch 7376 : 0.3264661133289337\n",
      "Training loss for batch 7377 : 0.18743346631526947\n",
      "Training loss for batch 7378 : 0.40059390664100647\n",
      "Training loss for batch 7379 : 0.03240125998854637\n",
      "Training loss for batch 7380 : 0.0\n",
      "Training loss for batch 7381 : 0.08660446852445602\n",
      "Training loss for batch 7382 : 0.19595910608768463\n",
      "Training loss for batch 7383 : 0.09959335625171661\n",
      "Training loss for batch 7384 : 0.001349637983366847\n",
      "Training loss for batch 7385 : 0.35692736506462097\n",
      "Training loss for batch 7386 : 0.23242861032485962\n",
      "Training loss for batch 7387 : 0.3243435323238373\n",
      "Training loss for batch 7388 : 0.05065933242440224\n",
      "Training loss for batch 7389 : 0.234929621219635\n",
      "Training loss for batch 7390 : 0.4662826955318451\n",
      "Training loss for batch 7391 : 0.03572075068950653\n",
      "Training loss for batch 7392 : 0.3054453730583191\n",
      "Training loss for batch 7393 : 0.1857222467660904\n",
      "Training loss for batch 7394 : 0.0\n",
      "Training loss for batch 7395 : 0.03766138106584549\n",
      "Training loss for batch 7396 : 0.06480678170919418\n",
      "Training loss for batch 7397 : 0.36530831456184387\n",
      "Training loss for batch 7398 : 0.09594861418008804\n",
      "Training loss for batch 7399 : 0.24955834448337555\n",
      "Training loss for batch 7400 : 0.3063405454158783\n",
      "Training loss for batch 7401 : 0.03619667515158653\n",
      "Training loss for batch 7402 : 0.2733350694179535\n",
      "Training loss for batch 7403 : 0.3899107277393341\n",
      "Training loss for batch 7404 : 0.056663185358047485\n",
      "Training loss for batch 7405 : 0.14292308688163757\n",
      "Training loss for batch 7406 : 0.09147147834300995\n",
      "Training loss for batch 7407 : 0.004177526105195284\n",
      "Training loss for batch 7408 : 0.028710193932056427\n",
      "Training loss for batch 7409 : -8.399197395192459e-05\n",
      "Training loss for batch 7410 : 0.039732709527015686\n",
      "Training loss for batch 7411 : 0.009680919349193573\n",
      "Training loss for batch 7412 : 0.0492396354675293\n",
      "Training loss for batch 7413 : 0.08769816905260086\n",
      "Training loss for batch 7414 : 0.3329688012599945\n",
      "Training loss for batch 7415 : 0.2170569747686386\n",
      "Training loss for batch 7416 : 0.024000953882932663\n",
      "Training loss for batch 7417 : 0.47381752729415894\n",
      "Training loss for batch 7418 : 0.10094987601041794\n",
      "Training loss for batch 7419 : 0.13715825974941254\n",
      "Training loss for batch 7420 : 0.1294838786125183\n",
      "Training loss for batch 7421 : 0.3520091474056244\n",
      "Training loss for batch 7422 : 0.16568869352340698\n",
      "Training loss for batch 7423 : 0.5504503846168518\n",
      "Training loss for batch 7424 : 0.3056856393814087\n",
      "Training loss for batch 7425 : 0.3803105056285858\n",
      "Training loss for batch 7426 : 0.08407841622829437\n",
      "Training loss for batch 7427 : 0.059978894889354706\n",
      "Training loss for batch 7428 : 0.01952200196683407\n",
      "Training loss for batch 7429 : 0.08546236902475357\n",
      "Training loss for batch 7430 : 0.1928713172674179\n",
      "Training loss for batch 7431 : 0.04296845197677612\n",
      "Training loss for batch 7432 : 0.08726104348897934\n",
      "Training loss for batch 7433 : 0.3873956799507141\n",
      "Training loss for batch 7434 : 0.0\n",
      "Training loss for batch 7435 : 0.020918618887662888\n",
      "Training loss for batch 7436 : 0.030027249827980995\n",
      "Training loss for batch 7437 : 0.035022322088479996\n",
      "Training loss for batch 7438 : 0.11961954832077026\n",
      "Training loss for batch 7439 : 0.07277592271566391\n",
      "Training loss for batch 7440 : 0.04570082575082779\n",
      "Training loss for batch 7441 : 0.5170970559120178\n",
      "Training loss for batch 7442 : 0.4094630777835846\n",
      "Training loss for batch 7443 : 0.04246678948402405\n",
      "Training loss for batch 7444 : 0.010066429153084755\n",
      "Training loss for batch 7445 : 0.26656466722488403\n",
      "Training loss for batch 7446 : 0.2679729759693146\n",
      "Training loss for batch 7447 : 0.7695905566215515\n",
      "Training loss for batch 7448 : 0.0013641815166920424\n",
      "Training loss for batch 7449 : 0.3905384838581085\n",
      "Training loss for batch 7450 : 0.0631142407655716\n",
      "Training loss for batch 7451 : 0.370171457529068\n",
      "Training loss for batch 7452 : 0.14653056859970093\n",
      "Training loss for batch 7453 : 0.3963717520236969\n",
      "Training loss for batch 7454 : 0.08612553030252457\n",
      "Training loss for batch 7455 : 0.43003031611442566\n",
      "Training loss for batch 7456 : 0.27227258682250977\n",
      "Training loss for batch 7457 : 0.04317079484462738\n",
      "Training loss for batch 7458 : 0.2818199396133423\n",
      "Training loss for batch 7459 : 1.1951143741607666\n",
      "Training loss for batch 7460 : 0.2981368601322174\n",
      "Training loss for batch 7461 : 0.27982115745544434\n",
      "Training loss for batch 7462 : 0.14393962919712067\n",
      "Training loss for batch 7463 : 0.33500778675079346\n",
      "Training loss for batch 7464 : 0.16156414151191711\n",
      "Training loss for batch 7465 : 0.026145752519369125\n",
      "Training loss for batch 7466 : 0.23289932310581207\n",
      "Training loss for batch 7467 : 0.02260330319404602\n",
      "Training loss for batch 7468 : 0.227469339966774\n",
      "Training loss for batch 7469 : 0.029399694874882698\n",
      "Training loss for batch 7470 : 0.6016368269920349\n",
      "Training loss for batch 7471 : 0.002554396167397499\n",
      "Training loss for batch 7472 : 0.1483498066663742\n",
      "Training loss for batch 7473 : 0.10087289661169052\n",
      "Training loss for batch 7474 : 0.12471488863229752\n",
      "Training loss for batch 7475 : 0.04459860920906067\n",
      "Training loss for batch 7476 : 0.26077812910079956\n",
      "Training loss for batch 7477 : 0.11950861662626266\n",
      "Training loss for batch 7478 : 0.30558639764785767\n",
      "Training loss for batch 7479 : 0.0724078118801117\n",
      "Training loss for batch 7480 : 0.20657938718795776\n",
      "Training loss for batch 7481 : 0.23217852413654327\n",
      "Training loss for batch 7482 : 0.18180856108665466\n",
      "Training loss for batch 7483 : 0.030407875776290894\n",
      "Training loss for batch 7484 : 0.01824019104242325\n",
      "Training loss for batch 7485 : 0.373159259557724\n",
      "Training loss for batch 7486 : 0.011604361236095428\n",
      "Training loss for batch 7487 : 0.05727473273873329\n",
      "Training loss for batch 7488 : 0.6524566411972046\n",
      "Training loss for batch 7489 : 0.025514720007777214\n",
      "Training loss for batch 7490 : 0.34642019867897034\n",
      "Training loss for batch 7491 : 0.036900319159030914\n",
      "Training loss for batch 7492 : 0.1288500428199768\n",
      "Training loss for batch 7493 : 0.47266337275505066\n",
      "Training loss for batch 7494 : 0.10775019973516464\n",
      "Training loss for batch 7495 : 0.1129496842622757\n",
      "Training loss for batch 7496 : 0.0078066689893603325\n",
      "Training loss for batch 7497 : 0.05933743715286255\n",
      "Training loss for batch 7498 : 0.0\n",
      "Training loss for batch 7499 : 0.0487859770655632\n",
      "Training loss for batch 7500 : 0.3817916214466095\n",
      "Training loss for batch 7501 : 0.20932623744010925\n",
      "Training loss for batch 7502 : 0.00010434786963742226\n",
      "Training loss for batch 7503 : 0.5384150743484497\n",
      "Training loss for batch 7504 : 0.2015048712491989\n",
      "Training loss for batch 7505 : 0.2805086374282837\n",
      "Training loss for batch 7506 : 0.3198922574520111\n",
      "Training loss for batch 7507 : 0.3729014992713928\n",
      "Training loss for batch 7508 : 0.08314387500286102\n",
      "Training loss for batch 7509 : 0.045011237263679504\n",
      "Training loss for batch 7510 : 0.2874401807785034\n",
      "Training loss for batch 7511 : 0.13124418258666992\n",
      "Training loss for batch 7512 : 0.2177000790834427\n",
      "Training loss for batch 7513 : 0.10154992341995239\n",
      "Training loss for batch 7514 : 0.02866367995738983\n",
      "Training loss for batch 7515 : 0.15605686604976654\n",
      "Training loss for batch 7516 : 0.16948328912258148\n",
      "Training loss for batch 7517 : 0.10211499780416489\n",
      "Training loss for batch 7518 : 0.29593151807785034\n",
      "Training loss for batch 7519 : 0.22968976199626923\n",
      "Training loss for batch 7520 : 0.08125635236501694\n",
      "Training loss for batch 7521 : 0.2795286774635315\n",
      "Training loss for batch 7522 : 0.3337070345878601\n",
      "Training loss for batch 7523 : 0.4181813597679138\n",
      "Training loss for batch 7524 : 0.3815453350543976\n",
      "Training loss for batch 7525 : 0.18327179551124573\n",
      "Training loss for batch 7526 : 0.03992288187146187\n",
      "Training loss for batch 7527 : 0.020520318299531937\n",
      "Training loss for batch 7528 : 0.0\n",
      "Training loss for batch 7529 : 0.16789557039737701\n",
      "Training loss for batch 7530 : 0.17262490093708038\n",
      "Training loss for batch 7531 : 0.12463366240262985\n",
      "Training loss for batch 7532 : 0.28147169947624207\n",
      "Training loss for batch 7533 : 0.1372268944978714\n",
      "Training loss for batch 7534 : 0.2299281358718872\n",
      "Training loss for batch 7535 : 0.3678930103778839\n",
      "Training loss for batch 7536 : 0.3281935453414917\n",
      "Training loss for batch 7537 : 0.0285076592117548\n",
      "Training loss for batch 7538 : -0.000870267569553107\n",
      "Training loss for batch 7539 : 0.1218739002943039\n",
      "Training loss for batch 7540 : 0.22110393643379211\n",
      "Training loss for batch 7541 : 0.03076726570725441\n",
      "Training loss for batch 7542 : 0.19382122159004211\n",
      "Training loss for batch 7543 : 0.013630491681396961\n",
      "Training loss for batch 7544 : 0.20936381816864014\n",
      "Training loss for batch 7545 : 0.00699923001229763\n",
      "Training loss for batch 7546 : 0.059696584939956665\n",
      "Training loss for batch 7547 : 0.04117410257458687\n",
      "Training loss for batch 7548 : 0.02591024339199066\n",
      "Training loss for batch 7549 : 0.03381139412522316\n",
      "Training loss for batch 7550 : 0.07732293754816055\n",
      "Training loss for batch 7551 : 0.4846561849117279\n",
      "Training loss for batch 7552 : 0.017308348789811134\n",
      "Training loss for batch 7553 : 0.3953768312931061\n",
      "Training loss for batch 7554 : 0.18037965893745422\n",
      "Training loss for batch 7555 : 0.399848997592926\n",
      "Training loss for batch 7556 : 0.20867297053337097\n",
      "Training loss for batch 7557 : 0.01601000502705574\n",
      "Training loss for batch 7558 : 0.3418424725532532\n",
      "Training loss for batch 7559 : 0.033778440207242966\n",
      "Training loss for batch 7560 : 0.07828730344772339\n",
      "Training loss for batch 7561 : 0.28002360463142395\n",
      "Training loss for batch 7562 : 0.2647859752178192\n",
      "Training loss for batch 7563 : 0.225398451089859\n",
      "Training loss for batch 7564 : 0.1334080994129181\n",
      "Training loss for batch 7565 : 0.13891366124153137\n",
      "Training loss for batch 7566 : 0.017928283661603928\n",
      "Training loss for batch 7567 : 0.41873541474342346\n",
      "Training loss for batch 7568 : 0.2730433940887451\n",
      "Training loss for batch 7569 : 0.19693252444267273\n",
      "Training loss for batch 7570 : 0.2040688842535019\n",
      "Training loss for batch 7571 : 0.18800905346870422\n",
      "Training loss for batch 7572 : 0.3480426073074341\n",
      "Training loss for batch 7573 : 0.17595356702804565\n",
      "Training loss for batch 7574 : 0.16015945374965668\n",
      "Training loss for batch 7575 : 0.03403681144118309\n",
      "Training loss for batch 7576 : 0.006373941898345947\n",
      "Training loss for batch 7577 : 0.15816351771354675\n",
      "Training loss for batch 7578 : 0.2191970944404602\n",
      "Training loss for batch 7579 : 0.17856331169605255\n",
      "Training loss for batch 7580 : 0.06826072186231613\n",
      "Training loss for batch 7581 : 0.4807569682598114\n",
      "Training loss for batch 7582 : 0.343387246131897\n",
      "Training loss for batch 7583 : 0.11277347058057785\n",
      "Training loss for batch 7584 : 0.35837167501449585\n",
      "Training loss for batch 7585 : 0.12596386671066284\n",
      "Training loss for batch 7586 : 0.1629280000925064\n",
      "Training loss for batch 7587 : 0.10199128091335297\n",
      "Training loss for batch 7588 : 0.3352009356021881\n",
      "Training loss for batch 7589 : 0.5433452129364014\n",
      "Training loss for batch 7590 : 0.061587002128362656\n",
      "Training loss for batch 7591 : 0.22689588367938995\n",
      "Training loss for batch 7592 : 0.3816205859184265\n",
      "Training loss for batch 7593 : 0.19729115068912506\n",
      "Training loss for batch 7594 : 0.09139680117368698\n",
      "Training loss for batch 7595 : 0.2724094092845917\n",
      "Training loss for batch 7596 : 0.31372928619384766\n",
      "Training loss for batch 7597 : 0.16724123060703278\n",
      "Training loss for batch 7598 : 0.4202558398246765\n",
      "Training loss for batch 7599 : 0.16147635877132416\n",
      "Training loss for batch 7600 : 0.44650956988334656\n",
      "Training loss for batch 7601 : 0.07135675102472305\n",
      "Training loss for batch 7602 : 0.05139337107539177\n",
      "Training loss for batch 7603 : 0.2525579333305359\n",
      "Training loss for batch 7604 : 0.3827425241470337\n",
      "Training loss for batch 7605 : 0.28235188126564026\n",
      "Training loss for batch 7606 : 0.19063244760036469\n",
      "Training loss for batch 7607 : 0.39775437116622925\n",
      "Training loss for batch 7608 : 0.1287500262260437\n",
      "Training loss for batch 7609 : 0.07583313435316086\n",
      "Training loss for batch 7610 : 0.3113715648651123\n",
      "Training loss for batch 7611 : 0.0411885604262352\n",
      "Training loss for batch 7612 : 0.06663093715906143\n",
      "Training loss for batch 7613 : 0.2086821049451828\n",
      "Training loss for batch 7614 : 0.3799442946910858\n",
      "Training loss for batch 7615 : 0.16302233934402466\n",
      "Training loss for batch 7616 : 0.06640765070915222\n",
      "Training loss for batch 7617 : 0.266849160194397\n",
      "Training loss for batch 7618 : 0.41577067971229553\n",
      "Training loss for batch 7619 : 0.27312594652175903\n",
      "Training loss for batch 7620 : 0.16052329540252686\n",
      "Training loss for batch 7621 : 0.15057191252708435\n",
      "Training loss for batch 7622 : 0.05490097403526306\n",
      "Training loss for batch 7623 : 0.30624452233314514\n",
      "Training loss for batch 7624 : 0.29988741874694824\n",
      "Training loss for batch 7625 : 0.05016162991523743\n",
      "Training loss for batch 7626 : 0.34823212027549744\n",
      "Training loss for batch 7627 : 0.13092556595802307\n",
      "Training loss for batch 7628 : 0.14024998247623444\n",
      "Training loss for batch 7629 : 0.032059650868177414\n",
      "Training loss for batch 7630 : 0.26298919320106506\n",
      "Training loss for batch 7631 : 0.009384463541209698\n",
      "Training loss for batch 7632 : 0.13621743023395538\n",
      "Training loss for batch 7633 : 0.060506485402584076\n",
      "Training loss for batch 7634 : 0.05030692741274834\n",
      "Training loss for batch 7635 : 0.2425967901945114\n",
      "Training loss for batch 7636 : 0.1121421605348587\n",
      "Training loss for batch 7637 : 0.20584437251091003\n",
      "Training loss for batch 7638 : 0.3858092129230499\n",
      "Training loss for batch 7639 : 0.29845571517944336\n",
      "Training loss for batch 7640 : 0.16412322223186493\n",
      "Training loss for batch 7641 : 0.15454840660095215\n",
      "Training loss for batch 7642 : 0.09717293083667755\n",
      "Training loss for batch 7643 : 0.34500446915626526\n",
      "Training loss for batch 7644 : 0.14933717250823975\n",
      "Training loss for batch 7645 : 0.006765178870409727\n",
      "Training loss for batch 7646 : 0.0716087743639946\n",
      "Training loss for batch 7647 : 0.03769367188215256\n",
      "Training loss for batch 7648 : 0.004740327596664429\n",
      "Training loss for batch 7649 : 0.12132398784160614\n",
      "Training loss for batch 7650 : 0.26897701621055603\n",
      "Training loss for batch 7651 : 0.21444399654865265\n",
      "Training loss for batch 7652 : 0.028624391183257103\n",
      "Training loss for batch 7653 : 0.0007920563220977783\n",
      "Training loss for batch 7654 : 0.0\n",
      "Training loss for batch 7655 : 0.07285282760858536\n",
      "Training loss for batch 7656 : 0.08212024718523026\n",
      "Training loss for batch 7657 : 0.034275270998477936\n",
      "Training loss for batch 7658 : 0.13764837384223938\n",
      "Training loss for batch 7659 : 0.2333942949771881\n",
      "Training loss for batch 7660 : 0.45274901390075684\n",
      "Training loss for batch 7661 : 0.02643711492419243\n",
      "Training loss for batch 7662 : 0.3269219398498535\n",
      "Training loss for batch 7663 : 0.40368568897247314\n",
      "Training loss for batch 7664 : 0.1718934029340744\n",
      "Training loss for batch 7665 : 0.20199556648731232\n",
      "Training loss for batch 7666 : 0.15627780556678772\n",
      "Training loss for batch 7667 : 0.02602563425898552\n",
      "Training loss for batch 7668 : 0.06793604791164398\n",
      "Training loss for batch 7669 : 0.440643310546875\n",
      "Training loss for batch 7670 : 0.054629623889923096\n",
      "Training loss for batch 7671 : 0.03009481355547905\n",
      "Training loss for batch 7672 : 0.1263657510280609\n",
      "Training loss for batch 7673 : 0.33513984084129333\n",
      "Training loss for batch 7674 : 0.10866311192512512\n",
      "Training loss for batch 7675 : 0.27041083574295044\n",
      "Training loss for batch 7676 : 0.4861258864402771\n",
      "Training loss for batch 7677 : 0.0725923702120781\n",
      "Training loss for batch 7678 : 0.16210392117500305\n",
      "Training loss for batch 7679 : 0.3759678304195404\n",
      "Training loss for batch 7680 : 0.2580496668815613\n",
      "Training loss for batch 7681 : 0.007107983343303204\n",
      "Training loss for batch 7682 : 0.05696932226419449\n",
      "Training loss for batch 7683 : 0.22620131075382233\n",
      "Training loss for batch 7684 : 0.1597783863544464\n",
      "Training loss for batch 7685 : 0.15056969225406647\n",
      "Training loss for batch 7686 : 0.1447901874780655\n",
      "Training loss for batch 7687 : 0.4466526508331299\n",
      "Training loss for batch 7688 : 0.011264480650424957\n",
      "Training loss for batch 7689 : 0.21928644180297852\n",
      "Training loss for batch 7690 : 0.0014208853244781494\n",
      "Training loss for batch 7691 : 0.3089725375175476\n",
      "Training loss for batch 7692 : 0.14292988181114197\n",
      "Training loss for batch 7693 : 0.22650140523910522\n",
      "Training loss for batch 7694 : 0.5628700256347656\n",
      "Training loss for batch 7695 : 0.09260448068380356\n",
      "Training loss for batch 7696 : 0.43593695759773254\n",
      "Training loss for batch 7697 : 0.8039246201515198\n",
      "Training loss for batch 7698 : 0.1131228432059288\n",
      "Training loss for batch 7699 : 0.057018499821424484\n",
      "Training loss for batch 7700 : 0.029246307909488678\n",
      "Training loss for batch 7701 : -0.0007991431630216539\n",
      "Training loss for batch 7702 : 0.1542729288339615\n",
      "Training loss for batch 7703 : 0.31524160504341125\n",
      "Training loss for batch 7704 : 0.042039960622787476\n",
      "Training loss for batch 7705 : 0.05040645971894264\n",
      "Training loss for batch 7706 : 0.1863262802362442\n",
      "Training loss for batch 7707 : 0.10993017256259918\n",
      "Training loss for batch 7708 : 0.0018739303341135383\n",
      "Training loss for batch 7709 : 0.1526048332452774\n",
      "Training loss for batch 7710 : 0.19050560891628265\n",
      "Training loss for batch 7711 : 0.02798960730433464\n",
      "Training loss for batch 7712 : 0.015232752077281475\n",
      "Training loss for batch 7713 : 0.6059347987174988\n",
      "Training loss for batch 7714 : 0.08321668207645416\n",
      "Training loss for batch 7715 : 0.4869091212749481\n",
      "Training loss for batch 7716 : 0.0042744893580675125\n",
      "Training loss for batch 7717 : 0.1964626908302307\n",
      "Training loss for batch 7718 : 0.2944713532924652\n",
      "Training loss for batch 7719 : 0.18602122366428375\n",
      "Training loss for batch 7720 : 0.10493079572916031\n",
      "Training loss for batch 7721 : 0.13661311566829681\n",
      "Training loss for batch 7722 : 0.513177752494812\n",
      "Training loss for batch 7723 : 0.1597668081521988\n",
      "Training loss for batch 7724 : 0.046683017164468765\n",
      "Training loss for batch 7725 : 0.0\n",
      "Training loss for batch 7726 : 0.003318504896014929\n",
      "Training loss for batch 7727 : 0.012974685057997704\n",
      "Training loss for batch 7728 : 0.2320423275232315\n",
      "Training loss for batch 7729 : 0.13726544380187988\n",
      "Training loss for batch 7730 : 0.0016762767918407917\n",
      "Training loss for batch 7731 : 0.08776631206274033\n",
      "Training loss for batch 7732 : 0.015033068135380745\n",
      "Training loss for batch 7733 : 0.11315419524908066\n",
      "Training loss for batch 7734 : 0.1321674883365631\n",
      "Training loss for batch 7735 : 0.06056321784853935\n",
      "Training loss for batch 7736 : 0.07012539356946945\n",
      "Training loss for batch 7737 : 0.012947211973369122\n",
      "Training loss for batch 7738 : 0.20759642124176025\n",
      "Training loss for batch 7739 : 0.2338780164718628\n",
      "Training loss for batch 7740 : 0.3034408390522003\n",
      "Training loss for batch 7741 : 0.15401986241340637\n",
      "Training loss for batch 7742 : 0.4656192660331726\n",
      "Training loss for batch 7743 : 0.036823902279138565\n",
      "Training loss for batch 7744 : 0.07733231782913208\n",
      "Training loss for batch 7745 : 0.32875877618789673\n",
      "Training loss for batch 7746 : 0.08945701271295547\n",
      "Training loss for batch 7747 : 0.07997831702232361\n",
      "Training loss for batch 7748 : 0.5622933506965637\n",
      "Training loss for batch 7749 : 0.45008033514022827\n",
      "Training loss for batch 7750 : 0.07719395309686661\n",
      "Training loss for batch 7751 : 0.0424780547618866\n",
      "Training loss for batch 7752 : 0.17484182119369507\n",
      "Training loss for batch 7753 : 0.09602507203817368\n",
      "Training loss for batch 7754 : 0.376042902469635\n",
      "Training loss for batch 7755 : 0.7948200702667236\n",
      "Training loss for batch 7756 : 0.2589941620826721\n",
      "Training loss for batch 7757 : 0.2143043428659439\n",
      "Training loss for batch 7758 : 0.1214505136013031\n",
      "Training loss for batch 7759 : 0.24210284650325775\n",
      "Training loss for batch 7760 : 0.09295713156461716\n",
      "Training loss for batch 7761 : 0.16513624787330627\n",
      "Training loss for batch 7762 : 0.13089601695537567\n",
      "Training loss for batch 7763 : 0.2282094806432724\n",
      "Training loss for batch 7764 : 0.26956093311309814\n",
      "Training loss for batch 7765 : 0.2587331533432007\n",
      "Training loss for batch 7766 : 0.01361232902854681\n",
      "Training loss for batch 7767 : 0.2461724430322647\n",
      "Training loss for batch 7768 : 0.09616672247648239\n",
      "Training loss for batch 7769 : 0.10121189057826996\n",
      "Training loss for batch 7770 : 0.2444760799407959\n",
      "Training loss for batch 7771 : 0.09705939143896103\n",
      "Training loss for batch 7772 : 0.032092951238155365\n",
      "Training loss for batch 7773 : 0.2446175515651703\n",
      "Training loss for batch 7774 : 0.19620493054389954\n",
      "Training loss for batch 7775 : 0.18039506673812866\n",
      "Training loss for batch 7776 : 0.27717727422714233\n",
      "Training loss for batch 7777 : 0.21372085809707642\n",
      "Training loss for batch 7778 : 0.2172444611787796\n",
      "Training loss for batch 7779 : 0.31802356243133545\n",
      "Training loss for batch 7780 : 0.0318002924323082\n",
      "Training loss for batch 7781 : 0.14224496483802795\n",
      "Training loss for batch 7782 : 0.3555223345756531\n",
      "Training loss for batch 7783 : 0.2803427278995514\n",
      "Training loss for batch 7784 : 0.27421343326568604\n",
      "Training loss for batch 7785 : 0.0541125051677227\n",
      "Training loss for batch 7786 : 0.1558629274368286\n",
      "Training loss for batch 7787 : 0.15665939450263977\n",
      "Training loss for batch 7788 : 0.46126624941825867\n",
      "Training loss for batch 7789 : 0.06734249740839005\n",
      "Training loss for batch 7790 : 0.009731664322316647\n",
      "Training loss for batch 7791 : 0.24591371417045593\n",
      "Training loss for batch 7792 : 0.2779974341392517\n",
      "Training loss for batch 7793 : 0.2271682322025299\n",
      "Training loss for batch 7794 : 0.1983453631401062\n",
      "Training loss for batch 7795 : 0.028670236468315125\n",
      "Training loss for batch 7796 : 0.08689205348491669\n",
      "Training loss for batch 7797 : 0.13807278871536255\n",
      "Training loss for batch 7798 : 0.19197826087474823\n",
      "Training loss for batch 7799 : 0.03810010850429535\n",
      "Training loss for batch 7800 : 0.3790694773197174\n",
      "Training loss for batch 7801 : 0.003674765583127737\n",
      "Training loss for batch 7802 : 0.41477370262145996\n",
      "Training loss for batch 7803 : 0.18940068781375885\n",
      "Training loss for batch 7804 : 0.4056398868560791\n",
      "Training loss for batch 7805 : 0.19436651468276978\n",
      "Training loss for batch 7806 : 0.39901161193847656\n",
      "Training loss for batch 7807 : 0.3363422155380249\n",
      "Training loss for batch 7808 : 0.07694030553102493\n",
      "Training loss for batch 7809 : 0.08189964294433594\n",
      "Training loss for batch 7810 : 0.6145719885826111\n",
      "Training loss for batch 7811 : 0.12898679077625275\n",
      "Training loss for batch 7812 : 0.24288703501224518\n",
      "Training loss for batch 7813 : 0.2139745056629181\n",
      "Training loss for batch 7814 : 0.32017555832862854\n",
      "Training loss for batch 7815 : 0.5378997921943665\n",
      "Training loss for batch 7816 : 0.12552744150161743\n",
      "Training loss for batch 7817 : 0.0\n",
      "Training loss for batch 7818 : 0.21813124418258667\n",
      "Training loss for batch 7819 : 0.09390269964933395\n",
      "Training loss for batch 7820 : 0.24064461886882782\n",
      "Training loss for batch 7821 : 0.25778496265411377\n",
      "Training loss for batch 7822 : 0.24909977614879608\n",
      "Training loss for batch 7823 : 0.16729161143302917\n",
      "Training loss for batch 7824 : 0.5975674390792847\n",
      "Training loss for batch 7825 : 0.1872195601463318\n",
      "Training loss for batch 7826 : 0.1668456643819809\n",
      "Training loss for batch 7827 : 0.01545007061213255\n",
      "Training loss for batch 7828 : 0.02564943954348564\n",
      "Training loss for batch 7829 : 0.12968476116657257\n",
      "Training loss for batch 7830 : 0.06970453262329102\n",
      "Training loss for batch 7831 : 0.046099644154310226\n",
      "Training loss for batch 7832 : 0.1733386516571045\n",
      "Training loss for batch 7833 : 0.05719977617263794\n",
      "Training loss for batch 7834 : 0.6000869274139404\n",
      "Training loss for batch 7835 : 0.07928192615509033\n",
      "Training loss for batch 7836 : 0.2071993052959442\n",
      "Training loss for batch 7837 : 0.3829044699668884\n",
      "Training loss for batch 7838 : 0.059541963040828705\n",
      "Training loss for batch 7839 : 0.08548178523778915\n",
      "Training loss for batch 7840 : 0.08389844745397568\n",
      "Training loss for batch 7841 : 0.023841889575123787\n",
      "Training loss for batch 7842 : 0.2092585265636444\n",
      "Training loss for batch 7843 : 0.24932487308979034\n",
      "Training loss for batch 7844 : 0.1641455739736557\n",
      "Training loss for batch 7845 : 0.2634665369987488\n",
      "Training loss for batch 7846 : 0.22198991477489471\n",
      "Training loss for batch 7847 : 0.45654866099357605\n",
      "Training loss for batch 7848 : 0.030614253133535385\n",
      "Training loss for batch 7849 : 0.0\n",
      "Training loss for batch 7850 : 0.0\n",
      "Training loss for batch 7851 : 0.12770412862300873\n",
      "Training loss for batch 7852 : 0.11192672699689865\n",
      "Training loss for batch 7853 : 0.47232693433761597\n",
      "Training loss for batch 7854 : 0.05554579570889473\n",
      "Training loss for batch 7855 : 0.0392586812376976\n",
      "Training loss for batch 7856 : 0.1346471607685089\n",
      "Training loss for batch 7857 : 0.33514803647994995\n",
      "Training loss for batch 7858 : 0.13963066041469574\n",
      "Training loss for batch 7859 : 0.07136797159910202\n",
      "Training loss for batch 7860 : 0.04286440089344978\n",
      "Training loss for batch 7861 : 0.0\n",
      "Training loss for batch 7862 : 0.17682698369026184\n",
      "Training loss for batch 7863 : 0.03206653892993927\n",
      "Training loss for batch 7864 : 0.4279302656650543\n",
      "Training loss for batch 7865 : 0.0418039970099926\n",
      "Training loss for batch 7866 : 0.26753726601600647\n",
      "Training loss for batch 7867 : 0.2687060236930847\n",
      "Training loss for batch 7868 : 0.2201329916715622\n",
      "Training loss for batch 7869 : 0.1322845220565796\n",
      "Training loss for batch 7870 : 0.27754753828048706\n",
      "Training loss for batch 7871 : 0.07711565494537354\n",
      "Training loss for batch 7872 : 0.058361947536468506\n",
      "Training loss for batch 7873 : 0.16494940221309662\n",
      "Training loss for batch 7874 : 0.0995195209980011\n",
      "Training loss for batch 7875 : 0.03813350573182106\n",
      "Training loss for batch 7876 : 0.11232645809650421\n",
      "Training loss for batch 7877 : 0.3232521712779999\n",
      "Training loss for batch 7878 : 0.24891561269760132\n",
      "Training loss for batch 7879 : 0.44104233384132385\n",
      "Training loss for batch 7880 : 0.03895311802625656\n",
      "Training loss for batch 7881 : 0.4105865955352783\n",
      "Training loss for batch 7882 : 0.2548479437828064\n",
      "Training loss for batch 7883 : 0.3170754611492157\n",
      "Training loss for batch 7884 : 0.23793649673461914\n",
      "Training loss for batch 7885 : 0.16671399772167206\n",
      "Training loss for batch 7886 : 0.08252362161874771\n",
      "Training loss for batch 7887 : 0.2697099447250366\n",
      "Training loss for batch 7888 : 0.1558985859155655\n",
      "Training loss for batch 7889 : 0.33564528822898865\n",
      "Training loss for batch 7890 : 0.09835965931415558\n",
      "Training loss for batch 7891 : 0.4269979000091553\n",
      "Training loss for batch 7892 : 0.2934454083442688\n",
      "Training loss for batch 7893 : 0.36933737993240356\n",
      "Training loss for batch 7894 : 0.21045957505702972\n",
      "Training loss for batch 7895 : 0.38012295961380005\n",
      "Training loss for batch 7896 : 0.03150853142142296\n",
      "Training loss for batch 7897 : 0.00951251108199358\n",
      "Training loss for batch 7898 : 0.09884295612573624\n",
      "Training loss for batch 7899 : 0.030835112556815147\n",
      "Training loss for batch 7900 : 0.07363013178110123\n",
      "Training loss for batch 7901 : 0.35299351811408997\n",
      "Training loss for batch 7902 : 0.020550141111016273\n",
      "Training loss for batch 7903 : 0.032448697835206985\n",
      "Training loss for batch 7904 : 0.21102455258369446\n",
      "Training loss for batch 7905 : 0.0\n",
      "Training loss for batch 7906 : 0.03619387373328209\n",
      "Training loss for batch 7907 : 0.176767498254776\n",
      "Training loss for batch 7908 : 0.05097556486725807\n",
      "Training loss for batch 7909 : 0.29205524921417236\n",
      "Training loss for batch 7910 : 0.20543645322322845\n",
      "Training loss for batch 7911 : 0.1286768615245819\n",
      "Training loss for batch 7912 : 0.14473257958889008\n",
      "Training loss for batch 7913 : 0.32744303345680237\n",
      "Training loss for batch 7914 : 0.3166964054107666\n",
      "Training loss for batch 7915 : 0.0\n",
      "Training loss for batch 7916 : 0.1263158619403839\n",
      "Training loss for batch 7917 : 0.23761671781539917\n",
      "Training loss for batch 7918 : 0.19345656037330627\n",
      "Training loss for batch 7919 : 0.3009466230869293\n",
      "Training loss for batch 7920 : 0.04797594994306564\n",
      "Training loss for batch 7921 : 0.24470339715480804\n",
      "Training loss for batch 7922 : 0.2827610969543457\n",
      "Training loss for batch 7923 : 0.36958712339401245\n",
      "Training loss for batch 7924 : 0.20738482475280762\n",
      "Training loss for batch 7925 : 0.00460080849006772\n",
      "Training loss for batch 7926 : 0.01874438114464283\n",
      "Training loss for batch 7927 : 0.6914095878601074\n",
      "Training loss for batch 7928 : 0.5896766781806946\n",
      "Training loss for batch 7929 : 0.011913667432963848\n",
      "Training loss for batch 7930 : 0.13823866844177246\n",
      "Training loss for batch 7931 : 0.12352918088436127\n",
      "Training loss for batch 7932 : 0.10455749928951263\n",
      "Training loss for batch 7933 : 0.03533356636762619\n",
      "Training loss for batch 7934 : 0.11842705309391022\n",
      "Training loss for batch 7935 : 0.348174124956131\n",
      "Training loss for batch 7936 : 0.5696765780448914\n",
      "Training loss for batch 7937 : 0.004441325087100267\n",
      "Training loss for batch 7938 : 0.19797714054584503\n",
      "Training loss for batch 7939 : 0.13520991802215576\n",
      "Training loss for batch 7940 : 0.12427844107151031\n",
      "Training loss for batch 7941 : 0.081294946372509\n",
      "Training loss for batch 7942 : 0.3537653088569641\n",
      "Training loss for batch 7943 : 0.525786817073822\n",
      "Training loss for batch 7944 : 0.46406739950180054\n",
      "Training loss for batch 7945 : 0.20667091012001038\n",
      "Training loss for batch 7946 : 0.08279097080230713\n",
      "Training loss for batch 7947 : 0.44182446599006653\n",
      "Training loss for batch 7948 : 0.021556731313467026\n",
      "Training loss for batch 7949 : 0.16636840999126434\n",
      "Training loss for batch 7950 : 0.30507075786590576\n",
      "Training loss for batch 7951 : 0.18802925944328308\n",
      "Training loss for batch 7952 : 0.29909321665763855\n",
      "Training loss for batch 7953 : 0.5383566617965698\n",
      "Training loss for batch 7954 : 0.3321419358253479\n",
      "Training loss for batch 7955 : 0.2750171422958374\n",
      "Training loss for batch 7956 : 0.002461373805999756\n",
      "Training loss for batch 7957 : 0.07338061183691025\n",
      "Training loss for batch 7958 : 0.11768266558647156\n",
      "Training loss for batch 7959 : 0.09563154727220535\n",
      "Training loss for batch 7960 : 0.07979157567024231\n",
      "Training loss for batch 7961 : 0.07346358895301819\n",
      "Training loss for batch 7962 : 0.12773169577121735\n",
      "Training loss for batch 7963 : 0.21606899797916412\n",
      "Training loss for batch 7964 : 0.0\n",
      "Training loss for batch 7965 : 0.0\n",
      "Training loss for batch 7966 : 0.1623210459947586\n",
      "Training loss for batch 7967 : 0.34691134095191956\n",
      "Training loss for batch 7968 : 0.0\n",
      "Training loss for batch 7969 : 0.34692618250846863\n",
      "Training loss for batch 7970 : 0.405465304851532\n",
      "Training loss for batch 7971 : 0.0955657958984375\n",
      "Training loss for batch 7972 : 0.27500465512275696\n",
      "Training loss for batch 7973 : 0.45768627524375916\n",
      "Training loss for batch 7974 : 0.1554657369852066\n",
      "Training loss for batch 7975 : 0.03170749545097351\n",
      "Training loss for batch 7976 : 0.09699937701225281\n",
      "Training loss for batch 7977 : 0.12135109305381775\n",
      "Training loss for batch 7978 : 0.15044964849948883\n",
      "Training loss for batch 7979 : 0.054134026169776917\n",
      "Training loss for batch 7980 : 0.07119238376617432\n",
      "Training loss for batch 7981 : 0.5105538368225098\n",
      "Training loss for batch 7982 : 0.22888781130313873\n",
      "Training loss for batch 7983 : 0.21906206011772156\n",
      "Training loss for batch 7984 : 0.2175414264202118\n",
      "Training loss for batch 7985 : 0.18242469429969788\n",
      "Training loss for batch 7986 : 0.2642867863178253\n",
      "Training loss for batch 7987 : 0.18762162327766418\n",
      "Training loss for batch 7988 : 0.2086317390203476\n",
      "Training loss for batch 7989 : 0.03499840199947357\n",
      "Training loss for batch 7990 : 0.21462538838386536\n",
      "Training loss for batch 7991 : 0.1866641640663147\n",
      "Training loss for batch 7992 : 0.5017038583755493\n",
      "Training loss for batch 7993 : 0.0012572010746225715\n",
      "Training loss for batch 7994 : 0.1862688809633255\n",
      "Training loss for batch 7995 : 0.324604332447052\n",
      "Training loss for batch 7996 : 0.2870264947414398\n",
      "Training loss for batch 7997 : 0.10636456310749054\n",
      "Training loss for batch 7998 : 0.537941038608551\n",
      "Training loss for batch 7999 : 0.2967982590198517\n",
      "Training loss for batch 8000 : 0.019983774051070213\n",
      "Training loss for batch 8001 : 0.2027043253183365\n",
      "Training loss for batch 8002 : 0.02516837604343891\n",
      "Training loss for batch 8003 : 0.041244253516197205\n",
      "Training loss for batch 8004 : 0.34611326456069946\n",
      "Training loss for batch 8005 : 0.08794049918651581\n",
      "Training loss for batch 8006 : 0.03544043004512787\n",
      "Training loss for batch 8007 : 0.2807404100894928\n",
      "Training loss for batch 8008 : 0.4697771370410919\n",
      "Training loss for batch 8009 : 0.2666815221309662\n",
      "Training loss for batch 8010 : 0.23846091330051422\n",
      "Training loss for batch 8011 : 0.5367685556411743\n",
      "Training loss for batch 8012 : 0.2420160323381424\n",
      "Training loss for batch 8013 : 0.04089939594268799\n",
      "Training loss for batch 8014 : 0.240437850356102\n",
      "Training loss for batch 8015 : 0.12696366012096405\n",
      "Training loss for batch 8016 : 0.06435996294021606\n",
      "Training loss for batch 8017 : 0.07287448644638062\n",
      "Training loss for batch 8018 : 0.6379817724227905\n",
      "Training loss for batch 8019 : 0.18785671889781952\n",
      "Training loss for batch 8020 : 0.14959166944026947\n",
      "Training loss for batch 8021 : 0.21251516044139862\n",
      "Training loss for batch 8022 : 0.01703908108174801\n",
      "Training loss for batch 8023 : 0.37888550758361816\n",
      "Training loss for batch 8024 : 0.3591972589492798\n",
      "Training loss for batch 8025 : 0.10877427458763123\n",
      "Training loss for batch 8026 : 0.059953514486551285\n",
      "Training loss for batch 8027 : 0.15620334446430206\n",
      "Training loss for batch 8028 : 0.7639997601509094\n",
      "Training loss for batch 8029 : 0.06230669468641281\n",
      "Training loss for batch 8030 : 0.10291077941656113\n",
      "Training loss for batch 8031 : 0.0772639811038971\n",
      "Training loss for batch 8032 : 0.18834976851940155\n",
      "Training loss for batch 8033 : 0.13022446632385254\n",
      "Training loss for batch 8034 : 0.11516954749822617\n",
      "Training loss for batch 8035 : 0.09688864648342133\n",
      "Training loss for batch 8036 : 0.48725003004074097\n",
      "Training loss for batch 8037 : 0.4777888059616089\n",
      "Training loss for batch 8038 : 0.1334877759218216\n",
      "Training loss for batch 8039 : 0.015086811035871506\n",
      "Training loss for batch 8040 : 0.017650699242949486\n",
      "Training loss for batch 8041 : 0.0005208253860473633\n",
      "Training loss for batch 8042 : 0.21679018437862396\n",
      "Training loss for batch 8043 : 0.3018103539943695\n",
      "Training loss for batch 8044 : 0.0684109628200531\n",
      "Training loss for batch 8045 : 0.03992903232574463\n",
      "Training loss for batch 8046 : 0.16728389263153076\n",
      "Training loss for batch 8047 : 0.10663796216249466\n",
      "Training loss for batch 8048 : 0.05705089122056961\n",
      "Training loss for batch 8049 : 0.06806834787130356\n",
      "Training loss for batch 8050 : 0.2740195691585541\n",
      "Training loss for batch 8051 : 0.5498899221420288\n",
      "Training loss for batch 8052 : 0.8585036993026733\n",
      "Training loss for batch 8053 : 0.14952468872070312\n",
      "Training loss for batch 8054 : -0.00020274076086934656\n",
      "Training loss for batch 8055 : 0.14063991606235504\n",
      "Training loss for batch 8056 : 0.12782219052314758\n",
      "Training loss for batch 8057 : 0.0760406032204628\n",
      "Training loss for batch 8058 : 0.26441439986228943\n",
      "Training loss for batch 8059 : 0.21181894838809967\n",
      "Training loss for batch 8060 : 0.1072535365819931\n",
      "Training loss for batch 8061 : 0.024878082796931267\n",
      "Training loss for batch 8062 : 0.1713261753320694\n",
      "Training loss for batch 8063 : 0.17117363214492798\n",
      "Training loss for batch 8064 : 0.23189003765583038\n",
      "Training loss for batch 8065 : 0.16001838445663452\n",
      "Training loss for batch 8066 : 0.23203350603580475\n",
      "Training loss for batch 8067 : 0.3480375409126282\n",
      "Training loss for batch 8068 : 0.18209323287010193\n",
      "Training loss for batch 8069 : 0.07280834019184113\n",
      "Training loss for batch 8070 : 0.0013179283123463392\n",
      "Training loss for batch 8071 : 0.5969408750534058\n",
      "Training loss for batch 8072 : 0.38375359773635864\n",
      "Training loss for batch 8073 : 0.03367094323039055\n",
      "Training loss for batch 8074 : 0.24106448888778687\n",
      "Training loss for batch 8075 : 0.10392200201749802\n",
      "Training loss for batch 8076 : 0.13720396161079407\n",
      "Training loss for batch 8077 : 0.05317971110343933\n",
      "Training loss for batch 8078 : 0.08122361451387405\n",
      "Training loss for batch 8079 : 0.1982068568468094\n",
      "Training loss for batch 8080 : 0.2626567482948303\n",
      "Training loss for batch 8081 : 0.07332385331392288\n",
      "Training loss for batch 8082 : 0.07958860695362091\n",
      "Training loss for batch 8083 : 0.10537955164909363\n",
      "Training loss for batch 8084 : 0.030785534530878067\n",
      "Training loss for batch 8085 : 0.29257258772850037\n",
      "Training loss for batch 8086 : 0.11828839033842087\n",
      "Training loss for batch 8087 : 0.04166444391012192\n",
      "Training loss for batch 8088 : 0.42325207591056824\n",
      "Training loss for batch 8089 : 0.3011576235294342\n",
      "Training loss for batch 8090 : 0.20388230681419373\n",
      "Training loss for batch 8091 : 0.25792527198791504\n",
      "Training loss for batch 8092 : 0.22281114757061005\n",
      "Training loss for batch 8093 : 0.15708111226558685\n",
      "Training loss for batch 8094 : 0.04041295498609543\n",
      "Training loss for batch 8095 : 0.24729357659816742\n",
      "Training loss for batch 8096 : 0.5283843278884888\n",
      "Training loss for batch 8097 : 0.06492428481578827\n",
      "Training loss for batch 8098 : 0.20967444777488708\n",
      "Training loss for batch 8099 : 0.054836783558130264\n",
      "Training loss for batch 8100 : 0.32625848054885864\n",
      "Training loss for batch 8101 : 0.12248878926038742\n",
      "Training loss for batch 8102 : 0.1949540674686432\n",
      "Training loss for batch 8103 : 0.21818652749061584\n",
      "Training loss for batch 8104 : 0.25399258732795715\n",
      "Training loss for batch 8105 : 0.06223197653889656\n",
      "Training loss for batch 8106 : 0.39499014616012573\n",
      "Training loss for batch 8107 : 0.0593537874519825\n",
      "Training loss for batch 8108 : 0.6131505370140076\n",
      "Training loss for batch 8109 : 0.187724307179451\n",
      "Training loss for batch 8110 : 0.027551651000976562\n",
      "Training loss for batch 8111 : 0.19325709342956543\n",
      "Training loss for batch 8112 : 0.21042364835739136\n",
      "Training loss for batch 8113 : 0.1406894028186798\n",
      "Training loss for batch 8114 : 0.1993962973356247\n",
      "Training loss for batch 8115 : 0.4701025187969208\n",
      "Training loss for batch 8116 : 0.27698278427124023\n",
      "Training loss for batch 8117 : 0.40403589606285095\n",
      "Training loss for batch 8118 : 0.2243657410144806\n",
      "Training loss for batch 8119 : 0.07114250957965851\n",
      "Training loss for batch 8120 : 0.20736907422542572\n",
      "Training loss for batch 8121 : 0.10488727688789368\n",
      "Training loss for batch 8122 : 0.08539938926696777\n",
      "Training loss for batch 8123 : 0.04028068855404854\n",
      "Training loss for batch 8124 : 0.15965136885643005\n",
      "Training loss for batch 8125 : 0.14431029558181763\n",
      "Training loss for batch 8126 : 0.2511163055896759\n",
      "Training loss for batch 8127 : 0.2910095155239105\n",
      "Training loss for batch 8128 : 0.3903436064720154\n",
      "Training loss for batch 8129 : 0.13516049087047577\n",
      "Training loss for batch 8130 : 0.49778205156326294\n",
      "Training loss for batch 8131 : 0.2454475462436676\n",
      "Training loss for batch 8132 : 0.11863449960947037\n",
      "Training loss for batch 8133 : 0.07572606205940247\n",
      "Training loss for batch 8134 : 0.299269437789917\n",
      "Training loss for batch 8135 : 0.27927860617637634\n",
      "Training loss for batch 8136 : 0.09047981351613998\n",
      "Training loss for batch 8137 : 0.3516053557395935\n",
      "Training loss for batch 8138 : 0.22230638563632965\n",
      "Training loss for batch 8139 : 0.29475197196006775\n",
      "Training loss for batch 8140 : 0.4904390573501587\n",
      "Training loss for batch 8141 : 0.054379381239414215\n",
      "Training loss for batch 8142 : 0.07347068190574646\n",
      "Training loss for batch 8143 : 0.17504779994487762\n",
      "Training loss for batch 8144 : 0.26288896799087524\n",
      "Training loss for batch 8145 : 0.2635476291179657\n",
      "Training loss for batch 8146 : 0.4124222695827484\n",
      "Training loss for batch 8147 : 0.1177583634853363\n",
      "Training loss for batch 8148 : 0.16959211230278015\n",
      "Training loss for batch 8149 : 0.10817230492830276\n",
      "Training loss for batch 8150 : 0.07765905559062958\n",
      "Training loss for batch 8151 : 0.79816073179245\n",
      "Training loss for batch 8152 : 0.20998245477676392\n",
      "Training loss for batch 8153 : 0.230250746011734\n",
      "Training loss for batch 8154 : 0.13498303294181824\n",
      "Training loss for batch 8155 : 0.09052731841802597\n",
      "Training loss for batch 8156 : 0.15512149035930634\n",
      "Training loss for batch 8157 : 0.21339015662670135\n",
      "Training loss for batch 8158 : 0.2638449966907501\n",
      "Training loss for batch 8159 : 0.02069968730211258\n",
      "Training loss for batch 8160 : 0.5875768661499023\n",
      "Training loss for batch 8161 : 0.039601441472768784\n",
      "Training loss for batch 8162 : 0.1878587007522583\n",
      "Training loss for batch 8163 : 0.23704937100410461\n",
      "Training loss for batch 8164 : 0.17323927581310272\n",
      "Training loss for batch 8165 : 0.10971670597791672\n",
      "Training loss for batch 8166 : 0.06791819632053375\n",
      "Training loss for batch 8167 : 0.053610071539878845\n",
      "Training loss for batch 8168 : 0.3586735129356384\n",
      "Training loss for batch 8169 : 0.20114022493362427\n",
      "Training loss for batch 8170 : 0.24660150706768036\n",
      "Training loss for batch 8171 : 0.16618147492408752\n",
      "Training loss for batch 8172 : 0.3408646285533905\n",
      "Training loss for batch 8173 : 0.20082829892635345\n",
      "Training loss for batch 8174 : 0.2343020737171173\n",
      "Training loss for batch 8175 : 0.09834250062704086\n",
      "Training loss for batch 8176 : 0.022403618320822716\n",
      "Training loss for batch 8177 : 0.2966049611568451\n",
      "Training loss for batch 8178 : 0.6016700863838196\n",
      "Training loss for batch 8179 : 0.3950825035572052\n",
      "Training loss for batch 8180 : 0.15433229506015778\n",
      "Training loss for batch 8181 : 0.17570239305496216\n",
      "Training loss for batch 8182 : 0.032287534326314926\n",
      "Training loss for batch 8183 : 0.17213690280914307\n",
      "Training loss for batch 8184 : 0.1930493861436844\n",
      "Training loss for batch 8185 : 0.3586878478527069\n",
      "Training loss for batch 8186 : 0.14617688953876495\n",
      "Training loss for batch 8187 : 0.031717486679553986\n",
      "Training loss for batch 8188 : 0.1283053308725357\n",
      "Training loss for batch 8189 : 0.06976732611656189\n",
      "Training loss for batch 8190 : 0.21060042083263397\n",
      "Training loss for batch 8191 : 0.13320055603981018\n",
      "Training loss for batch 8192 : 0.1988738775253296\n",
      "Training loss for batch 8193 : 0.046150363981723785\n",
      "Training loss for batch 8194 : 0.18061701953411102\n",
      "Training loss for batch 8195 : 0.2386971116065979\n",
      "Training loss for batch 8196 : 0.21904850006103516\n",
      "Training loss for batch 8197 : 0.06175333261489868\n",
      "Training loss for batch 8198 : 0.23745685815811157\n",
      "Training loss for batch 8199 : 0.23667769134044647\n",
      "Training loss for batch 8200 : 0.0193169042468071\n",
      "Training loss for batch 8201 : 0.22002418339252472\n",
      "Training loss for batch 8202 : 0.18446697294712067\n",
      "Training loss for batch 8203 : 0.24164867401123047\n",
      "Training loss for batch 8204 : 0.061947084963321686\n",
      "Training loss for batch 8205 : 0.14853118360042572\n",
      "Training loss for batch 8206 : 0.3306598365306854\n",
      "Training loss for batch 8207 : 0.373477578163147\n",
      "Training loss for batch 8208 : 0.21264033019542694\n",
      "Training loss for batch 8209 : 0.16800238192081451\n",
      "Training loss for batch 8210 : 0.12161484360694885\n",
      "Training loss for batch 8211 : 0.24186402559280396\n",
      "Training loss for batch 8212 : 0.1911449134349823\n",
      "Training loss for batch 8213 : 0.070222407579422\n",
      "Training loss for batch 8214 : 0.47415870428085327\n",
      "Training loss for batch 8215 : 0.18416325747966766\n",
      "Training loss for batch 8216 : 0.3418417274951935\n",
      "Training loss for batch 8217 : 0.23586265742778778\n",
      "Training loss for batch 8218 : 0.022978220134973526\n",
      "Training loss for batch 8219 : 0.16479873657226562\n",
      "Training loss for batch 8220 : 0.08538483828306198\n",
      "Training loss for batch 8221 : 0.21249307692050934\n",
      "Training loss for batch 8222 : 0.06818488240242004\n",
      "Training loss for batch 8223 : 0.2859380841255188\n",
      "Training loss for batch 8224 : 0.015251587145030499\n",
      "Training loss for batch 8225 : 0.23816102743148804\n",
      "Training loss for batch 8226 : 0.1426357626914978\n",
      "Training loss for batch 8227 : 0.09196167439222336\n",
      "Training loss for batch 8228 : 0.33171191811561584\n",
      "Training loss for batch 8229 : 0.30693715810775757\n",
      "Training loss for batch 8230 : 0.4088987112045288\n",
      "Training loss for batch 8231 : 0.357389360666275\n",
      "Training loss for batch 8232 : 0.18118120729923248\n",
      "Training loss for batch 8233 : 0.4302002191543579\n",
      "Training loss for batch 8234 : 0.19862566888332367\n",
      "Training loss for batch 8235 : 0.5415582656860352\n",
      "Training loss for batch 8236 : 0.47795814275741577\n",
      "Training loss for batch 8237 : 0.0779712125658989\n",
      "Training loss for batch 8238 : 0.3079487085342407\n",
      "Training loss for batch 8239 : 0.038994867354631424\n",
      "Training loss for batch 8240 : 0.250093013048172\n",
      "Training loss for batch 8241 : 0.3491523265838623\n",
      "Training loss for batch 8242 : 0.40365588665008545\n",
      "Training loss for batch 8243 : 0.07049494981765747\n",
      "Training loss for batch 8244 : 0.10561752319335938\n",
      "Training loss for batch 8245 : 0.22802890837192535\n",
      "Training loss for batch 8246 : 0.3166429102420807\n",
      "Training loss for batch 8247 : 0.13698412477970123\n",
      "Training loss for batch 8248 : 0.5873492360115051\n",
      "Training loss for batch 8249 : 0.27620774507522583\n",
      "Training loss for batch 8250 : 0.13033899664878845\n",
      "Training loss for batch 8251 : 0.37661001086235046\n",
      "Training loss for batch 8252 : 0.025380978360772133\n",
      "Training loss for batch 8253 : 0.11236482113599777\n",
      "Training loss for batch 8254 : 0.3701533079147339\n",
      "Training loss for batch 8255 : 0.17327360808849335\n",
      "Training loss for batch 8256 : 0.21437816321849823\n",
      "Training loss for batch 8257 : 0.29074910283088684\n",
      "Training loss for batch 8258 : 0.1524699628353119\n",
      "Training loss for batch 8259 : 0.24414792656898499\n",
      "Training loss for batch 8260 : 0.3262382745742798\n",
      "Training loss for batch 8261 : 0.27575990557670593\n",
      "Training loss for batch 8262 : 0.22248898446559906\n",
      "Training loss for batch 8263 : 0.09629292041063309\n",
      "Training loss for batch 8264 : 0.10106971114873886\n",
      "Training loss for batch 8265 : 0.34538525342941284\n",
      "Training loss for batch 8266 : 0.027212902903556824\n",
      "Training loss for batch 8267 : 0.19436314702033997\n",
      "Training loss for batch 8268 : 0.425398051738739\n",
      "Training loss for batch 8269 : 0.06886883080005646\n",
      "Training loss for batch 8270 : 0.1819135546684265\n",
      "Training loss for batch 8271 : 0.21905049681663513\n",
      "Training loss for batch 8272 : 0.012116402387619019\n",
      "Training loss for batch 8273 : 0.4545557498931885\n",
      "Training loss for batch 8274 : 0.28233304619789124\n",
      "Training loss for batch 8275 : 0.16716772317886353\n",
      "Training loss for batch 8276 : 0.12985055148601532\n",
      "Training loss for batch 8277 : 0.020781168714165688\n",
      "Training loss for batch 8278 : 0.0805453360080719\n",
      "Training loss for batch 8279 : 0.21488319337368011\n",
      "Training loss for batch 8280 : 0.17605718970298767\n",
      "Training loss for batch 8281 : 0.047844450920820236\n",
      "Training loss for batch 8282 : 0.14291667938232422\n",
      "Training loss for batch 8283 : 0.19531980156898499\n",
      "Training loss for batch 8284 : 0.19752660393714905\n",
      "Training loss for batch 8285 : 0.11358176916837692\n",
      "Training loss for batch 8286 : 0.02067006751894951\n",
      "Training loss for batch 8287 : 0.22710679471492767\n",
      "Training loss for batch 8288 : 0.4112335443496704\n",
      "Training loss for batch 8289 : 0.35765936970710754\n",
      "Training loss for batch 8290 : 0.12330687046051025\n",
      "Training loss for batch 8291 : 0.35463768243789673\n",
      "Training loss for batch 8292 : 0.04712500795722008\n",
      "Training loss for batch 8293 : 0.3410395383834839\n",
      "Training loss for batch 8294 : 0.5186505913734436\n",
      "Training loss for batch 8295 : 0.12372835725545883\n",
      "Training loss for batch 8296 : 0.05602150037884712\n",
      "Training loss for batch 8297 : 0.09527096152305603\n",
      "Training loss for batch 8298 : 0.06796301901340485\n",
      "Training loss for batch 8299 : 0.16415973007678986\n",
      "Training loss for batch 8300 : 0.2239149808883667\n",
      "Training loss for batch 8301 : 0.09773636609315872\n",
      "Training loss for batch 8302 : 0.2353062480688095\n",
      "Training loss for batch 8303 : 0.1292133927345276\n",
      "Training loss for batch 8304 : 0.14804044365882874\n",
      "Training loss for batch 8305 : 0.06603559106588364\n",
      "Training loss for batch 8306 : 0.2695653736591339\n",
      "Training loss for batch 8307 : 0.919803261756897\n",
      "Parameter containing:\n",
      "tensor(-0.0802, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 0.9197902083396912\n",
      "Training loss for batch 1 : 0.9197726845741272\n",
      "Training loss for batch 2 : 0.9197510480880737\n",
      "Training loss for batch 3 : 0.9197260141372681\n",
      "Training loss for batch 4 : 0.9196979403495789\n",
      "Training loss for batch 5 : 0.9196673631668091\n",
      "Training loss for batch 6 : 0.9196346998214722\n",
      "Training loss for batch 7 : 0.9196001291275024\n",
      "Training loss for batch 8 : 0.9195639491081238\n",
      "Training loss for batch 9 : 0.9195263981819153\n",
      "Training loss for batch 10 : 0.9194878339767456\n",
      "Training loss for batch 11 : 0.9194482564926147\n",
      "Training loss for batch 12 : 0.22842513024806976\n",
      "Training loss for batch 13 : 0.0\n",
      "Training loss for batch 14 : 0.07335299998521805\n",
      "Training loss for batch 15 : 0.08057890832424164\n",
      "Training loss for batch 16 : 0.054980456829071045\n",
      "Training loss for batch 17 : 0.07311451435089111\n",
      "Training loss for batch 18 : 0.1099364385008812\n",
      "Training loss for batch 19 : 0.1744501143693924\n",
      "Training loss for batch 20 : 0.28508540987968445\n",
      "Training loss for batch 21 : 0.09844504296779633\n",
      "Training loss for batch 22 : 0.024364084005355835\n",
      "Training loss for batch 23 : 0.04657580703496933\n",
      "Training loss for batch 24 : 0.015851598232984543\n",
      "Training loss for batch 25 : 0.20770491659641266\n",
      "Training loss for batch 26 : 0.06698601692914963\n",
      "Training loss for batch 27 : 0.11275093257427216\n",
      "Training loss for batch 28 : 0.014502700418233871\n",
      "Training loss for batch 29 : 0.2747919261455536\n",
      "Training loss for batch 30 : 0.0932627022266388\n",
      "Training loss for batch 31 : 0.24449452757835388\n",
      "Training loss for batch 32 : 0.23350264132022858\n",
      "Training loss for batch 33 : 0.1979936957359314\n",
      "Training loss for batch 34 : 0.3087776303291321\n",
      "Training loss for batch 35 : 0.38084328174591064\n",
      "Training loss for batch 36 : 0.13152727484703064\n",
      "Training loss for batch 37 : 0.20556749403476715\n",
      "Training loss for batch 38 : 0.015521742403507233\n",
      "Training loss for batch 39 : 0.18132136762142181\n",
      "Training loss for batch 40 : 0.11412879079580307\n",
      "Training loss for batch 41 : 0.17083266377449036\n",
      "Training loss for batch 42 : 0.5185808539390564\n",
      "Training loss for batch 43 : 0.16680637001991272\n",
      "Training loss for batch 44 : 0.10261279344558716\n",
      "Training loss for batch 45 : 0.059672169387340546\n",
      "Training loss for batch 46 : 0.05104125291109085\n",
      "Training loss for batch 47 : 0.13774995505809784\n",
      "Training loss for batch 48 : 0.14043766260147095\n",
      "Training loss for batch 49 : 0.0454496406018734\n",
      "Training loss for batch 50 : 0.06549317389726639\n",
      "Training loss for batch 51 : 0.396649569272995\n",
      "Training loss for batch 52 : 0.139805406332016\n",
      "Training loss for batch 53 : 0.2223062366247177\n",
      "Training loss for batch 54 : 0.0\n",
      "Training loss for batch 55 : 0.03298512101173401\n",
      "Training loss for batch 56 : 0.09907711297273636\n",
      "Training loss for batch 57 : 0.012526720762252808\n",
      "Training loss for batch 58 : 0.19670221209526062\n",
      "Training loss for batch 59 : 0.1500352919101715\n",
      "Training loss for batch 60 : 0.15233857929706573\n",
      "Training loss for batch 61 : 0.046196989715099335\n",
      "Training loss for batch 62 : 0.22115516662597656\n",
      "Training loss for batch 63 : 0.12765169143676758\n",
      "Training loss for batch 64 : 0.0673682764172554\n",
      "Training loss for batch 65 : 0.4318476915359497\n",
      "Training loss for batch 66 : 0.18639515340328217\n",
      "Training loss for batch 67 : 0.08849858492612839\n",
      "Training loss for batch 68 : 0.12494087219238281\n",
      "Training loss for batch 69 : 0.15392592549324036\n",
      "Training loss for batch 70 : 0.11430342495441437\n",
      "Training loss for batch 71 : 0.12638011574745178\n",
      "Training loss for batch 72 : 0.037854261696338654\n",
      "Training loss for batch 73 : 0.1206880658864975\n",
      "Training loss for batch 74 : 0.20588384568691254\n",
      "Training loss for batch 75 : 0.11861855536699295\n",
      "Training loss for batch 76 : 0.05283742770552635\n",
      "Training loss for batch 77 : 0.10048631578683853\n",
      "Training loss for batch 78 : 0.1191825270652771\n",
      "Training loss for batch 79 : 0.13051386177539825\n",
      "Training loss for batch 80 : 0.2970231771469116\n",
      "Training loss for batch 81 : 0.0007449118420481682\n",
      "Training loss for batch 82 : 0.17582707107067108\n",
      "Training loss for batch 83 : 0.006768941879272461\n",
      "Training loss for batch 84 : 0.13857892155647278\n",
      "Training loss for batch 85 : 0.13876697421073914\n",
      "Training loss for batch 86 : 0.3551335632801056\n",
      "Training loss for batch 87 : -0.0015275990590453148\n",
      "Training loss for batch 88 : 0.26455557346343994\n",
      "Training loss for batch 89 : 0.41868406534194946\n",
      "Training loss for batch 90 : 0.044396139681339264\n",
      "Training loss for batch 91 : 0.3390197157859802\n",
      "Training loss for batch 92 : 0.160537451505661\n",
      "Training loss for batch 93 : 0.06616804003715515\n",
      "Training loss for batch 94 : 0.11487165093421936\n",
      "Training loss for batch 95 : 0.051012612879276276\n",
      "Training loss for batch 96 : 0.0990850180387497\n",
      "Training loss for batch 97 : 0.030036263167858124\n",
      "Training loss for batch 98 : 0.10340571403503418\n",
      "Training loss for batch 99 : 0.05234252288937569\n",
      "Training loss for batch 100 : 0.017964422702789307\n",
      "Training loss for batch 101 : 0.40440261363983154\n",
      "Training loss for batch 102 : 0.15130218863487244\n",
      "Training loss for batch 103 : 0.41970932483673096\n",
      "Training loss for batch 104 : 0.5938686728477478\n",
      "Training loss for batch 105 : 0.061816856265068054\n",
      "Training loss for batch 106 : 0.1697341948747635\n",
      "Training loss for batch 107 : 0.1328955590724945\n",
      "Training loss for batch 108 : 0.12127692997455597\n",
      "Training loss for batch 109 : 0.16706207394599915\n",
      "Training loss for batch 110 : 0.1941046267747879\n",
      "Training loss for batch 111 : 0.2833152413368225\n",
      "Training loss for batch 112 : 0.3720915615558624\n",
      "Training loss for batch 113 : 0.2191677689552307\n",
      "Training loss for batch 114 : 4.64320182800293e-05\n",
      "Training loss for batch 115 : 0.101376011967659\n",
      "Training loss for batch 116 : 0.24940486252307892\n",
      "Training loss for batch 117 : 0.27996572852134705\n",
      "Training loss for batch 118 : 0.008285490795969963\n",
      "Training loss for batch 119 : 0.267395943403244\n",
      "Training loss for batch 120 : 0.10879717767238617\n",
      "Training loss for batch 121 : 0.0\n",
      "Training loss for batch 122 : 0.1586213856935501\n",
      "Training loss for batch 123 : 0.07228028774261475\n",
      "Training loss for batch 124 : 0.3141738772392273\n",
      "Training loss for batch 125 : 0.357099711894989\n",
      "Training loss for batch 126 : 0.011018584482371807\n",
      "Training loss for batch 127 : 0.007799113169312477\n",
      "Training loss for batch 128 : 0.1079874262213707\n",
      "Training loss for batch 129 : 0.28697261214256287\n",
      "Training loss for batch 130 : 0.05989769846200943\n",
      "Training loss for batch 131 : 0.6012488007545471\n",
      "Training loss for batch 132 : 0.33310896158218384\n",
      "Training loss for batch 133 : 0.02893895097076893\n",
      "Training loss for batch 134 : 0.1867060363292694\n",
      "Training loss for batch 135 : 0.0013462232891470194\n",
      "Training loss for batch 136 : 0.04112527146935463\n",
      "Training loss for batch 137 : 0.03785865381360054\n",
      "Training loss for batch 138 : 0.10706065595149994\n",
      "Training loss for batch 139 : 0.32391029596328735\n",
      "Training loss for batch 140 : 0.07425990700721741\n",
      "Training loss for batch 141 : 0.19329982995986938\n",
      "Training loss for batch 142 : 0.05280644819140434\n",
      "Training loss for batch 143 : 0.2512708902359009\n",
      "Training loss for batch 144 : 0.10177398473024368\n",
      "Training loss for batch 145 : 0.5146113038063049\n",
      "Training loss for batch 146 : 0.1735694855451584\n",
      "Training loss for batch 147 : 0.09048744291067123\n",
      "Training loss for batch 148 : 0.327573299407959\n",
      "Training loss for batch 149 : 0.05654006823897362\n",
      "Training loss for batch 150 : 0.013993566855788231\n",
      "Training loss for batch 151 : 0.08275045454502106\n",
      "Training loss for batch 152 : 0.14160515367984772\n",
      "Training loss for batch 153 : 0.28214162588119507\n",
      "Training loss for batch 154 : 0.14385360479354858\n",
      "Training loss for batch 155 : 0.04502972587943077\n",
      "Training loss for batch 156 : 0.19403541088104248\n",
      "Training loss for batch 157 : 0.21895602345466614\n",
      "Training loss for batch 158 : 0.021614814177155495\n",
      "Training loss for batch 159 : 0.1986604630947113\n",
      "Training loss for batch 160 : 0.11855531483888626\n",
      "Training loss for batch 161 : 0.017849069088697433\n",
      "Training loss for batch 162 : 0.036624908447265625\n",
      "Training loss for batch 163 : 0.15699362754821777\n",
      "Training loss for batch 164 : 0.18601219356060028\n",
      "Training loss for batch 165 : 0.1668844223022461\n",
      "Training loss for batch 166 : 0.18992523849010468\n",
      "Training loss for batch 167 : 0.3804681599140167\n",
      "Training loss for batch 168 : 0.22886592149734497\n",
      "Training loss for batch 169 : 0.0668909102678299\n",
      "Training loss for batch 170 : 0.015539799816906452\n",
      "Training loss for batch 171 : 0.245372012257576\n",
      "Training loss for batch 172 : 0.031621407717466354\n",
      "Training loss for batch 173 : 0.32379838824272156\n",
      "Training loss for batch 174 : 0.08383617550134659\n",
      "Training loss for batch 175 : 0.04298132285475731\n",
      "Training loss for batch 176 : 0.09204982221126556\n",
      "Training loss for batch 177 : 0.23273953795433044\n",
      "Training loss for batch 178 : 0.10046984255313873\n",
      "Training loss for batch 179 : 0.0914471223950386\n",
      "Training loss for batch 180 : 0.3451063334941864\n",
      "Training loss for batch 181 : 0.09204766899347305\n",
      "Training loss for batch 182 : 0.3003745973110199\n",
      "Training loss for batch 183 : 0.20717567205429077\n",
      "Training loss for batch 184 : 0.053606029599905014\n",
      "Training loss for batch 185 : 0.018883604556322098\n",
      "Training loss for batch 186 : 0.23450830578804016\n",
      "Training loss for batch 187 : 0.1833469569683075\n",
      "Training loss for batch 188 : 0.040638186037540436\n",
      "Training loss for batch 189 : 0.017349274829030037\n",
      "Training loss for batch 190 : 0.37478435039520264\n",
      "Training loss for batch 191 : 0.07215550541877747\n",
      "Training loss for batch 192 : 0.3107263445854187\n",
      "Training loss for batch 193 : 0.05818384513258934\n",
      "Training loss for batch 194 : 0.07077355682849884\n",
      "Training loss for batch 195 : 0.13741464912891388\n",
      "Training loss for batch 196 : 0.32945117354393005\n",
      "Training loss for batch 197 : 0.0942395031452179\n",
      "Training loss for batch 198 : 0.1905941516160965\n",
      "Training loss for batch 199 : 0.28970053791999817\n",
      "Training loss for batch 200 : 0.35890698432922363\n",
      "Training loss for batch 201 : 0.07184220105409622\n",
      "Training loss for batch 202 : 0.10828061401844025\n",
      "Training loss for batch 203 : 0.18839356303215027\n",
      "Training loss for batch 204 : 0.056618161499500275\n",
      "Training loss for batch 205 : 0.05446900427341461\n",
      "Training loss for batch 206 : 0.04323980584740639\n",
      "Training loss for batch 207 : 0.08531612157821655\n",
      "Training loss for batch 208 : 0.03139294311404228\n",
      "Training loss for batch 209 : 0.27974098920822144\n",
      "Training loss for batch 210 : 0.0\n",
      "Training loss for batch 211 : 0.17107723653316498\n",
      "Training loss for batch 212 : 0.3181312680244446\n",
      "Training loss for batch 213 : 0.08060640841722488\n",
      "Training loss for batch 214 : 0.301440954208374\n",
      "Training loss for batch 215 : 0.13036836683750153\n",
      "Training loss for batch 216 : 0.15856677293777466\n",
      "Training loss for batch 217 : 0.03842112049460411\n",
      "Training loss for batch 218 : 0.07983924448490143\n",
      "Training loss for batch 219 : 0.08005153387784958\n",
      "Training loss for batch 220 : 0.2016359269618988\n",
      "Training loss for batch 221 : 0.08704891055822372\n",
      "Training loss for batch 222 : 0.06103817746043205\n",
      "Training loss for batch 223 : 0.24158917367458344\n",
      "Training loss for batch 224 : 0.22536489367485046\n",
      "Training loss for batch 225 : 0.07323425263166428\n",
      "Training loss for batch 226 : 0.014606545679271221\n",
      "Training loss for batch 227 : 0.14960961043834686\n",
      "Training loss for batch 228 : 0.07727661728858948\n",
      "Training loss for batch 229 : 0.0855756551027298\n",
      "Training loss for batch 230 : 0.1865220069885254\n",
      "Training loss for batch 231 : 0.11639939248561859\n",
      "Training loss for batch 232 : 0.13791343569755554\n",
      "Training loss for batch 233 : 0.24898289144039154\n",
      "Training loss for batch 234 : 0.19718758761882782\n",
      "Training loss for batch 235 : 0.055624283850193024\n",
      "Training loss for batch 236 : 0.444619357585907\n",
      "Training loss for batch 237 : 0.1588718444108963\n",
      "Training loss for batch 238 : 0.02603403851389885\n",
      "Training loss for batch 239 : 0.30264174938201904\n",
      "Training loss for batch 240 : 0.050466109067201614\n",
      "Training loss for batch 241 : 0.22579358518123627\n",
      "Training loss for batch 242 : 0.04226218909025192\n",
      "Training loss for batch 243 : 0.34165459871292114\n",
      "Training loss for batch 244 : 0.17776770889759064\n",
      "Training loss for batch 245 : 0.050107020884752274\n",
      "Training loss for batch 246 : 0.268659383058548\n",
      "Training loss for batch 247 : 0.008766929619014263\n",
      "Training loss for batch 248 : 0.10970686376094818\n",
      "Training loss for batch 249 : 0.13756464421749115\n",
      "Training loss for batch 250 : 0.012620161287486553\n",
      "Training loss for batch 251 : 0.38924187421798706\n",
      "Training loss for batch 252 : 0.08513914793729782\n",
      "Training loss for batch 253 : 0.10375229269266129\n",
      "Training loss for batch 254 : 0.3018898665904999\n",
      "Training loss for batch 255 : 0.059855975210666656\n",
      "Training loss for batch 256 : 0.08840680867433548\n",
      "Training loss for batch 257 : 0.029757220298051834\n",
      "Training loss for batch 258 : 0.00489655090495944\n",
      "Training loss for batch 259 : 0.16665540635585785\n",
      "Training loss for batch 260 : 0.14543721079826355\n",
      "Training loss for batch 261 : 0.158915713429451\n",
      "Training loss for batch 262 : 0.3489043712615967\n",
      "Training loss for batch 263 : 0.05798783525824547\n",
      "Training loss for batch 264 : 0.03680931776762009\n",
      "Training loss for batch 265 : 0.5277153849601746\n",
      "Training loss for batch 266 : 0.09284757822751999\n",
      "Training loss for batch 267 : 0.367477148771286\n",
      "Training loss for batch 268 : 0.08412992209196091\n",
      "Training loss for batch 269 : 0.3261639475822449\n",
      "Training loss for batch 270 : 0.33081308007240295\n",
      "Training loss for batch 271 : 0.24873001873493195\n",
      "Training loss for batch 272 : 0.29249587655067444\n",
      "Training loss for batch 273 : 0.2710232734680176\n",
      "Training loss for batch 274 : 0.021703770384192467\n",
      "Training loss for batch 275 : 0.028249839320778847\n",
      "Training loss for batch 276 : 0.28529098629951477\n",
      "Training loss for batch 277 : 0.15767598152160645\n",
      "Training loss for batch 278 : 0.019956087693572044\n",
      "Training loss for batch 279 : 0.05115065351128578\n",
      "Training loss for batch 280 : 0.2375694364309311\n",
      "Training loss for batch 281 : 0.04994383081793785\n",
      "Training loss for batch 282 : 0.22491981089115143\n",
      "Training loss for batch 283 : 0.20075315237045288\n",
      "Training loss for batch 284 : 0.24674516916275024\n",
      "Training loss for batch 285 : 0.14815446734428406\n",
      "Training loss for batch 286 : 0.13014645874500275\n",
      "Training loss for batch 287 : 0.26999250054359436\n",
      "Training loss for batch 288 : 0.1150367483496666\n",
      "Training loss for batch 289 : 0.15727585554122925\n",
      "Training loss for batch 290 : 0.12699218094348907\n",
      "Training loss for batch 291 : 0.06171288713812828\n",
      "Training loss for batch 292 : 0.06299888342618942\n",
      "Training loss for batch 293 : 0.08333271741867065\n",
      "Training loss for batch 294 : 0.02974945306777954\n",
      "Training loss for batch 295 : 0.13913148641586304\n",
      "Training loss for batch 296 : 0.07184665650129318\n",
      "Training loss for batch 297 : 0.17921510338783264\n",
      "Training loss for batch 298 : 0.24245253205299377\n",
      "Training loss for batch 299 : 0.20326849818229675\n",
      "Training loss for batch 300 : 0.16629739105701447\n",
      "Training loss for batch 301 : 0.433034747838974\n",
      "Training loss for batch 302 : 0.1636308878660202\n",
      "Training loss for batch 303 : 0.083960622549057\n",
      "Training loss for batch 304 : 0.08372671902179718\n",
      "Training loss for batch 305 : 0.08431559056043625\n",
      "Training loss for batch 306 : 0.04870212823152542\n",
      "Training loss for batch 307 : 0.2910136282444\n",
      "Training loss for batch 308 : 0.19592876732349396\n",
      "Training loss for batch 309 : 0.027580665424466133\n",
      "Training loss for batch 310 : 0.11743217706680298\n",
      "Training loss for batch 311 : 0.032200444489717484\n",
      "Training loss for batch 312 : 0.014770478010177612\n",
      "Training loss for batch 313 : 0.05803650617599487\n",
      "Training loss for batch 314 : 0.056169718503952026\n",
      "Training loss for batch 315 : 0.05437946692109108\n",
      "Training loss for batch 316 : 0.05374079570174217\n",
      "Training loss for batch 317 : 0.020234253257513046\n",
      "Training loss for batch 318 : 0.026891328394412994\n",
      "Training loss for batch 319 : 0.24582014977931976\n",
      "Training loss for batch 320 : 0.10632288455963135\n",
      "Training loss for batch 321 : 0.11011462658643723\n",
      "Training loss for batch 322 : 0.32654398679733276\n",
      "Training loss for batch 323 : 0.057004667818546295\n",
      "Training loss for batch 324 : 0.189763605594635\n",
      "Training loss for batch 325 : 0.19633258879184723\n",
      "Training loss for batch 326 : 0.31240853667259216\n",
      "Training loss for batch 327 : 0.34971556067466736\n",
      "Training loss for batch 328 : 0.1335446536540985\n",
      "Training loss for batch 329 : 0.1321483850479126\n",
      "Training loss for batch 330 : 0.24845387041568756\n",
      "Training loss for batch 331 : 0.04530475288629532\n",
      "Training loss for batch 332 : 0.03823050484061241\n",
      "Training loss for batch 333 : 0.0\n",
      "Training loss for batch 334 : 0.16282878816127777\n",
      "Training loss for batch 335 : 0.1935294270515442\n",
      "Training loss for batch 336 : 0.033509086817502975\n",
      "Training loss for batch 337 : 0.17058764398097992\n",
      "Training loss for batch 338 : 0.06512633711099625\n",
      "Training loss for batch 339 : 0.05182984471321106\n",
      "Training loss for batch 340 : 0.02950654923915863\n",
      "Training loss for batch 341 : 0.31758710741996765\n",
      "Training loss for batch 342 : 0.33704057335853577\n",
      "Training loss for batch 343 : 0.13153354823589325\n",
      "Training loss for batch 344 : 0.16515958309173584\n",
      "Training loss for batch 345 : 0.019333457574248314\n",
      "Training loss for batch 346 : 0.13304373621940613\n",
      "Training loss for batch 347 : 0.029809124767780304\n",
      "Training loss for batch 348 : 0.2349039763212204\n",
      "Training loss for batch 349 : 0.257877379655838\n",
      "Training loss for batch 350 : 0.08007292449474335\n",
      "Training loss for batch 351 : 0.24664078652858734\n",
      "Training loss for batch 352 : 0.1364763379096985\n",
      "Training loss for batch 353 : 0.20187941193580627\n",
      "Training loss for batch 354 : 0.08885873854160309\n",
      "Training loss for batch 355 : 0.00808003917336464\n",
      "Training loss for batch 356 : 0.313614159822464\n",
      "Training loss for batch 357 : 0.11355554312467575\n",
      "Training loss for batch 358 : 0.05687541142106056\n",
      "Training loss for batch 359 : 0.001411308883689344\n",
      "Training loss for batch 360 : 0.3024849593639374\n",
      "Training loss for batch 361 : 0.2193223237991333\n",
      "Training loss for batch 362 : 0.12937098741531372\n",
      "Training loss for batch 363 : 0.026207556948065758\n",
      "Training loss for batch 364 : 0.020443597808480263\n",
      "Training loss for batch 365 : 0.1286756843328476\n",
      "Training loss for batch 366 : 0.20824085175991058\n",
      "Training loss for batch 367 : 0.19790852069854736\n",
      "Training loss for batch 368 : 0.0028873286210000515\n",
      "Training loss for batch 369 : 0.15541265904903412\n",
      "Training loss for batch 370 : 0.30304473638534546\n",
      "Training loss for batch 371 : 0.10760127007961273\n",
      "Training loss for batch 372 : 0.4964989423751831\n",
      "Training loss for batch 373 : 0.19040365517139435\n",
      "Training loss for batch 374 : 0.16170209646224976\n",
      "Training loss for batch 375 : 0.02105102315545082\n",
      "Training loss for batch 376 : 0.01799960248172283\n",
      "Training loss for batch 377 : 0.055685702711343765\n",
      "Training loss for batch 378 : 0.06645907461643219\n",
      "Training loss for batch 379 : 0.2607688009738922\n",
      "Training loss for batch 380 : 0.19979192316532135\n",
      "Training loss for batch 381 : 0.1731010377407074\n",
      "Training loss for batch 382 : 0.11598386615514755\n",
      "Training loss for batch 383 : 0.6334302425384521\n",
      "Training loss for batch 384 : 0.33328723907470703\n",
      "Training loss for batch 385 : 0.4421892464160919\n",
      "Training loss for batch 386 : 0.1364390105009079\n",
      "Training loss for batch 387 : 0.204255148768425\n",
      "Training loss for batch 388 : 0.0\n",
      "Training loss for batch 389 : 0.14068058133125305\n",
      "Training loss for batch 390 : 0.07802768051624298\n",
      "Training loss for batch 391 : 0.16324390470981598\n",
      "Training loss for batch 392 : 0.015158397145569324\n",
      "Training loss for batch 393 : 0.08298017084598541\n",
      "Training loss for batch 394 : 0.0\n",
      "Training loss for batch 395 : 0.13343068957328796\n",
      "Training loss for batch 396 : 0.2060295194387436\n",
      "Training loss for batch 397 : 0.12180981040000916\n",
      "Training loss for batch 398 : 0.07674489915370941\n",
      "Training loss for batch 399 : 0.04202267527580261\n",
      "Training loss for batch 400 : 0.14630253612995148\n",
      "Training loss for batch 401 : 0.021330576390028\n",
      "Training loss for batch 402 : 0.1255846619606018\n",
      "Training loss for batch 403 : 0.20442838966846466\n",
      "Training loss for batch 404 : 0.1417190134525299\n",
      "Training loss for batch 405 : 0.16583968698978424\n",
      "Training loss for batch 406 : 0.12617573142051697\n",
      "Training loss for batch 407 : 0.0590922012925148\n",
      "Training loss for batch 408 : 0.02748638205230236\n",
      "Training loss for batch 409 : 0.12321498245000839\n",
      "Training loss for batch 410 : 0.2783048748970032\n",
      "Training loss for batch 411 : 0.08051113784313202\n",
      "Training loss for batch 412 : 0.02581539750099182\n",
      "Training loss for batch 413 : 0.10614507645368576\n",
      "Training loss for batch 414 : 0.2198038101196289\n",
      "Training loss for batch 415 : 0.06870434433221817\n",
      "Training loss for batch 416 : 0.12284818291664124\n",
      "Training loss for batch 417 : 0.27209487557411194\n",
      "Training loss for batch 418 : 0.0\n",
      "Training loss for batch 419 : 0.4748431146144867\n",
      "Training loss for batch 420 : 0.1735689342021942\n",
      "Training loss for batch 421 : 0.1009366363286972\n",
      "Training loss for batch 422 : 0.26659736037254333\n",
      "Training loss for batch 423 : 0.41019734740257263\n",
      "Training loss for batch 424 : 0.18101516366004944\n",
      "Training loss for batch 425 : 0.0\n",
      "Training loss for batch 426 : 0.4979572594165802\n",
      "Training loss for batch 427 : 0.22088366746902466\n",
      "Training loss for batch 428 : 0.0048838271759450436\n",
      "Training loss for batch 429 : 0.014033108949661255\n",
      "Training loss for batch 430 : 0.18253977596759796\n",
      "Training loss for batch 431 : 0.21416015923023224\n",
      "Training loss for batch 432 : 0.12961238622665405\n",
      "Training loss for batch 433 : 0.1814308762550354\n",
      "Training loss for batch 434 : 0.15297472476959229\n",
      "Training loss for batch 435 : 0.11766554415225983\n",
      "Training loss for batch 436 : 0.23939435184001923\n",
      "Training loss for batch 437 : 0.05374215170741081\n",
      "Training loss for batch 438 : 0.0\n",
      "Training loss for batch 439 : 0.15407590568065643\n",
      "Training loss for batch 440 : 0.05305115878582001\n",
      "Training loss for batch 441 : 0.3037053346633911\n",
      "Training loss for batch 442 : 0.02287459559738636\n",
      "Training loss for batch 443 : 0.040159426629543304\n",
      "Training loss for batch 444 : 0.008729921653866768\n",
      "Training loss for batch 445 : 0.3489859104156494\n",
      "Training loss for batch 446 : 0.051358677446842194\n",
      "Training loss for batch 447 : 0.19980980455875397\n",
      "Training loss for batch 448 : 0.18980981409549713\n",
      "Training loss for batch 449 : 0.21003898978233337\n",
      "Training loss for batch 450 : 0.3087863624095917\n",
      "Training loss for batch 451 : 0.03091517463326454\n",
      "Training loss for batch 452 : 0.06854073703289032\n",
      "Training loss for batch 453 : 0.38117536902427673\n",
      "Training loss for batch 454 : 0.18639960885047913\n",
      "Training loss for batch 455 : 0.048871129751205444\n",
      "Training loss for batch 456 : 0.4472706615924835\n",
      "Training loss for batch 457 : 0.09228344261646271\n",
      "Training loss for batch 458 : 0.35498034954071045\n",
      "Training loss for batch 459 : 0.012206761166453362\n",
      "Training loss for batch 460 : 0.16158263385295868\n",
      "Training loss for batch 461 : 0.043027784675359726\n",
      "Training loss for batch 462 : 0.17065119743347168\n",
      "Training loss for batch 463 : 0.041043709963560104\n",
      "Training loss for batch 464 : 0.3586783707141876\n",
      "Training loss for batch 465 : 0.1017967015504837\n",
      "Training loss for batch 466 : -0.00015996256843209267\n",
      "Training loss for batch 467 : 0.038128335028886795\n",
      "Training loss for batch 468 : 0.13903747498989105\n",
      "Training loss for batch 469 : 0.053757935762405396\n",
      "Training loss for batch 470 : 0.1435958743095398\n",
      "Training loss for batch 471 : 0.005447968374937773\n",
      "Training loss for batch 472 : 0.3309098482131958\n",
      "Training loss for batch 473 : 0.06001851707696915\n",
      "Training loss for batch 474 : 0.07097771763801575\n",
      "Training loss for batch 475 : 0.09605024755001068\n",
      "Training loss for batch 476 : 0.36930355429649353\n",
      "Training loss for batch 477 : 0.0009089245577342808\n",
      "Training loss for batch 478 : 0.01176663301885128\n",
      "Training loss for batch 479 : 0.17761316895484924\n",
      "Training loss for batch 480 : 0.3660871684551239\n",
      "Training loss for batch 481 : 0.753346860408783\n",
      "Training loss for batch 482 : 0.08444616198539734\n",
      "Training loss for batch 483 : 0.048805736005306244\n",
      "Training loss for batch 484 : 0.3360591530799866\n",
      "Training loss for batch 485 : 0.10471616685390472\n",
      "Training loss for batch 486 : 0.04963419958949089\n",
      "Training loss for batch 487 : 0.35525238513946533\n",
      "Training loss for batch 488 : 0.15999643504619598\n",
      "Training loss for batch 489 : 0.13048841059207916\n",
      "Training loss for batch 490 : 0.18687550723552704\n",
      "Training loss for batch 491 : 0.40133291482925415\n",
      "Training loss for batch 492 : 0.0017341773491352797\n",
      "Training loss for batch 493 : 0.07841591536998749\n",
      "Training loss for batch 494 : 0.1575922667980194\n",
      "Training loss for batch 495 : 0.29822102189064026\n",
      "Training loss for batch 496 : 0.23077872395515442\n",
      "Training loss for batch 497 : 0.2814124822616577\n",
      "Training loss for batch 498 : 0.008675351738929749\n",
      "Training loss for batch 499 : 0.14210174977779388\n",
      "Training loss for batch 500 : 0.15306703746318817\n",
      "Training loss for batch 501 : 0.043132200837135315\n",
      "Training loss for batch 502 : 0.16563615202903748\n",
      "Training loss for batch 503 : 0.1121552512049675\n",
      "Training loss for batch 504 : 0.3106009066104889\n",
      "Training loss for batch 505 : 0.0\n",
      "Training loss for batch 506 : 0.16594348847866058\n",
      "Training loss for batch 507 : 0.02555103786289692\n",
      "Training loss for batch 508 : 0.08131983876228333\n",
      "Training loss for batch 509 : 0.2421361804008484\n",
      "Training loss for batch 510 : 0.13860228657722473\n",
      "Training loss for batch 511 : 0.23835521936416626\n",
      "Training loss for batch 512 : 0.08957921713590622\n",
      "Training loss for batch 513 : 0.0337466225028038\n",
      "Training loss for batch 514 : 0.0790579617023468\n",
      "Training loss for batch 515 : 0.0\n",
      "Training loss for batch 516 : 0.004132675938308239\n",
      "Training loss for batch 517 : 0.03239438310265541\n",
      "Training loss for batch 518 : 0.4037085175514221\n",
      "Training loss for batch 519 : 0.014228260144591331\n",
      "Training loss for batch 520 : 0.033967919647693634\n",
      "Training loss for batch 521 : 0.2830086350440979\n",
      "Training loss for batch 522 : 0.09532666206359863\n",
      "Training loss for batch 523 : 0.13784360885620117\n",
      "Training loss for batch 524 : 0.35210466384887695\n",
      "Training loss for batch 525 : 0.4905443489551544\n",
      "Training loss for batch 526 : 0.11857949942350388\n",
      "Training loss for batch 527 : 0.14799869060516357\n",
      "Training loss for batch 528 : 0.08911881595849991\n",
      "Training loss for batch 529 : 0.017673522233963013\n",
      "Training loss for batch 530 : 0.23651322722434998\n",
      "Training loss for batch 531 : 0.024976886808872223\n",
      "Training loss for batch 532 : 0.03208443522453308\n",
      "Training loss for batch 533 : 0.19680887460708618\n",
      "Training loss for batch 534 : 0.23960155248641968\n",
      "Training loss for batch 535 : 0.0\n",
      "Training loss for batch 536 : 0.0630752295255661\n",
      "Training loss for batch 537 : 0.15359030663967133\n",
      "Training loss for batch 538 : 0.314537912607193\n",
      "Training loss for batch 539 : 0.37777143716812134\n",
      "Training loss for batch 540 : 0.28822195529937744\n",
      "Training loss for batch 541 : 0.005193482153117657\n",
      "Training loss for batch 542 : 0.03513515740633011\n",
      "Training loss for batch 543 : 0.06665351241827011\n",
      "Training loss for batch 544 : 0.002447714563459158\n",
      "Training loss for batch 545 : 0.0\n",
      "Training loss for batch 546 : 0.1637367308139801\n",
      "Training loss for batch 547 : 0.013527422212064266\n",
      "Training loss for batch 548 : 0.3941305875778198\n",
      "Training loss for batch 549 : 0.050669435411691666\n",
      "Training loss for batch 550 : 0.19419845938682556\n",
      "Training loss for batch 551 : 0.12275871634483337\n",
      "Training loss for batch 552 : 0.26625296473503113\n",
      "Training loss for batch 553 : 0.062194161117076874\n",
      "Training loss for batch 554 : 0.2481262981891632\n",
      "Training loss for batch 555 : 0.2857644259929657\n",
      "Training loss for batch 556 : 0.12638597190380096\n",
      "Training loss for batch 557 : 0.03474157303571701\n",
      "Training loss for batch 558 : 0.06257520616054535\n",
      "Training loss for batch 559 : 0.28258195519447327\n",
      "Training loss for batch 560 : 0.3875950276851654\n",
      "Training loss for batch 561 : 0.1593528389930725\n",
      "Training loss for batch 562 : 0.20706814527511597\n",
      "Training loss for batch 563 : 0.28352683782577515\n",
      "Training loss for batch 564 : 0.2945670485496521\n",
      "Training loss for batch 565 : 0.10953869670629501\n",
      "Training loss for batch 566 : 0.31564703583717346\n",
      "Training loss for batch 567 : 0.049734633415937424\n",
      "Training loss for batch 568 : 0.08267965167760849\n",
      "Training loss for batch 569 : 0.4317419230937958\n",
      "Training loss for batch 570 : 0.3743962347507477\n",
      "Training loss for batch 571 : 0.27640432119369507\n",
      "Training loss for batch 572 : 0.006846201606094837\n",
      "Training loss for batch 573 : 0.330400675535202\n",
      "Training loss for batch 574 : 0.33209359645843506\n",
      "Training loss for batch 575 : 0.07988692820072174\n",
      "Training loss for batch 576 : 0.051799025386571884\n",
      "Training loss for batch 577 : -0.00019273094949312508\n",
      "Training loss for batch 578 : 0.056804362684488297\n",
      "Training loss for batch 579 : 0.4160098135471344\n",
      "Training loss for batch 580 : 0.4880193769931793\n",
      "Training loss for batch 581 : 0.022413084283471107\n",
      "Training loss for batch 582 : 0.1497052162885666\n",
      "Training loss for batch 583 : 0.12572385370731354\n",
      "Training loss for batch 584 : 0.7787243127822876\n",
      "Training loss for batch 585 : 0.0896768793463707\n",
      "Training loss for batch 586 : 0.0541740357875824\n",
      "Training loss for batch 587 : 0.036330293864011765\n",
      "Training loss for batch 588 : 0.04497534781694412\n",
      "Training loss for batch 589 : 0.023751717060804367\n",
      "Training loss for batch 590 : 0.03641143813729286\n",
      "Training loss for batch 591 : 0.23428304493427277\n",
      "Training loss for batch 592 : 0.3584435284137726\n",
      "Training loss for batch 593 : 0.2502018213272095\n",
      "Training loss for batch 594 : 0.09434893727302551\n",
      "Training loss for batch 595 : 0.05023352801799774\n",
      "Training loss for batch 596 : 0.34912922978401184\n",
      "Training loss for batch 597 : 0.1735202521085739\n",
      "Training loss for batch 598 : 0.16858786344528198\n",
      "Training loss for batch 599 : 0.03459592163562775\n",
      "Training loss for batch 600 : 0.1941242814064026\n",
      "Training loss for batch 601 : 0.22700172662734985\n",
      "Training loss for batch 602 : 0.1963387280702591\n",
      "Training loss for batch 603 : 0.16692665219306946\n",
      "Training loss for batch 604 : 0.2079559862613678\n",
      "Training loss for batch 605 : 0.10302409529685974\n",
      "Training loss for batch 606 : 0.15615223348140717\n",
      "Training loss for batch 607 : 0.2686226963996887\n",
      "Training loss for batch 608 : 0.3654336631298065\n",
      "Training loss for batch 609 : 0.3108151853084564\n",
      "Training loss for batch 610 : 0.24173469841480255\n",
      "Training loss for batch 611 : 0.19654835760593414\n",
      "Training loss for batch 612 : 0.016124697402119637\n",
      "Training loss for batch 613 : 0.26269084215164185\n",
      "Training loss for batch 614 : 0.015984565019607544\n",
      "Training loss for batch 615 : 0.04009201377630234\n",
      "Training loss for batch 616 : 0.3023471534252167\n",
      "Training loss for batch 617 : 0.08309511840343475\n",
      "Training loss for batch 618 : 0.2978495955467224\n",
      "Training loss for batch 619 : 0.07471607625484467\n",
      "Training loss for batch 620 : 0.051776766777038574\n",
      "Training loss for batch 621 : 0.3045291602611542\n",
      "Training loss for batch 622 : 0.08823420852422714\n",
      "Training loss for batch 623 : 0.0008182128658518195\n",
      "Training loss for batch 624 : 0.04749644547700882\n",
      "Training loss for batch 625 : 0.14716200530529022\n",
      "Training loss for batch 626 : 0.00792327057570219\n",
      "Training loss for batch 627 : 0.12885843217372894\n",
      "Training loss for batch 628 : 0.1276295930147171\n",
      "Training loss for batch 629 : 0.4469054043292999\n",
      "Training loss for batch 630 : 0.13083642721176147\n",
      "Training loss for batch 631 : 0.10091164708137512\n",
      "Training loss for batch 632 : 0.061921775341033936\n",
      "Training loss for batch 633 : 0.26600420475006104\n",
      "Training loss for batch 634 : 0.01536028366535902\n",
      "Training loss for batch 635 : 0.08660586178302765\n",
      "Training loss for batch 636 : 0.1448839157819748\n",
      "Training loss for batch 637 : 0.08817565441131592\n",
      "Training loss for batch 638 : 0.025512324646115303\n",
      "Training loss for batch 639 : -0.00010301141446689144\n",
      "Training loss for batch 640 : 0.21145956218242645\n",
      "Training loss for batch 641 : 0.05371752008795738\n",
      "Training loss for batch 642 : 0.003533557988703251\n",
      "Training loss for batch 643 : 0.3097016215324402\n",
      "Training loss for batch 644 : 0.01865706965327263\n",
      "Training loss for batch 645 : 0.433053582906723\n",
      "Training loss for batch 646 : 0.050995856523513794\n",
      "Training loss for batch 647 : 0.0930597335100174\n",
      "Training loss for batch 648 : 0.3417056202888489\n",
      "Training loss for batch 649 : 0.2316967397928238\n",
      "Training loss for batch 650 : 0.34437263011932373\n",
      "Training loss for batch 651 : 0.09368138015270233\n",
      "Training loss for batch 652 : 0.23495088517665863\n",
      "Training loss for batch 653 : 0.4523296058177948\n",
      "Training loss for batch 654 : 0.029854176566004753\n",
      "Training loss for batch 655 : 0.1033073216676712\n",
      "Training loss for batch 656 : 0.13409273326396942\n",
      "Training loss for batch 657 : 0.27029624581336975\n",
      "Training loss for batch 658 : 0.1271466165781021\n",
      "Training loss for batch 659 : 0.28309738636016846\n",
      "Training loss for batch 660 : 0.11767783761024475\n",
      "Training loss for batch 661 : 0.023853719234466553\n",
      "Training loss for batch 662 : 0.08023712038993835\n",
      "Training loss for batch 663 : 0.03114442154765129\n",
      "Training loss for batch 664 : 0.06402799487113953\n",
      "Training loss for batch 665 : 0.14042223989963531\n",
      "Training loss for batch 666 : 0.11508627980947495\n",
      "Training loss for batch 667 : 0.32361268997192383\n",
      "Training loss for batch 668 : 0.42328017950057983\n",
      "Training loss for batch 669 : 0.32943451404571533\n",
      "Training loss for batch 670 : 0.188250333070755\n",
      "Training loss for batch 671 : 0.26680076122283936\n",
      "Training loss for batch 672 : 0.07927247881889343\n",
      "Training loss for batch 673 : 0.2701219618320465\n",
      "Training loss for batch 674 : 0.19448238611221313\n",
      "Training loss for batch 675 : 0.09039726853370667\n",
      "Training loss for batch 676 : 0.14546488225460052\n",
      "Training loss for batch 677 : 0.06439919024705887\n",
      "Training loss for batch 678 : 0.0449494831264019\n",
      "Training loss for batch 679 : 0.06718450784683228\n",
      "Training loss for batch 680 : 0.1425308734178543\n",
      "Training loss for batch 681 : 0.09702929109334946\n",
      "Training loss for batch 682 : 0.30396711826324463\n",
      "Training loss for batch 683 : 0.06699895858764648\n",
      "Training loss for batch 684 : 0.23465801775455475\n",
      "Training loss for batch 685 : 0.2582691013813019\n",
      "Training loss for batch 686 : 0.23408867418766022\n",
      "Training loss for batch 687 : 0.17836955189704895\n",
      "Training loss for batch 688 : 0.08823656290769577\n",
      "Training loss for batch 689 : 0.17711304128170013\n",
      "Training loss for batch 690 : 0.15375389158725739\n",
      "Training loss for batch 691 : 0.14192800223827362\n",
      "Training loss for batch 692 : 0.028396738693118095\n",
      "Training loss for batch 693 : 0.09031348675489426\n",
      "Training loss for batch 694 : 0.12654195725917816\n",
      "Training loss for batch 695 : 0.33298468589782715\n",
      "Training loss for batch 696 : 0.030732566490769386\n",
      "Training loss for batch 697 : 0.11471916735172272\n",
      "Training loss for batch 698 : 0.2262849509716034\n",
      "Training loss for batch 699 : 0.1967049241065979\n",
      "Training loss for batch 700 : 0.09040012955665588\n",
      "Training loss for batch 701 : 0.1805303543806076\n",
      "Training loss for batch 702 : 0.05983152240514755\n",
      "Training loss for batch 703 : 0.11696019023656845\n",
      "Training loss for batch 704 : 0.008821666240692139\n",
      "Training loss for batch 705 : 0.0\n",
      "Training loss for batch 706 : 0.34003400802612305\n",
      "Training loss for batch 707 : 0.17011630535125732\n",
      "Training loss for batch 708 : -6.558059976669028e-05\n",
      "Training loss for batch 709 : 0.13824795186519623\n",
      "Training loss for batch 710 : 0.04642196744680405\n",
      "Training loss for batch 711 : 0.37934717535972595\n",
      "Training loss for batch 712 : 0.10800734907388687\n",
      "Training loss for batch 713 : 0.20112480223178864\n",
      "Training loss for batch 714 : 0.1847543865442276\n",
      "Training loss for batch 715 : 0.5534459352493286\n",
      "Training loss for batch 716 : 0.2287127673625946\n",
      "Training loss for batch 717 : 0.13170993328094482\n",
      "Training loss for batch 718 : 0.013604696840047836\n",
      "Training loss for batch 719 : 0.10903266072273254\n",
      "Training loss for batch 720 : 0.39176657795906067\n",
      "Training loss for batch 721 : 0.04048217833042145\n",
      "Training loss for batch 722 : 0.24614813923835754\n",
      "Training loss for batch 723 : 0.14114589989185333\n",
      "Training loss for batch 724 : 0.2860843241214752\n",
      "Training loss for batch 725 : 0.16880033910274506\n",
      "Training loss for batch 726 : 0.12573865056037903\n",
      "Training loss for batch 727 : 0.1304977834224701\n",
      "Training loss for batch 728 : 0.015306453220546246\n",
      "Training loss for batch 729 : 0.30988791584968567\n",
      "Training loss for batch 730 : 0.13017868995666504\n",
      "Training loss for batch 731 : 0.17729105055332184\n",
      "Training loss for batch 732 : 0.26700472831726074\n",
      "Training loss for batch 733 : 0.01564226858317852\n",
      "Training loss for batch 734 : 0.11321922391653061\n",
      "Training loss for batch 735 : 0.01479207631200552\n",
      "Training loss for batch 736 : 0.16576705873012543\n",
      "Training loss for batch 737 : 0.25179868936538696\n",
      "Training loss for batch 738 : 0.07285449653863907\n",
      "Training loss for batch 739 : 0.21992991864681244\n",
      "Training loss for batch 740 : 0.38572990894317627\n",
      "Training loss for batch 741 : 0.09331271052360535\n",
      "Training loss for batch 742 : 0.06242956221103668\n",
      "Training loss for batch 743 : 0.08879880607128143\n",
      "Training loss for batch 744 : 0.1977883130311966\n",
      "Training loss for batch 745 : 0.026331018656492233\n",
      "Training loss for batch 746 : 0.21481068432331085\n",
      "Training loss for batch 747 : 0.14795225858688354\n",
      "Training loss for batch 748 : 0.06274061650037766\n",
      "Training loss for batch 749 : 0.027674060314893723\n",
      "Training loss for batch 750 : 0.1947977989912033\n",
      "Training loss for batch 751 : 0.25710874795913696\n",
      "Training loss for batch 752 : 0.15227235853672028\n",
      "Training loss for batch 753 : 0.06587734073400497\n",
      "Training loss for batch 754 : 0.052142832428216934\n",
      "Training loss for batch 755 : 0.26193106174468994\n",
      "Training loss for batch 756 : 0.28611740469932556\n",
      "Training loss for batch 757 : 0.026228351518511772\n",
      "Training loss for batch 758 : 0.07512114197015762\n",
      "Training loss for batch 759 : 0.04862977936863899\n",
      "Training loss for batch 760 : 0.119333416223526\n",
      "Training loss for batch 761 : 0.1016172394156456\n",
      "Training loss for batch 762 : 0.1982136368751526\n",
      "Training loss for batch 763 : 0.3713473379611969\n",
      "Training loss for batch 764 : 0.20161129534244537\n",
      "Training loss for batch 765 : 0.105083167552948\n",
      "Training loss for batch 766 : 0.10588225722312927\n",
      "Training loss for batch 767 : 0.10405701398849487\n",
      "Training loss for batch 768 : 0.005898273549973965\n",
      "Training loss for batch 769 : 0.28076010942459106\n",
      "Training loss for batch 770 : 0.07774948328733444\n",
      "Training loss for batch 771 : 0.17111709713935852\n",
      "Training loss for batch 772 : 0.1127234697341919\n",
      "Training loss for batch 773 : 0.2608170211315155\n",
      "Training loss for batch 774 : 0.010602399706840515\n",
      "Training loss for batch 775 : 0.013968266546726227\n",
      "Training loss for batch 776 : 0.3924977481365204\n",
      "Training loss for batch 777 : 0.10659324377775192\n",
      "Training loss for batch 778 : 0.20140954852104187\n",
      "Training loss for batch 779 : 0.06207527220249176\n",
      "Training loss for batch 780 : 0.13225291669368744\n",
      "Training loss for batch 781 : 0.05744234472513199\n",
      "Training loss for batch 782 : 0.10093280673027039\n",
      "Training loss for batch 783 : 0.06219939514994621\n",
      "Training loss for batch 784 : 0.10522422194480896\n",
      "Training loss for batch 785 : 0.10718992352485657\n",
      "Training loss for batch 786 : 0.15195563435554504\n",
      "Training loss for batch 787 : 0.18856418132781982\n",
      "Training loss for batch 788 : 0.375287264585495\n",
      "Training loss for batch 789 : 0.2121879905462265\n",
      "Training loss for batch 790 : 0.2460143268108368\n",
      "Training loss for batch 791 : 0.009440523572266102\n",
      "Training loss for batch 792 : 0.11550664156675339\n",
      "Training loss for batch 793 : 0.2605053186416626\n",
      "Training loss for batch 794 : 0.23951417207717896\n",
      "Training loss for batch 795 : 0.40796199440956116\n",
      "Training loss for batch 796 : 0.14764919877052307\n",
      "Training loss for batch 797 : 0.036337073892354965\n",
      "Training loss for batch 798 : 0.05251210555434227\n",
      "Training loss for batch 799 : 0.15680059790611267\n",
      "Training loss for batch 800 : 0.1350594311952591\n",
      "Training loss for batch 801 : 0.11576981842517853\n",
      "Training loss for batch 802 : 0.05380970239639282\n",
      "Training loss for batch 803 : 0.08374591171741486\n",
      "Training loss for batch 804 : 0.003735913895070553\n",
      "Training loss for batch 805 : 0.17447781562805176\n",
      "Training loss for batch 806 : 0.048448458313941956\n",
      "Training loss for batch 807 : 0.12657976150512695\n",
      "Training loss for batch 808 : 0.24507638812065125\n",
      "Training loss for batch 809 : 0.16460020840168\n",
      "Training loss for batch 810 : 0.20740392804145813\n",
      "Training loss for batch 811 : 0.3485272228717804\n",
      "Training loss for batch 812 : 0.12553060054779053\n",
      "Training loss for batch 813 : 0.1740485280752182\n",
      "Training loss for batch 814 : 0.2792220115661621\n",
      "Training loss for batch 815 : 0.2501206398010254\n",
      "Training loss for batch 816 : 0.01550224982202053\n",
      "Training loss for batch 817 : 0.07987990975379944\n",
      "Training loss for batch 818 : 0.07075852900743484\n",
      "Training loss for batch 819 : 0.023070335388183594\n",
      "Training loss for batch 820 : 0.01724052056670189\n",
      "Training loss for batch 821 : 0.5091761350631714\n",
      "Training loss for batch 822 : 0.12577763199806213\n",
      "Training loss for batch 823 : 0.13050101697444916\n",
      "Training loss for batch 824 : 0.0735355094075203\n",
      "Training loss for batch 825 : 0.015061226673424244\n",
      "Training loss for batch 826 : 0.10834483802318573\n",
      "Training loss for batch 827 : 0.11179531365633011\n",
      "Training loss for batch 828 : 0.23325495421886444\n",
      "Training loss for batch 829 : 0.08299560844898224\n",
      "Training loss for batch 830 : 0.04400073364377022\n",
      "Training loss for batch 831 : 0.19280876219272614\n",
      "Training loss for batch 832 : 0.07267796993255615\n",
      "Training loss for batch 833 : 0.039174385368824005\n",
      "Training loss for batch 834 : 0.24315683543682098\n",
      "Training loss for batch 835 : 0.0422058068215847\n",
      "Training loss for batch 836 : 0.11372055858373642\n",
      "Training loss for batch 837 : 0.04800749197602272\n",
      "Training loss for batch 838 : 0.2566869556903839\n",
      "Training loss for batch 839 : 0.07898502051830292\n",
      "Training loss for batch 840 : 0.03796953707933426\n",
      "Training loss for batch 841 : 0.26778218150138855\n",
      "Training loss for batch 842 : 0.374328076839447\n",
      "Training loss for batch 843 : 0.028000900521874428\n",
      "Training loss for batch 844 : 0.03062441013753414\n",
      "Training loss for batch 845 : 0.09290854632854462\n",
      "Training loss for batch 846 : 0.126535564661026\n",
      "Training loss for batch 847 : 0.048795901238918304\n",
      "Training loss for batch 848 : 0.1071038693189621\n",
      "Training loss for batch 849 : 0.4014073610305786\n",
      "Training loss for batch 850 : 0.027031630277633667\n",
      "Training loss for batch 851 : 0.16240020096302032\n",
      "Training loss for batch 852 : 0.14790162444114685\n",
      "Training loss for batch 853 : 0.11353538185358047\n",
      "Training loss for batch 854 : 0.23728734254837036\n",
      "Training loss for batch 855 : 0.12613052129745483\n",
      "Training loss for batch 856 : 0.1056186705827713\n",
      "Training loss for batch 857 : 0.00463465228676796\n",
      "Training loss for batch 858 : 0.2155553251504898\n",
      "Training loss for batch 859 : 0.36028873920440674\n",
      "Training loss for batch 860 : 0.2772008180618286\n",
      "Training loss for batch 861 : 0.03974160924553871\n",
      "Training loss for batch 862 : 0.014832347631454468\n",
      "Training loss for batch 863 : 0.0845857709646225\n",
      "Training loss for batch 864 : 0.0036620416212826967\n",
      "Training loss for batch 865 : 0.021373357623815536\n",
      "Training loss for batch 866 : 0.2137845754623413\n",
      "Training loss for batch 867 : 0.12465471774339676\n",
      "Training loss for batch 868 : 0.15135261416435242\n",
      "Training loss for batch 869 : 0.12695550918579102\n",
      "Training loss for batch 870 : 0.3816673457622528\n",
      "Training loss for batch 871 : 0.03086821362376213\n",
      "Training loss for batch 872 : 0.030078906565904617\n",
      "Training loss for batch 873 : 0.11460975557565689\n",
      "Training loss for batch 874 : 0.026331475004553795\n",
      "Training loss for batch 875 : 0.08398667722940445\n",
      "Training loss for batch 876 : 0.21163524687290192\n",
      "Training loss for batch 877 : 0.0716724619269371\n",
      "Training loss for batch 878 : 0.004689457826316357\n",
      "Training loss for batch 879 : 0.20761840045452118\n",
      "Training loss for batch 880 : 0.1610117107629776\n",
      "Training loss for batch 881 : 0.10880161821842194\n",
      "Training loss for batch 882 : 0.0877545177936554\n",
      "Training loss for batch 883 : 0.06280054152011871\n",
      "Training loss for batch 884 : 0.04093309864401817\n",
      "Training loss for batch 885 : 0.2159431278705597\n",
      "Training loss for batch 886 : 0.27097493410110474\n",
      "Training loss for batch 887 : 0.039425577968358994\n",
      "Training loss for batch 888 : 0.3576013445854187\n",
      "Training loss for batch 889 : 0.1530769169330597\n",
      "Training loss for batch 890 : 0.13442273437976837\n",
      "Training loss for batch 891 : 0.3064552843570709\n",
      "Training loss for batch 892 : 0.11991482973098755\n",
      "Training loss for batch 893 : 0.10712234675884247\n",
      "Training loss for batch 894 : 0.4780782163143158\n",
      "Training loss for batch 895 : 0.10808050632476807\n",
      "Training loss for batch 896 : 0.31306126713752747\n",
      "Training loss for batch 897 : 0.023512113839387894\n",
      "Training loss for batch 898 : 0.0\n",
      "Training loss for batch 899 : 0.35739651322364807\n",
      "Training loss for batch 900 : 0.0980907678604126\n",
      "Training loss for batch 901 : 0.17330214381217957\n",
      "Training loss for batch 902 : 0.15791431069374084\n",
      "Training loss for batch 903 : 0.06398608535528183\n",
      "Training loss for batch 904 : 0.1255766749382019\n",
      "Training loss for batch 905 : 0.0036864280700683594\n",
      "Training loss for batch 906 : 0.10660654306411743\n",
      "Training loss for batch 907 : 0.27520886063575745\n",
      "Training loss for batch 908 : 0.06122744455933571\n",
      "Training loss for batch 909 : 0.10380731523036957\n",
      "Training loss for batch 910 : 0.031901370733976364\n",
      "Training loss for batch 911 : 0.010045466013252735\n",
      "Training loss for batch 912 : 0.07755769044160843\n",
      "Training loss for batch 913 : 0.32723626494407654\n",
      "Training loss for batch 914 : 0.0886034220457077\n",
      "Training loss for batch 915 : 0.10101357847452164\n",
      "Training loss for batch 916 : 0.2977014482021332\n",
      "Training loss for batch 917 : 0.11036798357963562\n",
      "Training loss for batch 918 : 0.058004796504974365\n",
      "Training loss for batch 919 : 0.3119853734970093\n",
      "Training loss for batch 920 : 0.04258642718195915\n",
      "Training loss for batch 921 : 0.19326505064964294\n",
      "Training loss for batch 922 : 0.24918068945407867\n",
      "Training loss for batch 923 : 0.08171506971120834\n",
      "Training loss for batch 924 : 0.33173900842666626\n",
      "Training loss for batch 925 : 0.151536226272583\n",
      "Training loss for batch 926 : 0.21301284432411194\n",
      "Training loss for batch 927 : 0.13005590438842773\n",
      "Training loss for batch 928 : 0.12160719186067581\n",
      "Training loss for batch 929 : 0.24952846765518188\n",
      "Training loss for batch 930 : 0.13050605356693268\n",
      "Training loss for batch 931 : 0.07050535082817078\n",
      "Training loss for batch 932 : 0.11603578925132751\n",
      "Training loss for batch 933 : 0.19953377544879913\n",
      "Training loss for batch 934 : 0.18158555030822754\n",
      "Training loss for batch 935 : 0.04133718088269234\n",
      "Training loss for batch 936 : 0.09449551999568939\n",
      "Training loss for batch 937 : 0.16457189619541168\n",
      "Training loss for batch 938 : 0.18782000243663788\n",
      "Training loss for batch 939 : 0.03093128465116024\n",
      "Training loss for batch 940 : 0.036043956875801086\n",
      "Training loss for batch 941 : 0.0514754094183445\n",
      "Training loss for batch 942 : 0.02180745266377926\n",
      "Training loss for batch 943 : 0.08067712187767029\n",
      "Training loss for batch 944 : 0.013906697742640972\n",
      "Training loss for batch 945 : 0.31651902198791504\n",
      "Training loss for batch 946 : 0.13410232961177826\n",
      "Training loss for batch 947 : 0.20278304815292358\n",
      "Training loss for batch 948 : 0.06265784800052643\n",
      "Training loss for batch 949 : 0.30880260467529297\n",
      "Training loss for batch 950 : 0.09857066720724106\n",
      "Training loss for batch 951 : 0.07982543110847473\n",
      "Training loss for batch 952 : 0.06992166489362717\n",
      "Training loss for batch 953 : 0.013423671945929527\n",
      "Training loss for batch 954 : 0.3153858780860901\n",
      "Training loss for batch 955 : 0.02837950363755226\n",
      "Training loss for batch 956 : 0.2960120141506195\n",
      "Training loss for batch 957 : 0.19907529652118683\n",
      "Training loss for batch 958 : 0.05132051557302475\n",
      "Training loss for batch 959 : 0.027374515309929848\n",
      "Training loss for batch 960 : 0.18291421234607697\n",
      "Training loss for batch 961 : 0.036481257528066635\n",
      "Training loss for batch 962 : 0.1487218588590622\n",
      "Training loss for batch 963 : 0.026099195703864098\n",
      "Training loss for batch 964 : 0.3287068009376526\n",
      "Training loss for batch 965 : 0.0274753887206316\n",
      "Training loss for batch 966 : 0.022462494671344757\n",
      "Training loss for batch 967 : 0.07005591690540314\n",
      "Training loss for batch 968 : 0.056602515280246735\n",
      "Training loss for batch 969 : 0.18759210407733917\n",
      "Training loss for batch 970 : 0.3362457752227783\n",
      "Training loss for batch 971 : 0.14605146646499634\n",
      "Training loss for batch 972 : 0.0\n",
      "Training loss for batch 973 : 0.11815817654132843\n",
      "Training loss for batch 974 : 0.1964561492204666\n",
      "Training loss for batch 975 : 0.08256340771913528\n",
      "Training loss for batch 976 : 0.01622035913169384\n",
      "Training loss for batch 977 : 0.20614022016525269\n",
      "Training loss for batch 978 : 0.07791884243488312\n",
      "Training loss for batch 979 : 0.1243172287940979\n",
      "Training loss for batch 980 : 0.023855656385421753\n",
      "Training loss for batch 981 : 0.05631390959024429\n",
      "Training loss for batch 982 : 0.07447484880685806\n",
      "Training loss for batch 983 : 0.013267617672681808\n",
      "Training loss for batch 984 : 0.011511067859828472\n",
      "Training loss for batch 985 : 0.12124810367822647\n",
      "Training loss for batch 986 : 0.31145137548446655\n",
      "Training loss for batch 987 : 0.0380912609398365\n",
      "Training loss for batch 988 : 0.2727867662906647\n",
      "Training loss for batch 989 : 0.0\n",
      "Training loss for batch 990 : 0.33113178610801697\n",
      "Training loss for batch 991 : 0.03322000056505203\n",
      "Training loss for batch 992 : 0.158156618475914\n",
      "Training loss for batch 993 : 0.0\n",
      "Training loss for batch 994 : 0.10042235255241394\n",
      "Training loss for batch 995 : 0.2628133296966553\n",
      "Training loss for batch 996 : 0.09863977134227753\n",
      "Training loss for batch 997 : 0.276271253824234\n",
      "Training loss for batch 998 : 0.33040547370910645\n",
      "Training loss for batch 999 : 0.235308438539505\n",
      "Training loss for batch 1000 : 0.08871864527463913\n",
      "Training loss for batch 1001 : 0.0737798735499382\n",
      "Training loss for batch 1002 : 0.2562240958213806\n",
      "Training loss for batch 1003 : 0.21102464199066162\n",
      "Training loss for batch 1004 : 0.10377956181764603\n",
      "Training loss for batch 1005 : 0.12638823688030243\n",
      "Training loss for batch 1006 : 0.13733842968940735\n",
      "Training loss for batch 1007 : 0.20479534566402435\n",
      "Training loss for batch 1008 : 0.05111420154571533\n",
      "Training loss for batch 1009 : 0.1305147260427475\n",
      "Training loss for batch 1010 : 0.16468612849712372\n",
      "Training loss for batch 1011 : 0.049745284020900726\n",
      "Training loss for batch 1012 : 0.07597176730632782\n",
      "Training loss for batch 1013 : 0.37367427349090576\n",
      "Training loss for batch 1014 : 0.062255002558231354\n",
      "Training loss for batch 1015 : 0.08240386098623276\n",
      "Training loss for batch 1016 : 0.26833251118659973\n",
      "Training loss for batch 1017 : 0.3557693064212799\n",
      "Training loss for batch 1018 : 0.10438255220651627\n",
      "Training loss for batch 1019 : 0.05034139007329941\n",
      "Training loss for batch 1020 : 0.03202958405017853\n",
      "Training loss for batch 1021 : 0.16188673675060272\n",
      "Training loss for batch 1022 : 0.0\n",
      "Training loss for batch 1023 : 0.060558825731277466\n",
      "Training loss for batch 1024 : 0.33917516469955444\n",
      "Training loss for batch 1025 : 0.08613446354866028\n",
      "Training loss for batch 1026 : 0.22444117069244385\n",
      "Training loss for batch 1027 : 0.4122197926044464\n",
      "Training loss for batch 1028 : 0.0\n",
      "Training loss for batch 1029 : -0.0006524483324028552\n",
      "Training loss for batch 1030 : 0.2919299602508545\n",
      "Training loss for batch 1031 : 0.35281774401664734\n",
      "Training loss for batch 1032 : 0.3393045663833618\n",
      "Training loss for batch 1033 : 0.11613955348730087\n",
      "Training loss for batch 1034 : 0.2526148557662964\n",
      "Training loss for batch 1035 : 0.343877375125885\n",
      "Training loss for batch 1036 : 0.06690191477537155\n",
      "Training loss for batch 1037 : 0.2175198793411255\n",
      "Training loss for batch 1038 : 0.03079964779317379\n",
      "Training loss for batch 1039 : 0.14435963332653046\n",
      "Training loss for batch 1040 : 0.184658020734787\n",
      "Training loss for batch 1041 : 0.0771283209323883\n",
      "Training loss for batch 1042 : 0.24252726137638092\n",
      "Training loss for batch 1043 : 0.03406884893774986\n",
      "Training loss for batch 1044 : 0.04972454532980919\n",
      "Training loss for batch 1045 : 0.4424116909503937\n",
      "Training loss for batch 1046 : 0.446879118680954\n",
      "Training loss for batch 1047 : 0.03097967430949211\n",
      "Training loss for batch 1048 : 0.016332870349287987\n",
      "Training loss for batch 1049 : 0.22747273743152618\n",
      "Training loss for batch 1050 : 0.13702666759490967\n",
      "Training loss for batch 1051 : 0.10818707197904587\n",
      "Training loss for batch 1052 : 0.08549308776855469\n",
      "Training loss for batch 1053 : 0.2640850245952606\n",
      "Training loss for batch 1054 : 0.38183215260505676\n",
      "Training loss for batch 1055 : 0.253004789352417\n",
      "Training loss for batch 1056 : 0.22268077731132507\n",
      "Training loss for batch 1057 : 0.014517784118652344\n",
      "Training loss for batch 1058 : 0.21890397369861603\n",
      "Training loss for batch 1059 : 0.3011711835861206\n",
      "Training loss for batch 1060 : 0.030691275373101234\n",
      "Training loss for batch 1061 : 0.04750743508338928\n",
      "Training loss for batch 1062 : 0.1726357638835907\n",
      "Training loss for batch 1063 : 0.02965518645942211\n",
      "Training loss for batch 1064 : 0.16249391436576843\n",
      "Training loss for batch 1065 : 0.0710797905921936\n",
      "Training loss for batch 1066 : 0.04247645288705826\n",
      "Training loss for batch 1067 : 0.08283212780952454\n",
      "Training loss for batch 1068 : 0.16658197343349457\n",
      "Training loss for batch 1069 : 0.11208134889602661\n",
      "Training loss for batch 1070 : 0.13832135498523712\n",
      "Training loss for batch 1071 : 0.23171760141849518\n",
      "Training loss for batch 1072 : 0.0\n",
      "Training loss for batch 1073 : 0.18098121881484985\n",
      "Training loss for batch 1074 : 0.02611924335360527\n",
      "Training loss for batch 1075 : 0.010665802285075188\n",
      "Training loss for batch 1076 : 0.009870330803096294\n",
      "Training loss for batch 1077 : 0.17161566019058228\n",
      "Training loss for batch 1078 : 0.041134800761938095\n",
      "Training loss for batch 1079 : 0.1766442060470581\n",
      "Training loss for batch 1080 : 0.023269962519407272\n",
      "Training loss for batch 1081 : 0.05127260088920593\n",
      "Training loss for batch 1082 : 0.19244061410427094\n",
      "Training loss for batch 1083 : 0.14005883038043976\n",
      "Training loss for batch 1084 : 0.0025828282814472914\n",
      "Training loss for batch 1085 : 0.29312241077423096\n",
      "Training loss for batch 1086 : 0.04967042803764343\n",
      "Training loss for batch 1087 : 0.03894604369997978\n",
      "Training loss for batch 1088 : 0.030637234449386597\n",
      "Training loss for batch 1089 : 0.15655551850795746\n",
      "Training loss for batch 1090 : 0.08350804448127747\n",
      "Training loss for batch 1091 : 0.07463958859443665\n",
      "Training loss for batch 1092 : 0.1695050299167633\n",
      "Training loss for batch 1093 : 0.40914252400398254\n",
      "Training loss for batch 1094 : 0.26081743836402893\n",
      "Training loss for batch 1095 : 0.04925304278731346\n",
      "Training loss for batch 1096 : 0.09163880348205566\n",
      "Training loss for batch 1097 : 0.11297546327114105\n",
      "Training loss for batch 1098 : 0.35364410281181335\n",
      "Training loss for batch 1099 : 0.15529532730579376\n",
      "Training loss for batch 1100 : 0.0\n",
      "Training loss for batch 1101 : 0.2108110785484314\n",
      "Training loss for batch 1102 : 0.1952580213546753\n",
      "Training loss for batch 1103 : 0.2655123174190521\n",
      "Training loss for batch 1104 : 0.40503087639808655\n",
      "Training loss for batch 1105 : 0.045519277453422546\n",
      "Training loss for batch 1106 : 0.23156283795833588\n",
      "Training loss for batch 1107 : 0.003019861876964569\n",
      "Training loss for batch 1108 : 0.05748141556978226\n",
      "Training loss for batch 1109 : 0.10909588634967804\n",
      "Training loss for batch 1110 : 0.024327507242560387\n",
      "Training loss for batch 1111 : 0.3216143846511841\n",
      "Training loss for batch 1112 : 0.7349616289138794\n",
      "Training loss for batch 1113 : 0.3814373016357422\n",
      "Training loss for batch 1114 : 0.024602064862847328\n",
      "Training loss for batch 1115 : 0.01031209621578455\n",
      "Training loss for batch 1116 : 0.11512555927038193\n",
      "Training loss for batch 1117 : 0.08572021126747131\n",
      "Training loss for batch 1118 : 0.23845437169075012\n",
      "Training loss for batch 1119 : 0.026722313836216927\n",
      "Training loss for batch 1120 : 0.06349793076515198\n",
      "Training loss for batch 1121 : 0.29708224534988403\n",
      "Training loss for batch 1122 : 0.12002265453338623\n",
      "Training loss for batch 1123 : 0.21952679753303528\n",
      "Training loss for batch 1124 : 0.26962751150131226\n",
      "Training loss for batch 1125 : 0.04017060622572899\n",
      "Training loss for batch 1126 : 0.1239599883556366\n",
      "Training loss for batch 1127 : 0.07584922015666962\n",
      "Training loss for batch 1128 : 0.045452993363142014\n",
      "Training loss for batch 1129 : 0.05337909981608391\n",
      "Training loss for batch 1130 : 0.0031273465137928724\n",
      "Training loss for batch 1131 : 0.2549496591091156\n",
      "Training loss for batch 1132 : 0.08115781843662262\n",
      "Training loss for batch 1133 : 0.41450726985931396\n",
      "Training loss for batch 1134 : 0.07433848083019257\n",
      "Training loss for batch 1135 : 0.283715158700943\n",
      "Training loss for batch 1136 : 0.3783269226551056\n",
      "Training loss for batch 1137 : 0.23258814215660095\n",
      "Training loss for batch 1138 : 0.2286166250705719\n",
      "Training loss for batch 1139 : 0.08343430608510971\n",
      "Training loss for batch 1140 : 0.23601244390010834\n",
      "Training loss for batch 1141 : 0.17929860949516296\n",
      "Training loss for batch 1142 : 0.008987775072455406\n",
      "Training loss for batch 1143 : 0.027007922530174255\n",
      "Training loss for batch 1144 : 0.08670666813850403\n",
      "Training loss for batch 1145 : 0.09877623617649078\n",
      "Training loss for batch 1146 : 0.09195971488952637\n",
      "Training loss for batch 1147 : 0.3276761472225189\n",
      "Training loss for batch 1148 : 0.10457707941532135\n",
      "Training loss for batch 1149 : 0.1319928616285324\n",
      "Training loss for batch 1150 : 0.254652202129364\n",
      "Training loss for batch 1151 : 0.19832517206668854\n",
      "Training loss for batch 1152 : 0.0\n",
      "Training loss for batch 1153 : 0.20566146075725555\n",
      "Training loss for batch 1154 : 0.2707056701183319\n",
      "Training loss for batch 1155 : 0.07912803441286087\n",
      "Training loss for batch 1156 : 0.1946418285369873\n",
      "Training loss for batch 1157 : 0.07619483023881912\n",
      "Training loss for batch 1158 : 0.04319828748703003\n",
      "Training loss for batch 1159 : 0.04301254451274872\n",
      "Training loss for batch 1160 : 0.20785653591156006\n",
      "Training loss for batch 1161 : 0.05262529477477074\n",
      "Training loss for batch 1162 : 0.06777120381593704\n",
      "Training loss for batch 1163 : 0.24076566100120544\n",
      "Training loss for batch 1164 : 0.14521172642707825\n",
      "Training loss for batch 1165 : 0.04366825520992279\n",
      "Training loss for batch 1166 : 0.11846594512462616\n",
      "Training loss for batch 1167 : 0.13707931339740753\n",
      "Training loss for batch 1168 : 0.3165528476238251\n",
      "Training loss for batch 1169 : 0.034085292369127274\n",
      "Training loss for batch 1170 : 0.22961343824863434\n",
      "Training loss for batch 1171 : 0.11747711151838303\n",
      "Training loss for batch 1172 : 0.1492290198802948\n",
      "Training loss for batch 1173 : 0.12160522490739822\n",
      "Training loss for batch 1174 : 0.07034078240394592\n",
      "Training loss for batch 1175 : 0.1944337785243988\n",
      "Training loss for batch 1176 : 0.0\n",
      "Training loss for batch 1177 : 0.17749540507793427\n",
      "Training loss for batch 1178 : 0.06055814400315285\n",
      "Training loss for batch 1179 : 0.35284245014190674\n",
      "Training loss for batch 1180 : 0.09013976156711578\n",
      "Training loss for batch 1181 : 0.2142198234796524\n",
      "Training loss for batch 1182 : 0.02448124811053276\n",
      "Training loss for batch 1183 : 0.06713417172431946\n",
      "Training loss for batch 1184 : 0.07036099582910538\n",
      "Training loss for batch 1185 : 0.01773856393992901\n",
      "Training loss for batch 1186 : 0.22604696452617645\n",
      "Training loss for batch 1187 : 0.0\n",
      "Training loss for batch 1188 : 0.21682947874069214\n",
      "Training loss for batch 1189 : 0.2176109403371811\n",
      "Training loss for batch 1190 : 0.5591174364089966\n",
      "Training loss for batch 1191 : 0.17404597997665405\n",
      "Training loss for batch 1192 : 0.02124987542629242\n",
      "Training loss for batch 1193 : 0.2389482855796814\n",
      "Training loss for batch 1194 : 0.26908183097839355\n",
      "Training loss for batch 1195 : 0.15949079394340515\n",
      "Training loss for batch 1196 : 0.0851738229393959\n",
      "Training loss for batch 1197 : 0.27108126878738403\n",
      "Training loss for batch 1198 : 0.4149225354194641\n",
      "Training loss for batch 1199 : 0.0\n",
      "Training loss for batch 1200 : 0.16870538890361786\n",
      "Training loss for batch 1201 : 0.011778159998357296\n",
      "Training loss for batch 1202 : 0.43994927406311035\n",
      "Training loss for batch 1203 : 0.012209306471049786\n",
      "Training loss for batch 1204 : 0.04127410799264908\n",
      "Training loss for batch 1205 : 0.08360709995031357\n",
      "Training loss for batch 1206 : 0.30553823709487915\n",
      "Training loss for batch 1207 : 0.23505647480487823\n",
      "Training loss for batch 1208 : 0.07200086116790771\n",
      "Training loss for batch 1209 : 0.08364254981279373\n",
      "Training loss for batch 1210 : 0.17244192957878113\n",
      "Training loss for batch 1211 : 0.017605002969503403\n",
      "Training loss for batch 1212 : 0.138381227850914\n",
      "Training loss for batch 1213 : 0.0848909541964531\n",
      "Training loss for batch 1214 : 0.218916118144989\n",
      "Training loss for batch 1215 : 0.23482467234134674\n",
      "Training loss for batch 1216 : 0.16371263563632965\n",
      "Training loss for batch 1217 : 0.09950631856918335\n",
      "Training loss for batch 1218 : 0.0764957070350647\n",
      "Training loss for batch 1219 : 0.0388181135058403\n",
      "Training loss for batch 1220 : 0.0617184080183506\n",
      "Training loss for batch 1221 : 0.07765716314315796\n",
      "Training loss for batch 1222 : 0.29700884222984314\n",
      "Training loss for batch 1223 : 0.08578911423683167\n",
      "Training loss for batch 1224 : 0.12592951953411102\n",
      "Training loss for batch 1225 : 0.0\n",
      "Training loss for batch 1226 : 0.030515145510435104\n",
      "Training loss for batch 1227 : 0.06991568207740784\n",
      "Training loss for batch 1228 : 0.366795152425766\n",
      "Training loss for batch 1229 : 0.18175172805786133\n",
      "Training loss for batch 1230 : 0.31823432445526123\n",
      "Training loss for batch 1231 : 0.34331417083740234\n",
      "Training loss for batch 1232 : -0.0005573260132223368\n",
      "Training loss for batch 1233 : 0.07317417860031128\n",
      "Training loss for batch 1234 : 0.06572193652391434\n",
      "Training loss for batch 1235 : 0.2948424220085144\n",
      "Training loss for batch 1236 : 0.1200440376996994\n",
      "Training loss for batch 1237 : 0.033845894038677216\n",
      "Training loss for batch 1238 : 0.10337480157613754\n",
      "Training loss for batch 1239 : 0.03774330019950867\n",
      "Training loss for batch 1240 : 0.2330225706100464\n",
      "Training loss for batch 1241 : 0.1589665561914444\n",
      "Training loss for batch 1242 : 0.04223821684718132\n",
      "Training loss for batch 1243 : 0.0735582634806633\n",
      "Training loss for batch 1244 : 0.09530164301395416\n",
      "Training loss for batch 1245 : 0.14781396090984344\n",
      "Training loss for batch 1246 : 0.15086209774017334\n",
      "Training loss for batch 1247 : 0.1367271989583969\n",
      "Training loss for batch 1248 : 0.10275471955537796\n",
      "Training loss for batch 1249 : 0.2366843819618225\n",
      "Training loss for batch 1250 : 0.19052264094352722\n",
      "Training loss for batch 1251 : 0.10423356294631958\n",
      "Training loss for batch 1252 : 0.13641487061977386\n",
      "Training loss for batch 1253 : 0.06407850235700607\n",
      "Training loss for batch 1254 : 0.034159716218709946\n",
      "Training loss for batch 1255 : 0.1253313273191452\n",
      "Training loss for batch 1256 : 0.12709413468837738\n",
      "Training loss for batch 1257 : 0.03212795406579971\n",
      "Training loss for batch 1258 : 0.31717416644096375\n",
      "Training loss for batch 1259 : 0.11825394630432129\n",
      "Training loss for batch 1260 : 0.0\n",
      "Training loss for batch 1261 : 0.4157717227935791\n",
      "Training loss for batch 1262 : 0.012997160665690899\n",
      "Training loss for batch 1263 : 0.32887011766433716\n",
      "Training loss for batch 1264 : 0.050085604190826416\n",
      "Training loss for batch 1265 : 0.13100160658359528\n",
      "Training loss for batch 1266 : 0.050762902945280075\n",
      "Training loss for batch 1267 : 0.05063735693693161\n",
      "Training loss for batch 1268 : 0.011491517536342144\n",
      "Training loss for batch 1269 : 0.3629426658153534\n",
      "Training loss for batch 1270 : 0.05344843491911888\n",
      "Training loss for batch 1271 : 0.08275430649518967\n",
      "Training loss for batch 1272 : 0.23550337553024292\n",
      "Training loss for batch 1273 : 0.025566060096025467\n",
      "Training loss for batch 1274 : 0.41746532917022705\n",
      "Training loss for batch 1275 : 0.2516912519931793\n",
      "Training loss for batch 1276 : 0.0\n",
      "Training loss for batch 1277 : 0.059169188141822815\n",
      "Training loss for batch 1278 : 0.005111008882522583\n",
      "Training loss for batch 1279 : 0.2832585275173187\n",
      "Training loss for batch 1280 : 0.07116364687681198\n",
      "Training loss for batch 1281 : 0.02357601188123226\n",
      "Training loss for batch 1282 : 0.10648122429847717\n",
      "Training loss for batch 1283 : 0.02611459419131279\n",
      "Training loss for batch 1284 : 0.4766109883785248\n",
      "Training loss for batch 1285 : 0.08260148763656616\n",
      "Training loss for batch 1286 : 0.28176653385162354\n",
      "Training loss for batch 1287 : 0.15142764151096344\n",
      "Training loss for batch 1288 : 0.07033520191907883\n",
      "Training loss for batch 1289 : 0.10892841964960098\n",
      "Training loss for batch 1290 : 0.2578674554824829\n",
      "Training loss for batch 1291 : 0.41033831238746643\n",
      "Training loss for batch 1292 : 0.1287454068660736\n",
      "Training loss for batch 1293 : 0.20789068937301636\n",
      "Training loss for batch 1294 : 0.01315380074083805\n",
      "Training loss for batch 1295 : 0.12048887461423874\n",
      "Training loss for batch 1296 : 0.013673089444637299\n",
      "Training loss for batch 1297 : 0.06277793645858765\n",
      "Training loss for batch 1298 : 0.12719392776489258\n",
      "Training loss for batch 1299 : 0.09044711291790009\n",
      "Training loss for batch 1300 : 0.2665005028247833\n",
      "Training loss for batch 1301 : 0.5052187442779541\n",
      "Training loss for batch 1302 : 0.6422954797744751\n",
      "Training loss for batch 1303 : 0.21470579504966736\n",
      "Training loss for batch 1304 : 0.558506190776825\n",
      "Training loss for batch 1305 : 0.23033253848552704\n",
      "Training loss for batch 1306 : 0.39174023270606995\n",
      "Training loss for batch 1307 : 0.10417746752500534\n",
      "Training loss for batch 1308 : 0.13524828851222992\n",
      "Training loss for batch 1309 : 0.3971860110759735\n",
      "Training loss for batch 1310 : 0.11569608747959137\n",
      "Training loss for batch 1311 : 0.11385224014520645\n",
      "Training loss for batch 1312 : 0.423103928565979\n",
      "Training loss for batch 1313 : 0.15875042974948883\n",
      "Training loss for batch 1314 : 0.1377672702074051\n",
      "Training loss for batch 1315 : 0.03587208315730095\n",
      "Training loss for batch 1316 : 0.162903293967247\n",
      "Training loss for batch 1317 : 0.04453904554247856\n",
      "Training loss for batch 1318 : 0.0865810215473175\n",
      "Training loss for batch 1319 : 0.14466895163059235\n",
      "Training loss for batch 1320 : 0.17726890742778778\n",
      "Training loss for batch 1321 : 0.22602012753486633\n",
      "Training loss for batch 1322 : 0.07038368284702301\n",
      "Training loss for batch 1323 : 0.20815283060073853\n",
      "Training loss for batch 1324 : 0.22327512502670288\n",
      "Training loss for batch 1325 : 0.12069283425807953\n",
      "Training loss for batch 1326 : 0.2614474892616272\n",
      "Training loss for batch 1327 : 0.0\n",
      "Training loss for batch 1328 : 0.18526098132133484\n",
      "Training loss for batch 1329 : 0.33204004168510437\n",
      "Training loss for batch 1330 : 0.5306422710418701\n",
      "Training loss for batch 1331 : 0.054760657250881195\n",
      "Training loss for batch 1332 : 0.2519146502017975\n",
      "Training loss for batch 1333 : 0.04977750778198242\n",
      "Training loss for batch 1334 : 0.010917754843831062\n",
      "Training loss for batch 1335 : 0.048488546162843704\n",
      "Training loss for batch 1336 : 0.06635426729917526\n",
      "Training loss for batch 1337 : 0.09633126854896545\n",
      "Training loss for batch 1338 : 0.1801968514919281\n",
      "Training loss for batch 1339 : 0.1851285994052887\n",
      "Training loss for batch 1340 : 0.060733694583177567\n",
      "Training loss for batch 1341 : 0.2911956012248993\n",
      "Training loss for batch 1342 : 0.08776890486478806\n",
      "Training loss for batch 1343 : 0.1769639253616333\n",
      "Training loss for batch 1344 : 0.1454331874847412\n",
      "Training loss for batch 1345 : 0.30365103483200073\n",
      "Training loss for batch 1346 : 0.23378688097000122\n",
      "Training loss for batch 1347 : 3.790855407714844e-05\n",
      "Training loss for batch 1348 : 0.16634519398212433\n",
      "Training loss for batch 1349 : 0.2191772311925888\n",
      "Training loss for batch 1350 : 0.26312458515167236\n",
      "Training loss for batch 1351 : 0.10708225518465042\n",
      "Training loss for batch 1352 : 0.3876391649246216\n",
      "Training loss for batch 1353 : 0.17943979799747467\n",
      "Training loss for batch 1354 : 0.06143787503242493\n",
      "Training loss for batch 1355 : 0.15286101400852203\n",
      "Training loss for batch 1356 : 0.11128267645835876\n",
      "Training loss for batch 1357 : 0.160414457321167\n",
      "Training loss for batch 1358 : 0.022598091512918472\n",
      "Training loss for batch 1359 : 0.04577754810452461\n",
      "Training loss for batch 1360 : 0.2559969127178192\n",
      "Training loss for batch 1361 : 0.23962059617042542\n",
      "Training loss for batch 1362 : 0.06176568940281868\n",
      "Training loss for batch 1363 : 0.26195621490478516\n",
      "Training loss for batch 1364 : 0.11922820657491684\n",
      "Training loss for batch 1365 : 0.0005185207119211555\n",
      "Training loss for batch 1366 : 0.21937742829322815\n",
      "Training loss for batch 1367 : 0.056271571666002274\n",
      "Training loss for batch 1368 : 0.35175931453704834\n",
      "Training loss for batch 1369 : 0.16759538650512695\n",
      "Training loss for batch 1370 : 0.11659238487482071\n",
      "Training loss for batch 1371 : 0.05630183219909668\n",
      "Training loss for batch 1372 : 0.0735517367720604\n",
      "Training loss for batch 1373 : 0.44634854793548584\n",
      "Training loss for batch 1374 : 0.09320370852947235\n",
      "Training loss for batch 1375 : 0.11687158048152924\n",
      "Training loss for batch 1376 : 0.09342706203460693\n",
      "Training loss for batch 1377 : 0.0035190193448215723\n",
      "Training loss for batch 1378 : 0.5014992952346802\n",
      "Training loss for batch 1379 : 0.20496892929077148\n",
      "Training loss for batch 1380 : 0.21106961369514465\n",
      "Training loss for batch 1381 : 0.16497473418712616\n",
      "Training loss for batch 1382 : 0.021855656057596207\n",
      "Training loss for batch 1383 : 0.10512258112430573\n",
      "Training loss for batch 1384 : 0.19592596590518951\n",
      "Training loss for batch 1385 : 0.22723886370658875\n",
      "Training loss for batch 1386 : 0.06320749968290329\n",
      "Training loss for batch 1387 : 0.25433433055877686\n",
      "Training loss for batch 1388 : 0.08961733430624008\n",
      "Training loss for batch 1389 : 0.03708997741341591\n",
      "Training loss for batch 1390 : 0.06179683282971382\n",
      "Training loss for batch 1391 : 0.41978245973587036\n",
      "Training loss for batch 1392 : 0.009005323983728886\n",
      "Training loss for batch 1393 : 0.1275118738412857\n",
      "Training loss for batch 1394 : 0.0859508141875267\n",
      "Training loss for batch 1395 : 0.2738044261932373\n",
      "Training loss for batch 1396 : 0.3983648419380188\n",
      "Training loss for batch 1397 : 0.16073840856552124\n",
      "Training loss for batch 1398 : 0.051679376512765884\n",
      "Training loss for batch 1399 : 0.025112953037023544\n",
      "Training loss for batch 1400 : 0.0212099589407444\n",
      "Training loss for batch 1401 : 0.058440081775188446\n",
      "Training loss for batch 1402 : 0.2670702636241913\n",
      "Training loss for batch 1403 : 0.11132559180259705\n",
      "Training loss for batch 1404 : 0.10456410050392151\n",
      "Training loss for batch 1405 : 0.0008059442043304443\n",
      "Training loss for batch 1406 : 0.19702540338039398\n",
      "Training loss for batch 1407 : 0.16299457848072052\n",
      "Training loss for batch 1408 : 0.1348453313112259\n",
      "Training loss for batch 1409 : 0.19117119908332825\n",
      "Training loss for batch 1410 : 0.23798879981040955\n",
      "Training loss for batch 1411 : 0.2419978231191635\n",
      "Training loss for batch 1412 : 0.2822569012641907\n",
      "Training loss for batch 1413 : 0.17910335958003998\n",
      "Training loss for batch 1414 : 0.058536089956760406\n",
      "Training loss for batch 1415 : 0.0005962352151982486\n",
      "Training loss for batch 1416 : 0.056606046855449677\n",
      "Training loss for batch 1417 : 0.10024464875459671\n",
      "Training loss for batch 1418 : 0.04442852362990379\n",
      "Training loss for batch 1419 : 0.11858630180358887\n",
      "Training loss for batch 1420 : 0.02039121836423874\n",
      "Training loss for batch 1421 : 0.16755074262619019\n",
      "Training loss for batch 1422 : 0.10079428553581238\n",
      "Training loss for batch 1423 : 0.17815092206001282\n",
      "Training loss for batch 1424 : 0.33939898014068604\n",
      "Training loss for batch 1425 : 0.32892653346061707\n",
      "Training loss for batch 1426 : 0.06989286094903946\n",
      "Training loss for batch 1427 : 0.10294181853532791\n",
      "Training loss for batch 1428 : 0.13931430876255035\n",
      "Training loss for batch 1429 : 0.24189990758895874\n",
      "Training loss for batch 1430 : 0.37482747435569763\n",
      "Training loss for batch 1431 : 0.0542055144906044\n",
      "Training loss for batch 1432 : 0.029259921982884407\n",
      "Training loss for batch 1433 : 0.4876933693885803\n",
      "Training loss for batch 1434 : 0.07884001731872559\n",
      "Training loss for batch 1435 : 0.1853688508272171\n",
      "Training loss for batch 1436 : 0.03367288410663605\n",
      "Training loss for batch 1437 : 0.19430725276470184\n",
      "Training loss for batch 1438 : 0.23833194375038147\n",
      "Training loss for batch 1439 : 0.07121779769659042\n",
      "Training loss for batch 1440 : 0.10359065234661102\n",
      "Training loss for batch 1441 : 0.025083307176828384\n",
      "Training loss for batch 1442 : 0.15972255170345306\n",
      "Training loss for batch 1443 : 0.40917956829071045\n",
      "Training loss for batch 1444 : 0.03112516924738884\n",
      "Training loss for batch 1445 : 0.1129838079214096\n",
      "Training loss for batch 1446 : 0.009274384006857872\n",
      "Training loss for batch 1447 : 0.06649003177881241\n",
      "Training loss for batch 1448 : 0.18164518475532532\n",
      "Training loss for batch 1449 : 0.43444502353668213\n",
      "Training loss for batch 1450 : 0.09075841307640076\n",
      "Training loss for batch 1451 : 0.19496579468250275\n",
      "Training loss for batch 1452 : 0.2849254906177521\n",
      "Training loss for batch 1453 : 0.1970616579055786\n",
      "Training loss for batch 1454 : 0.15131263434886932\n",
      "Training loss for batch 1455 : 0.2602708637714386\n",
      "Training loss for batch 1456 : 0.07076425850391388\n",
      "Training loss for batch 1457 : 0.36472731828689575\n",
      "Training loss for batch 1458 : 0.21428655087947845\n",
      "Training loss for batch 1459 : 0.29221266508102417\n",
      "Training loss for batch 1460 : 0.30221298336982727\n",
      "Training loss for batch 1461 : 0.016122421249747276\n",
      "Training loss for batch 1462 : 0.6229379773139954\n",
      "Training loss for batch 1463 : 0.03164839372038841\n",
      "Training loss for batch 1464 : 0.22395223379135132\n",
      "Training loss for batch 1465 : 0.2927388846874237\n",
      "Training loss for batch 1466 : 0.10500886291265488\n",
      "Training loss for batch 1467 : 0.10847938060760498\n",
      "Training loss for batch 1468 : 0.03922192007303238\n",
      "Training loss for batch 1469 : 0.0005749906995333731\n",
      "Training loss for batch 1470 : 0.0963825210928917\n",
      "Training loss for batch 1471 : 0.13577726483345032\n",
      "Training loss for batch 1472 : 0.06326916813850403\n",
      "Training loss for batch 1473 : 0.13803666830062866\n",
      "Training loss for batch 1474 : 0.03956988453865051\n",
      "Training loss for batch 1475 : 0.3115909695625305\n",
      "Training loss for batch 1476 : 0.05652996897697449\n",
      "Training loss for batch 1477 : 0.27853524684906006\n",
      "Training loss for batch 1478 : 0.2259901612997055\n",
      "Training loss for batch 1479 : 0.028006047010421753\n",
      "Training loss for batch 1480 : 0.06131170317530632\n",
      "Training loss for batch 1481 : 0.10678216814994812\n",
      "Training loss for batch 1482 : 0.43578699231147766\n",
      "Training loss for batch 1483 : 0.4260958731174469\n",
      "Training loss for batch 1484 : 0.10978825390338898\n",
      "Training loss for batch 1485 : 0.09277813136577606\n",
      "Training loss for batch 1486 : 0.07480766624212265\n",
      "Training loss for batch 1487 : 0.15018288791179657\n",
      "Training loss for batch 1488 : 0.01324443705379963\n",
      "Training loss for batch 1489 : 0.17113199830055237\n",
      "Training loss for batch 1490 : 0.059811290353536606\n",
      "Training loss for batch 1491 : 0.2602439820766449\n",
      "Training loss for batch 1492 : 0.1891995370388031\n",
      "Training loss for batch 1493 : 0.0819106176495552\n",
      "Training loss for batch 1494 : 0.2704226076602936\n",
      "Training loss for batch 1495 : 0.43807411193847656\n",
      "Training loss for batch 1496 : 0.029297571629285812\n",
      "Training loss for batch 1497 : 0.14476150274276733\n",
      "Training loss for batch 1498 : 0.1332065463066101\n",
      "Training loss for batch 1499 : 0.017762625589966774\n",
      "Training loss for batch 1500 : 0.27930599451065063\n",
      "Training loss for batch 1501 : 0.2516041398048401\n",
      "Training loss for batch 1502 : 0.31500157713890076\n",
      "Training loss for batch 1503 : 0.2209693044424057\n",
      "Training loss for batch 1504 : 0.060878366231918335\n",
      "Training loss for batch 1505 : 0.3661098778247833\n",
      "Training loss for batch 1506 : 0.05468902364373207\n",
      "Training loss for batch 1507 : 0.3661864697933197\n",
      "Training loss for batch 1508 : 0.02579934149980545\n",
      "Training loss for batch 1509 : 0.22301270067691803\n",
      "Training loss for batch 1510 : 0.23109352588653564\n",
      "Training loss for batch 1511 : 0.04968889430165291\n",
      "Training loss for batch 1512 : 0.19299611449241638\n",
      "Training loss for batch 1513 : 0.25988879799842834\n",
      "Training loss for batch 1514 : 0.15560689568519592\n",
      "Training loss for batch 1515 : 0.03715889900922775\n",
      "Training loss for batch 1516 : 0.1327836662530899\n",
      "Training loss for batch 1517 : 0.03643253445625305\n",
      "Training loss for batch 1518 : 0.03315073251724243\n",
      "Training loss for batch 1519 : 0.22623306512832642\n",
      "Training loss for batch 1520 : 0.08521410077810287\n",
      "Training loss for batch 1521 : 0.4593249559402466\n",
      "Training loss for batch 1522 : 0.0802127793431282\n",
      "Training loss for batch 1523 : 0.33337801694869995\n",
      "Training loss for batch 1524 : 0.10012105852365494\n",
      "Training loss for batch 1525 : 0.16557563841342926\n",
      "Training loss for batch 1526 : 0.1675785332918167\n",
      "Training loss for batch 1527 : 0.3491697311401367\n",
      "Training loss for batch 1528 : 0.05598345398902893\n",
      "Training loss for batch 1529 : 0.9448500275611877\n",
      "Training loss for batch 1530 : 0.008655677549540997\n",
      "Training loss for batch 1531 : 0.0\n",
      "Training loss for batch 1532 : 0.06003179773688316\n",
      "Training loss for batch 1533 : 0.06209176406264305\n",
      "Training loss for batch 1534 : 0.09186279028654099\n",
      "Training loss for batch 1535 : 0.09367495775222778\n",
      "Training loss for batch 1536 : 0.07621187716722488\n",
      "Training loss for batch 1537 : 0.030834704637527466\n",
      "Training loss for batch 1538 : 0.13314686715602875\n",
      "Training loss for batch 1539 : 0.24394001066684723\n",
      "Training loss for batch 1540 : 0.46360212564468384\n",
      "Training loss for batch 1541 : 0.008222351782023907\n",
      "Training loss for batch 1542 : 0.40189626812934875\n",
      "Training loss for batch 1543 : 0.007260370533913374\n",
      "Training loss for batch 1544 : 0.24733933806419373\n",
      "Training loss for batch 1545 : 0.24224238097667694\n",
      "Training loss for batch 1546 : 0.059839602559804916\n",
      "Training loss for batch 1547 : 0.0854034572839737\n",
      "Training loss for batch 1548 : 0.2885379493236542\n",
      "Training loss for batch 1549 : 0.011934221722185612\n",
      "Training loss for batch 1550 : 0.10758727788925171\n",
      "Training loss for batch 1551 : 0.014956574887037277\n",
      "Training loss for batch 1552 : 0.18563778698444366\n",
      "Training loss for batch 1553 : 0.021522749215364456\n",
      "Training loss for batch 1554 : 0.1460302472114563\n",
      "Training loss for batch 1555 : 0.0\n",
      "Training loss for batch 1556 : 0.0\n",
      "Training loss for batch 1557 : 0.09079563617706299\n",
      "Training loss for batch 1558 : 0.2849648594856262\n",
      "Training loss for batch 1559 : 0.11867454648017883\n",
      "Training loss for batch 1560 : 0.13807666301727295\n",
      "Training loss for batch 1561 : 0.3195571303367615\n",
      "Training loss for batch 1562 : 0.21709655225276947\n",
      "Training loss for batch 1563 : 0.5267210602760315\n",
      "Training loss for batch 1564 : 0.22449813783168793\n",
      "Training loss for batch 1565 : 0.2434348464012146\n",
      "Training loss for batch 1566 : 0.07848770916461945\n",
      "Training loss for batch 1567 : 0.1931995153427124\n",
      "Training loss for batch 1568 : 0.12200593948364258\n",
      "Training loss for batch 1569 : 0.15476317703723907\n",
      "Training loss for batch 1570 : 0.26586228609085083\n",
      "Training loss for batch 1571 : 0.11623609066009521\n",
      "Training loss for batch 1572 : 0.12343522906303406\n",
      "Training loss for batch 1573 : 0.1792251616716385\n",
      "Training loss for batch 1574 : 0.387633353471756\n",
      "Training loss for batch 1575 : 0.0\n",
      "Training loss for batch 1576 : 0.0\n",
      "Training loss for batch 1577 : 0.23788189888000488\n",
      "Training loss for batch 1578 : 0.45514407753944397\n",
      "Training loss for batch 1579 : 0.07824891060590744\n",
      "Training loss for batch 1580 : 0.1252528876066208\n",
      "Training loss for batch 1581 : 0.29628610610961914\n",
      "Training loss for batch 1582 : 0.0623740628361702\n",
      "Training loss for batch 1583 : 0.046468280255794525\n",
      "Training loss for batch 1584 : 0.24965479969978333\n",
      "Training loss for batch 1585 : 0.07787614315748215\n",
      "Training loss for batch 1586 : 0.2368040382862091\n",
      "Training loss for batch 1587 : 0.05765465646982193\n",
      "Training loss for batch 1588 : 0.12584461271762848\n",
      "Training loss for batch 1589 : 0.06620065867900848\n",
      "Training loss for batch 1590 : 0.25227123498916626\n",
      "Training loss for batch 1591 : 0.021384945139288902\n",
      "Training loss for batch 1592 : 0.2287837266921997\n",
      "Training loss for batch 1593 : 0.04187282919883728\n",
      "Training loss for batch 1594 : 0.0452832356095314\n",
      "Training loss for batch 1595 : 0.2751412093639374\n",
      "Training loss for batch 1596 : 0.04893837869167328\n",
      "Training loss for batch 1597 : 0.5169082283973694\n",
      "Training loss for batch 1598 : 0.3622082769870758\n",
      "Training loss for batch 1599 : 0.28776124119758606\n",
      "Training loss for batch 1600 : 0.10164652019739151\n",
      "Training loss for batch 1601 : 0.6105672121047974\n",
      "Training loss for batch 1602 : 0.21686676144599915\n",
      "Training loss for batch 1603 : 0.38866350054740906\n",
      "Training loss for batch 1604 : 0.05646912753582001\n",
      "Training loss for batch 1605 : 0.014637872576713562\n",
      "Training loss for batch 1606 : 0.058489758521318436\n",
      "Training loss for batch 1607 : 0.042863309383392334\n",
      "Training loss for batch 1608 : 0.08206000924110413\n",
      "Training loss for batch 1609 : 0.17889229953289032\n",
      "Training loss for batch 1610 : 0.18014675378799438\n",
      "Training loss for batch 1611 : 0.16724297404289246\n",
      "Training loss for batch 1612 : 0.13920733332633972\n",
      "Training loss for batch 1613 : 0.14434054493904114\n",
      "Training loss for batch 1614 : 0.1516246199607849\n",
      "Training loss for batch 1615 : 0.12766754627227783\n",
      "Training loss for batch 1616 : 0.06027170270681381\n",
      "Training loss for batch 1617 : 0.22198456525802612\n",
      "Training loss for batch 1618 : 0.2565034031867981\n",
      "Training loss for batch 1619 : 0.007192200981080532\n",
      "Training loss for batch 1620 : 0.10902460664510727\n",
      "Training loss for batch 1621 : 0.10559079051017761\n",
      "Training loss for batch 1622 : 0.290332555770874\n",
      "Training loss for batch 1623 : 0.017393317073583603\n",
      "Training loss for batch 1624 : 0.34408846497535706\n",
      "Training loss for batch 1625 : 0.30994364619255066\n",
      "Training loss for batch 1626 : 0.11617032438516617\n",
      "Training loss for batch 1627 : 0.07509761303663254\n",
      "Training loss for batch 1628 : 0.02525365725159645\n",
      "Training loss for batch 1629 : 0.6989113688468933\n",
      "Training loss for batch 1630 : 0.028964901342988014\n",
      "Training loss for batch 1631 : 0.4116384983062744\n",
      "Training loss for batch 1632 : 0.45198461413383484\n",
      "Training loss for batch 1633 : 0.13508418202400208\n",
      "Training loss for batch 1634 : 0.01601376011967659\n",
      "Training loss for batch 1635 : 0.10301224142313004\n",
      "Training loss for batch 1636 : 0.04974452406167984\n",
      "Training loss for batch 1637 : 0.12494184076786041\n",
      "Training loss for batch 1638 : 0.10612897574901581\n",
      "Training loss for batch 1639 : 0.14675256609916687\n",
      "Training loss for batch 1640 : 0.04764827340841293\n",
      "Training loss for batch 1641 : 0.09902629256248474\n",
      "Training loss for batch 1642 : 0.3950052857398987\n",
      "Training loss for batch 1643 : 0.02749321423470974\n",
      "Training loss for batch 1644 : 0.1496880203485489\n",
      "Training loss for batch 1645 : 0.0813189446926117\n",
      "Training loss for batch 1646 : 0.06413789838552475\n",
      "Training loss for batch 1647 : 0.07747655361890793\n",
      "Training loss for batch 1648 : 0.030378518626093864\n",
      "Training loss for batch 1649 : 0.04913109168410301\n",
      "Training loss for batch 1650 : 0.08509739488363266\n",
      "Training loss for batch 1651 : 0.06660056114196777\n",
      "Training loss for batch 1652 : 0.25146740674972534\n",
      "Training loss for batch 1653 : 0.03642113879323006\n",
      "Training loss for batch 1654 : 0.08381179720163345\n",
      "Training loss for batch 1655 : 0.004075626842677593\n",
      "Training loss for batch 1656 : 0.08632823079824448\n",
      "Training loss for batch 1657 : 0.2660045623779297\n",
      "Training loss for batch 1658 : 0.11880089342594147\n",
      "Training loss for batch 1659 : 0.06816314160823822\n",
      "Training loss for batch 1660 : 0.008557056076824665\n",
      "Training loss for batch 1661 : 0.38791972398757935\n",
      "Training loss for batch 1662 : 0.07107565551996231\n",
      "Training loss for batch 1663 : 0.02562512271106243\n",
      "Training loss for batch 1664 : 0.15872077643871307\n",
      "Training loss for batch 1665 : 0.10326466709375381\n",
      "Training loss for batch 1666 : 0.10228630155324936\n",
      "Training loss for batch 1667 : 0.0\n",
      "Training loss for batch 1668 : 0.3912982642650604\n",
      "Training loss for batch 1669 : 0.32577309012413025\n",
      "Training loss for batch 1670 : 0.30384907126426697\n",
      "Training loss for batch 1671 : 0.21165421605110168\n",
      "Training loss for batch 1672 : 0.2442733347415924\n",
      "Training loss for batch 1673 : 0.13449478149414062\n",
      "Training loss for batch 1674 : 0.006433677393943071\n",
      "Training loss for batch 1675 : 0.026360295712947845\n",
      "Training loss for batch 1676 : 0.21566271781921387\n",
      "Training loss for batch 1677 : 0.15726116299629211\n",
      "Training loss for batch 1678 : 0.15395444631576538\n",
      "Training loss for batch 1679 : 0.0726918876171112\n",
      "Training loss for batch 1680 : 0.12731939554214478\n",
      "Training loss for batch 1681 : 0.06627009809017181\n",
      "Training loss for batch 1682 : 0.048570048063993454\n",
      "Training loss for batch 1683 : 0.34077587723731995\n",
      "Training loss for batch 1684 : 0.40836140513420105\n",
      "Training loss for batch 1685 : 0.20988798141479492\n",
      "Training loss for batch 1686 : 0.042578112334012985\n",
      "Training loss for batch 1687 : 0.24657794833183289\n",
      "Training loss for batch 1688 : 0.11876331269741058\n",
      "Training loss for batch 1689 : 0.07629501074552536\n",
      "Training loss for batch 1690 : 0.6322276592254639\n",
      "Training loss for batch 1691 : 0.030424442142248154\n",
      "Training loss for batch 1692 : 0.22444437444210052\n",
      "Training loss for batch 1693 : 0.10797275602817535\n",
      "Training loss for batch 1694 : 0.05190145969390869\n",
      "Training loss for batch 1695 : 0.2867005467414856\n",
      "Training loss for batch 1696 : 0.019294079393148422\n",
      "Training loss for batch 1697 : 0.08107735961675644\n",
      "Training loss for batch 1698 : 0.1654881089925766\n",
      "Training loss for batch 1699 : 0.0\n",
      "Training loss for batch 1700 : 0.18308046460151672\n",
      "Training loss for batch 1701 : 0.1502678543329239\n",
      "Training loss for batch 1702 : 0.08389124274253845\n",
      "Training loss for batch 1703 : 0.09251546859741211\n",
      "Training loss for batch 1704 : 0.009336807765066624\n",
      "Training loss for batch 1705 : 0.07502856105566025\n",
      "Training loss for batch 1706 : 0.1254340261220932\n",
      "Training loss for batch 1707 : 0.020017137750983238\n",
      "Training loss for batch 1708 : 0.06212005764245987\n",
      "Training loss for batch 1709 : 0.27034473419189453\n",
      "Training loss for batch 1710 : 0.09607871621847153\n",
      "Training loss for batch 1711 : 0.2433708757162094\n",
      "Training loss for batch 1712 : 0.020039353519678116\n",
      "Training loss for batch 1713 : 0.016984855756163597\n",
      "Training loss for batch 1714 : 0.2804257869720459\n",
      "Training loss for batch 1715 : 0.268654465675354\n",
      "Training loss for batch 1716 : 0.0731179490685463\n",
      "Training loss for batch 1717 : 0.0\n",
      "Training loss for batch 1718 : 0.13935807347297668\n",
      "Training loss for batch 1719 : 0.06364268809556961\n",
      "Training loss for batch 1720 : 0.06312204152345657\n",
      "Training loss for batch 1721 : 0.0905873030424118\n",
      "Training loss for batch 1722 : 0.4663427770137787\n",
      "Training loss for batch 1723 : 0.011775556020438671\n",
      "Training loss for batch 1724 : 0.06660532206296921\n",
      "Training loss for batch 1725 : 0.06941424310207367\n",
      "Training loss for batch 1726 : 0.17874032258987427\n",
      "Training loss for batch 1727 : 0.12508265674114227\n",
      "Training loss for batch 1728 : 0.2531781494617462\n",
      "Training loss for batch 1729 : 0.0937599241733551\n",
      "Training loss for batch 1730 : 0.0979083701968193\n",
      "Training loss for batch 1731 : 0.06372493505477905\n",
      "Training loss for batch 1732 : 0.06435642391443253\n",
      "Training loss for batch 1733 : 0.0013345549814403057\n",
      "Training loss for batch 1734 : 0.18932999670505524\n",
      "Training loss for batch 1735 : 0.25606173276901245\n",
      "Training loss for batch 1736 : 0.0537782721221447\n",
      "Training loss for batch 1737 : 0.2321006953716278\n",
      "Training loss for batch 1738 : 0.0838334709405899\n",
      "Training loss for batch 1739 : 8.32071527838707e-05\n",
      "Training loss for batch 1740 : 0.10039594769477844\n",
      "Training loss for batch 1741 : 0.4970565736293793\n",
      "Training loss for batch 1742 : 0.2158346027135849\n",
      "Training loss for batch 1743 : 0.07110083848237991\n",
      "Training loss for batch 1744 : 0.21078996360301971\n",
      "Training loss for batch 1745 : 0.18649663031101227\n",
      "Training loss for batch 1746 : 0.055284686386585236\n",
      "Training loss for batch 1747 : 0.2038811594247818\n",
      "Training loss for batch 1748 : 0.07806527614593506\n",
      "Training loss for batch 1749 : 0.21405690908432007\n",
      "Training loss for batch 1750 : 0.1841907650232315\n",
      "Training loss for batch 1751 : 0.10664926469326019\n",
      "Training loss for batch 1752 : 0.22913417220115662\n",
      "Training loss for batch 1753 : 0.0023486516438424587\n",
      "Training loss for batch 1754 : 0.11404622346162796\n",
      "Training loss for batch 1755 : 0.01180926151573658\n",
      "Training loss for batch 1756 : 0.1595563441514969\n",
      "Training loss for batch 1757 : 0.15686972439289093\n",
      "Training loss for batch 1758 : 0.14719834923744202\n",
      "Training loss for batch 1759 : 0.09093447774648666\n",
      "Training loss for batch 1760 : 0.33937445282936096\n",
      "Training loss for batch 1761 : 0.3527100384235382\n",
      "Training loss for batch 1762 : 0.058386389166116714\n",
      "Training loss for batch 1763 : 0.09383874386548996\n",
      "Training loss for batch 1764 : 0.22098076343536377\n",
      "Training loss for batch 1765 : 0.16779114305973053\n",
      "Training loss for batch 1766 : 0.11876178532838821\n",
      "Training loss for batch 1767 : 0.2479809671640396\n",
      "Training loss for batch 1768 : 0.1451692134141922\n",
      "Training loss for batch 1769 : 0.24780647456645966\n",
      "Training loss for batch 1770 : 0.01165488176047802\n",
      "Training loss for batch 1771 : 0.04695313796401024\n",
      "Training loss for batch 1772 : 0.3108770251274109\n",
      "Training loss for batch 1773 : 0.31514671444892883\n",
      "Training loss for batch 1774 : 0.06371474266052246\n",
      "Training loss for batch 1775 : 0.10584071278572083\n",
      "Training loss for batch 1776 : 0.34706491231918335\n",
      "Training loss for batch 1777 : 0.05341976508498192\n",
      "Training loss for batch 1778 : 0.09551490098237991\n",
      "Training loss for batch 1779 : 0.12593206763267517\n",
      "Training loss for batch 1780 : 0.008617032319307327\n",
      "Training loss for batch 1781 : 0.056129392236471176\n",
      "Training loss for batch 1782 : 0.2597324252128601\n",
      "Training loss for batch 1783 : 0.015646908432245255\n",
      "Training loss for batch 1784 : 0.08803781867027283\n",
      "Training loss for batch 1785 : 0.0\n",
      "Training loss for batch 1786 : 0.8171393275260925\n",
      "Training loss for batch 1787 : 0.12301330268383026\n",
      "Training loss for batch 1788 : 0.38700270652770996\n",
      "Training loss for batch 1789 : 0.3513337969779968\n",
      "Training loss for batch 1790 : 0.08063951879739761\n",
      "Training loss for batch 1791 : 0.42281675338745117\n",
      "Training loss for batch 1792 : 0.020490875467658043\n",
      "Training loss for batch 1793 : 0.03040442056953907\n",
      "Training loss for batch 1794 : 0.05796024203300476\n",
      "Training loss for batch 1795 : 0.08880718797445297\n",
      "Training loss for batch 1796 : 0.016396593302488327\n",
      "Training loss for batch 1797 : 0.27514004707336426\n",
      "Training loss for batch 1798 : 0.4397275745868683\n",
      "Training loss for batch 1799 : 0.13096560537815094\n",
      "Training loss for batch 1800 : 0.20869080722332\n",
      "Training loss for batch 1801 : 0.07341069728136063\n",
      "Training loss for batch 1802 : 0.17280156910419464\n",
      "Training loss for batch 1803 : 0.07240238785743713\n",
      "Training loss for batch 1804 : 0.13918663561344147\n",
      "Training loss for batch 1805 : 0.18930503726005554\n",
      "Training loss for batch 1806 : 0.048372358083724976\n",
      "Training loss for batch 1807 : 0.009712358936667442\n",
      "Training loss for batch 1808 : 0.042108140885829926\n",
      "Training loss for batch 1809 : 0.2726826071739197\n",
      "Training loss for batch 1810 : 0.29856613278388977\n",
      "Training loss for batch 1811 : 0.06520513445138931\n",
      "Training loss for batch 1812 : 0.23494403064250946\n",
      "Training loss for batch 1813 : 0.13852404057979584\n",
      "Training loss for batch 1814 : 0.18180614709854126\n",
      "Training loss for batch 1815 : 0.434205561876297\n",
      "Training loss for batch 1816 : 0.14244306087493896\n",
      "Training loss for batch 1817 : 0.17619352042675018\n",
      "Training loss for batch 1818 : 0.16012905538082123\n",
      "Training loss for batch 1819 : 0.22444918751716614\n",
      "Training loss for batch 1820 : 0.22181697189807892\n",
      "Training loss for batch 1821 : 0.020173054188489914\n",
      "Training loss for batch 1822 : 0.2392832487821579\n",
      "Training loss for batch 1823 : 0.07927384227514267\n",
      "Training loss for batch 1824 : 0.0937938317656517\n",
      "Training loss for batch 1825 : 0.19398677349090576\n",
      "Training loss for batch 1826 : 0.0\n",
      "Training loss for batch 1827 : 0.23495234549045563\n",
      "Training loss for batch 1828 : 0.03267931565642357\n",
      "Training loss for batch 1829 : 0.0\n",
      "Training loss for batch 1830 : 0.15408095717430115\n",
      "Training loss for batch 1831 : 0.23682251572608948\n",
      "Training loss for batch 1832 : 0.39340877532958984\n",
      "Training loss for batch 1833 : 0.15149341523647308\n",
      "Training loss for batch 1834 : 0.20509395003318787\n",
      "Training loss for batch 1835 : 0.22720670700073242\n",
      "Training loss for batch 1836 : 0.16832441091537476\n",
      "Training loss for batch 1837 : 0.08611937612295151\n",
      "Training loss for batch 1838 : 0.2424953728914261\n",
      "Training loss for batch 1839 : 0.2257312685251236\n",
      "Training loss for batch 1840 : 0.25843915343284607\n",
      "Training loss for batch 1841 : 0.1562492698431015\n",
      "Training loss for batch 1842 : 0.10822917520999908\n",
      "Training loss for batch 1843 : 0.010460734367370605\n",
      "Training loss for batch 1844 : 0.04661313444375992\n",
      "Training loss for batch 1845 : 0.052078962326049805\n",
      "Training loss for batch 1846 : 0.10103610157966614\n",
      "Training loss for batch 1847 : 0.12481497973203659\n",
      "Training loss for batch 1848 : 0.06343726813793182\n",
      "Training loss for batch 1849 : 0.45056453347206116\n",
      "Training loss for batch 1850 : 0.048622529953718185\n",
      "Training loss for batch 1851 : 0.31867462396621704\n",
      "Training loss for batch 1852 : 0.1077755019068718\n",
      "Training loss for batch 1853 : 0.16552138328552246\n",
      "Training loss for batch 1854 : 0.08839999884366989\n",
      "Training loss for batch 1855 : 0.16267353296279907\n",
      "Training loss for batch 1856 : 0.04016488045454025\n",
      "Training loss for batch 1857 : 0.0704406350851059\n",
      "Training loss for batch 1858 : 0.33331233263015747\n",
      "Training loss for batch 1859 : 0.11433269083499908\n",
      "Training loss for batch 1860 : 0.0613626204431057\n",
      "Training loss for batch 1861 : 0.009942669421434402\n",
      "Training loss for batch 1862 : 0.006139128934592009\n",
      "Training loss for batch 1863 : 0.01308808010071516\n",
      "Training loss for batch 1864 : 0.15629208087921143\n",
      "Training loss for batch 1865 : 0.1187247782945633\n",
      "Training loss for batch 1866 : 0.10891972482204437\n",
      "Training loss for batch 1867 : 0.038705360144376755\n",
      "Training loss for batch 1868 : 0.10461096465587616\n",
      "Training loss for batch 1869 : 0.1900162398815155\n",
      "Training loss for batch 1870 : 0.27435430884361267\n",
      "Training loss for batch 1871 : 0.20249812304973602\n",
      "Training loss for batch 1872 : 0.01821373589336872\n",
      "Training loss for batch 1873 : 0.018627246841788292\n",
      "Training loss for batch 1874 : 0.11978177726268768\n",
      "Training loss for batch 1875 : 0.2598055601119995\n",
      "Training loss for batch 1876 : 0.07693246006965637\n",
      "Training loss for batch 1877 : 0.5651904344558716\n",
      "Training loss for batch 1878 : 0.058150291442871094\n",
      "Training loss for batch 1879 : 0.010374877601861954\n",
      "Training loss for batch 1880 : 0.39835765957832336\n",
      "Training loss for batch 1881 : 0.13164538145065308\n",
      "Training loss for batch 1882 : 0.2954334318637848\n",
      "Training loss for batch 1883 : 0.0010674595832824707\n",
      "Training loss for batch 1884 : 0.13743150234222412\n",
      "Training loss for batch 1885 : 0.26099035143852234\n",
      "Training loss for batch 1886 : 0.045015785843133926\n",
      "Training loss for batch 1887 : 0.14293526113033295\n",
      "Training loss for batch 1888 : 0.2805081903934479\n",
      "Training loss for batch 1889 : 0.11212433874607086\n",
      "Training loss for batch 1890 : 0.1594579517841339\n",
      "Training loss for batch 1891 : 0.11539411544799805\n",
      "Training loss for batch 1892 : 0.1665199100971222\n",
      "Training loss for batch 1893 : 0.12341589480638504\n",
      "Training loss for batch 1894 : 0.2292507290840149\n",
      "Training loss for batch 1895 : 0.450211763381958\n",
      "Training loss for batch 1896 : 0.31709378957748413\n",
      "Training loss for batch 1897 : 0.09681371599435806\n",
      "Training loss for batch 1898 : 0.30084335803985596\n",
      "Training loss for batch 1899 : 0.373782753944397\n",
      "Training loss for batch 1900 : 0.39858707785606384\n",
      "Training loss for batch 1901 : 0.002599544357508421\n",
      "Training loss for batch 1902 : 0.0365619882941246\n",
      "Training loss for batch 1903 : 0.06548971682786942\n",
      "Training loss for batch 1904 : 0.11232162266969681\n",
      "Training loss for batch 1905 : 0.20459043979644775\n",
      "Training loss for batch 1906 : 0.29707053303718567\n",
      "Training loss for batch 1907 : 0.17045889794826508\n",
      "Training loss for batch 1908 : 0.25507068634033203\n",
      "Training loss for batch 1909 : 0.1504763513803482\n",
      "Training loss for batch 1910 : 0.11716753244400024\n",
      "Training loss for batch 1911 : 0.03788534924387932\n",
      "Training loss for batch 1912 : 0.1607040911912918\n",
      "Training loss for batch 1913 : 0.15084850788116455\n",
      "Training loss for batch 1914 : 0.09290269017219543\n",
      "Training loss for batch 1915 : 0.21500489115715027\n",
      "Training loss for batch 1916 : 0.12007250636816025\n",
      "Training loss for batch 1917 : 0.015855545178055763\n",
      "Training loss for batch 1918 : 0.15408068895339966\n",
      "Training loss for batch 1919 : 0.23043718934059143\n",
      "Training loss for batch 1920 : 0.09265202283859253\n",
      "Training loss for batch 1921 : 0.05067519471049309\n",
      "Training loss for batch 1922 : 0.15417617559432983\n",
      "Training loss for batch 1923 : 0.13872866332530975\n",
      "Training loss for batch 1924 : 0.02619895711541176\n",
      "Training loss for batch 1925 : 0.1866450160741806\n",
      "Training loss for batch 1926 : 0.24262294173240662\n",
      "Training loss for batch 1927 : 0.24219638109207153\n",
      "Training loss for batch 1928 : 0.33292368054389954\n",
      "Training loss for batch 1929 : 0.3670623004436493\n",
      "Training loss for batch 1930 : 0.11387291550636292\n",
      "Training loss for batch 1931 : 0.17792285978794098\n",
      "Training loss for batch 1932 : 0.31980055570602417\n",
      "Training loss for batch 1933 : 0.2886415123939514\n",
      "Training loss for batch 1934 : 0.26849696040153503\n",
      "Training loss for batch 1935 : 0.026283135637640953\n",
      "Training loss for batch 1936 : 0.00018227100372314453\n",
      "Training loss for batch 1937 : 0.18226194381713867\n",
      "Training loss for batch 1938 : 0.07960907369852066\n",
      "Training loss for batch 1939 : 0.02795431576669216\n",
      "Training loss for batch 1940 : 0.18579153716564178\n",
      "Training loss for batch 1941 : 0.22061948478221893\n",
      "Training loss for batch 1942 : 0.21242094039916992\n",
      "Training loss for batch 1943 : 0.09151872992515564\n",
      "Training loss for batch 1944 : 0.18200509250164032\n",
      "Training loss for batch 1945 : 0.1010172888636589\n",
      "Training loss for batch 1946 : 0.07143393158912659\n",
      "Training loss for batch 1947 : 0.2372906506061554\n",
      "Training loss for batch 1948 : 0.13875815272331238\n",
      "Training loss for batch 1949 : 0.0485130213201046\n",
      "Training loss for batch 1950 : 0.11859764158725739\n",
      "Training loss for batch 1951 : 0.10523874312639236\n",
      "Training loss for batch 1952 : 0.18185988068580627\n",
      "Training loss for batch 1953 : 0.21246866881847382\n",
      "Training loss for batch 1954 : 0.28544074296951294\n",
      "Training loss for batch 1955 : 0.14554806053638458\n",
      "Training loss for batch 1956 : 0.13870488107204437\n",
      "Training loss for batch 1957 : 0.2142316699028015\n",
      "Training loss for batch 1958 : 0.0058197095058858395\n",
      "Training loss for batch 1959 : 0.24979765713214874\n",
      "Training loss for batch 1960 : 0.06922644376754761\n",
      "Training loss for batch 1961 : 0.379500150680542\n",
      "Training loss for batch 1962 : 0.21936316788196564\n",
      "Training loss for batch 1963 : 0.20342150330543518\n",
      "Training loss for batch 1964 : 0.11898063123226166\n",
      "Training loss for batch 1965 : 0.04316341504454613\n",
      "Training loss for batch 1966 : 0.19950927793979645\n",
      "Training loss for batch 1967 : 0.06677395850419998\n",
      "Training loss for batch 1968 : 0.0027544996701180935\n",
      "Training loss for batch 1969 : 0.1057267040014267\n",
      "Training loss for batch 1970 : 0.07051731646060944\n",
      "Training loss for batch 1971 : 0.07549574971199036\n",
      "Training loss for batch 1972 : 0.09493296593427658\n",
      "Training loss for batch 1973 : 0.08700878173112869\n",
      "Training loss for batch 1974 : 0.03847504407167435\n",
      "Training loss for batch 1975 : 0.06500717997550964\n",
      "Training loss for batch 1976 : 0.024130340665578842\n",
      "Training loss for batch 1977 : 0.13805587589740753\n",
      "Training loss for batch 1978 : 0.019410911947488785\n",
      "Training loss for batch 1979 : 0.23442256450653076\n",
      "Training loss for batch 1980 : 0.05463983491063118\n",
      "Training loss for batch 1981 : 0.15391375124454498\n",
      "Training loss for batch 1982 : 0.005984626244753599\n",
      "Training loss for batch 1983 : 0.17343728244304657\n",
      "Training loss for batch 1984 : 0.20423391461372375\n",
      "Training loss for batch 1985 : 0.2140863686800003\n",
      "Training loss for batch 1986 : 0.08611249178647995\n",
      "Training loss for batch 1987 : 0.5256981253623962\n",
      "Training loss for batch 1988 : 0.08112472295761108\n",
      "Training loss for batch 1989 : 0.2720969617366791\n",
      "Training loss for batch 1990 : 0.07385420799255371\n",
      "Training loss for batch 1991 : 0.11442791670560837\n",
      "Training loss for batch 1992 : 0.03352085500955582\n",
      "Training loss for batch 1993 : 0.18025755882263184\n",
      "Training loss for batch 1994 : 0.25335732102394104\n",
      "Training loss for batch 1995 : 0.18413883447647095\n",
      "Training loss for batch 1996 : 0.17285586893558502\n",
      "Training loss for batch 1997 : 0.2745673954486847\n",
      "Training loss for batch 1998 : 0.15339425206184387\n",
      "Training loss for batch 1999 : 0.23717401921749115\n",
      "Training loss for batch 2000 : 0.2549095153808594\n",
      "Training loss for batch 2001 : 0.016965975984930992\n",
      "Training loss for batch 2002 : 0.26883232593536377\n",
      "Training loss for batch 2003 : 0.21336951851844788\n",
      "Training loss for batch 2004 : -0.0001231119967997074\n",
      "Training loss for batch 2005 : 0.25966939330101013\n",
      "Training loss for batch 2006 : 0.3053901195526123\n",
      "Training loss for batch 2007 : 0.20932318270206451\n",
      "Training loss for batch 2008 : 0.17916928231716156\n",
      "Training loss for batch 2009 : 0.010932154022157192\n",
      "Training loss for batch 2010 : 0.10499905794858932\n",
      "Training loss for batch 2011 : 0.16266344487667084\n",
      "Training loss for batch 2012 : 0.0009576500160619617\n",
      "Training loss for batch 2013 : 0.21634839475154877\n",
      "Training loss for batch 2014 : 0.15631942451000214\n",
      "Training loss for batch 2015 : 0.25538378953933716\n",
      "Training loss for batch 2016 : -4.149401138420217e-05\n",
      "Training loss for batch 2017 : 0.1268399953842163\n",
      "Training loss for batch 2018 : 0.06893748790025711\n",
      "Training loss for batch 2019 : 0.048163339495658875\n",
      "Training loss for batch 2020 : 0.37050166726112366\n",
      "Training loss for batch 2021 : 0.1876887083053589\n",
      "Training loss for batch 2022 : 0.0867212787270546\n",
      "Training loss for batch 2023 : 0.26973429322242737\n",
      "Training loss for batch 2024 : 0.06828238070011139\n",
      "Training loss for batch 2025 : 0.3063216805458069\n",
      "Training loss for batch 2026 : 0.05081886425614357\n",
      "Training loss for batch 2027 : 0.06396239995956421\n",
      "Training loss for batch 2028 : 0.04769483953714371\n",
      "Training loss for batch 2029 : 0.21532182395458221\n",
      "Training loss for batch 2030 : 0.2548116445541382\n",
      "Training loss for batch 2031 : 0.1140337735414505\n",
      "Training loss for batch 2032 : 0.18523846566677094\n",
      "Training loss for batch 2033 : 0.02297002635896206\n",
      "Training loss for batch 2034 : 0.1658637970685959\n",
      "Training loss for batch 2035 : 0.19062386453151703\n",
      "Training loss for batch 2036 : 0.28789591789245605\n",
      "Training loss for batch 2037 : 0.12498000264167786\n",
      "Training loss for batch 2038 : 0.24968451261520386\n",
      "Training loss for batch 2039 : 0.3643344044685364\n",
      "Training loss for batch 2040 : 0.44098618626594543\n",
      "Training loss for batch 2041 : 0.12919612228870392\n",
      "Training loss for batch 2042 : 0.34655413031578064\n",
      "Training loss for batch 2043 : 0.03885993734002113\n",
      "Training loss for batch 2044 : 0.3561652600765228\n",
      "Training loss for batch 2045 : 0.0018270015716552734\n",
      "Training loss for batch 2046 : 0.1267540603876114\n",
      "Training loss for batch 2047 : 0.0\n",
      "Training loss for batch 2048 : 0.2186422348022461\n",
      "Training loss for batch 2049 : 0.1381542831659317\n",
      "Training loss for batch 2050 : 0.21403269469738007\n",
      "Training loss for batch 2051 : 0.22166836261749268\n",
      "Training loss for batch 2052 : 0.009107452817261219\n",
      "Training loss for batch 2053 : 0.10822680592536926\n",
      "Training loss for batch 2054 : 0.0913219004869461\n",
      "Training loss for batch 2055 : 0.1215856596827507\n",
      "Training loss for batch 2056 : 0.313215970993042\n",
      "Training loss for batch 2057 : 0.05385984107851982\n",
      "Training loss for batch 2058 : 0.27873846888542175\n",
      "Training loss for batch 2059 : 0.05469280108809471\n",
      "Training loss for batch 2060 : 0.1521395891904831\n",
      "Training loss for batch 2061 : 0.011114220134913921\n",
      "Training loss for batch 2062 : 0.1473332941532135\n",
      "Training loss for batch 2063 : 0.11312537640333176\n",
      "Training loss for batch 2064 : 0.32944953441619873\n",
      "Training loss for batch 2065 : 0.11172188073396683\n",
      "Training loss for batch 2066 : 0.3154137134552002\n",
      "Training loss for batch 2067 : 0.4171479344367981\n",
      "Training loss for batch 2068 : 0.3009456992149353\n",
      "Training loss for batch 2069 : 0.1857130229473114\n",
      "Training loss for batch 2070 : 0.30370402336120605\n",
      "Training loss for batch 2071 : 0.23056016862392426\n",
      "Training loss for batch 2072 : 0.11551780253648758\n",
      "Training loss for batch 2073 : 0.21423231065273285\n",
      "Training loss for batch 2074 : 0.06251227110624313\n",
      "Training loss for batch 2075 : 0.3664976954460144\n",
      "Training loss for batch 2076 : 0.23027409613132477\n",
      "Training loss for batch 2077 : 0.13111965358257294\n",
      "Training loss for batch 2078 : 0.20473001897335052\n",
      "Training loss for batch 2079 : 0.2211904674768448\n",
      "Training loss for batch 2080 : 0.028444766998291016\n",
      "Training loss for batch 2081 : 0.09977222234010696\n",
      "Training loss for batch 2082 : 0.3692455589771271\n",
      "Training loss for batch 2083 : 0.039674095809459686\n",
      "Training loss for batch 2084 : 0.036965541541576385\n",
      "Training loss for batch 2085 : 0.11088544130325317\n",
      "Training loss for batch 2086 : 0.4590221345424652\n",
      "Training loss for batch 2087 : 0.07882117480039597\n",
      "Training loss for batch 2088 : 0.19137099385261536\n",
      "Training loss for batch 2089 : 0.04902341961860657\n",
      "Training loss for batch 2090 : 0.2523844242095947\n",
      "Training loss for batch 2091 : 0.035077549517154694\n",
      "Training loss for batch 2092 : 0.0934547409415245\n",
      "Training loss for batch 2093 : 0.052815958857536316\n",
      "Training loss for batch 2094 : 0.20984986424446106\n",
      "Training loss for batch 2095 : 0.004186840727925301\n",
      "Training loss for batch 2096 : 0.03947535902261734\n",
      "Training loss for batch 2097 : 0.266146719455719\n",
      "Training loss for batch 2098 : 0.17066703736782074\n",
      "Training loss for batch 2099 : 0.05788308382034302\n",
      "Training loss for batch 2100 : 0.03614656999707222\n",
      "Training loss for batch 2101 : 0.006397032644599676\n",
      "Training loss for batch 2102 : 0.36710917949676514\n",
      "Training loss for batch 2103 : 0.15990719199180603\n",
      "Training loss for batch 2104 : 0.021452799439430237\n",
      "Training loss for batch 2105 : 0.18164706230163574\n",
      "Training loss for batch 2106 : 0.03092152252793312\n",
      "Training loss for batch 2107 : 0.09388325363397598\n",
      "Training loss for batch 2108 : 0.31519749760627747\n",
      "Training loss for batch 2109 : 0.047346197068691254\n",
      "Training loss for batch 2110 : 0.2701011598110199\n",
      "Training loss for batch 2111 : 0.14358939230442047\n",
      "Training loss for batch 2112 : 0.2184419184923172\n",
      "Training loss for batch 2113 : 0.2666662633419037\n",
      "Training loss for batch 2114 : 0.31058618426322937\n",
      "Training loss for batch 2115 : 0.2274053692817688\n",
      "Training loss for batch 2116 : 0.0018582642078399658\n",
      "Training loss for batch 2117 : 0.23953959345817566\n",
      "Training loss for batch 2118 : 0.1840471774339676\n",
      "Training loss for batch 2119 : 0.09838294982910156\n",
      "Training loss for batch 2120 : 0.34675461053848267\n",
      "Training loss for batch 2121 : 0.11493575572967529\n",
      "Training loss for batch 2122 : 0.3245537281036377\n",
      "Training loss for batch 2123 : 0.1306692212820053\n",
      "Training loss for batch 2124 : 0.25473329424858093\n",
      "Training loss for batch 2125 : 0.10892091691493988\n",
      "Training loss for batch 2126 : 0.4332389831542969\n",
      "Training loss for batch 2127 : 0.3432023823261261\n",
      "Training loss for batch 2128 : 0.21599145233631134\n",
      "Training loss for batch 2129 : 0.06443125009536743\n",
      "Training loss for batch 2130 : 0.2250031977891922\n",
      "Training loss for batch 2131 : 0.25557202100753784\n",
      "Training loss for batch 2132 : 0.1952144056558609\n",
      "Training loss for batch 2133 : 0.0441741943359375\n",
      "Training loss for batch 2134 : 0.43370017409324646\n",
      "Training loss for batch 2135 : 0.35543930530548096\n",
      "Training loss for batch 2136 : 0.3164970278739929\n",
      "Training loss for batch 2137 : 0.07469814270734787\n",
      "Training loss for batch 2138 : 0.32058292627334595\n",
      "Training loss for batch 2139 : 0.32308533787727356\n",
      "Training loss for batch 2140 : 0.09641313552856445\n",
      "Training loss for batch 2141 : 0.15165968239307404\n",
      "Training loss for batch 2142 : 0.014384687878191471\n",
      "Training loss for batch 2143 : 0.3002716898918152\n",
      "Training loss for batch 2144 : 0.0009187459945678711\n",
      "Training loss for batch 2145 : 0.11803530156612396\n",
      "Training loss for batch 2146 : 0.4271145164966583\n",
      "Training loss for batch 2147 : 0.010359139181673527\n",
      "Training loss for batch 2148 : 0.0701269581913948\n",
      "Training loss for batch 2149 : 0.44291311502456665\n",
      "Training loss for batch 2150 : 0.10457486659288406\n",
      "Training loss for batch 2151 : 0.05797811970114708\n",
      "Training loss for batch 2152 : 0.3602563738822937\n",
      "Training loss for batch 2153 : 0.23613996803760529\n",
      "Training loss for batch 2154 : 0.32997506856918335\n",
      "Training loss for batch 2155 : 0.3857014775276184\n",
      "Training loss for batch 2156 : 0.02707631327211857\n",
      "Training loss for batch 2157 : 0.041445739567279816\n",
      "Training loss for batch 2158 : 0.27769333124160767\n",
      "Training loss for batch 2159 : 0.08919792622327805\n",
      "Training loss for batch 2160 : 0.10871012508869171\n",
      "Training loss for batch 2161 : 0.04255710542201996\n",
      "Training loss for batch 2162 : 0.2606422007083893\n",
      "Training loss for batch 2163 : 0.235077366232872\n",
      "Training loss for batch 2164 : 0.030680786818265915\n",
      "Training loss for batch 2165 : 0.08455561101436615\n",
      "Training loss for batch 2166 : 0.12330715358257294\n",
      "Training loss for batch 2167 : 0.20505547523498535\n",
      "Training loss for batch 2168 : 0.24431338906288147\n",
      "Training loss for batch 2169 : 0.010425880551338196\n",
      "Training loss for batch 2170 : 0.12513065338134766\n",
      "Training loss for batch 2171 : 0.16254332661628723\n",
      "Training loss for batch 2172 : 0.0620882511138916\n",
      "Training loss for batch 2173 : 0.1461009383201599\n",
      "Training loss for batch 2174 : 0.1888512372970581\n",
      "Training loss for batch 2175 : 0.07968752086162567\n",
      "Training loss for batch 2176 : 0.03430977463722229\n",
      "Training loss for batch 2177 : 0.024958621710538864\n",
      "Training loss for batch 2178 : 0.30534133315086365\n",
      "Training loss for batch 2179 : 0.3248596787452698\n",
      "Training loss for batch 2180 : 0.0948328897356987\n",
      "Training loss for batch 2181 : 0.05898222327232361\n",
      "Training loss for batch 2182 : 0.17664729058742523\n",
      "Training loss for batch 2183 : 0.276826411485672\n",
      "Training loss for batch 2184 : 0.16840311884880066\n",
      "Training loss for batch 2185 : 0.11230653524398804\n",
      "Training loss for batch 2186 : 0.19840775430202484\n",
      "Training loss for batch 2187 : 0.40558093786239624\n",
      "Training loss for batch 2188 : 0.07778024673461914\n",
      "Training loss for batch 2189 : 0.29199641942977905\n",
      "Training loss for batch 2190 : 0.052731819450855255\n",
      "Training loss for batch 2191 : 0.29891493916511536\n",
      "Training loss for batch 2192 : 0.0011990865459665656\n",
      "Training loss for batch 2193 : 0.0\n",
      "Training loss for batch 2194 : 0.023246154189109802\n",
      "Training loss for batch 2195 : 0.1608300656080246\n",
      "Training loss for batch 2196 : 0.2460290938615799\n",
      "Training loss for batch 2197 : 0.2572430670261383\n",
      "Training loss for batch 2198 : 0.12151464819908142\n",
      "Training loss for batch 2199 : 0.19077356159687042\n",
      "Training loss for batch 2200 : 0.47214484214782715\n",
      "Training loss for batch 2201 : 0.16284792125225067\n",
      "Training loss for batch 2202 : 0.012044956907629967\n",
      "Training loss for batch 2203 : 0.26669952273368835\n",
      "Training loss for batch 2204 : 0.04162779822945595\n",
      "Training loss for batch 2205 : 0.03434556722640991\n",
      "Training loss for batch 2206 : 0.02745782770216465\n",
      "Training loss for batch 2207 : 0.11511561274528503\n",
      "Training loss for batch 2208 : 0.08678737282752991\n",
      "Training loss for batch 2209 : 0.09354215115308762\n",
      "Training loss for batch 2210 : 0.20479167997837067\n",
      "Training loss for batch 2211 : 0.42004111409187317\n",
      "Training loss for batch 2212 : 0.25268709659576416\n",
      "Training loss for batch 2213 : 0.2269555628299713\n",
      "Training loss for batch 2214 : 0.28477713465690613\n",
      "Training loss for batch 2215 : 0.0\n",
      "Training loss for batch 2216 : 0.1341618448495865\n",
      "Training loss for batch 2217 : 0.392469584941864\n",
      "Training loss for batch 2218 : 0.34499502182006836\n",
      "Training loss for batch 2219 : 0.11182040721178055\n",
      "Training loss for batch 2220 : 0.2503662705421448\n",
      "Training loss for batch 2221 : 0.02929435484111309\n",
      "Training loss for batch 2222 : 0.17648737132549286\n",
      "Training loss for batch 2223 : 0.10205687582492828\n",
      "Training loss for batch 2224 : 0.04635648429393768\n",
      "Training loss for batch 2225 : 0.05715402960777283\n",
      "Training loss for batch 2226 : 0.33829978108406067\n",
      "Training loss for batch 2227 : 0.08342478424310684\n",
      "Training loss for batch 2228 : 0.07303895056247711\n",
      "Training loss for batch 2229 : 0.23505988717079163\n",
      "Training loss for batch 2230 : 0.23465585708618164\n",
      "Training loss for batch 2231 : 0.16619601845741272\n",
      "Training loss for batch 2232 : 0.297760933637619\n",
      "Training loss for batch 2233 : 0.028795810416340828\n",
      "Training loss for batch 2234 : 0.12798388302326202\n",
      "Training loss for batch 2235 : 0.14309680461883545\n",
      "Training loss for batch 2236 : 0.010910282842814922\n",
      "Training loss for batch 2237 : 0.04645286500453949\n",
      "Training loss for batch 2238 : 0.43085312843322754\n",
      "Training loss for batch 2239 : 0.0979989767074585\n",
      "Training loss for batch 2240 : 0.3197232186794281\n",
      "Training loss for batch 2241 : 0.0212139543145895\n",
      "Training loss for batch 2242 : 0.36721575260162354\n",
      "Training loss for batch 2243 : 0.0233107041567564\n",
      "Training loss for batch 2244 : 0.25469374656677246\n",
      "Training loss for batch 2245 : 0.21223962306976318\n",
      "Training loss for batch 2246 : 0.3406272232532501\n",
      "Training loss for batch 2247 : 0.0025686423759907484\n",
      "Training loss for batch 2248 : 0.06896542757749557\n",
      "Training loss for batch 2249 : 0.1549614518880844\n",
      "Training loss for batch 2250 : 0.09433203190565109\n",
      "Training loss for batch 2251 : 0.06760279834270477\n",
      "Training loss for batch 2252 : 0.12073343992233276\n",
      "Training loss for batch 2253 : 0.23385021090507507\n",
      "Training loss for batch 2254 : 0.02808225527405739\n",
      "Training loss for batch 2255 : 0.12125606834888458\n",
      "Training loss for batch 2256 : 0.3220421373844147\n",
      "Training loss for batch 2257 : 0.09980662167072296\n",
      "Training loss for batch 2258 : 0.03452223166823387\n",
      "Training loss for batch 2259 : 0.2603142559528351\n",
      "Training loss for batch 2260 : 0.2735419273376465\n",
      "Training loss for batch 2261 : 0.05073225870728493\n",
      "Training loss for batch 2262 : 0.3500722050666809\n",
      "Training loss for batch 2263 : 0.24828927218914032\n",
      "Training loss for batch 2264 : 0.3000950515270233\n",
      "Training loss for batch 2265 : 0.23991449177265167\n",
      "Training loss for batch 2266 : 0.05254868417978287\n",
      "Training loss for batch 2267 : 0.11195661872625351\n",
      "Training loss for batch 2268 : 0.08843338489532471\n",
      "Training loss for batch 2269 : 0.00448945164680481\n",
      "Training loss for batch 2270 : 0.20498239994049072\n",
      "Training loss for batch 2271 : 0.6129145622253418\n",
      "Training loss for batch 2272 : 0.12158993631601334\n",
      "Training loss for batch 2273 : 0.24641311168670654\n",
      "Training loss for batch 2274 : 0.19311590492725372\n",
      "Training loss for batch 2275 : 0.0\n",
      "Training loss for batch 2276 : 0.07320142537355423\n",
      "Training loss for batch 2277 : 0.1423698365688324\n",
      "Training loss for batch 2278 : 0.026344915851950645\n",
      "Training loss for batch 2279 : 0.2467740923166275\n",
      "Training loss for batch 2280 : 0.15688371658325195\n",
      "Training loss for batch 2281 : 0.2074241042137146\n",
      "Training loss for batch 2282 : 0.03419983386993408\n",
      "Training loss for batch 2283 : 0.1224086806178093\n",
      "Training loss for batch 2284 : 0.19784094393253326\n",
      "Training loss for batch 2285 : 0.258515328168869\n",
      "Training loss for batch 2286 : 0.14131705462932587\n",
      "Training loss for batch 2287 : 0.06711526960134506\n",
      "Training loss for batch 2288 : 0.23559792339801788\n",
      "Training loss for batch 2289 : 0.11979562789201736\n",
      "Training loss for batch 2290 : 0.07833680510520935\n",
      "Training loss for batch 2291 : 0.12047477811574936\n",
      "Training loss for batch 2292 : 0.19896547496318817\n",
      "Training loss for batch 2293 : 0.13056017458438873\n",
      "Training loss for batch 2294 : 0.20636428892612457\n",
      "Training loss for batch 2295 : 0.08140406757593155\n",
      "Training loss for batch 2296 : -0.00035926446435041726\n",
      "Training loss for batch 2297 : 0.18897825479507446\n",
      "Training loss for batch 2298 : 0.019803639501333237\n",
      "Training loss for batch 2299 : 0.25777339935302734\n",
      "Training loss for batch 2300 : 0.015708625316619873\n",
      "Training loss for batch 2301 : 0.08552236109972\n",
      "Training loss for batch 2302 : 0.166563481092453\n",
      "Training loss for batch 2303 : 0.25345301628112793\n",
      "Training loss for batch 2304 : 0.34254273772239685\n",
      "Training loss for batch 2305 : 0.17581185698509216\n",
      "Training loss for batch 2306 : 0.12872272729873657\n",
      "Training loss for batch 2307 : 0.8344990015029907\n",
      "Training loss for batch 2308 : 0.17594340443611145\n",
      "Training loss for batch 2309 : 0.13972008228302002\n",
      "Training loss for batch 2310 : 0.20983518660068512\n",
      "Training loss for batch 2311 : 0.09141603112220764\n",
      "Training loss for batch 2312 : 0.008908339776098728\n",
      "Training loss for batch 2313 : 0.039762966334819794\n",
      "Training loss for batch 2314 : 0.2230902910232544\n",
      "Training loss for batch 2315 : 0.18118079006671906\n",
      "Training loss for batch 2316 : 0.31051549315452576\n",
      "Training loss for batch 2317 : 0.21742787957191467\n",
      "Training loss for batch 2318 : 0.337478369474411\n",
      "Training loss for batch 2319 : 0.2866745889186859\n",
      "Training loss for batch 2320 : 0.07673188298940659\n",
      "Training loss for batch 2321 : 0.23463116586208344\n",
      "Training loss for batch 2322 : 0.17016558349132538\n",
      "Training loss for batch 2323 : 0.2165564000606537\n",
      "Training loss for batch 2324 : 0.15169893205165863\n",
      "Training loss for batch 2325 : 0.03079478070139885\n",
      "Training loss for batch 2326 : 0.03311598300933838\n",
      "Training loss for batch 2327 : 0.2478276789188385\n",
      "Training loss for batch 2328 : 0.040128860622644424\n",
      "Training loss for batch 2329 : 0.010678659193217754\n",
      "Training loss for batch 2330 : -0.001422942616045475\n",
      "Training loss for batch 2331 : 0.13976068794727325\n",
      "Training loss for batch 2332 : 0.23639334738254547\n",
      "Training loss for batch 2333 : 0.06398891657590866\n",
      "Training loss for batch 2334 : 0.1288362294435501\n",
      "Training loss for batch 2335 : 0.4356958866119385\n",
      "Training loss for batch 2336 : 0.4994458854198456\n",
      "Training loss for batch 2337 : 0.0504537858068943\n",
      "Training loss for batch 2338 : 0.0\n",
      "Training loss for batch 2339 : 0.1936551034450531\n",
      "Training loss for batch 2340 : 0.12275852262973785\n",
      "Training loss for batch 2341 : 0.005438510328531265\n",
      "Training loss for batch 2342 : 0.028147906064987183\n",
      "Training loss for batch 2343 : 0.14449389278888702\n",
      "Training loss for batch 2344 : 0.034117959439754486\n",
      "Training loss for batch 2345 : 0.18807819485664368\n",
      "Training loss for batch 2346 : 0.05185859650373459\n",
      "Training loss for batch 2347 : 0.06695888191461563\n",
      "Training loss for batch 2348 : 0.2694284915924072\n",
      "Training loss for batch 2349 : 0.25930511951446533\n",
      "Training loss for batch 2350 : 0.36418431997299194\n",
      "Training loss for batch 2351 : 0.3013251721858978\n",
      "Training loss for batch 2352 : 0.43679478764533997\n",
      "Training loss for batch 2353 : 0.2844918668270111\n",
      "Training loss for batch 2354 : 0.32751110196113586\n",
      "Training loss for batch 2355 : 0.015784548595547676\n",
      "Training loss for batch 2356 : -0.000706035119947046\n",
      "Training loss for batch 2357 : 0.11326061934232712\n",
      "Training loss for batch 2358 : 0.1207011491060257\n",
      "Training loss for batch 2359 : 0.13578563928604126\n",
      "Training loss for batch 2360 : 0.13175040483474731\n",
      "Training loss for batch 2361 : 0.3635570704936981\n",
      "Training loss for batch 2362 : 0.08467104285955429\n",
      "Training loss for batch 2363 : 0.0\n",
      "Training loss for batch 2364 : 0.21102462708950043\n",
      "Training loss for batch 2365 : 0.07372816652059555\n",
      "Training loss for batch 2366 : 0.09867561608552933\n",
      "Training loss for batch 2367 : 0.33421677350997925\n",
      "Training loss for batch 2368 : 0.28069669008255005\n",
      "Training loss for batch 2369 : 0.14821265637874603\n",
      "Training loss for batch 2370 : -0.0001657669199630618\n",
      "Training loss for batch 2371 : 0.19509102404117584\n",
      "Training loss for batch 2372 : 0.0\n",
      "Training loss for batch 2373 : 0.00202111410908401\n",
      "Training loss for batch 2374 : 0.05816835165023804\n",
      "Training loss for batch 2375 : 0.21914441883563995\n",
      "Training loss for batch 2376 : 0.33164146542549133\n",
      "Training loss for batch 2377 : 0.19208571314811707\n",
      "Training loss for batch 2378 : 0.08481475710868835\n",
      "Training loss for batch 2379 : 0.06219390034675598\n",
      "Training loss for batch 2380 : 0.04001915082335472\n",
      "Training loss for batch 2381 : 0.10274279862642288\n",
      "Training loss for batch 2382 : 0.18274402618408203\n",
      "Training loss for batch 2383 : 0.11878417432308197\n",
      "Training loss for batch 2384 : 0.010752130299806595\n",
      "Training loss for batch 2385 : 0.0\n",
      "Training loss for batch 2386 : 0.040673255920410156\n",
      "Training loss for batch 2387 : 0.019179876893758774\n",
      "Training loss for batch 2388 : -0.0003447991330176592\n",
      "Training loss for batch 2389 : 0.01848902739584446\n",
      "Training loss for batch 2390 : 0.02837100252509117\n",
      "Training loss for batch 2391 : 0.2824185788631439\n",
      "Training loss for batch 2392 : 0.13416379690170288\n",
      "Training loss for batch 2393 : 0.027283839881420135\n",
      "Training loss for batch 2394 : 0.0563352033495903\n",
      "Training loss for batch 2395 : 0.045178744941949844\n",
      "Training loss for batch 2396 : 0.06651442497968674\n",
      "Training loss for batch 2397 : 0.007727987598627806\n",
      "Training loss for batch 2398 : 0.029671648517251015\n",
      "Training loss for batch 2399 : 0.13603144884109497\n",
      "Training loss for batch 2400 : 0.04442252591252327\n",
      "Training loss for batch 2401 : 0.16631083190441132\n",
      "Training loss for batch 2402 : 0.082864910364151\n",
      "Training loss for batch 2403 : 0.20655463635921478\n",
      "Training loss for batch 2404 : 0.41890421509742737\n",
      "Training loss for batch 2405 : 0.1122114434838295\n",
      "Training loss for batch 2406 : 0.3287215828895569\n",
      "Training loss for batch 2407 : 0.11781208217144012\n",
      "Training loss for batch 2408 : 0.15183009207248688\n",
      "Training loss for batch 2409 : 0.1659892499446869\n",
      "Training loss for batch 2410 : 0.34152764081954956\n",
      "Training loss for batch 2411 : 0.22234512865543365\n",
      "Training loss for batch 2412 : 0.15623591840267181\n",
      "Training loss for batch 2413 : 0.15676529705524445\n",
      "Training loss for batch 2414 : 0.10273286700248718\n",
      "Training loss for batch 2415 : 0.05384211987257004\n",
      "Training loss for batch 2416 : 0.36037811636924744\n",
      "Training loss for batch 2417 : 0.2545399069786072\n",
      "Training loss for batch 2418 : 0.0032904744148254395\n",
      "Training loss for batch 2419 : 0.3370089828968048\n",
      "Training loss for batch 2420 : 0.12152130901813507\n",
      "Training loss for batch 2421 : 0.4443504512310028\n",
      "Training loss for batch 2422 : 0.3705577254295349\n",
      "Training loss for batch 2423 : 0.06012987717986107\n",
      "Training loss for batch 2424 : 0.0\n",
      "Training loss for batch 2425 : 0.1718824803829193\n",
      "Training loss for batch 2426 : 0.13911433517932892\n",
      "Training loss for batch 2427 : 0.003259582445025444\n",
      "Training loss for batch 2428 : 0.061656199395656586\n",
      "Training loss for batch 2429 : 0.06467808783054352\n",
      "Training loss for batch 2430 : 0.008092537522315979\n",
      "Training loss for batch 2431 : 0.09497599303722382\n",
      "Training loss for batch 2432 : 0.2371346652507782\n",
      "Training loss for batch 2433 : 0.0028499565087258816\n",
      "Training loss for batch 2434 : 0.11580517143011093\n",
      "Training loss for batch 2435 : 0.07339205592870712\n",
      "Training loss for batch 2436 : 0.2808506190776825\n",
      "Training loss for batch 2437 : 0.027681611478328705\n",
      "Training loss for batch 2438 : 0.18181277811527252\n",
      "Training loss for batch 2439 : 0.2688867449760437\n",
      "Training loss for batch 2440 : 0.0980285257101059\n",
      "Training loss for batch 2441 : 0.2507611811161041\n",
      "Training loss for batch 2442 : 0.024282583966851234\n",
      "Training loss for batch 2443 : 0.11724527180194855\n",
      "Training loss for batch 2444 : 0.07629569619894028\n",
      "Training loss for batch 2445 : 0.0035438400227576494\n",
      "Training loss for batch 2446 : 0.0032288632355630398\n",
      "Training loss for batch 2447 : 0.06779446452856064\n",
      "Training loss for batch 2448 : 0.08147037774324417\n",
      "Training loss for batch 2449 : 0.10934057831764221\n",
      "Training loss for batch 2450 : 0.41133248805999756\n",
      "Training loss for batch 2451 : 0.03345264494419098\n",
      "Training loss for batch 2452 : 0.18496261537075043\n",
      "Training loss for batch 2453 : 0.10854282230138779\n",
      "Training loss for batch 2454 : 0.006491822190582752\n",
      "Training loss for batch 2455 : 0.2398265302181244\n",
      "Training loss for batch 2456 : 0.40523409843444824\n",
      "Training loss for batch 2457 : 0.015050376765429974\n",
      "Training loss for batch 2458 : 0.3020012080669403\n",
      "Training loss for batch 2459 : 0.08453918248414993\n",
      "Training loss for batch 2460 : 0.3854752779006958\n",
      "Training loss for batch 2461 : 0.163751482963562\n",
      "Training loss for batch 2462 : 0.29229000210762024\n",
      "Training loss for batch 2463 : 0.0689966008067131\n",
      "Training loss for batch 2464 : 0.05749828740954399\n",
      "Training loss for batch 2465 : 0.06789067387580872\n",
      "Training loss for batch 2466 : 0.022778842598199844\n",
      "Training loss for batch 2467 : 0.6388606429100037\n",
      "Training loss for batch 2468 : 0.023289920762181282\n",
      "Training loss for batch 2469 : 0.24513043463230133\n",
      "Training loss for batch 2470 : 0.20093972980976105\n",
      "Training loss for batch 2471 : 0.2644224762916565\n",
      "Training loss for batch 2472 : 0.04941936954855919\n",
      "Training loss for batch 2473 : 0.27691540122032166\n",
      "Training loss for batch 2474 : 0.18826255202293396\n",
      "Training loss for batch 2475 : 0.15636184811592102\n",
      "Training loss for batch 2476 : 0.27752190828323364\n",
      "Training loss for batch 2477 : 0.21176332235336304\n",
      "Training loss for batch 2478 : 0.25996318459510803\n",
      "Training loss for batch 2479 : 0.05960884690284729\n",
      "Training loss for batch 2480 : 0.03871927782893181\n",
      "Training loss for batch 2481 : 0.03351283073425293\n",
      "Training loss for batch 2482 : 0.1356489360332489\n",
      "Training loss for batch 2483 : 0.20277740061283112\n",
      "Training loss for batch 2484 : 0.2705897092819214\n",
      "Training loss for batch 2485 : 0.04853418469429016\n",
      "Training loss for batch 2486 : 0.23451323807239532\n",
      "Training loss for batch 2487 : 0.31517234444618225\n",
      "Training loss for batch 2488 : 0.18925021588802338\n",
      "Training loss for batch 2489 : 0.09415630251169205\n",
      "Training loss for batch 2490 : 0.515778124332428\n",
      "Training loss for batch 2491 : 0.23910126090049744\n",
      "Training loss for batch 2492 : 0.002369737485423684\n",
      "Training loss for batch 2493 : 0.058798354119062424\n",
      "Training loss for batch 2494 : 0.018549630418419838\n",
      "Training loss for batch 2495 : 0.1775151789188385\n",
      "Training loss for batch 2496 : 0.2557675242424011\n",
      "Training loss for batch 2497 : 0.35282620787620544\n",
      "Training loss for batch 2498 : 0.23840174078941345\n",
      "Training loss for batch 2499 : 0.13797375559806824\n",
      "Training loss for batch 2500 : 0.09375545382499695\n",
      "Training loss for batch 2501 : 0.26888608932495117\n",
      "Training loss for batch 2502 : 0.20538446307182312\n",
      "Training loss for batch 2503 : 0.32287847995758057\n",
      "Training loss for batch 2504 : 0.22144722938537598\n",
      "Training loss for batch 2505 : 0.2640204429626465\n",
      "Training loss for batch 2506 : 0.408594012260437\n",
      "Training loss for batch 2507 : 0.07593975961208344\n",
      "Training loss for batch 2508 : 0.25847455859184265\n",
      "Training loss for batch 2509 : 0.21361291408538818\n",
      "Training loss for batch 2510 : 0.24616926908493042\n",
      "Training loss for batch 2511 : 0.08761341124773026\n",
      "Training loss for batch 2512 : 0.030185779556632042\n",
      "Training loss for batch 2513 : 0.4029061198234558\n",
      "Training loss for batch 2514 : 0.12791085243225098\n",
      "Training loss for batch 2515 : 0.2019871175289154\n",
      "Training loss for batch 2516 : 0.3633536994457245\n",
      "Training loss for batch 2517 : 0.18708516657352448\n",
      "Training loss for batch 2518 : 0.03887330740690231\n",
      "Training loss for batch 2519 : 0.18256740272045135\n",
      "Training loss for batch 2520 : 0.014929573982954025\n",
      "Training loss for batch 2521 : 0.1655133068561554\n",
      "Training loss for batch 2522 : 0.09090390801429749\n",
      "Training loss for batch 2523 : 0.22234289348125458\n",
      "Training loss for batch 2524 : 0.33989784121513367\n",
      "Training loss for batch 2525 : 0.17813768982887268\n",
      "Training loss for batch 2526 : 0.12725010514259338\n",
      "Training loss for batch 2527 : 0.3694343864917755\n",
      "Training loss for batch 2528 : 0.05705622211098671\n",
      "Training loss for batch 2529 : 0.044857122004032135\n",
      "Training loss for batch 2530 : 0.3636127710342407\n",
      "Training loss for batch 2531 : 0.13397280871868134\n",
      "Training loss for batch 2532 : 0.48376786708831787\n",
      "Training loss for batch 2533 : 0.02199474722146988\n",
      "Training loss for batch 2534 : 0.02980363368988037\n",
      "Training loss for batch 2535 : 0.0\n",
      "Training loss for batch 2536 : 0.07935923337936401\n",
      "Training loss for batch 2537 : 0.016028907150030136\n",
      "Training loss for batch 2538 : 0.12452492117881775\n",
      "Training loss for batch 2539 : 0.3365609645843506\n",
      "Training loss for batch 2540 : 0.14163488149642944\n",
      "Training loss for batch 2541 : 0.10343372076749802\n",
      "Training loss for batch 2542 : 0.19329926371574402\n",
      "Training loss for batch 2543 : 0.03434431925415993\n",
      "Training loss for batch 2544 : 0.1538221836090088\n",
      "Training loss for batch 2545 : 0.2752314805984497\n",
      "Training loss for batch 2546 : 0.08111993968486786\n",
      "Training loss for batch 2547 : 0.14275802671909332\n",
      "Training loss for batch 2548 : 0.01266762986779213\n",
      "Training loss for batch 2549 : 0.1203455999493599\n",
      "Training loss for batch 2550 : 0.21191269159317017\n",
      "Training loss for batch 2551 : 0.13748733699321747\n",
      "Training loss for batch 2552 : 0.14993134140968323\n",
      "Training loss for batch 2553 : 0.1738034337759018\n",
      "Training loss for batch 2554 : 0.049913279712200165\n",
      "Training loss for batch 2555 : 0.39799049496650696\n",
      "Training loss for batch 2556 : 0.26621079444885254\n",
      "Training loss for batch 2557 : 0.11827608942985535\n",
      "Training loss for batch 2558 : 0.10199958086013794\n",
      "Training loss for batch 2559 : 0.1726764440536499\n",
      "Training loss for batch 2560 : 0.10484768450260162\n",
      "Training loss for batch 2561 : 0.07595060765743256\n",
      "Training loss for batch 2562 : 0.10218650102615356\n",
      "Training loss for batch 2563 : 0.0\n",
      "Training loss for batch 2564 : 0.021896295249462128\n",
      "Training loss for batch 2565 : 0.15559691190719604\n",
      "Training loss for batch 2566 : 0.09654341638088226\n",
      "Training loss for batch 2567 : 0.08223079890012741\n",
      "Training loss for batch 2568 : 0.08107002079486847\n",
      "Training loss for batch 2569 : 0.24488157033920288\n",
      "Training loss for batch 2570 : -0.0005929232574999332\n",
      "Training loss for batch 2571 : 0.24276357889175415\n",
      "Training loss for batch 2572 : 0.1316114217042923\n",
      "Training loss for batch 2573 : 0.14672373235225677\n",
      "Training loss for batch 2574 : 0.10552605986595154\n",
      "Training loss for batch 2575 : 0.16054168343544006\n",
      "Training loss for batch 2576 : 0.09788791835308075\n",
      "Training loss for batch 2577 : 0.29353347420692444\n",
      "Training loss for batch 2578 : 0.036702319979667664\n",
      "Training loss for batch 2579 : 0.06231693550944328\n",
      "Training loss for batch 2580 : 0.08810847997665405\n",
      "Training loss for batch 2581 : 0.012515058740973473\n",
      "Training loss for batch 2582 : 0.10435106605291367\n",
      "Training loss for batch 2583 : 0.3178589940071106\n",
      "Training loss for batch 2584 : 0.08420447260141373\n",
      "Training loss for batch 2585 : 0.07993579655885696\n",
      "Training loss for batch 2586 : 0.23921367526054382\n",
      "Training loss for batch 2587 : 0.08080825954675674\n",
      "Training loss for batch 2588 : 0.2206708788871765\n",
      "Training loss for batch 2589 : 0.37496858835220337\n",
      "Training loss for batch 2590 : 0.24866530299186707\n",
      "Training loss for batch 2591 : 0.23272286355495453\n",
      "Training loss for batch 2592 : 0.07946587353944778\n",
      "Training loss for batch 2593 : 0.05982089415192604\n",
      "Training loss for batch 2594 : 0.045957352966070175\n",
      "Training loss for batch 2595 : 0.5630735754966736\n",
      "Training loss for batch 2596 : 0.2579572796821594\n",
      "Training loss for batch 2597 : 0.3681315779685974\n",
      "Training loss for batch 2598 : 0.06657388806343079\n",
      "Training loss for batch 2599 : 0.2900950610637665\n",
      "Training loss for batch 2600 : 0.08220899105072021\n",
      "Training loss for batch 2601 : 0.4009413421154022\n",
      "Training loss for batch 2602 : 0.12894871830940247\n",
      "Training loss for batch 2603 : 0.03362270072102547\n",
      "Training loss for batch 2604 : 0.21161651611328125\n",
      "Training loss for batch 2605 : 0.07211152464151382\n",
      "Training loss for batch 2606 : 0.08578287065029144\n",
      "Training loss for batch 2607 : 0.12815739214420319\n",
      "Training loss for batch 2608 : 0.08029604703187943\n",
      "Training loss for batch 2609 : 0.15126869082450867\n",
      "Training loss for batch 2610 : 0.10125711560249329\n",
      "Training loss for batch 2611 : 0.09679695218801498\n",
      "Training loss for batch 2612 : 0.3258904218673706\n",
      "Training loss for batch 2613 : 0.19943350553512573\n",
      "Training loss for batch 2614 : 0.15868648886680603\n",
      "Training loss for batch 2615 : 0.09534918516874313\n",
      "Training loss for batch 2616 : 0.322735458612442\n",
      "Training loss for batch 2617 : 0.11134196072816849\n",
      "Training loss for batch 2618 : 0.12899211049079895\n",
      "Training loss for batch 2619 : 0.14336977899074554\n",
      "Training loss for batch 2620 : 0.13217973709106445\n",
      "Training loss for batch 2621 : 0.14731433987617493\n",
      "Training loss for batch 2622 : 0.5530775189399719\n",
      "Training loss for batch 2623 : 0.13028985261917114\n",
      "Training loss for batch 2624 : 0.17058168351650238\n",
      "Training loss for batch 2625 : 0.017572680488228798\n",
      "Training loss for batch 2626 : 0.007422993890941143\n",
      "Training loss for batch 2627 : 0.3501661717891693\n",
      "Training loss for batch 2628 : 0.30261749029159546\n",
      "Training loss for batch 2629 : 0.04665336012840271\n",
      "Training loss for batch 2630 : 0.04198320955038071\n",
      "Training loss for batch 2631 : 0.037126362323760986\n",
      "Training loss for batch 2632 : 0.14090070128440857\n",
      "Training loss for batch 2633 : 0.06086067110300064\n",
      "Training loss for batch 2634 : 0.1050441637635231\n",
      "Training loss for batch 2635 : 0.02540670335292816\n",
      "Training loss for batch 2636 : 0.2542707026004791\n",
      "Training loss for batch 2637 : 0.25122010707855225\n",
      "Training loss for batch 2638 : 0.041651081293821335\n",
      "Training loss for batch 2639 : 0.2201569825410843\n",
      "Training loss for batch 2640 : 0.4988190829753876\n",
      "Training loss for batch 2641 : 0.2777329087257385\n",
      "Training loss for batch 2642 : 0.08898168802261353\n",
      "Training loss for batch 2643 : 0.13659892976284027\n",
      "Training loss for batch 2644 : 0.366387277841568\n",
      "Training loss for batch 2645 : 0.2600431740283966\n",
      "Training loss for batch 2646 : 0.3244456648826599\n",
      "Training loss for batch 2647 : 0.1251162588596344\n",
      "Training loss for batch 2648 : 0.03245559334754944\n",
      "Training loss for batch 2649 : 0.24794118106365204\n",
      "Training loss for batch 2650 : 0.2776929438114166\n",
      "Training loss for batch 2651 : 0.21306566894054413\n",
      "Training loss for batch 2652 : 0.008186368271708488\n",
      "Training loss for batch 2653 : 0.15672022104263306\n",
      "Training loss for batch 2654 : 0.12169661372900009\n",
      "Training loss for batch 2655 : 0.3575872778892517\n",
      "Training loss for batch 2656 : 0.2340964525938034\n",
      "Training loss for batch 2657 : 0.21686089038848877\n",
      "Training loss for batch 2658 : 0.15124741196632385\n",
      "Training loss for batch 2659 : 0.23380957543849945\n",
      "Training loss for batch 2660 : 0.17261461913585663\n",
      "Training loss for batch 2661 : 0.10959401726722717\n",
      "Training loss for batch 2662 : 0.196352019906044\n",
      "Training loss for batch 2663 : 0.18745890259742737\n",
      "Training loss for batch 2664 : 0.013729915022850037\n",
      "Training loss for batch 2665 : 0.25990167260169983\n",
      "Training loss for batch 2666 : 0.21924841403961182\n",
      "Training loss for batch 2667 : 0.09980735927820206\n",
      "Training loss for batch 2668 : 0.17201045155525208\n",
      "Training loss for batch 2669 : 0.2713920772075653\n",
      "Training loss for batch 2670 : 0.010552644729614258\n",
      "Training loss for batch 2671 : 0.22606772184371948\n",
      "Training loss for batch 2672 : 0.01203059870749712\n",
      "Training loss for batch 2673 : 0.2035984992980957\n",
      "Training loss for batch 2674 : 0.235747829079628\n",
      "Training loss for batch 2675 : 0.04435526952147484\n",
      "Training loss for batch 2676 : 0.1550406664609909\n",
      "Training loss for batch 2677 : 0.029042305424809456\n",
      "Training loss for batch 2678 : 0.09104691445827484\n",
      "Training loss for batch 2679 : 0.32458776235580444\n",
      "Training loss for batch 2680 : 0.19694359600543976\n",
      "Training loss for batch 2681 : 0.0707726776599884\n",
      "Training loss for batch 2682 : 0.043925944715738297\n",
      "Training loss for batch 2683 : 0.20823805034160614\n",
      "Training loss for batch 2684 : 0.2653578519821167\n",
      "Training loss for batch 2685 : 0.09046842902898788\n",
      "Training loss for batch 2686 : 0.22035609185695648\n",
      "Training loss for batch 2687 : 0.0663386657834053\n",
      "Training loss for batch 2688 : 0.22395318746566772\n",
      "Training loss for batch 2689 : 0.08343269675970078\n",
      "Training loss for batch 2690 : 0.40659189224243164\n",
      "Training loss for batch 2691 : 0.2430049329996109\n",
      "Training loss for batch 2692 : 0.1419776827096939\n",
      "Training loss for batch 2693 : 0.3280642032623291\n",
      "Training loss for batch 2694 : 0.13285882771015167\n",
      "Training loss for batch 2695 : 0.0783204510807991\n",
      "Training loss for batch 2696 : 0.21518246829509735\n",
      "Training loss for batch 2697 : 0.37787944078445435\n",
      "Training loss for batch 2698 : 0.17300885915756226\n",
      "Training loss for batch 2699 : 0.0768958330154419\n",
      "Training loss for batch 2700 : 0.003694097278639674\n",
      "Training loss for batch 2701 : 0.3071400225162506\n",
      "Training loss for batch 2702 : 0.14500796794891357\n",
      "Training loss for batch 2703 : 0.26471877098083496\n",
      "Training loss for batch 2704 : 0.3208748400211334\n",
      "Training loss for batch 2705 : 0.042858753353357315\n",
      "Training loss for batch 2706 : 0.052200958132743835\n",
      "Training loss for batch 2707 : 0.12635201215744019\n",
      "Training loss for batch 2708 : 0.13594098389148712\n",
      "Training loss for batch 2709 : 0.0\n",
      "Training loss for batch 2710 : 0.20261049270629883\n",
      "Training loss for batch 2711 : 0.07800408452749252\n",
      "Training loss for batch 2712 : 0.11469125747680664\n",
      "Training loss for batch 2713 : 0.2167746126651764\n",
      "Training loss for batch 2714 : 0.27904507517814636\n",
      "Training loss for batch 2715 : 0.040612466633319855\n",
      "Training loss for batch 2716 : 0.29599663615226746\n",
      "Training loss for batch 2717 : 0.12520046532154083\n",
      "Training loss for batch 2718 : 0.053546831011772156\n",
      "Training loss for batch 2719 : 0.3083743155002594\n",
      "Training loss for batch 2720 : 0.08254271745681763\n",
      "Training loss for batch 2721 : 0.02812129259109497\n",
      "Training loss for batch 2722 : 0.08982338756322861\n",
      "Training loss for batch 2723 : 0.15193131566047668\n",
      "Training loss for batch 2724 : 0.18094325065612793\n",
      "Training loss for batch 2725 : 0.17240509390830994\n",
      "Training loss for batch 2726 : 0.19566592574119568\n",
      "Training loss for batch 2727 : 0.17326819896697998\n",
      "Training loss for batch 2728 : 0.024613726884126663\n",
      "Training loss for batch 2729 : 0.20722101628780365\n",
      "Training loss for batch 2730 : 0.1732098013162613\n",
      "Training loss for batch 2731 : 0.01471717655658722\n",
      "Training loss for batch 2732 : 0.09235554188489914\n",
      "Training loss for batch 2733 : 0.07181524485349655\n",
      "Training loss for batch 2734 : 0.2897035777568817\n",
      "Training loss for batch 2735 : 0.2080642282962799\n",
      "Training loss for batch 2736 : 0.14266139268875122\n",
      "Training loss for batch 2737 : 0.09291613847017288\n",
      "Training loss for batch 2738 : 0.05289091542363167\n",
      "Training loss for batch 2739 : 0.09265926480293274\n",
      "Training loss for batch 2740 : 0.08084103465080261\n",
      "Training loss for batch 2741 : 0.09837457537651062\n",
      "Training loss for batch 2742 : 0.2610728442668915\n",
      "Training loss for batch 2743 : 0.477450430393219\n",
      "Training loss for batch 2744 : 0.28432542085647583\n",
      "Training loss for batch 2745 : 0.008332699537277222\n",
      "Training loss for batch 2746 : 0.06810019910335541\n",
      "Training loss for batch 2747 : 0.1873551905155182\n",
      "Training loss for batch 2748 : 0.33767426013946533\n",
      "Training loss for batch 2749 : 0.059214044362306595\n",
      "Training loss for batch 2750 : 0.2159786820411682\n",
      "Training loss for batch 2751 : 0.09457468241453171\n",
      "Training loss for batch 2752 : 0.13508456945419312\n",
      "Training loss for batch 2753 : 0.13891616463661194\n",
      "Training loss for batch 2754 : 0.21919482946395874\n",
      "Training loss for batch 2755 : 0.16364997625350952\n",
      "Training loss for batch 2756 : 0.015496592037379742\n",
      "Training loss for batch 2757 : 0.12147027254104614\n",
      "Training loss for batch 2758 : 0.2701719403266907\n",
      "Training loss for batch 2759 : 0.08186038583517075\n",
      "Training loss for batch 2760 : 0.21725444495677948\n",
      "Training loss for batch 2761 : 0.17310985922813416\n",
      "Training loss for batch 2762 : 0.1155330091714859\n",
      "Training loss for batch 2763 : 0.0412079319357872\n",
      "Training loss for batch 2764 : 0.36736419796943665\n",
      "Training loss for batch 2765 : 0.043015457689762115\n",
      "Training loss for batch 2766 : 0.23780176043510437\n",
      "Training loss for batch 2767 : 0.1695648580789566\n",
      "Training loss for batch 2768 : 0.5305846333503723\n",
      "Training loss for batch 2769 : 0.30015265941619873\n",
      "Training loss for batch 2770 : 0.15016354620456696\n",
      "Training loss for batch 2771 : 0.0936274304986\n",
      "Training loss for batch 2772 : 0.13313043117523193\n",
      "Training loss for batch 2773 : 0.09290413558483124\n",
      "Training loss for batch 2774 : 0.06327550113201141\n",
      "Training loss for batch 2775 : 0.08316443860530853\n",
      "Training loss for batch 2776 : 0.2548777163028717\n",
      "Training loss for batch 2777 : 0.02507976070046425\n",
      "Training loss for batch 2778 : 0.23721632361412048\n",
      "Training loss for batch 2779 : 0.3674401640892029\n",
      "Training loss for batch 2780 : 0.010109780356287956\n",
      "Training loss for batch 2781 : 0.070783331990242\n",
      "Training loss for batch 2782 : 0.147067129611969\n",
      "Training loss for batch 2783 : 0.30761927366256714\n",
      "Training loss for batch 2784 : 0.056993499398231506\n",
      "Training loss for batch 2785 : 0.1507801115512848\n",
      "Training loss for batch 2786 : 0.2401134967803955\n",
      "Training loss for batch 2787 : 0.1452481597661972\n",
      "Training loss for batch 2788 : 0.1141795739531517\n",
      "Training loss for batch 2789 : 0.012874647974967957\n",
      "Training loss for batch 2790 : 0.1394544541835785\n",
      "Training loss for batch 2791 : 0.4002346694469452\n",
      "Training loss for batch 2792 : 0.34536483883857727\n",
      "Training loss for batch 2793 : 0.06716965138912201\n",
      "Training loss for batch 2794 : 0.007543332874774933\n",
      "Training loss for batch 2795 : 0.04496106505393982\n",
      "Training loss for batch 2796 : 0.1714855134487152\n",
      "Training loss for batch 2797 : 0.23470652103424072\n",
      "Training loss for batch 2798 : 0.16089046001434326\n",
      "Training loss for batch 2799 : 0.32418888807296753\n",
      "Training loss for batch 2800 : 0.34215474128723145\n",
      "Training loss for batch 2801 : 0.0649094507098198\n",
      "Training loss for batch 2802 : 0.0868174284696579\n",
      "Training loss for batch 2803 : 0.15420465171337128\n",
      "Training loss for batch 2804 : 0.2072451114654541\n",
      "Training loss for batch 2805 : 0.11475729197263718\n",
      "Training loss for batch 2806 : 0.38344070315361023\n",
      "Training loss for batch 2807 : 0.3028195798397064\n",
      "Training loss for batch 2808 : 0.29171982407569885\n",
      "Training loss for batch 2809 : 0.03842117637395859\n",
      "Training loss for batch 2810 : 0.3262439966201782\n",
      "Training loss for batch 2811 : 0.25183767080307007\n",
      "Training loss for batch 2812 : 0.3271733224391937\n",
      "Training loss for batch 2813 : 0.20549829304218292\n",
      "Training loss for batch 2814 : 0.019055429846048355\n",
      "Training loss for batch 2815 : 0.2693788409233093\n",
      "Training loss for batch 2816 : 0.10964453220367432\n",
      "Training loss for batch 2817 : 0.25991079211235046\n",
      "Training loss for batch 2818 : 0.21744385361671448\n",
      "Training loss for batch 2819 : 0.10746642202138901\n",
      "Training loss for batch 2820 : 0.31774336099624634\n",
      "Training loss for batch 2821 : 0.12761065363883972\n",
      "Training loss for batch 2822 : 0.12649820744991302\n",
      "Training loss for batch 2823 : 0.17328912019729614\n",
      "Training loss for batch 2824 : 0.125186026096344\n",
      "Training loss for batch 2825 : 0.08928677439689636\n",
      "Training loss for batch 2826 : 0.3697567880153656\n",
      "Training loss for batch 2827 : 0.31230244040489197\n",
      "Training loss for batch 2828 : 0.43285971879959106\n",
      "Training loss for batch 2829 : 0.24983862042427063\n",
      "Training loss for batch 2830 : 0.05469387769699097\n",
      "Training loss for batch 2831 : 0.01617007702589035\n",
      "Training loss for batch 2832 : 0.3647584319114685\n",
      "Training loss for batch 2833 : 0.07543865591287613\n",
      "Training loss for batch 2834 : 0.17774730920791626\n",
      "Training loss for batch 2835 : 0.38667458295822144\n",
      "Training loss for batch 2836 : 0.22114711999893188\n",
      "Training loss for batch 2837 : 0.25609394907951355\n",
      "Training loss for batch 2838 : 0.3109164535999298\n",
      "Training loss for batch 2839 : 0.056277476251125336\n",
      "Training loss for batch 2840 : 0.036670926958322525\n",
      "Training loss for batch 2841 : 0.0886913612484932\n",
      "Training loss for batch 2842 : 0.13949382305145264\n",
      "Training loss for batch 2843 : 0.12770290672779083\n",
      "Training loss for batch 2844 : 0.12745505571365356\n",
      "Training loss for batch 2845 : 0.19262433052062988\n",
      "Training loss for batch 2846 : 0.08581404387950897\n",
      "Training loss for batch 2847 : 0.08965800702571869\n",
      "Training loss for batch 2848 : 0.12387821823358536\n",
      "Training loss for batch 2849 : 0.0325019396841526\n",
      "Training loss for batch 2850 : 0.15146967768669128\n",
      "Training loss for batch 2851 : 0.30697229504585266\n",
      "Training loss for batch 2852 : -0.001379084074869752\n",
      "Training loss for batch 2853 : 0.0\n",
      "Training loss for batch 2854 : -0.0009430694044567645\n",
      "Training loss for batch 2855 : 0.0\n",
      "Training loss for batch 2856 : 0.04600098729133606\n",
      "Training loss for batch 2857 : 0.3711600601673126\n",
      "Training loss for batch 2858 : 0.3412102460861206\n",
      "Training loss for batch 2859 : 0.3141605257987976\n",
      "Training loss for batch 2860 : 0.04312568157911301\n",
      "Training loss for batch 2861 : 0.22578315436840057\n",
      "Training loss for batch 2862 : 0.17951327562332153\n",
      "Training loss for batch 2863 : 0.16675733029842377\n",
      "Training loss for batch 2864 : 0.0761677622795105\n",
      "Training loss for batch 2865 : 0.12457583844661713\n",
      "Training loss for batch 2866 : 0.0337003618478775\n",
      "Training loss for batch 2867 : 0.09780213236808777\n",
      "Training loss for batch 2868 : 0.0\n",
      "Training loss for batch 2869 : 0.06485984474420547\n",
      "Training loss for batch 2870 : 0.1793842315673828\n",
      "Training loss for batch 2871 : 0.4944526255130768\n",
      "Training loss for batch 2872 : 0.19480255246162415\n",
      "Training loss for batch 2873 : 0.30198419094085693\n",
      "Training loss for batch 2874 : 0.027110092341899872\n",
      "Training loss for batch 2875 : 0.15806728601455688\n",
      "Training loss for batch 2876 : 0.24095475673675537\n",
      "Training loss for batch 2877 : 0.05482816696166992\n",
      "Training loss for batch 2878 : 0.16773846745491028\n",
      "Training loss for batch 2879 : 0.11480478942394257\n",
      "Training loss for batch 2880 : 0.34891802072525024\n",
      "Training loss for batch 2881 : 0.3068832755088806\n",
      "Training loss for batch 2882 : 0.03199157863855362\n",
      "Training loss for batch 2883 : 0.08879229426383972\n",
      "Training loss for batch 2884 : -0.001968440366908908\n",
      "Training loss for batch 2885 : 0.3795500695705414\n",
      "Training loss for batch 2886 : 0.5694499611854553\n",
      "Training loss for batch 2887 : 0.0944293737411499\n",
      "Training loss for batch 2888 : 0.12206997722387314\n",
      "Training loss for batch 2889 : 0.17301149666309357\n",
      "Training loss for batch 2890 : 0.10691393911838531\n",
      "Training loss for batch 2891 : 0.20682433247566223\n",
      "Training loss for batch 2892 : 0.059358686208724976\n",
      "Training loss for batch 2893 : 0.042637597769498825\n",
      "Training loss for batch 2894 : 0.04205571487545967\n",
      "Training loss for batch 2895 : 0.04449176415801048\n",
      "Training loss for batch 2896 : 0.06838870048522949\n",
      "Training loss for batch 2897 : 0.0019319653511047363\n",
      "Training loss for batch 2898 : 0.09857623279094696\n",
      "Training loss for batch 2899 : 0.1542617529630661\n",
      "Training loss for batch 2900 : 0.37185138463974\n",
      "Training loss for batch 2901 : 0.11773953586816788\n",
      "Training loss for batch 2902 : 0.3310231566429138\n",
      "Training loss for batch 2903 : 0.210264191031456\n",
      "Training loss for batch 2904 : 0.07640432566404343\n",
      "Training loss for batch 2905 : 0.1901804506778717\n",
      "Training loss for batch 2906 : 0.11279425024986267\n",
      "Training loss for batch 2907 : 0.15053242444992065\n",
      "Training loss for batch 2908 : 0.00398494815453887\n",
      "Training loss for batch 2909 : 0.34083691239356995\n",
      "Training loss for batch 2910 : 0.06524726748466492\n",
      "Training loss for batch 2911 : 0.0002316733298357576\n",
      "Training loss for batch 2912 : 0.19423744082450867\n",
      "Training loss for batch 2913 : 0.029065720736980438\n",
      "Training loss for batch 2914 : 0.20896276831626892\n",
      "Training loss for batch 2915 : 0.14027546346187592\n",
      "Training loss for batch 2916 : 0.10268228501081467\n",
      "Training loss for batch 2917 : 0.17889247834682465\n",
      "Training loss for batch 2918 : 0.008192996494472027\n",
      "Training loss for batch 2919 : 0.2050921469926834\n",
      "Training loss for batch 2920 : 0.054479122161865234\n",
      "Training loss for batch 2921 : 0.11927357316017151\n",
      "Training loss for batch 2922 : 0.007577226497232914\n",
      "Training loss for batch 2923 : 0.1643759161233902\n",
      "Training loss for batch 2924 : 0.42250126600265503\n",
      "Training loss for batch 2925 : 0.22785337269306183\n",
      "Training loss for batch 2926 : 0.05633437633514404\n",
      "Training loss for batch 2927 : 0.14022088050842285\n",
      "Training loss for batch 2928 : 0.2240518182516098\n",
      "Training loss for batch 2929 : 0.06561191380023956\n",
      "Training loss for batch 2930 : 0.025419926270842552\n",
      "Training loss for batch 2931 : 0.2814221978187561\n",
      "Training loss for batch 2932 : 0.04424935206770897\n",
      "Training loss for batch 2933 : 0.0030058634001761675\n",
      "Training loss for batch 2934 : 0.107188880443573\n",
      "Training loss for batch 2935 : 0.1685950756072998\n",
      "Training loss for batch 2936 : 0.38191139698028564\n",
      "Training loss for batch 2937 : 0.072544164955616\n",
      "Training loss for batch 2938 : 0.0009176830644719303\n",
      "Training loss for batch 2939 : 0.15676423907279968\n",
      "Training loss for batch 2940 : 0.029635829851031303\n",
      "Training loss for batch 2941 : 0.1296018362045288\n",
      "Training loss for batch 2942 : 0.20050954818725586\n",
      "Training loss for batch 2943 : 0.260612428188324\n",
      "Training loss for batch 2944 : 0.07611925154924393\n",
      "Training loss for batch 2945 : 0.2255488485097885\n",
      "Training loss for batch 2946 : 0.16519196331501007\n",
      "Training loss for batch 2947 : 0.11280092597007751\n",
      "Training loss for batch 2948 : 0.08217233419418335\n",
      "Training loss for batch 2949 : 0.0943329706788063\n",
      "Training loss for batch 2950 : 0.053532492369413376\n",
      "Training loss for batch 2951 : 0.18673071265220642\n",
      "Training loss for batch 2952 : 0.011398698203265667\n",
      "Training loss for batch 2953 : 0.1424935758113861\n",
      "Training loss for batch 2954 : 0.4184544086456299\n",
      "Training loss for batch 2955 : 0.09670144319534302\n",
      "Training loss for batch 2956 : 0.01680406741797924\n",
      "Training loss for batch 2957 : 0.26715874671936035\n",
      "Training loss for batch 2958 : 0.12939025461673737\n",
      "Training loss for batch 2959 : 0.19354002177715302\n",
      "Training loss for batch 2960 : 0.2097276747226715\n",
      "Training loss for batch 2961 : 0.013948151841759682\n",
      "Training loss for batch 2962 : 0.09847623109817505\n",
      "Training loss for batch 2963 : 0.4496231973171234\n",
      "Training loss for batch 2964 : 0.11553795635700226\n",
      "Training loss for batch 2965 : 0.7593511939048767\n",
      "Training loss for batch 2966 : 0.24551594257354736\n",
      "Training loss for batch 2967 : 0.30545759201049805\n",
      "Training loss for batch 2968 : 0.02377934381365776\n",
      "Training loss for batch 2969 : 0.023564111441373825\n",
      "Training loss for batch 2970 : 0.3429076373577118\n",
      "Training loss for batch 2971 : 0.37125468254089355\n",
      "Training loss for batch 2972 : 0.14838016033172607\n",
      "Training loss for batch 2973 : 0.06674492359161377\n",
      "Training loss for batch 2974 : 0.025890842080116272\n",
      "Training loss for batch 2975 : 0.15460218489170074\n",
      "Training loss for batch 2976 : 0.020310185849666595\n",
      "Training loss for batch 2977 : 0.46280306577682495\n",
      "Training loss for batch 2978 : 0.2689994275569916\n",
      "Training loss for batch 2979 : 0.07047555595636368\n",
      "Training loss for batch 2980 : 0.13794521987438202\n",
      "Training loss for batch 2981 : 0.148507222533226\n",
      "Training loss for batch 2982 : 0.2907962203025818\n",
      "Training loss for batch 2983 : 0.07949300855398178\n",
      "Training loss for batch 2984 : 0.03941810131072998\n",
      "Training loss for batch 2985 : 0.16760455071926117\n",
      "Training loss for batch 2986 : 0.3216436803340912\n",
      "Training loss for batch 2987 : 0.06931790709495544\n",
      "Training loss for batch 2988 : 0.3254091739654541\n",
      "Training loss for batch 2989 : 0.07491624355316162\n",
      "Training loss for batch 2990 : 0.10904643684625626\n",
      "Training loss for batch 2991 : 0.6816525459289551\n",
      "Training loss for batch 2992 : 0.33587127923965454\n",
      "Training loss for batch 2993 : 0.4428589642047882\n",
      "Training loss for batch 2994 : 0.001124948263168335\n",
      "Training loss for batch 2995 : 0.13729608058929443\n",
      "Training loss for batch 2996 : 0.298115074634552\n",
      "Training loss for batch 2997 : 0.0867857038974762\n",
      "Training loss for batch 2998 : 0.05979648232460022\n",
      "Training loss for batch 2999 : 0.26481351256370544\n",
      "Training loss for batch 3000 : 0.040922850370407104\n",
      "Training loss for batch 3001 : 0.18661299347877502\n",
      "Training loss for batch 3002 : 0.014105883426964283\n",
      "Training loss for batch 3003 : 0.11232498288154602\n",
      "Training loss for batch 3004 : 0.40876227617263794\n",
      "Training loss for batch 3005 : 0.11173982173204422\n",
      "Training loss for batch 3006 : 0.022853460162878036\n",
      "Training loss for batch 3007 : 0.08344238996505737\n",
      "Training loss for batch 3008 : 0.4012881815433502\n",
      "Training loss for batch 3009 : 0.0937788188457489\n",
      "Training loss for batch 3010 : 0.24863150715827942\n",
      "Training loss for batch 3011 : 0.06641848385334015\n",
      "Training loss for batch 3012 : 0.5018468499183655\n",
      "Training loss for batch 3013 : 0.04094988480210304\n",
      "Training loss for batch 3014 : 0.39693689346313477\n",
      "Training loss for batch 3015 : 0.15975314378738403\n",
      "Training loss for batch 3016 : 0.2701193690299988\n",
      "Training loss for batch 3017 : 0.06634610891342163\n",
      "Training loss for batch 3018 : 0.08717767894268036\n",
      "Training loss for batch 3019 : 0.08217025548219681\n",
      "Training loss for batch 3020 : 0.19554033875465393\n",
      "Training loss for batch 3021 : 0.12679174542427063\n",
      "Training loss for batch 3022 : 0.12358158081769943\n",
      "Training loss for batch 3023 : 0.03703911229968071\n",
      "Training loss for batch 3024 : 0.03185242414474487\n",
      "Training loss for batch 3025 : 0.28298264741897583\n",
      "Training loss for batch 3026 : 0.005899711977690458\n",
      "Training loss for batch 3027 : 0.4704381227493286\n",
      "Training loss for batch 3028 : 0.07060682773590088\n",
      "Training loss for batch 3029 : 0.5220826268196106\n",
      "Training loss for batch 3030 : 0.2429625540971756\n",
      "Training loss for batch 3031 : 0.05654124543070793\n",
      "Training loss for batch 3032 : 0.2077162116765976\n",
      "Training loss for batch 3033 : 0.0790489912033081\n",
      "Training loss for batch 3034 : 0.14070554077625275\n",
      "Training loss for batch 3035 : 0.5111340284347534\n",
      "Training loss for batch 3036 : 0.2897964119911194\n",
      "Training loss for batch 3037 : 0.015748482197523117\n",
      "Training loss for batch 3038 : 0.16392040252685547\n",
      "Training loss for batch 3039 : 0.047311924397945404\n",
      "Training loss for batch 3040 : 0.25492891669273376\n",
      "Training loss for batch 3041 : 0.004122464451938868\n",
      "Training loss for batch 3042 : 0.07257886230945587\n",
      "Training loss for batch 3043 : 0.0968097373843193\n",
      "Training loss for batch 3044 : 0.22316165268421173\n",
      "Training loss for batch 3045 : 0.19793877005577087\n",
      "Training loss for batch 3046 : 0.12350843846797943\n",
      "Training loss for batch 3047 : 0.23723292350769043\n",
      "Training loss for batch 3048 : 0.12634243071079254\n",
      "Training loss for batch 3049 : 0.22137866914272308\n",
      "Training loss for batch 3050 : 0.10732021927833557\n",
      "Training loss for batch 3051 : 0.21018756926059723\n",
      "Training loss for batch 3052 : 0.0015742480754852295\n",
      "Training loss for batch 3053 : 0.34323421120643616\n",
      "Training loss for batch 3054 : 0.2782646417617798\n",
      "Training loss for batch 3055 : 0.04285622388124466\n",
      "Training loss for batch 3056 : 0.28242236375808716\n",
      "Training loss for batch 3057 : 0.10300954431295395\n",
      "Training loss for batch 3058 : 0.1730244904756546\n",
      "Training loss for batch 3059 : 0.11818498373031616\n",
      "Training loss for batch 3060 : 0.09564392268657684\n",
      "Training loss for batch 3061 : 0.36449214816093445\n",
      "Training loss for batch 3062 : 0.19947393238544464\n",
      "Training loss for batch 3063 : 0.258695513010025\n",
      "Training loss for batch 3064 : 0.01768641732633114\n",
      "Training loss for batch 3065 : 0.23743408918380737\n",
      "Training loss for batch 3066 : 0.023816745728254318\n",
      "Training loss for batch 3067 : 0.15083220601081848\n",
      "Training loss for batch 3068 : 0.023955397307872772\n",
      "Training loss for batch 3069 : 0.10156099498271942\n",
      "Training loss for batch 3070 : 0.26346448063850403\n",
      "Training loss for batch 3071 : 0.2427951991558075\n",
      "Training loss for batch 3072 : 0.09376493096351624\n",
      "Training loss for batch 3073 : 0.03905695676803589\n",
      "Training loss for batch 3074 : 0.014215228147804737\n",
      "Training loss for batch 3075 : 0.1997079700231552\n",
      "Training loss for batch 3076 : 0.11403319239616394\n",
      "Training loss for batch 3077 : 0.194697767496109\n",
      "Training loss for batch 3078 : 0.0264371819794178\n",
      "Training loss for batch 3079 : 0.3855363428592682\n",
      "Training loss for batch 3080 : 0.2754300534725189\n",
      "Training loss for batch 3081 : 0.1442059874534607\n",
      "Training loss for batch 3082 : 0.16623526811599731\n",
      "Training loss for batch 3083 : 0.32527151703834534\n",
      "Training loss for batch 3084 : 0.2995136082172394\n",
      "Training loss for batch 3085 : 0.07486380636692047\n",
      "Training loss for batch 3086 : 0.2615733742713928\n",
      "Training loss for batch 3087 : 0.028083493933081627\n",
      "Training loss for batch 3088 : 0.3750050365924835\n",
      "Training loss for batch 3089 : 0.2441580891609192\n",
      "Training loss for batch 3090 : 0.018470346927642822\n",
      "Training loss for batch 3091 : 0.13348886370658875\n",
      "Training loss for batch 3092 : 0.1461549997329712\n",
      "Training loss for batch 3093 : 0.01616077683866024\n",
      "Training loss for batch 3094 : 0.1496642678976059\n",
      "Training loss for batch 3095 : 0.09269460290670395\n",
      "Training loss for batch 3096 : 0.1175135150551796\n",
      "Training loss for batch 3097 : 0.038127876818180084\n",
      "Training loss for batch 3098 : 0.2258945107460022\n",
      "Training loss for batch 3099 : 0.11380049586296082\n",
      "Training loss for batch 3100 : 0.22258734703063965\n",
      "Training loss for batch 3101 : 0.1701725721359253\n",
      "Training loss for batch 3102 : 0.27949100732803345\n",
      "Training loss for batch 3103 : 0.05176329240202904\n",
      "Training loss for batch 3104 : 0.14022143185138702\n",
      "Training loss for batch 3105 : 0.04359998181462288\n",
      "Training loss for batch 3106 : 0.23194463551044464\n",
      "Training loss for batch 3107 : 0.3009556829929352\n",
      "Training loss for batch 3108 : 0.48501405119895935\n",
      "Training loss for batch 3109 : 0.02362469770014286\n",
      "Training loss for batch 3110 : 0.03915712609887123\n",
      "Training loss for batch 3111 : 0.20953741669654846\n",
      "Training loss for batch 3112 : 0.36052969098091125\n",
      "Training loss for batch 3113 : 0.15956783294677734\n",
      "Training loss for batch 3114 : 0.05659328028559685\n",
      "Training loss for batch 3115 : 0.15990373492240906\n",
      "Training loss for batch 3116 : 0.2295815497636795\n",
      "Training loss for batch 3117 : 0.13737495243549347\n",
      "Training loss for batch 3118 : 0.009456461295485497\n",
      "Training loss for batch 3119 : 0.26413989067077637\n",
      "Training loss for batch 3120 : 0.23726201057434082\n",
      "Training loss for batch 3121 : 0.31853726506233215\n",
      "Training loss for batch 3122 : 0.12377795577049255\n",
      "Training loss for batch 3123 : 0.0068418607115745544\n",
      "Training loss for batch 3124 : 0.0032500624656677246\n",
      "Training loss for batch 3125 : 0.1528177410364151\n",
      "Training loss for batch 3126 : 0.1886194497346878\n",
      "Training loss for batch 3127 : 0.22944098711013794\n",
      "Training loss for batch 3128 : 0.11560683697462082\n",
      "Training loss for batch 3129 : 0.11804410815238953\n",
      "Training loss for batch 3130 : 0.19002185761928558\n",
      "Training loss for batch 3131 : 0.14632372558116913\n",
      "Training loss for batch 3132 : 0.2600669264793396\n",
      "Training loss for batch 3133 : 0.028660278767347336\n",
      "Training loss for batch 3134 : 0.27019357681274414\n",
      "Training loss for batch 3135 : 0.27814289927482605\n",
      "Training loss for batch 3136 : 0.23977035284042358\n",
      "Training loss for batch 3137 : 0.09829914569854736\n",
      "Training loss for batch 3138 : 0.08177131414413452\n",
      "Training loss for batch 3139 : 0.07526237517595291\n",
      "Training loss for batch 3140 : 0.08003093302249908\n",
      "Training loss for batch 3141 : 0.04764442890882492\n",
      "Training loss for batch 3142 : 0.2431434839963913\n",
      "Training loss for batch 3143 : 0.24514424800872803\n",
      "Training loss for batch 3144 : 0.13412578403949738\n",
      "Training loss for batch 3145 : 0.0505995973944664\n",
      "Training loss for batch 3146 : 0.2855813503265381\n",
      "Training loss for batch 3147 : 0.07258583605289459\n",
      "Training loss for batch 3148 : 0.3701261579990387\n",
      "Training loss for batch 3149 : 0.04218924045562744\n",
      "Training loss for batch 3150 : 0.08564718812704086\n",
      "Training loss for batch 3151 : 0.1467646211385727\n",
      "Training loss for batch 3152 : 0.10714992135763168\n",
      "Training loss for batch 3153 : 0.1129581406712532\n",
      "Training loss for batch 3154 : 0.24447563290596008\n",
      "Training loss for batch 3155 : 0.10048314183950424\n",
      "Training loss for batch 3156 : 0.21520818769931793\n",
      "Training loss for batch 3157 : 0.25540798902511597\n",
      "Training loss for batch 3158 : 0.37966760993003845\n",
      "Training loss for batch 3159 : 0.1685119867324829\n",
      "Training loss for batch 3160 : 0.4698745012283325\n",
      "Training loss for batch 3161 : 0.05151160806417465\n",
      "Training loss for batch 3162 : 0.05972973629832268\n",
      "Training loss for batch 3163 : 0.23110106587409973\n",
      "Training loss for batch 3164 : 0.022487472742795944\n",
      "Training loss for batch 3165 : 0.026128346100449562\n",
      "Training loss for batch 3166 : 0.1492035984992981\n",
      "Training loss for batch 3167 : 0.12949366867542267\n",
      "Training loss for batch 3168 : 0.027575723826885223\n",
      "Training loss for batch 3169 : 0.08907508105039597\n",
      "Training loss for batch 3170 : 0.03169931098818779\n",
      "Training loss for batch 3171 : 0.3022705018520355\n",
      "Training loss for batch 3172 : 0.1319320648908615\n",
      "Training loss for batch 3173 : 0.015167578123509884\n",
      "Training loss for batch 3174 : 0.18455368280410767\n",
      "Training loss for batch 3175 : 0.18750908970832825\n",
      "Training loss for batch 3176 : 0.08868391066789627\n",
      "Training loss for batch 3177 : 0.06463921815156937\n",
      "Training loss for batch 3178 : 0.022756945341825485\n",
      "Training loss for batch 3179 : 0.056637443602085114\n",
      "Training loss for batch 3180 : 0.14609691500663757\n",
      "Training loss for batch 3181 : 0.0021261773072183132\n",
      "Training loss for batch 3182 : 0.16236470639705658\n",
      "Training loss for batch 3183 : 0.04402405023574829\n",
      "Training loss for batch 3184 : 0.13573819398880005\n",
      "Training loss for batch 3185 : 0.20805476605892181\n",
      "Training loss for batch 3186 : 0.27128365635871887\n",
      "Training loss for batch 3187 : 0.26983678340911865\n",
      "Training loss for batch 3188 : 0.11822384595870972\n",
      "Training loss for batch 3189 : 0.22275637090206146\n",
      "Training loss for batch 3190 : 0.3058691620826721\n",
      "Training loss for batch 3191 : 0.019368549808859825\n",
      "Training loss for batch 3192 : 0.10146830976009369\n",
      "Training loss for batch 3193 : 0.06319083273410797\n",
      "Training loss for batch 3194 : 0.0738641694188118\n",
      "Training loss for batch 3195 : 0.1510312259197235\n",
      "Training loss for batch 3196 : 0.06958112865686417\n",
      "Training loss for batch 3197 : 0.2135741263628006\n",
      "Training loss for batch 3198 : 0.041347991675138474\n",
      "Training loss for batch 3199 : 0.24462474882602692\n",
      "Training loss for batch 3200 : 0.15497428178787231\n",
      "Training loss for batch 3201 : 0.019669121131300926\n",
      "Training loss for batch 3202 : 0.006764437071979046\n",
      "Training loss for batch 3203 : 0.26946428418159485\n",
      "Training loss for batch 3204 : 0.23119182884693146\n",
      "Training loss for batch 3205 : 0.38048264384269714\n",
      "Training loss for batch 3206 : 0.32442551851272583\n",
      "Training loss for batch 3207 : 0.21464809775352478\n",
      "Training loss for batch 3208 : 0.20242857933044434\n",
      "Training loss for batch 3209 : 0.08722256869077682\n",
      "Training loss for batch 3210 : 0.27863243222236633\n",
      "Training loss for batch 3211 : 0.2062027007341385\n",
      "Training loss for batch 3212 : 0.3396817147731781\n",
      "Training loss for batch 3213 : 0.19653037190437317\n",
      "Training loss for batch 3214 : 0.006483500357717276\n",
      "Training loss for batch 3215 : 0.2931244969367981\n",
      "Training loss for batch 3216 : 0.5952104330062866\n",
      "Training loss for batch 3217 : 0.09331240504980087\n",
      "Training loss for batch 3218 : 0.32746216654777527\n",
      "Training loss for batch 3219 : 0.23238666355609894\n",
      "Training loss for batch 3220 : 0.12265737354755402\n",
      "Training loss for batch 3221 : 0.276899516582489\n",
      "Training loss for batch 3222 : 0.2124621868133545\n",
      "Training loss for batch 3223 : 0.0630524754524231\n",
      "Training loss for batch 3224 : 0.21609614789485931\n",
      "Training loss for batch 3225 : 0.11828899383544922\n",
      "Training loss for batch 3226 : 0.1318937987089157\n",
      "Training loss for batch 3227 : 0.280276894569397\n",
      "Training loss for batch 3228 : 0.029894549399614334\n",
      "Training loss for batch 3229 : 0.06481446325778961\n",
      "Training loss for batch 3230 : 0.09644893556833267\n",
      "Training loss for batch 3231 : 0.038697127252817154\n",
      "Training loss for batch 3232 : 0.1854405403137207\n",
      "Training loss for batch 3233 : 0.2276628315448761\n",
      "Training loss for batch 3234 : 0.13982056081295013\n",
      "Training loss for batch 3235 : 0.2615758180618286\n",
      "Training loss for batch 3236 : 0.19440390169620514\n",
      "Training loss for batch 3237 : 0.32897838950157166\n",
      "Training loss for batch 3238 : 0.2879451513290405\n",
      "Training loss for batch 3239 : 0.2499304711818695\n",
      "Training loss for batch 3240 : 0.3240785002708435\n",
      "Training loss for batch 3241 : 0.043602608144283295\n",
      "Training loss for batch 3242 : 0.2978672385215759\n",
      "Training loss for batch 3243 : 0.18055054545402527\n",
      "Training loss for batch 3244 : 0.21131813526153564\n",
      "Training loss for batch 3245 : 0.17942816019058228\n",
      "Training loss for batch 3246 : 0.05411496013402939\n",
      "Training loss for batch 3247 : 0.28148022294044495\n",
      "Training loss for batch 3248 : 0.10191285610198975\n",
      "Training loss for batch 3249 : 0.14845995604991913\n",
      "Training loss for batch 3250 : 0.2752150893211365\n",
      "Training loss for batch 3251 : 0.15086102485656738\n",
      "Training loss for batch 3252 : 0.13845695555210114\n",
      "Training loss for batch 3253 : 0.33869826793670654\n",
      "Training loss for batch 3254 : 0.17125827074050903\n",
      "Training loss for batch 3255 : 0.05581726133823395\n",
      "Training loss for batch 3256 : 0.21725140511989594\n",
      "Training loss for batch 3257 : 0.06193507835268974\n",
      "Training loss for batch 3258 : 0.05176445096731186\n",
      "Training loss for batch 3259 : 0.3855346739292145\n",
      "Training loss for batch 3260 : 0.08246324956417084\n",
      "Training loss for batch 3261 : 0.27150657773017883\n",
      "Training loss for batch 3262 : 0.28654950857162476\n",
      "Training loss for batch 3263 : -0.0017979987896978855\n",
      "Training loss for batch 3264 : 0.006340822670608759\n",
      "Training loss for batch 3265 : 0.16969382762908936\n",
      "Training loss for batch 3266 : 0.1350017637014389\n",
      "Training loss for batch 3267 : 0.19391416013240814\n",
      "Training loss for batch 3268 : 0.21193398535251617\n",
      "Training loss for batch 3269 : 0.0918426439166069\n",
      "Training loss for batch 3270 : 0.007438947446644306\n",
      "Training loss for batch 3271 : 0.2269151210784912\n",
      "Training loss for batch 3272 : 0.025068657472729683\n",
      "Training loss for batch 3273 : 0.20731264352798462\n",
      "Training loss for batch 3274 : 0.0416463240981102\n",
      "Training loss for batch 3275 : 0.11144130676984787\n",
      "Training loss for batch 3276 : 0.14118430018424988\n",
      "Training loss for batch 3277 : 0.00835389830172062\n",
      "Training loss for batch 3278 : 0.22432836890220642\n",
      "Training loss for batch 3279 : 0.03764745220541954\n",
      "Training loss for batch 3280 : 0.017929529771208763\n",
      "Training loss for batch 3281 : 0.0728023573756218\n",
      "Training loss for batch 3282 : 0.35848268866539\n",
      "Training loss for batch 3283 : 0.1121046170592308\n",
      "Training loss for batch 3284 : 0.2804408073425293\n",
      "Training loss for batch 3285 : 0.011402532458305359\n",
      "Training loss for batch 3286 : 0.03617851063609123\n",
      "Training loss for batch 3287 : 0.3181386888027191\n",
      "Training loss for batch 3288 : 0.4452996850013733\n",
      "Training loss for batch 3289 : -0.0004667652247007936\n",
      "Training loss for batch 3290 : 0.1095581203699112\n",
      "Training loss for batch 3291 : 0.09475237131118774\n",
      "Training loss for batch 3292 : 0.041200630366802216\n",
      "Training loss for batch 3293 : 0.14408427476882935\n",
      "Training loss for batch 3294 : 0.19276714324951172\n",
      "Training loss for batch 3295 : 0.270919531583786\n",
      "Training loss for batch 3296 : 0.11828175187110901\n",
      "Training loss for batch 3297 : 0.2689862847328186\n",
      "Training loss for batch 3298 : 0.5421580672264099\n",
      "Training loss for batch 3299 : 0.08797817677259445\n",
      "Training loss for batch 3300 : 0.14756396412849426\n",
      "Training loss for batch 3301 : 0.13140438497066498\n",
      "Training loss for batch 3302 : 0.26477205753326416\n",
      "Training loss for batch 3303 : 0.16499590873718262\n",
      "Training loss for batch 3304 : 0.026690667495131493\n",
      "Training loss for batch 3305 : 0.06112487241625786\n",
      "Training loss for batch 3306 : 0.2678130865097046\n",
      "Training loss for batch 3307 : 0.12766847014427185\n",
      "Training loss for batch 3308 : 0.4084300994873047\n",
      "Training loss for batch 3309 : 0.10443377494812012\n",
      "Training loss for batch 3310 : 0.3137613534927368\n",
      "Training loss for batch 3311 : 0.542414128780365\n",
      "Training loss for batch 3312 : 0.0197504460811615\n",
      "Training loss for batch 3313 : 0.20278184115886688\n",
      "Training loss for batch 3314 : 0.0650174468755722\n",
      "Training loss for batch 3315 : 0.12016154825687408\n",
      "Training loss for batch 3316 : 0.4050722122192383\n",
      "Training loss for batch 3317 : 0.05870545655488968\n",
      "Training loss for batch 3318 : 0.17220449447631836\n",
      "Training loss for batch 3319 : 0.07577372342348099\n",
      "Training loss for batch 3320 : 0.19331175088882446\n",
      "Training loss for batch 3321 : 0.0\n",
      "Training loss for batch 3322 : 0.07633379101753235\n",
      "Training loss for batch 3323 : 0.10838489234447479\n",
      "Training loss for batch 3324 : 0.004040623549371958\n",
      "Training loss for batch 3325 : 0.296266108751297\n",
      "Training loss for batch 3326 : 0.04748072475194931\n",
      "Training loss for batch 3327 : 0.14073966443538666\n",
      "Training loss for batch 3328 : 0.2899357080459595\n",
      "Training loss for batch 3329 : 0.12570343911647797\n",
      "Training loss for batch 3330 : 0.06311024725437164\n",
      "Training loss for batch 3331 : 0.010379210114479065\n",
      "Training loss for batch 3332 : 0.5035615563392639\n",
      "Training loss for batch 3333 : 0.05540762469172478\n",
      "Training loss for batch 3334 : 0.3065502345561981\n",
      "Training loss for batch 3335 : 0.20993797481060028\n",
      "Training loss for batch 3336 : 0.03287211060523987\n",
      "Training loss for batch 3337 : 0.031744275242090225\n",
      "Training loss for batch 3338 : 0.08715897798538208\n",
      "Training loss for batch 3339 : 0.25958722829818726\n",
      "Training loss for batch 3340 : 0.3860339820384979\n",
      "Training loss for batch 3341 : 0.1864226758480072\n",
      "Training loss for batch 3342 : 0.28651976585388184\n",
      "Training loss for batch 3343 : 0.31096401810646057\n",
      "Training loss for batch 3344 : 0.03198105841875076\n",
      "Training loss for batch 3345 : 0.12308760732412338\n",
      "Training loss for batch 3346 : 0.012798971496522427\n",
      "Training loss for batch 3347 : 0.2073451429605484\n",
      "Training loss for batch 3348 : 0.07958848029375076\n",
      "Training loss for batch 3349 : 0.251209557056427\n",
      "Training loss for batch 3350 : 0.049703214317560196\n",
      "Training loss for batch 3351 : 0.10313448309898376\n",
      "Training loss for batch 3352 : 0.11346656084060669\n",
      "Training loss for batch 3353 : 0.33042922616004944\n",
      "Training loss for batch 3354 : 0.4348292946815491\n",
      "Training loss for batch 3355 : -0.0018583047203719616\n",
      "Training loss for batch 3356 : 0.3140277564525604\n",
      "Training loss for batch 3357 : 0.15505202114582062\n",
      "Training loss for batch 3358 : 0.19958871603012085\n",
      "Training loss for batch 3359 : 0.16134658455848694\n",
      "Training loss for batch 3360 : 0.029483580961823463\n",
      "Training loss for batch 3361 : 0.22432443499565125\n",
      "Training loss for batch 3362 : 0.013345882296562195\n",
      "Training loss for batch 3363 : 0.25122731924057007\n",
      "Training loss for batch 3364 : 0.262987345457077\n",
      "Training loss for batch 3365 : 0.22684471309185028\n",
      "Training loss for batch 3366 : 0.08695907145738602\n",
      "Training loss for batch 3367 : 0.07414756715297699\n",
      "Training loss for batch 3368 : 0.14882849156856537\n",
      "Training loss for batch 3369 : 0.24363593757152557\n",
      "Training loss for batch 3370 : 0.08541716635227203\n",
      "Training loss for batch 3371 : 0.07933208346366882\n",
      "Training loss for batch 3372 : 0.22993986308574677\n",
      "Training loss for batch 3373 : 0.02971513196825981\n",
      "Training loss for batch 3374 : 0.01169576309621334\n",
      "Training loss for batch 3375 : 0.1528821438550949\n",
      "Training loss for batch 3376 : 0.30103951692581177\n",
      "Training loss for batch 3377 : 0.07373789697885513\n",
      "Training loss for batch 3378 : 0.1598118096590042\n",
      "Training loss for batch 3379 : 0.13752394914627075\n",
      "Training loss for batch 3380 : 0.1261807233095169\n",
      "Training loss for batch 3381 : 0.013863494619727135\n",
      "Training loss for batch 3382 : 0.058414068073034286\n",
      "Training loss for batch 3383 : 0.108976811170578\n",
      "Training loss for batch 3384 : 0.4432326555252075\n",
      "Training loss for batch 3385 : 0.27386045455932617\n",
      "Training loss for batch 3386 : 0.25719839334487915\n",
      "Training loss for batch 3387 : 0.28811293840408325\n",
      "Training loss for batch 3388 : 0.31141719222068787\n",
      "Training loss for batch 3389 : 0.2367093563079834\n",
      "Training loss for batch 3390 : 0.2515810430049896\n",
      "Training loss for batch 3391 : 0.07552787661552429\n",
      "Training loss for batch 3392 : 0.07580823451280594\n",
      "Training loss for batch 3393 : 0.3231111466884613\n",
      "Training loss for batch 3394 : 0.049291763454675674\n",
      "Training loss for batch 3395 : 0.1618359386920929\n",
      "Training loss for batch 3396 : 0.14586091041564941\n",
      "Training loss for batch 3397 : 0.22643743455410004\n",
      "Training loss for batch 3398 : 0.07036664336919785\n",
      "Training loss for batch 3399 : 0.4345124363899231\n",
      "Training loss for batch 3400 : 0.2527919411659241\n",
      "Training loss for batch 3401 : 0.15213149785995483\n",
      "Training loss for batch 3402 : 0.10317625850439072\n",
      "Training loss for batch 3403 : 0.09918162226676941\n",
      "Training loss for batch 3404 : 0.19419118762016296\n",
      "Training loss for batch 3405 : 0.06431111693382263\n",
      "Training loss for batch 3406 : 0.0153091074898839\n",
      "Training loss for batch 3407 : 0.21064412593841553\n",
      "Training loss for batch 3408 : 0.08074871450662613\n",
      "Training loss for batch 3409 : 0.023162376135587692\n",
      "Training loss for batch 3410 : 0.039665281772613525\n",
      "Training loss for batch 3411 : 0.08166243135929108\n",
      "Training loss for batch 3412 : 0.18430353701114655\n",
      "Training loss for batch 3413 : 0.15863557159900665\n",
      "Training loss for batch 3414 : 0.2580055892467499\n",
      "Training loss for batch 3415 : 0.05780870467424393\n",
      "Training loss for batch 3416 : 0.3800003230571747\n",
      "Training loss for batch 3417 : 0.03972305357456207\n",
      "Training loss for batch 3418 : 0.08311518281698227\n",
      "Training loss for batch 3419 : 0.21366149187088013\n",
      "Training loss for batch 3420 : 0.240406334400177\n",
      "Training loss for batch 3421 : 0.10240828990936279\n",
      "Training loss for batch 3422 : 0.12534087896347046\n",
      "Training loss for batch 3423 : 0.3705079257488251\n",
      "Training loss for batch 3424 : 0.3273044228553772\n",
      "Training loss for batch 3425 : 0.33845943212509155\n",
      "Training loss for batch 3426 : 0.17597907781600952\n",
      "Training loss for batch 3427 : 0.11410215497016907\n",
      "Training loss for batch 3428 : 0.16194885969161987\n",
      "Training loss for batch 3429 : 0.03559861704707146\n",
      "Training loss for batch 3430 : 0.20098567008972168\n",
      "Training loss for batch 3431 : 0.06701651960611343\n",
      "Training loss for batch 3432 : 0.0005993446102365851\n",
      "Training loss for batch 3433 : 0.08687113970518112\n",
      "Training loss for batch 3434 : 0.09869039803743362\n",
      "Training loss for batch 3435 : 0.057760246098041534\n",
      "Training loss for batch 3436 : 0.4067484736442566\n",
      "Training loss for batch 3437 : 0.012421658262610435\n",
      "Training loss for batch 3438 : 0.26060011982917786\n",
      "Training loss for batch 3439 : 0.13598085939884186\n",
      "Training loss for batch 3440 : 0.09050107002258301\n",
      "Training loss for batch 3441 : 0.3941527009010315\n",
      "Training loss for batch 3442 : 0.5385305285453796\n",
      "Training loss for batch 3443 : -0.0012238583294674754\n",
      "Training loss for batch 3444 : 0.1334114670753479\n",
      "Training loss for batch 3445 : 0.12387317419052124\n",
      "Training loss for batch 3446 : 0.10829965025186539\n",
      "Training loss for batch 3447 : 0.022434934973716736\n",
      "Training loss for batch 3448 : 0.23725570738315582\n",
      "Training loss for batch 3449 : 0.007442981004714966\n",
      "Training loss for batch 3450 : 0.036336202174425125\n",
      "Training loss for batch 3451 : 0.2571225166320801\n",
      "Training loss for batch 3452 : 0.21118539571762085\n",
      "Training loss for batch 3453 : 0.10566311329603195\n",
      "Training loss for batch 3454 : 0.0640212818980217\n",
      "Training loss for batch 3455 : 0.3182053565979004\n",
      "Training loss for batch 3456 : 0.22065289318561554\n",
      "Training loss for batch 3457 : 0.26249393820762634\n",
      "Training loss for batch 3458 : 0.33712661266326904\n",
      "Training loss for batch 3459 : 0.5035481452941895\n",
      "Training loss for batch 3460 : 0.24853962659835815\n",
      "Training loss for batch 3461 : 0.29718899726867676\n",
      "Training loss for batch 3462 : 0.19068841636180878\n",
      "Training loss for batch 3463 : 0.17127490043640137\n",
      "Training loss for batch 3464 : 0.12709808349609375\n",
      "Training loss for batch 3465 : 0.2825007438659668\n",
      "Training loss for batch 3466 : 0.14304882287979126\n",
      "Training loss for batch 3467 : 0.2885676920413971\n",
      "Training loss for batch 3468 : 0.28028684854507446\n",
      "Training loss for batch 3469 : 0.8249732851982117\n",
      "Training loss for batch 3470 : 0.1216704398393631\n",
      "Training loss for batch 3471 : 0.03281563147902489\n",
      "Training loss for batch 3472 : 0.20252929627895355\n",
      "Training loss for batch 3473 : 0.04925836622714996\n",
      "Training loss for batch 3474 : 0.16136306524276733\n",
      "Training loss for batch 3475 : 0.13935329020023346\n",
      "Training loss for batch 3476 : 0.27026641368865967\n",
      "Training loss for batch 3477 : 0.2024441808462143\n",
      "Training loss for batch 3478 : 0.05579739436507225\n",
      "Training loss for batch 3479 : 0.15444806218147278\n",
      "Training loss for batch 3480 : 0.05301974341273308\n",
      "Training loss for batch 3481 : 0.2610853612422943\n",
      "Training loss for batch 3482 : 0.14266489446163177\n",
      "Training loss for batch 3483 : 0.08832459896802902\n",
      "Training loss for batch 3484 : 0.07105094939470291\n",
      "Training loss for batch 3485 : 0.0\n",
      "Training loss for batch 3486 : 0.022057995200157166\n",
      "Training loss for batch 3487 : 0.04628197103738785\n",
      "Training loss for batch 3488 : 0.32054874300956726\n",
      "Training loss for batch 3489 : 0.3153480589389801\n",
      "Training loss for batch 3490 : 0.1281747817993164\n",
      "Training loss for batch 3491 : 0.07046764343976974\n",
      "Training loss for batch 3492 : 0.16085553169250488\n",
      "Training loss for batch 3493 : 0.08744215220212936\n",
      "Training loss for batch 3494 : 0.034042712301015854\n",
      "Training loss for batch 3495 : 0.21125800907611847\n",
      "Training loss for batch 3496 : 0.2848987281322479\n",
      "Training loss for batch 3497 : 0.45887431502342224\n",
      "Training loss for batch 3498 : 0.5741885900497437\n",
      "Training loss for batch 3499 : 0.06561644375324249\n",
      "Training loss for batch 3500 : 0.16266389191150665\n",
      "Training loss for batch 3501 : 0.322698175907135\n",
      "Training loss for batch 3502 : 0.11994420737028122\n",
      "Training loss for batch 3503 : 0.3198848366737366\n",
      "Training loss for batch 3504 : 0.29956021904945374\n",
      "Training loss for batch 3505 : 0.3926815688610077\n",
      "Training loss for batch 3506 : 0.17830753326416016\n",
      "Training loss for batch 3507 : 0.02030595764517784\n",
      "Training loss for batch 3508 : 0.05382144823670387\n",
      "Training loss for batch 3509 : 0.19518621265888214\n",
      "Training loss for batch 3510 : 0.2203468680381775\n",
      "Training loss for batch 3511 : 0.09647656232118607\n",
      "Training loss for batch 3512 : 0.11754252761602402\n",
      "Training loss for batch 3513 : 0.0\n",
      "Training loss for batch 3514 : 0.03396301716566086\n",
      "Training loss for batch 3515 : 0.5815466642379761\n",
      "Training loss for batch 3516 : 0.03042810596525669\n",
      "Training loss for batch 3517 : 0.17640629410743713\n",
      "Training loss for batch 3518 : 0.13258247077465057\n",
      "Training loss for batch 3519 : 0.05485513061285019\n",
      "Training loss for batch 3520 : 0.0033428864553570747\n",
      "Training loss for batch 3521 : 0.09978316724300385\n",
      "Training loss for batch 3522 : 0.23185831308364868\n",
      "Training loss for batch 3523 : 0.22516819834709167\n",
      "Training loss for batch 3524 : 0.02930072322487831\n",
      "Training loss for batch 3525 : 0.009911949746310711\n",
      "Training loss for batch 3526 : 0.4134596288204193\n",
      "Training loss for batch 3527 : 0.2541793882846832\n",
      "Training loss for batch 3528 : 0.020999807864427567\n",
      "Training loss for batch 3529 : 0.2926388382911682\n",
      "Training loss for batch 3530 : 0.20686334371566772\n",
      "Training loss for batch 3531 : 0.004217187874019146\n",
      "Training loss for batch 3532 : 0.057555489242076874\n",
      "Training loss for batch 3533 : 0.00842251442372799\n",
      "Training loss for batch 3534 : 0.08885221183300018\n",
      "Training loss for batch 3535 : 0.07613760977983475\n",
      "Training loss for batch 3536 : 0.2059788852930069\n",
      "Training loss for batch 3537 : 0.11218815296888351\n",
      "Training loss for batch 3538 : 0.05592833459377289\n",
      "Training loss for batch 3539 : 0.10418715327978134\n",
      "Training loss for batch 3540 : 0.11316870898008347\n",
      "Training loss for batch 3541 : 0.0535554438829422\n",
      "Training loss for batch 3542 : 0.14332787692546844\n",
      "Training loss for batch 3543 : 0.0\n",
      "Training loss for batch 3544 : 0.10259653627872467\n",
      "Training loss for batch 3545 : 0.09238827228546143\n",
      "Training loss for batch 3546 : 0.030108271166682243\n",
      "Training loss for batch 3547 : 0.12591934204101562\n",
      "Training loss for batch 3548 : 0.3044724762439728\n",
      "Training loss for batch 3549 : 0.2582867443561554\n",
      "Training loss for batch 3550 : 0.33896419405937195\n",
      "Training loss for batch 3551 : 0.29309791326522827\n",
      "Training loss for batch 3552 : 0.06289275735616684\n",
      "Training loss for batch 3553 : 0.11874569952487946\n",
      "Training loss for batch 3554 : 0.024144712835550308\n",
      "Training loss for batch 3555 : 0.14450418949127197\n",
      "Training loss for batch 3556 : 0.09874260425567627\n",
      "Training loss for batch 3557 : 0.04491925239562988\n",
      "Training loss for batch 3558 : 0.243150532245636\n",
      "Training loss for batch 3559 : 0.051760073751211166\n",
      "Training loss for batch 3560 : 0.5635698437690735\n",
      "Training loss for batch 3561 : 0.3201727271080017\n",
      "Training loss for batch 3562 : 0.055427879095077515\n",
      "Training loss for batch 3563 : 0.08174736797809601\n",
      "Training loss for batch 3564 : 0.2462574541568756\n",
      "Training loss for batch 3565 : 0.20898227393627167\n",
      "Training loss for batch 3566 : 0.20848217606544495\n",
      "Training loss for batch 3567 : 0.0924835056066513\n",
      "Training loss for batch 3568 : 0.026609191671013832\n",
      "Training loss for batch 3569 : 0.024098403751850128\n",
      "Training loss for batch 3570 : 0.17328335344791412\n",
      "Training loss for batch 3571 : 0.12943273782730103\n",
      "Training loss for batch 3572 : 0.03973209112882614\n",
      "Training loss for batch 3573 : 0.20776192843914032\n",
      "Training loss for batch 3574 : 0.09848745167255402\n",
      "Training loss for batch 3575 : 0.27002251148223877\n",
      "Training loss for batch 3576 : 0.24026235938072205\n",
      "Training loss for batch 3577 : 0.20973335206508636\n",
      "Training loss for batch 3578 : 0.05179782211780548\n",
      "Training loss for batch 3579 : 0.12305384874343872\n",
      "Training loss for batch 3580 : 0.2185613065958023\n",
      "Training loss for batch 3581 : 0.10567354410886765\n",
      "Training loss for batch 3582 : 0.25737112760543823\n",
      "Training loss for batch 3583 : 0.32526499032974243\n",
      "Training loss for batch 3584 : 0.20316876471042633\n",
      "Training loss for batch 3585 : 0.32871484756469727\n",
      "Training loss for batch 3586 : 0.12209711223840714\n",
      "Training loss for batch 3587 : 0.008416700176894665\n",
      "Training loss for batch 3588 : 0.0\n",
      "Training loss for batch 3589 : 0.24256089329719543\n",
      "Training loss for batch 3590 : 0.08773549646139145\n",
      "Training loss for batch 3591 : 0.22669285535812378\n",
      "Training loss for batch 3592 : 0.1531277298927307\n",
      "Training loss for batch 3593 : 0.17470882833003998\n",
      "Training loss for batch 3594 : 0.18945032358169556\n",
      "Training loss for batch 3595 : 0.049153685569763184\n",
      "Training loss for batch 3596 : 0.04295298457145691\n",
      "Training loss for batch 3597 : 0.3779783844947815\n",
      "Training loss for batch 3598 : 0.22722570598125458\n",
      "Training loss for batch 3599 : 0.12910212576389313\n",
      "Training loss for batch 3600 : 0.08235761523246765\n",
      "Training loss for batch 3601 : 0.2951749563217163\n",
      "Training loss for batch 3602 : 0.3093988001346588\n",
      "Training loss for batch 3603 : 0.14494773745536804\n",
      "Training loss for batch 3604 : 0.42310675978660583\n",
      "Training loss for batch 3605 : 0.46943092346191406\n",
      "Training loss for batch 3606 : 0.09859758615493774\n",
      "Training loss for batch 3607 : 0.22180691361427307\n",
      "Training loss for batch 3608 : 0.10486795753240585\n",
      "Training loss for batch 3609 : 0.1654706746339798\n",
      "Training loss for batch 3610 : 0.11656296253204346\n",
      "Training loss for batch 3611 : 0.09250610321760178\n",
      "Training loss for batch 3612 : 0.027433304116129875\n",
      "Training loss for batch 3613 : 0.11132343858480453\n",
      "Training loss for batch 3614 : 0.12481547892093658\n",
      "Training loss for batch 3615 : 0.12203191220760345\n",
      "Training loss for batch 3616 : 0.4835487902164459\n",
      "Training loss for batch 3617 : 0.004920045379549265\n",
      "Training loss for batch 3618 : 0.1231079250574112\n",
      "Training loss for batch 3619 : 0.18508215248584747\n",
      "Training loss for batch 3620 : 0.15907631814479828\n",
      "Training loss for batch 3621 : 0.02496572770178318\n",
      "Training loss for batch 3622 : 0.018196769058704376\n",
      "Training loss for batch 3623 : 0.137944296002388\n",
      "Training loss for batch 3624 : 0.016338547691702843\n",
      "Training loss for batch 3625 : 0.12543167173862457\n",
      "Training loss for batch 3626 : 0.15730471909046173\n",
      "Training loss for batch 3627 : 0.08216644078493118\n",
      "Training loss for batch 3628 : 0.18560589849948883\n",
      "Training loss for batch 3629 : 0.031185341998934746\n",
      "Training loss for batch 3630 : 0.3668143153190613\n",
      "Training loss for batch 3631 : 0.013340210542082787\n",
      "Training loss for batch 3632 : 0.14300940930843353\n",
      "Training loss for batch 3633 : 0.11174967139959335\n",
      "Training loss for batch 3634 : 0.200804203748703\n",
      "Training loss for batch 3635 : 0.19432035088539124\n",
      "Training loss for batch 3636 : 0.15974760055541992\n",
      "Training loss for batch 3637 : 0.09961004555225372\n",
      "Training loss for batch 3638 : 0.021813824772834778\n",
      "Training loss for batch 3639 : 0.2578219771385193\n",
      "Training loss for batch 3640 : 0.3059760332107544\n",
      "Training loss for batch 3641 : 0.0484190434217453\n",
      "Training loss for batch 3642 : 0.03115515224635601\n",
      "Training loss for batch 3643 : 0.12919624149799347\n",
      "Training loss for batch 3644 : 0.1038643941283226\n",
      "Training loss for batch 3645 : 0.05733344331383705\n",
      "Training loss for batch 3646 : 0.09582145512104034\n",
      "Training loss for batch 3647 : 0.12392416596412659\n",
      "Training loss for batch 3648 : 0.25094613432884216\n",
      "Training loss for batch 3649 : 0.33094263076782227\n",
      "Training loss for batch 3650 : 0.26937103271484375\n",
      "Training loss for batch 3651 : 0.12476547062397003\n",
      "Training loss for batch 3652 : 0.06258498877286911\n",
      "Training loss for batch 3653 : 0.13710185885429382\n",
      "Training loss for batch 3654 : 0.11519600450992584\n",
      "Training loss for batch 3655 : 0.10889245569705963\n",
      "Training loss for batch 3656 : 0.14213496446609497\n",
      "Training loss for batch 3657 : 0.2507562041282654\n",
      "Training loss for batch 3658 : 0.2927244305610657\n",
      "Training loss for batch 3659 : 0.06297051161527634\n",
      "Training loss for batch 3660 : 0.10536166280508041\n",
      "Training loss for batch 3661 : 0.042038630694150925\n",
      "Training loss for batch 3662 : 0.2683086693286896\n",
      "Training loss for batch 3663 : 0.07756157964468002\n",
      "Training loss for batch 3664 : 0.06517385691404343\n",
      "Training loss for batch 3665 : 0.07296190410852432\n",
      "Training loss for batch 3666 : 0.2553064525127411\n",
      "Training loss for batch 3667 : 0.1924683004617691\n",
      "Training loss for batch 3668 : 0.04328036308288574\n",
      "Training loss for batch 3669 : 0.022406669333577156\n",
      "Training loss for batch 3670 : 0.02843586355447769\n",
      "Training loss for batch 3671 : 0.03872634842991829\n",
      "Training loss for batch 3672 : 0.08342528343200684\n",
      "Training loss for batch 3673 : 0.05469205230474472\n",
      "Training loss for batch 3674 : 0.03630918636918068\n",
      "Training loss for batch 3675 : 0.20101390779018402\n",
      "Training loss for batch 3676 : 0.23555102944374084\n",
      "Training loss for batch 3677 : 0.21272671222686768\n",
      "Training loss for batch 3678 : 0.008070499636232853\n",
      "Training loss for batch 3679 : 0.09264331310987473\n",
      "Training loss for batch 3680 : 0.028850937262177467\n",
      "Training loss for batch 3681 : 0.05727337300777435\n",
      "Training loss for batch 3682 : 0.21210600435733795\n",
      "Training loss for batch 3683 : 0.16056212782859802\n",
      "Training loss for batch 3684 : 0.33300378918647766\n",
      "Training loss for batch 3685 : 0.1047154888510704\n",
      "Training loss for batch 3686 : 0.16497468948364258\n",
      "Training loss for batch 3687 : 0.13182467222213745\n",
      "Training loss for batch 3688 : 0.24842092394828796\n",
      "Training loss for batch 3689 : 0.3601144850254059\n",
      "Training loss for batch 3690 : 0.0\n",
      "Training loss for batch 3691 : 0.09048870950937271\n",
      "Training loss for batch 3692 : 0.5464785099029541\n",
      "Training loss for batch 3693 : 0.13438159227371216\n",
      "Training loss for batch 3694 : 0.18928532302379608\n",
      "Training loss for batch 3695 : 0.042822085320949554\n",
      "Training loss for batch 3696 : 0.3065716028213501\n",
      "Training loss for batch 3697 : 0.1731952279806137\n",
      "Training loss for batch 3698 : 0.07784079760313034\n",
      "Training loss for batch 3699 : 0.0941615104675293\n",
      "Training loss for batch 3700 : 0.06796323508024216\n",
      "Training loss for batch 3701 : 0.0006210330175235868\n",
      "Training loss for batch 3702 : 0.19658192992210388\n",
      "Training loss for batch 3703 : 0.3402805030345917\n",
      "Training loss for batch 3704 : 0.08775225281715393\n",
      "Training loss for batch 3705 : 0.08055543154478073\n",
      "Training loss for batch 3706 : 0.27462485432624817\n",
      "Training loss for batch 3707 : 0.025494400411844254\n",
      "Training loss for batch 3708 : 0.08488824218511581\n",
      "Training loss for batch 3709 : 0.12046806514263153\n",
      "Training loss for batch 3710 : 0.011869252659380436\n",
      "Training loss for batch 3711 : 0.2764100432395935\n",
      "Training loss for batch 3712 : 0.0005941788549534976\n",
      "Training loss for batch 3713 : 0.009469936601817608\n",
      "Training loss for batch 3714 : 0.2198914885520935\n",
      "Training loss for batch 3715 : 0.08321110159158707\n",
      "Training loss for batch 3716 : 0.31135115027427673\n",
      "Training loss for batch 3717 : 0.06655964255332947\n",
      "Training loss for batch 3718 : 0.02238127775490284\n",
      "Training loss for batch 3719 : 0.05463369935750961\n",
      "Training loss for batch 3720 : 0.48784157633781433\n",
      "Training loss for batch 3721 : 0.07166619598865509\n",
      "Training loss for batch 3722 : 0.10694710910320282\n",
      "Training loss for batch 3723 : 0.0578354075551033\n",
      "Training loss for batch 3724 : 0.25163379311561584\n",
      "Training loss for batch 3725 : 0.07906335592269897\n",
      "Training loss for batch 3726 : 0.24163325130939484\n",
      "Training loss for batch 3727 : 0.22783654928207397\n",
      "Training loss for batch 3728 : 0.24134138226509094\n",
      "Training loss for batch 3729 : 0.14222674071788788\n",
      "Training loss for batch 3730 : 0.2668038010597229\n",
      "Training loss for batch 3731 : 0.27909693121910095\n",
      "Training loss for batch 3732 : 0.19162774085998535\n",
      "Training loss for batch 3733 : 0.5205294489860535\n",
      "Training loss for batch 3734 : 0.03878617659211159\n",
      "Training loss for batch 3735 : 0.1601513922214508\n",
      "Training loss for batch 3736 : 0.27520138025283813\n",
      "Training loss for batch 3737 : 0.0050428747199475765\n",
      "Training loss for batch 3738 : 0.018899215385317802\n",
      "Training loss for batch 3739 : 0.2361985296010971\n",
      "Training loss for batch 3740 : 0.21899157762527466\n",
      "Training loss for batch 3741 : 0.2107209712266922\n",
      "Training loss for batch 3742 : 0.347362756729126\n",
      "Training loss for batch 3743 : 0.15511026978492737\n",
      "Training loss for batch 3744 : 0.22818151116371155\n",
      "Training loss for batch 3745 : 0.3744857907295227\n",
      "Training loss for batch 3746 : 0.20155689120292664\n",
      "Training loss for batch 3747 : 0.05693541094660759\n",
      "Training loss for batch 3748 : 0.054816119372844696\n",
      "Training loss for batch 3749 : 0.065715491771698\n",
      "Training loss for batch 3750 : 0.0\n",
      "Training loss for batch 3751 : 0.25846609473228455\n",
      "Training loss for batch 3752 : 0.3443855345249176\n",
      "Training loss for batch 3753 : 0.05131207033991814\n",
      "Training loss for batch 3754 : 0.2983208894729614\n",
      "Training loss for batch 3755 : 0.2513470947742462\n",
      "Training loss for batch 3756 : 0.027206694707274437\n",
      "Training loss for batch 3757 : 0.07972830533981323\n",
      "Training loss for batch 3758 : 0.15874497592449188\n",
      "Training loss for batch 3759 : 0.2766871154308319\n",
      "Training loss for batch 3760 : 0.07922632247209549\n",
      "Training loss for batch 3761 : 0.2673321068286896\n",
      "Training loss for batch 3762 : 0.007174510508775711\n",
      "Training loss for batch 3763 : 0.2007230520248413\n",
      "Training loss for batch 3764 : 0.32996195554733276\n",
      "Training loss for batch 3765 : 0.08040393888950348\n",
      "Training loss for batch 3766 : 0.3024348020553589\n",
      "Training loss for batch 3767 : 0.37111538648605347\n",
      "Training loss for batch 3768 : 0.044208332896232605\n",
      "Training loss for batch 3769 : 0.04154876619577408\n",
      "Training loss for batch 3770 : 0.001220391714014113\n",
      "Training loss for batch 3771 : 0.29101797938346863\n",
      "Training loss for batch 3772 : 0.38243281841278076\n",
      "Training loss for batch 3773 : 0.4284510612487793\n",
      "Training loss for batch 3774 : 0.24405130743980408\n",
      "Training loss for batch 3775 : 0.03682854771614075\n",
      "Training loss for batch 3776 : 0.3953210115432739\n",
      "Training loss for batch 3777 : 0.05859399959445\n",
      "Training loss for batch 3778 : 0.12262395769357681\n",
      "Training loss for batch 3779 : 0.17373938858509064\n",
      "Training loss for batch 3780 : 0.1192370057106018\n",
      "Training loss for batch 3781 : 0.19389764964580536\n",
      "Training loss for batch 3782 : 0.3189958333969116\n",
      "Training loss for batch 3783 : 0.13673672080039978\n",
      "Training loss for batch 3784 : 0.260885626077652\n",
      "Training loss for batch 3785 : 0.14151057600975037\n",
      "Training loss for batch 3786 : 0.09782952815294266\n",
      "Training loss for batch 3787 : 0.06918241083621979\n",
      "Training loss for batch 3788 : 0.4740367829799652\n",
      "Training loss for batch 3789 : 0.0730501040816307\n",
      "Training loss for batch 3790 : 0.1754082441329956\n",
      "Training loss for batch 3791 : 0.2526678442955017\n",
      "Training loss for batch 3792 : 0.20351575314998627\n",
      "Training loss for batch 3793 : 0.005212106741964817\n",
      "Training loss for batch 3794 : 0.02558419108390808\n",
      "Training loss for batch 3795 : 0.20455601811408997\n",
      "Training loss for batch 3796 : 0.4325607120990753\n",
      "Training loss for batch 3797 : 0.0837303102016449\n",
      "Training loss for batch 3798 : 0.08276500552892685\n",
      "Training loss for batch 3799 : 0.39181971549987793\n",
      "Training loss for batch 3800 : 0.12154128402471542\n",
      "Training loss for batch 3801 : 0.04634953662753105\n",
      "Training loss for batch 3802 : 0.14907413721084595\n",
      "Training loss for batch 3803 : 0.08092472702264786\n",
      "Training loss for batch 3804 : 0.35776618123054504\n",
      "Training loss for batch 3805 : 0.20874226093292236\n",
      "Training loss for batch 3806 : 0.14841070771217346\n",
      "Training loss for batch 3807 : 0.21105626225471497\n",
      "Training loss for batch 3808 : 0.24686840176582336\n",
      "Training loss for batch 3809 : 0.030960213392972946\n",
      "Training loss for batch 3810 : 0.035833749920129776\n",
      "Training loss for batch 3811 : 0.5179314017295837\n",
      "Training loss for batch 3812 : 0.02699054777622223\n",
      "Training loss for batch 3813 : 0.2270868867635727\n",
      "Training loss for batch 3814 : 0.21066021919250488\n",
      "Training loss for batch 3815 : 0.07250295579433441\n",
      "Training loss for batch 3816 : 0.19589002430438995\n",
      "Training loss for batch 3817 : 0.3599609136581421\n",
      "Training loss for batch 3818 : 0.0014615004183724523\n",
      "Training loss for batch 3819 : 0.0920025110244751\n",
      "Training loss for batch 3820 : 0.48939067125320435\n",
      "Training loss for batch 3821 : 0.11566530168056488\n",
      "Training loss for batch 3822 : 0.0165691077709198\n",
      "Training loss for batch 3823 : 0.10021767020225525\n",
      "Training loss for batch 3824 : 0.16792233288288116\n",
      "Training loss for batch 3825 : 0.19636212289333344\n",
      "Training loss for batch 3826 : 0.0\n",
      "Training loss for batch 3827 : 0.17100302875041962\n",
      "Training loss for batch 3828 : 0.1545368731021881\n",
      "Training loss for batch 3829 : 0.36409732699394226\n",
      "Training loss for batch 3830 : 0.11359361559152603\n",
      "Training loss for batch 3831 : 0.18683286011219025\n",
      "Training loss for batch 3832 : 0.01129987183958292\n",
      "Training loss for batch 3833 : 0.16943614184856415\n",
      "Training loss for batch 3834 : 0.19924341142177582\n",
      "Training loss for batch 3835 : 0.19349439442157745\n",
      "Training loss for batch 3836 : 0.11330635100603104\n",
      "Training loss for batch 3837 : 0.10444343835115433\n",
      "Training loss for batch 3838 : 0.05944850295782089\n",
      "Training loss for batch 3839 : 0.03918895125389099\n",
      "Training loss for batch 3840 : 0.1513691544532776\n",
      "Training loss for batch 3841 : 0.25590968132019043\n",
      "Training loss for batch 3842 : 0.04885699599981308\n",
      "Training loss for batch 3843 : 0.19890134036540985\n",
      "Training loss for batch 3844 : 0.01796349324285984\n",
      "Training loss for batch 3845 : 0.24222010374069214\n",
      "Training loss for batch 3846 : 0.21268495917320251\n",
      "Training loss for batch 3847 : 0.13642989099025726\n",
      "Training loss for batch 3848 : 0.05859934166073799\n",
      "Training loss for batch 3849 : 0.039019081741571426\n",
      "Training loss for batch 3850 : 0.2772732973098755\n",
      "Training loss for batch 3851 : 0.21486958861351013\n",
      "Training loss for batch 3852 : 0.07361863553524017\n",
      "Training loss for batch 3853 : 0.10156500339508057\n",
      "Training loss for batch 3854 : 0.09907662123441696\n",
      "Training loss for batch 3855 : 0.06966786086559296\n",
      "Training loss for batch 3856 : 0.0\n",
      "Training loss for batch 3857 : 0.15058191120624542\n",
      "Training loss for batch 3858 : 0.11218439787626266\n",
      "Training loss for batch 3859 : 0.11904365569353104\n",
      "Training loss for batch 3860 : 0.11806422472000122\n",
      "Training loss for batch 3861 : 0.21817857027053833\n",
      "Training loss for batch 3862 : 0.29360413551330566\n",
      "Training loss for batch 3863 : 0.062016505748033524\n",
      "Training loss for batch 3864 : 0.2282048612833023\n",
      "Training loss for batch 3865 : 0.09994957596063614\n",
      "Training loss for batch 3866 : 0.35746702551841736\n",
      "Training loss for batch 3867 : 0.004367406014353037\n",
      "Training loss for batch 3868 : 0.13067734241485596\n",
      "Training loss for batch 3869 : 0.034410905092954636\n",
      "Training loss for batch 3870 : 0.20806488394737244\n",
      "Training loss for batch 3871 : 0.4857460856437683\n",
      "Training loss for batch 3872 : 0.1769426316022873\n",
      "Training loss for batch 3873 : 0.3802037835121155\n",
      "Training loss for batch 3874 : 0.24528779089450836\n",
      "Training loss for batch 3875 : 0.015271030366420746\n",
      "Training loss for batch 3876 : 0.16947314143180847\n",
      "Training loss for batch 3877 : 0.09220682084560394\n",
      "Training loss for batch 3878 : 0.1422414928674698\n",
      "Training loss for batch 3879 : 0.21313047409057617\n",
      "Training loss for batch 3880 : 0.22542111575603485\n",
      "Training loss for batch 3881 : 0.045278485864400864\n",
      "Training loss for batch 3882 : 0.008526584133505821\n",
      "Training loss for batch 3883 : 0.4744589924812317\n",
      "Training loss for batch 3884 : 0.2600753903388977\n",
      "Training loss for batch 3885 : 0.053819991648197174\n",
      "Training loss for batch 3886 : 0.06294213235378265\n",
      "Training loss for batch 3887 : 0.2509983777999878\n",
      "Training loss for batch 3888 : 0.08102165162563324\n",
      "Training loss for batch 3889 : 0.13718962669372559\n",
      "Training loss for batch 3890 : 0.15050466358661652\n",
      "Training loss for batch 3891 : 0.10362568497657776\n",
      "Training loss for batch 3892 : 0.2168692946434021\n",
      "Training loss for batch 3893 : 0.11452367156744003\n",
      "Training loss for batch 3894 : 0.16938675940036774\n",
      "Training loss for batch 3895 : 0.10776261985301971\n",
      "Training loss for batch 3896 : 0.015530499629676342\n",
      "Training loss for batch 3897 : 0.09324707090854645\n",
      "Training loss for batch 3898 : 0.016456685960292816\n",
      "Training loss for batch 3899 : 0.13596871495246887\n",
      "Training loss for batch 3900 : 0.09959408640861511\n",
      "Training loss for batch 3901 : 0.23470844328403473\n",
      "Training loss for batch 3902 : 0.3085460960865021\n",
      "Training loss for batch 3903 : 0.045227449387311935\n",
      "Training loss for batch 3904 : 0.11236630380153656\n",
      "Training loss for batch 3905 : 0.0861169770359993\n",
      "Training loss for batch 3906 : 0.12058574706315994\n",
      "Training loss for batch 3907 : 0.18001845479011536\n",
      "Training loss for batch 3908 : 0.030270474031567574\n",
      "Training loss for batch 3909 : 0.16615177690982819\n",
      "Training loss for batch 3910 : 0.09279977530241013\n",
      "Training loss for batch 3911 : 0.030558928847312927\n",
      "Training loss for batch 3912 : 0.06571663171052933\n",
      "Training loss for batch 3913 : 0.19993755221366882\n",
      "Training loss for batch 3914 : 0.041596490889787674\n",
      "Training loss for batch 3915 : 0.029879065230488777\n",
      "Training loss for batch 3916 : 0.027848534286022186\n",
      "Training loss for batch 3917 : 0.26010948419570923\n",
      "Training loss for batch 3918 : 0.24493248760700226\n",
      "Training loss for batch 3919 : 0.27886196970939636\n",
      "Training loss for batch 3920 : 0.13866060972213745\n",
      "Training loss for batch 3921 : 0.07779233902692795\n",
      "Training loss for batch 3922 : 0.08354119956493378\n",
      "Training loss for batch 3923 : 0.4828934669494629\n",
      "Training loss for batch 3924 : 0.1804816722869873\n",
      "Training loss for batch 3925 : 0.0061057559214532375\n",
      "Training loss for batch 3926 : 0.03867879509925842\n",
      "Training loss for batch 3927 : 0.21851550042629242\n",
      "Training loss for batch 3928 : 0.2683722674846649\n",
      "Training loss for batch 3929 : 0.1029830053448677\n",
      "Training loss for batch 3930 : 0.1693984717130661\n",
      "Training loss for batch 3931 : 0.08644212782382965\n",
      "Training loss for batch 3932 : 0.2333015501499176\n",
      "Training loss for batch 3933 : 0.10555782169103622\n",
      "Training loss for batch 3934 : 0.18073052167892456\n",
      "Training loss for batch 3935 : 0.21959182620048523\n",
      "Training loss for batch 3936 : 0.3462895452976227\n",
      "Training loss for batch 3937 : 0.0\n",
      "Training loss for batch 3938 : 0.1199754923582077\n",
      "Training loss for batch 3939 : 0.1193331778049469\n",
      "Training loss for batch 3940 : 0.1457141935825348\n",
      "Training loss for batch 3941 : 0.0127054862678051\n",
      "Training loss for batch 3942 : 0.2445560246706009\n",
      "Training loss for batch 3943 : 0.1118931770324707\n",
      "Training loss for batch 3944 : 0.13181011378765106\n",
      "Training loss for batch 3945 : 0.0362408310174942\n",
      "Training loss for batch 3946 : 0.07850680500268936\n",
      "Training loss for batch 3947 : 0.10882510244846344\n",
      "Training loss for batch 3948 : 0.010702205821871758\n",
      "Training loss for batch 3949 : 0.28725606203079224\n",
      "Training loss for batch 3950 : 0.0621381439268589\n",
      "Training loss for batch 3951 : 0.037871282547712326\n",
      "Training loss for batch 3952 : 0.0\n",
      "Training loss for batch 3953 : 0.13051815330982208\n",
      "Training loss for batch 3954 : 0.38205069303512573\n",
      "Training loss for batch 3955 : 0.2638000249862671\n",
      "Training loss for batch 3956 : 0.2319725602865219\n",
      "Training loss for batch 3957 : 0.3430270552635193\n",
      "Training loss for batch 3958 : 0.25288546085357666\n",
      "Training loss for batch 3959 : 0.03721665218472481\n",
      "Training loss for batch 3960 : 0.1977371871471405\n",
      "Training loss for batch 3961 : 0.06962740421295166\n",
      "Training loss for batch 3962 : 0.062034957110881805\n",
      "Training loss for batch 3963 : 0.40840649604797363\n",
      "Training loss for batch 3964 : 0.0361579954624176\n",
      "Training loss for batch 3965 : 0.3619759678840637\n",
      "Training loss for batch 3966 : 0.0531044527888298\n",
      "Training loss for batch 3967 : 0.08799769729375839\n",
      "Training loss for batch 3968 : 0.08577068895101547\n",
      "Training loss for batch 3969 : 0.1802213490009308\n",
      "Training loss for batch 3970 : 0.061093345284461975\n",
      "Training loss for batch 3971 : 0.20553116500377655\n",
      "Training loss for batch 3972 : 0.09737799316644669\n",
      "Training loss for batch 3973 : 0.08064895868301392\n",
      "Training loss for batch 3974 : 0.27839553356170654\n",
      "Training loss for batch 3975 : 0.05369175598025322\n",
      "Training loss for batch 3976 : 0.3480578064918518\n",
      "Training loss for batch 3977 : 0.3436219096183777\n",
      "Training loss for batch 3978 : 0.25893497467041016\n",
      "Training loss for batch 3979 : 0.305359810590744\n",
      "Training loss for batch 3980 : 0.3800014853477478\n",
      "Training loss for batch 3981 : 0.025970567017793655\n",
      "Training loss for batch 3982 : 0.022403080016374588\n",
      "Training loss for batch 3983 : 0.02347813919186592\n",
      "Training loss for batch 3984 : 0.1260475069284439\n",
      "Training loss for batch 3985 : 0.034812554717063904\n",
      "Training loss for batch 3986 : 0.5030675530433655\n",
      "Training loss for batch 3987 : 0.22461894154548645\n",
      "Training loss for batch 3988 : 0.1661805659532547\n",
      "Training loss for batch 3989 : 0.07941782474517822\n",
      "Training loss for batch 3990 : 0.04345926642417908\n",
      "Training loss for batch 3991 : 0.037059634923934937\n",
      "Training loss for batch 3992 : 0.4990597665309906\n",
      "Training loss for batch 3993 : 0.057088229805231094\n",
      "Training loss for batch 3994 : 0.08992338925600052\n",
      "Training loss for batch 3995 : 0.09615612030029297\n",
      "Training loss for batch 3996 : -0.00027972666430287063\n",
      "Training loss for batch 3997 : 0.3336637020111084\n",
      "Training loss for batch 3998 : 0.17609652876853943\n",
      "Training loss for batch 3999 : 0.07066507637500763\n",
      "Training loss for batch 4000 : 0.0\n",
      "Training loss for batch 4001 : 0.027554409578442574\n",
      "Training loss for batch 4002 : 0.017715251073241234\n",
      "Training loss for batch 4003 : 0.1240188330411911\n",
      "Training loss for batch 4004 : 0.10926365107297897\n",
      "Training loss for batch 4005 : 0.2608574628829956\n",
      "Training loss for batch 4006 : 0.024502594023942947\n",
      "Training loss for batch 4007 : 0.07620421797037125\n",
      "Training loss for batch 4008 : 0.365887850522995\n",
      "Training loss for batch 4009 : 0.031751785427331924\n",
      "Training loss for batch 4010 : 0.025942889973521233\n",
      "Training loss for batch 4011 : 0.1708982139825821\n",
      "Training loss for batch 4012 : 0.2357560396194458\n",
      "Training loss for batch 4013 : 0.01561710424721241\n",
      "Training loss for batch 4014 : 0.08491149544715881\n",
      "Training loss for batch 4015 : 0.025605227798223495\n",
      "Training loss for batch 4016 : 0.29962435364723206\n",
      "Training loss for batch 4017 : 0.048020701855421066\n",
      "Training loss for batch 4018 : 0.295562744140625\n",
      "Training loss for batch 4019 : 0.4536116123199463\n",
      "Training loss for batch 4020 : 0.21719315648078918\n",
      "Training loss for batch 4021 : 0.1179480105638504\n",
      "Training loss for batch 4022 : 0.3995034396648407\n",
      "Training loss for batch 4023 : 0.0\n",
      "Training loss for batch 4024 : 0.23032061755657196\n",
      "Training loss for batch 4025 : 0.042196355760097504\n",
      "Training loss for batch 4026 : 0.13020440936088562\n",
      "Training loss for batch 4027 : 0.14433743059635162\n",
      "Training loss for batch 4028 : 0.6957565546035767\n",
      "Training loss for batch 4029 : 0.06878962367773056\n",
      "Training loss for batch 4030 : 0.09558792412281036\n",
      "Training loss for batch 4031 : -0.0003809373592957854\n",
      "Training loss for batch 4032 : 0.04069232940673828\n",
      "Training loss for batch 4033 : 0.1785939484834671\n",
      "Training loss for batch 4034 : 0.2143828123807907\n",
      "Training loss for batch 4035 : 0.3340388834476471\n",
      "Training loss for batch 4036 : 0.09846921265125275\n",
      "Training loss for batch 4037 : 0.22417210042476654\n",
      "Training loss for batch 4038 : 0.44840535521507263\n",
      "Training loss for batch 4039 : 0.14318610727787018\n",
      "Training loss for batch 4040 : 0.30017828941345215\n",
      "Training loss for batch 4041 : 0.18449169397354126\n",
      "Training loss for batch 4042 : 0.10029760748147964\n",
      "Training loss for batch 4043 : 0.16016548871994019\n",
      "Training loss for batch 4044 : 0.15442410111427307\n",
      "Training loss for batch 4045 : 0.12264623492956161\n",
      "Training loss for batch 4046 : 0.23174075782299042\n",
      "Training loss for batch 4047 : 0.08591637760400772\n",
      "Training loss for batch 4048 : 0.13683807849884033\n",
      "Training loss for batch 4049 : 0.27129876613616943\n",
      "Training loss for batch 4050 : 0.053814031183719635\n",
      "Training loss for batch 4051 : 0.04938279464840889\n",
      "Training loss for batch 4052 : 0.06474937498569489\n",
      "Training loss for batch 4053 : 0.13221420347690582\n",
      "Training loss for batch 4054 : 0.3842695951461792\n",
      "Training loss for batch 4055 : 0.23492369055747986\n",
      "Training loss for batch 4056 : 0.3086167573928833\n",
      "Training loss for batch 4057 : 0.052969448268413544\n",
      "Training loss for batch 4058 : 0.06088104844093323\n",
      "Training loss for batch 4059 : 0.5137179493904114\n",
      "Training loss for batch 4060 : 0.41580843925476074\n",
      "Training loss for batch 4061 : 0.2459363341331482\n",
      "Training loss for batch 4062 : 0.09486246854066849\n",
      "Training loss for batch 4063 : 0.5115295648574829\n",
      "Training loss for batch 4064 : 0.06733688712120056\n",
      "Training loss for batch 4065 : 0.023637836799025536\n",
      "Training loss for batch 4066 : 0.10224106162786484\n",
      "Training loss for batch 4067 : 0.0013116400223225355\n",
      "Training loss for batch 4068 : 0.023363441228866577\n",
      "Training loss for batch 4069 : 0.1028272807598114\n",
      "Training loss for batch 4070 : 0.4948262870311737\n",
      "Training loss for batch 4071 : 0.03492388129234314\n",
      "Training loss for batch 4072 : 0.05319802090525627\n",
      "Training loss for batch 4073 : 0.042878035455942154\n",
      "Training loss for batch 4074 : 0.3481948971748352\n",
      "Training loss for batch 4075 : 0.24536563456058502\n",
      "Training loss for batch 4076 : 0.29650044441223145\n",
      "Training loss for batch 4077 : 0.032074082642793655\n",
      "Training loss for batch 4078 : 0.30360347032546997\n",
      "Training loss for batch 4079 : 0.28636839985847473\n",
      "Training loss for batch 4080 : 0.2714249789714813\n",
      "Training loss for batch 4081 : 0.25555339455604553\n",
      "Training loss for batch 4082 : 0.11093135178089142\n",
      "Training loss for batch 4083 : 0.2948174476623535\n",
      "Training loss for batch 4084 : 0.14112654328346252\n",
      "Training loss for batch 4085 : 0.13792680203914642\n",
      "Training loss for batch 4086 : 0.0718718022108078\n",
      "Training loss for batch 4087 : 0.24153684079647064\n",
      "Training loss for batch 4088 : 0.17002859711647034\n",
      "Training loss for batch 4089 : 0.1167551651597023\n",
      "Training loss for batch 4090 : 0.4288332760334015\n",
      "Training loss for batch 4091 : -0.002086447551846504\n",
      "Training loss for batch 4092 : 0.18133610486984253\n",
      "Training loss for batch 4093 : 0.443589985370636\n",
      "Training loss for batch 4094 : 0.0965532585978508\n",
      "Training loss for batch 4095 : 0.1395391821861267\n",
      "Training loss for batch 4096 : 0.37404197454452515\n",
      "Training loss for batch 4097 : 0.21452096104621887\n",
      "Training loss for batch 4098 : 0.017044832929968834\n",
      "Training loss for batch 4099 : 0.4845912456512451\n",
      "Training loss for batch 4100 : 0.11854776740074158\n",
      "Training loss for batch 4101 : 0.08461522310972214\n",
      "Training loss for batch 4102 : 0.19069881737232208\n",
      "Training loss for batch 4103 : 0.08535043895244598\n",
      "Training loss for batch 4104 : -0.0006769432802684605\n",
      "Training loss for batch 4105 : 0.08152937144041061\n",
      "Training loss for batch 4106 : 0.17115333676338196\n",
      "Training loss for batch 4107 : 0.21732044219970703\n",
      "Training loss for batch 4108 : 0.0815858319401741\n",
      "Training loss for batch 4109 : 0.10407070815563202\n",
      "Training loss for batch 4110 : 0.15374135971069336\n",
      "Training loss for batch 4111 : 0.16993393003940582\n",
      "Training loss for batch 4112 : 0.27066025137901306\n",
      "Training loss for batch 4113 : 0.016259143128991127\n",
      "Training loss for batch 4114 : 0.11033319681882858\n",
      "Training loss for batch 4115 : 0.05288749933242798\n",
      "Training loss for batch 4116 : 0.05875830352306366\n",
      "Training loss for batch 4117 : 0.08123756945133209\n",
      "Training loss for batch 4118 : 0.2825692892074585\n",
      "Training loss for batch 4119 : 0.13805271685123444\n",
      "Training loss for batch 4120 : 0.09265153110027313\n",
      "Training loss for batch 4121 : 0.1826602965593338\n",
      "Training loss for batch 4122 : 0.13401690125465393\n",
      "Training loss for batch 4123 : 0.07424549013376236\n",
      "Training loss for batch 4124 : 0.13880078494548798\n",
      "Training loss for batch 4125 : 0.22772523760795593\n",
      "Training loss for batch 4126 : 0.35426241159439087\n",
      "Training loss for batch 4127 : 0.21712088584899902\n",
      "Training loss for batch 4128 : 0.024505505338311195\n",
      "Training loss for batch 4129 : 0.6571575403213501\n",
      "Training loss for batch 4130 : 0.16227780282497406\n",
      "Training loss for batch 4131 : 0.1884821653366089\n",
      "Training loss for batch 4132 : 0.14314931631088257\n",
      "Training loss for batch 4133 : 0.056721799075603485\n",
      "Training loss for batch 4134 : 0.1385384202003479\n",
      "Training loss for batch 4135 : 0.07249026000499725\n",
      "Training loss for batch 4136 : 0.05181106552481651\n",
      "Training loss for batch 4137 : 0.30876386165618896\n",
      "Training loss for batch 4138 : 0.06948789209127426\n",
      "Training loss for batch 4139 : 0.2508465349674225\n",
      "Training loss for batch 4140 : 0.13713012635707855\n",
      "Training loss for batch 4141 : 0.023954346776008606\n",
      "Training loss for batch 4142 : 0.06892220675945282\n",
      "Training loss for batch 4143 : 0.07435670495033264\n",
      "Training loss for batch 4144 : 0.14287251234054565\n",
      "Training loss for batch 4145 : 0.09077049046754837\n",
      "Training loss for batch 4146 : 0.25209900736808777\n",
      "Training loss for batch 4147 : 0.12221366167068481\n",
      "Training loss for batch 4148 : 0.06338915228843689\n",
      "Training loss for batch 4149 : 0.03587339445948601\n",
      "Training loss for batch 4150 : 0.14694419503211975\n",
      "Training loss for batch 4151 : 0.050792593508958817\n",
      "Training loss for batch 4152 : 0.009078734554350376\n",
      "Training loss for batch 4153 : 0.21299515664577484\n",
      "Training loss for batch 4154 : 0.05476444214582443\n",
      "Training loss for batch 4155 : 0.07100161910057068\n",
      "Training loss for batch 4156 : 0.5286964774131775\n",
      "Training loss for batch 4157 : 0.05132632330060005\n",
      "Training loss for batch 4158 : 0.028156956657767296\n",
      "Training loss for batch 4159 : 0.5638492703437805\n",
      "Training loss for batch 4160 : 0.08078455924987793\n",
      "Training loss for batch 4161 : 0.16061119735240936\n",
      "Training loss for batch 4162 : 0.017049232497811317\n",
      "Training loss for batch 4163 : 0.2544924020767212\n",
      "Training loss for batch 4164 : 0.23112645745277405\n",
      "Training loss for batch 4165 : 0.06224972382187843\n",
      "Training loss for batch 4166 : 0.019476911053061485\n",
      "Training loss for batch 4167 : 0.44233912229537964\n",
      "Training loss for batch 4168 : 0.2643166780471802\n",
      "Training loss for batch 4169 : 0.17673277854919434\n",
      "Training loss for batch 4170 : 0.159750834107399\n",
      "Training loss for batch 4171 : 0.020281584933400154\n",
      "Training loss for batch 4172 : 0.5586367845535278\n",
      "Training loss for batch 4173 : 0.06884749233722687\n",
      "Training loss for batch 4174 : 0.41765090823173523\n",
      "Training loss for batch 4175 : 0.5607045292854309\n",
      "Training loss for batch 4176 : 0.16618171334266663\n",
      "Training loss for batch 4177 : 0.26901280879974365\n",
      "Training loss for batch 4178 : 0.09975513070821762\n",
      "Training loss for batch 4179 : 0.6341786980628967\n",
      "Training loss for batch 4180 : 0.014458945021033287\n",
      "Training loss for batch 4181 : 0.09530853480100632\n",
      "Training loss for batch 4182 : 0.3181869685649872\n",
      "Training loss for batch 4183 : 0.1423162966966629\n",
      "Training loss for batch 4184 : 0.2864508330821991\n",
      "Training loss for batch 4185 : 0.20502930879592896\n",
      "Training loss for batch 4186 : 0.10955800116062164\n",
      "Training loss for batch 4187 : 0.06315311044454575\n",
      "Training loss for batch 4188 : 0.05231534689664841\n",
      "Training loss for batch 4189 : 0.5100854635238647\n",
      "Training loss for batch 4190 : 0.3134438991546631\n",
      "Training loss for batch 4191 : 0.21008355915546417\n",
      "Training loss for batch 4192 : 0.27957120537757874\n",
      "Training loss for batch 4193 : 0.38488370180130005\n",
      "Training loss for batch 4194 : 0.11444481462240219\n",
      "Training loss for batch 4195 : 0.06602439284324646\n",
      "Training loss for batch 4196 : 0.41706395149230957\n",
      "Training loss for batch 4197 : 0.16292887926101685\n",
      "Training loss for batch 4198 : 0.5132499933242798\n",
      "Training loss for batch 4199 : 0.1200258806347847\n",
      "Training loss for batch 4200 : 0.2156587541103363\n",
      "Training loss for batch 4201 : 0.25132641196250916\n",
      "Training loss for batch 4202 : 0.018202222883701324\n",
      "Training loss for batch 4203 : 0.27617207169532776\n",
      "Training loss for batch 4204 : 0.058080755174160004\n",
      "Training loss for batch 4205 : 0.1541340947151184\n",
      "Training loss for batch 4206 : 0.21854661405086517\n",
      "Training loss for batch 4207 : 0.09075064957141876\n",
      "Training loss for batch 4208 : 0.1533534675836563\n",
      "Training loss for batch 4209 : 0.07978948950767517\n",
      "Training loss for batch 4210 : 0.21555665135383606\n",
      "Training loss for batch 4211 : 0.19202739000320435\n",
      "Training loss for batch 4212 : 0.10488446056842804\n",
      "Training loss for batch 4213 : 0.14336735010147095\n",
      "Training loss for batch 4214 : 0.0714532732963562\n",
      "Training loss for batch 4215 : 0.37770238518714905\n",
      "Training loss for batch 4216 : 0.23703120648860931\n",
      "Training loss for batch 4217 : 0.28790098428726196\n",
      "Training loss for batch 4218 : 0.11070184409618378\n",
      "Training loss for batch 4219 : 0.020540272817015648\n",
      "Training loss for batch 4220 : 0.0819767564535141\n",
      "Training loss for batch 4221 : 0.021887466311454773\n",
      "Training loss for batch 4222 : -0.0003927288926206529\n",
      "Training loss for batch 4223 : 0.0\n",
      "Training loss for batch 4224 : 0.0\n",
      "Training loss for batch 4225 : 0.08651553094387054\n",
      "Training loss for batch 4226 : 0.31661704182624817\n",
      "Training loss for batch 4227 : 0.2601301372051239\n",
      "Training loss for batch 4228 : 0.36496180295944214\n",
      "Training loss for batch 4229 : 0.18312379717826843\n",
      "Training loss for batch 4230 : 0.16786107420921326\n",
      "Training loss for batch 4231 : 0.12687307596206665\n",
      "Training loss for batch 4232 : 0.28432270884513855\n",
      "Training loss for batch 4233 : 0.028547687456011772\n",
      "Training loss for batch 4234 : 0.3220539093017578\n",
      "Training loss for batch 4235 : 0.028529226779937744\n",
      "Training loss for batch 4236 : 0.33075952529907227\n",
      "Training loss for batch 4237 : 0.3406197130680084\n",
      "Training loss for batch 4238 : 0.0747290626168251\n",
      "Training loss for batch 4239 : 0.21898934245109558\n",
      "Training loss for batch 4240 : 0.0986701026558876\n",
      "Training loss for batch 4241 : 0.1725528985261917\n",
      "Training loss for batch 4242 : 0.1544043868780136\n",
      "Training loss for batch 4243 : 0.11155657470226288\n",
      "Training loss for batch 4244 : 0.09091466665267944\n",
      "Training loss for batch 4245 : 0.22576290369033813\n",
      "Training loss for batch 4246 : 0.03826021030545235\n",
      "Training loss for batch 4247 : 0.3116852641105652\n",
      "Training loss for batch 4248 : 0.06594258546829224\n",
      "Training loss for batch 4249 : 0.24398837983608246\n",
      "Training loss for batch 4250 : 0.08959734439849854\n",
      "Training loss for batch 4251 : 0.023420704528689384\n",
      "Training loss for batch 4252 : 0.08485063910484314\n",
      "Training loss for batch 4253 : 0.048081040382385254\n",
      "Training loss for batch 4254 : 0.26841676235198975\n",
      "Training loss for batch 4255 : 0.25901663303375244\n",
      "Training loss for batch 4256 : 0.3146092891693115\n",
      "Training loss for batch 4257 : 0.09768974035978317\n",
      "Training loss for batch 4258 : 0.023105569183826447\n",
      "Training loss for batch 4259 : 0.14848773181438446\n",
      "Training loss for batch 4260 : 0.19271662831306458\n",
      "Training loss for batch 4261 : 0.10365698486566544\n",
      "Training loss for batch 4262 : 0.23425889015197754\n",
      "Training loss for batch 4263 : 0.2191310077905655\n",
      "Training loss for batch 4264 : 0.2118104100227356\n",
      "Training loss for batch 4265 : 0.03537045419216156\n",
      "Training loss for batch 4266 : 0.0478823259472847\n",
      "Training loss for batch 4267 : 0.26493677496910095\n",
      "Training loss for batch 4268 : 0.12067023664712906\n",
      "Training loss for batch 4269 : 0.08374883979558945\n",
      "Training loss for batch 4270 : 0.31292489171028137\n",
      "Training loss for batch 4271 : 0.24338960647583008\n",
      "Training loss for batch 4272 : 0.14145931601524353\n",
      "Training loss for batch 4273 : 0.0026068147271871567\n",
      "Training loss for batch 4274 : 0.3703824281692505\n",
      "Training loss for batch 4275 : 0.21705713868141174\n",
      "Training loss for batch 4276 : 0.103220134973526\n",
      "Training loss for batch 4277 : 0.12484558671712875\n",
      "Training loss for batch 4278 : 0.2148563712835312\n",
      "Training loss for batch 4279 : 0.12950773537158966\n",
      "Training loss for batch 4280 : 0.15658529102802277\n",
      "Training loss for batch 4281 : 0.06496554613113403\n",
      "Training loss for batch 4282 : 0.0506243035197258\n",
      "Training loss for batch 4283 : -0.00027423177380114794\n",
      "Training loss for batch 4284 : 0.08521468192338943\n",
      "Training loss for batch 4285 : 0.46135812997817993\n",
      "Training loss for batch 4286 : 0.12358309328556061\n",
      "Training loss for batch 4287 : 0.04044988378882408\n",
      "Training loss for batch 4288 : 0.09981760382652283\n",
      "Training loss for batch 4289 : 0.1566617339849472\n",
      "Training loss for batch 4290 : 0.05246782675385475\n",
      "Training loss for batch 4291 : 0.43792274594306946\n",
      "Training loss for batch 4292 : 0.18737497925758362\n",
      "Training loss for batch 4293 : 0.13184861838817596\n",
      "Training loss for batch 4294 : 0.20040813088417053\n",
      "Training loss for batch 4295 : 0.17639753222465515\n",
      "Training loss for batch 4296 : 0.13546667993068695\n",
      "Training loss for batch 4297 : 0.28273913264274597\n",
      "Training loss for batch 4298 : 0.23762722313404083\n",
      "Training loss for batch 4299 : 0.26881247758865356\n",
      "Training loss for batch 4300 : 0.15119172632694244\n",
      "Training loss for batch 4301 : 0.12439259141683578\n",
      "Training loss for batch 4302 : 0.4086100161075592\n",
      "Training loss for batch 4303 : 0.1450997292995453\n",
      "Training loss for batch 4304 : 0.10912112891674042\n",
      "Training loss for batch 4305 : 0.29148069024086\n",
      "Training loss for batch 4306 : 0.10871944576501846\n",
      "Training loss for batch 4307 : 0.1363062560558319\n",
      "Training loss for batch 4308 : 0.2536904811859131\n",
      "Training loss for batch 4309 : 0.20806771516799927\n",
      "Training loss for batch 4310 : 0.3990870416164398\n",
      "Training loss for batch 4311 : 0.15048637986183167\n",
      "Training loss for batch 4312 : 0.01858716830611229\n",
      "Training loss for batch 4313 : 0.179041787981987\n",
      "Training loss for batch 4314 : 0.025386273860931396\n",
      "Training loss for batch 4315 : 0.27662986516952515\n",
      "Training loss for batch 4316 : 0.007059026043862104\n",
      "Training loss for batch 4317 : 0.18192291259765625\n",
      "Training loss for batch 4318 : 0.3328918516635895\n",
      "Training loss for batch 4319 : 0.1370932012796402\n",
      "Training loss for batch 4320 : 0.21705296635627747\n",
      "Training loss for batch 4321 : 0.033041272312402725\n",
      "Training loss for batch 4322 : 0.37404438853263855\n",
      "Training loss for batch 4323 : 0.13412611186504364\n",
      "Training loss for batch 4324 : 0.47450363636016846\n",
      "Training loss for batch 4325 : 0.0\n",
      "Training loss for batch 4326 : 0.11721503734588623\n",
      "Training loss for batch 4327 : 0.018755581229925156\n",
      "Training loss for batch 4328 : 0.19358602166175842\n",
      "Training loss for batch 4329 : 0.12380810081958771\n",
      "Training loss for batch 4330 : 0.020217102020978928\n",
      "Training loss for batch 4331 : 0.1882084459066391\n",
      "Training loss for batch 4332 : 0.19706225395202637\n",
      "Training loss for batch 4333 : 0.02288195863366127\n",
      "Training loss for batch 4334 : 0.2999120354652405\n",
      "Training loss for batch 4335 : 0.20295685529708862\n",
      "Training loss for batch 4336 : 0.2262442708015442\n",
      "Training loss for batch 4337 : 0.0041493079625070095\n",
      "Training loss for batch 4338 : 0.0661076232790947\n",
      "Training loss for batch 4339 : 0.25758078694343567\n",
      "Training loss for batch 4340 : 0.20008520781993866\n",
      "Training loss for batch 4341 : 0.08840981870889664\n",
      "Training loss for batch 4342 : 0.11119505763053894\n",
      "Training loss for batch 4343 : 0.118630051612854\n",
      "Training loss for batch 4344 : 0.0783829540014267\n",
      "Training loss for batch 4345 : 0.21556854248046875\n",
      "Training loss for batch 4346 : 0.29329079389572144\n",
      "Training loss for batch 4347 : 0.20828072726726532\n",
      "Training loss for batch 4348 : 0.054462283849716187\n",
      "Training loss for batch 4349 : 0.14602641761302948\n",
      "Training loss for batch 4350 : 0.05494417995214462\n",
      "Training loss for batch 4351 : 0.04576420783996582\n",
      "Training loss for batch 4352 : 0.009379546158015728\n",
      "Training loss for batch 4353 : 0.26011303067207336\n",
      "Training loss for batch 4354 : 0.09210284799337387\n",
      "Training loss for batch 4355 : 0.1030520647764206\n",
      "Training loss for batch 4356 : 0.2438698410987854\n",
      "Training loss for batch 4357 : 0.054395198822021484\n",
      "Training loss for batch 4358 : 0.5947158932685852\n",
      "Training loss for batch 4359 : 0.139121413230896\n",
      "Training loss for batch 4360 : 0.33273598551750183\n",
      "Training loss for batch 4361 : 0.21471545100212097\n",
      "Training loss for batch 4362 : 0.16278012096881866\n",
      "Training loss for batch 4363 : 0.11524999141693115\n",
      "Training loss for batch 4364 : 0.0815364271402359\n",
      "Training loss for batch 4365 : 0.22817207872867584\n",
      "Training loss for batch 4366 : 0.15426911413669586\n",
      "Training loss for batch 4367 : 0.13076961040496826\n",
      "Training loss for batch 4368 : 0.15743054449558258\n",
      "Training loss for batch 4369 : 0.28306981921195984\n",
      "Training loss for batch 4370 : 0.19320416450500488\n",
      "Training loss for batch 4371 : 0.03829842060804367\n",
      "Training loss for batch 4372 : 0.025976702570915222\n",
      "Training loss for batch 4373 : 0.11562775075435638\n",
      "Training loss for batch 4374 : 0.10325352102518082\n",
      "Training loss for batch 4375 : 0.27988162636756897\n",
      "Training loss for batch 4376 : 0.2470487654209137\n",
      "Training loss for batch 4377 : 0.09190814942121506\n",
      "Training loss for batch 4378 : 0.028433941304683685\n",
      "Training loss for batch 4379 : 0.31248435378074646\n",
      "Training loss for batch 4380 : 0.1551111787557602\n",
      "Training loss for batch 4381 : 0.1284259557723999\n",
      "Training loss for batch 4382 : 0.19081157445907593\n",
      "Training loss for batch 4383 : 0.4731774628162384\n",
      "Training loss for batch 4384 : 0.14060485363006592\n",
      "Training loss for batch 4385 : 0.12889990210533142\n",
      "Training loss for batch 4386 : 0.3377106487751007\n",
      "Training loss for batch 4387 : 0.1713637411594391\n",
      "Training loss for batch 4388 : 0.01672404631972313\n",
      "Training loss for batch 4389 : 0.06690885126590729\n",
      "Training loss for batch 4390 : 0.08725269883871078\n",
      "Training loss for batch 4391 : 0.14875918626785278\n",
      "Training loss for batch 4392 : 0.2653498351573944\n",
      "Training loss for batch 4393 : 0.2446959763765335\n",
      "Training loss for batch 4394 : 0.3638460338115692\n",
      "Training loss for batch 4395 : 0.15687058866024017\n",
      "Training loss for batch 4396 : 0.4631142318248749\n",
      "Training loss for batch 4397 : 0.0637771487236023\n",
      "Training loss for batch 4398 : 0.05121345445513725\n",
      "Training loss for batch 4399 : 0.370743066072464\n",
      "Training loss for batch 4400 : 0.3385315537452698\n",
      "Training loss for batch 4401 : 0.20455454289913177\n",
      "Training loss for batch 4402 : 0.00660153292119503\n",
      "Training loss for batch 4403 : 0.14592057466506958\n",
      "Training loss for batch 4404 : 0.1779433637857437\n",
      "Training loss for batch 4405 : 0.0802266001701355\n",
      "Training loss for batch 4406 : 0.0340934619307518\n",
      "Training loss for batch 4407 : 0.3557485342025757\n",
      "Training loss for batch 4408 : 0.006169488187879324\n",
      "Training loss for batch 4409 : 0.12365581840276718\n",
      "Training loss for batch 4410 : 0.16466256976127625\n",
      "Training loss for batch 4411 : 0.05163273960351944\n",
      "Training loss for batch 4412 : 0.28355127573013306\n",
      "Training loss for batch 4413 : 0.1090470403432846\n",
      "Training loss for batch 4414 : 0.06354997307062149\n",
      "Training loss for batch 4415 : 0.04214492067694664\n",
      "Training loss for batch 4416 : 0.13085903227329254\n",
      "Training loss for batch 4417 : 0.03507695719599724\n",
      "Training loss for batch 4418 : 0.1248321458697319\n",
      "Training loss for batch 4419 : 0.4844454824924469\n",
      "Training loss for batch 4420 : 0.12035392224788666\n",
      "Training loss for batch 4421 : 0.04472469165921211\n",
      "Training loss for batch 4422 : 0.174136221408844\n",
      "Training loss for batch 4423 : 0.7042728662490845\n",
      "Training loss for batch 4424 : 0.29812854528427124\n",
      "Training loss for batch 4425 : 0.18862521648406982\n",
      "Training loss for batch 4426 : 0.15162454545497894\n",
      "Training loss for batch 4427 : 0.06111517921090126\n",
      "Training loss for batch 4428 : 0.11319583654403687\n",
      "Training loss for batch 4429 : 0.029135949909687042\n",
      "Training loss for batch 4430 : 0.0586511567234993\n",
      "Training loss for batch 4431 : 0.32373133301734924\n",
      "Training loss for batch 4432 : 0.2024432271718979\n",
      "Training loss for batch 4433 : 0.03754076361656189\n",
      "Training loss for batch 4434 : 0.021229198202490807\n",
      "Training loss for batch 4435 : 0.045494481921195984\n",
      "Training loss for batch 4436 : 0.10588739812374115\n",
      "Training loss for batch 4437 : 0.07900169491767883\n",
      "Training loss for batch 4438 : 0.45987561345100403\n",
      "Training loss for batch 4439 : 0.11992251873016357\n",
      "Training loss for batch 4440 : 0.14722537994384766\n",
      "Training loss for batch 4441 : 0.38283729553222656\n",
      "Training loss for batch 4442 : 0.5174902081489563\n",
      "Training loss for batch 4443 : 0.18513262271881104\n",
      "Training loss for batch 4444 : 0.48593366146087646\n",
      "Training loss for batch 4445 : 0.3622889518737793\n",
      "Training loss for batch 4446 : 0.19552558660507202\n",
      "Training loss for batch 4447 : 0.13306719064712524\n",
      "Training loss for batch 4448 : 0.32293087244033813\n",
      "Training loss for batch 4449 : 0.22536711394786835\n",
      "Training loss for batch 4450 : 0.052425362169742584\n",
      "Training loss for batch 4451 : 0.41972285509109497\n",
      "Training loss for batch 4452 : 0.0023097703233361244\n",
      "Training loss for batch 4453 : 0.21475063264369965\n",
      "Training loss for batch 4454 : 0.16372154653072357\n",
      "Training loss for batch 4455 : 0.19731217622756958\n",
      "Training loss for batch 4456 : 0.27370190620422363\n",
      "Training loss for batch 4457 : 0.16766341030597687\n",
      "Training loss for batch 4458 : 0.10392718017101288\n",
      "Training loss for batch 4459 : 0.13114023208618164\n",
      "Training loss for batch 4460 : 0.34418413043022156\n",
      "Training loss for batch 4461 : 0.0495312362909317\n",
      "Training loss for batch 4462 : 0.301950603723526\n",
      "Training loss for batch 4463 : 0.07148214429616928\n",
      "Training loss for batch 4464 : 0.27941256761550903\n",
      "Training loss for batch 4465 : 0.11564303934574127\n",
      "Training loss for batch 4466 : 0.2101207822561264\n",
      "Training loss for batch 4467 : 0.17454567551612854\n",
      "Training loss for batch 4468 : 0.21987180411815643\n",
      "Training loss for batch 4469 : 0.09975690394639969\n",
      "Training loss for batch 4470 : 0.09127689898014069\n",
      "Training loss for batch 4471 : 0.13426470756530762\n",
      "Training loss for batch 4472 : 0.23165659606456757\n",
      "Training loss for batch 4473 : 0.601830005645752\n",
      "Training loss for batch 4474 : 0.11574621498584747\n",
      "Training loss for batch 4475 : 0.2802543342113495\n",
      "Training loss for batch 4476 : 0.06042523682117462\n",
      "Training loss for batch 4477 : 0.04676654934883118\n",
      "Training loss for batch 4478 : 0.053644709289073944\n",
      "Training loss for batch 4479 : 0.17153935134410858\n",
      "Training loss for batch 4480 : 0.14503644406795502\n",
      "Training loss for batch 4481 : 0.33504217863082886\n",
      "Training loss for batch 4482 : 0.42810389399528503\n",
      "Training loss for batch 4483 : 0.049451395869255066\n",
      "Training loss for batch 4484 : 0.4927266240119934\n",
      "Training loss for batch 4485 : 0.2262093424797058\n",
      "Training loss for batch 4486 : 0.45560914278030396\n",
      "Training loss for batch 4487 : 0.21944169700145721\n",
      "Training loss for batch 4488 : 0.36643895506858826\n",
      "Training loss for batch 4489 : 0.22806957364082336\n",
      "Training loss for batch 4490 : 0.16180799901485443\n",
      "Training loss for batch 4491 : 0.08553172647953033\n",
      "Training loss for batch 4492 : 0.03035353682935238\n",
      "Training loss for batch 4493 : 0.35430076718330383\n",
      "Training loss for batch 4494 : 0.02365683950483799\n",
      "Training loss for batch 4495 : 0.1926880031824112\n",
      "Training loss for batch 4496 : 0.18987466394901276\n",
      "Training loss for batch 4497 : 0.182844877243042\n",
      "Training loss for batch 4498 : 0.060620274394750595\n",
      "Training loss for batch 4499 : 0.20096652209758759\n",
      "Training loss for batch 4500 : 0.3360753655433655\n",
      "Training loss for batch 4501 : 0.13528203964233398\n",
      "Training loss for batch 4502 : 0.008305654861032963\n",
      "Training loss for batch 4503 : 0.11674512177705765\n",
      "Training loss for batch 4504 : 0.3201253414154053\n",
      "Training loss for batch 4505 : 0.2226722240447998\n",
      "Training loss for batch 4506 : 0.1295388787984848\n",
      "Training loss for batch 4507 : 0.09335477650165558\n",
      "Training loss for batch 4508 : 0.2043587863445282\n",
      "Training loss for batch 4509 : 0.3507026433944702\n",
      "Training loss for batch 4510 : 0.01918676868081093\n",
      "Training loss for batch 4511 : 0.07601016014814377\n",
      "Training loss for batch 4512 : 0.476467102766037\n",
      "Training loss for batch 4513 : 0.9062764644622803\n",
      "Training loss for batch 4514 : 0.08768291026353836\n",
      "Training loss for batch 4515 : 0.061173662543296814\n",
      "Training loss for batch 4516 : 0.40818795561790466\n",
      "Training loss for batch 4517 : 0.05277413874864578\n",
      "Training loss for batch 4518 : 0.1867387592792511\n",
      "Training loss for batch 4519 : 0.25799325108528137\n",
      "Training loss for batch 4520 : 0.04161754250526428\n",
      "Training loss for batch 4521 : 0.1113525852560997\n",
      "Training loss for batch 4522 : 0.14402669668197632\n",
      "Training loss for batch 4523 : 0.24527496099472046\n",
      "Training loss for batch 4524 : 0.27966105937957764\n",
      "Training loss for batch 4525 : 0.3004966676235199\n",
      "Training loss for batch 4526 : 0.048415057361125946\n",
      "Training loss for batch 4527 : 0.250020295381546\n",
      "Training loss for batch 4528 : 0.2940351963043213\n",
      "Training loss for batch 4529 : 0.25206780433654785\n",
      "Training loss for batch 4530 : 0.11423052847385406\n",
      "Training loss for batch 4531 : 0.10840190201997757\n",
      "Training loss for batch 4532 : 0.09043532609939575\n",
      "Training loss for batch 4533 : 0.12855468690395355\n",
      "Training loss for batch 4534 : 0.033549390733242035\n",
      "Training loss for batch 4535 : 0.20486657321453094\n",
      "Training loss for batch 4536 : 0.030976636335253716\n",
      "Training loss for batch 4537 : 0.13757172226905823\n",
      "Training loss for batch 4538 : 0.1617249846458435\n",
      "Training loss for batch 4539 : 0.21948474645614624\n",
      "Training loss for batch 4540 : 0.08606307208538055\n",
      "Training loss for batch 4541 : 0.5619828104972839\n",
      "Training loss for batch 4542 : 0.07048193365335464\n",
      "Training loss for batch 4543 : 0.14065316319465637\n",
      "Training loss for batch 4544 : 0.13764670491218567\n",
      "Training loss for batch 4545 : 0.07997462153434753\n",
      "Training loss for batch 4546 : 0.0043355789966881275\n",
      "Training loss for batch 4547 : 0.2717071771621704\n",
      "Training loss for batch 4548 : 0.045524004846811295\n",
      "Training loss for batch 4549 : 0.13672436773777008\n",
      "Training loss for batch 4550 : 0.22221602499485016\n",
      "Training loss for batch 4551 : 0.35894057154655457\n",
      "Training loss for batch 4552 : 0.1097458228468895\n",
      "Training loss for batch 4553 : 0.023480743169784546\n",
      "Training loss for batch 4554 : 0.30309170484542847\n",
      "Training loss for batch 4555 : 0.050598904490470886\n",
      "Training loss for batch 4556 : 0.021946966648101807\n",
      "Training loss for batch 4557 : 0.37234219908714294\n",
      "Training loss for batch 4558 : 0.22082197666168213\n",
      "Training loss for batch 4559 : 0.3823416233062744\n",
      "Training loss for batch 4560 : 0.09771820157766342\n",
      "Training loss for batch 4561 : 0.18049654364585876\n",
      "Training loss for batch 4562 : 0.009642223827540874\n",
      "Training loss for batch 4563 : 0.19536042213439941\n",
      "Training loss for batch 4564 : 0.0387105718255043\n",
      "Training loss for batch 4565 : 0.07773841172456741\n",
      "Training loss for batch 4566 : 0.08208441734313965\n",
      "Training loss for batch 4567 : 0.32123658061027527\n",
      "Training loss for batch 4568 : 0.047779519110918045\n",
      "Training loss for batch 4569 : 0.4549259841442108\n",
      "Training loss for batch 4570 : 0.38979679346084595\n",
      "Training loss for batch 4571 : 0.13179071247577667\n",
      "Training loss for batch 4572 : 0.03222350403666496\n",
      "Training loss for batch 4573 : 0.053836382925510406\n",
      "Training loss for batch 4574 : 0.004837771411985159\n",
      "Training loss for batch 4575 : 0.10047677904367447\n",
      "Training loss for batch 4576 : 0.12579430639743805\n",
      "Training loss for batch 4577 : 0.4214477837085724\n",
      "Training loss for batch 4578 : 0.08987241238355637\n",
      "Training loss for batch 4579 : 0.06622955948114395\n",
      "Training loss for batch 4580 : 0.050500109791755676\n",
      "Training loss for batch 4581 : 0.08142930269241333\n",
      "Training loss for batch 4582 : 0.2743975520133972\n",
      "Training loss for batch 4583 : 0.17289826273918152\n",
      "Training loss for batch 4584 : 0.19376502931118011\n",
      "Training loss for batch 4585 : 0.1467374712228775\n",
      "Training loss for batch 4586 : 0.05414237827062607\n",
      "Training loss for batch 4587 : 0.27599194645881653\n",
      "Training loss for batch 4588 : 0.1269647181034088\n",
      "Training loss for batch 4589 : 0.18349990248680115\n",
      "Training loss for batch 4590 : 0.24560502171516418\n",
      "Training loss for batch 4591 : 0.04305737465620041\n",
      "Training loss for batch 4592 : 0.13731618225574493\n",
      "Training loss for batch 4593 : 0.24295473098754883\n",
      "Training loss for batch 4594 : 0.06538243591785431\n",
      "Training loss for batch 4595 : 0.2027934491634369\n",
      "Training loss for batch 4596 : 0.09564432501792908\n",
      "Training loss for batch 4597 : 0.4310266375541687\n",
      "Training loss for batch 4598 : 0.09395124018192291\n",
      "Training loss for batch 4599 : 0.2099265605211258\n",
      "Training loss for batch 4600 : 0.4364924728870392\n",
      "Training loss for batch 4601 : 0.20388929545879364\n",
      "Training loss for batch 4602 : 0.07063086330890656\n",
      "Training loss for batch 4603 : 0.17779049277305603\n",
      "Training loss for batch 4604 : 0.1242903470993042\n",
      "Training loss for batch 4605 : 0.12272030860185623\n",
      "Training loss for batch 4606 : 0.2530451714992523\n",
      "Training loss for batch 4607 : 0.20929312705993652\n",
      "Training loss for batch 4608 : 0.2554989457130432\n",
      "Training loss for batch 4609 : 0.2902733385562897\n",
      "Training loss for batch 4610 : 0.21832188963890076\n",
      "Training loss for batch 4611 : 0.4707382321357727\n",
      "Training loss for batch 4612 : 0.12134602665901184\n",
      "Training loss for batch 4613 : 0.011548414826393127\n",
      "Training loss for batch 4614 : 0.16218265891075134\n",
      "Training loss for batch 4615 : 0.12337671220302582\n",
      "Training loss for batch 4616 : 0.31810346245765686\n",
      "Training loss for batch 4617 : 0.1781950145959854\n",
      "Training loss for batch 4618 : 0.2011093944311142\n",
      "Training loss for batch 4619 : 0.2849746644496918\n",
      "Training loss for batch 4620 : 0.19193029403686523\n",
      "Training loss for batch 4621 : 0.20732863247394562\n",
      "Training loss for batch 4622 : 0.11666136980056763\n",
      "Training loss for batch 4623 : 0.13906964659690857\n",
      "Training loss for batch 4624 : 0.2943970263004303\n",
      "Training loss for batch 4625 : 0.06960155814886093\n",
      "Training loss for batch 4626 : 0.0\n",
      "Training loss for batch 4627 : 0.3554016947746277\n",
      "Training loss for batch 4628 : 0.15714876353740692\n",
      "Training loss for batch 4629 : 0.2141191065311432\n",
      "Training loss for batch 4630 : 0.1573597937822342\n",
      "Training loss for batch 4631 : 0.36339041590690613\n",
      "Training loss for batch 4632 : 0.3491860628128052\n",
      "Training loss for batch 4633 : 0.2690463066101074\n",
      "Training loss for batch 4634 : 0.15915408730506897\n",
      "Training loss for batch 4635 : 0.22093577682971954\n",
      "Training loss for batch 4636 : 0.3387637436389923\n",
      "Training loss for batch 4637 : 0.17261944711208344\n",
      "Training loss for batch 4638 : 0.3159046769142151\n",
      "Training loss for batch 4639 : 0.07399733364582062\n",
      "Training loss for batch 4640 : 0.015367579646408558\n",
      "Training loss for batch 4641 : 0.16911181807518005\n",
      "Training loss for batch 4642 : 0.10115472972393036\n",
      "Training loss for batch 4643 : 0.07379467785358429\n",
      "Training loss for batch 4644 : 0.09960923343896866\n",
      "Training loss for batch 4645 : 0.22879087924957275\n",
      "Training loss for batch 4646 : 0.4485465884208679\n",
      "Training loss for batch 4647 : 0.4434601068496704\n",
      "Training loss for batch 4648 : 0.14242731034755707\n",
      "Training loss for batch 4649 : 0.2943073809146881\n",
      "Training loss for batch 4650 : 0.0626673698425293\n",
      "Training loss for batch 4651 : 0.028633510693907738\n",
      "Training loss for batch 4652 : 0.06065833196043968\n",
      "Training loss for batch 4653 : 0.06607038527727127\n",
      "Training loss for batch 4654 : 0.12934549152851105\n",
      "Training loss for batch 4655 : 0.16408191621303558\n",
      "Training loss for batch 4656 : 0.12781615555286407\n",
      "Training loss for batch 4657 : 0.38795584440231323\n",
      "Training loss for batch 4658 : 0.12803910672664642\n",
      "Training loss for batch 4659 : 0.44994837045669556\n",
      "Training loss for batch 4660 : 0.2074839472770691\n",
      "Training loss for batch 4661 : 0.15931832790374756\n",
      "Training loss for batch 4662 : 0.1665935516357422\n",
      "Training loss for batch 4663 : 0.07476469874382019\n",
      "Training loss for batch 4664 : 0.18691609799861908\n",
      "Training loss for batch 4665 : 0.009777466766536236\n",
      "Training loss for batch 4666 : 0.25288090109825134\n",
      "Training loss for batch 4667 : 0.12309267371892929\n",
      "Training loss for batch 4668 : 0.17237786948680878\n",
      "Training loss for batch 4669 : 0.2516307532787323\n",
      "Training loss for batch 4670 : 0.3869971036911011\n",
      "Training loss for batch 4671 : 0.3730975389480591\n",
      "Training loss for batch 4672 : 0.03636749088764191\n",
      "Training loss for batch 4673 : 0.06268646568059921\n",
      "Training loss for batch 4674 : 0.35605350136756897\n",
      "Training loss for batch 4675 : 0.200494647026062\n",
      "Training loss for batch 4676 : 0.07330596446990967\n",
      "Training loss for batch 4677 : 0.03068550117313862\n",
      "Training loss for batch 4678 : 0.08381016552448273\n",
      "Training loss for batch 4679 : 0.414290189743042\n",
      "Training loss for batch 4680 : 0.11009244620800018\n",
      "Training loss for batch 4681 : 0.1628756821155548\n",
      "Training loss for batch 4682 : 0.2976125478744507\n",
      "Training loss for batch 4683 : 0.2588113844394684\n",
      "Training loss for batch 4684 : 0.050475358963012695\n",
      "Training loss for batch 4685 : 0.12459169328212738\n",
      "Training loss for batch 4686 : 0.15772676467895508\n",
      "Training loss for batch 4687 : 0.15174932777881622\n",
      "Training loss for batch 4688 : 0.11720547080039978\n",
      "Training loss for batch 4689 : 0.049834053963422775\n",
      "Training loss for batch 4690 : 0.47256705164909363\n",
      "Training loss for batch 4691 : 0.19679449498653412\n",
      "Training loss for batch 4692 : 0.036099351942539215\n",
      "Training loss for batch 4693 : 0.06251727789640427\n",
      "Training loss for batch 4694 : 0.0\n",
      "Training loss for batch 4695 : 0.1268826127052307\n",
      "Training loss for batch 4696 : 0.05349648743867874\n",
      "Training loss for batch 4697 : 0.08226229250431061\n",
      "Training loss for batch 4698 : 0.05275813490152359\n",
      "Training loss for batch 4699 : 0.073735311627388\n",
      "Training loss for batch 4700 : 0.11103186756372452\n",
      "Training loss for batch 4701 : 0.43972596526145935\n",
      "Training loss for batch 4702 : 0.07066912949085236\n",
      "Training loss for batch 4703 : 0.07537361234426498\n",
      "Training loss for batch 4704 : 0.3363223373889923\n",
      "Training loss for batch 4705 : 0.04950474947690964\n",
      "Training loss for batch 4706 : 0.08042941242456436\n",
      "Training loss for batch 4707 : 0.09043267369270325\n",
      "Training loss for batch 4708 : 0.2549223005771637\n",
      "Training loss for batch 4709 : 0.05879393592476845\n",
      "Training loss for batch 4710 : 0.09187034517526627\n",
      "Training loss for batch 4711 : 0.15029826760292053\n",
      "Training loss for batch 4712 : 0.21623587608337402\n",
      "Training loss for batch 4713 : 0.20905210077762604\n",
      "Training loss for batch 4714 : 0.07327340543270111\n",
      "Training loss for batch 4715 : 0.04549519717693329\n",
      "Training loss for batch 4716 : 0.14587759971618652\n",
      "Training loss for batch 4717 : 0.01875796541571617\n",
      "Training loss for batch 4718 : 0.11692731082439423\n",
      "Training loss for batch 4719 : 0.0721825435757637\n",
      "Training loss for batch 4720 : 0.015408682636916637\n",
      "Training loss for batch 4721 : 0.23380112648010254\n",
      "Training loss for batch 4722 : 0.079636350274086\n",
      "Training loss for batch 4723 : 0.31100571155548096\n",
      "Training loss for batch 4724 : 0.10420015454292297\n",
      "Training loss for batch 4725 : 0.07057276368141174\n",
      "Training loss for batch 4726 : 0.110502228140831\n",
      "Training loss for batch 4727 : 0.13960576057434082\n",
      "Training loss for batch 4728 : 0.13457530736923218\n",
      "Training loss for batch 4729 : 0.16226887702941895\n",
      "Training loss for batch 4730 : 0.1782410889863968\n",
      "Training loss for batch 4731 : 0.18370464444160461\n",
      "Training loss for batch 4732 : 0.0763310045003891\n",
      "Training loss for batch 4733 : 0.14272266626358032\n",
      "Training loss for batch 4734 : 0.040737323462963104\n",
      "Training loss for batch 4735 : -0.0005112092476338148\n",
      "Training loss for batch 4736 : 0.49830466508865356\n",
      "Training loss for batch 4737 : 0.04075760394334793\n",
      "Training loss for batch 4738 : 0.15952543914318085\n",
      "Training loss for batch 4739 : 0.1931835412979126\n",
      "Training loss for batch 4740 : 0.013419259339571\n",
      "Training loss for batch 4741 : 0.3968212902545929\n",
      "Training loss for batch 4742 : 0.04307190328836441\n",
      "Training loss for batch 4743 : 0.21082238852977753\n",
      "Training loss for batch 4744 : 0.07419166713953018\n",
      "Training loss for batch 4745 : 0.08383466303348541\n",
      "Training loss for batch 4746 : 0.15576261281967163\n",
      "Training loss for batch 4747 : 0.06472562253475189\n",
      "Training loss for batch 4748 : 0.48780858516693115\n",
      "Training loss for batch 4749 : 0.32950207591056824\n",
      "Training loss for batch 4750 : 0.2745339274406433\n",
      "Training loss for batch 4751 : 0.13189777731895447\n",
      "Training loss for batch 4752 : 0.1084713414311409\n",
      "Training loss for batch 4753 : 0.41052186489105225\n",
      "Training loss for batch 4754 : 0.15782402455806732\n",
      "Training loss for batch 4755 : 0.2988585829734802\n",
      "Training loss for batch 4756 : 0.3813019096851349\n",
      "Training loss for batch 4757 : 0.25516340136528015\n",
      "Training loss for batch 4758 : 0.20092083513736725\n",
      "Training loss for batch 4759 : 0.10758554190397263\n",
      "Training loss for batch 4760 : 0.3596382439136505\n",
      "Training loss for batch 4761 : 0.2603198289871216\n",
      "Training loss for batch 4762 : 0.2573716938495636\n",
      "Training loss for batch 4763 : 0.1663263887166977\n",
      "Training loss for batch 4764 : 0.12378579378128052\n",
      "Training loss for batch 4765 : 0.39742031693458557\n",
      "Training loss for batch 4766 : 0.3922238051891327\n",
      "Training loss for batch 4767 : 0.0027935008984059095\n",
      "Training loss for batch 4768 : 0.0\n",
      "Training loss for batch 4769 : 0.021644528955221176\n",
      "Training loss for batch 4770 : 0.4313787817955017\n",
      "Training loss for batch 4771 : 0.11668072640895844\n",
      "Training loss for batch 4772 : 0.05345441401004791\n",
      "Training loss for batch 4773 : 0.32401806116104126\n",
      "Training loss for batch 4774 : 0.0\n",
      "Training loss for batch 4775 : 0.06897146999835968\n",
      "Training loss for batch 4776 : 0.16273386776447296\n",
      "Training loss for batch 4777 : 0.026913058012723923\n",
      "Training loss for batch 4778 : 0.04182751476764679\n",
      "Training loss for batch 4779 : 0.16301625967025757\n",
      "Training loss for batch 4780 : 0.18682928383350372\n",
      "Training loss for batch 4781 : 0.0860590785741806\n",
      "Training loss for batch 4782 : 0.057140737771987915\n",
      "Training loss for batch 4783 : 0.15979357063770294\n",
      "Training loss for batch 4784 : 0.3818657100200653\n",
      "Training loss for batch 4785 : 0.07253430783748627\n",
      "Training loss for batch 4786 : 0.04369330406188965\n",
      "Training loss for batch 4787 : 0.155663400888443\n",
      "Training loss for batch 4788 : 0.3481062948703766\n",
      "Training loss for batch 4789 : 0.13231994211673737\n",
      "Training loss for batch 4790 : 0.22287532687187195\n",
      "Training loss for batch 4791 : 0.20426905155181885\n",
      "Training loss for batch 4792 : 0.13164466619491577\n",
      "Training loss for batch 4793 : 0.013825677335262299\n",
      "Training loss for batch 4794 : 0.08698111027479172\n",
      "Training loss for batch 4795 : 0.13399949669837952\n",
      "Training loss for batch 4796 : 0.05689384788274765\n",
      "Training loss for batch 4797 : 0.12450344115495682\n",
      "Training loss for batch 4798 : 0.05600770190358162\n",
      "Training loss for batch 4799 : 0.10724910348653793\n",
      "Training loss for batch 4800 : 0.15816247463226318\n",
      "Training loss for batch 4801 : 0.20193536579608917\n",
      "Training loss for batch 4802 : 0.06457975506782532\n",
      "Training loss for batch 4803 : 0.0743955448269844\n",
      "Training loss for batch 4804 : 0.16399024426937103\n",
      "Training loss for batch 4805 : 0.08372647315263748\n",
      "Training loss for batch 4806 : 0.16633765399456024\n",
      "Training loss for batch 4807 : 0.14323320984840393\n",
      "Training loss for batch 4808 : 0.055564913898706436\n",
      "Training loss for batch 4809 : 0.01983102224767208\n",
      "Training loss for batch 4810 : 0.1280883550643921\n",
      "Training loss for batch 4811 : 0.16673986613750458\n",
      "Training loss for batch 4812 : 0.056961655616760254\n",
      "Training loss for batch 4813 : 0.24948418140411377\n",
      "Training loss for batch 4814 : 0.34438392519950867\n",
      "Training loss for batch 4815 : 0.03376816213130951\n",
      "Training loss for batch 4816 : 0.2841126024723053\n",
      "Training loss for batch 4817 : 0.2073109745979309\n",
      "Training loss for batch 4818 : 0.33370694518089294\n",
      "Training loss for batch 4819 : 0.07840834558010101\n",
      "Training loss for batch 4820 : 0.0160432830452919\n",
      "Training loss for batch 4821 : 0.237533837556839\n",
      "Training loss for batch 4822 : 0.17395704984664917\n",
      "Training loss for batch 4823 : 0.28383544087409973\n",
      "Training loss for batch 4824 : 0.27385029196739197\n",
      "Training loss for batch 4825 : 0.0173394363373518\n",
      "Training loss for batch 4826 : 0.08248821645975113\n",
      "Training loss for batch 4827 : 0.07136422395706177\n",
      "Training loss for batch 4828 : 0.1747131049633026\n",
      "Training loss for batch 4829 : 0.02423405833542347\n",
      "Training loss for batch 4830 : 0.022802773863077164\n",
      "Training loss for batch 4831 : 0.0074903881177306175\n",
      "Training loss for batch 4832 : 0.10362269729375839\n",
      "Training loss for batch 4833 : 0.05063348263502121\n",
      "Training loss for batch 4834 : 0.28820866346359253\n",
      "Training loss for batch 4835 : 0.3999447226524353\n",
      "Training loss for batch 4836 : 0.4277520179748535\n",
      "Training loss for batch 4837 : 0.04724759981036186\n",
      "Training loss for batch 4838 : 0.34722596406936646\n",
      "Training loss for batch 4839 : 0.23230811953544617\n",
      "Training loss for batch 4840 : 0.08520601689815521\n",
      "Training loss for batch 4841 : 0.22783127427101135\n",
      "Training loss for batch 4842 : 0.10906288027763367\n",
      "Training loss for batch 4843 : 0.20557335019111633\n",
      "Training loss for batch 4844 : 0.027654150500893593\n",
      "Training loss for batch 4845 : 0.24128706753253937\n",
      "Training loss for batch 4846 : 0.21970771253108978\n",
      "Training loss for batch 4847 : 0.019250700250267982\n",
      "Training loss for batch 4848 : 0.10842057317495346\n",
      "Training loss for batch 4849 : 0.12481250613927841\n",
      "Training loss for batch 4850 : 0.12487161159515381\n",
      "Training loss for batch 4851 : 0.3219403028488159\n",
      "Training loss for batch 4852 : 0.11217033118009567\n",
      "Training loss for batch 4853 : 0.01445674803107977\n",
      "Training loss for batch 4854 : 0.10787562280893326\n",
      "Training loss for batch 4855 : 0.10816550254821777\n",
      "Training loss for batch 4856 : 0.43883031606674194\n",
      "Training loss for batch 4857 : 0.30163300037384033\n",
      "Training loss for batch 4858 : 0.30846813321113586\n",
      "Training loss for batch 4859 : 0.07104387879371643\n",
      "Training loss for batch 4860 : 0.10131798684597015\n",
      "Training loss for batch 4861 : 0.13019481301307678\n",
      "Training loss for batch 4862 : 0.12885408103466034\n",
      "Training loss for batch 4863 : 0.1376694291830063\n",
      "Training loss for batch 4864 : 0.26414260268211365\n",
      "Training loss for batch 4865 : 0.1592254638671875\n",
      "Training loss for batch 4866 : 0.09514877945184708\n",
      "Training loss for batch 4867 : 0.2093181014060974\n",
      "Training loss for batch 4868 : 0.032501496374607086\n",
      "Training loss for batch 4869 : 0.08487942814826965\n",
      "Training loss for batch 4870 : 0.1683342009782791\n",
      "Training loss for batch 4871 : 0.16138388216495514\n",
      "Training loss for batch 4872 : 0.676444947719574\n",
      "Training loss for batch 4873 : 0.24982112646102905\n",
      "Training loss for batch 4874 : 0.3067111670970917\n",
      "Training loss for batch 4875 : 0.08678171783685684\n",
      "Training loss for batch 4876 : 0.14940424263477325\n",
      "Training loss for batch 4877 : 0.06804875284433365\n",
      "Training loss for batch 4878 : 0.1614699363708496\n",
      "Training loss for batch 4879 : 0.08722630143165588\n",
      "Training loss for batch 4880 : 0.19021527469158173\n",
      "Training loss for batch 4881 : 0.2750178277492523\n",
      "Training loss for batch 4882 : 0.34175407886505127\n",
      "Training loss for batch 4883 : 0.14663606882095337\n",
      "Training loss for batch 4884 : 0.22358304262161255\n",
      "Training loss for batch 4885 : 0.04271337389945984\n",
      "Training loss for batch 4886 : 0.3165469765663147\n",
      "Training loss for batch 4887 : 0.14588326215744019\n",
      "Training loss for batch 4888 : 0.14777778089046478\n",
      "Training loss for batch 4889 : 0.07985944300889969\n",
      "Training loss for batch 4890 : 0.22180336713790894\n",
      "Training loss for batch 4891 : 0.29201003909111023\n",
      "Training loss for batch 4892 : 0.2717777192592621\n",
      "Training loss for batch 4893 : 0.19587433338165283\n",
      "Training loss for batch 4894 : 0.10076797753572464\n",
      "Training loss for batch 4895 : 0.12851713597774506\n",
      "Training loss for batch 4896 : 0.17465901374816895\n",
      "Training loss for batch 4897 : 0.10543933510780334\n",
      "Training loss for batch 4898 : 0.20482437312602997\n",
      "Training loss for batch 4899 : 0.08624774217605591\n",
      "Training loss for batch 4900 : 0.2072337567806244\n",
      "Training loss for batch 4901 : 0.016824763268232346\n",
      "Training loss for batch 4902 : 0.017171543091535568\n",
      "Training loss for batch 4903 : 0.35417088866233826\n",
      "Training loss for batch 4904 : 0.17935635149478912\n",
      "Training loss for batch 4905 : 0.06004935875535011\n",
      "Training loss for batch 4906 : 0.2834637761116028\n",
      "Training loss for batch 4907 : 0.25860223174095154\n",
      "Training loss for batch 4908 : 0.2829316556453705\n",
      "Training loss for batch 4909 : 0.41745680570602417\n",
      "Training loss for batch 4910 : 0.36568883061408997\n",
      "Training loss for batch 4911 : 0.1005016341805458\n",
      "Training loss for batch 4912 : 0.08361721783876419\n",
      "Training loss for batch 4913 : 0.2732069492340088\n",
      "Training loss for batch 4914 : 0.09854647517204285\n",
      "Training loss for batch 4915 : 0.1745113730430603\n",
      "Training loss for batch 4916 : 0.47446778416633606\n",
      "Training loss for batch 4917 : 0.20117917656898499\n",
      "Training loss for batch 4918 : 0.0026852828450500965\n",
      "Training loss for batch 4919 : 0.18961673974990845\n",
      "Training loss for batch 4920 : 0.1261424720287323\n",
      "Training loss for batch 4921 : 0.1229788064956665\n",
      "Training loss for batch 4922 : 0.1177598312497139\n",
      "Training loss for batch 4923 : 0.3785454034805298\n",
      "Training loss for batch 4924 : 0.1498376727104187\n",
      "Training loss for batch 4925 : 0.5353140830993652\n",
      "Training loss for batch 4926 : 0.20331212878227234\n",
      "Training loss for batch 4927 : 0.22627975046634674\n",
      "Training loss for batch 4928 : 0.1593082845211029\n",
      "Training loss for batch 4929 : 0.2137855440378189\n",
      "Training loss for batch 4930 : 0.3046151399612427\n",
      "Training loss for batch 4931 : 0.37030160427093506\n",
      "Training loss for batch 4932 : 0.2882315218448639\n",
      "Training loss for batch 4933 : 0.2267485111951828\n",
      "Training loss for batch 4934 : 0.14056366682052612\n",
      "Training loss for batch 4935 : 0.2620369791984558\n",
      "Training loss for batch 4936 : 0.05497817322611809\n",
      "Training loss for batch 4937 : 0.17264306545257568\n",
      "Training loss for batch 4938 : 0.15121625363826752\n",
      "Training loss for batch 4939 : 0.0004834552528336644\n",
      "Training loss for batch 4940 : 0.23281781375408173\n",
      "Training loss for batch 4941 : 0.42913126945495605\n",
      "Training loss for batch 4942 : 0.09846986085176468\n",
      "Training loss for batch 4943 : 0.17494097352027893\n",
      "Training loss for batch 4944 : 0.03465534374117851\n",
      "Training loss for batch 4945 : 0.06329206377267838\n",
      "Training loss for batch 4946 : 0.014218996278941631\n",
      "Training loss for batch 4947 : 0.04083766043186188\n",
      "Training loss for batch 4948 : 0.1442025899887085\n",
      "Training loss for batch 4949 : 0.3532712757587433\n",
      "Training loss for batch 4950 : 0.04730330780148506\n",
      "Training loss for batch 4951 : 0.08034569025039673\n",
      "Training loss for batch 4952 : 0.16270187497138977\n",
      "Training loss for batch 4953 : 0.1493334323167801\n",
      "Training loss for batch 4954 : 0.3427456021308899\n",
      "Training loss for batch 4955 : 0.12593384087085724\n",
      "Training loss for batch 4956 : 0.31843748688697815\n",
      "Training loss for batch 4957 : 0.11438504606485367\n",
      "Training loss for batch 4958 : 0.07115491479635239\n",
      "Training loss for batch 4959 : 0.16424855589866638\n",
      "Training loss for batch 4960 : 0.013244198635220528\n",
      "Training loss for batch 4961 : 0.10164042562246323\n",
      "Training loss for batch 4962 : 0.23083284497261047\n",
      "Training loss for batch 4963 : 0.13980166614055634\n",
      "Training loss for batch 4964 : 0.07628637552261353\n",
      "Training loss for batch 4965 : 0.1262432187795639\n",
      "Training loss for batch 4966 : 0.3594636917114258\n",
      "Training loss for batch 4967 : 0.02095983549952507\n",
      "Training loss for batch 4968 : 0.3379921615123749\n",
      "Training loss for batch 4969 : 0.03209971636533737\n",
      "Training loss for batch 4970 : 0.2551092505455017\n",
      "Training loss for batch 4971 : 0.08104506880044937\n",
      "Training loss for batch 4972 : 0.22790297865867615\n",
      "Training loss for batch 4973 : 0.14289194345474243\n",
      "Training loss for batch 4974 : 0.15088456869125366\n",
      "Training loss for batch 4975 : 0.00504507590085268\n",
      "Training loss for batch 4976 : 0.13531209528446198\n",
      "Training loss for batch 4977 : 0.4353787899017334\n",
      "Training loss for batch 4978 : 0.1822877824306488\n",
      "Training loss for batch 4979 : 0.11444211006164551\n",
      "Training loss for batch 4980 : 0.09320662915706635\n",
      "Training loss for batch 4981 : 0.06592802703380585\n",
      "Training loss for batch 4982 : 0.03400978446006775\n",
      "Training loss for batch 4983 : 0.01666344329714775\n",
      "Training loss for batch 4984 : 0.0989510640501976\n",
      "Training loss for batch 4985 : 0.13782593607902527\n",
      "Training loss for batch 4986 : 0.13529729843139648\n",
      "Training loss for batch 4987 : 0.20989784598350525\n",
      "Training loss for batch 4988 : 0.38935762643814087\n",
      "Training loss for batch 4989 : 0.09981845319271088\n",
      "Training loss for batch 4990 : 0.1600448042154312\n",
      "Training loss for batch 4991 : 0.22938454151153564\n",
      "Training loss for batch 4992 : 0.32643383741378784\n",
      "Training loss for batch 4993 : 0.1508491188287735\n",
      "Training loss for batch 4994 : 0.5925107002258301\n",
      "Training loss for batch 4995 : 0.0520610548555851\n",
      "Training loss for batch 4996 : 0.15152330696582794\n",
      "Training loss for batch 4997 : 0.1251191943883896\n",
      "Training loss for batch 4998 : 0.11587285995483398\n",
      "Training loss for batch 4999 : 0.2833710312843323\n",
      "Training loss for batch 5000 : 0.0\n",
      "Training loss for batch 5001 : 0.013740813359618187\n",
      "Training loss for batch 5002 : 0.002002526307478547\n",
      "Training loss for batch 5003 : 0.057113949209451675\n",
      "Training loss for batch 5004 : 0.016417067497968674\n",
      "Training loss for batch 5005 : 0.15990571677684784\n",
      "Training loss for batch 5006 : 0.04654368385672569\n",
      "Training loss for batch 5007 : 0.08566169440746307\n",
      "Training loss for batch 5008 : 0.2897186875343323\n",
      "Training loss for batch 5009 : 0.17051491141319275\n",
      "Training loss for batch 5010 : 0.5032013654708862\n",
      "Training loss for batch 5011 : 0.15604105591773987\n",
      "Training loss for batch 5012 : 0.04379378259181976\n",
      "Training loss for batch 5013 : -0.0005149245262145996\n",
      "Training loss for batch 5014 : 0.2684939205646515\n",
      "Training loss for batch 5015 : 0.13226082921028137\n",
      "Training loss for batch 5016 : 0.1400497555732727\n",
      "Training loss for batch 5017 : 0.01094040460884571\n",
      "Training loss for batch 5018 : 0.11789269000291824\n",
      "Training loss for batch 5019 : 0.143515944480896\n",
      "Training loss for batch 5020 : 0.26686203479766846\n",
      "Training loss for batch 5021 : 0.28852808475494385\n",
      "Training loss for batch 5022 : 0.12607768177986145\n",
      "Training loss for batch 5023 : 0.15844281017780304\n",
      "Training loss for batch 5024 : 0.18353824317455292\n",
      "Training loss for batch 5025 : 0.16324636340141296\n",
      "Training loss for batch 5026 : 0.3771915137767792\n",
      "Training loss for batch 5027 : 0.4875679612159729\n",
      "Training loss for batch 5028 : 0.07427751272916794\n",
      "Training loss for batch 5029 : 0.0634925439953804\n",
      "Training loss for batch 5030 : 0.3138144314289093\n",
      "Training loss for batch 5031 : 0.1352749615907669\n",
      "Training loss for batch 5032 : 0.03299089893698692\n",
      "Training loss for batch 5033 : 0.0\n",
      "Training loss for batch 5034 : 0.04671839252114296\n",
      "Training loss for batch 5035 : 0.13145044445991516\n",
      "Training loss for batch 5036 : 0.4154413342475891\n",
      "Training loss for batch 5037 : 0.17772534489631653\n",
      "Training loss for batch 5038 : 0.6771357655525208\n",
      "Training loss for batch 5039 : 0.010176248848438263\n",
      "Training loss for batch 5040 : 0.2148750126361847\n",
      "Training loss for batch 5041 : 0.09061995148658752\n",
      "Training loss for batch 5042 : 0.1486181616783142\n",
      "Training loss for batch 5043 : 0.5119149684906006\n",
      "Training loss for batch 5044 : 0.3933311700820923\n",
      "Training loss for batch 5045 : 0.16976520419120789\n",
      "Training loss for batch 5046 : 0.17004284262657166\n",
      "Training loss for batch 5047 : 0.37900853157043457\n",
      "Training loss for batch 5048 : 0.28911903500556946\n",
      "Training loss for batch 5049 : 0.06920858472585678\n",
      "Training loss for batch 5050 : 0.39891478419303894\n",
      "Training loss for batch 5051 : 0.008305943571031094\n",
      "Training loss for batch 5052 : 0.02103278785943985\n",
      "Training loss for batch 5053 : 0.1998712122440338\n",
      "Training loss for batch 5054 : 0.0379871129989624\n",
      "Training loss for batch 5055 : 0.17430444061756134\n",
      "Training loss for batch 5056 : 0.16109022498130798\n",
      "Training loss for batch 5057 : 0.16947869956493378\n",
      "Training loss for batch 5058 : 0.21070052683353424\n",
      "Training loss for batch 5059 : 0.06762882322072983\n",
      "Training loss for batch 5060 : 0.25279808044433594\n",
      "Training loss for batch 5061 : 0.2592470049858093\n",
      "Training loss for batch 5062 : 0.10193296521902084\n",
      "Training loss for batch 5063 : 0.27207812666893005\n",
      "Training loss for batch 5064 : 0.021380404010415077\n",
      "Training loss for batch 5065 : 0.2303791344165802\n",
      "Training loss for batch 5066 : 0.3064150810241699\n",
      "Training loss for batch 5067 : 0.03565032407641411\n",
      "Training loss for batch 5068 : 0.387424498796463\n",
      "Training loss for batch 5069 : 0.03285866230726242\n",
      "Training loss for batch 5070 : 0.8317093253135681\n",
      "Training loss for batch 5071 : 0.04967918619513512\n",
      "Training loss for batch 5072 : 0.09663371741771698\n",
      "Training loss for batch 5073 : 0.3016587793827057\n",
      "Training loss for batch 5074 : 0.10788270086050034\n",
      "Training loss for batch 5075 : 0.2893209159374237\n",
      "Training loss for batch 5076 : 0.24953322112560272\n",
      "Training loss for batch 5077 : 0.34676238894462585\n",
      "Training loss for batch 5078 : 0.4817011058330536\n",
      "Training loss for batch 5079 : 0.14776600897312164\n",
      "Training loss for batch 5080 : 0.5904297828674316\n",
      "Training loss for batch 5081 : 0.06948699057102203\n",
      "Training loss for batch 5082 : 0.25572454929351807\n",
      "Training loss for batch 5083 : 0.4167643189430237\n",
      "Training loss for batch 5084 : 0.07100286334753036\n",
      "Training loss for batch 5085 : 0.17633365094661713\n",
      "Training loss for batch 5086 : 0.016953682526946068\n",
      "Training loss for batch 5087 : 8.895000064512715e-05\n",
      "Training loss for batch 5088 : 0.05594375729560852\n",
      "Training loss for batch 5089 : 0.21214039623737335\n",
      "Training loss for batch 5090 : 0.37460681796073914\n",
      "Training loss for batch 5091 : 0.604212760925293\n",
      "Training loss for batch 5092 : 0.04606838524341583\n",
      "Training loss for batch 5093 : 0.11887962371110916\n",
      "Training loss for batch 5094 : 0.3004065454006195\n",
      "Training loss for batch 5095 : 0.2965981364250183\n",
      "Training loss for batch 5096 : 0.2559093236923218\n",
      "Training loss for batch 5097 : 0.122097447514534\n",
      "Training loss for batch 5098 : 0.014473329298198223\n",
      "Training loss for batch 5099 : 0.022295458242297173\n",
      "Training loss for batch 5100 : 0.01715659722685814\n",
      "Training loss for batch 5101 : 0.08625385165214539\n",
      "Training loss for batch 5102 : 0.08298391848802567\n",
      "Training loss for batch 5103 : 0.030376756563782692\n",
      "Training loss for batch 5104 : 0.23316733539104462\n",
      "Training loss for batch 5105 : 0.19302597641944885\n",
      "Training loss for batch 5106 : 0.4647667706012726\n",
      "Training loss for batch 5107 : 0.07939866185188293\n",
      "Training loss for batch 5108 : 0.24911613762378693\n",
      "Training loss for batch 5109 : 0.3117440938949585\n",
      "Training loss for batch 5110 : 0.20931851863861084\n",
      "Training loss for batch 5111 : 0.3122127950191498\n",
      "Training loss for batch 5112 : 0.03596153110265732\n",
      "Training loss for batch 5113 : 0.18969422578811646\n",
      "Training loss for batch 5114 : 0.1066778227686882\n",
      "Training loss for batch 5115 : 0.31901559233665466\n",
      "Training loss for batch 5116 : 0.22443751990795135\n",
      "Training loss for batch 5117 : 0.14353416860103607\n",
      "Training loss for batch 5118 : 0.006684234365820885\n",
      "Training loss for batch 5119 : 0.20829984545707703\n",
      "Training loss for batch 5120 : 0.2869377136230469\n",
      "Training loss for batch 5121 : 0.12100441008806229\n",
      "Training loss for batch 5122 : 0.06907054781913757\n",
      "Training loss for batch 5123 : 0.0017474889755249023\n",
      "Training loss for batch 5124 : 0.2569688558578491\n",
      "Training loss for batch 5125 : 0.5600103735923767\n",
      "Training loss for batch 5126 : 0.08807535469532013\n",
      "Training loss for batch 5127 : 0.056776292622089386\n",
      "Training loss for batch 5128 : 0.20414453744888306\n",
      "Training loss for batch 5129 : 0.24356839060783386\n",
      "Training loss for batch 5130 : 0.0014418592909350991\n",
      "Training loss for batch 5131 : 0.2876625061035156\n",
      "Training loss for batch 5132 : 0.05035967752337456\n",
      "Training loss for batch 5133 : 0.2674294412136078\n",
      "Training loss for batch 5134 : 0.15528205037117004\n",
      "Training loss for batch 5135 : 0.12011563777923584\n",
      "Training loss for batch 5136 : 0.07641666382551193\n",
      "Training loss for batch 5137 : 0.2267576903104782\n",
      "Training loss for batch 5138 : 0.034440044313669205\n",
      "Training loss for batch 5139 : 0.24890807271003723\n",
      "Training loss for batch 5140 : 0.27335599064826965\n",
      "Training loss for batch 5141 : 0.11900255084037781\n",
      "Training loss for batch 5142 : 0.1253589689731598\n",
      "Training loss for batch 5143 : 0.3727574944496155\n",
      "Training loss for batch 5144 : 0.1258835792541504\n",
      "Training loss for batch 5145 : 0.22157038748264313\n",
      "Training loss for batch 5146 : 0.22136783599853516\n",
      "Training loss for batch 5147 : 0.2106723040342331\n",
      "Training loss for batch 5148 : 0.029018741101026535\n",
      "Training loss for batch 5149 : 0.21996648609638214\n",
      "Training loss for batch 5150 : 0.344952791929245\n",
      "Training loss for batch 5151 : 0.18063651025295258\n",
      "Training loss for batch 5152 : 0.17087800800800323\n",
      "Training loss for batch 5153 : 0.2546476125717163\n",
      "Training loss for batch 5154 : 0.10215329378843307\n",
      "Training loss for batch 5155 : 0.08851489424705505\n",
      "Training loss for batch 5156 : 0.10130971670150757\n",
      "Training loss for batch 5157 : 0.028610792011022568\n",
      "Training loss for batch 5158 : 0.11407095938920975\n",
      "Training loss for batch 5159 : 0.17081595957279205\n",
      "Training loss for batch 5160 : 0.05799465999007225\n",
      "Training loss for batch 5161 : 0.1440672129392624\n",
      "Training loss for batch 5162 : 0.29209983348846436\n",
      "Training loss for batch 5163 : 0.14980031549930573\n",
      "Training loss for batch 5164 : 0.02995578572154045\n",
      "Training loss for batch 5165 : 0.10114087909460068\n",
      "Training loss for batch 5166 : 0.18189634382724762\n",
      "Training loss for batch 5167 : 0.10520953685045242\n",
      "Training loss for batch 5168 : 0.1212657019495964\n",
      "Training loss for batch 5169 : 0.0681489109992981\n",
      "Training loss for batch 5170 : 0.20919811725616455\n",
      "Training loss for batch 5171 : 0.08795929700136185\n",
      "Training loss for batch 5172 : 0.18346010148525238\n",
      "Training loss for batch 5173 : 0.0613376721739769\n",
      "Training loss for batch 5174 : 0.17999045550823212\n",
      "Training loss for batch 5175 : 0.11367112398147583\n",
      "Training loss for batch 5176 : 0.3478352725505829\n",
      "Training loss for batch 5177 : 0.07789643853902817\n",
      "Training loss for batch 5178 : 0.07734604924917221\n",
      "Training loss for batch 5179 : 0.04276005178689957\n",
      "Training loss for batch 5180 : 0.1794460564851761\n",
      "Training loss for batch 5181 : 0.4390968382358551\n",
      "Training loss for batch 5182 : 0.07481412589550018\n",
      "Training loss for batch 5183 : 0.3061946630477905\n",
      "Training loss for batch 5184 : 0.2838211953639984\n",
      "Training loss for batch 5185 : 0.23905855417251587\n",
      "Training loss for batch 5186 : 0.021310659125447273\n",
      "Training loss for batch 5187 : 0.20205846428871155\n",
      "Training loss for batch 5188 : 0.11804933100938797\n",
      "Training loss for batch 5189 : 0.3236166536808014\n",
      "Training loss for batch 5190 : 0.21278774738311768\n",
      "Training loss for batch 5191 : 0.3117302358150482\n",
      "Training loss for batch 5192 : 0.33597493171691895\n",
      "Training loss for batch 5193 : 0.0009828440379351377\n",
      "Training loss for batch 5194 : 0.22559139132499695\n",
      "Training loss for batch 5195 : 0.2509162724018097\n",
      "Training loss for batch 5196 : 0.06256942451000214\n",
      "Training loss for batch 5197 : 0.14034640789031982\n",
      "Training loss for batch 5198 : 0.2033870667219162\n",
      "Training loss for batch 5199 : 0.18926402926445007\n",
      "Training loss for batch 5200 : 0.23425327241420746\n",
      "Training loss for batch 5201 : 0.01577048748731613\n",
      "Training loss for batch 5202 : 0.05286524444818497\n",
      "Training loss for batch 5203 : 0.27700307965278625\n",
      "Training loss for batch 5204 : 0.33393657207489014\n",
      "Training loss for batch 5205 : 0.022203590720891953\n",
      "Training loss for batch 5206 : 0.19297464191913605\n",
      "Training loss for batch 5207 : 0.2582858204841614\n",
      "Training loss for batch 5208 : 0.09260324388742447\n",
      "Training loss for batch 5209 : 0.012970682233572006\n",
      "Training loss for batch 5210 : 0.16888244450092316\n",
      "Training loss for batch 5211 : 0.24179141223430634\n",
      "Training loss for batch 5212 : 0.2430773228406906\n",
      "Training loss for batch 5213 : 0.2841622233390808\n",
      "Training loss for batch 5214 : 0.2403247356414795\n",
      "Training loss for batch 5215 : 0.3353451192378998\n",
      "Training loss for batch 5216 : 0.03624051436781883\n",
      "Training loss for batch 5217 : 0.058852292597293854\n",
      "Training loss for batch 5218 : 0.21978183090686798\n",
      "Training loss for batch 5219 : 0.053816765546798706\n",
      "Training loss for batch 5220 : 0.3561966121196747\n",
      "Training loss for batch 5221 : 0.2750440835952759\n",
      "Training loss for batch 5222 : 0.2116907238960266\n",
      "Training loss for batch 5223 : 0.09798552095890045\n",
      "Training loss for batch 5224 : 0.12486496567726135\n",
      "Training loss for batch 5225 : 0.006402570754289627\n",
      "Training loss for batch 5226 : 0.15386393666267395\n",
      "Training loss for batch 5227 : 0.03744092583656311\n",
      "Training loss for batch 5228 : 0.04634016007184982\n",
      "Training loss for batch 5229 : 0.2927263677120209\n",
      "Training loss for batch 5230 : 0.1528085619211197\n",
      "Training loss for batch 5231 : 0.25348666310310364\n",
      "Training loss for batch 5232 : 0.0432681106030941\n",
      "Training loss for batch 5233 : 0.023650644347071648\n",
      "Training loss for batch 5234 : 0.15157921612262726\n",
      "Training loss for batch 5235 : 0.1144750788807869\n",
      "Training loss for batch 5236 : 0.35989370942115784\n",
      "Training loss for batch 5237 : 0.08915668725967407\n",
      "Training loss for batch 5238 : 0.05517754703760147\n",
      "Training loss for batch 5239 : 0.013408050872385502\n",
      "Training loss for batch 5240 : 0.03656573221087456\n",
      "Training loss for batch 5241 : 0.17518019676208496\n",
      "Training loss for batch 5242 : 0.04138830304145813\n",
      "Training loss for batch 5243 : 0.14059153199195862\n",
      "Training loss for batch 5244 : 0.21040841937065125\n",
      "Training loss for batch 5245 : 0.18808671832084656\n",
      "Training loss for batch 5246 : 0.16790154576301575\n",
      "Training loss for batch 5247 : 0.23592238128185272\n",
      "Training loss for batch 5248 : 0.017740756273269653\n",
      "Training loss for batch 5249 : 0.13451482355594635\n",
      "Training loss for batch 5250 : 0.21394170820713043\n",
      "Training loss for batch 5251 : 0.20052078366279602\n",
      "Training loss for batch 5252 : 0.07400275766849518\n",
      "Training loss for batch 5253 : 0.1481131911277771\n",
      "Training loss for batch 5254 : 0.3355204164981842\n",
      "Training loss for batch 5255 : 0.05588157847523689\n",
      "Training loss for batch 5256 : 0.03855713829398155\n",
      "Training loss for batch 5257 : 0.0582142248749733\n",
      "Training loss for batch 5258 : 0.29855024814605713\n",
      "Training loss for batch 5259 : 0.047158725559711456\n",
      "Training loss for batch 5260 : 0.12550082802772522\n",
      "Training loss for batch 5261 : 0.06987989693880081\n",
      "Training loss for batch 5262 : 0.10074429214000702\n",
      "Training loss for batch 5263 : 0.17220784723758698\n",
      "Training loss for batch 5264 : 0.18532070517539978\n",
      "Training loss for batch 5265 : 0.10475156456232071\n",
      "Training loss for batch 5266 : 0.2373572736978531\n",
      "Training loss for batch 5267 : 0.2029324471950531\n",
      "Training loss for batch 5268 : 0.20609113574028015\n",
      "Training loss for batch 5269 : 0.017390169203281403\n",
      "Training loss for batch 5270 : 0.04604293778538704\n",
      "Training loss for batch 5271 : 0.21295972168445587\n",
      "Training loss for batch 5272 : 0.27965423464775085\n",
      "Training loss for batch 5273 : 0.11723642796278\n",
      "Training loss for batch 5274 : 0.17774321138858795\n",
      "Training loss for batch 5275 : 0.14467687904834747\n",
      "Training loss for batch 5276 : 0.01318337582051754\n",
      "Training loss for batch 5277 : 0.028145112097263336\n",
      "Training loss for batch 5278 : 0.2492365837097168\n",
      "Training loss for batch 5279 : 0.12098311632871628\n",
      "Training loss for batch 5280 : 0.44743648171424866\n",
      "Training loss for batch 5281 : 0.2556755542755127\n",
      "Training loss for batch 5282 : 0.17597566545009613\n",
      "Training loss for batch 5283 : 0.06525428593158722\n",
      "Training loss for batch 5284 : 0.16752836108207703\n",
      "Training loss for batch 5285 : 0.1805633306503296\n",
      "Training loss for batch 5286 : 0.09768756479024887\n",
      "Training loss for batch 5287 : 0.08564409613609314\n",
      "Training loss for batch 5288 : 0.23903363943099976\n",
      "Training loss for batch 5289 : 0.06130523234605789\n",
      "Training loss for batch 5290 : 0.014712389558553696\n",
      "Training loss for batch 5291 : 0.023493006825447083\n",
      "Training loss for batch 5292 : 0.1805853247642517\n",
      "Training loss for batch 5293 : 0.33237892389297485\n",
      "Training loss for batch 5294 : 0.19769787788391113\n",
      "Training loss for batch 5295 : 0.21600237488746643\n",
      "Training loss for batch 5296 : 0.35432305932044983\n",
      "Training loss for batch 5297 : 0.002907784655690193\n",
      "Training loss for batch 5298 : 0.10962548851966858\n",
      "Training loss for batch 5299 : 0.09084902703762054\n",
      "Training loss for batch 5300 : 0.11967796087265015\n",
      "Training loss for batch 5301 : 0.31504112482070923\n",
      "Training loss for batch 5302 : 0.3780699372291565\n",
      "Training loss for batch 5303 : 0.11853887140750885\n",
      "Training loss for batch 5304 : 0.12009324878454208\n",
      "Training loss for batch 5305 : 0.3722156286239624\n",
      "Training loss for batch 5306 : 0.18641702830791473\n",
      "Training loss for batch 5307 : 0.03884725645184517\n",
      "Training loss for batch 5308 : 0.0032573333010077477\n",
      "Training loss for batch 5309 : 0.2063320130109787\n",
      "Training loss for batch 5310 : 0.1397673785686493\n",
      "Training loss for batch 5311 : 0.04331713169813156\n",
      "Training loss for batch 5312 : 0.36295926570892334\n",
      "Training loss for batch 5313 : 0.1635795682668686\n",
      "Training loss for batch 5314 : 0.16116659343242645\n",
      "Training loss for batch 5315 : 0.3009085953235626\n",
      "Training loss for batch 5316 : 0.09455938637256622\n",
      "Training loss for batch 5317 : -0.00012869293277617544\n",
      "Training loss for batch 5318 : 0.0419243685901165\n",
      "Training loss for batch 5319 : 0.3111747205257416\n",
      "Training loss for batch 5320 : 0.09297287464141846\n",
      "Training loss for batch 5321 : 0.017437733709812164\n",
      "Training loss for batch 5322 : 0.021428551524877548\n",
      "Training loss for batch 5323 : 0.03364458307623863\n",
      "Training loss for batch 5324 : 0.46744900941848755\n",
      "Training loss for batch 5325 : 0.14756281673908234\n",
      "Training loss for batch 5326 : 0.3343851566314697\n",
      "Training loss for batch 5327 : 0.08226066082715988\n",
      "Training loss for batch 5328 : 0.30420082807540894\n",
      "Training loss for batch 5329 : 0.30713996291160583\n",
      "Training loss for batch 5330 : 0.25267311930656433\n",
      "Training loss for batch 5331 : 0.023027585819363594\n",
      "Training loss for batch 5332 : 0.26362091302871704\n",
      "Training loss for batch 5333 : 0.01762993261218071\n",
      "Training loss for batch 5334 : 0.4053252041339874\n",
      "Training loss for batch 5335 : 0.07853724807500839\n",
      "Training loss for batch 5336 : 0.3322449028491974\n",
      "Training loss for batch 5337 : 0.3429856598377228\n",
      "Training loss for batch 5338 : 0.06917589902877808\n",
      "Training loss for batch 5339 : 0.2400113344192505\n",
      "Training loss for batch 5340 : 0.15922461450099945\n",
      "Training loss for batch 5341 : 0.28291574120521545\n",
      "Training loss for batch 5342 : 0.3862673044204712\n",
      "Training loss for batch 5343 : 0.23571956157684326\n",
      "Training loss for batch 5344 : 0.3580924868583679\n",
      "Training loss for batch 5345 : 0.105324886739254\n",
      "Training loss for batch 5346 : 0.02123759314417839\n",
      "Training loss for batch 5347 : 0.041805755347013474\n",
      "Training loss for batch 5348 : 0.407954603433609\n",
      "Training loss for batch 5349 : 0.21565257012844086\n",
      "Training loss for batch 5350 : 0.33080536127090454\n",
      "Training loss for batch 5351 : 0.15152309834957123\n",
      "Training loss for batch 5352 : 0.25073596835136414\n",
      "Training loss for batch 5353 : 0.09269879013299942\n",
      "Training loss for batch 5354 : 0.1954658478498459\n",
      "Training loss for batch 5355 : 0.29354527592658997\n",
      "Training loss for batch 5356 : 0.23865918815135956\n",
      "Training loss for batch 5357 : 0.031174542382359505\n",
      "Training loss for batch 5358 : 0.03720404952764511\n",
      "Training loss for batch 5359 : 0.03947338089346886\n",
      "Training loss for batch 5360 : 0.1541544646024704\n",
      "Training loss for batch 5361 : 0.15836220979690552\n",
      "Training loss for batch 5362 : 0.17383608222007751\n",
      "Training loss for batch 5363 : 0.27836447954177856\n",
      "Training loss for batch 5364 : 0.028865128755569458\n",
      "Training loss for batch 5365 : 0.6573725938796997\n",
      "Training loss for batch 5366 : 0.08448812365531921\n",
      "Training loss for batch 5367 : 0.25905993580818176\n",
      "Training loss for batch 5368 : 0.19843243062496185\n",
      "Training loss for batch 5369 : 0.0\n",
      "Training loss for batch 5370 : 0.2683013379573822\n",
      "Training loss for batch 5371 : 0.1963864266872406\n",
      "Training loss for batch 5372 : 0.10308817028999329\n",
      "Training loss for batch 5373 : 0.11216738075017929\n",
      "Training loss for batch 5374 : 0.2632792294025421\n",
      "Training loss for batch 5375 : 0.0496835820376873\n",
      "Training loss for batch 5376 : 0.2702874541282654\n",
      "Training loss for batch 5377 : 0.22141268849372864\n",
      "Training loss for batch 5378 : 0.004142154473811388\n",
      "Training loss for batch 5379 : 0.02644508145749569\n",
      "Training loss for batch 5380 : 0.19341377913951874\n",
      "Training loss for batch 5381 : 0.11487597227096558\n",
      "Training loss for batch 5382 : 0.30721670389175415\n",
      "Training loss for batch 5383 : 0.024027198553085327\n",
      "Training loss for batch 5384 : 0.07378813624382019\n",
      "Training loss for batch 5385 : 0.3343454599380493\n",
      "Training loss for batch 5386 : 0.2526695728302002\n",
      "Training loss for batch 5387 : 0.2613200843334198\n",
      "Training loss for batch 5388 : 0.1061106026172638\n",
      "Training loss for batch 5389 : 0.2286345660686493\n",
      "Training loss for batch 5390 : 0.0398559533059597\n",
      "Training loss for batch 5391 : 0.1695195734500885\n",
      "Training loss for batch 5392 : 0.07504377514123917\n",
      "Training loss for batch 5393 : 0.17461374402046204\n",
      "Training loss for batch 5394 : 0.0690527930855751\n",
      "Training loss for batch 5395 : 0.11807216703891754\n",
      "Training loss for batch 5396 : 0.25807803869247437\n",
      "Training loss for batch 5397 : 0.029542075470089912\n",
      "Training loss for batch 5398 : 0.039208658039569855\n",
      "Training loss for batch 5399 : 0.1005556508898735\n",
      "Training loss for batch 5400 : 0.12402943521738052\n",
      "Training loss for batch 5401 : 0.33939796686172485\n",
      "Training loss for batch 5402 : 0.0613851360976696\n",
      "Training loss for batch 5403 : 0.2540178894996643\n",
      "Training loss for batch 5404 : 0.054599568247795105\n",
      "Training loss for batch 5405 : 0.34847596287727356\n",
      "Training loss for batch 5406 : 0.08484579622745514\n",
      "Training loss for batch 5407 : 1.282042384147644\n",
      "Training loss for batch 5408 : 0.26267901062965393\n",
      "Training loss for batch 5409 : 0.23931437730789185\n",
      "Training loss for batch 5410 : 0.07224155962467194\n",
      "Training loss for batch 5411 : 0.34327825903892517\n",
      "Training loss for batch 5412 : 0.3230796754360199\n",
      "Training loss for batch 5413 : 0.10859265923500061\n",
      "Training loss for batch 5414 : 0.15628565847873688\n",
      "Training loss for batch 5415 : 0.0\n",
      "Training loss for batch 5416 : 0.36485812067985535\n",
      "Training loss for batch 5417 : 0.20540054142475128\n",
      "Training loss for batch 5418 : 0.17092397809028625\n",
      "Training loss for batch 5419 : 0.019812796264886856\n",
      "Training loss for batch 5420 : 0.21795260906219482\n",
      "Training loss for batch 5421 : 0.026299001649022102\n",
      "Training loss for batch 5422 : 0.1399429738521576\n",
      "Training loss for batch 5423 : 0.2771783471107483\n",
      "Training loss for batch 5424 : 0.2664797902107239\n",
      "Training loss for batch 5425 : 0.0245978906750679\n",
      "Training loss for batch 5426 : 0.04497151076793671\n",
      "Training loss for batch 5427 : 0.2704806923866272\n",
      "Training loss for batch 5428 : 0.18273362517356873\n",
      "Training loss for batch 5429 : 0.37585172057151794\n",
      "Training loss for batch 5430 : 0.4724689722061157\n",
      "Training loss for batch 5431 : 0.30333155393600464\n",
      "Training loss for batch 5432 : 0.04440166801214218\n",
      "Training loss for batch 5433 : 0.026052378118038177\n",
      "Training loss for batch 5434 : 0.06998907774686813\n",
      "Training loss for batch 5435 : 0.08534038066864014\n",
      "Training loss for batch 5436 : 0.07010023295879364\n",
      "Training loss for batch 5437 : 0.2289774864912033\n",
      "Training loss for batch 5438 : 0.011686921119689941\n",
      "Training loss for batch 5439 : 0.18723127245903015\n",
      "Training loss for batch 5440 : 0.021645963191986084\n",
      "Training loss for batch 5441 : 0.05058397725224495\n",
      "Training loss for batch 5442 : 0.05064137279987335\n",
      "Training loss for batch 5443 : 0.17965613305568695\n",
      "Training loss for batch 5444 : 0.322254478931427\n",
      "Training loss for batch 5445 : 0.3743431270122528\n",
      "Training loss for batch 5446 : 0.46677982807159424\n",
      "Training loss for batch 5447 : 0.25300243496894836\n",
      "Training loss for batch 5448 : 0.06616205722093582\n",
      "Training loss for batch 5449 : 0.15950828790664673\n",
      "Training loss for batch 5450 : 0.13159456849098206\n",
      "Training loss for batch 5451 : 0.11545591056346893\n",
      "Training loss for batch 5452 : 0.2903287708759308\n",
      "Training loss for batch 5453 : 0.052516814321279526\n",
      "Training loss for batch 5454 : 0.01651846431195736\n",
      "Training loss for batch 5455 : 0.07490625232458115\n",
      "Training loss for batch 5456 : 0.1976952850818634\n",
      "Training loss for batch 5457 : 0.006394340191036463\n",
      "Training loss for batch 5458 : 0.06239732727408409\n",
      "Training loss for batch 5459 : 0.14046035706996918\n",
      "Training loss for batch 5460 : 0.17313170433044434\n",
      "Training loss for batch 5461 : 0.08275023847818375\n",
      "Training loss for batch 5462 : 0.30628424882888794\n",
      "Training loss for batch 5463 : 0.1308739185333252\n",
      "Training loss for batch 5464 : 0.025505751371383667\n",
      "Training loss for batch 5465 : 0.31245794892311096\n",
      "Training loss for batch 5466 : 0.038208961486816406\n",
      "Training loss for batch 5467 : 0.14937376976013184\n",
      "Training loss for batch 5468 : 0.18301893770694733\n",
      "Training loss for batch 5469 : 0.01699080504477024\n",
      "Training loss for batch 5470 : 0.07109837979078293\n",
      "Training loss for batch 5471 : 0.1875728964805603\n",
      "Training loss for batch 5472 : 0.12897427380084991\n",
      "Training loss for batch 5473 : 0.10546472668647766\n",
      "Training loss for batch 5474 : 0.0\n",
      "Training loss for batch 5475 : 0.1469743400812149\n",
      "Training loss for batch 5476 : 0.2939879894256592\n",
      "Training loss for batch 5477 : 0.30959224700927734\n",
      "Training loss for batch 5478 : 0.2259589582681656\n",
      "Training loss for batch 5479 : 0.039283137768507004\n",
      "Training loss for batch 5480 : 0.01875505968928337\n",
      "Training loss for batch 5481 : 0.0238144900649786\n",
      "Training loss for batch 5482 : 0.06302930414676666\n",
      "Training loss for batch 5483 : 0.26435112953186035\n",
      "Training loss for batch 5484 : 0.16866792738437653\n",
      "Training loss for batch 5485 : 0.25276848673820496\n",
      "Training loss for batch 5486 : 0.12115766108036041\n",
      "Training loss for batch 5487 : 0.19338135421276093\n",
      "Training loss for batch 5488 : 0.20287594199180603\n",
      "Training loss for batch 5489 : 0.2713613510131836\n",
      "Training loss for batch 5490 : 0.04416482150554657\n",
      "Training loss for batch 5491 : 0.02822674997150898\n",
      "Training loss for batch 5492 : 0.047195449471473694\n",
      "Training loss for batch 5493 : 0.004841324873268604\n",
      "Training loss for batch 5494 : 0.360798180103302\n",
      "Training loss for batch 5495 : 0.3810564875602722\n",
      "Training loss for batch 5496 : 0.06113976985216141\n",
      "Training loss for batch 5497 : 0.15070496499538422\n",
      "Training loss for batch 5498 : 0.4462815821170807\n",
      "Training loss for batch 5499 : 0.12302151322364807\n",
      "Training loss for batch 5500 : 0.25441840291023254\n",
      "Training loss for batch 5501 : 0.22428427636623383\n",
      "Training loss for batch 5502 : 0.019937768578529358\n",
      "Training loss for batch 5503 : 0.03409445658326149\n",
      "Training loss for batch 5504 : 0.42131876945495605\n",
      "Training loss for batch 5505 : 0.19177962839603424\n",
      "Training loss for batch 5506 : 0.12922610342502594\n",
      "Training loss for batch 5507 : 0.010916998609900475\n",
      "Training loss for batch 5508 : 0.14495716989040375\n",
      "Training loss for batch 5509 : 0.1360415518283844\n",
      "Training loss for batch 5510 : 0.2706510126590729\n",
      "Training loss for batch 5511 : 0.31654196977615356\n",
      "Training loss for batch 5512 : -0.002191519597545266\n",
      "Training loss for batch 5513 : 0.002445606980472803\n",
      "Training loss for batch 5514 : 0.06531906127929688\n",
      "Training loss for batch 5515 : 0.19214224815368652\n",
      "Training loss for batch 5516 : 0.17016679048538208\n",
      "Training loss for batch 5517 : 0.229662224650383\n",
      "Training loss for batch 5518 : 0.3864777386188507\n",
      "Training loss for batch 5519 : 0.3264281153678894\n",
      "Training loss for batch 5520 : 0.10758952796459198\n",
      "Training loss for batch 5521 : 0.18101365864276886\n",
      "Training loss for batch 5522 : 0.22668078541755676\n",
      "Training loss for batch 5523 : 0.19382286071777344\n",
      "Training loss for batch 5524 : 0.24168890714645386\n",
      "Training loss for batch 5525 : 0.02095717564225197\n",
      "Training loss for batch 5526 : 0.15827536582946777\n",
      "Training loss for batch 5527 : 0.20267672836780548\n",
      "Training loss for batch 5528 : 0.01987466774880886\n",
      "Training loss for batch 5529 : 0.1458505392074585\n",
      "Training loss for batch 5530 : 0.008816917426884174\n",
      "Training loss for batch 5531 : 0.042496923357248306\n",
      "Training loss for batch 5532 : 0.303751677274704\n",
      "Training loss for batch 5533 : 0.19630761444568634\n",
      "Training loss for batch 5534 : 0.08699563145637512\n",
      "Training loss for batch 5535 : 0.5609347224235535\n",
      "Training loss for batch 5536 : 0.17759542167186737\n",
      "Training loss for batch 5537 : 0.32327982783317566\n",
      "Training loss for batch 5538 : 0.08475184440612793\n",
      "Training loss for batch 5539 : 0.009572729468345642\n",
      "Training loss for batch 5540 : 0.2024143487215042\n",
      "Training loss for batch 5541 : 0.005112062208354473\n",
      "Training loss for batch 5542 : 0.0354955680668354\n",
      "Training loss for batch 5543 : 0.00985229853540659\n",
      "Training loss for batch 5544 : 0.10478691756725311\n",
      "Training loss for batch 5545 : 0.18550480902194977\n",
      "Training loss for batch 5546 : 0.20063035190105438\n",
      "Training loss for batch 5547 : 0.018833976238965988\n",
      "Training loss for batch 5548 : 0.06231698766350746\n",
      "Training loss for batch 5549 : 0.18756522238254547\n",
      "Training loss for batch 5550 : 0.6134534478187561\n",
      "Training loss for batch 5551 : 0.21362705528736115\n",
      "Training loss for batch 5552 : 0.17158065736293793\n",
      "Training loss for batch 5553 : 0.3022257089614868\n",
      "Training loss for batch 5554 : 0.6469009518623352\n",
      "Training loss for batch 5555 : 0.1809237003326416\n",
      "Training loss for batch 5556 : 0.04469825327396393\n",
      "Training loss for batch 5557 : 0.5512139201164246\n",
      "Training loss for batch 5558 : 0.19541923701763153\n",
      "Training loss for batch 5559 : 0.24377816915512085\n",
      "Training loss for batch 5560 : 0.027664165943861008\n",
      "Training loss for batch 5561 : 0.08280416578054428\n",
      "Training loss for batch 5562 : 0.3028619587421417\n",
      "Training loss for batch 5563 : 0.17180974781513214\n",
      "Training loss for batch 5564 : 0.24100108444690704\n",
      "Training loss for batch 5565 : 0.3245044946670532\n",
      "Training loss for batch 5566 : 0.3357165455818176\n",
      "Training loss for batch 5567 : 0.09045985341072083\n",
      "Training loss for batch 5568 : 0.26909908652305603\n",
      "Training loss for batch 5569 : 0.09967709332704544\n",
      "Training loss for batch 5570 : 0.2567429840564728\n",
      "Training loss for batch 5571 : 0.2339058816432953\n",
      "Training loss for batch 5572 : 0.21110308170318604\n",
      "Training loss for batch 5573 : 0.12417944520711899\n",
      "Training loss for batch 5574 : 0.050890978425741196\n",
      "Training loss for batch 5575 : 0.32983314990997314\n",
      "Training loss for batch 5576 : 0.13380922377109528\n",
      "Training loss for batch 5577 : 0.030394937843084335\n",
      "Training loss for batch 5578 : 0.23760616779327393\n",
      "Training loss for batch 5579 : 0.4095158576965332\n",
      "Training loss for batch 5580 : 0.14616145193576813\n",
      "Training loss for batch 5581 : 0.28306931257247925\n",
      "Training loss for batch 5582 : 0.20256805419921875\n",
      "Training loss for batch 5583 : 0.2695910632610321\n",
      "Training loss for batch 5584 : 0.07245862483978271\n",
      "Training loss for batch 5585 : 0.06005283072590828\n",
      "Training loss for batch 5586 : 0.18055841326713562\n",
      "Training loss for batch 5587 : 0.12211678177118301\n",
      "Training loss for batch 5588 : 0.09730001538991928\n",
      "Training loss for batch 5589 : 0.2733379304409027\n",
      "Training loss for batch 5590 : 0.23914867639541626\n",
      "Training loss for batch 5591 : 0.15342704951763153\n",
      "Training loss for batch 5592 : 0.09031457453966141\n",
      "Training loss for batch 5593 : 0.07716577500104904\n",
      "Training loss for batch 5594 : 0.24704116582870483\n",
      "Training loss for batch 5595 : 0.10402479767799377\n",
      "Training loss for batch 5596 : 0.49010396003723145\n",
      "Training loss for batch 5597 : 0.0\n",
      "Training loss for batch 5598 : 0.21980750560760498\n",
      "Training loss for batch 5599 : 0.04672124981880188\n",
      "Training loss for batch 5600 : 0.3433397710323334\n",
      "Training loss for batch 5601 : 0.19761329889297485\n",
      "Training loss for batch 5602 : 0.3291253447532654\n",
      "Training loss for batch 5603 : 0.39147019386291504\n",
      "Training loss for batch 5604 : 0.06535274535417557\n",
      "Training loss for batch 5605 : 0.14867061376571655\n",
      "Training loss for batch 5606 : 0.27036383748054504\n",
      "Training loss for batch 5607 : 0.30722689628601074\n",
      "Training loss for batch 5608 : 0.21730288863182068\n",
      "Training loss for batch 5609 : 0.16999392211437225\n",
      "Training loss for batch 5610 : 0.10816363990306854\n",
      "Training loss for batch 5611 : 0.2866213619709015\n",
      "Training loss for batch 5612 : 0.4023796319961548\n",
      "Training loss for batch 5613 : 0.0734749436378479\n",
      "Training loss for batch 5614 : 0.2522047758102417\n",
      "Training loss for batch 5615 : 0.3742706775665283\n",
      "Training loss for batch 5616 : 0.047231275588274\n",
      "Training loss for batch 5617 : 0.021230584010481834\n",
      "Training loss for batch 5618 : 0.007418298162519932\n",
      "Training loss for batch 5619 : 0.23319599032402039\n",
      "Training loss for batch 5620 : 0.020841490477323532\n",
      "Training loss for batch 5621 : 0.2108916938304901\n",
      "Training loss for batch 5622 : 0.10650164633989334\n",
      "Training loss for batch 5623 : 0.21067596971988678\n",
      "Training loss for batch 5624 : 0.3291998505592346\n",
      "Training loss for batch 5625 : 0.34489113092422485\n",
      "Training loss for batch 5626 : 0.029217429459095\n",
      "Training loss for batch 5627 : 0.023979375138878822\n",
      "Training loss for batch 5628 : 0.22673620283603668\n",
      "Training loss for batch 5629 : 0.34805503487586975\n",
      "Training loss for batch 5630 : 0.05860406160354614\n",
      "Training loss for batch 5631 : 0.2819906771183014\n",
      "Training loss for batch 5632 : 0.3208087086677551\n",
      "Training loss for batch 5633 : 0.20153526961803436\n",
      "Training loss for batch 5634 : 0.04837748780846596\n",
      "Training loss for batch 5635 : 0.19716116786003113\n",
      "Training loss for batch 5636 : 0.07355032861232758\n",
      "Training loss for batch 5637 : 0.19463059306144714\n",
      "Training loss for batch 5638 : 0.4600546360015869\n",
      "Training loss for batch 5639 : 0.14296209812164307\n",
      "Training loss for batch 5640 : 0.11267209053039551\n",
      "Training loss for batch 5641 : 0.2383922040462494\n",
      "Training loss for batch 5642 : 0.136551633477211\n",
      "Training loss for batch 5643 : 0.015007657930254936\n",
      "Training loss for batch 5644 : 0.07655428349971771\n",
      "Training loss for batch 5645 : 0.2757299542427063\n",
      "Training loss for batch 5646 : 0.09484798461198807\n",
      "Training loss for batch 5647 : 0.4395511746406555\n",
      "Training loss for batch 5648 : 0.07322455197572708\n",
      "Training loss for batch 5649 : 0.2715568244457245\n",
      "Training loss for batch 5650 : 0.0561172291636467\n",
      "Training loss for batch 5651 : 0.1497330218553543\n",
      "Training loss for batch 5652 : 0.2356480062007904\n",
      "Training loss for batch 5653 : 0.11080485582351685\n",
      "Training loss for batch 5654 : 0.2198702096939087\n",
      "Training loss for batch 5655 : 0.16733397543430328\n",
      "Training loss for batch 5656 : 0.2809295058250427\n",
      "Training loss for batch 5657 : 0.04557114094495773\n",
      "Training loss for batch 5658 : 0.24743810296058655\n",
      "Training loss for batch 5659 : 0.14247550070285797\n",
      "Training loss for batch 5660 : 0.09018491953611374\n",
      "Training loss for batch 5661 : 0.017800362780690193\n",
      "Training loss for batch 5662 : 0.38234150409698486\n",
      "Training loss for batch 5663 : 0.05517128109931946\n",
      "Training loss for batch 5664 : 0.21812954545021057\n",
      "Training loss for batch 5665 : 0.4474134147167206\n",
      "Training loss for batch 5666 : 0.24997478723526\n",
      "Training loss for batch 5667 : 0.325929194688797\n",
      "Training loss for batch 5668 : 0.029864609241485596\n",
      "Training loss for batch 5669 : 0.20390383899211884\n",
      "Training loss for batch 5670 : 0.3424789309501648\n",
      "Training loss for batch 5671 : 0.041800353676080704\n",
      "Training loss for batch 5672 : 0.06907279789447784\n",
      "Training loss for batch 5673 : 0.11328931152820587\n",
      "Training loss for batch 5674 : 0.16008582711219788\n",
      "Training loss for batch 5675 : 0.019832901656627655\n",
      "Training loss for batch 5676 : 0.013115933164954185\n",
      "Training loss for batch 5677 : 0.18252891302108765\n",
      "Training loss for batch 5678 : 0.35255151987075806\n",
      "Training loss for batch 5679 : 0.009852957911789417\n",
      "Training loss for batch 5680 : 0.08215026557445526\n",
      "Training loss for batch 5681 : 0.011439663358032703\n",
      "Training loss for batch 5682 : 0.13086721301078796\n",
      "Training loss for batch 5683 : 0.0\n",
      "Training loss for batch 5684 : 0.2927455008029938\n",
      "Training loss for batch 5685 : 0.10373616218566895\n",
      "Training loss for batch 5686 : 0.1804528683423996\n",
      "Training loss for batch 5687 : 0.21283048391342163\n",
      "Training loss for batch 5688 : 0.5151839852333069\n",
      "Training loss for batch 5689 : 0.498915433883667\n",
      "Training loss for batch 5690 : 0.22543971240520477\n",
      "Training loss for batch 5691 : 0.24855315685272217\n",
      "Training loss for batch 5692 : 0.0776156336069107\n",
      "Training loss for batch 5693 : 0.09060172736644745\n",
      "Training loss for batch 5694 : 0.21469134092330933\n",
      "Training loss for batch 5695 : 0.09276309609413147\n",
      "Training loss for batch 5696 : 0.12871111929416656\n",
      "Training loss for batch 5697 : 0.1457691341638565\n",
      "Training loss for batch 5698 : 0.3757782578468323\n",
      "Training loss for batch 5699 : 0.0\n",
      "Training loss for batch 5700 : 0.1517276018857956\n",
      "Training loss for batch 5701 : 0.12029090523719788\n",
      "Training loss for batch 5702 : 0.08023790270090103\n",
      "Training loss for batch 5703 : 0.21473431587219238\n",
      "Training loss for batch 5704 : 0.08467070013284683\n",
      "Training loss for batch 5705 : 0.09524893760681152\n",
      "Training loss for batch 5706 : 0.07559902966022491\n",
      "Training loss for batch 5707 : 0.14024175703525543\n",
      "Training loss for batch 5708 : 0.10817195475101471\n",
      "Training loss for batch 5709 : 0.11582151800394058\n",
      "Training loss for batch 5710 : 0.4748087227344513\n",
      "Training loss for batch 5711 : 0.33584192395210266\n",
      "Training loss for batch 5712 : 0.06299659609794617\n",
      "Training loss for batch 5713 : 0.2321108728647232\n",
      "Training loss for batch 5714 : 0.08288794755935669\n",
      "Training loss for batch 5715 : 0.26171717047691345\n",
      "Training loss for batch 5716 : 0.2501954734325409\n",
      "Training loss for batch 5717 : 0.10577064007520676\n",
      "Training loss for batch 5718 : 0.3074691891670227\n",
      "Training loss for batch 5719 : 0.12876754999160767\n",
      "Training loss for batch 5720 : 0.18912653625011444\n",
      "Training loss for batch 5721 : 0.12853287160396576\n",
      "Training loss for batch 5722 : 0.1442267745733261\n",
      "Training loss for batch 5723 : 0.13568058609962463\n",
      "Training loss for batch 5724 : 0.19243384897708893\n",
      "Training loss for batch 5725 : 0.3727133870124817\n",
      "Training loss for batch 5726 : 0.35484400391578674\n",
      "Training loss for batch 5727 : 0.44388073682785034\n",
      "Training loss for batch 5728 : 0.38837912678718567\n",
      "Training loss for batch 5729 : 0.03215276077389717\n",
      "Training loss for batch 5730 : 0.28354543447494507\n",
      "Training loss for batch 5731 : 0.2626112103462219\n",
      "Training loss for batch 5732 : 0.11146127432584763\n",
      "Training loss for batch 5733 : 0.12820300459861755\n",
      "Training loss for batch 5734 : 0.28467699885368347\n",
      "Training loss for batch 5735 : 0.20339198410511017\n",
      "Training loss for batch 5736 : 0.11561410129070282\n",
      "Training loss for batch 5737 : 0.06400785595178604\n",
      "Training loss for batch 5738 : 0.09108776599168777\n",
      "Training loss for batch 5739 : 0.2990725040435791\n",
      "Training loss for batch 5740 : 0.2371513843536377\n",
      "Training loss for batch 5741 : 0.11164508759975433\n",
      "Training loss for batch 5742 : 0.09459300339221954\n",
      "Training loss for batch 5743 : 0.2063746303319931\n",
      "Training loss for batch 5744 : 0.1828715056180954\n",
      "Training loss for batch 5745 : 0.24682003259658813\n",
      "Training loss for batch 5746 : 0.20353208482265472\n",
      "Training loss for batch 5747 : 0.12092142552137375\n",
      "Training loss for batch 5748 : 0.08686048537492752\n",
      "Training loss for batch 5749 : 0.11639944463968277\n",
      "Training loss for batch 5750 : 0.22552138566970825\n",
      "Training loss for batch 5751 : 0.1892961859703064\n",
      "Training loss for batch 5752 : 0.175177663564682\n",
      "Training loss for batch 5753 : 0.2050858736038208\n",
      "Training loss for batch 5754 : 0.056677401065826416\n",
      "Training loss for batch 5755 : 0.07321616262197495\n",
      "Training loss for batch 5756 : 0.014204910025000572\n",
      "Training loss for batch 5757 : 0.04350001737475395\n",
      "Training loss for batch 5758 : 0.07800940424203873\n",
      "Training loss for batch 5759 : 0.08370483666658401\n",
      "Training loss for batch 5760 : 0.2867487967014313\n",
      "Training loss for batch 5761 : 0.040443290024995804\n",
      "Training loss for batch 5762 : 0.18975169956684113\n",
      "Training loss for batch 5763 : 0.24494272470474243\n",
      "Training loss for batch 5764 : 0.24409019947052002\n",
      "Training loss for batch 5765 : 0.05630932003259659\n",
      "Training loss for batch 5766 : 0.19169947504997253\n",
      "Training loss for batch 5767 : 0.23287548124790192\n",
      "Training loss for batch 5768 : 0.06887441873550415\n",
      "Training loss for batch 5769 : 0.12478527426719666\n",
      "Training loss for batch 5770 : 0.25722581148147583\n",
      "Training loss for batch 5771 : 0.1163635402917862\n",
      "Training loss for batch 5772 : 0.04519220069050789\n",
      "Training loss for batch 5773 : 0.05477854609489441\n",
      "Training loss for batch 5774 : 0.20397189259529114\n",
      "Training loss for batch 5775 : 0.10144350677728653\n",
      "Training loss for batch 5776 : 0.17867015302181244\n",
      "Training loss for batch 5777 : 0.19547557830810547\n",
      "Training loss for batch 5778 : 0.299296498298645\n",
      "Training loss for batch 5779 : 0.3576856851577759\n",
      "Training loss for batch 5780 : 0.06599465757608414\n",
      "Training loss for batch 5781 : 0.2824271321296692\n",
      "Training loss for batch 5782 : 0.2588938772678375\n",
      "Training loss for batch 5783 : 0.07220067083835602\n",
      "Training loss for batch 5784 : 0.1838420033454895\n",
      "Training loss for batch 5785 : 0.16607540845870972\n",
      "Training loss for batch 5786 : 0.14606013894081116\n",
      "Training loss for batch 5787 : 0.2584843635559082\n",
      "Training loss for batch 5788 : 0.16606968641281128\n",
      "Training loss for batch 5789 : 0.39571064710617065\n",
      "Training loss for batch 5790 : 0.02267777919769287\n",
      "Training loss for batch 5791 : 0.28550082445144653\n",
      "Training loss for batch 5792 : 0.3424931764602661\n",
      "Training loss for batch 5793 : 0.0220416821539402\n",
      "Training loss for batch 5794 : 0.24145692586898804\n",
      "Training loss for batch 5795 : 0.16780143976211548\n",
      "Training loss for batch 5796 : 0.2681877315044403\n",
      "Training loss for batch 5797 : 0.07303591817617416\n",
      "Training loss for batch 5798 : 0.1309157907962799\n",
      "Training loss for batch 5799 : 0.14128714799880981\n",
      "Training loss for batch 5800 : 0.16909447312355042\n",
      "Training loss for batch 5801 : 0.4777229130268097\n",
      "Training loss for batch 5802 : 0.05156850442290306\n",
      "Training loss for batch 5803 : 0.2646491825580597\n",
      "Training loss for batch 5804 : 0.0\n",
      "Training loss for batch 5805 : 0.09635494649410248\n",
      "Training loss for batch 5806 : 0.09615650773048401\n",
      "Training loss for batch 5807 : 0.21543897688388824\n",
      "Training loss for batch 5808 : 0.13117821514606476\n",
      "Training loss for batch 5809 : 0.30951932072639465\n",
      "Training loss for batch 5810 : 0.11154475808143616\n",
      "Training loss for batch 5811 : 0.15655285120010376\n",
      "Training loss for batch 5812 : 0.21166370809078217\n",
      "Training loss for batch 5813 : 0.5356995463371277\n",
      "Training loss for batch 5814 : 0.5451473593711853\n",
      "Training loss for batch 5815 : 0.17046307027339935\n",
      "Training loss for batch 5816 : 0.06509067863225937\n",
      "Training loss for batch 5817 : 0.26407819986343384\n",
      "Training loss for batch 5818 : 0.3444471061229706\n",
      "Training loss for batch 5819 : 0.10134018212556839\n",
      "Training loss for batch 5820 : 0.11730574071407318\n",
      "Training loss for batch 5821 : 0.03298228979110718\n",
      "Training loss for batch 5822 : 0.14447258412837982\n",
      "Training loss for batch 5823 : 0.08690334111452103\n",
      "Training loss for batch 5824 : 0.09001678228378296\n",
      "Training loss for batch 5825 : 0.27118980884552\n",
      "Training loss for batch 5826 : 0.2612414062023163\n",
      "Training loss for batch 5827 : 0.2180715650320053\n",
      "Training loss for batch 5828 : 0.194626584649086\n",
      "Training loss for batch 5829 : 0.3970131576061249\n",
      "Training loss for batch 5830 : 0.09356582164764404\n",
      "Training loss for batch 5831 : 0.05407131463289261\n",
      "Training loss for batch 5832 : 0.28844329714775085\n",
      "Training loss for batch 5833 : 0.433283269405365\n",
      "Training loss for batch 5834 : 0.27883636951446533\n",
      "Training loss for batch 5835 : 0.016627922654151917\n",
      "Training loss for batch 5836 : 0.15054193139076233\n",
      "Training loss for batch 5837 : 0.1487506777048111\n",
      "Training loss for batch 5838 : 0.22357594966888428\n",
      "Training loss for batch 5839 : 0.009813150390982628\n",
      "Training loss for batch 5840 : 0.029260050505399704\n",
      "Training loss for batch 5841 : 0.24057096242904663\n",
      "Training loss for batch 5842 : 0.0659099593758583\n",
      "Training loss for batch 5843 : 0.14908303320407867\n",
      "Training loss for batch 5844 : 0.11842605471611023\n",
      "Training loss for batch 5845 : 0.03313368186354637\n",
      "Training loss for batch 5846 : 0.11118409037590027\n",
      "Training loss for batch 5847 : 0.24044159054756165\n",
      "Training loss for batch 5848 : 0.08958899974822998\n",
      "Training loss for batch 5849 : 0.3576068878173828\n",
      "Training loss for batch 5850 : 0.4188874065876007\n",
      "Training loss for batch 5851 : 0.27896934747695923\n",
      "Training loss for batch 5852 : 0.11922778189182281\n",
      "Training loss for batch 5853 : 0.15324117243289948\n",
      "Training loss for batch 5854 : 0.0886775478720665\n",
      "Training loss for batch 5855 : 0.18432952463626862\n",
      "Training loss for batch 5856 : 0.02971772663295269\n",
      "Training loss for batch 5857 : 0.0447777584195137\n",
      "Training loss for batch 5858 : 0.051008813083171844\n",
      "Training loss for batch 5859 : 0.04450281709432602\n",
      "Training loss for batch 5860 : 0.2462412416934967\n",
      "Training loss for batch 5861 : 0.2898951768875122\n",
      "Training loss for batch 5862 : 0.21293538808822632\n",
      "Training loss for batch 5863 : 0.29096418619155884\n",
      "Training loss for batch 5864 : 0.3192594349384308\n",
      "Training loss for batch 5865 : 0.09765665978193283\n",
      "Training loss for batch 5866 : 0.03693542629480362\n",
      "Training loss for batch 5867 : 0.009021858684718609\n",
      "Training loss for batch 5868 : 0.2664977014064789\n",
      "Training loss for batch 5869 : 0.21885190904140472\n",
      "Training loss for batch 5870 : 0.38789311051368713\n",
      "Training loss for batch 5871 : 0.023334089666604996\n",
      "Training loss for batch 5872 : 0.12155500799417496\n",
      "Training loss for batch 5873 : 0.12231230735778809\n",
      "Training loss for batch 5874 : 0.07626081258058548\n",
      "Training loss for batch 5875 : 0.017432011663913727\n",
      "Training loss for batch 5876 : 0.06239103898406029\n",
      "Training loss for batch 5877 : 0.26472532749176025\n",
      "Training loss for batch 5878 : 0.06764180958271027\n",
      "Training loss for batch 5879 : 0.14005324244499207\n",
      "Training loss for batch 5880 : 0.41631752252578735\n",
      "Training loss for batch 5881 : 0.17504501342773438\n",
      "Training loss for batch 5882 : 0.04195577651262283\n",
      "Training loss for batch 5883 : 0.15201608836650848\n",
      "Training loss for batch 5884 : 0.15090088546276093\n",
      "Training loss for batch 5885 : 0.06530624628067017\n",
      "Training loss for batch 5886 : 0.24662840366363525\n",
      "Training loss for batch 5887 : 0.1875419020652771\n",
      "Training loss for batch 5888 : 0.2128787636756897\n",
      "Training loss for batch 5889 : 0.36030447483062744\n",
      "Training loss for batch 5890 : 0.0723194107413292\n",
      "Training loss for batch 5891 : 0.09656275063753128\n",
      "Training loss for batch 5892 : 0.2663361728191376\n",
      "Training loss for batch 5893 : 0.06351473182439804\n",
      "Training loss for batch 5894 : 0.6888735294342041\n",
      "Training loss for batch 5895 : 0.05525209382176399\n",
      "Training loss for batch 5896 : 0.0777265802025795\n",
      "Training loss for batch 5897 : 0.24057021737098694\n",
      "Training loss for batch 5898 : 0.3102829158306122\n",
      "Training loss for batch 5899 : 0.1907023936510086\n",
      "Training loss for batch 5900 : 0.08662216365337372\n",
      "Training loss for batch 5901 : 0.3336256742477417\n",
      "Training loss for batch 5902 : 0.09763026982545853\n",
      "Training loss for batch 5903 : 0.14058911800384521\n",
      "Training loss for batch 5904 : 0.17349521815776825\n",
      "Training loss for batch 5905 : 0.18827033042907715\n",
      "Training loss for batch 5906 : 0.1710827350616455\n",
      "Training loss for batch 5907 : 0.5398516058921814\n",
      "Training loss for batch 5908 : 0.22697649896144867\n",
      "Training loss for batch 5909 : 0.25392580032348633\n",
      "Training loss for batch 5910 : 0.03064553067088127\n",
      "Training loss for batch 5911 : 0.44827982783317566\n",
      "Training loss for batch 5912 : 0.07538028806447983\n",
      "Training loss for batch 5913 : 0.18264979124069214\n",
      "Training loss for batch 5914 : 0.1033874899148941\n",
      "Training loss for batch 5915 : 0.2829118072986603\n",
      "Training loss for batch 5916 : 0.018346022814512253\n",
      "Training loss for batch 5917 : 0.2081451714038849\n",
      "Training loss for batch 5918 : 0.19702796638011932\n",
      "Training loss for batch 5919 : 0.23750901222229004\n",
      "Training loss for batch 5920 : 0.16889138519763947\n",
      "Training loss for batch 5921 : 0.16089218854904175\n",
      "Training loss for batch 5922 : 0.27550196647644043\n",
      "Training loss for batch 5923 : 0.016316944733262062\n",
      "Training loss for batch 5924 : 0.06114505976438522\n",
      "Training loss for batch 5925 : 0.09923654049634933\n",
      "Training loss for batch 5926 : 0.28578370809555054\n",
      "Training loss for batch 5927 : 0.2065148800611496\n",
      "Training loss for batch 5928 : 0.34579744935035706\n",
      "Training loss for batch 5929 : 0.193485826253891\n",
      "Training loss for batch 5930 : 0.1642623096704483\n",
      "Training loss for batch 5931 : 0.5150538682937622\n",
      "Training loss for batch 5932 : 0.44761893153190613\n",
      "Training loss for batch 5933 : 0.2596745193004608\n",
      "Training loss for batch 5934 : 0.08927148580551147\n",
      "Training loss for batch 5935 : 0.1907094419002533\n",
      "Training loss for batch 5936 : 0.030065272003412247\n",
      "Training loss for batch 5937 : 0.46345970034599304\n",
      "Training loss for batch 5938 : 0.17114588618278503\n",
      "Training loss for batch 5939 : 0.11185653507709503\n",
      "Training loss for batch 5940 : 0.3273879289627075\n",
      "Training loss for batch 5941 : 0.36487358808517456\n",
      "Training loss for batch 5942 : 0.023130593821406364\n",
      "Training loss for batch 5943 : 0.3101409077644348\n",
      "Training loss for batch 5944 : 0.15512527525424957\n",
      "Training loss for batch 5945 : 0.24926674365997314\n",
      "Training loss for batch 5946 : 0.24633121490478516\n",
      "Training loss for batch 5947 : 0.020918605849146843\n",
      "Training loss for batch 5948 : 0.1102256029844284\n",
      "Training loss for batch 5949 : 0.11740395426750183\n",
      "Training loss for batch 5950 : 0.09132778644561768\n",
      "Training loss for batch 5951 : 0.2644030749797821\n",
      "Training loss for batch 5952 : 0.046395450830459595\n",
      "Training loss for batch 5953 : 0.30369940400123596\n",
      "Training loss for batch 5954 : 0.6522543430328369\n",
      "Training loss for batch 5955 : 0.10700137913227081\n",
      "Training loss for batch 5956 : 0.07144195586442947\n",
      "Training loss for batch 5957 : 0.2018272578716278\n",
      "Training loss for batch 5958 : 0.43822383880615234\n",
      "Training loss for batch 5959 : 0.14021486043930054\n",
      "Training loss for batch 5960 : 0.1738971769809723\n",
      "Training loss for batch 5961 : 0.23974984884262085\n",
      "Training loss for batch 5962 : 0.17450958490371704\n",
      "Training loss for batch 5963 : 0.1025744304060936\n",
      "Training loss for batch 5964 : 0.18473421037197113\n",
      "Training loss for batch 5965 : 0.08836176246404648\n",
      "Training loss for batch 5966 : 0.0562577024102211\n",
      "Training loss for batch 5967 : 0.020221572369337082\n",
      "Training loss for batch 5968 : 0.13213634490966797\n",
      "Training loss for batch 5969 : 0.1655551642179489\n",
      "Training loss for batch 5970 : 0.12914960086345673\n",
      "Training loss for batch 5971 : 0.2855280339717865\n",
      "Training loss for batch 5972 : 0.062421590089797974\n",
      "Training loss for batch 5973 : 0.12691092491149902\n",
      "Training loss for batch 5974 : 0.3892265558242798\n",
      "Training loss for batch 5975 : 0.029173297807574272\n",
      "Training loss for batch 5976 : 0.22783225774765015\n",
      "Training loss for batch 5977 : 0.241918683052063\n",
      "Training loss for batch 5978 : 0.10768257826566696\n",
      "Training loss for batch 5979 : 0.0844591036438942\n",
      "Training loss for batch 5980 : 0.22035633027553558\n",
      "Training loss for batch 5981 : 0.05241423845291138\n",
      "Training loss for batch 5982 : 0.10978211462497711\n",
      "Training loss for batch 5983 : 0.21684861183166504\n",
      "Training loss for batch 5984 : 0.43826761841773987\n",
      "Training loss for batch 5985 : 0.17936334013938904\n",
      "Training loss for batch 5986 : 0.14137516915798187\n",
      "Training loss for batch 5987 : 0.16179515421390533\n",
      "Training loss for batch 5988 : 0.0\n",
      "Training loss for batch 5989 : 0.11806296557188034\n",
      "Training loss for batch 5990 : 0.15334433317184448\n",
      "Training loss for batch 5991 : 0.1548672765493393\n",
      "Training loss for batch 5992 : 0.3140115439891815\n",
      "Training loss for batch 5993 : 0.048439983278512955\n",
      "Training loss for batch 5994 : 0.021484749391674995\n",
      "Training loss for batch 5995 : 0.006087651010602713\n",
      "Training loss for batch 5996 : 0.09653270244598389\n",
      "Training loss for batch 5997 : 0.25035321712493896\n",
      "Training loss for batch 5998 : 0.21219979226589203\n",
      "Training loss for batch 5999 : 0.2836154103279114\n",
      "Training loss for batch 6000 : -0.0009739447268657386\n",
      "Training loss for batch 6001 : 0.13104303181171417\n",
      "Training loss for batch 6002 : 0.2096688598394394\n",
      "Training loss for batch 6003 : 0.13191969692707062\n",
      "Training loss for batch 6004 : 0.5959252119064331\n",
      "Training loss for batch 6005 : 0.21746684610843658\n",
      "Training loss for batch 6006 : 0.18475571274757385\n",
      "Training loss for batch 6007 : 0.019658295437693596\n",
      "Training loss for batch 6008 : 0.05415041744709015\n",
      "Training loss for batch 6009 : 0.13329823315143585\n",
      "Training loss for batch 6010 : 0.05159673094749451\n",
      "Training loss for batch 6011 : 0.2659744620323181\n",
      "Training loss for batch 6012 : 0.16471026837825775\n",
      "Training loss for batch 6013 : 0.3057401180267334\n",
      "Training loss for batch 6014 : 0.16611284017562866\n",
      "Training loss for batch 6015 : -0.002576961647719145\n",
      "Training loss for batch 6016 : 0.14036217331886292\n",
      "Training loss for batch 6017 : 0.6340302228927612\n",
      "Training loss for batch 6018 : 0.08404121547937393\n",
      "Training loss for batch 6019 : 0.27315330505371094\n",
      "Training loss for batch 6020 : 0.012914590537548065\n",
      "Training loss for batch 6021 : 0.12317810952663422\n",
      "Training loss for batch 6022 : 0.1699495166540146\n",
      "Training loss for batch 6023 : 0.10370615869760513\n",
      "Training loss for batch 6024 : 0.03373462334275246\n",
      "Training loss for batch 6025 : 0.0022044829092919827\n",
      "Training loss for batch 6026 : 0.2034248411655426\n",
      "Training loss for batch 6027 : 0.1838676482439041\n",
      "Training loss for batch 6028 : 0.2470364272594452\n",
      "Training loss for batch 6029 : 0.06199568510055542\n",
      "Training loss for batch 6030 : 0.1068057268857956\n",
      "Training loss for batch 6031 : 0.21631310880184174\n",
      "Training loss for batch 6032 : 0.1451980620622635\n",
      "Training loss for batch 6033 : 0.3305109441280365\n",
      "Training loss for batch 6034 : 0.06917200982570648\n",
      "Training loss for batch 6035 : 0.04284131899476051\n",
      "Training loss for batch 6036 : 0.2987956404685974\n",
      "Training loss for batch 6037 : 0.2524312138557434\n",
      "Training loss for batch 6038 : 0.22529390454292297\n",
      "Training loss for batch 6039 : 0.2884035110473633\n",
      "Training loss for batch 6040 : 0.15009306371212006\n",
      "Training loss for batch 6041 : 0.027412910014390945\n",
      "Training loss for batch 6042 : 0.31562796235084534\n",
      "Training loss for batch 6043 : 0.05582566186785698\n",
      "Training loss for batch 6044 : 0.11371481418609619\n",
      "Training loss for batch 6045 : 0.11965171992778778\n",
      "Training loss for batch 6046 : 0.28233686089515686\n",
      "Training loss for batch 6047 : 0.25673118233680725\n",
      "Training loss for batch 6048 : 0.028248626738786697\n",
      "Training loss for batch 6049 : 0.197297140955925\n",
      "Training loss for batch 6050 : 0.3380098342895508\n",
      "Training loss for batch 6051 : 0.08054745197296143\n",
      "Training loss for batch 6052 : 0.22347743809223175\n",
      "Training loss for batch 6053 : 0.48525869846343994\n",
      "Training loss for batch 6054 : 0.21298183500766754\n",
      "Training loss for batch 6055 : 0.2957373857498169\n",
      "Training loss for batch 6056 : 0.059086937457323074\n",
      "Training loss for batch 6057 : 0.12982873618602753\n",
      "Training loss for batch 6058 : 0.07572998851537704\n",
      "Training loss for batch 6059 : 0.19301491975784302\n",
      "Training loss for batch 6060 : 0.0890793576836586\n",
      "Training loss for batch 6061 : 0.02704215608537197\n",
      "Training loss for batch 6062 : 0.028636621311306953\n",
      "Training loss for batch 6063 : 0.1588292419910431\n",
      "Training loss for batch 6064 : 0.21884699165821075\n",
      "Training loss for batch 6065 : 0.16669771075248718\n",
      "Training loss for batch 6066 : 0.12996099889278412\n",
      "Training loss for batch 6067 : 0.17240485548973083\n",
      "Training loss for batch 6068 : 0.08925427496433258\n",
      "Training loss for batch 6069 : 0.16488157212734222\n",
      "Training loss for batch 6070 : 0.038499847054481506\n",
      "Training loss for batch 6071 : 0.03497530519962311\n",
      "Training loss for batch 6072 : -0.000215478939935565\n",
      "Training loss for batch 6073 : 0.0\n",
      "Training loss for batch 6074 : 0.0985926166176796\n",
      "Training loss for batch 6075 : 0.09522154182195663\n",
      "Training loss for batch 6076 : 0.005799253936856985\n",
      "Training loss for batch 6077 : 0.08754970878362656\n",
      "Training loss for batch 6078 : 0.14320078492164612\n",
      "Training loss for batch 6079 : 0.17471182346343994\n",
      "Training loss for batch 6080 : 0.05541853606700897\n",
      "Training loss for batch 6081 : 0.26770660281181335\n",
      "Training loss for batch 6082 : 0.14169180393218994\n",
      "Training loss for batch 6083 : 0.23441916704177856\n",
      "Training loss for batch 6084 : 0.3059214949607849\n",
      "Training loss for batch 6085 : 0.25005480647087097\n",
      "Training loss for batch 6086 : 0.05094027519226074\n",
      "Training loss for batch 6087 : 0.45655930042266846\n",
      "Training loss for batch 6088 : 0.11995652318000793\n",
      "Training loss for batch 6089 : 0.09112949669361115\n",
      "Training loss for batch 6090 : 0.17301589250564575\n",
      "Training loss for batch 6091 : 0.28302982449531555\n",
      "Training loss for batch 6092 : 0.14876550436019897\n",
      "Training loss for batch 6093 : 0.15130218863487244\n",
      "Training loss for batch 6094 : 0.0581754706799984\n",
      "Training loss for batch 6095 : 0.008739620447158813\n",
      "Training loss for batch 6096 : 0.21914762258529663\n",
      "Training loss for batch 6097 : 0.19066710770130157\n",
      "Training loss for batch 6098 : 0.09472113102674484\n",
      "Training loss for batch 6099 : 0.07412207126617432\n",
      "Training loss for batch 6100 : 0.23140265047550201\n",
      "Training loss for batch 6101 : 0.09316372126340866\n",
      "Training loss for batch 6102 : 0.23519520461559296\n",
      "Training loss for batch 6103 : 0.026393651962280273\n",
      "Training loss for batch 6104 : 0.2674330770969391\n",
      "Training loss for batch 6105 : 0.1817300021648407\n",
      "Training loss for batch 6106 : 0.026574179530143738\n",
      "Training loss for batch 6107 : 0.07504745572805405\n",
      "Training loss for batch 6108 : 0.11271150410175323\n",
      "Training loss for batch 6109 : 0.30624136328697205\n",
      "Training loss for batch 6110 : 0.1579667329788208\n",
      "Training loss for batch 6111 : 0.28039899468421936\n",
      "Training loss for batch 6112 : 0.1990579068660736\n",
      "Training loss for batch 6113 : 0.32374870777130127\n",
      "Training loss for batch 6114 : 0.05869079381227493\n",
      "Training loss for batch 6115 : 0.17362503707408905\n",
      "Training loss for batch 6116 : 0.08704780042171478\n",
      "Training loss for batch 6117 : 0.011512098833918571\n",
      "Training loss for batch 6118 : 0.19352354109287262\n",
      "Training loss for batch 6119 : 0.07924206554889679\n",
      "Training loss for batch 6120 : 0.06844756752252579\n",
      "Training loss for batch 6121 : 0.016765743494033813\n",
      "Training loss for batch 6122 : 0.380520761013031\n",
      "Training loss for batch 6123 : 0.18699125945568085\n",
      "Training loss for batch 6124 : 0.03601321950554848\n",
      "Training loss for batch 6125 : 0.15430866181850433\n",
      "Training loss for batch 6126 : 0.27079829573631287\n",
      "Training loss for batch 6127 : 0.22353112697601318\n",
      "Training loss for batch 6128 : 0.2642344534397125\n",
      "Training loss for batch 6129 : 0.06061220169067383\n",
      "Training loss for batch 6130 : 0.084386445581913\n",
      "Training loss for batch 6131 : 0.3267717957496643\n",
      "Training loss for batch 6132 : 0.11455706506967545\n",
      "Training loss for batch 6133 : 0.10856438428163528\n",
      "Training loss for batch 6134 : 0.10494827479124069\n",
      "Training loss for batch 6135 : 0.09851372241973877\n",
      "Training loss for batch 6136 : 0.0211546178907156\n",
      "Training loss for batch 6137 : 0.06414079666137695\n",
      "Training loss for batch 6138 : 0.2308247983455658\n",
      "Training loss for batch 6139 : 0.22288699448108673\n",
      "Training loss for batch 6140 : 0.07135115563869476\n",
      "Training loss for batch 6141 : 0.4051395356655121\n",
      "Training loss for batch 6142 : 0.09034916758537292\n",
      "Training loss for batch 6143 : 0.09758689254522324\n",
      "Training loss for batch 6144 : 0.00918794609606266\n",
      "Training loss for batch 6145 : 0.006003588438034058\n",
      "Training loss for batch 6146 : 0.007277653552591801\n",
      "Training loss for batch 6147 : 0.023731356486678123\n",
      "Training loss for batch 6148 : 0.1070384755730629\n",
      "Training loss for batch 6149 : 0.039415474981069565\n",
      "Training loss for batch 6150 : 0.14657282829284668\n",
      "Training loss for batch 6151 : 0.03226843848824501\n",
      "Training loss for batch 6152 : 0.06814368069171906\n",
      "Training loss for batch 6153 : 0.3294715881347656\n",
      "Training loss for batch 6154 : 0.11297819763422012\n",
      "Training loss for batch 6155 : 0.010900996625423431\n",
      "Training loss for batch 6156 : 0.3990184962749481\n",
      "Training loss for batch 6157 : 0.2716432213783264\n",
      "Training loss for batch 6158 : 0.0032622949220240116\n",
      "Training loss for batch 6159 : 0.30652937293052673\n",
      "Training loss for batch 6160 : 0.3820056617259979\n",
      "Training loss for batch 6161 : 0.04048444703221321\n",
      "Training loss for batch 6162 : 0.07835643738508224\n",
      "Training loss for batch 6163 : 0.12172728776931763\n",
      "Training loss for batch 6164 : 0.2518335282802582\n",
      "Training loss for batch 6165 : 0.2838066518306732\n",
      "Training loss for batch 6166 : 0.31442543864250183\n",
      "Training loss for batch 6167 : 0.0632835403084755\n",
      "Training loss for batch 6168 : 0.11512095481157303\n",
      "Training loss for batch 6169 : 0.07011277973651886\n",
      "Training loss for batch 6170 : 0.3939368426799774\n",
      "Training loss for batch 6171 : 0.007339735981076956\n",
      "Training loss for batch 6172 : 0.05026869848370552\n",
      "Training loss for batch 6173 : 0.48948460817337036\n",
      "Training loss for batch 6174 : 0.21257208287715912\n",
      "Training loss for batch 6175 : 0.3129650950431824\n",
      "Training loss for batch 6176 : 0.010054310783743858\n",
      "Training loss for batch 6177 : 0.06610329449176788\n",
      "Training loss for batch 6178 : 0.11456692963838577\n",
      "Training loss for batch 6179 : 0.11493238061666489\n",
      "Training loss for batch 6180 : 0.28519895672798157\n",
      "Training loss for batch 6181 : 0.07255923002958298\n",
      "Training loss for batch 6182 : 0.22927136719226837\n",
      "Training loss for batch 6183 : 0.18267953395843506\n",
      "Training loss for batch 6184 : 0.1567932516336441\n",
      "Training loss for batch 6185 : 0.38867583870887756\n",
      "Training loss for batch 6186 : 0.06013379245996475\n",
      "Training loss for batch 6187 : 0.018473049625754356\n",
      "Training loss for batch 6188 : 0.09515413641929626\n",
      "Training loss for batch 6189 : 0.11578287184238434\n",
      "Training loss for batch 6190 : 0.2082681804895401\n",
      "Training loss for batch 6191 : 0.17273633182048798\n",
      "Training loss for batch 6192 : 0.21798758208751678\n",
      "Training loss for batch 6193 : 0.15005865693092346\n",
      "Training loss for batch 6194 : 0.0976964682340622\n",
      "Training loss for batch 6195 : 0.16320998966693878\n",
      "Training loss for batch 6196 : 0.056250933557748795\n",
      "Training loss for batch 6197 : 0.2540346384048462\n",
      "Training loss for batch 6198 : 0.08471745252609253\n",
      "Training loss for batch 6199 : 0.2897889018058777\n",
      "Training loss for batch 6200 : 0.12135293334722519\n",
      "Training loss for batch 6201 : 0.033693332225084305\n",
      "Training loss for batch 6202 : 0.3857699930667877\n",
      "Training loss for batch 6203 : 0.25843510031700134\n",
      "Training loss for batch 6204 : 0.04167288914322853\n",
      "Training loss for batch 6205 : 0.4758310317993164\n",
      "Training loss for batch 6206 : 0.06469555199146271\n",
      "Training loss for batch 6207 : 0.14608055353164673\n",
      "Training loss for batch 6208 : 0.12766413390636444\n",
      "Training loss for batch 6209 : 0.21359536051750183\n",
      "Training loss for batch 6210 : 0.16999103128910065\n",
      "Training loss for batch 6211 : 0.3311864733695984\n",
      "Training loss for batch 6212 : 0.08056759834289551\n",
      "Training loss for batch 6213 : 0.010551730170845985\n",
      "Training loss for batch 6214 : 0.27808260917663574\n",
      "Training loss for batch 6215 : 0.030076544731855392\n",
      "Training loss for batch 6216 : 0.011422773823142052\n",
      "Training loss for batch 6217 : 0.06053704023361206\n",
      "Training loss for batch 6218 : 0.29328081011772156\n",
      "Training loss for batch 6219 : 0.26002418994903564\n",
      "Training loss for batch 6220 : 0.06823287159204483\n",
      "Training loss for batch 6221 : 0.14660035073757172\n",
      "Training loss for batch 6222 : 0.3258976340293884\n",
      "Training loss for batch 6223 : 0.08771394193172455\n",
      "Training loss for batch 6224 : 0.05625760927796364\n",
      "Training loss for batch 6225 : 0.010416547767817974\n",
      "Training loss for batch 6226 : 0.08670662343502045\n",
      "Training loss for batch 6227 : 0.14253675937652588\n",
      "Training loss for batch 6228 : 0.014377663843333721\n",
      "Training loss for batch 6229 : 0.16423353552818298\n",
      "Training loss for batch 6230 : 0.3421158194541931\n",
      "Training loss for batch 6231 : 0.3527161478996277\n",
      "Training loss for batch 6232 : 0.24128732085227966\n",
      "Training loss for batch 6233 : 0.013023416511714458\n",
      "Training loss for batch 6234 : 0.23384703695774078\n",
      "Training loss for batch 6235 : 0.16254378855228424\n",
      "Training loss for batch 6236 : 0.18798594176769257\n",
      "Training loss for batch 6237 : 0.008117477409541607\n",
      "Training loss for batch 6238 : 0.36658769845962524\n",
      "Training loss for batch 6239 : 0.17769378423690796\n",
      "Training loss for batch 6240 : 0.246955007314682\n",
      "Training loss for batch 6241 : 0.0014465253334492445\n",
      "Training loss for batch 6242 : 0.08091098815202713\n",
      "Training loss for batch 6243 : 0.0805235207080841\n",
      "Training loss for batch 6244 : 0.0\n",
      "Training loss for batch 6245 : 0.1608416885137558\n",
      "Training loss for batch 6246 : 0.0741007924079895\n",
      "Training loss for batch 6247 : 0.2695866823196411\n",
      "Training loss for batch 6248 : 0.14568136632442474\n",
      "Training loss for batch 6249 : 0.1298273503780365\n",
      "Training loss for batch 6250 : 0.20781052112579346\n",
      "Training loss for batch 6251 : 0.6300347447395325\n",
      "Training loss for batch 6252 : 0.11457677185535431\n",
      "Training loss for batch 6253 : 0.1868542730808258\n",
      "Training loss for batch 6254 : 0.08601997792720795\n",
      "Training loss for batch 6255 : 0.43849244713783264\n",
      "Training loss for batch 6256 : 0.1579117327928543\n",
      "Training loss for batch 6257 : 0.1223226860165596\n",
      "Training loss for batch 6258 : 0.11571186035871506\n",
      "Training loss for batch 6259 : 0.31942057609558105\n",
      "Training loss for batch 6260 : 0.331292986869812\n",
      "Training loss for batch 6261 : 0.09764043241739273\n",
      "Training loss for batch 6262 : 0.1566469669342041\n",
      "Training loss for batch 6263 : 0.09982431679964066\n",
      "Training loss for batch 6264 : 0.22565476596355438\n",
      "Training loss for batch 6265 : 0.19747593998908997\n",
      "Training loss for batch 6266 : 0.0012137492885813117\n",
      "Training loss for batch 6267 : 0.027339313179254532\n",
      "Training loss for batch 6268 : 0.15215490758419037\n",
      "Training loss for batch 6269 : 0.3221453130245209\n",
      "Training loss for batch 6270 : 0.33777549862861633\n",
      "Training loss for batch 6271 : 0.14375926554203033\n",
      "Training loss for batch 6272 : 0.2739070951938629\n",
      "Training loss for batch 6273 : 0.07099522650241852\n",
      "Training loss for batch 6274 : 0.08424528688192368\n",
      "Training loss for batch 6275 : 0.01625591143965721\n",
      "Training loss for batch 6276 : 0.030400102958083153\n",
      "Training loss for batch 6277 : 0.42682841420173645\n",
      "Training loss for batch 6278 : 0.3325442671775818\n",
      "Training loss for batch 6279 : 0.05082276463508606\n",
      "Training loss for batch 6280 : 0.0897669866681099\n",
      "Training loss for batch 6281 : 0.505045473575592\n",
      "Training loss for batch 6282 : 0.13764263689517975\n",
      "Training loss for batch 6283 : 0.1842787265777588\n",
      "Training loss for batch 6284 : 0.16307616233825684\n",
      "Training loss for batch 6285 : 0.05534312501549721\n",
      "Training loss for batch 6286 : 0.09027980268001556\n",
      "Training loss for batch 6287 : 0.07789496332406998\n",
      "Training loss for batch 6288 : 0.12876170873641968\n",
      "Training loss for batch 6289 : 0.5097938179969788\n",
      "Training loss for batch 6290 : 0.034923188388347626\n",
      "Training loss for batch 6291 : 0.28974229097366333\n",
      "Training loss for batch 6292 : 0.11522609740495682\n",
      "Training loss for batch 6293 : 0.1815641224384308\n",
      "Training loss for batch 6294 : 0.31646427512168884\n",
      "Training loss for batch 6295 : 0.08214939385652542\n",
      "Training loss for batch 6296 : 0.05694250762462616\n",
      "Training loss for batch 6297 : 0.023687997832894325\n",
      "Training loss for batch 6298 : 0.07171471416950226\n",
      "Training loss for batch 6299 : 0.07514916360378265\n",
      "Training loss for batch 6300 : 0.021596357226371765\n",
      "Training loss for batch 6301 : 0.19399473071098328\n",
      "Training loss for batch 6302 : 0.048624422401189804\n",
      "Training loss for batch 6303 : 0.2763294279575348\n",
      "Training loss for batch 6304 : 0.12468434125185013\n",
      "Training loss for batch 6305 : 0.050577279180288315\n",
      "Training loss for batch 6306 : 0.024363543838262558\n",
      "Training loss for batch 6307 : 0.013545848429203033\n",
      "Training loss for batch 6308 : 0.02440861240029335\n",
      "Training loss for batch 6309 : 0.09177935868501663\n",
      "Training loss for batch 6310 : 0.023454083129763603\n",
      "Training loss for batch 6311 : 0.39868268370628357\n",
      "Training loss for batch 6312 : 0.23523835837841034\n",
      "Training loss for batch 6313 : 0.30081191658973694\n",
      "Training loss for batch 6314 : 0.15279105305671692\n",
      "Training loss for batch 6315 : 0.22253349423408508\n",
      "Training loss for batch 6316 : 0.0862663984298706\n",
      "Training loss for batch 6317 : 0.07170802354812622\n",
      "Training loss for batch 6318 : 0.37823691964149475\n",
      "Training loss for batch 6319 : 0.05948587879538536\n",
      "Training loss for batch 6320 : 0.2773900032043457\n",
      "Training loss for batch 6321 : 0.061160895973443985\n",
      "Training loss for batch 6322 : 0.23931916058063507\n",
      "Training loss for batch 6323 : 0.06999263912439346\n",
      "Training loss for batch 6324 : 0.02790960669517517\n",
      "Training loss for batch 6325 : 0.24110138416290283\n",
      "Training loss for batch 6326 : 0.08494441211223602\n",
      "Training loss for batch 6327 : 0.23183029890060425\n",
      "Training loss for batch 6328 : 0.08850868046283722\n",
      "Training loss for batch 6329 : 0.45927363634109497\n",
      "Training loss for batch 6330 : 0.004546974319964647\n",
      "Training loss for batch 6331 : 0.2319822609424591\n",
      "Training loss for batch 6332 : 0.2998236119747162\n",
      "Training loss for batch 6333 : 0.290551096200943\n",
      "Training loss for batch 6334 : 0.2774569094181061\n",
      "Training loss for batch 6335 : 0.0026870376896113157\n",
      "Training loss for batch 6336 : 0.03643506020307541\n",
      "Training loss for batch 6337 : 0.2762968838214874\n",
      "Training loss for batch 6338 : 0.12220489978790283\n",
      "Training loss for batch 6339 : 0.1321752369403839\n",
      "Training loss for batch 6340 : 0.25036174058914185\n",
      "Training loss for batch 6341 : 0.031945787370204926\n",
      "Training loss for batch 6342 : 0.15750576555728912\n",
      "Training loss for batch 6343 : 0.22296607494354248\n",
      "Training loss for batch 6344 : 0.09708993136882782\n",
      "Training loss for batch 6345 : 0.08679590374231339\n",
      "Training loss for batch 6346 : 0.0\n",
      "Training loss for batch 6347 : 0.34200266003608704\n",
      "Training loss for batch 6348 : 0.10287195444107056\n",
      "Training loss for batch 6349 : 0.10397019982337952\n",
      "Training loss for batch 6350 : 0.15874257683753967\n",
      "Training loss for batch 6351 : 0.35905182361602783\n",
      "Training loss for batch 6352 : 0.19496116042137146\n",
      "Training loss for batch 6353 : 0.12161695957183838\n",
      "Training loss for batch 6354 : 0.35576993227005005\n",
      "Training loss for batch 6355 : 0.08417271077632904\n",
      "Training loss for batch 6356 : 0.015273167751729488\n",
      "Training loss for batch 6357 : 0.2532021701335907\n",
      "Training loss for batch 6358 : 0.5066284537315369\n",
      "Training loss for batch 6359 : 0.13849127292633057\n",
      "Training loss for batch 6360 : 0.12034136056900024\n",
      "Training loss for batch 6361 : 0.316673219203949\n",
      "Training loss for batch 6362 : 0.30962681770324707\n",
      "Training loss for batch 6363 : 0.18199275434017181\n",
      "Training loss for batch 6364 : 0.2920507788658142\n",
      "Training loss for batch 6365 : 0.33058691024780273\n",
      "Training loss for batch 6366 : 0.08151517808437347\n",
      "Training loss for batch 6367 : 0.134549081325531\n",
      "Training loss for batch 6368 : 0.22897934913635254\n",
      "Training loss for batch 6369 : 0.25147995352745056\n",
      "Training loss for batch 6370 : 0.0068914019502699375\n",
      "Training loss for batch 6371 : 0.28843721747398376\n",
      "Training loss for batch 6372 : 0.08613522350788116\n",
      "Training loss for batch 6373 : 0.31722745299339294\n",
      "Training loss for batch 6374 : 0.37635648250579834\n",
      "Training loss for batch 6375 : 0.014031041413545609\n",
      "Training loss for batch 6376 : 0.021775415167212486\n",
      "Training loss for batch 6377 : 0.02851676382124424\n",
      "Training loss for batch 6378 : 0.11771228909492493\n",
      "Training loss for batch 6379 : 0.22493718564510345\n",
      "Training loss for batch 6380 : 0.3101290464401245\n",
      "Training loss for batch 6381 : 0.21385681629180908\n",
      "Training loss for batch 6382 : 0.2274181842803955\n",
      "Training loss for batch 6383 : 0.025597766041755676\n",
      "Training loss for batch 6384 : 0.15696656703948975\n",
      "Training loss for batch 6385 : 0.09485004842281342\n",
      "Training loss for batch 6386 : 0.1639300286769867\n",
      "Training loss for batch 6387 : 0.354057639837265\n",
      "Training loss for batch 6388 : 0.023761313408613205\n",
      "Training loss for batch 6389 : 0.18612155318260193\n",
      "Training loss for batch 6390 : 0.012322336435317993\n",
      "Training loss for batch 6391 : 0.35273826122283936\n",
      "Training loss for batch 6392 : 0.45359569787979126\n",
      "Training loss for batch 6393 : 0.21789593994617462\n",
      "Training loss for batch 6394 : 0.10753154754638672\n",
      "Training loss for batch 6395 : 0.06830953061580658\n",
      "Training loss for batch 6396 : 0.18773718178272247\n",
      "Training loss for batch 6397 : 0.15838533639907837\n",
      "Training loss for batch 6398 : 0.09430187940597534\n",
      "Training loss for batch 6399 : 0.15892045199871063\n",
      "Training loss for batch 6400 : 0.3891282379627228\n",
      "Training loss for batch 6401 : 0.06847753375768661\n",
      "Training loss for batch 6402 : 0.10026567429304123\n",
      "Training loss for batch 6403 : 0.11955317109823227\n",
      "Training loss for batch 6404 : 0.011325186118483543\n",
      "Training loss for batch 6405 : 0.2994655668735504\n",
      "Training loss for batch 6406 : 0.03340179845690727\n",
      "Training loss for batch 6407 : 0.02919960767030716\n",
      "Training loss for batch 6408 : 0.11881322413682938\n",
      "Training loss for batch 6409 : 0.08252692967653275\n",
      "Training loss for batch 6410 : 0.11204054951667786\n",
      "Training loss for batch 6411 : 0.027243666350841522\n",
      "Training loss for batch 6412 : 0.4398130476474762\n",
      "Training loss for batch 6413 : 0.29350849986076355\n",
      "Training loss for batch 6414 : 0.09629469364881516\n",
      "Training loss for batch 6415 : 0.04588548466563225\n",
      "Training loss for batch 6416 : 0.06896233558654785\n",
      "Training loss for batch 6417 : 0.04420226067304611\n",
      "Training loss for batch 6418 : 0.2931970953941345\n",
      "Training loss for batch 6419 : 0.22422128915786743\n",
      "Training loss for batch 6420 : 0.012780560180544853\n",
      "Training loss for batch 6421 : 0.32822659611701965\n",
      "Training loss for batch 6422 : 0.2775234580039978\n",
      "Training loss for batch 6423 : 0.06319296360015869\n",
      "Training loss for batch 6424 : 0.10892017185688019\n",
      "Training loss for batch 6425 : 0.10023514926433563\n",
      "Training loss for batch 6426 : -0.00012294104089960456\n",
      "Training loss for batch 6427 : 0.08582192659378052\n",
      "Training loss for batch 6428 : 0.1535108983516693\n",
      "Training loss for batch 6429 : 0.11071725189685822\n",
      "Training loss for batch 6430 : 0.303667277097702\n",
      "Training loss for batch 6431 : 0.0478932186961174\n",
      "Training loss for batch 6432 : 0.0\n",
      "Training loss for batch 6433 : 0.02591691166162491\n",
      "Training loss for batch 6434 : 0.12381523847579956\n",
      "Training loss for batch 6435 : 0.128648579120636\n",
      "Training loss for batch 6436 : 0.2716539204120636\n",
      "Training loss for batch 6437 : 0.39711254835128784\n",
      "Training loss for batch 6438 : 0.05539880692958832\n",
      "Training loss for batch 6439 : 0.010886158794164658\n",
      "Training loss for batch 6440 : 0.15564318001270294\n",
      "Training loss for batch 6441 : 0.22111637890338898\n",
      "Training loss for batch 6442 : 0.23556877672672272\n",
      "Training loss for batch 6443 : 0.15797361731529236\n",
      "Training loss for batch 6444 : 0.15075495839118958\n",
      "Training loss for batch 6445 : 0.2536452114582062\n",
      "Training loss for batch 6446 : 0.07411330938339233\n",
      "Training loss for batch 6447 : 0.22260867059230804\n",
      "Training loss for batch 6448 : 0.2566719651222229\n",
      "Training loss for batch 6449 : 0.15943993628025055\n",
      "Training loss for batch 6450 : 0.12023456394672394\n",
      "Training loss for batch 6451 : 0.21613362431526184\n",
      "Training loss for batch 6452 : 0.2918183505535126\n",
      "Training loss for batch 6453 : 0.3088437616825104\n",
      "Training loss for batch 6454 : 0.0011005004635080695\n",
      "Training loss for batch 6455 : 0.2900250256061554\n",
      "Training loss for batch 6456 : 0.2309969961643219\n",
      "Training loss for batch 6457 : 0.06367960572242737\n",
      "Training loss for batch 6458 : 0.42574775218963623\n",
      "Training loss for batch 6459 : 0.09122408926486969\n",
      "Training loss for batch 6460 : 0.15958960354328156\n",
      "Training loss for batch 6461 : 0.31729158759117126\n",
      "Training loss for batch 6462 : 0.12076261639595032\n",
      "Training loss for batch 6463 : 0.10519902408123016\n",
      "Training loss for batch 6464 : 0.1594487875699997\n",
      "Training loss for batch 6465 : -0.0010650248732417822\n",
      "Training loss for batch 6466 : 0.15361514687538147\n",
      "Training loss for batch 6467 : 0.3355307877063751\n",
      "Training loss for batch 6468 : 0.1753428429365158\n",
      "Training loss for batch 6469 : 0.005703519098460674\n",
      "Training loss for batch 6470 : 0.19009646773338318\n",
      "Training loss for batch 6471 : 0.0424555204808712\n",
      "Training loss for batch 6472 : 0.11702433228492737\n",
      "Training loss for batch 6473 : 0.11988318711519241\n",
      "Training loss for batch 6474 : 0.08637417107820511\n",
      "Training loss for batch 6475 : 0.015657350420951843\n",
      "Training loss for batch 6476 : 0.034379735589027405\n",
      "Training loss for batch 6477 : 0.2567862570285797\n",
      "Training loss for batch 6478 : 0.13023605942726135\n",
      "Training loss for batch 6479 : 0.6509345173835754\n",
      "Training loss for batch 6480 : 0.09668493270874023\n",
      "Training loss for batch 6481 : 0.21520331501960754\n",
      "Training loss for batch 6482 : 0.20311947166919708\n",
      "Training loss for batch 6483 : 0.11684046685695648\n",
      "Training loss for batch 6484 : 0.027558762580156326\n",
      "Training loss for batch 6485 : 0.23582732677459717\n",
      "Training loss for batch 6486 : 0.25066500902175903\n",
      "Training loss for batch 6487 : 0.02347726747393608\n",
      "Training loss for batch 6488 : 0.23346734046936035\n",
      "Training loss for batch 6489 : 0.18725226819515228\n",
      "Training loss for batch 6490 : 0.15228430926799774\n",
      "Training loss for batch 6491 : 0.3597676753997803\n",
      "Training loss for batch 6492 : 0.019008208066225052\n",
      "Training loss for batch 6493 : 0.26119348406791687\n",
      "Training loss for batch 6494 : 0.02746196649968624\n",
      "Training loss for batch 6495 : 0.3306039869785309\n",
      "Training loss for batch 6496 : -0.0008833737228997052\n",
      "Training loss for batch 6497 : 0.1141946092247963\n",
      "Training loss for batch 6498 : 0.1325189769268036\n",
      "Training loss for batch 6499 : 0.30722522735595703\n",
      "Training loss for batch 6500 : 0.06789644807577133\n",
      "Training loss for batch 6501 : 0.45517680048942566\n",
      "Training loss for batch 6502 : 0.0476405993103981\n",
      "Training loss for batch 6503 : 0.13407224416732788\n",
      "Training loss for batch 6504 : 0.34170615673065186\n",
      "Training loss for batch 6505 : 0.02865121141076088\n",
      "Training loss for batch 6506 : 0.3040599524974823\n",
      "Training loss for batch 6507 : 0.10772563517093658\n",
      "Training loss for batch 6508 : 0.01953718066215515\n",
      "Training loss for batch 6509 : 0.1108132004737854\n",
      "Training loss for batch 6510 : 0.3842542767524719\n",
      "Training loss for batch 6511 : 0.021564709022641182\n",
      "Training loss for batch 6512 : 0.08072062581777573\n",
      "Training loss for batch 6513 : 0.11225835233926773\n",
      "Training loss for batch 6514 : 0.14136484265327454\n",
      "Training loss for batch 6515 : 0.04830155894160271\n",
      "Training loss for batch 6516 : 0.06793380528688431\n",
      "Training loss for batch 6517 : 0.3217669129371643\n",
      "Training loss for batch 6518 : 0.22936028242111206\n",
      "Training loss for batch 6519 : 0.35974979400634766\n",
      "Training loss for batch 6520 : 0.04917571693658829\n",
      "Training loss for batch 6521 : 0.4177307188510895\n",
      "Training loss for batch 6522 : 0.38920027017593384\n",
      "Training loss for batch 6523 : 0.05899255722761154\n",
      "Training loss for batch 6524 : 0.10810715705156326\n",
      "Training loss for batch 6525 : 0.042838238179683685\n",
      "Training loss for batch 6526 : 0.032362762838602066\n",
      "Training loss for batch 6527 : 0.08100162446498871\n",
      "Training loss for batch 6528 : 0.046440932899713516\n",
      "Training loss for batch 6529 : 0.2807711362838745\n",
      "Training loss for batch 6530 : 0.12232352793216705\n",
      "Training loss for batch 6531 : 0.1606658399105072\n",
      "Training loss for batch 6532 : 0.024542680010199547\n",
      "Training loss for batch 6533 : 0.3313252925872803\n",
      "Training loss for batch 6534 : 0.023563534021377563\n",
      "Training loss for batch 6535 : 0.12391618639230728\n",
      "Training loss for batch 6536 : 0.02609569951891899\n",
      "Training loss for batch 6537 : 0.1190052181482315\n",
      "Training loss for batch 6538 : 0.19677038490772247\n",
      "Training loss for batch 6539 : 0.07658122479915619\n",
      "Training loss for batch 6540 : 0.22693157196044922\n",
      "Training loss for batch 6541 : 0.00016867122030816972\n",
      "Training loss for batch 6542 : 0.4714728891849518\n",
      "Training loss for batch 6543 : 0.4118676781654358\n",
      "Training loss for batch 6544 : 0.08063715696334839\n",
      "Training loss for batch 6545 : 0.20181316137313843\n",
      "Training loss for batch 6546 : 0.08987222611904144\n",
      "Training loss for batch 6547 : 0.027423862367868423\n",
      "Training loss for batch 6548 : 0.009620482102036476\n",
      "Training loss for batch 6549 : 0.05646820738911629\n",
      "Training loss for batch 6550 : 0.24325457215309143\n",
      "Training loss for batch 6551 : 0.16174738109111786\n",
      "Training loss for batch 6552 : 0.1141095906496048\n",
      "Training loss for batch 6553 : 0.10492746531963348\n",
      "Training loss for batch 6554 : 0.09726493060588837\n",
      "Training loss for batch 6555 : 0.20047003030776978\n",
      "Training loss for batch 6556 : 0.06238822638988495\n",
      "Training loss for batch 6557 : 0.22776921093463898\n",
      "Training loss for batch 6558 : 0.11012768745422363\n",
      "Training loss for batch 6559 : 0.2618873715400696\n",
      "Training loss for batch 6560 : 0.333403617143631\n",
      "Training loss for batch 6561 : 0.2886921167373657\n",
      "Training loss for batch 6562 : 0.10518862307071686\n",
      "Training loss for batch 6563 : 0.3551981449127197\n",
      "Training loss for batch 6564 : 0.2066371589899063\n",
      "Training loss for batch 6565 : 0.20695234835147858\n",
      "Training loss for batch 6566 : 0.2705693542957306\n",
      "Training loss for batch 6567 : 0.13262540102005005\n",
      "Training loss for batch 6568 : 0.2721464931964874\n",
      "Training loss for batch 6569 : 0.29076579213142395\n",
      "Training loss for batch 6570 : 0.05422147363424301\n",
      "Training loss for batch 6571 : 0.030283691361546516\n",
      "Training loss for batch 6572 : 0.10907826572656631\n",
      "Training loss for batch 6573 : 0.26133087277412415\n",
      "Training loss for batch 6574 : 0.17510873079299927\n",
      "Training loss for batch 6575 : 0.10766032338142395\n",
      "Training loss for batch 6576 : 0.11185610294342041\n",
      "Training loss for batch 6577 : 0.22909028828144073\n",
      "Training loss for batch 6578 : 0.12308377772569656\n",
      "Training loss for batch 6579 : 0.29663917422294617\n",
      "Training loss for batch 6580 : 0.03392031043767929\n",
      "Training loss for batch 6581 : 0.33728256821632385\n",
      "Training loss for batch 6582 : 0.060933779925107956\n",
      "Training loss for batch 6583 : 0.21749183535575867\n",
      "Training loss for batch 6584 : 0.03475860133767128\n",
      "Training loss for batch 6585 : 0.28275808691978455\n",
      "Training loss for batch 6586 : 0.07756999880075455\n",
      "Training loss for batch 6587 : 0.1354437619447708\n",
      "Training loss for batch 6588 : 0.3553893566131592\n",
      "Training loss for batch 6589 : 0.023040663450956345\n",
      "Training loss for batch 6590 : 0.15612702071666718\n",
      "Training loss for batch 6591 : 0.12387317419052124\n",
      "Training loss for batch 6592 : 0.14709056913852692\n",
      "Training loss for batch 6593 : 0.03493903577327728\n",
      "Training loss for batch 6594 : 0.25603553652763367\n",
      "Training loss for batch 6595 : 0.2669467628002167\n",
      "Training loss for batch 6596 : 0.3457648456096649\n",
      "Training loss for batch 6597 : 0.004461090080440044\n",
      "Training loss for batch 6598 : 0.18820595741271973\n",
      "Training loss for batch 6599 : 0.022830605506896973\n",
      "Training loss for batch 6600 : 0.22716161608695984\n",
      "Training loss for batch 6601 : 0.41226261854171753\n",
      "Training loss for batch 6602 : 0.3319302201271057\n",
      "Training loss for batch 6603 : 0.005715599283576012\n",
      "Training loss for batch 6604 : 0.2267657071352005\n",
      "Training loss for batch 6605 : 0.3710508644580841\n",
      "Training loss for batch 6606 : 0.06790953874588013\n",
      "Training loss for batch 6607 : 0.19127586483955383\n",
      "Training loss for batch 6608 : 0.025330981239676476\n",
      "Training loss for batch 6609 : 0.2619723677635193\n",
      "Training loss for batch 6610 : 0.12165912240743637\n",
      "Training loss for batch 6611 : 0.16584698855876923\n",
      "Training loss for batch 6612 : 0.10794737190008163\n",
      "Training loss for batch 6613 : 0.21780312061309814\n",
      "Training loss for batch 6614 : 0.23161886632442474\n",
      "Training loss for batch 6615 : 0.09954437613487244\n",
      "Training loss for batch 6616 : 0.21905049681663513\n",
      "Training loss for batch 6617 : 0.2127799540758133\n",
      "Training loss for batch 6618 : 0.2282319962978363\n",
      "Training loss for batch 6619 : 0.07928050309419632\n",
      "Training loss for batch 6620 : 0.17513258755207062\n",
      "Training loss for batch 6621 : 0.021975547075271606\n",
      "Training loss for batch 6622 : 0.09146802127361298\n",
      "Training loss for batch 6623 : 0.3180958926677704\n",
      "Training loss for batch 6624 : 0.10406343638896942\n",
      "Training loss for batch 6625 : 0.08861404657363892\n",
      "Training loss for batch 6626 : 0.14652279019355774\n",
      "Training loss for batch 6627 : 0.06340792775154114\n",
      "Training loss for batch 6628 : 0.05913659930229187\n",
      "Training loss for batch 6629 : 0.1956857442855835\n",
      "Training loss for batch 6630 : 0.03970128297805786\n",
      "Training loss for batch 6631 : 0.437855064868927\n",
      "Training loss for batch 6632 : 0.17956221103668213\n",
      "Training loss for batch 6633 : 0.21868233382701874\n",
      "Training loss for batch 6634 : 0.027021151036024094\n",
      "Training loss for batch 6635 : 0.06358008086681366\n",
      "Training loss for batch 6636 : 0.15096285939216614\n",
      "Training loss for batch 6637 : 0.1720520406961441\n",
      "Training loss for batch 6638 : 0.5794661045074463\n",
      "Training loss for batch 6639 : 0.003837674856185913\n",
      "Training loss for batch 6640 : 0.07410222291946411\n",
      "Training loss for batch 6641 : 0.32890093326568604\n",
      "Training loss for batch 6642 : 0.05419198051095009\n",
      "Training loss for batch 6643 : 0.12694808840751648\n",
      "Training loss for batch 6644 : 0.040297262370586395\n",
      "Training loss for batch 6645 : 0.23460711538791656\n",
      "Training loss for batch 6646 : 0.20774726569652557\n",
      "Training loss for batch 6647 : 0.03011177107691765\n",
      "Training loss for batch 6648 : 0.3259197473526001\n",
      "Training loss for batch 6649 : 0.48001575469970703\n",
      "Training loss for batch 6650 : 0.15266621112823486\n",
      "Training loss for batch 6651 : 0.16618365049362183\n",
      "Training loss for batch 6652 : 0.2233978658914566\n",
      "Training loss for batch 6653 : 0.4904276132583618\n",
      "Training loss for batch 6654 : 0.10360591113567352\n",
      "Training loss for batch 6655 : 0.592796266078949\n",
      "Training loss for batch 6656 : 0.24419574439525604\n",
      "Training loss for batch 6657 : 0.14626556634902954\n",
      "Training loss for batch 6658 : 0.22912482917308807\n",
      "Training loss for batch 6659 : 0.6820114850997925\n",
      "Training loss for batch 6660 : 0.24326235055923462\n",
      "Training loss for batch 6661 : 0.04702507331967354\n",
      "Training loss for batch 6662 : 0.062347412109375\n",
      "Training loss for batch 6663 : 0.05342484265565872\n",
      "Training loss for batch 6664 : 0.06936808675527573\n",
      "Training loss for batch 6665 : 0.10022340714931488\n",
      "Training loss for batch 6666 : 0.0029979285318404436\n",
      "Training loss for batch 6667 : 0.05498313903808594\n",
      "Training loss for batch 6668 : 0.3039064109325409\n",
      "Training loss for batch 6669 : 0.05339816212654114\n",
      "Training loss for batch 6670 : 0.012786508537828922\n",
      "Training loss for batch 6671 : 0.2505747675895691\n",
      "Training loss for batch 6672 : 0.21317307651042938\n",
      "Training loss for batch 6673 : 0.11624424159526825\n",
      "Training loss for batch 6674 : 0.09113219380378723\n",
      "Training loss for batch 6675 : 0.09963355958461761\n",
      "Training loss for batch 6676 : 0.04209566116333008\n",
      "Training loss for batch 6677 : 0.1728803515434265\n",
      "Training loss for batch 6678 : 0.29121676087379456\n",
      "Training loss for batch 6679 : 0.07783498615026474\n",
      "Training loss for batch 6680 : 0.10419733822345734\n",
      "Training loss for batch 6681 : 0.04081573337316513\n",
      "Training loss for batch 6682 : 0.2549682557582855\n",
      "Training loss for batch 6683 : 0.4281825125217438\n",
      "Training loss for batch 6684 : 0.234379380941391\n",
      "Training loss for batch 6685 : 0.25938066840171814\n",
      "Training loss for batch 6686 : 0.061934906989336014\n",
      "Training loss for batch 6687 : 0.3111366033554077\n",
      "Training loss for batch 6688 : 0.3752307593822479\n",
      "Training loss for batch 6689 : 0.24616849422454834\n",
      "Training loss for batch 6690 : 0.1693626195192337\n",
      "Training loss for batch 6691 : 0.30712658166885376\n",
      "Training loss for batch 6692 : 0.03617808222770691\n",
      "Training loss for batch 6693 : 0.08314047753810883\n",
      "Training loss for batch 6694 : 0.22735199332237244\n",
      "Training loss for batch 6695 : 0.07637505978345871\n",
      "Training loss for batch 6696 : 0.24702662229537964\n",
      "Training loss for batch 6697 : 0.2137908935546875\n",
      "Training loss for batch 6698 : 0.3284573256969452\n",
      "Training loss for batch 6699 : 0.2114107608795166\n",
      "Training loss for batch 6700 : 0.3403478264808655\n",
      "Training loss for batch 6701 : 0.17716221511363983\n",
      "Training loss for batch 6702 : 0.037821922451257706\n",
      "Training loss for batch 6703 : 0.015061388723552227\n",
      "Training loss for batch 6704 : 0.16471268236637115\n",
      "Training loss for batch 6705 : 0.22144852578639984\n",
      "Training loss for batch 6706 : 0.2780635356903076\n",
      "Training loss for batch 6707 : 0.03450198471546173\n",
      "Training loss for batch 6708 : 0.05641251057386398\n",
      "Training loss for batch 6709 : 0.1236824318766594\n",
      "Training loss for batch 6710 : 0.11947640031576157\n",
      "Training loss for batch 6711 : 0.010461685247719288\n",
      "Training loss for batch 6712 : 0.10926509648561478\n",
      "Training loss for batch 6713 : 0.1679583191871643\n",
      "Training loss for batch 6714 : 0.26984313130378723\n",
      "Training loss for batch 6715 : 0.19988127052783966\n",
      "Training loss for batch 6716 : 0.4138967990875244\n",
      "Training loss for batch 6717 : 0.0\n",
      "Training loss for batch 6718 : 0.37762221693992615\n",
      "Training loss for batch 6719 : 0.0861043632030487\n",
      "Training loss for batch 6720 : 0.19488076865673065\n",
      "Training loss for batch 6721 : 0.34053534269332886\n",
      "Training loss for batch 6722 : 0.12008675187826157\n",
      "Training loss for batch 6723 : 0.11885398626327515\n",
      "Training loss for batch 6724 : 0.2513062357902527\n",
      "Training loss for batch 6725 : 0.007213079370558262\n",
      "Training loss for batch 6726 : 0.32692256569862366\n",
      "Training loss for batch 6727 : 0.06399393081665039\n",
      "Training loss for batch 6728 : 0.005777877755463123\n",
      "Training loss for batch 6729 : 0.011342508718371391\n",
      "Training loss for batch 6730 : 0.10888315737247467\n",
      "Training loss for batch 6731 : 0.0579671785235405\n",
      "Training loss for batch 6732 : 0.029584085568785667\n",
      "Training loss for batch 6733 : 0.18026703596115112\n",
      "Training loss for batch 6734 : 0.03968339413404465\n",
      "Training loss for batch 6735 : 0.03554718941450119\n",
      "Training loss for batch 6736 : 0.14104397594928741\n",
      "Training loss for batch 6737 : 0.18787525594234467\n",
      "Training loss for batch 6738 : 0.0\n",
      "Training loss for batch 6739 : 0.2345445454120636\n",
      "Training loss for batch 6740 : 0.16199389100074768\n",
      "Training loss for batch 6741 : 0.15468324720859528\n",
      "Training loss for batch 6742 : 0.11725498735904694\n",
      "Training loss for batch 6743 : 0.07581685483455658\n",
      "Training loss for batch 6744 : 0.028545033186674118\n",
      "Training loss for batch 6745 : 0.38693103194236755\n",
      "Training loss for batch 6746 : 0.017741765826940536\n",
      "Training loss for batch 6747 : 0.0\n",
      "Training loss for batch 6748 : 0.2436712682247162\n",
      "Training loss for batch 6749 : 0.1184554249048233\n",
      "Training loss for batch 6750 : 0.12377661466598511\n",
      "Training loss for batch 6751 : 0.28098565340042114\n",
      "Training loss for batch 6752 : 0.09923364222049713\n",
      "Training loss for batch 6753 : 0.25900620222091675\n",
      "Training loss for batch 6754 : 0.010480032302439213\n",
      "Training loss for batch 6755 : 0.052027031779289246\n",
      "Training loss for batch 6756 : 0.001393317710608244\n",
      "Training loss for batch 6757 : 0.18160997331142426\n",
      "Training loss for batch 6758 : 0.08201809227466583\n",
      "Training loss for batch 6759 : 0.36732104420661926\n",
      "Training loss for batch 6760 : 0.26753634214401245\n",
      "Training loss for batch 6761 : 0.17618712782859802\n",
      "Training loss for batch 6762 : 0.05362110212445259\n",
      "Training loss for batch 6763 : 0.1778573840856552\n",
      "Training loss for batch 6764 : 0.07093861699104309\n",
      "Training loss for batch 6765 : 0.07800035178661346\n",
      "Training loss for batch 6766 : 0.17363452911376953\n",
      "Training loss for batch 6767 : 0.06368119269609451\n",
      "Training loss for batch 6768 : 0.02785491943359375\n",
      "Training loss for batch 6769 : 0.11432284861803055\n",
      "Training loss for batch 6770 : 0.18128639459609985\n",
      "Training loss for batch 6771 : 0.24142009019851685\n",
      "Training loss for batch 6772 : 0.0012280252994969487\n",
      "Training loss for batch 6773 : 0.08793795108795166\n",
      "Training loss for batch 6774 : 0.16135624051094055\n",
      "Training loss for batch 6775 : 0.12745769321918488\n",
      "Training loss for batch 6776 : 0.3284550905227661\n",
      "Training loss for batch 6777 : 0.5294750928878784\n",
      "Training loss for batch 6778 : 0.31941184401512146\n",
      "Training loss for batch 6779 : 0.13989295065402985\n",
      "Training loss for batch 6780 : 0.4298464357852936\n",
      "Training loss for batch 6781 : 0.18548569083213806\n",
      "Training loss for batch 6782 : 0.10088908672332764\n",
      "Training loss for batch 6783 : 0.18782185018062592\n",
      "Training loss for batch 6784 : 0.4358682334423065\n",
      "Training loss for batch 6785 : 0.0888129249215126\n",
      "Training loss for batch 6786 : 0.09611603617668152\n",
      "Training loss for batch 6787 : 0.19767600297927856\n",
      "Training loss for batch 6788 : 0.19175966084003448\n",
      "Training loss for batch 6789 : 0.0303715281188488\n",
      "Training loss for batch 6790 : 0.3887520730495453\n",
      "Training loss for batch 6791 : 0.014854910783469677\n",
      "Training loss for batch 6792 : 0.31847766041755676\n",
      "Training loss for batch 6793 : 0.06254485994577408\n",
      "Training loss for batch 6794 : 0.08118824660778046\n",
      "Training loss for batch 6795 : 0.15083575248718262\n",
      "Training loss for batch 6796 : 0.1261223405599594\n",
      "Training loss for batch 6797 : 0.13488979637622833\n",
      "Training loss for batch 6798 : 0.12591132521629333\n",
      "Training loss for batch 6799 : 0.030656609684228897\n",
      "Training loss for batch 6800 : 0.21108025312423706\n",
      "Training loss for batch 6801 : 0.08448085933923721\n",
      "Training loss for batch 6802 : 0.03771400451660156\n",
      "Training loss for batch 6803 : 0.18012143671512604\n",
      "Training loss for batch 6804 : 0.24441023170948029\n",
      "Training loss for batch 6805 : 0.10967054963111877\n",
      "Training loss for batch 6806 : 0.17606210708618164\n",
      "Training loss for batch 6807 : 0.03943391889333725\n",
      "Training loss for batch 6808 : 0.07027243822813034\n",
      "Training loss for batch 6809 : 0.20828877389431\n",
      "Training loss for batch 6810 : 0.03788658231496811\n",
      "Training loss for batch 6811 : 0.20823438465595245\n",
      "Training loss for batch 6812 : 0.13929392397403717\n",
      "Training loss for batch 6813 : 0.13990344107151031\n",
      "Training loss for batch 6814 : 0.09284047782421112\n",
      "Training loss for batch 6815 : 0.039975520223379135\n",
      "Training loss for batch 6816 : 0.013713706284761429\n",
      "Training loss for batch 6817 : 0.14991024136543274\n",
      "Training loss for batch 6818 : 0.09236302971839905\n",
      "Training loss for batch 6819 : 0.17629945278167725\n",
      "Training loss for batch 6820 : 0.31195786595344543\n",
      "Training loss for batch 6821 : 0.2461174726486206\n",
      "Training loss for batch 6822 : 0.11185047030448914\n",
      "Training loss for batch 6823 : 0.40789344906806946\n",
      "Training loss for batch 6824 : 0.24735206365585327\n",
      "Training loss for batch 6825 : 0.31647270917892456\n",
      "Training loss for batch 6826 : 0.14252647757530212\n",
      "Training loss for batch 6827 : 0.044691506773233414\n",
      "Training loss for batch 6828 : 0.21975162625312805\n",
      "Training loss for batch 6829 : 0.06814803183078766\n",
      "Training loss for batch 6830 : 0.20399966835975647\n",
      "Training loss for batch 6831 : 0.0006612560246139765\n",
      "Training loss for batch 6832 : 0.30777791142463684\n",
      "Training loss for batch 6833 : 0.09504207968711853\n",
      "Training loss for batch 6834 : 0.05278380215167999\n",
      "Training loss for batch 6835 : 0.2448781430721283\n",
      "Training loss for batch 6836 : 0.01224600337445736\n",
      "Training loss for batch 6837 : 0.18596118688583374\n",
      "Training loss for batch 6838 : 0.1102568656206131\n",
      "Training loss for batch 6839 : 0.0\n",
      "Training loss for batch 6840 : 0.18474441766738892\n",
      "Training loss for batch 6841 : 0.28853839635849\n",
      "Training loss for batch 6842 : 0.1139877513051033\n",
      "Training loss for batch 6843 : 0.22397808730602264\n",
      "Training loss for batch 6844 : 0.13060089945793152\n",
      "Training loss for batch 6845 : 0.0\n",
      "Training loss for batch 6846 : 0.3588791787624359\n",
      "Training loss for batch 6847 : 0.18133142590522766\n",
      "Training loss for batch 6848 : 0.07946452498435974\n",
      "Training loss for batch 6849 : 0.05443338677287102\n",
      "Training loss for batch 6850 : 0.0642932653427124\n",
      "Training loss for batch 6851 : 0.008014654740691185\n",
      "Training loss for batch 6852 : 0.3340000808238983\n",
      "Training loss for batch 6853 : 0.16407842934131622\n",
      "Training loss for batch 6854 : 0.1676241010427475\n",
      "Training loss for batch 6855 : 0.357858270406723\n",
      "Training loss for batch 6856 : 0.18877941370010376\n",
      "Training loss for batch 6857 : 0.2466380000114441\n",
      "Training loss for batch 6858 : 0.12437043339014053\n",
      "Training loss for batch 6859 : 0.4708159565925598\n",
      "Training loss for batch 6860 : 0.25868886709213257\n",
      "Training loss for batch 6861 : 0.17516858875751495\n",
      "Training loss for batch 6862 : 0.27190402150154114\n",
      "Training loss for batch 6863 : 0.014555473811924458\n",
      "Training loss for batch 6864 : 0.028659772127866745\n",
      "Training loss for batch 6865 : 0.23652824759483337\n",
      "Training loss for batch 6866 : 0.49446234107017517\n",
      "Training loss for batch 6867 : 0.12175139039754868\n",
      "Training loss for batch 6868 : 0.20545834302902222\n",
      "Training loss for batch 6869 : 0.15130925178527832\n",
      "Training loss for batch 6870 : 0.4269693195819855\n",
      "Training loss for batch 6871 : 0.08329806476831436\n",
      "Training loss for batch 6872 : 0.2550572156906128\n",
      "Training loss for batch 6873 : 0.08357628434896469\n",
      "Training loss for batch 6874 : 0.2433178573846817\n",
      "Training loss for batch 6875 : 0.017094722017645836\n",
      "Training loss for batch 6876 : 0.031796298921108246\n",
      "Training loss for batch 6877 : 0.12369398027658463\n",
      "Training loss for batch 6878 : 0.25755253434181213\n",
      "Training loss for batch 6879 : 0.07185181230306625\n",
      "Training loss for batch 6880 : 0.06074533984065056\n",
      "Training loss for batch 6881 : 0.28524383902549744\n",
      "Training loss for batch 6882 : 0.02343011274933815\n",
      "Training loss for batch 6883 : 0.08842753618955612\n",
      "Training loss for batch 6884 : 0.2758937478065491\n",
      "Training loss for batch 6885 : 0.1456676572561264\n",
      "Training loss for batch 6886 : 0.06542319059371948\n",
      "Training loss for batch 6887 : 0.20090839266777039\n",
      "Training loss for batch 6888 : 0.0\n",
      "Training loss for batch 6889 : 0.24647404253482819\n",
      "Training loss for batch 6890 : 0.10749474912881851\n",
      "Training loss for batch 6891 : 0.040939800441265106\n",
      "Training loss for batch 6892 : 0.16140228509902954\n",
      "Training loss for batch 6893 : 0.050061311572790146\n",
      "Training loss for batch 6894 : 0.04280232638120651\n",
      "Training loss for batch 6895 : 0.26594942808151245\n",
      "Training loss for batch 6896 : 0.3690201938152313\n",
      "Training loss for batch 6897 : 0.3357783854007721\n",
      "Training loss for batch 6898 : 0.21505963802337646\n",
      "Training loss for batch 6899 : 0.1261437088251114\n",
      "Training loss for batch 6900 : 0.062114909291267395\n",
      "Training loss for batch 6901 : 0.2562812268733978\n",
      "Training loss for batch 6902 : 0.24350886046886444\n",
      "Training loss for batch 6903 : 0.038146715611219406\n",
      "Training loss for batch 6904 : 0.058354754000902176\n",
      "Training loss for batch 6905 : 0.09138915687799454\n",
      "Training loss for batch 6906 : 0.07840853184461594\n",
      "Training loss for batch 6907 : 0.18976013362407684\n",
      "Training loss for batch 6908 : 0.08061888068914413\n",
      "Training loss for batch 6909 : 0.0401102751493454\n",
      "Training loss for batch 6910 : 0.40622419118881226\n",
      "Training loss for batch 6911 : 0.08889240771532059\n",
      "Training loss for batch 6912 : 0.10726990550756454\n",
      "Training loss for batch 6913 : 0.13888339698314667\n",
      "Training loss for batch 6914 : 0.3181852698326111\n",
      "Training loss for batch 6915 : 0.11447009444236755\n",
      "Training loss for batch 6916 : 0.1771908551454544\n",
      "Training loss for batch 6917 : 0.1778647005558014\n",
      "Training loss for batch 6918 : 0.10603874176740646\n",
      "Training loss for batch 6919 : 0.21647247672080994\n",
      "Training loss for batch 6920 : 0.15527530014514923\n",
      "Training loss for batch 6921 : 0.3432421088218689\n",
      "Training loss for batch 6922 : 0.10613453388214111\n",
      "Training loss for batch 6923 : 0.022198546677827835\n",
      "Training loss for batch 6924 : 0.5911300778388977\n",
      "Training loss for batch 6925 : 0.04705718532204628\n",
      "Training loss for batch 6926 : 0.13935717940330505\n",
      "Training loss for batch 6927 : 0.29857224225997925\n",
      "Training loss for batch 6928 : 0.07208866626024246\n",
      "Training loss for batch 6929 : 0.025352513417601585\n",
      "Training loss for batch 6930 : 0.07777810841798782\n",
      "Training loss for batch 6931 : 0.31706446409225464\n",
      "Training loss for batch 6932 : 0.09460924565792084\n",
      "Training loss for batch 6933 : 0.009669572114944458\n",
      "Training loss for batch 6934 : 0.07893774658441544\n",
      "Training loss for batch 6935 : 0.14148403704166412\n",
      "Training loss for batch 6936 : 0.15647602081298828\n",
      "Training loss for batch 6937 : 0.21523317694664001\n",
      "Training loss for batch 6938 : 0.10780689120292664\n",
      "Training loss for batch 6939 : 0.18702326714992523\n",
      "Training loss for batch 6940 : 0.029129398986697197\n",
      "Training loss for batch 6941 : 0.1310083121061325\n",
      "Training loss for batch 6942 : 0.26403671503067017\n",
      "Training loss for batch 6943 : 0.0452696867287159\n",
      "Training loss for batch 6944 : 0.08325864374637604\n",
      "Training loss for batch 6945 : 0.007452691439539194\n",
      "Training loss for batch 6946 : 0.025841237977147102\n",
      "Training loss for batch 6947 : 0.20670944452285767\n",
      "Training loss for batch 6948 : 0.08819544315338135\n",
      "Training loss for batch 6949 : 0.032249223440885544\n",
      "Training loss for batch 6950 : 0.06332822144031525\n",
      "Training loss for batch 6951 : 0.20458801090717316\n",
      "Training loss for batch 6952 : 0.5126957297325134\n",
      "Training loss for batch 6953 : 0.19795635342597961\n",
      "Training loss for batch 6954 : 0.11278661340475082\n",
      "Training loss for batch 6955 : -0.001959019573405385\n",
      "Training loss for batch 6956 : 0.2407544106245041\n",
      "Training loss for batch 6957 : 0.0\n",
      "Training loss for batch 6958 : 0.03760388121008873\n",
      "Training loss for batch 6959 : 0.1866140067577362\n",
      "Training loss for batch 6960 : 0.198154017329216\n",
      "Training loss for batch 6961 : 0.1522216945886612\n",
      "Training loss for batch 6962 : 0.18481488525867462\n",
      "Training loss for batch 6963 : 0.1765751987695694\n",
      "Training loss for batch 6964 : 0.020060818642377853\n",
      "Training loss for batch 6965 : 0.36819344758987427\n",
      "Training loss for batch 6966 : 0.018555447459220886\n",
      "Training loss for batch 6967 : 0.2843307852745056\n",
      "Training loss for batch 6968 : 0.11379767954349518\n",
      "Training loss for batch 6969 : 0.018109256401658058\n",
      "Training loss for batch 6970 : 0.15623271465301514\n",
      "Training loss for batch 6971 : 0.07760556787252426\n",
      "Training loss for batch 6972 : 0.05842132121324539\n",
      "Training loss for batch 6973 : 0.15239860117435455\n",
      "Training loss for batch 6974 : 0.05386614054441452\n",
      "Training loss for batch 6975 : 1.0877565145492554\n",
      "Training loss for batch 6976 : 0.19635552167892456\n",
      "Training loss for batch 6977 : 0.0518966019153595\n",
      "Training loss for batch 6978 : 0.1728900969028473\n",
      "Training loss for batch 6979 : 0.1379149854183197\n",
      "Training loss for batch 6980 : 0.40199196338653564\n",
      "Training loss for batch 6981 : 0.22837725281715393\n",
      "Training loss for batch 6982 : 0.2823461592197418\n",
      "Training loss for batch 6983 : 0.18534377217292786\n",
      "Training loss for batch 6984 : 0.3238050639629364\n",
      "Training loss for batch 6985 : 0.24088047444820404\n",
      "Training loss for batch 6986 : 0.3925873637199402\n",
      "Training loss for batch 6987 : 0.31521618366241455\n",
      "Training loss for batch 6988 : 0.01688278093934059\n",
      "Training loss for batch 6989 : 0.10100656002759933\n",
      "Training loss for batch 6990 : 0.08209852874279022\n",
      "Training loss for batch 6991 : 0.03244597092270851\n",
      "Training loss for batch 6992 : 0.387820839881897\n",
      "Training loss for batch 6993 : 0.05357716977596283\n",
      "Training loss for batch 6994 : 0.04986463487148285\n",
      "Training loss for batch 6995 : 0.2650843560695648\n",
      "Training loss for batch 6996 : 0.4061018228530884\n",
      "Training loss for batch 6997 : 0.10037356615066528\n",
      "Training loss for batch 6998 : 0.10624925792217255\n",
      "Training loss for batch 6999 : 0.1287851780653\n",
      "Training loss for batch 7000 : 0.022856131196022034\n",
      "Training loss for batch 7001 : 0.3658306896686554\n",
      "Training loss for batch 7002 : 0.15938207507133484\n",
      "Training loss for batch 7003 : 0.31533390283584595\n",
      "Training loss for batch 7004 : 0.14310745894908905\n",
      "Training loss for batch 7005 : 0.22081175446510315\n",
      "Training loss for batch 7006 : 0.07149156928062439\n",
      "Training loss for batch 7007 : 0.021064216271042824\n",
      "Training loss for batch 7008 : 0.06933644413948059\n",
      "Training loss for batch 7009 : 0.0\n",
      "Training loss for batch 7010 : 0.027201861143112183\n",
      "Training loss for batch 7011 : 0.15756629407405853\n",
      "Training loss for batch 7012 : 0.03184370696544647\n",
      "Training loss for batch 7013 : 0.5179904103279114\n",
      "Training loss for batch 7014 : 0.2323770970106125\n",
      "Training loss for batch 7015 : 0.013165161944925785\n",
      "Training loss for batch 7016 : 0.1892411708831787\n",
      "Training loss for batch 7017 : 0.3887462615966797\n",
      "Training loss for batch 7018 : 0.09726656973361969\n",
      "Training loss for batch 7019 : 0.26062682271003723\n",
      "Training loss for batch 7020 : 0.28389105200767517\n",
      "Training loss for batch 7021 : 0.2917936444282532\n",
      "Training loss for batch 7022 : 0.2654125392436981\n",
      "Training loss for batch 7023 : 0.2500634491443634\n",
      "Training loss for batch 7024 : 0.15040519833564758\n",
      "Training loss for batch 7025 : 0.3985394239425659\n",
      "Training loss for batch 7026 : 0.08864625543355942\n",
      "Training loss for batch 7027 : 0.24779774248600006\n",
      "Training loss for batch 7028 : 0.29796600341796875\n",
      "Training loss for batch 7029 : 0.21199539303779602\n",
      "Training loss for batch 7030 : 0.12055255472660065\n",
      "Training loss for batch 7031 : 0.2896725535392761\n",
      "Training loss for batch 7032 : 0.054892078042030334\n",
      "Training loss for batch 7033 : 0.06807336956262589\n",
      "Training loss for batch 7034 : 0.24166032671928406\n",
      "Training loss for batch 7035 : 0.1378781795501709\n",
      "Training loss for batch 7036 : 0.40528595447540283\n",
      "Training loss for batch 7037 : 0.11515294760465622\n",
      "Training loss for batch 7038 : 0.42915740609169006\n",
      "Training loss for batch 7039 : 0.009270813316106796\n",
      "Training loss for batch 7040 : 0.03712574765086174\n",
      "Training loss for batch 7041 : 0.267829030752182\n",
      "Training loss for batch 7042 : 0.018135813996195793\n",
      "Training loss for batch 7043 : 0.27190324664115906\n",
      "Training loss for batch 7044 : 0.21003831923007965\n",
      "Training loss for batch 7045 : 0.3167986273765564\n",
      "Training loss for batch 7046 : 0.06918641924858093\n",
      "Training loss for batch 7047 : 0.004562835209071636\n",
      "Training loss for batch 7048 : 0.0163123719394207\n",
      "Training loss for batch 7049 : 0.27236077189445496\n",
      "Training loss for batch 7050 : 0.042599305510520935\n",
      "Training loss for batch 7051 : 0.11775321513414383\n",
      "Training loss for batch 7052 : 0.1384955644607544\n",
      "Training loss for batch 7053 : 0.22331181168556213\n",
      "Training loss for batch 7054 : 0.05204059183597565\n",
      "Training loss for batch 7055 : 0.1539217233657837\n",
      "Training loss for batch 7056 : 0.19569000601768494\n",
      "Training loss for batch 7057 : 0.2614453136920929\n",
      "Training loss for batch 7058 : 0.15009477734565735\n",
      "Training loss for batch 7059 : 0.09251514077186584\n",
      "Training loss for batch 7060 : 0.18226774036884308\n",
      "Training loss for batch 7061 : 0.039171002805233\n",
      "Training loss for batch 7062 : 0.22018785774707794\n",
      "Training loss for batch 7063 : 0.11025537550449371\n",
      "Training loss for batch 7064 : 0.03180376812815666\n",
      "Training loss for batch 7065 : 0.22934061288833618\n",
      "Training loss for batch 7066 : 0.45843368768692017\n",
      "Training loss for batch 7067 : 0.44700711965560913\n",
      "Training loss for batch 7068 : 0.22927111387252808\n",
      "Training loss for batch 7069 : 0.08216641843318939\n",
      "Training loss for batch 7070 : 0.020914480090141296\n",
      "Training loss for batch 7071 : 0.10494414716959\n",
      "Training loss for batch 7072 : 0.16971556842327118\n",
      "Training loss for batch 7073 : 0.05448048561811447\n",
      "Training loss for batch 7074 : 0.050470657646656036\n",
      "Training loss for batch 7075 : 0.20336848497390747\n",
      "Training loss for batch 7076 : 0.12852054834365845\n",
      "Training loss for batch 7077 : 0.13671182096004486\n",
      "Training loss for batch 7078 : 0.23526990413665771\n",
      "Training loss for batch 7079 : 0.057724419981241226\n",
      "Training loss for batch 7080 : 0.050010256469249725\n",
      "Training loss for batch 7081 : 0.2805195450782776\n",
      "Training loss for batch 7082 : 0.18359948694705963\n",
      "Training loss for batch 7083 : 0.0778423324227333\n",
      "Training loss for batch 7084 : 0.43586263060569763\n",
      "Training loss for batch 7085 : 0.01835235022008419\n",
      "Training loss for batch 7086 : 0.09036668390035629\n",
      "Training loss for batch 7087 : 0.16420133411884308\n",
      "Training loss for batch 7088 : 0.07397626340389252\n",
      "Training loss for batch 7089 : 0.2144850790500641\n",
      "Training loss for batch 7090 : 0.11238580197095871\n",
      "Training loss for batch 7091 : 0.11212049424648285\n",
      "Training loss for batch 7092 : 0.05670282244682312\n",
      "Training loss for batch 7093 : 0.02585558034479618\n",
      "Training loss for batch 7094 : 0.20385393500328064\n",
      "Training loss for batch 7095 : 0.24830955266952515\n",
      "Training loss for batch 7096 : 0.10899292677640915\n",
      "Training loss for batch 7097 : 0.09635146707296371\n",
      "Training loss for batch 7098 : 0.09361366927623749\n",
      "Training loss for batch 7099 : 0.07637931406497955\n",
      "Training loss for batch 7100 : 0.12863339483737946\n",
      "Training loss for batch 7101 : 0.3994843661785126\n",
      "Training loss for batch 7102 : 0.01930893585085869\n",
      "Training loss for batch 7103 : 0.23760035634040833\n",
      "Training loss for batch 7104 : 0.25654709339141846\n",
      "Training loss for batch 7105 : 0.0750623345375061\n",
      "Training loss for batch 7106 : 0.10120220482349396\n",
      "Training loss for batch 7107 : 0.2817115783691406\n",
      "Training loss for batch 7108 : -0.0022268134634941816\n",
      "Training loss for batch 7109 : 0.28101128339767456\n",
      "Training loss for batch 7110 : 0.35931065678596497\n",
      "Training loss for batch 7111 : 0.22499392926692963\n",
      "Training loss for batch 7112 : 0.0866272822022438\n",
      "Training loss for batch 7113 : 0.2211647629737854\n",
      "Training loss for batch 7114 : 0.0\n",
      "Training loss for batch 7115 : 0.08905629813671112\n",
      "Training loss for batch 7116 : 0.0\n",
      "Training loss for batch 7117 : 0.15906816720962524\n",
      "Training loss for batch 7118 : 0.0\n",
      "Training loss for batch 7119 : 0.1262466013431549\n",
      "Training loss for batch 7120 : 0.25579172372817993\n",
      "Training loss for batch 7121 : 0.10070710629224777\n",
      "Training loss for batch 7122 : 0.18515056371688843\n",
      "Training loss for batch 7123 : 0.39152634143829346\n",
      "Training loss for batch 7124 : 0.03353079408407211\n",
      "Training loss for batch 7125 : 0.3688755929470062\n",
      "Training loss for batch 7126 : 0.021414030343294144\n",
      "Training loss for batch 7127 : 0.09028749167919159\n",
      "Training loss for batch 7128 : 0.1950131058692932\n",
      "Training loss for batch 7129 : 0.4640873372554779\n",
      "Training loss for batch 7130 : 0.1599138379096985\n",
      "Training loss for batch 7131 : 0.3010883033275604\n",
      "Training loss for batch 7132 : 0.2601684629917145\n",
      "Training loss for batch 7133 : 0.0035482547245919704\n",
      "Training loss for batch 7134 : 0.17360152304172516\n",
      "Training loss for batch 7135 : 0.18046219646930695\n",
      "Training loss for batch 7136 : 0.22255243360996246\n",
      "Training loss for batch 7137 : 0.2486572116613388\n",
      "Training loss for batch 7138 : 0.2632085382938385\n",
      "Training loss for batch 7139 : 0.11176683008670807\n",
      "Training loss for batch 7140 : 0.18798866868019104\n",
      "Training loss for batch 7141 : 0.05099930241703987\n",
      "Training loss for batch 7142 : 0.1741781085729599\n",
      "Training loss for batch 7143 : 0.3408032953739166\n",
      "Training loss for batch 7144 : 0.08094178885221481\n",
      "Training loss for batch 7145 : -0.0008326899260282516\n",
      "Training loss for batch 7146 : 0.1642991304397583\n",
      "Training loss for batch 7147 : 0.049604691565036774\n",
      "Training loss for batch 7148 : 0.20024895668029785\n",
      "Training loss for batch 7149 : 0.02407245524227619\n",
      "Training loss for batch 7150 : 0.16772156953811646\n",
      "Training loss for batch 7151 : 0.0\n",
      "Training loss for batch 7152 : 0.009347191080451012\n",
      "Training loss for batch 7153 : 0.10261709988117218\n",
      "Training loss for batch 7154 : 0.00493419636040926\n",
      "Training loss for batch 7155 : 0.019187122583389282\n",
      "Training loss for batch 7156 : 0.03982573747634888\n",
      "Training loss for batch 7157 : 0.12336540222167969\n",
      "Training loss for batch 7158 : 0.05548137053847313\n",
      "Training loss for batch 7159 : 0.30972060561180115\n",
      "Training loss for batch 7160 : 0.14251521229743958\n",
      "Training loss for batch 7161 : 0.2781408131122589\n",
      "Training loss for batch 7162 : 0.02489127591252327\n",
      "Training loss for batch 7163 : 0.03864827752113342\n",
      "Training loss for batch 7164 : 0.21845729649066925\n",
      "Training loss for batch 7165 : 0.0653153508901596\n",
      "Training loss for batch 7166 : 0.23092488944530487\n",
      "Training loss for batch 7167 : 0.5952214598655701\n",
      "Training loss for batch 7168 : 0.5764930248260498\n",
      "Training loss for batch 7169 : 0.24538636207580566\n",
      "Training loss for batch 7170 : 0.059578828513622284\n",
      "Training loss for batch 7171 : 0.1763189733028412\n",
      "Training loss for batch 7172 : 0.45859986543655396\n",
      "Training loss for batch 7173 : 0.21107730269432068\n",
      "Training loss for batch 7174 : 0.09732858091592789\n",
      "Training loss for batch 7175 : 0.2663697898387909\n",
      "Training loss for batch 7176 : 0.10647428780794144\n",
      "Training loss for batch 7177 : 0.32076495885849\n",
      "Training loss for batch 7178 : 0.28932055830955505\n",
      "Training loss for batch 7179 : 0.05451495200395584\n",
      "Training loss for batch 7180 : 0.29792696237564087\n",
      "Training loss for batch 7181 : 0.14837977290153503\n",
      "Training loss for batch 7182 : 0.17292867600917816\n",
      "Training loss for batch 7183 : 0.3011695146560669\n",
      "Training loss for batch 7184 : 0.3172502815723419\n",
      "Training loss for batch 7185 : 0.0\n",
      "Training loss for batch 7186 : 0.1568305939435959\n",
      "Training loss for batch 7187 : 0.21301288902759552\n",
      "Training loss for batch 7188 : 0.29055139422416687\n",
      "Training loss for batch 7189 : 0.11412958055734634\n",
      "Training loss for batch 7190 : 0.16629421710968018\n",
      "Training loss for batch 7191 : 0.23327967524528503\n",
      "Training loss for batch 7192 : 0.08565493673086166\n",
      "Training loss for batch 7193 : 0.29289576411247253\n",
      "Training loss for batch 7194 : 0.14220397174358368\n",
      "Training loss for batch 7195 : 0.24752694368362427\n",
      "Training loss for batch 7196 : 0.1489282101392746\n",
      "Training loss for batch 7197 : 0.07201442122459412\n",
      "Training loss for batch 7198 : 0.5116312503814697\n",
      "Training loss for batch 7199 : 0.2907927930355072\n",
      "Training loss for batch 7200 : 0.15619774162769318\n",
      "Training loss for batch 7201 : 0.0719209611415863\n",
      "Training loss for batch 7202 : 0.020354054868221283\n",
      "Training loss for batch 7203 : 0.027175497263669968\n",
      "Training loss for batch 7204 : 0.2621169090270996\n",
      "Training loss for batch 7205 : 0.014117700047791004\n",
      "Training loss for batch 7206 : 0.05071103572845459\n",
      "Training loss for batch 7207 : 0.006800240371376276\n",
      "Training loss for batch 7208 : 0.21665853261947632\n",
      "Training loss for batch 7209 : 0.35615020990371704\n",
      "Training loss for batch 7210 : 0.10892608761787415\n",
      "Training loss for batch 7211 : 0.027517113834619522\n",
      "Training loss for batch 7212 : 0.17574714124202728\n",
      "Training loss for batch 7213 : 0.17470954358577728\n",
      "Training loss for batch 7214 : 0.06768752634525299\n",
      "Training loss for batch 7215 : 0.0\n",
      "Training loss for batch 7216 : 0.24043971300125122\n",
      "Training loss for batch 7217 : 0.19439829885959625\n",
      "Training loss for batch 7218 : 0.44509968161582947\n",
      "Training loss for batch 7219 : 0.3416951596736908\n",
      "Training loss for batch 7220 : 0.015363922342658043\n",
      "Training loss for batch 7221 : 0.15168428421020508\n",
      "Training loss for batch 7222 : 0.0644771158695221\n",
      "Training loss for batch 7223 : 0.14941181242465973\n",
      "Training loss for batch 7224 : 0.03760837763547897\n",
      "Training loss for batch 7225 : 0.28971928358078003\n",
      "Training loss for batch 7226 : 0.21074716746807098\n",
      "Training loss for batch 7227 : 0.003727694507688284\n",
      "Training loss for batch 7228 : 0.01834593340754509\n",
      "Training loss for batch 7229 : 0.1008462980389595\n",
      "Training loss for batch 7230 : 0.07096720486879349\n",
      "Training loss for batch 7231 : 0.061561744660139084\n",
      "Training loss for batch 7232 : 0.21671998500823975\n",
      "Training loss for batch 7233 : 0.07293584197759628\n",
      "Training loss for batch 7234 : 0.07734069228172302\n",
      "Training loss for batch 7235 : 0.07202956080436707\n",
      "Training loss for batch 7236 : 0.0960565134882927\n",
      "Training loss for batch 7237 : 0.025783173739910126\n",
      "Training loss for batch 7238 : 0.29897934198379517\n",
      "Training loss for batch 7239 : 0.10154812037944794\n",
      "Training loss for batch 7240 : 0.04553724825382233\n",
      "Training loss for batch 7241 : 0.3081032931804657\n",
      "Training loss for batch 7242 : 0.03512778878211975\n",
      "Training loss for batch 7243 : 0.30143117904663086\n",
      "Training loss for batch 7244 : 0.11530308425426483\n",
      "Training loss for batch 7245 : 0.16566422581672668\n",
      "Training loss for batch 7246 : 0.39681240916252136\n",
      "Training loss for batch 7247 : 0.10580433905124664\n",
      "Training loss for batch 7248 : 0.23502595722675323\n",
      "Training loss for batch 7249 : 0.3743106424808502\n",
      "Training loss for batch 7250 : 0.06300622969865799\n",
      "Training loss for batch 7251 : 0.5672866106033325\n",
      "Training loss for batch 7252 : 0.12293858826160431\n",
      "Training loss for batch 7253 : 0.3350873589515686\n",
      "Training loss for batch 7254 : 0.5163693428039551\n",
      "Training loss for batch 7255 : 0.1538841724395752\n",
      "Training loss for batch 7256 : 0.11166962236166\n",
      "Training loss for batch 7257 : 0.2484096884727478\n",
      "Training loss for batch 7258 : 0.32263466715812683\n",
      "Training loss for batch 7259 : 0.030552828684449196\n",
      "Training loss for batch 7260 : 0.040252719074487686\n",
      "Training loss for batch 7261 : 0.13376060128211975\n",
      "Training loss for batch 7262 : 0.3368862271308899\n",
      "Training loss for batch 7263 : 0.46501150727272034\n",
      "Training loss for batch 7264 : 0.0897369459271431\n",
      "Training loss for batch 7265 : 0.07903926819562912\n",
      "Training loss for batch 7266 : 0.17335659265518188\n",
      "Training loss for batch 7267 : 0.01594180054962635\n",
      "Training loss for batch 7268 : 0.3707123100757599\n",
      "Training loss for batch 7269 : 0.30394792556762695\n",
      "Training loss for batch 7270 : 0.18317636847496033\n",
      "Training loss for batch 7271 : 0.06382301449775696\n",
      "Training loss for batch 7272 : 0.09153423458337784\n",
      "Training loss for batch 7273 : 0.04843813180923462\n",
      "Training loss for batch 7274 : 0.03341331332921982\n",
      "Training loss for batch 7275 : 0.07614759355783463\n",
      "Training loss for batch 7276 : 0.3609403669834137\n",
      "Training loss for batch 7277 : 0.1618199646472931\n",
      "Training loss for batch 7278 : 0.039328861981630325\n",
      "Training loss for batch 7279 : 0.06307006627321243\n",
      "Training loss for batch 7280 : 0.11331360042095184\n",
      "Training loss for batch 7281 : 0.6727957129478455\n",
      "Training loss for batch 7282 : 0.14952875673770905\n",
      "Training loss for batch 7283 : 0.2564203441143036\n",
      "Training loss for batch 7284 : 0.08055756986141205\n",
      "Training loss for batch 7285 : 0.14701682329177856\n",
      "Training loss for batch 7286 : 0.15846355259418488\n",
      "Training loss for batch 7287 : 0.03923378512263298\n",
      "Training loss for batch 7288 : 0.0349549725651741\n",
      "Training loss for batch 7289 : 0.018277177587151527\n",
      "Training loss for batch 7290 : 0.02402336150407791\n",
      "Training loss for batch 7291 : 0.07040087133646011\n",
      "Training loss for batch 7292 : 0.09307850897312164\n",
      "Training loss for batch 7293 : 0.31412145495414734\n",
      "Training loss for batch 7294 : 0.08207236975431442\n",
      "Training loss for batch 7295 : 0.2632963955402374\n",
      "Training loss for batch 7296 : 0.059138230979442596\n",
      "Training loss for batch 7297 : 0.06384725123643875\n",
      "Training loss for batch 7298 : 0.02257581800222397\n",
      "Training loss for batch 7299 : 0.16327930986881256\n",
      "Training loss for batch 7300 : 0.060199037194252014\n",
      "Training loss for batch 7301 : 0.3195517659187317\n",
      "Training loss for batch 7302 : 0.19157372415065765\n",
      "Training loss for batch 7303 : 0.14624325931072235\n",
      "Training loss for batch 7304 : 0.29874616861343384\n",
      "Training loss for batch 7305 : 0.051098525524139404\n",
      "Training loss for batch 7306 : 0.014909499324858189\n",
      "Training loss for batch 7307 : 0.058355607092380524\n",
      "Training loss for batch 7308 : 0.00843530148267746\n",
      "Training loss for batch 7309 : 0.14678794145584106\n",
      "Training loss for batch 7310 : 0.11531445384025574\n",
      "Training loss for batch 7311 : 0.06591598689556122\n",
      "Training loss for batch 7312 : 0.12470052391290665\n",
      "Training loss for batch 7313 : 0.045356765389442444\n",
      "Training loss for batch 7314 : 0.07677070796489716\n",
      "Training loss for batch 7315 : 0.04394809901714325\n",
      "Training loss for batch 7316 : 0.1951771080493927\n",
      "Training loss for batch 7317 : 0.11270097643136978\n",
      "Training loss for batch 7318 : 0.013735843822360039\n",
      "Training loss for batch 7319 : 0.1913411170244217\n",
      "Training loss for batch 7320 : 0.011601577512919903\n",
      "Training loss for batch 7321 : 0.015914959833025932\n",
      "Training loss for batch 7322 : 0.17388585209846497\n",
      "Training loss for batch 7323 : 0.06758825480937958\n",
      "Training loss for batch 7324 : 0.14089596271514893\n",
      "Training loss for batch 7325 : 0.1393028199672699\n",
      "Training loss for batch 7326 : 0.12659068405628204\n",
      "Training loss for batch 7327 : 0.21772658824920654\n",
      "Training loss for batch 7328 : 0.0023183540906757116\n",
      "Training loss for batch 7329 : 0.06803597509860992\n",
      "Training loss for batch 7330 : 0.47972241044044495\n",
      "Training loss for batch 7331 : 0.04353123903274536\n",
      "Training loss for batch 7332 : 0.041611526161432266\n",
      "Training loss for batch 7333 : 0.1376946121454239\n",
      "Training loss for batch 7334 : 0.2633579671382904\n",
      "Training loss for batch 7335 : 0.0555354505777359\n",
      "Training loss for batch 7336 : 0.8619572520256042\n",
      "Training loss for batch 7337 : 0.04441701993346214\n",
      "Training loss for batch 7338 : 0.3884273171424866\n",
      "Training loss for batch 7339 : 0.08659519255161285\n",
      "Training loss for batch 7340 : 0.09802905470132828\n",
      "Training loss for batch 7341 : 0.3666180372238159\n",
      "Training loss for batch 7342 : 0.0299602672457695\n",
      "Training loss for batch 7343 : 0.16995927691459656\n",
      "Training loss for batch 7344 : 0.3574521541595459\n",
      "Training loss for batch 7345 : 0.6825206279754639\n",
      "Training loss for batch 7346 : 0.35789889097213745\n",
      "Training loss for batch 7347 : 0.03094436600804329\n",
      "Training loss for batch 7348 : 0.16419775784015656\n",
      "Training loss for batch 7349 : 0.036189865320920944\n",
      "Training loss for batch 7350 : 0.10863877087831497\n",
      "Training loss for batch 7351 : 0.015200952999293804\n",
      "Training loss for batch 7352 : 0.14274632930755615\n",
      "Training loss for batch 7353 : 0.2148684710264206\n",
      "Training loss for batch 7354 : 0.13667462766170502\n",
      "Training loss for batch 7355 : 0.225587397813797\n",
      "Training loss for batch 7356 : 0.17201419174671173\n",
      "Training loss for batch 7357 : 0.18308520317077637\n",
      "Training loss for batch 7358 : 0.15740948915481567\n",
      "Training loss for batch 7359 : 0.03162696957588196\n",
      "Training loss for batch 7360 : 0.05890662223100662\n",
      "Training loss for batch 7361 : 0.06386540830135345\n",
      "Training loss for batch 7362 : 0.043442532420158386\n",
      "Training loss for batch 7363 : 0.09114542603492737\n",
      "Training loss for batch 7364 : 0.31294918060302734\n",
      "Training loss for batch 7365 : 0.019488777965307236\n",
      "Training loss for batch 7366 : 0.02552584558725357\n",
      "Training loss for batch 7367 : 0.22643712162971497\n",
      "Training loss for batch 7368 : 0.19559957087039948\n",
      "Training loss for batch 7369 : 0.32784008979797363\n",
      "Training loss for batch 7370 : 0.20656800270080566\n",
      "Training loss for batch 7371 : 0.32393282651901245\n",
      "Training loss for batch 7372 : 0.2116476595401764\n",
      "Training loss for batch 7373 : 0.10095857083797455\n",
      "Training loss for batch 7374 : 0.05456378683447838\n",
      "Training loss for batch 7375 : 0.13872139155864716\n",
      "Training loss for batch 7376 : 0.0453859381377697\n",
      "Training loss for batch 7377 : 0.008486409671604633\n",
      "Training loss for batch 7378 : 0.14987076818943024\n",
      "Training loss for batch 7379 : 0.07985025644302368\n",
      "Training loss for batch 7380 : 0.26540496945381165\n",
      "Training loss for batch 7381 : 0.001456062076613307\n",
      "Training loss for batch 7382 : 0.09661802649497986\n",
      "Training loss for batch 7383 : 0.2990857660770416\n",
      "Training loss for batch 7384 : 0.044913627207279205\n",
      "Training loss for batch 7385 : 0.061989203095436096\n",
      "Training loss for batch 7386 : 0.13560250401496887\n",
      "Training loss for batch 7387 : 0.2725902497768402\n",
      "Training loss for batch 7388 : 0.05496640130877495\n",
      "Training loss for batch 7389 : 0.06426326185464859\n",
      "Training loss for batch 7390 : 0.15208329260349274\n",
      "Training loss for batch 7391 : 0.25352242588996887\n",
      "Training loss for batch 7392 : 0.05647251754999161\n",
      "Training loss for batch 7393 : 0.2825900614261627\n",
      "Training loss for batch 7394 : 0.04592447727918625\n",
      "Training loss for batch 7395 : 0.09062481671571732\n",
      "Training loss for batch 7396 : 0.3990352153778076\n",
      "Training loss for batch 7397 : 0.2586788833141327\n",
      "Training loss for batch 7398 : 0.40229079127311707\n",
      "Training loss for batch 7399 : 0.004532923456281424\n",
      "Training loss for batch 7400 : 0.19111289083957672\n",
      "Training loss for batch 7401 : 0.07765083014965057\n",
      "Training loss for batch 7402 : 0.3479174077510834\n",
      "Training loss for batch 7403 : 0.36087721586227417\n",
      "Training loss for batch 7404 : 0.3023979663848877\n",
      "Training loss for batch 7405 : 0.04122211039066315\n",
      "Training loss for batch 7406 : 0.20661567151546478\n",
      "Training loss for batch 7407 : 0.17112740874290466\n",
      "Training loss for batch 7408 : 0.2100171446800232\n",
      "Training loss for batch 7409 : 0.16960416734218597\n",
      "Training loss for batch 7410 : 0.012760007753968239\n",
      "Training loss for batch 7411 : 0.2979920506477356\n",
      "Training loss for batch 7412 : 0.10420872271060944\n",
      "Training loss for batch 7413 : 0.3290029764175415\n",
      "Training loss for batch 7414 : 0.1564679741859436\n",
      "Training loss for batch 7415 : 0.20493437349796295\n",
      "Training loss for batch 7416 : 0.10675584524869919\n",
      "Training loss for batch 7417 : 0.15441739559173584\n",
      "Training loss for batch 7418 : 0.14873531460762024\n",
      "Training loss for batch 7419 : 0.07677137106657028\n",
      "Training loss for batch 7420 : 0.3560374677181244\n",
      "Training loss for batch 7421 : 0.07459475845098495\n",
      "Training loss for batch 7422 : 0.0\n",
      "Training loss for batch 7423 : 0.0573844388127327\n",
      "Training loss for batch 7424 : 0.027472417801618576\n",
      "Training loss for batch 7425 : 0.06635909527540207\n",
      "Training loss for batch 7426 : 0.41144269704818726\n",
      "Training loss for batch 7427 : 0.23869013786315918\n",
      "Training loss for batch 7428 : 0.11094342172145844\n",
      "Training loss for batch 7429 : 0.1500326544046402\n",
      "Training loss for batch 7430 : 0.17822858691215515\n",
      "Training loss for batch 7431 : 0.0019092162838205695\n",
      "Training loss for batch 7432 : 0.017370324581861496\n",
      "Training loss for batch 7433 : 0.33769863843917847\n",
      "Training loss for batch 7434 : 0.18865501880645752\n",
      "Training loss for batch 7435 : 0.17972931265830994\n",
      "Training loss for batch 7436 : 0.0\n",
      "Training loss for batch 7437 : 0.18823392689228058\n",
      "Training loss for batch 7438 : 0.10644391924142838\n",
      "Training loss for batch 7439 : 0.193175807595253\n",
      "Training loss for batch 7440 : 0.11913517862558365\n",
      "Training loss for batch 7441 : 0.03815615177154541\n",
      "Training loss for batch 7442 : 0.02400113083422184\n",
      "Training loss for batch 7443 : 0.41599082946777344\n",
      "Training loss for batch 7444 : 0.4052836298942566\n",
      "Training loss for batch 7445 : 0.35985833406448364\n",
      "Training loss for batch 7446 : 0.09413789957761765\n",
      "Training loss for batch 7447 : 0.22817081212997437\n",
      "Training loss for batch 7448 : 0.09274183958768845\n",
      "Training loss for batch 7449 : 0.0753970667719841\n",
      "Training loss for batch 7450 : 0.1016777902841568\n",
      "Training loss for batch 7451 : 0.02071147970855236\n",
      "Training loss for batch 7452 : 0.06982702761888504\n",
      "Training loss for batch 7453 : 0.2135038822889328\n",
      "Training loss for batch 7454 : 0.229467511177063\n",
      "Training loss for batch 7455 : 0.12514562904834747\n",
      "Training loss for batch 7456 : 0.03518840670585632\n",
      "Training loss for batch 7457 : 0.10839822143316269\n",
      "Training loss for batch 7458 : 0.01563025638461113\n",
      "Training loss for batch 7459 : -0.0006876307306811213\n",
      "Training loss for batch 7460 : 0.2208680957555771\n",
      "Training loss for batch 7461 : 0.024399280548095703\n",
      "Training loss for batch 7462 : 0.17321781814098358\n",
      "Training loss for batch 7463 : 0.2866613268852234\n",
      "Training loss for batch 7464 : 0.021853182464838028\n",
      "Training loss for batch 7465 : 0.11721939593553543\n",
      "Training loss for batch 7466 : 0.002895999001339078\n",
      "Training loss for batch 7467 : 0.19418634474277496\n",
      "Training loss for batch 7468 : 0.17080430686473846\n",
      "Training loss for batch 7469 : 0.09021025896072388\n",
      "Training loss for batch 7470 : 0.24833175539970398\n",
      "Training loss for batch 7471 : 0.13905462622642517\n",
      "Training loss for batch 7472 : 0.13090768456459045\n",
      "Training loss for batch 7473 : 0.22811371088027954\n",
      "Training loss for batch 7474 : 0.002899355720728636\n",
      "Training loss for batch 7475 : 0.4947172701358795\n",
      "Training loss for batch 7476 : 0.0031361482106149197\n",
      "Training loss for batch 7477 : 0.15797457098960876\n",
      "Training loss for batch 7478 : 0.09613839536905289\n",
      "Training loss for batch 7479 : 0.3320448696613312\n",
      "Training loss for batch 7480 : 0.17163436114788055\n",
      "Training loss for batch 7481 : 0.09805519878864288\n",
      "Training loss for batch 7482 : 0.08955734223127365\n",
      "Training loss for batch 7483 : 0.18724897503852844\n",
      "Training loss for batch 7484 : 0.3027176856994629\n",
      "Training loss for batch 7485 : 0.09522093087434769\n",
      "Training loss for batch 7486 : 0.367590993642807\n",
      "Training loss for batch 7487 : 0.23750625550746918\n",
      "Training loss for batch 7488 : 0.36671289801597595\n",
      "Training loss for batch 7489 : 0.36458566784858704\n",
      "Training loss for batch 7490 : -1.4612029190175235e-05\n",
      "Training loss for batch 7491 : 0.10645981878042221\n",
      "Training loss for batch 7492 : 0.18942628800868988\n",
      "Training loss for batch 7493 : 0.03997955098748207\n",
      "Training loss for batch 7494 : 0.28509044647216797\n",
      "Training loss for batch 7495 : 0.23544761538505554\n",
      "Training loss for batch 7496 : 0.05862082168459892\n",
      "Training loss for batch 7497 : 0.26386314630508423\n",
      "Training loss for batch 7498 : 0.2313406616449356\n",
      "Training loss for batch 7499 : 0.1316191554069519\n",
      "Training loss for batch 7500 : 0.18224601447582245\n",
      "Training loss for batch 7501 : 0.2025119960308075\n",
      "Training loss for batch 7502 : 0.10654684156179428\n",
      "Training loss for batch 7503 : 0.1387704610824585\n",
      "Training loss for batch 7504 : 0.22623245418071747\n",
      "Training loss for batch 7505 : 0.18875299394130707\n",
      "Training loss for batch 7506 : 0.12971122562885284\n",
      "Training loss for batch 7507 : 0.09933027625083923\n",
      "Training loss for batch 7508 : 0.015796516090631485\n",
      "Training loss for batch 7509 : 0.15393739938735962\n",
      "Training loss for batch 7510 : 0.07310890406370163\n",
      "Training loss for batch 7511 : 0.13152992725372314\n",
      "Training loss for batch 7512 : 0.032294705510139465\n",
      "Training loss for batch 7513 : 0.10963625460863113\n",
      "Training loss for batch 7514 : 0.09707755595445633\n",
      "Training loss for batch 7515 : 0.42375287413597107\n",
      "Training loss for batch 7516 : 0.09651459008455276\n",
      "Training loss for batch 7517 : 0.10580512881278992\n",
      "Training loss for batch 7518 : 0.08096018433570862\n",
      "Training loss for batch 7519 : 0.27069950103759766\n",
      "Training loss for batch 7520 : 0.1718074083328247\n",
      "Training loss for batch 7521 : 0.21652022004127502\n",
      "Training loss for batch 7522 : 0.37964096665382385\n",
      "Training loss for batch 7523 : 0.2322477400302887\n",
      "Training loss for batch 7524 : 0.12682637572288513\n",
      "Training loss for batch 7525 : 0.7086801528930664\n",
      "Training loss for batch 7526 : 0.025508912280201912\n",
      "Training loss for batch 7527 : 0.3654158413410187\n",
      "Training loss for batch 7528 : 0.19790661334991455\n",
      "Training loss for batch 7529 : 0.17691300809383392\n",
      "Training loss for batch 7530 : 0.05703958123922348\n",
      "Training loss for batch 7531 : 0.2213953584432602\n",
      "Training loss for batch 7532 : 0.2626475989818573\n",
      "Training loss for batch 7533 : 0.11920033395290375\n",
      "Training loss for batch 7534 : 0.23410570621490479\n",
      "Training loss for batch 7535 : 0.0849539265036583\n",
      "Training loss for batch 7536 : 0.6341602206230164\n",
      "Training loss for batch 7537 : 0.22442315518856049\n",
      "Training loss for batch 7538 : 0.2651800215244293\n",
      "Training loss for batch 7539 : 0.026613958179950714\n",
      "Training loss for batch 7540 : 0.389949768781662\n",
      "Training loss for batch 7541 : 0.06766495853662491\n",
      "Training loss for batch 7542 : 0.18686969578266144\n",
      "Training loss for batch 7543 : 0.07834207266569138\n",
      "Training loss for batch 7544 : 0.06952042132616043\n",
      "Training loss for batch 7545 : 0.0627652183175087\n",
      "Training loss for batch 7546 : 0.3241664469242096\n",
      "Training loss for batch 7547 : 0.22731059789657593\n",
      "Training loss for batch 7548 : 0.24515599012374878\n",
      "Training loss for batch 7549 : 0.16932113468647003\n",
      "Training loss for batch 7550 : 0.05434129387140274\n",
      "Training loss for batch 7551 : 0.330241322517395\n",
      "Training loss for batch 7552 : 0.2598237693309784\n",
      "Training loss for batch 7553 : 0.06813705712556839\n",
      "Training loss for batch 7554 : 0.38308316469192505\n",
      "Training loss for batch 7555 : 0.058925483375787735\n",
      "Training loss for batch 7556 : 0.22549109160900116\n",
      "Training loss for batch 7557 : 0.07161420583724976\n",
      "Training loss for batch 7558 : 0.10129693150520325\n",
      "Training loss for batch 7559 : 0.3493020236492157\n",
      "Training loss for batch 7560 : 0.12876808643341064\n",
      "Training loss for batch 7561 : 0.4702723026275635\n",
      "Training loss for batch 7562 : 0.13120687007904053\n",
      "Training loss for batch 7563 : 0.12870219349861145\n",
      "Training loss for batch 7564 : 0.07212308794260025\n",
      "Training loss for batch 7565 : 0.0\n",
      "Training loss for batch 7566 : 0.3704620599746704\n",
      "Training loss for batch 7567 : 0.19772055745124817\n",
      "Training loss for batch 7568 : 0.44469061493873596\n",
      "Training loss for batch 7569 : 0.21942822635173798\n",
      "Training loss for batch 7570 : 0.28999656438827515\n",
      "Training loss for batch 7571 : 0.08581817150115967\n",
      "Training loss for batch 7572 : 0.00840752013027668\n",
      "Training loss for batch 7573 : 0.04386288672685623\n",
      "Training loss for batch 7574 : 0.5636582970619202\n",
      "Training loss for batch 7575 : 0.22141416370868683\n",
      "Training loss for batch 7576 : 0.2153390794992447\n",
      "Training loss for batch 7577 : 0.09762121737003326\n",
      "Training loss for batch 7578 : 0.0365738719701767\n",
      "Training loss for batch 7579 : 0.14879284799098969\n",
      "Training loss for batch 7580 : 0.2296142876148224\n",
      "Training loss for batch 7581 : 0.015900887548923492\n",
      "Training loss for batch 7582 : 0.03598792105913162\n",
      "Training loss for batch 7583 : 0.03576383739709854\n",
      "Training loss for batch 7584 : 0.03885810077190399\n",
      "Training loss for batch 7585 : 0.3452463746070862\n",
      "Training loss for batch 7586 : 0.03594406321644783\n",
      "Training loss for batch 7587 : 0.23056882619857788\n",
      "Training loss for batch 7588 : 0.20038151741027832\n",
      "Training loss for batch 7589 : 0.13083839416503906\n",
      "Training loss for batch 7590 : 0.0\n",
      "Training loss for batch 7591 : 0.19278466701507568\n",
      "Training loss for batch 7592 : 0.2425754964351654\n",
      "Training loss for batch 7593 : 0.00015160441398620605\n",
      "Training loss for batch 7594 : 0.013451735489070415\n",
      "Training loss for batch 7595 : 0.14308294653892517\n",
      "Training loss for batch 7596 : 0.18729721009731293\n",
      "Training loss for batch 7597 : 0.2954029440879822\n",
      "Training loss for batch 7598 : 0.11064000427722931\n",
      "Training loss for batch 7599 : 0.04253251478075981\n",
      "Training loss for batch 7600 : 0.10569435358047485\n",
      "Training loss for batch 7601 : 0.055705077946186066\n",
      "Training loss for batch 7602 : 0.4287582039833069\n",
      "Training loss for batch 7603 : 0.0425381176173687\n",
      "Training loss for batch 7604 : 0.19729335606098175\n",
      "Training loss for batch 7605 : 0.22946877777576447\n",
      "Training loss for batch 7606 : 0.1765248030424118\n",
      "Training loss for batch 7607 : 0.01792491041123867\n",
      "Training loss for batch 7608 : 0.10610484331846237\n",
      "Training loss for batch 7609 : 0.03503327816724777\n",
      "Training loss for batch 7610 : 0.10461333394050598\n",
      "Training loss for batch 7611 : 0.5463702082633972\n",
      "Training loss for batch 7612 : 0.44883400201797485\n",
      "Training loss for batch 7613 : 0.21116064488887787\n",
      "Training loss for batch 7614 : 0.13423757255077362\n",
      "Training loss for batch 7615 : 0.2755510210990906\n",
      "Training loss for batch 7616 : 0.12666074931621552\n",
      "Training loss for batch 7617 : 0.14336910843849182\n",
      "Training loss for batch 7618 : 0.398032546043396\n",
      "Training loss for batch 7619 : 0.08336936682462692\n",
      "Training loss for batch 7620 : 0.21359103918075562\n",
      "Training loss for batch 7621 : 0.21548309922218323\n",
      "Training loss for batch 7622 : 0.1741708517074585\n",
      "Training loss for batch 7623 : 0.17462143301963806\n",
      "Training loss for batch 7624 : 0.04886258766055107\n",
      "Training loss for batch 7625 : 0.18970471620559692\n",
      "Training loss for batch 7626 : 0.5962216854095459\n",
      "Training loss for batch 7627 : 0.09644800424575806\n",
      "Training loss for batch 7628 : 0.04047103971242905\n",
      "Training loss for batch 7629 : 0.4889368712902069\n",
      "Training loss for batch 7630 : 0.18254996836185455\n",
      "Training loss for batch 7631 : 0.5347869992256165\n",
      "Training loss for batch 7632 : 0.3851202428340912\n",
      "Training loss for batch 7633 : 0.24116818606853485\n",
      "Training loss for batch 7634 : 0.06874838471412659\n",
      "Training loss for batch 7635 : 0.0941678062081337\n",
      "Training loss for batch 7636 : 0.1826578825712204\n",
      "Training loss for batch 7637 : 0.3912956416606903\n",
      "Training loss for batch 7638 : 0.11475193500518799\n",
      "Training loss for batch 7639 : 0.6420388221740723\n",
      "Training loss for batch 7640 : 0.0\n",
      "Training loss for batch 7641 : 0.08628284931182861\n",
      "Training loss for batch 7642 : 0.013894372619688511\n",
      "Training loss for batch 7643 : 0.06527851521968842\n",
      "Training loss for batch 7644 : 0.22713449597358704\n",
      "Training loss for batch 7645 : 0.08390969783067703\n",
      "Training loss for batch 7646 : 0.14700834453105927\n",
      "Training loss for batch 7647 : 0.052853334695100784\n",
      "Training loss for batch 7648 : 0.09469678997993469\n",
      "Training loss for batch 7649 : 0.12078496813774109\n",
      "Training loss for batch 7650 : 0.08792763948440552\n",
      "Training loss for batch 7651 : 0.21805615723133087\n",
      "Training loss for batch 7652 : 0.2590909004211426\n",
      "Training loss for batch 7653 : 0.11617989093065262\n",
      "Training loss for batch 7654 : 0.04855247586965561\n",
      "Training loss for batch 7655 : 0.14041365683078766\n",
      "Training loss for batch 7656 : 0.19606970250606537\n",
      "Training loss for batch 7657 : 0.12785765528678894\n",
      "Training loss for batch 7658 : 0.17467546463012695\n",
      "Training loss for batch 7659 : 0.18060791492462158\n",
      "Training loss for batch 7660 : 0.3144778609275818\n",
      "Training loss for batch 7661 : 0.1750541627407074\n",
      "Training loss for batch 7662 : 0.3859928548336029\n",
      "Training loss for batch 7663 : 0.05020744353532791\n",
      "Training loss for batch 7664 : 0.09963464736938477\n",
      "Training loss for batch 7665 : 0.14622652530670166\n",
      "Training loss for batch 7666 : 0.1475776582956314\n",
      "Training loss for batch 7667 : 0.17923712730407715\n",
      "Training loss for batch 7668 : 0.2656652331352234\n",
      "Training loss for batch 7669 : 0.12358483672142029\n",
      "Training loss for batch 7670 : 0.10146115720272064\n",
      "Training loss for batch 7671 : 0.10418778657913208\n",
      "Training loss for batch 7672 : 0.08798309415578842\n",
      "Training loss for batch 7673 : 0.13033078610897064\n",
      "Training loss for batch 7674 : 0.06548445671796799\n",
      "Training loss for batch 7675 : 0.23783281445503235\n",
      "Training loss for batch 7676 : 0.25594189763069153\n",
      "Training loss for batch 7677 : 0.23581650853157043\n",
      "Training loss for batch 7678 : 0.11966121196746826\n",
      "Training loss for batch 7679 : 0.08935831487178802\n",
      "Training loss for batch 7680 : 0.2599489688873291\n",
      "Training loss for batch 7681 : 0.13288506865501404\n",
      "Training loss for batch 7682 : 0.07095221430063248\n",
      "Training loss for batch 7683 : 0.4731390178203583\n",
      "Training loss for batch 7684 : 0.03336150571703911\n",
      "Training loss for batch 7685 : 0.5884420275688171\n",
      "Training loss for batch 7686 : 0.240960955619812\n",
      "Training loss for batch 7687 : 0.14528153836727142\n",
      "Training loss for batch 7688 : 0.08835912495851517\n",
      "Training loss for batch 7689 : 0.2522992193698883\n",
      "Training loss for batch 7690 : 0.08662860840559006\n",
      "Training loss for batch 7691 : 0.22217997908592224\n",
      "Training loss for batch 7692 : 0.13802915811538696\n",
      "Training loss for batch 7693 : 0.2718102037906647\n",
      "Training loss for batch 7694 : 0.05813650041818619\n",
      "Training loss for batch 7695 : 0.19300729036331177\n",
      "Training loss for batch 7696 : 0.3922847807407379\n",
      "Training loss for batch 7697 : 0.3748560845851898\n",
      "Training loss for batch 7698 : 0.1497812718153\n",
      "Training loss for batch 7699 : 0.16290372610092163\n",
      "Training loss for batch 7700 : 0.1363610327243805\n",
      "Training loss for batch 7701 : 0.28200823068618774\n",
      "Training loss for batch 7702 : 0.24107301235198975\n",
      "Training loss for batch 7703 : 0.07544064521789551\n",
      "Training loss for batch 7704 : 0.28928324580192566\n",
      "Training loss for batch 7705 : 0.16333827376365662\n",
      "Training loss for batch 7706 : 0.018214311450719833\n",
      "Training loss for batch 7707 : 0.06859724223613739\n",
      "Training loss for batch 7708 : 0.03398118540644646\n",
      "Training loss for batch 7709 : 0.2805863618850708\n",
      "Training loss for batch 7710 : 0.3267321288585663\n",
      "Training loss for batch 7711 : 0.3424769937992096\n",
      "Training loss for batch 7712 : 0.04791012406349182\n",
      "Training loss for batch 7713 : 0.17839375138282776\n",
      "Training loss for batch 7714 : 0.07800819724798203\n",
      "Training loss for batch 7715 : 0.026363277807831764\n",
      "Training loss for batch 7716 : 0.4539288580417633\n",
      "Training loss for batch 7717 : 0.011115292087197304\n",
      "Training loss for batch 7718 : 0.09165079891681671\n",
      "Training loss for batch 7719 : 0.22487278282642365\n",
      "Training loss for batch 7720 : 0.11605378985404968\n",
      "Training loss for batch 7721 : 0.3641405701637268\n",
      "Training loss for batch 7722 : 0.16187413036823273\n",
      "Training loss for batch 7723 : 0.28983309864997864\n",
      "Training loss for batch 7724 : 0.05405532568693161\n",
      "Training loss for batch 7725 : 0.06154103949666023\n",
      "Training loss for batch 7726 : 0.10409119725227356\n",
      "Training loss for batch 7727 : 0.1585702896118164\n",
      "Training loss for batch 7728 : -0.0028594103641808033\n",
      "Training loss for batch 7729 : 0.038288235664367676\n",
      "Training loss for batch 7730 : 0.007509177550673485\n",
      "Training loss for batch 7731 : 0.035678695887327194\n",
      "Training loss for batch 7732 : 0.1867011934518814\n",
      "Training loss for batch 7733 : 0.0403299406170845\n",
      "Training loss for batch 7734 : 0.4114803373813629\n",
      "Training loss for batch 7735 : 0.15032778680324554\n",
      "Training loss for batch 7736 : 0.3881291151046753\n",
      "Training loss for batch 7737 : 0.05598193407058716\n",
      "Training loss for batch 7738 : 0.1790803074836731\n",
      "Training loss for batch 7739 : 0.1504659801721573\n",
      "Training loss for batch 7740 : 0.17129868268966675\n",
      "Training loss for batch 7741 : 0.03735709562897682\n",
      "Training loss for batch 7742 : 0.19118469953536987\n",
      "Training loss for batch 7743 : 0.006730305962264538\n",
      "Training loss for batch 7744 : 0.1704453080892563\n",
      "Training loss for batch 7745 : 0.02788395807147026\n",
      "Training loss for batch 7746 : 0.16508308053016663\n",
      "Training loss for batch 7747 : 0.2592279613018036\n",
      "Training loss for batch 7748 : 0.20891033113002777\n",
      "Training loss for batch 7749 : 0.23034264147281647\n",
      "Training loss for batch 7750 : 0.2606605291366577\n",
      "Training loss for batch 7751 : 0.038311369717121124\n",
      "Training loss for batch 7752 : 0.27546340227127075\n",
      "Training loss for batch 7753 : -0.0006326988223008811\n",
      "Training loss for batch 7754 : 0.12925855815410614\n",
      "Training loss for batch 7755 : 0.08553776144981384\n",
      "Training loss for batch 7756 : 0.038194477558135986\n",
      "Training loss for batch 7757 : 0.33873340487480164\n",
      "Training loss for batch 7758 : 0.14466454088687897\n",
      "Training loss for batch 7759 : 0.15477806329727173\n",
      "Training loss for batch 7760 : 0.3665831685066223\n",
      "Training loss for batch 7761 : 0.3012056350708008\n",
      "Training loss for batch 7762 : 0.3830014169216156\n",
      "Training loss for batch 7763 : 0.10588355362415314\n",
      "Training loss for batch 7764 : 0.07289403676986694\n",
      "Training loss for batch 7765 : 0.21529695391654968\n",
      "Training loss for batch 7766 : 0.21189945936203003\n",
      "Training loss for batch 7767 : 0.0905163362622261\n",
      "Training loss for batch 7768 : 0.3940267562866211\n",
      "Training loss for batch 7769 : 0.12736432254314423\n",
      "Training loss for batch 7770 : 0.2751690745353699\n",
      "Training loss for batch 7771 : 0.4264795780181885\n",
      "Training loss for batch 7772 : 0.25009024143218994\n",
      "Training loss for batch 7773 : 0.1565372496843338\n",
      "Training loss for batch 7774 : 0.2326095998287201\n",
      "Training loss for batch 7775 : 0.020908784121274948\n",
      "Training loss for batch 7776 : 0.07598210126161575\n",
      "Training loss for batch 7777 : 0.13650327920913696\n",
      "Training loss for batch 7778 : 0.15511450171470642\n",
      "Training loss for batch 7779 : 0.09365125745534897\n",
      "Training loss for batch 7780 : 0.596379816532135\n",
      "Training loss for batch 7781 : 0.08095290511846542\n",
      "Training loss for batch 7782 : 0.0828300416469574\n",
      "Training loss for batch 7783 : 0.04690582677721977\n",
      "Training loss for batch 7784 : 0.09015299379825592\n",
      "Training loss for batch 7785 : 0.04742603376507759\n",
      "Training loss for batch 7786 : 0.07038196176290512\n",
      "Training loss for batch 7787 : 0.20326755940914154\n",
      "Training loss for batch 7788 : 0.5165001749992371\n",
      "Training loss for batch 7789 : 0.02402787283062935\n",
      "Training loss for batch 7790 : 0.2639572322368622\n",
      "Training loss for batch 7791 : 0.2624600827693939\n",
      "Training loss for batch 7792 : 0.2786983847618103\n",
      "Training loss for batch 7793 : 0.32107701897621155\n",
      "Training loss for batch 7794 : 0.23639646172523499\n",
      "Training loss for batch 7795 : 0.5115317106246948\n",
      "Training loss for batch 7796 : 0.2239590734243393\n",
      "Training loss for batch 7797 : 0.022954633459448814\n",
      "Training loss for batch 7798 : 0.11527081578969955\n",
      "Training loss for batch 7799 : 0.2763574719429016\n",
      "Training loss for batch 7800 : 0.08526994287967682\n",
      "Training loss for batch 7801 : 0.11267104744911194\n",
      "Training loss for batch 7802 : 0.12781904637813568\n",
      "Training loss for batch 7803 : 0.16495856642723083\n",
      "Training loss for batch 7804 : 0.14913241565227509\n",
      "Training loss for batch 7805 : 0.39256760478019714\n",
      "Training loss for batch 7806 : 0.20777815580368042\n",
      "Training loss for batch 7807 : 0.028914153575897217\n",
      "Training loss for batch 7808 : 0.17399190366268158\n",
      "Training loss for batch 7809 : 0.2785075306892395\n",
      "Training loss for batch 7810 : 0.12846389412879944\n",
      "Training loss for batch 7811 : 0.021497413516044617\n",
      "Training loss for batch 7812 : 0.11458202451467514\n",
      "Training loss for batch 7813 : 0.17843683063983917\n",
      "Training loss for batch 7814 : 0.04576761648058891\n",
      "Training loss for batch 7815 : 0.2616307735443115\n",
      "Training loss for batch 7816 : 0.19122757017612457\n",
      "Training loss for batch 7817 : 0.4512045681476593\n",
      "Training loss for batch 7818 : 0.0799293965101242\n",
      "Training loss for batch 7819 : 0.027520958334207535\n",
      "Training loss for batch 7820 : 0.0\n",
      "Training loss for batch 7821 : 0.34396934509277344\n",
      "Training loss for batch 7822 : 0.07138466089963913\n",
      "Training loss for batch 7823 : 0.24398531019687653\n",
      "Training loss for batch 7824 : 0.019899049773812294\n",
      "Training loss for batch 7825 : 0.1269865483045578\n",
      "Training loss for batch 7826 : 0.27488911151885986\n",
      "Training loss for batch 7827 : 0.17887184023857117\n",
      "Training loss for batch 7828 : 0.20143042504787445\n",
      "Training loss for batch 7829 : 0.18345996737480164\n",
      "Training loss for batch 7830 : 0.4019507169723511\n",
      "Training loss for batch 7831 : 0.019078098237514496\n",
      "Training loss for batch 7832 : 0.07586489617824554\n",
      "Training loss for batch 7833 : 0.2685562074184418\n",
      "Training loss for batch 7834 : 0.044047288596630096\n",
      "Training loss for batch 7835 : 0.00904084276407957\n",
      "Training loss for batch 7836 : 0.2749897241592407\n",
      "Training loss for batch 7837 : 0.20697350800037384\n",
      "Training loss for batch 7838 : 0.06788657605648041\n",
      "Training loss for batch 7839 : 0.2722882926464081\n",
      "Training loss for batch 7840 : 0.02713015303015709\n",
      "Training loss for batch 7841 : 0.04766128212213516\n",
      "Training loss for batch 7842 : 0.12355928122997284\n",
      "Training loss for batch 7843 : 0.15831202268600464\n",
      "Training loss for batch 7844 : 0.15731173753738403\n",
      "Training loss for batch 7845 : 0.0031333393417298794\n",
      "Training loss for batch 7846 : 0.21648383140563965\n",
      "Training loss for batch 7847 : 0.12461145967245102\n",
      "Training loss for batch 7848 : 0.023999931290745735\n",
      "Training loss for batch 7849 : 0.0\n",
      "Training loss for batch 7850 : 0.0666297897696495\n",
      "Training loss for batch 7851 : 0.09767032414674759\n",
      "Training loss for batch 7852 : 0.060680702328681946\n",
      "Training loss for batch 7853 : 0.007469574920833111\n",
      "Training loss for batch 7854 : 0.09337109327316284\n",
      "Training loss for batch 7855 : 0.19521358609199524\n",
      "Training loss for batch 7856 : 0.40672871470451355\n",
      "Training loss for batch 7857 : 0.5490609407424927\n",
      "Training loss for batch 7858 : 0.06330716609954834\n",
      "Training loss for batch 7859 : 0.12027052789926529\n",
      "Training loss for batch 7860 : 0.11843306571245193\n",
      "Training loss for batch 7861 : 0.17635983228683472\n",
      "Training loss for batch 7862 : 0.1770317554473877\n",
      "Training loss for batch 7863 : 0.26473939418792725\n",
      "Training loss for batch 7864 : 0.13380677998065948\n",
      "Training loss for batch 7865 : 0.044383905827999115\n",
      "Training loss for batch 7866 : 0.08581410348415375\n",
      "Training loss for batch 7867 : 0.2044287621974945\n",
      "Training loss for batch 7868 : 0.17724454402923584\n",
      "Training loss for batch 7869 : 0.022015636786818504\n",
      "Training loss for batch 7870 : 0.03782331943511963\n",
      "Training loss for batch 7871 : 0.3450102210044861\n",
      "Training loss for batch 7872 : 0.04433096572756767\n",
      "Training loss for batch 7873 : 0.21410837769508362\n",
      "Training loss for batch 7874 : 0.20861771702766418\n",
      "Training loss for batch 7875 : 0.08299432694911957\n",
      "Training loss for batch 7876 : 0.2611943185329437\n",
      "Training loss for batch 7877 : 0.28019100427627563\n",
      "Training loss for batch 7878 : 0.017320793122053146\n",
      "Training loss for batch 7879 : 0.2837163209915161\n",
      "Training loss for batch 7880 : 0.24384427070617676\n",
      "Training loss for batch 7881 : 0.0\n",
      "Training loss for batch 7882 : 0.03400953859090805\n",
      "Training loss for batch 7883 : 0.16967476904392242\n",
      "Training loss for batch 7884 : 0.05785086750984192\n",
      "Training loss for batch 7885 : 0.3286404013633728\n",
      "Training loss for batch 7886 : 0.419230580329895\n",
      "Training loss for batch 7887 : 0.13437728583812714\n",
      "Training loss for batch 7888 : 0.07888323068618774\n",
      "Training loss for batch 7889 : 0.04692104086279869\n",
      "Training loss for batch 7890 : 0.044259052723646164\n",
      "Training loss for batch 7891 : 0.12727423012256622\n",
      "Training loss for batch 7892 : 0.2711704969406128\n",
      "Training loss for batch 7893 : 0.1832285225391388\n",
      "Training loss for batch 7894 : 0.17202046513557434\n",
      "Training loss for batch 7895 : 0.07667896896600723\n",
      "Training loss for batch 7896 : 0.24807922542095184\n",
      "Training loss for batch 7897 : 0.27807629108428955\n",
      "Training loss for batch 7898 : 0.018545610830187798\n",
      "Training loss for batch 7899 : 0.0647081807255745\n",
      "Training loss for batch 7900 : 0.1509283185005188\n",
      "Training loss for batch 7901 : 0.18449023365974426\n",
      "Training loss for batch 7902 : 0.06666743755340576\n",
      "Training loss for batch 7903 : 0.30017900466918945\n",
      "Training loss for batch 7904 : 0.15441088378429413\n",
      "Training loss for batch 7905 : 0.16174818575382233\n",
      "Training loss for batch 7906 : 0.23326554894447327\n",
      "Training loss for batch 7907 : 0.024872632697224617\n",
      "Training loss for batch 7908 : 0.49517545104026794\n",
      "Training loss for batch 7909 : 0.3109464943408966\n",
      "Training loss for batch 7910 : 0.3503601551055908\n",
      "Training loss for batch 7911 : 0.11346789449453354\n",
      "Training loss for batch 7912 : 0.2574298679828644\n",
      "Training loss for batch 7913 : 0.19643355906009674\n",
      "Training loss for batch 7914 : 0.03833308070898056\n",
      "Training loss for batch 7915 : 0.28310585021972656\n",
      "Training loss for batch 7916 : 0.1618175506591797\n",
      "Training loss for batch 7917 : 0.33818280696868896\n",
      "Training loss for batch 7918 : 0.036189496517181396\n",
      "Training loss for batch 7919 : 0.21084019541740417\n",
      "Training loss for batch 7920 : 0.3021281063556671\n",
      "Training loss for batch 7921 : 0.0\n",
      "Training loss for batch 7922 : 0.057067036628723145\n",
      "Training loss for batch 7923 : 0.13948564231395721\n",
      "Training loss for batch 7924 : 0.026172205805778503\n",
      "Training loss for batch 7925 : 0.14021308720111847\n",
      "Training loss for batch 7926 : 0.047379422932863235\n",
      "Training loss for batch 7927 : 0.3914722502231598\n",
      "Training loss for batch 7928 : 0.2554295063018799\n",
      "Training loss for batch 7929 : 0.010784050449728966\n",
      "Training loss for batch 7930 : 0.025398224592208862\n",
      "Training loss for batch 7931 : 0.05275373533368111\n",
      "Training loss for batch 7932 : 0.028576543554663658\n",
      "Training loss for batch 7933 : 0.0445328913629055\n",
      "Training loss for batch 7934 : 0.21449339389801025\n",
      "Training loss for batch 7935 : 0.13547120988368988\n",
      "Training loss for batch 7936 : 0.19565293192863464\n",
      "Training loss for batch 7937 : 0.48874375224113464\n",
      "Training loss for batch 7938 : 0.195903480052948\n",
      "Training loss for batch 7939 : 0.0009413845837116241\n",
      "Training loss for batch 7940 : 0.333853542804718\n",
      "Training loss for batch 7941 : 0.19598202407360077\n",
      "Training loss for batch 7942 : 0.08181310445070267\n",
      "Training loss for batch 7943 : 0.1778663694858551\n",
      "Training loss for batch 7944 : 0.3164679706096649\n",
      "Training loss for batch 7945 : 0.09449999034404755\n",
      "Training loss for batch 7946 : 0.025037918239831924\n",
      "Training loss for batch 7947 : 0.05594108626246452\n",
      "Training loss for batch 7948 : 0.3326556086540222\n",
      "Training loss for batch 7949 : 0.47919583320617676\n",
      "Training loss for batch 7950 : 0.09546606242656708\n",
      "Training loss for batch 7951 : 0.13219808042049408\n",
      "Training loss for batch 7952 : 0.13181273639202118\n",
      "Training loss for batch 7953 : 0.1263773888349533\n",
      "Training loss for batch 7954 : 0.04868984594941139\n",
      "Training loss for batch 7955 : 0.2585870921611786\n",
      "Training loss for batch 7956 : 0.028415724635124207\n",
      "Training loss for batch 7957 : 0.3097113370895386\n",
      "Training loss for batch 7958 : 0.3256150484085083\n",
      "Training loss for batch 7959 : 0.28416335582733154\n",
      "Training loss for batch 7960 : 0.233025461435318\n",
      "Training loss for batch 7961 : 0.1352023184299469\n",
      "Training loss for batch 7962 : 0.05485711246728897\n",
      "Training loss for batch 7963 : 0.24141499400138855\n",
      "Training loss for batch 7964 : 0.006771688815206289\n",
      "Training loss for batch 7965 : 0.0\n",
      "Training loss for batch 7966 : 0.008043348789215088\n",
      "Training loss for batch 7967 : 0.08239492028951645\n",
      "Training loss for batch 7968 : 0.25166580080986023\n",
      "Training loss for batch 7969 : 0.32521164417266846\n",
      "Training loss for batch 7970 : 0.2789667844772339\n",
      "Training loss for batch 7971 : 0.3730994164943695\n",
      "Training loss for batch 7972 : 0.38842475414276123\n",
      "Training loss for batch 7973 : 0.11528678238391876\n",
      "Training loss for batch 7974 : 0.5466433167457581\n",
      "Training loss for batch 7975 : 0.041906245052814484\n",
      "Training loss for batch 7976 : 0.06069332733750343\n",
      "Training loss for batch 7977 : -0.0016063491348177195\n",
      "Training loss for batch 7978 : 0.05300792306661606\n",
      "Training loss for batch 7979 : 0.26529866456985474\n",
      "Training loss for batch 7980 : 0.35715192556381226\n",
      "Training loss for batch 7981 : 0.07503256946802139\n",
      "Training loss for batch 7982 : 0.24454480409622192\n",
      "Training loss for batch 7983 : 0.1909267157316208\n",
      "Training loss for batch 7984 : 0.1981431096792221\n",
      "Training loss for batch 7985 : 0.0580911710858345\n",
      "Training loss for batch 7986 : 0.14224685728549957\n",
      "Training loss for batch 7987 : 0.11719193309545517\n",
      "Training loss for batch 7988 : 0.07707619667053223\n",
      "Training loss for batch 7989 : 0.196172297000885\n",
      "Training loss for batch 7990 : 0.056246183812618256\n",
      "Training loss for batch 7991 : 0.3513765335083008\n",
      "Training loss for batch 7992 : 0.056564364582300186\n",
      "Training loss for batch 7993 : 0.052342742681503296\n",
      "Training loss for batch 7994 : 0.17990058660507202\n",
      "Training loss for batch 7995 : 0.1299007534980774\n",
      "Training loss for batch 7996 : 0.13190525770187378\n",
      "Training loss for batch 7997 : 0.05688532441854477\n",
      "Training loss for batch 7998 : 0.0\n",
      "Training loss for batch 7999 : 0.40297773480415344\n",
      "Training loss for batch 8000 : 0.07390129566192627\n",
      "Training loss for batch 8001 : 0.06078898161649704\n",
      "Training loss for batch 8002 : 0.2105017453432083\n",
      "Training loss for batch 8003 : 0.014088163152337074\n",
      "Training loss for batch 8004 : 0.029132001101970673\n",
      "Training loss for batch 8005 : 0.3357391357421875\n",
      "Training loss for batch 8006 : 0.06472206860780716\n",
      "Training loss for batch 8007 : 0.17244866490364075\n",
      "Training loss for batch 8008 : 0.15336500108242035\n",
      "Training loss for batch 8009 : 0.32779568433761597\n",
      "Training loss for batch 8010 : 0.11897627264261246\n",
      "Training loss for batch 8011 : 0.159873828291893\n",
      "Training loss for batch 8012 : 0.35138240456581116\n",
      "Training loss for batch 8013 : 0.06028071790933609\n",
      "Training loss for batch 8014 : 0.07644180953502655\n",
      "Training loss for batch 8015 : 0.6348657608032227\n",
      "Training loss for batch 8016 : 0.30848634243011475\n",
      "Training loss for batch 8017 : 0.10758558660745621\n",
      "Training loss for batch 8018 : 0.02795913815498352\n",
      "Training loss for batch 8019 : 0.0034267432056367397\n",
      "Training loss for batch 8020 : 0.32465311884880066\n",
      "Training loss for batch 8021 : 0.08317434042692184\n",
      "Training loss for batch 8022 : 0.1526133269071579\n",
      "Training loss for batch 8023 : 0.018741745501756668\n",
      "Training loss for batch 8024 : 0.11211402714252472\n",
      "Training loss for batch 8025 : 0.01989988423883915\n",
      "Training loss for batch 8026 : 0.08434377610683441\n",
      "Training loss for batch 8027 : 0.01378307119011879\n",
      "Training loss for batch 8028 : 0.037856005132198334\n",
      "Training loss for batch 8029 : 0.07991039752960205\n",
      "Training loss for batch 8030 : 0.1574486941099167\n",
      "Training loss for batch 8031 : 0.03673408553004265\n",
      "Training loss for batch 8032 : 0.13437797129154205\n",
      "Training loss for batch 8033 : 0.08496001362800598\n",
      "Training loss for batch 8034 : 0.07636019587516785\n",
      "Training loss for batch 8035 : 0.42117631435394287\n",
      "Training loss for batch 8036 : 0.26182660460472107\n",
      "Training loss for batch 8037 : 0.20597544312477112\n",
      "Training loss for batch 8038 : 0.16208411753177643\n",
      "Training loss for batch 8039 : 0.015881245955824852\n",
      "Training loss for batch 8040 : 0.02038692496716976\n",
      "Training loss for batch 8041 : 0.09632712602615356\n",
      "Training loss for batch 8042 : 0.12323986738920212\n",
      "Training loss for batch 8043 : 0.038398075848817825\n",
      "Training loss for batch 8044 : 0.09705328196287155\n",
      "Training loss for batch 8045 : 0.2801198363304138\n",
      "Training loss for batch 8046 : 0.19548138976097107\n",
      "Training loss for batch 8047 : 0.08703535050153732\n",
      "Training loss for batch 8048 : 0.23090237379074097\n",
      "Training loss for batch 8049 : 0.26282456517219543\n",
      "Training loss for batch 8050 : 0.22359997034072876\n",
      "Training loss for batch 8051 : 0.06102822348475456\n",
      "Training loss for batch 8052 : 0.3262379765510559\n",
      "Training loss for batch 8053 : 0.31005626916885376\n",
      "Training loss for batch 8054 : 0.056757621467113495\n",
      "Training loss for batch 8055 : 0.011295696720480919\n",
      "Training loss for batch 8056 : 0.10688301175832748\n",
      "Training loss for batch 8057 : 0.028300294652581215\n",
      "Training loss for batch 8058 : 0.07754194736480713\n",
      "Training loss for batch 8059 : 0.19444553554058075\n",
      "Training loss for batch 8060 : 0.008775101974606514\n",
      "Training loss for batch 8061 : 0.0423387810587883\n",
      "Training loss for batch 8062 : 0.1116446852684021\n",
      "Training loss for batch 8063 : 0.19549041986465454\n",
      "Training loss for batch 8064 : 0.08995570242404938\n",
      "Training loss for batch 8065 : 0.13036683201789856\n",
      "Training loss for batch 8066 : 0.24905052781105042\n",
      "Training loss for batch 8067 : 0.0\n",
      "Training loss for batch 8068 : 0.18174569308757782\n",
      "Training loss for batch 8069 : 0.40949174761772156\n",
      "Training loss for batch 8070 : 0.44194677472114563\n",
      "Training loss for batch 8071 : 0.07402125746011734\n",
      "Training loss for batch 8072 : 0.028105955570936203\n",
      "Training loss for batch 8073 : 0.027459418401122093\n",
      "Training loss for batch 8074 : 0.15126775205135345\n",
      "Training loss for batch 8075 : 0.10440098494291306\n",
      "Training loss for batch 8076 : 0.08247701823711395\n",
      "Training loss for batch 8077 : 0.32580554485321045\n",
      "Training loss for batch 8078 : 0.07784891873598099\n",
      "Training loss for batch 8079 : 0.22220830619335175\n",
      "Training loss for batch 8080 : 0.21844704449176788\n",
      "Training loss for batch 8081 : 0.12528623640537262\n",
      "Training loss for batch 8082 : 0.15622399747371674\n",
      "Training loss for batch 8083 : 0.2107008397579193\n",
      "Training loss for batch 8084 : 0.0011800130596384406\n",
      "Training loss for batch 8085 : 0.05523224547505379\n",
      "Training loss for batch 8086 : 0.011609602719545364\n",
      "Training loss for batch 8087 : 0.1665463000535965\n",
      "Training loss for batch 8088 : 0.2807528078556061\n",
      "Training loss for batch 8089 : 0.39206331968307495\n",
      "Training loss for batch 8090 : 0.18013891577720642\n",
      "Training loss for batch 8091 : 0.26847174763679504\n",
      "Training loss for batch 8092 : 0.0681704506278038\n",
      "Training loss for batch 8093 : 0.04440448060631752\n",
      "Training loss for batch 8094 : 0.0008313409052789211\n",
      "Training loss for batch 8095 : 0.32708144187927246\n",
      "Training loss for batch 8096 : 0.128460094332695\n",
      "Training loss for batch 8097 : 0.10447037220001221\n",
      "Training loss for batch 8098 : 0.3072452247142792\n",
      "Training loss for batch 8099 : 0.1352335810661316\n",
      "Training loss for batch 8100 : 0.27480724453926086\n",
      "Training loss for batch 8101 : 0.16785261034965515\n",
      "Training loss for batch 8102 : 0.16288232803344727\n",
      "Training loss for batch 8103 : 0.19812585413455963\n",
      "Training loss for batch 8104 : 0.019483452662825584\n",
      "Training loss for batch 8105 : 0.18433290719985962\n",
      "Training loss for batch 8106 : 0.26994630694389343\n",
      "Training loss for batch 8107 : 0.06734783947467804\n",
      "Training loss for batch 8108 : -0.0018876980757340789\n",
      "Training loss for batch 8109 : 0.060017891228199005\n",
      "Training loss for batch 8110 : 0.030192922800779343\n",
      "Training loss for batch 8111 : 0.0165366530418396\n",
      "Training loss for batch 8112 : 0.1216268315911293\n",
      "Training loss for batch 8113 : 0.05880114436149597\n",
      "Training loss for batch 8114 : 0.2822812497615814\n",
      "Training loss for batch 8115 : 0.051397353410720825\n",
      "Training loss for batch 8116 : 0.0\n",
      "Training loss for batch 8117 : 0.0\n",
      "Training loss for batch 8118 : 0.06018562614917755\n",
      "Training loss for batch 8119 : 0.4145086109638214\n",
      "Training loss for batch 8120 : 0.04713429510593414\n",
      "Training loss for batch 8121 : 0.10564244538545609\n",
      "Training loss for batch 8122 : 0.3180205523967743\n",
      "Training loss for batch 8123 : 0.3330385982990265\n",
      "Training loss for batch 8124 : 0.021309809759259224\n",
      "Training loss for batch 8125 : 0.1291198581457138\n",
      "Training loss for batch 8126 : 0.09149766713380814\n",
      "Training loss for batch 8127 : 0.03833916410803795\n",
      "Training loss for batch 8128 : 0.01976221427321434\n",
      "Training loss for batch 8129 : 0.22905483841896057\n",
      "Training loss for batch 8130 : 0.12125371396541595\n",
      "Training loss for batch 8131 : 0.0\n",
      "Training loss for batch 8132 : 0.1319863498210907\n",
      "Training loss for batch 8133 : 0.09856902062892914\n",
      "Training loss for batch 8134 : 0.08554684370756149\n",
      "Training loss for batch 8135 : 0.2390381246805191\n",
      "Training loss for batch 8136 : 0.2863115966320038\n",
      "Training loss for batch 8137 : 0.05677742511034012\n",
      "Training loss for batch 8138 : 0.3002431392669678\n",
      "Training loss for batch 8139 : 0.21819598972797394\n",
      "Training loss for batch 8140 : 0.18727128207683563\n",
      "Training loss for batch 8141 : 0.15427201986312866\n",
      "Training loss for batch 8142 : 0.1541658639907837\n",
      "Training loss for batch 8143 : 0.3007592260837555\n",
      "Training loss for batch 8144 : 0.22266072034835815\n",
      "Training loss for batch 8145 : 0.03900880739092827\n",
      "Training loss for batch 8146 : 0.058383531868457794\n",
      "Training loss for batch 8147 : 0.08690827339887619\n",
      "Training loss for batch 8148 : 0.2595709562301636\n",
      "Training loss for batch 8149 : 0.39759260416030884\n",
      "Training loss for batch 8150 : 0.48354944586753845\n",
      "Training loss for batch 8151 : 0.1758139431476593\n",
      "Training loss for batch 8152 : 0.06291348487138748\n",
      "Training loss for batch 8153 : 0.16888633370399475\n",
      "Training loss for batch 8154 : 0.4477024972438812\n",
      "Training loss for batch 8155 : 0.40933874249458313\n",
      "Training loss for batch 8156 : 0.36236700415611267\n",
      "Training loss for batch 8157 : 0.23763301968574524\n",
      "Training loss for batch 8158 : 0.27308547496795654\n",
      "Training loss for batch 8159 : 0.08367481082677841\n",
      "Training loss for batch 8160 : 0.008481403812766075\n",
      "Training loss for batch 8161 : 0.258499413728714\n",
      "Training loss for batch 8162 : 0.05691475793719292\n",
      "Training loss for batch 8163 : 0.0468418225646019\n",
      "Training loss for batch 8164 : 0.010746664367616177\n",
      "Training loss for batch 8165 : 0.042010653764009476\n",
      "Training loss for batch 8166 : 0.17115044593811035\n",
      "Training loss for batch 8167 : 0.22509466111660004\n",
      "Training loss for batch 8168 : 0.3868803381919861\n",
      "Training loss for batch 8169 : 0.16899608075618744\n",
      "Training loss for batch 8170 : 0.46061986684799194\n",
      "Training loss for batch 8171 : 0.149724543094635\n",
      "Training loss for batch 8172 : 0.050441186875104904\n",
      "Training loss for batch 8173 : 0.12057533115148544\n",
      "Training loss for batch 8174 : 0.25441300868988037\n",
      "Training loss for batch 8175 : 0.12292502075433731\n",
      "Training loss for batch 8176 : 0.16108113527297974\n",
      "Training loss for batch 8177 : 0.11064910888671875\n",
      "Training loss for batch 8178 : 0.23879770934581757\n",
      "Training loss for batch 8179 : 0.413357138633728\n",
      "Training loss for batch 8180 : 0.10711325705051422\n",
      "Training loss for batch 8181 : 0.02224118635058403\n",
      "Training loss for batch 8182 : 0.04177304357290268\n",
      "Training loss for batch 8183 : 0.20551379024982452\n",
      "Training loss for batch 8184 : 0.17310631275177002\n",
      "Training loss for batch 8185 : 0.05899932608008385\n",
      "Training loss for batch 8186 : 0.3502020835876465\n",
      "Training loss for batch 8187 : 0.2357979267835617\n",
      "Training loss for batch 8188 : 0.31239593029022217\n",
      "Training loss for batch 8189 : 0.34133464097976685\n",
      "Training loss for batch 8190 : 0.09910593926906586\n",
      "Training loss for batch 8191 : 0.14996962249279022\n",
      "Training loss for batch 8192 : 0.05342055857181549\n",
      "Training loss for batch 8193 : 0.03893899917602539\n",
      "Training loss for batch 8194 : 0.2900283932685852\n",
      "Training loss for batch 8195 : 0.2623603940010071\n",
      "Training loss for batch 8196 : 0.28838562965393066\n",
      "Training loss for batch 8197 : 0.013490059413015842\n",
      "Training loss for batch 8198 : 0.080745629966259\n",
      "Training loss for batch 8199 : 0.15785010159015656\n",
      "Training loss for batch 8200 : 0.15497376024723053\n",
      "Training loss for batch 8201 : 0.2060144543647766\n",
      "Training loss for batch 8202 : 0.02410867065191269\n",
      "Training loss for batch 8203 : 0.09242556989192963\n",
      "Training loss for batch 8204 : 0.47648781538009644\n",
      "Training loss for batch 8205 : 0.34925198554992676\n",
      "Training loss for batch 8206 : 0.32042258977890015\n",
      "Training loss for batch 8207 : 0.16320490837097168\n",
      "Training loss for batch 8208 : 0.1893026977777481\n",
      "Training loss for batch 8209 : 0.1739121526479721\n",
      "Training loss for batch 8210 : 0.34200841188430786\n",
      "Training loss for batch 8211 : 0.07694088667631149\n",
      "Training loss for batch 8212 : 0.008885828778147697\n",
      "Training loss for batch 8213 : 0.019211243838071823\n",
      "Training loss for batch 8214 : 0.058101870119571686\n",
      "Training loss for batch 8215 : 0.42594727873802185\n",
      "Training loss for batch 8216 : 0.18169941008090973\n",
      "Training loss for batch 8217 : 0.4911789298057556\n",
      "Training loss for batch 8218 : 0.37459731101989746\n",
      "Training loss for batch 8219 : 0.02647496946156025\n",
      "Training loss for batch 8220 : 0.42246294021606445\n",
      "Training loss for batch 8221 : 0.018829360604286194\n",
      "Training loss for batch 8222 : 0.25676488876342773\n",
      "Training loss for batch 8223 : 0.4125695824623108\n",
      "Training loss for batch 8224 : 0.2639602720737457\n",
      "Training loss for batch 8225 : 0.08691690117120743\n",
      "Training loss for batch 8226 : 0.19985775649547577\n",
      "Training loss for batch 8227 : 0.21485105156898499\n",
      "Training loss for batch 8228 : 0.13007546961307526\n",
      "Training loss for batch 8229 : 0.19055908918380737\n",
      "Training loss for batch 8230 : 0.11956671625375748\n",
      "Training loss for batch 8231 : 0.1344187706708908\n",
      "Training loss for batch 8232 : 0.46348249912261963\n",
      "Training loss for batch 8233 : 0.1178942620754242\n",
      "Training loss for batch 8234 : 0.06856580078601837\n",
      "Training loss for batch 8235 : 0.08545506000518799\n",
      "Training loss for batch 8236 : 0.09141666442155838\n",
      "Training loss for batch 8237 : 0.06928170472383499\n",
      "Training loss for batch 8238 : 0.3186117708683014\n",
      "Training loss for batch 8239 : 0.15424321591854095\n",
      "Training loss for batch 8240 : 0.15133409202098846\n",
      "Training loss for batch 8241 : 0.15417471528053284\n",
      "Training loss for batch 8242 : 0.13486990332603455\n",
      "Training loss for batch 8243 : 0.08642078936100006\n",
      "Training loss for batch 8244 : 0.0006417114054784179\n",
      "Training loss for batch 8245 : 0.024090617895126343\n",
      "Training loss for batch 8246 : 0.2500161826610565\n",
      "Training loss for batch 8247 : 0.11229178309440613\n",
      "Training loss for batch 8248 : 0.08954890817403793\n",
      "Training loss for batch 8249 : 0.38123083114624023\n",
      "Training loss for batch 8250 : 0.4442492127418518\n",
      "Training loss for batch 8251 : 0.3643304407596588\n",
      "Training loss for batch 8252 : 0.2970711588859558\n",
      "Training loss for batch 8253 : 0.3115304112434387\n",
      "Training loss for batch 8254 : 0.1772013008594513\n",
      "Training loss for batch 8255 : 0.2810707986354828\n",
      "Training loss for batch 8256 : 0.20283861458301544\n",
      "Training loss for batch 8257 : 0.2742777168750763\n",
      "Training loss for batch 8258 : 0.24452029168605804\n",
      "Training loss for batch 8259 : 0.06645587086677551\n",
      "Training loss for batch 8260 : 0.17066392302513123\n",
      "Training loss for batch 8261 : 0.20266371965408325\n",
      "Training loss for batch 8262 : 0.4095592200756073\n",
      "Training loss for batch 8263 : 0.029214564710855484\n",
      "Training loss for batch 8264 : 0.16528388857841492\n",
      "Training loss for batch 8265 : 0.12455955892801285\n",
      "Training loss for batch 8266 : 0.004898647777736187\n",
      "Training loss for batch 8267 : 0.0\n",
      "Training loss for batch 8268 : 0.13138918578624725\n",
      "Training loss for batch 8269 : 0.0916675254702568\n",
      "Training loss for batch 8270 : 0.2816954255104065\n",
      "Training loss for batch 8271 : 0.031257838010787964\n",
      "Training loss for batch 8272 : 0.28830793499946594\n",
      "Training loss for batch 8273 : 0.09785840660333633\n",
      "Training loss for batch 8274 : 0.06336954981088638\n",
      "Training loss for batch 8275 : 0.3711809813976288\n",
      "Training loss for batch 8276 : 0.0924449935555458\n",
      "Training loss for batch 8277 : 0.21874549984931946\n",
      "Training loss for batch 8278 : 0.17420507967472076\n",
      "Training loss for batch 8279 : 0.43674710392951965\n",
      "Training loss for batch 8280 : 0.1604723036289215\n",
      "Training loss for batch 8281 : 0.02321162074804306\n",
      "Training loss for batch 8282 : 0.27406948804855347\n",
      "Training loss for batch 8283 : 0.18054385483264923\n",
      "Training loss for batch 8284 : 0.017658408731222153\n",
      "Training loss for batch 8285 : 0.2848692536354065\n",
      "Training loss for batch 8286 : 0.31346553564071655\n",
      "Training loss for batch 8287 : 0.2714741826057434\n",
      "Training loss for batch 8288 : 0.3006582260131836\n",
      "Training loss for batch 8289 : 0.039324596524238586\n",
      "Training loss for batch 8290 : 0.13131654262542725\n",
      "Training loss for batch 8291 : 0.2822488844394684\n",
      "Training loss for batch 8292 : 0.2976091504096985\n",
      "Training loss for batch 8293 : 0.219279944896698\n",
      "Training loss for batch 8294 : 0.14173929393291473\n",
      "Training loss for batch 8295 : 0.3236045837402344\n",
      "Training loss for batch 8296 : 0.2494264841079712\n",
      "Training loss for batch 8297 : 0.1586904078722\n",
      "Training loss for batch 8298 : 0.355333536863327\n",
      "Training loss for batch 8299 : 0.1449069082736969\n",
      "Training loss for batch 8300 : 0.07938416302204132\n",
      "Training loss for batch 8301 : 0.18708570301532745\n",
      "Training loss for batch 8302 : 0.03645508363842964\n",
      "Training loss for batch 8303 : 0.04771687090396881\n",
      "Training loss for batch 8304 : 0.23336893320083618\n",
      "Training loss for batch 8305 : 0.16731178760528564\n",
      "Training loss for batch 8306 : 0.1378747671842575\n",
      "Training loss for batch 8307 : 0.8600276112556458\n",
      "Parameter containing:\n",
      "tensor(-0.1400, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 0.8600147366523743\n",
      "Training loss for batch 1 : 0.8599967956542969\n",
      "Training loss for batch 2 : 0.85997474193573\n",
      "Training loss for batch 3 : 0.859948992729187\n",
      "Training loss for batch 4 : 0.8599201440811157\n",
      "Training loss for batch 5 : 0.8598885536193848\n",
      "Training loss for batch 6 : 0.8598547577857971\n",
      "Training loss for batch 7 : 0.8598190546035767\n",
      "Training loss for batch 8 : 0.2073860913515091\n",
      "Training loss for batch 9 : 0.8597469329833984\n",
      "Training loss for batch 10 : 0.8597104549407959\n",
      "Training loss for batch 11 : 0.8596726059913635\n",
      "Training loss for batch 12 : 0.8596336245536804\n",
      "Training loss for batch 13 : 0.05186624452471733\n",
      "Training loss for batch 14 : 0.2165367305278778\n",
      "Training loss for batch 15 : 0.0\n",
      "Training loss for batch 16 : 0.08074579387903214\n",
      "Training loss for batch 17 : 0.21974937617778778\n",
      "Training loss for batch 18 : 0.5837157964706421\n",
      "Training loss for batch 19 : 0.18316787481307983\n",
      "Training loss for batch 20 : 0.1454898566007614\n",
      "Training loss for batch 21 : 0.17608758807182312\n",
      "Training loss for batch 22 : 0.019544249400496483\n",
      "Training loss for batch 23 : 0.09451241791248322\n",
      "Training loss for batch 24 : 0.07453331351280212\n",
      "Training loss for batch 25 : 0.00048428773880004883\n",
      "Training loss for batch 26 : 0.032345615327358246\n",
      "Training loss for batch 27 : 0.19140779972076416\n",
      "Training loss for batch 28 : 0.08704357594251633\n",
      "Training loss for batch 29 : 0.1781361997127533\n",
      "Training loss for batch 30 : 0.0860273540019989\n",
      "Training loss for batch 31 : 0.033995404839515686\n",
      "Training loss for batch 32 : 0.04350821301341057\n",
      "Training loss for batch 33 : 0.0645081177353859\n",
      "Training loss for batch 34 : 0.09182160347700119\n",
      "Training loss for batch 35 : 0.3761160671710968\n",
      "Training loss for batch 36 : 0.23177549242973328\n",
      "Training loss for batch 37 : 0.08291344344615936\n",
      "Training loss for batch 38 : 0.029735881835222244\n",
      "Training loss for batch 39 : 0.07798845320940018\n",
      "Training loss for batch 40 : 0.04430786892771721\n",
      "Training loss for batch 41 : 0.1114184707403183\n",
      "Training loss for batch 42 : 0.12284980714321136\n",
      "Training loss for batch 43 : 0.01159087847918272\n",
      "Training loss for batch 44 : 0.11271004378795624\n",
      "Training loss for batch 45 : 0.08070328831672668\n",
      "Training loss for batch 46 : 0.08625923097133636\n",
      "Training loss for batch 47 : 0.10634273290634155\n",
      "Training loss for batch 48 : 0.06639020144939423\n",
      "Training loss for batch 49 : 0.13658888638019562\n",
      "Training loss for batch 50 : 0.22158733010292053\n",
      "Training loss for batch 51 : 0.4283348321914673\n",
      "Training loss for batch 52 : 0.23672538995742798\n",
      "Training loss for batch 53 : 0.11449365317821503\n",
      "Training loss for batch 54 : 0.1157626137137413\n",
      "Training loss for batch 55 : 0.15917544066905975\n",
      "Training loss for batch 56 : 0.33776015043258667\n",
      "Training loss for batch 57 : 0.08208899945020676\n",
      "Training loss for batch 58 : 0.045573946088552475\n",
      "Training loss for batch 59 : 0.061882905662059784\n",
      "Training loss for batch 60 : 0.10767178237438202\n",
      "Training loss for batch 61 : 0.012758170254528522\n",
      "Training loss for batch 62 : 0.20012222230434418\n",
      "Training loss for batch 63 : 0.11375850439071655\n",
      "Training loss for batch 64 : 0.06409809738397598\n",
      "Training loss for batch 65 : 0.1355096995830536\n",
      "Training loss for batch 66 : 0.025335900485515594\n",
      "Training loss for batch 67 : 0.28319430351257324\n",
      "Training loss for batch 68 : 0.41377943754196167\n",
      "Training loss for batch 69 : 0.012032990343868732\n",
      "Training loss for batch 70 : 0.07798019796609879\n",
      "Training loss for batch 71 : 0.031171519309282303\n",
      "Training loss for batch 72 : 0.32799261808395386\n",
      "Training loss for batch 73 : 0.096895731985569\n",
      "Training loss for batch 74 : 0.3120326101779938\n",
      "Training loss for batch 75 : 0.11357171833515167\n",
      "Training loss for batch 76 : 0.11644507199525833\n",
      "Training loss for batch 77 : 0.19686171412467957\n",
      "Training loss for batch 78 : 0.06877988576889038\n",
      "Training loss for batch 79 : 0.013286938890814781\n",
      "Training loss for batch 80 : 0.020085684955120087\n",
      "Training loss for batch 81 : -0.0007392331026494503\n",
      "Training loss for batch 82 : 0.05539478734135628\n",
      "Training loss for batch 83 : 0.11204487085342407\n",
      "Training loss for batch 84 : 0.01683322712779045\n",
      "Training loss for batch 85 : 0.1411723643541336\n",
      "Training loss for batch 86 : 0.25021958351135254\n",
      "Training loss for batch 87 : 0.05792646110057831\n",
      "Training loss for batch 88 : 0.15063327550888062\n",
      "Training loss for batch 89 : 0.16346138715744019\n",
      "Training loss for batch 90 : 0.04666805639863014\n",
      "Training loss for batch 91 : 0.2721168100833893\n",
      "Training loss for batch 92 : 0.09609480202198029\n",
      "Training loss for batch 93 : 0.05344943702220917\n",
      "Training loss for batch 94 : 0.06055799499154091\n",
      "Training loss for batch 95 : 0.10738334059715271\n",
      "Training loss for batch 96 : 0.028046976774930954\n",
      "Training loss for batch 97 : 0.0028717475943267345\n",
      "Training loss for batch 98 : 0.099101722240448\n",
      "Training loss for batch 99 : 0.04718466475605965\n",
      "Training loss for batch 100 : 0.26292163133621216\n",
      "Training loss for batch 101 : 0.08542241901159286\n",
      "Training loss for batch 102 : 0.1885758638381958\n",
      "Training loss for batch 103 : 0.09934251755475998\n",
      "Training loss for batch 104 : 0.09398350864648819\n",
      "Training loss for batch 105 : 0.3982850909233093\n",
      "Training loss for batch 106 : 0.08967136591672897\n",
      "Training loss for batch 107 : 0.3479706346988678\n",
      "Training loss for batch 108 : 0.050677672028541565\n",
      "Training loss for batch 109 : 0.07642614096403122\n",
      "Training loss for batch 110 : 0.067257359623909\n",
      "Training loss for batch 111 : 0.25265660881996155\n",
      "Training loss for batch 112 : 0.13851754367351532\n",
      "Training loss for batch 113 : 0.06797782331705093\n",
      "Training loss for batch 114 : 0.2428504079580307\n",
      "Training loss for batch 115 : 0.2964242398738861\n",
      "Training loss for batch 116 : 0.08897557109594345\n",
      "Training loss for batch 117 : 0.2860579192638397\n",
      "Training loss for batch 118 : 0.02683815360069275\n",
      "Training loss for batch 119 : 0.003724699839949608\n",
      "Training loss for batch 120 : 0.2067493498325348\n",
      "Training loss for batch 121 : 0.05192797631025314\n",
      "Training loss for batch 122 : 0.08802895247936249\n",
      "Training loss for batch 123 : 0.07374584674835205\n",
      "Training loss for batch 124 : 0.07860817760229111\n",
      "Training loss for batch 125 : 0.007422088645398617\n",
      "Training loss for batch 126 : 0.24859881401062012\n",
      "Training loss for batch 127 : 0.15017956495285034\n",
      "Training loss for batch 128 : 0.03135048598051071\n",
      "Training loss for batch 129 : 0.053463347256183624\n",
      "Training loss for batch 130 : 0.22391973435878754\n",
      "Training loss for batch 131 : 0.023151036351919174\n",
      "Training loss for batch 132 : 0.650999903678894\n",
      "Training loss for batch 133 : 0.3184521794319153\n",
      "Training loss for batch 134 : 0.06007790565490723\n",
      "Training loss for batch 135 : 0.09678085148334503\n",
      "Training loss for batch 136 : 0.17172056436538696\n",
      "Training loss for batch 137 : 0.14716507494449615\n",
      "Training loss for batch 138 : 0.04097206890583038\n",
      "Training loss for batch 139 : 0.10036642849445343\n",
      "Training loss for batch 140 : 0.14285144209861755\n",
      "Training loss for batch 141 : 0.14734113216400146\n",
      "Training loss for batch 142 : 0.06648358702659607\n",
      "Training loss for batch 143 : 0.12802578508853912\n",
      "Training loss for batch 144 : 0.23624280095100403\n",
      "Training loss for batch 145 : 0.24260228872299194\n",
      "Training loss for batch 146 : 0.1327037662267685\n",
      "Training loss for batch 147 : 0.2533520758152008\n",
      "Training loss for batch 148 : 0.10963650047779083\n",
      "Training loss for batch 149 : 0.03411855548620224\n",
      "Training loss for batch 150 : 0.07824450731277466\n",
      "Training loss for batch 151 : 0.27750468254089355\n",
      "Training loss for batch 152 : 0.07418354600667953\n",
      "Training loss for batch 153 : 0.04829428717494011\n",
      "Training loss for batch 154 : 0.41744133830070496\n",
      "Training loss for batch 155 : 0.4605098068714142\n",
      "Training loss for batch 156 : 0.21146303415298462\n",
      "Training loss for batch 157 : 0.03502437472343445\n",
      "Training loss for batch 158 : 0.0\n",
      "Training loss for batch 159 : 0.17762860655784607\n",
      "Training loss for batch 160 : 0.09455094486474991\n",
      "Training loss for batch 161 : 0.26467907428741455\n",
      "Training loss for batch 162 : 0.08244909346103668\n",
      "Training loss for batch 163 : 0.2119143307209015\n",
      "Training loss for batch 164 : 0.01133408211171627\n",
      "Training loss for batch 165 : 0.36538422107696533\n",
      "Training loss for batch 166 : 0.13335783779621124\n",
      "Training loss for batch 167 : 0.22784413397312164\n",
      "Training loss for batch 168 : 0.10871294140815735\n",
      "Training loss for batch 169 : 0.05166228488087654\n",
      "Training loss for batch 170 : 0.22372683882713318\n",
      "Training loss for batch 171 : 0.0940113216638565\n",
      "Training loss for batch 172 : 0.23171760141849518\n",
      "Training loss for batch 173 : 0.21120062470436096\n",
      "Training loss for batch 174 : 0.2804173529148102\n",
      "Training loss for batch 175 : 0.12459420412778854\n",
      "Training loss for batch 176 : 0.03147171810269356\n",
      "Training loss for batch 177 : 0.04772175848484039\n",
      "Training loss for batch 178 : 0.10006844252347946\n",
      "Training loss for batch 179 : 0.26673221588134766\n",
      "Training loss for batch 180 : 0.09077245742082596\n",
      "Training loss for batch 181 : 0.03553254157304764\n",
      "Training loss for batch 182 : 0.048004988580942154\n",
      "Training loss for batch 183 : 0.16275955736637115\n",
      "Training loss for batch 184 : 0.1655162125825882\n",
      "Training loss for batch 185 : 0.18047088384628296\n",
      "Training loss for batch 186 : 0.22153764963150024\n",
      "Training loss for batch 187 : 0.16031762957572937\n",
      "Training loss for batch 188 : 0.1979176253080368\n",
      "Training loss for batch 189 : 0.033313874155282974\n",
      "Training loss for batch 190 : 0.03528902307152748\n",
      "Training loss for batch 191 : 0.045763734728097916\n",
      "Training loss for batch 192 : 0.06426313519477844\n",
      "Training loss for batch 193 : 0.19189077615737915\n",
      "Training loss for batch 194 : 0.13054916262626648\n",
      "Training loss for batch 195 : 0.15588083863258362\n",
      "Training loss for batch 196 : 0.21301674842834473\n",
      "Training loss for batch 197 : 0.1531924307346344\n",
      "Training loss for batch 198 : 0.008139291778206825\n",
      "Training loss for batch 199 : 0.06031063571572304\n",
      "Training loss for batch 200 : 0.27459612488746643\n",
      "Training loss for batch 201 : 0.044165514409542084\n",
      "Training loss for batch 202 : 0.23080672323703766\n",
      "Training loss for batch 203 : 0.10523401200771332\n",
      "Training loss for batch 204 : 0.043095044791698456\n",
      "Training loss for batch 205 : 0.2001601755619049\n",
      "Training loss for batch 206 : 0.03227256238460541\n",
      "Training loss for batch 207 : 0.25206634402275085\n",
      "Training loss for batch 208 : 0.3049625754356384\n",
      "Training loss for batch 209 : 0.21976801753044128\n",
      "Training loss for batch 210 : -0.0005308272666297853\n",
      "Training loss for batch 211 : 0.23735840618610382\n",
      "Training loss for batch 212 : 0.03756071999669075\n",
      "Training loss for batch 213 : 0.18916980922222137\n",
      "Training loss for batch 214 : 0.15273049473762512\n",
      "Training loss for batch 215 : 0.03661341220140457\n",
      "Training loss for batch 216 : 0.11467007547616959\n",
      "Training loss for batch 217 : 0.055161722004413605\n",
      "Training loss for batch 218 : 0.0510234460234642\n",
      "Training loss for batch 219 : 0.27644869685173035\n",
      "Training loss for batch 220 : 0.3191489577293396\n",
      "Training loss for batch 221 : 0.14718160033226013\n",
      "Training loss for batch 222 : 0.20515567064285278\n",
      "Training loss for batch 223 : 0.054254550486803055\n",
      "Training loss for batch 224 : 0.22817093133926392\n",
      "Training loss for batch 225 : 0.21400795876979828\n",
      "Training loss for batch 226 : 0.017523571848869324\n",
      "Training loss for batch 227 : 0.10041867941617966\n",
      "Training loss for batch 228 : 0.23652273416519165\n",
      "Training loss for batch 229 : 0.11335210502147675\n",
      "Training loss for batch 230 : 0.08135220408439636\n",
      "Training loss for batch 231 : 0.09600240737199783\n",
      "Training loss for batch 232 : 0.07452321797609329\n",
      "Training loss for batch 233 : 0.14831258356571198\n",
      "Training loss for batch 234 : 0.14366373419761658\n",
      "Training loss for batch 235 : 0.10675417631864548\n",
      "Training loss for batch 236 : 0.1740996539592743\n",
      "Training loss for batch 237 : 0.040717869997024536\n",
      "Training loss for batch 238 : 0.08205092698335648\n",
      "Training loss for batch 239 : 0.28302037715911865\n",
      "Training loss for batch 240 : 0.007092642597854137\n",
      "Training loss for batch 241 : 0.3499511480331421\n",
      "Training loss for batch 242 : 0.03277587890625\n",
      "Training loss for batch 243 : 0.13751867413520813\n",
      "Training loss for batch 244 : 0.022275229915976524\n",
      "Training loss for batch 245 : 0.5391578078269958\n",
      "Training loss for batch 246 : 0.20623907446861267\n",
      "Training loss for batch 247 : 0.1362897902727127\n",
      "Training loss for batch 248 : 0.01854012906551361\n",
      "Training loss for batch 249 : 0.05703199282288551\n",
      "Training loss for batch 250 : 0.068216472864151\n",
      "Training loss for batch 251 : 0.1676664501428604\n",
      "Training loss for batch 252 : 0.23101265728473663\n",
      "Training loss for batch 253 : 0.24694086611270905\n",
      "Training loss for batch 254 : 0.2538304030895233\n",
      "Training loss for batch 255 : -0.001966380048543215\n",
      "Training loss for batch 256 : 0.20884543657302856\n",
      "Training loss for batch 257 : 0.2487020641565323\n",
      "Training loss for batch 258 : 0.05616597831249237\n",
      "Training loss for batch 259 : 0.21754026412963867\n",
      "Training loss for batch 260 : 0.11123181879520416\n",
      "Training loss for batch 261 : 0.008939321152865887\n",
      "Training loss for batch 262 : 0.0008187519852072\n",
      "Training loss for batch 263 : 0.07876786589622498\n",
      "Training loss for batch 264 : 0.19588153064250946\n",
      "Training loss for batch 265 : 0.03894249349832535\n",
      "Training loss for batch 266 : 0.09144977480173111\n",
      "Training loss for batch 267 : 0.10448906570672989\n",
      "Training loss for batch 268 : 0.07375344634056091\n",
      "Training loss for batch 269 : 0.02379055693745613\n",
      "Training loss for batch 270 : 0.0036270718555897474\n",
      "Training loss for batch 271 : 0.0411934070289135\n",
      "Training loss for batch 272 : 0.16817064583301544\n",
      "Training loss for batch 273 : 0.24260255694389343\n",
      "Training loss for batch 274 : 0.06870303303003311\n",
      "Training loss for batch 275 : 0.11416812986135483\n",
      "Training loss for batch 276 : 0.01588153839111328\n",
      "Training loss for batch 277 : 0.1406734734773636\n",
      "Training loss for batch 278 : 0.06661182641983032\n",
      "Training loss for batch 279 : 0.10621190071105957\n",
      "Training loss for batch 280 : 0.28333523869514465\n",
      "Training loss for batch 281 : 0.14718574285507202\n",
      "Training loss for batch 282 : 0.05244392901659012\n",
      "Training loss for batch 283 : 0.14995913207530975\n",
      "Training loss for batch 284 : 0.20451369881629944\n",
      "Training loss for batch 285 : 0.02781686931848526\n",
      "Training loss for batch 286 : 0.022782810032367706\n",
      "Training loss for batch 287 : 0.569148063659668\n",
      "Training loss for batch 288 : 0.11994870752096176\n",
      "Training loss for batch 289 : 0.1658971607685089\n",
      "Training loss for batch 290 : 0.11403001099824905\n",
      "Training loss for batch 291 : 0.20952613651752472\n",
      "Training loss for batch 292 : 0.09774908423423767\n",
      "Training loss for batch 293 : 0.12972572445869446\n",
      "Training loss for batch 294 : 0.07381252199411392\n",
      "Training loss for batch 295 : 0.035030465573072433\n",
      "Training loss for batch 296 : 0.13411085307598114\n",
      "Training loss for batch 297 : 0.0026599138509482145\n",
      "Training loss for batch 298 : 0.04034797102212906\n",
      "Training loss for batch 299 : 0.0212678462266922\n",
      "Training loss for batch 300 : 0.033252764493227005\n",
      "Training loss for batch 301 : 0.0\n",
      "Training loss for batch 302 : 0.03568091616034508\n",
      "Training loss for batch 303 : 0.16238848865032196\n",
      "Training loss for batch 304 : 0.10491316020488739\n",
      "Training loss for batch 305 : 0.058903153985738754\n",
      "Training loss for batch 306 : 0.3063013553619385\n",
      "Training loss for batch 307 : 0.3775829076766968\n",
      "Training loss for batch 308 : 0.0\n",
      "Training loss for batch 309 : 0.048420049250125885\n",
      "Training loss for batch 310 : 0.07097365707159042\n",
      "Training loss for batch 311 : 0.004311989061534405\n",
      "Training loss for batch 312 : 0.1325874924659729\n",
      "Training loss for batch 313 : 0.06315455585718155\n",
      "Training loss for batch 314 : 0.22386467456817627\n",
      "Training loss for batch 315 : 0.07148457318544388\n",
      "Training loss for batch 316 : 0.07041473686695099\n",
      "Training loss for batch 317 : 0.26814165711402893\n",
      "Training loss for batch 318 : 0.15469834208488464\n",
      "Training loss for batch 319 : 0.010434753261506557\n",
      "Training loss for batch 320 : 0.1180771067738533\n",
      "Training loss for batch 321 : 0.07884833216667175\n",
      "Training loss for batch 322 : 0.08671239018440247\n",
      "Training loss for batch 323 : 0.21276479959487915\n",
      "Training loss for batch 324 : 0.13593654334545135\n",
      "Training loss for batch 325 : 0.04756993055343628\n",
      "Training loss for batch 326 : 0.14616946876049042\n",
      "Training loss for batch 327 : 0.1898966133594513\n",
      "Training loss for batch 328 : 0.14165818691253662\n",
      "Training loss for batch 329 : 0.2374088317155838\n",
      "Training loss for batch 330 : 0.21933774650096893\n",
      "Training loss for batch 331 : 0.0\n",
      "Training loss for batch 332 : 0.11705396324396133\n",
      "Training loss for batch 333 : 0.05979510396718979\n",
      "Training loss for batch 334 : 0.24506314098834991\n",
      "Training loss for batch 335 : 0.1600990742444992\n",
      "Training loss for batch 336 : 0.16828130185604095\n",
      "Training loss for batch 337 : 0.001671711914241314\n",
      "Training loss for batch 338 : 0.0040559289045631886\n",
      "Training loss for batch 339 : 0.004863846581429243\n",
      "Training loss for batch 340 : -0.00023232423700392246\n",
      "Training loss for batch 341 : 0.030144404619932175\n",
      "Training loss for batch 342 : 0.12891636788845062\n",
      "Training loss for batch 343 : 0.1768644154071808\n",
      "Training loss for batch 344 : 0.11825717240571976\n",
      "Training loss for batch 345 : 0.0\n",
      "Training loss for batch 346 : 0.16611848771572113\n",
      "Training loss for batch 347 : 0.5198436379432678\n",
      "Training loss for batch 348 : 0.0822945237159729\n",
      "Training loss for batch 349 : 0.02327035740017891\n",
      "Training loss for batch 350 : 0.1893395036458969\n",
      "Training loss for batch 351 : 0.1288248896598816\n",
      "Training loss for batch 352 : 0.2570642828941345\n",
      "Training loss for batch 353 : 0.09410829097032547\n",
      "Training loss for batch 354 : 0.0013806507922708988\n",
      "Training loss for batch 355 : 0.5806124806404114\n",
      "Training loss for batch 356 : 0.10595336556434631\n",
      "Training loss for batch 357 : 0.0383324921131134\n",
      "Training loss for batch 358 : 0.3485814034938812\n",
      "Training loss for batch 359 : 0.0637705847620964\n",
      "Training loss for batch 360 : 0.28653788566589355\n",
      "Training loss for batch 361 : 0.031226126477122307\n",
      "Training loss for batch 362 : 0.04952853173017502\n",
      "Training loss for batch 363 : 0.12712571024894714\n",
      "Training loss for batch 364 : 0.0940914899110794\n",
      "Training loss for batch 365 : 0.26905369758605957\n",
      "Training loss for batch 366 : 0.07905050367116928\n",
      "Training loss for batch 367 : 0.030911007896065712\n",
      "Training loss for batch 368 : 0.05701610818505287\n",
      "Training loss for batch 369 : 0.2595236599445343\n",
      "Training loss for batch 370 : 0.02678324282169342\n",
      "Training loss for batch 371 : 0.10641614347696304\n",
      "Training loss for batch 372 : 0.24912361800670624\n",
      "Training loss for batch 373 : 0.030907869338989258\n",
      "Training loss for batch 374 : 0.005685816518962383\n",
      "Training loss for batch 375 : 0.2486753612756729\n",
      "Training loss for batch 376 : 0.13197726011276245\n",
      "Training loss for batch 377 : 0.026141639798879623\n",
      "Training loss for batch 378 : 0.059633295983076096\n",
      "Training loss for batch 379 : 0.2037552446126938\n",
      "Training loss for batch 380 : 0.0027069351635873318\n",
      "Training loss for batch 381 : 0.02006685361266136\n",
      "Training loss for batch 382 : 0.11533977091312408\n",
      "Training loss for batch 383 : 0.14591968059539795\n",
      "Training loss for batch 384 : 0.19603705406188965\n",
      "Training loss for batch 385 : 0.24667806923389435\n",
      "Training loss for batch 386 : 0.19624078273773193\n",
      "Training loss for batch 387 : 0.13594010472297668\n",
      "Training loss for batch 388 : 0.36626070737838745\n",
      "Training loss for batch 389 : 0.11732374876737595\n",
      "Training loss for batch 390 : 0.19063730537891388\n",
      "Training loss for batch 391 : 0.2459147721529007\n",
      "Training loss for batch 392 : 0.43614518642425537\n",
      "Training loss for batch 393 : 0.12230726331472397\n",
      "Training loss for batch 394 : 0.17358633875846863\n",
      "Training loss for batch 395 : 0.1102788969874382\n",
      "Training loss for batch 396 : 0.014639407396316528\n",
      "Training loss for batch 397 : 0.3641056716442108\n",
      "Training loss for batch 398 : 0.14392368495464325\n",
      "Training loss for batch 399 : 0.0\n",
      "Training loss for batch 400 : 0.15886245667934418\n",
      "Training loss for batch 401 : 0.4597685933113098\n",
      "Training loss for batch 402 : 0.07245040684938431\n",
      "Training loss for batch 403 : 0.048347264528274536\n",
      "Training loss for batch 404 : 0.12740132212638855\n",
      "Training loss for batch 405 : 0.17287679016590118\n",
      "Training loss for batch 406 : 0.13845019042491913\n",
      "Training loss for batch 407 : 0.0052269743755459785\n",
      "Training loss for batch 408 : 0.2308312952518463\n",
      "Training loss for batch 409 : 0.0871644839644432\n",
      "Training loss for batch 410 : 0.00048707559471949935\n",
      "Training loss for batch 411 : 0.2976383566856384\n",
      "Training loss for batch 412 : 0.0\n",
      "Training loss for batch 413 : 0.16522540152072906\n",
      "Training loss for batch 414 : 0.04458671808242798\n",
      "Training loss for batch 415 : 0.14909546077251434\n",
      "Training loss for batch 416 : 0.15181684494018555\n",
      "Training loss for batch 417 : 0.16878052055835724\n",
      "Training loss for batch 418 : 0.08698795735836029\n",
      "Training loss for batch 419 : 0.01439693570137024\n",
      "Training loss for batch 420 : 0.07686695456504822\n",
      "Training loss for batch 421 : 0.15203237533569336\n",
      "Training loss for batch 422 : 0.1264360100030899\n",
      "Training loss for batch 423 : 0.11387215554714203\n",
      "Training loss for batch 424 : 0.04459049925208092\n",
      "Training loss for batch 425 : 0.1280553936958313\n",
      "Training loss for batch 426 : 0.016551358625292778\n",
      "Training loss for batch 427 : 0.19320736825466156\n",
      "Training loss for batch 428 : 0.16558131575584412\n",
      "Training loss for batch 429 : 0.3304497003555298\n",
      "Training loss for batch 430 : 0.0061118267476558685\n",
      "Training loss for batch 431 : 0.0\n",
      "Training loss for batch 432 : 0.01206478476524353\n",
      "Training loss for batch 433 : 0.18239855766296387\n",
      "Training loss for batch 434 : 0.33231019973754883\n",
      "Training loss for batch 435 : 0.2153559923171997\n",
      "Training loss for batch 436 : 0.321249783039093\n",
      "Training loss for batch 437 : 0.3249824643135071\n",
      "Training loss for batch 438 : 0.06668635457754135\n",
      "Training loss for batch 439 : 0.14450877904891968\n",
      "Training loss for batch 440 : 0.10798227041959763\n",
      "Training loss for batch 441 : 0.1440381407737732\n",
      "Training loss for batch 442 : 0.13140623271465302\n",
      "Training loss for batch 443 : 0.13648618757724762\n",
      "Training loss for batch 444 : 0.22295378148555756\n",
      "Training loss for batch 445 : 0.1521831452846527\n",
      "Training loss for batch 446 : 0.13429030776023865\n",
      "Training loss for batch 447 : 0.28022268414497375\n",
      "Training loss for batch 448 : 0.055486880242824554\n",
      "Training loss for batch 449 : 0.03291760012507439\n",
      "Training loss for batch 450 : 0.07458530366420746\n",
      "Training loss for batch 451 : 0.16195815801620483\n",
      "Training loss for batch 452 : 0.13285815715789795\n",
      "Training loss for batch 453 : 0.11396755278110504\n",
      "Training loss for batch 454 : 0.08487211167812347\n",
      "Training loss for batch 455 : 0.2684614956378937\n",
      "Training loss for batch 456 : 0.18512599170207977\n",
      "Training loss for batch 457 : 0.15596163272857666\n",
      "Training loss for batch 458 : 0.0077321333810687065\n",
      "Training loss for batch 459 : 0.17743931710720062\n",
      "Training loss for batch 460 : 0.011331509798765182\n",
      "Training loss for batch 461 : 0.035336073487997055\n",
      "Training loss for batch 462 : 0.11567063629627228\n",
      "Training loss for batch 463 : 0.14127971231937408\n",
      "Training loss for batch 464 : 0.09782587736845016\n",
      "Training loss for batch 465 : 0.022375386208295822\n",
      "Training loss for batch 466 : 0.18762360513210297\n",
      "Training loss for batch 467 : 0.0708397924900055\n",
      "Training loss for batch 468 : 0.09165360778570175\n",
      "Training loss for batch 469 : 0.09525033831596375\n",
      "Training loss for batch 470 : 0.21554556488990784\n",
      "Training loss for batch 471 : 0.2007763385772705\n",
      "Training loss for batch 472 : 0.03879212960600853\n",
      "Training loss for batch 473 : 0.1114153116941452\n",
      "Training loss for batch 474 : 0.09906895458698273\n",
      "Training loss for batch 475 : 0.06838102638721466\n",
      "Training loss for batch 476 : 0.19539019465446472\n",
      "Training loss for batch 477 : 0.14514635503292084\n",
      "Training loss for batch 478 : 0.09459841251373291\n",
      "Training loss for batch 479 : 0.03900325670838356\n",
      "Training loss for batch 480 : 0.31308454275131226\n",
      "Training loss for batch 481 : 0.2501518130302429\n",
      "Training loss for batch 482 : 0.030828794464468956\n",
      "Training loss for batch 483 : 0.08813875913619995\n",
      "Training loss for batch 484 : 0.07469742745161057\n",
      "Training loss for batch 485 : 0.14656417071819305\n",
      "Training loss for batch 486 : 0.12712711095809937\n",
      "Training loss for batch 487 : 0.07327327132225037\n",
      "Training loss for batch 488 : 0.3430747091770172\n",
      "Training loss for batch 489 : 0.03766929730772972\n",
      "Training loss for batch 490 : 0.03304430469870567\n",
      "Training loss for batch 491 : 0.15775084495544434\n",
      "Training loss for batch 492 : 0.0025536715984344482\n",
      "Training loss for batch 493 : 0.2428402304649353\n",
      "Training loss for batch 494 : 0.0017875432968139648\n",
      "Training loss for batch 495 : 0.06332989037036896\n",
      "Training loss for batch 496 : 0.3756369650363922\n",
      "Training loss for batch 497 : 0.28012779355049133\n",
      "Training loss for batch 498 : 0.11859256774187088\n",
      "Training loss for batch 499 : 0.046928904950618744\n",
      "Training loss for batch 500 : 0.03903728723526001\n",
      "Training loss for batch 501 : 0.08964419364929199\n",
      "Training loss for batch 502 : 0.017331812530755997\n",
      "Training loss for batch 503 : 0.06855573505163193\n",
      "Training loss for batch 504 : 0.03298373520374298\n",
      "Training loss for batch 505 : 0.04890383780002594\n",
      "Training loss for batch 506 : 0.30789244174957275\n",
      "Training loss for batch 507 : -0.00010874528379645199\n",
      "Training loss for batch 508 : 0.05139416828751564\n",
      "Training loss for batch 509 : 0.02880670689046383\n",
      "Training loss for batch 510 : 0.13941897451877594\n",
      "Training loss for batch 511 : 0.005942881107330322\n",
      "Training loss for batch 512 : 0.10198485106229782\n",
      "Training loss for batch 513 : 0.0\n",
      "Training loss for batch 514 : 0.21509262919425964\n",
      "Training loss for batch 515 : 0.08781837671995163\n",
      "Training loss for batch 516 : 0.2942788302898407\n",
      "Training loss for batch 517 : 0.0395243838429451\n",
      "Training loss for batch 518 : 0.021778427064418793\n",
      "Training loss for batch 519 : 0.07463320344686508\n",
      "Training loss for batch 520 : 0.16907797753810883\n",
      "Training loss for batch 521 : 0.19479821622371674\n",
      "Training loss for batch 522 : 0.12406928837299347\n",
      "Training loss for batch 523 : 0.11818109452724457\n",
      "Training loss for batch 524 : 0.15181580185890198\n",
      "Training loss for batch 525 : 0.033573634922504425\n",
      "Training loss for batch 526 : 0.0616801418364048\n",
      "Training loss for batch 527 : 0.14856044948101044\n",
      "Training loss for batch 528 : 0.2630072236061096\n",
      "Training loss for batch 529 : 0.2286488115787506\n",
      "Training loss for batch 530 : 0.07924126833677292\n",
      "Training loss for batch 531 : 0.1730094999074936\n",
      "Training loss for batch 532 : 0.18102075159549713\n",
      "Training loss for batch 533 : 0.12610995769500732\n",
      "Training loss for batch 534 : 0.0\n",
      "Training loss for batch 535 : 0.2074345350265503\n",
      "Training loss for batch 536 : 0.0030448834877461195\n",
      "Training loss for batch 537 : 0.30686768889427185\n",
      "Training loss for batch 538 : 0.18203330039978027\n",
      "Training loss for batch 539 : 0.1410437524318695\n",
      "Training loss for batch 540 : 0.4180518388748169\n",
      "Training loss for batch 541 : 0.3802068829536438\n",
      "Training loss for batch 542 : 0.008947163820266724\n",
      "Training loss for batch 543 : 0.07048985362052917\n",
      "Training loss for batch 544 : 0.20670343935489655\n",
      "Training loss for batch 545 : 0.14351525902748108\n",
      "Training loss for batch 546 : 0.08941744267940521\n",
      "Training loss for batch 547 : 0.23527269065380096\n",
      "Training loss for batch 548 : 0.17727907001972198\n",
      "Training loss for batch 549 : 0.16838008165359497\n",
      "Training loss for batch 550 : 0.009029114618897438\n",
      "Training loss for batch 551 : 0.13655054569244385\n",
      "Training loss for batch 552 : 0.1501072198152542\n",
      "Training loss for batch 553 : 0.09243176132440567\n",
      "Training loss for batch 554 : 0.19601544737815857\n",
      "Training loss for batch 555 : 0.11492504924535751\n",
      "Training loss for batch 556 : 0.059945035725831985\n",
      "Training loss for batch 557 : 0.09856514632701874\n",
      "Training loss for batch 558 : 0.07113731652498245\n",
      "Training loss for batch 559 : 0.20778220891952515\n",
      "Training loss for batch 560 : -0.0023158653639256954\n",
      "Training loss for batch 561 : 0.18996840715408325\n",
      "Training loss for batch 562 : 0.34979158639907837\n",
      "Training loss for batch 563 : 0.12739640474319458\n",
      "Training loss for batch 564 : 0.08517365157604218\n",
      "Training loss for batch 565 : 0.014286180958151817\n",
      "Training loss for batch 566 : 0.1475892961025238\n",
      "Training loss for batch 567 : 0.1590975821018219\n",
      "Training loss for batch 568 : 0.031300753355026245\n",
      "Training loss for batch 569 : 0.04084588587284088\n",
      "Training loss for batch 570 : 0.10067661106586456\n",
      "Training loss for batch 571 : 0.00418679928407073\n",
      "Training loss for batch 572 : 0.03495063632726669\n",
      "Training loss for batch 573 : 0.07345381379127502\n",
      "Training loss for batch 574 : 0.31235960125923157\n",
      "Training loss for batch 575 : 0.004927800968289375\n",
      "Training loss for batch 576 : 0.1796666830778122\n",
      "Training loss for batch 577 : 0.2240738719701767\n",
      "Training loss for batch 578 : 0.08784298598766327\n",
      "Training loss for batch 579 : 0.10954924672842026\n",
      "Training loss for batch 580 : 0.07330932468175888\n",
      "Training loss for batch 581 : 0.0009440581197850406\n",
      "Training loss for batch 582 : 0.09338396787643433\n",
      "Training loss for batch 583 : 0.1822044849395752\n",
      "Training loss for batch 584 : 0.18759989738464355\n",
      "Training loss for batch 585 : 0.4952159523963928\n",
      "Training loss for batch 586 : 0.34641602635383606\n",
      "Training loss for batch 587 : 0.016402244567871094\n",
      "Training loss for batch 588 : 0.18698856234550476\n",
      "Training loss for batch 589 : 0.20731613039970398\n",
      "Training loss for batch 590 : 0.011897832155227661\n",
      "Training loss for batch 591 : 0.14131757616996765\n",
      "Training loss for batch 592 : 0.1242099180817604\n",
      "Training loss for batch 593 : 0.18349185585975647\n",
      "Training loss for batch 594 : 0.29957473278045654\n",
      "Training loss for batch 595 : 0.23300281167030334\n",
      "Training loss for batch 596 : 0.10067611187696457\n",
      "Training loss for batch 597 : 0.07837539911270142\n",
      "Training loss for batch 598 : 0.003959715366363525\n",
      "Training loss for batch 599 : 0.2519078850746155\n",
      "Training loss for batch 600 : 0.12078201025724411\n",
      "Training loss for batch 601 : 0.11912097781896591\n",
      "Training loss for batch 602 : 0.0005841156234964728\n",
      "Training loss for batch 603 : 0.127623051404953\n",
      "Training loss for batch 604 : 0.04631977155804634\n",
      "Training loss for batch 605 : -0.00019678384705912322\n",
      "Training loss for batch 606 : 0.14582332968711853\n",
      "Training loss for batch 607 : 0.1482004076242447\n",
      "Training loss for batch 608 : 0.24533621966838837\n",
      "Training loss for batch 609 : 0.18010056018829346\n",
      "Training loss for batch 610 : 0.21189218759536743\n",
      "Training loss for batch 611 : 0.010217210277915001\n",
      "Training loss for batch 612 : 0.0\n",
      "Training loss for batch 613 : 0.04547636955976486\n",
      "Training loss for batch 614 : 0.2628902196884155\n",
      "Training loss for batch 615 : 0.07261142134666443\n",
      "Training loss for batch 616 : 0.2706094980239868\n",
      "Training loss for batch 617 : 0.041584134101867676\n",
      "Training loss for batch 618 : 0.24838270246982574\n",
      "Training loss for batch 619 : 0.3143076002597809\n",
      "Training loss for batch 620 : 0.02080824039876461\n",
      "Training loss for batch 621 : 0.023210017010569572\n",
      "Training loss for batch 622 : 0.16329221427440643\n",
      "Training loss for batch 623 : 0.27780187129974365\n",
      "Training loss for batch 624 : 0.271549254655838\n",
      "Training loss for batch 625 : 0.2115696668624878\n",
      "Training loss for batch 626 : 0.1872740387916565\n",
      "Training loss for batch 627 : 0.008197605609893799\n",
      "Training loss for batch 628 : 0.015062637627124786\n",
      "Training loss for batch 629 : 0.1395578384399414\n",
      "Training loss for batch 630 : 0.006808427162468433\n",
      "Training loss for batch 631 : 0.22783783078193665\n",
      "Training loss for batch 632 : 0.10003086179494858\n",
      "Training loss for batch 633 : 0.04075933247804642\n",
      "Training loss for batch 634 : 0.1979949027299881\n",
      "Training loss for batch 635 : 0.2262272834777832\n",
      "Training loss for batch 636 : 0.0\n",
      "Training loss for batch 637 : 0.18841984868049622\n",
      "Training loss for batch 638 : 0.19445492327213287\n",
      "Training loss for batch 639 : 0.10290348529815674\n",
      "Training loss for batch 640 : 0.07003186643123627\n",
      "Training loss for batch 641 : 0.019612740725278854\n",
      "Training loss for batch 642 : 0.16999414563179016\n",
      "Training loss for batch 643 : 0.04760390520095825\n",
      "Training loss for batch 644 : 0.3094443678855896\n",
      "Training loss for batch 645 : 0.04584819823503494\n",
      "Training loss for batch 646 : 0.0031244605779647827\n",
      "Training loss for batch 647 : 0.13058489561080933\n",
      "Training loss for batch 648 : 0.1268402636051178\n",
      "Training loss for batch 649 : 0.046788156032562256\n",
      "Training loss for batch 650 : -0.0014132384676486254\n",
      "Training loss for batch 651 : 0.1065204069018364\n",
      "Training loss for batch 652 : 0.25364378094673157\n",
      "Training loss for batch 653 : 0.1744755208492279\n",
      "Training loss for batch 654 : 0.06961961090564728\n",
      "Training loss for batch 655 : 0.11969953775405884\n",
      "Training loss for batch 656 : 0.18930919468402863\n",
      "Training loss for batch 657 : 0.1232474148273468\n",
      "Training loss for batch 658 : 0.11689179390668869\n",
      "Training loss for batch 659 : 0.15819092094898224\n",
      "Training loss for batch 660 : 0.28926900029182434\n",
      "Training loss for batch 661 : 0.12392866611480713\n",
      "Training loss for batch 662 : 0.32420864701271057\n",
      "Training loss for batch 663 : 0.35947316884994507\n",
      "Training loss for batch 664 : 0.013174275867640972\n",
      "Training loss for batch 665 : 0.01858583092689514\n",
      "Training loss for batch 666 : 0.32515957951545715\n",
      "Training loss for batch 667 : 0.052376050502061844\n",
      "Training loss for batch 668 : 0.10779442638158798\n",
      "Training loss for batch 669 : 0.07235651463270187\n",
      "Training loss for batch 670 : 0.14142057299613953\n",
      "Training loss for batch 671 : 0.009238414466381073\n",
      "Training loss for batch 672 : 0.14380885660648346\n",
      "Training loss for batch 673 : 0.005592863075435162\n",
      "Training loss for batch 674 : 0.03257087245583534\n",
      "Training loss for batch 675 : 0.06869744509458542\n",
      "Training loss for batch 676 : 0.05943301320075989\n",
      "Training loss for batch 677 : 0.0\n",
      "Training loss for batch 678 : 0.09364636242389679\n",
      "Training loss for batch 679 : 0.034110177308321\n",
      "Training loss for batch 680 : 0.1830376535654068\n",
      "Training loss for batch 681 : 0.07108369469642639\n",
      "Training loss for batch 682 : 0.00012957057333551347\n",
      "Training loss for batch 683 : 0.2704339027404785\n",
      "Training loss for batch 684 : 0.13433445990085602\n",
      "Training loss for batch 685 : 0.2569679617881775\n",
      "Training loss for batch 686 : 0.09015300124883652\n",
      "Training loss for batch 687 : 0.08732911199331284\n",
      "Training loss for batch 688 : 0.16686241328716278\n",
      "Training loss for batch 689 : 0.0\n",
      "Training loss for batch 690 : 0.27970632910728455\n",
      "Training loss for batch 691 : 0.11095277220010757\n",
      "Training loss for batch 692 : 0.26673799753189087\n",
      "Training loss for batch 693 : 0.04978977143764496\n",
      "Training loss for batch 694 : 0.1287766844034195\n",
      "Training loss for batch 695 : 0.21999146044254303\n",
      "Training loss for batch 696 : 0.13716156780719757\n",
      "Training loss for batch 697 : 0.33417579531669617\n",
      "Training loss for batch 698 : 0.09023171663284302\n",
      "Training loss for batch 699 : 0.22301515936851501\n",
      "Training loss for batch 700 : 0.08972367644309998\n",
      "Training loss for batch 701 : 0.02992183342576027\n",
      "Training loss for batch 702 : 0.24767665565013885\n",
      "Training loss for batch 703 : 0.1321888417005539\n",
      "Training loss for batch 704 : 0.04951687157154083\n",
      "Training loss for batch 705 : 0.21018505096435547\n",
      "Training loss for batch 706 : 0.12948468327522278\n",
      "Training loss for batch 707 : 0.18659336864948273\n",
      "Training loss for batch 708 : 0.18050648272037506\n",
      "Training loss for batch 709 : 0.18296517431735992\n",
      "Training loss for batch 710 : 0.1951083391904831\n",
      "Training loss for batch 711 : 0.07003442943096161\n",
      "Training loss for batch 712 : 0.11883671581745148\n",
      "Training loss for batch 713 : 0.08226580172777176\n",
      "Training loss for batch 714 : 0.3234347701072693\n",
      "Training loss for batch 715 : 0.21744848787784576\n",
      "Training loss for batch 716 : 0.24967406690120697\n",
      "Training loss for batch 717 : 0.055615440011024475\n",
      "Training loss for batch 718 : 0.34433677792549133\n",
      "Training loss for batch 719 : 0.31254851818084717\n",
      "Training loss for batch 720 : 0.15420062839984894\n",
      "Training loss for batch 721 : 0.21557120978832245\n",
      "Training loss for batch 722 : 0.05911010131239891\n",
      "Training loss for batch 723 : 0.00922398827970028\n",
      "Training loss for batch 724 : 0.04499680921435356\n",
      "Training loss for batch 725 : 0.15096917748451233\n",
      "Training loss for batch 726 : 0.07645221799612045\n",
      "Training loss for batch 727 : 0.10426731407642365\n",
      "Training loss for batch 728 : 0.08263423293828964\n",
      "Training loss for batch 729 : 0.11436966061592102\n",
      "Training loss for batch 730 : 0.15146926045417786\n",
      "Training loss for batch 731 : 0.0\n",
      "Training loss for batch 732 : 0.12979868054389954\n",
      "Training loss for batch 733 : 0.16026952862739563\n",
      "Training loss for batch 734 : 0.0\n",
      "Training loss for batch 735 : 0.09185174107551575\n",
      "Training loss for batch 736 : 0.05647128447890282\n",
      "Training loss for batch 737 : 0.008654093369841576\n",
      "Training loss for batch 738 : 0.1163221076130867\n",
      "Training loss for batch 739 : 0.24099446833133698\n",
      "Training loss for batch 740 : 0.18260806798934937\n",
      "Training loss for batch 741 : 0.08722998946905136\n",
      "Training loss for batch 742 : 0.002323823282495141\n",
      "Training loss for batch 743 : 0.34112295508384705\n",
      "Training loss for batch 744 : 0.04327875003218651\n",
      "Training loss for batch 745 : 0.18170350790023804\n",
      "Training loss for batch 746 : 0.060402385890483856\n",
      "Training loss for batch 747 : 0.07445750385522842\n",
      "Training loss for batch 748 : 0.11394131928682327\n",
      "Training loss for batch 749 : 0.28692159056663513\n",
      "Training loss for batch 750 : 0.1221068948507309\n",
      "Training loss for batch 751 : 0.02305295318365097\n",
      "Training loss for batch 752 : 0.11155615746974945\n",
      "Training loss for batch 753 : 0.06209792569279671\n",
      "Training loss for batch 754 : 0.24576738476753235\n",
      "Training loss for batch 755 : 0.043783701956272125\n",
      "Training loss for batch 756 : 0.06199328601360321\n",
      "Training loss for batch 757 : 0.33653977513313293\n",
      "Training loss for batch 758 : 0.8257941007614136\n",
      "Training loss for batch 759 : 0.04972795024514198\n",
      "Training loss for batch 760 : 0.10041207075119019\n",
      "Training loss for batch 761 : 0.29472649097442627\n",
      "Training loss for batch 762 : 0.3848446011543274\n",
      "Training loss for batch 763 : 0.02800440788269043\n",
      "Training loss for batch 764 : 0.10417498648166656\n",
      "Training loss for batch 765 : 0.030280515551567078\n",
      "Training loss for batch 766 : 0.08795693516731262\n",
      "Training loss for batch 767 : 0.0\n",
      "Training loss for batch 768 : 0.11374526470899582\n",
      "Training loss for batch 769 : 0.04831229895353317\n",
      "Training loss for batch 770 : 0.20466624200344086\n",
      "Training loss for batch 771 : 0.09388494491577148\n",
      "Training loss for batch 772 : 0.03673291206359863\n",
      "Training loss for batch 773 : -0.0010751932859420776\n",
      "Training loss for batch 774 : 0.07238461077213287\n",
      "Training loss for batch 775 : 0.31022754311561584\n",
      "Training loss for batch 776 : 0.30622124671936035\n",
      "Training loss for batch 777 : 0.04058205336332321\n",
      "Training loss for batch 778 : 0.04383518919348717\n",
      "Training loss for batch 779 : 0.05133813992142677\n",
      "Training loss for batch 780 : 0.08288827538490295\n",
      "Training loss for batch 781 : 0.04439114034175873\n",
      "Training loss for batch 782 : 0.11346355080604553\n",
      "Training loss for batch 783 : 0.10984384268522263\n",
      "Training loss for batch 784 : 0.0005797644844278693\n",
      "Training loss for batch 785 : 0.3142158091068268\n",
      "Training loss for batch 786 : 0.21121090650558472\n",
      "Training loss for batch 787 : 0.3002702593803406\n",
      "Training loss for batch 788 : 0.13499554991722107\n",
      "Training loss for batch 789 : 0.4050609767436981\n",
      "Training loss for batch 790 : 0.008401374332606792\n",
      "Training loss for batch 791 : 0.03634157031774521\n",
      "Training loss for batch 792 : 0.03849588334560394\n",
      "Training loss for batch 793 : 0.07064928859472275\n",
      "Training loss for batch 794 : 0.13313508033752441\n",
      "Training loss for batch 795 : 0.001450981479138136\n",
      "Training loss for batch 796 : 0.18581317365169525\n",
      "Training loss for batch 797 : 0.00342584983445704\n",
      "Training loss for batch 798 : 0.04365778714418411\n",
      "Training loss for batch 799 : 0.03827633708715439\n",
      "Training loss for batch 800 : 0.19132231175899506\n",
      "Training loss for batch 801 : 0.14746294915676117\n",
      "Training loss for batch 802 : 0.02680608630180359\n",
      "Training loss for batch 803 : 0.10482010990381241\n",
      "Training loss for batch 804 : 0.12253722548484802\n",
      "Training loss for batch 805 : 0.13503599166870117\n",
      "Training loss for batch 806 : 0.05920926854014397\n",
      "Training loss for batch 807 : 0.04995008558034897\n",
      "Training loss for batch 808 : 0.06897792965173721\n",
      "Training loss for batch 809 : 0.19664369523525238\n",
      "Training loss for batch 810 : 0.3265203535556793\n",
      "Training loss for batch 811 : 0.2517607510089874\n",
      "Training loss for batch 812 : 0.22426635026931763\n",
      "Training loss for batch 813 : 0.034077852964401245\n",
      "Training loss for batch 814 : 0.29555317759513855\n",
      "Training loss for batch 815 : 0.30672889947891235\n",
      "Training loss for batch 816 : 0.08618749678134918\n",
      "Training loss for batch 817 : 0.09619021415710449\n",
      "Training loss for batch 818 : 0.12295755743980408\n",
      "Training loss for batch 819 : 0.045451436191797256\n",
      "Training loss for batch 820 : 0.1118851751089096\n",
      "Training loss for batch 821 : 0.0874670147895813\n",
      "Training loss for batch 822 : 0.3245430290699005\n",
      "Training loss for batch 823 : 0.13609346747398376\n",
      "Training loss for batch 824 : 0.021224189549684525\n",
      "Training loss for batch 825 : 0.07726640999317169\n",
      "Training loss for batch 826 : 0.2560341954231262\n",
      "Training loss for batch 827 : 0.05912655219435692\n",
      "Training loss for batch 828 : 0.403270959854126\n",
      "Training loss for batch 829 : 0.30439841747283936\n",
      "Training loss for batch 830 : 0.09870311617851257\n",
      "Training loss for batch 831 : 0.15064725279808044\n",
      "Training loss for batch 832 : 0.03401657193899155\n",
      "Training loss for batch 833 : 0.21895872056484222\n",
      "Training loss for batch 834 : 0.06055017560720444\n",
      "Training loss for batch 835 : 0.08143873512744904\n",
      "Training loss for batch 836 : 0.014915361069142818\n",
      "Training loss for batch 837 : 0.36160770058631897\n",
      "Training loss for batch 838 : 0.13792259991168976\n",
      "Training loss for batch 839 : 0.321390300989151\n",
      "Training loss for batch 840 : 0.07933655381202698\n",
      "Training loss for batch 841 : 0.3030671179294586\n",
      "Training loss for batch 842 : 0.18269449472427368\n",
      "Training loss for batch 843 : 0.2643008530139923\n",
      "Training loss for batch 844 : 0.06427787244319916\n",
      "Training loss for batch 845 : 0.031455978751182556\n",
      "Training loss for batch 846 : 0.18095333874225616\n",
      "Training loss for batch 847 : 0.3640114367008209\n",
      "Training loss for batch 848 : 0.03379218652844429\n",
      "Training loss for batch 849 : 0.20924806594848633\n",
      "Training loss for batch 850 : 0.052890658378601074\n",
      "Training loss for batch 851 : 0.06092394143342972\n",
      "Training loss for batch 852 : 0.027940182015299797\n",
      "Training loss for batch 853 : 0.1091713160276413\n",
      "Training loss for batch 854 : 0.14232318103313446\n",
      "Training loss for batch 855 : 0.04710403084754944\n",
      "Training loss for batch 856 : 0.17829428613185883\n",
      "Training loss for batch 857 : 0.2397095412015915\n",
      "Training loss for batch 858 : 0.0932997465133667\n",
      "Training loss for batch 859 : 0.3624865412712097\n",
      "Training loss for batch 860 : 0.07900311797857285\n",
      "Training loss for batch 861 : 0.04055686295032501\n",
      "Training loss for batch 862 : 0.37615713477134705\n",
      "Training loss for batch 863 : 0.020810674875974655\n",
      "Training loss for batch 864 : 0.393267959356308\n",
      "Training loss for batch 865 : 0.30618637800216675\n",
      "Training loss for batch 866 : 0.023277131840586662\n",
      "Training loss for batch 867 : 0.10813269019126892\n",
      "Training loss for batch 868 : 0.0929812639951706\n",
      "Training loss for batch 869 : 0.24664348363876343\n",
      "Training loss for batch 870 : 0.07563639432191849\n",
      "Training loss for batch 871 : 0.034449171274900436\n",
      "Training loss for batch 872 : 0.012268761172890663\n",
      "Training loss for batch 873 : 0.24159826338291168\n",
      "Training loss for batch 874 : 0.6562811136245728\n",
      "Training loss for batch 875 : 0.10814094543457031\n",
      "Training loss for batch 876 : 0.0765327662229538\n",
      "Training loss for batch 877 : 0.1582430601119995\n",
      "Training loss for batch 878 : 0.1229582354426384\n",
      "Training loss for batch 879 : 0.12183959782123566\n",
      "Training loss for batch 880 : 0.019529936835169792\n",
      "Training loss for batch 881 : 0.019489910453557968\n",
      "Training loss for batch 882 : 0.19886229932308197\n",
      "Training loss for batch 883 : 0.08198845386505127\n",
      "Training loss for batch 884 : 0.11365252733230591\n",
      "Training loss for batch 885 : 0.005785195156931877\n",
      "Training loss for batch 886 : 0.14271977543830872\n",
      "Training loss for batch 887 : 0.11476460099220276\n",
      "Training loss for batch 888 : 0.019390475004911423\n",
      "Training loss for batch 889 : 0.18317776918411255\n",
      "Training loss for batch 890 : 0.15869829058647156\n",
      "Training loss for batch 891 : 0.1275482475757599\n",
      "Training loss for batch 892 : 0.20248126983642578\n",
      "Training loss for batch 893 : 0.24649867415428162\n",
      "Training loss for batch 894 : 0.09582863748073578\n",
      "Training loss for batch 895 : 0.10584405809640884\n",
      "Training loss for batch 896 : 0.17871630191802979\n",
      "Training loss for batch 897 : 0.0995018482208252\n",
      "Training loss for batch 898 : 0.17196907103061676\n",
      "Training loss for batch 899 : 0.3200436234474182\n",
      "Training loss for batch 900 : 0.13145168125629425\n",
      "Training loss for batch 901 : 0.28034207224845886\n",
      "Training loss for batch 902 : 0.12848475575447083\n",
      "Training loss for batch 903 : 0.003403604030609131\n",
      "Training loss for batch 904 : 0.09217297285795212\n",
      "Training loss for batch 905 : 0.15974943339824677\n",
      "Training loss for batch 906 : 0.29336223006248474\n",
      "Training loss for batch 907 : 0.06572342664003372\n",
      "Training loss for batch 908 : 0.12274163961410522\n",
      "Training loss for batch 909 : 0.06159456819295883\n",
      "Training loss for batch 910 : 0.0316535048186779\n",
      "Training loss for batch 911 : 0.19228288531303406\n",
      "Training loss for batch 912 : 0.19487707316875458\n",
      "Training loss for batch 913 : 0.0400439016520977\n",
      "Training loss for batch 914 : 0.1317104697227478\n",
      "Training loss for batch 915 : 0.1354864239692688\n",
      "Training loss for batch 916 : 0.004527360200881958\n",
      "Training loss for batch 917 : 0.02168293483555317\n",
      "Training loss for batch 918 : 0.22193491458892822\n",
      "Training loss for batch 919 : 0.06789112836122513\n",
      "Training loss for batch 920 : 0.10244865715503693\n",
      "Training loss for batch 921 : 0.317859411239624\n",
      "Training loss for batch 922 : 0.05352875590324402\n",
      "Training loss for batch 923 : 0.24437987804412842\n",
      "Training loss for batch 924 : 0.06364797055721283\n",
      "Training loss for batch 925 : 0.07637646794319153\n",
      "Training loss for batch 926 : 0.1595468670129776\n",
      "Training loss for batch 927 : 0.0072390674613416195\n",
      "Training loss for batch 928 : 0.14535771310329437\n",
      "Training loss for batch 929 : 0.3078477084636688\n",
      "Training loss for batch 930 : 0.401178777217865\n",
      "Training loss for batch 931 : 0.1457122266292572\n",
      "Training loss for batch 932 : 0.43628108501434326\n",
      "Training loss for batch 933 : 0.32010334730148315\n",
      "Training loss for batch 934 : 0.1167600229382515\n",
      "Training loss for batch 935 : 0.2836610972881317\n",
      "Training loss for batch 936 : 0.13860920071601868\n",
      "Training loss for batch 937 : 0.15268480777740479\n",
      "Training loss for batch 938 : 0.2010452151298523\n",
      "Training loss for batch 939 : 0.05027033016085625\n",
      "Training loss for batch 940 : 0.01310001965612173\n",
      "Training loss for batch 941 : 0.0003861685690935701\n",
      "Training loss for batch 942 : 0.2739660441875458\n",
      "Training loss for batch 943 : 0.219840407371521\n",
      "Training loss for batch 944 : 0.32874420285224915\n",
      "Training loss for batch 945 : 0.29592326283454895\n",
      "Training loss for batch 946 : 0.16897325217723846\n",
      "Training loss for batch 947 : 0.26724353432655334\n",
      "Training loss for batch 948 : 0.17504559457302094\n",
      "Training loss for batch 949 : 0.23879338800907135\n",
      "Training loss for batch 950 : 0.11368945240974426\n",
      "Training loss for batch 951 : 0.22730059921741486\n",
      "Training loss for batch 952 : 0.017717543989419937\n",
      "Training loss for batch 953 : 0.24088934063911438\n",
      "Training loss for batch 954 : 0.04055047035217285\n",
      "Training loss for batch 955 : 0.09666010737419128\n",
      "Training loss for batch 956 : 0.17284329235553741\n",
      "Training loss for batch 957 : 0.029415976256132126\n",
      "Training loss for batch 958 : 0.09096567332744598\n",
      "Training loss for batch 959 : 0.053860023617744446\n",
      "Training loss for batch 960 : 0.019792821258306503\n",
      "Training loss for batch 961 : 0.44340431690216064\n",
      "Training loss for batch 962 : 0.13543565571308136\n",
      "Training loss for batch 963 : 0.04899322986602783\n",
      "Training loss for batch 964 : 0.018389681354165077\n",
      "Training loss for batch 965 : 0.24639789760112762\n",
      "Training loss for batch 966 : 0.12031619995832443\n",
      "Training loss for batch 967 : 0.005986737087368965\n",
      "Training loss for batch 968 : 0.2082177698612213\n",
      "Training loss for batch 969 : 0.10781974345445633\n",
      "Training loss for batch 970 : 0.12212418019771576\n",
      "Training loss for batch 971 : 0.08270764350891113\n",
      "Training loss for batch 972 : 0.2518426179885864\n",
      "Training loss for batch 973 : 0.10015421360731125\n",
      "Training loss for batch 974 : 0.08673973381519318\n",
      "Training loss for batch 975 : 0.06335507333278656\n",
      "Training loss for batch 976 : 0.40398216247558594\n",
      "Training loss for batch 977 : 0.10253503173589706\n",
      "Training loss for batch 978 : 0.23347920179367065\n",
      "Training loss for batch 979 : 0.18268440663814545\n",
      "Training loss for batch 980 : 0.1563463807106018\n",
      "Training loss for batch 981 : 0.07293238490819931\n",
      "Training loss for batch 982 : 0.14401203393936157\n",
      "Training loss for batch 983 : 0.23902490735054016\n",
      "Training loss for batch 984 : 0.14865610003471375\n",
      "Training loss for batch 985 : 0.16678902506828308\n",
      "Training loss for batch 986 : 0.2752876579761505\n",
      "Training loss for batch 987 : 0.1693241000175476\n",
      "Training loss for batch 988 : 0.02541286125779152\n",
      "Training loss for batch 989 : 0.052131157368421555\n",
      "Training loss for batch 990 : 0.20373564958572388\n",
      "Training loss for batch 991 : 0.39740779995918274\n",
      "Training loss for batch 992 : 0.1300516277551651\n",
      "Training loss for batch 993 : 0.34004518389701843\n",
      "Training loss for batch 994 : 0.1370791643857956\n",
      "Training loss for batch 995 : 0.024347703903913498\n",
      "Training loss for batch 996 : 0.19435760378837585\n",
      "Training loss for batch 997 : 0.17104153335094452\n",
      "Training loss for batch 998 : 0.1080898866057396\n",
      "Training loss for batch 999 : 0.04396623373031616\n",
      "Training loss for batch 1000 : 0.1614713966846466\n",
      "Training loss for batch 1001 : 0.12195449322462082\n",
      "Training loss for batch 1002 : 0.09798102080821991\n",
      "Training loss for batch 1003 : 0.13434331119060516\n",
      "Training loss for batch 1004 : 0.04453585669398308\n",
      "Training loss for batch 1005 : 0.027967415750026703\n",
      "Training loss for batch 1006 : 0.056649334728717804\n",
      "Training loss for batch 1007 : 0.04552623629570007\n",
      "Training loss for batch 1008 : 0.2494279146194458\n",
      "Training loss for batch 1009 : 0.10397199541330338\n",
      "Training loss for batch 1010 : 0.028220951557159424\n",
      "Training loss for batch 1011 : 0.12782149016857147\n",
      "Training loss for batch 1012 : 0.04657150059938431\n",
      "Training loss for batch 1013 : 0.22309154272079468\n",
      "Training loss for batch 1014 : 0.016698917374014854\n",
      "Training loss for batch 1015 : 0.09831026196479797\n",
      "Training loss for batch 1016 : 0.0022573235910385847\n",
      "Training loss for batch 1017 : 0.04770677164196968\n",
      "Training loss for batch 1018 : 0.250636488199234\n",
      "Training loss for batch 1019 : 0.09690739214420319\n",
      "Training loss for batch 1020 : 0.016017358750104904\n",
      "Training loss for batch 1021 : 0.0443723089993\n",
      "Training loss for batch 1022 : 0.19486775994300842\n",
      "Training loss for batch 1023 : 0.05697432905435562\n",
      "Training loss for batch 1024 : 0.3368927240371704\n",
      "Training loss for batch 1025 : 0.07030300796031952\n",
      "Training loss for batch 1026 : 0.04717005789279938\n",
      "Training loss for batch 1027 : 0.19801323115825653\n",
      "Training loss for batch 1028 : 0.11238159984350204\n",
      "Training loss for batch 1029 : 0.032417457550764084\n",
      "Training loss for batch 1030 : 0.05928995832800865\n",
      "Training loss for batch 1031 : 0.07121776789426804\n",
      "Training loss for batch 1032 : 0.24513602256774902\n",
      "Training loss for batch 1033 : 0.2563166916370392\n",
      "Training loss for batch 1034 : 0.1954079270362854\n",
      "Training loss for batch 1035 : 0.11237075924873352\n",
      "Training loss for batch 1036 : 0.22638198733329773\n",
      "Training loss for batch 1037 : 0.28186795115470886\n",
      "Training loss for batch 1038 : 0.035398341715335846\n",
      "Training loss for batch 1039 : 0.1412397027015686\n",
      "Training loss for batch 1040 : 0.12098503857851028\n",
      "Training loss for batch 1041 : 0.13127875328063965\n",
      "Training loss for batch 1042 : 0.10399450361728668\n",
      "Training loss for batch 1043 : 0.3051469624042511\n",
      "Training loss for batch 1044 : 0.07790002971887589\n",
      "Training loss for batch 1045 : 0.1524588167667389\n",
      "Training loss for batch 1046 : 0.3971787095069885\n",
      "Training loss for batch 1047 : 0.16471806168556213\n",
      "Training loss for batch 1048 : 0.02645730972290039\n",
      "Training loss for batch 1049 : 0.45417898893356323\n",
      "Training loss for batch 1050 : 0.021920830011367798\n",
      "Training loss for batch 1051 : 0.0093888184055686\n",
      "Training loss for batch 1052 : 0.2613999843597412\n",
      "Training loss for batch 1053 : 0.13954316079616547\n",
      "Training loss for batch 1054 : 0.04247628524899483\n",
      "Training loss for batch 1055 : 0.2105545699596405\n",
      "Training loss for batch 1056 : 0.21734754741191864\n",
      "Training loss for batch 1057 : 0.24072591960430145\n",
      "Training loss for batch 1058 : 0.05515299364924431\n",
      "Training loss for batch 1059 : 0.1880485713481903\n",
      "Training loss for batch 1060 : 0.006382376421242952\n",
      "Training loss for batch 1061 : 0.1992339789867401\n",
      "Training loss for batch 1062 : 0.04827554523944855\n",
      "Training loss for batch 1063 : 0.11503224074840546\n",
      "Training loss for batch 1064 : 0.21447555720806122\n",
      "Training loss for batch 1065 : 0.13642489910125732\n",
      "Training loss for batch 1066 : 0.3232037425041199\n",
      "Training loss for batch 1067 : 0.26781365275382996\n",
      "Training loss for batch 1068 : 0.14067067205905914\n",
      "Training loss for batch 1069 : 0.2128010392189026\n",
      "Training loss for batch 1070 : 0.08283846825361252\n",
      "Training loss for batch 1071 : 0.19919390976428986\n",
      "Training loss for batch 1072 : 0.3703491687774658\n",
      "Training loss for batch 1073 : 0.3762720823287964\n",
      "Training loss for batch 1074 : 0.15366889536380768\n",
      "Training loss for batch 1075 : 0.06040879338979721\n",
      "Training loss for batch 1076 : 0.12977956235408783\n",
      "Training loss for batch 1077 : 0.1916288584470749\n",
      "Training loss for batch 1078 : 0.2358626276254654\n",
      "Training loss for batch 1079 : 0.028489287942647934\n",
      "Training loss for batch 1080 : 0.052259065210819244\n",
      "Training loss for batch 1081 : 0.19237932562828064\n",
      "Training loss for batch 1082 : 0.1252177506685257\n",
      "Training loss for batch 1083 : 0.24084419012069702\n",
      "Training loss for batch 1084 : 0.2136620581150055\n",
      "Training loss for batch 1085 : 0.3641223907470703\n",
      "Training loss for batch 1086 : 0.15178732573986053\n",
      "Training loss for batch 1087 : 0.15118123590946198\n",
      "Training loss for batch 1088 : 0.24768498539924622\n",
      "Training loss for batch 1089 : 0.10023503005504608\n",
      "Training loss for batch 1090 : 0.040726374834775925\n",
      "Training loss for batch 1091 : 0.07613468170166016\n",
      "Training loss for batch 1092 : 0.021630030125379562\n",
      "Training loss for batch 1093 : 0.021224234253168106\n",
      "Training loss for batch 1094 : 0.2022889256477356\n",
      "Training loss for batch 1095 : 0.6723713278770447\n",
      "Training loss for batch 1096 : 0.05315611883997917\n",
      "Training loss for batch 1097 : 0.06263115257024765\n",
      "Training loss for batch 1098 : 0.28956565260887146\n",
      "Training loss for batch 1099 : 0.1395033746957779\n",
      "Training loss for batch 1100 : 0.10751446336507797\n",
      "Training loss for batch 1101 : 0.3235394358634949\n",
      "Training loss for batch 1102 : 0.0988481193780899\n",
      "Training loss for batch 1103 : 0.1885533183813095\n",
      "Training loss for batch 1104 : 0.16853776574134827\n",
      "Training loss for batch 1105 : 0.21322721242904663\n",
      "Training loss for batch 1106 : 0.0840086042881012\n",
      "Training loss for batch 1107 : 0.004469084553420544\n",
      "Training loss for batch 1108 : 0.14575664699077606\n",
      "Training loss for batch 1109 : 0.24995312094688416\n",
      "Training loss for batch 1110 : 0.10855426639318466\n",
      "Training loss for batch 1111 : 0.163784921169281\n",
      "Training loss for batch 1112 : 0.1265876442193985\n",
      "Training loss for batch 1113 : 0.028028953820466995\n",
      "Training loss for batch 1114 : 0.20665094256401062\n",
      "Training loss for batch 1115 : 0.14868858456611633\n",
      "Training loss for batch 1116 : 0.004376890137791634\n",
      "Training loss for batch 1117 : 0.014580514281988144\n",
      "Training loss for batch 1118 : 0.282539427280426\n",
      "Training loss for batch 1119 : 0.06924829632043839\n",
      "Training loss for batch 1120 : 0.12684519588947296\n",
      "Training loss for batch 1121 : 0.2155282348394394\n",
      "Training loss for batch 1122 : 0.046181272715330124\n",
      "Training loss for batch 1123 : 0.07478415220975876\n",
      "Training loss for batch 1124 : 0.001939625944942236\n",
      "Training loss for batch 1125 : 0.1634809523820877\n",
      "Training loss for batch 1126 : 0.27831870317459106\n",
      "Training loss for batch 1127 : 0.014002040028572083\n",
      "Training loss for batch 1128 : 0.09182576090097427\n",
      "Training loss for batch 1129 : 0.02696944773197174\n",
      "Training loss for batch 1130 : 0.2293090522289276\n",
      "Training loss for batch 1131 : 0.08904799818992615\n",
      "Training loss for batch 1132 : 0.148076131939888\n",
      "Training loss for batch 1133 : 0.1649796962738037\n",
      "Training loss for batch 1134 : 0.1194099634885788\n",
      "Training loss for batch 1135 : 0.09214319288730621\n",
      "Training loss for batch 1136 : 0.37605541944503784\n",
      "Training loss for batch 1137 : 0.048522282391786575\n",
      "Training loss for batch 1138 : 0.053842104971408844\n",
      "Training loss for batch 1139 : 0.1353835016489029\n",
      "Training loss for batch 1140 : 0.32183128595352173\n",
      "Training loss for batch 1141 : 0.010682296007871628\n",
      "Training loss for batch 1142 : 0.13318559527397156\n",
      "Training loss for batch 1143 : 0.05452509596943855\n",
      "Training loss for batch 1144 : 0.02168160304427147\n",
      "Training loss for batch 1145 : 0.0012150477850809693\n",
      "Training loss for batch 1146 : 0.036496132612228394\n",
      "Training loss for batch 1147 : 0.05609345808625221\n",
      "Training loss for batch 1148 : 0.0868130475282669\n",
      "Training loss for batch 1149 : 0.053923528641462326\n",
      "Training loss for batch 1150 : 0.14227986335754395\n",
      "Training loss for batch 1151 : 0.003203501459211111\n",
      "Training loss for batch 1152 : 0.0544903539121151\n",
      "Training loss for batch 1153 : 0.04454050958156586\n",
      "Training loss for batch 1154 : 0.30657243728637695\n",
      "Training loss for batch 1155 : 0.33557748794555664\n",
      "Training loss for batch 1156 : 0.09169185161590576\n",
      "Training loss for batch 1157 : 0.0380890779197216\n",
      "Training loss for batch 1158 : 0.11743806302547455\n",
      "Training loss for batch 1159 : 0.08891584724187851\n",
      "Training loss for batch 1160 : 0.015790900215506554\n",
      "Training loss for batch 1161 : 0.15147623419761658\n",
      "Training loss for batch 1162 : 0.25441214442253113\n",
      "Training loss for batch 1163 : 0.05824675410985947\n",
      "Training loss for batch 1164 : 0.21522343158721924\n",
      "Training loss for batch 1165 : 0.14730116724967957\n",
      "Training loss for batch 1166 : 0.04605363681912422\n",
      "Training loss for batch 1167 : 0.021413134410977364\n",
      "Training loss for batch 1168 : 0.25000855326652527\n",
      "Training loss for batch 1169 : 0.12919479608535767\n",
      "Training loss for batch 1170 : 0.07525837421417236\n",
      "Training loss for batch 1171 : 0.16211317479610443\n",
      "Training loss for batch 1172 : 0.3797323703765869\n",
      "Training loss for batch 1173 : 0.19482608139514923\n",
      "Training loss for batch 1174 : 0.09488137066364288\n",
      "Training loss for batch 1175 : -3.271282184869051e-05\n",
      "Training loss for batch 1176 : 0.2254415899515152\n",
      "Training loss for batch 1177 : 0.21116074919700623\n",
      "Training loss for batch 1178 : 0.1390492469072342\n",
      "Training loss for batch 1179 : 0.09883219003677368\n",
      "Training loss for batch 1180 : 0.04763893410563469\n",
      "Training loss for batch 1181 : 0.05245303362607956\n",
      "Training loss for batch 1182 : 0.32259607315063477\n",
      "Training loss for batch 1183 : 0.17070160806179047\n",
      "Training loss for batch 1184 : 0.21405062079429626\n",
      "Training loss for batch 1185 : 0.10930485278367996\n",
      "Training loss for batch 1186 : 0.1273319572210312\n",
      "Training loss for batch 1187 : 0.08887605369091034\n",
      "Training loss for batch 1188 : 0.13793160021305084\n",
      "Training loss for batch 1189 : 0.043746188282966614\n",
      "Training loss for batch 1190 : 0.18257667124271393\n",
      "Training loss for batch 1191 : 0.16819220781326294\n",
      "Training loss for batch 1192 : 0.029033292084932327\n",
      "Training loss for batch 1193 : 0.12415973842144012\n",
      "Training loss for batch 1194 : 0.11733078211545944\n",
      "Training loss for batch 1195 : 0.2402644008398056\n",
      "Training loss for batch 1196 : 0.07451089471578598\n",
      "Training loss for batch 1197 : 0.3034471273422241\n",
      "Training loss for batch 1198 : 0.06090700626373291\n",
      "Training loss for batch 1199 : 0.40124350786209106\n",
      "Training loss for batch 1200 : 0.16271986067295074\n",
      "Training loss for batch 1201 : 0.27483558654785156\n",
      "Training loss for batch 1202 : 0.35191264748573303\n",
      "Training loss for batch 1203 : 0.16382858157157898\n",
      "Training loss for batch 1204 : 0.23207205533981323\n",
      "Training loss for batch 1205 : 0.017868705093860626\n",
      "Training loss for batch 1206 : 0.7211264967918396\n",
      "Training loss for batch 1207 : 0.0785369798541069\n",
      "Training loss for batch 1208 : 0.08769764006137848\n",
      "Training loss for batch 1209 : 0.134477436542511\n",
      "Training loss for batch 1210 : 0.057964012026786804\n",
      "Training loss for batch 1211 : 0.012049641460180283\n",
      "Training loss for batch 1212 : 0.2214900404214859\n",
      "Training loss for batch 1213 : 0.14913007616996765\n",
      "Training loss for batch 1214 : 0.10121221840381622\n",
      "Training loss for batch 1215 : 0.0009605884552001953\n",
      "Training loss for batch 1216 : 0.1719883382320404\n",
      "Training loss for batch 1217 : 0.04510398209095001\n",
      "Training loss for batch 1218 : 0.04874004051089287\n",
      "Training loss for batch 1219 : 0.03852317854762077\n",
      "Training loss for batch 1220 : 0.10899166017770767\n",
      "Training loss for batch 1221 : 0.10119140893220901\n",
      "Training loss for batch 1222 : 0.0\n",
      "Training loss for batch 1223 : 0.06969371438026428\n",
      "Training loss for batch 1224 : 0.0478060282766819\n",
      "Training loss for batch 1225 : 0.2038109302520752\n",
      "Training loss for batch 1226 : 0.061424620449543\n",
      "Training loss for batch 1227 : 0.04985161870718002\n",
      "Training loss for batch 1228 : 0.2954476773738861\n",
      "Training loss for batch 1229 : 0.04331234097480774\n",
      "Training loss for batch 1230 : 0.1773039549589157\n",
      "Training loss for batch 1231 : 0.2588759958744049\n",
      "Training loss for batch 1232 : 0.03591519594192505\n",
      "Training loss for batch 1233 : 0.06005766987800598\n",
      "Training loss for batch 1234 : 0.11960604041814804\n",
      "Training loss for batch 1235 : 0.25359153747558594\n",
      "Training loss for batch 1236 : 0.10261955112218857\n",
      "Training loss for batch 1237 : 0.255256712436676\n",
      "Training loss for batch 1238 : 0.17201855778694153\n",
      "Training loss for batch 1239 : 0.06936046481132507\n",
      "Training loss for batch 1240 : 0.35003867745399475\n",
      "Training loss for batch 1241 : 0.12506410479545593\n",
      "Training loss for batch 1242 : 0.09214767813682556\n",
      "Training loss for batch 1243 : 0.3085949420928955\n",
      "Training loss for batch 1244 : 0.19793032109737396\n",
      "Training loss for batch 1245 : 0.052468523383140564\n",
      "Training loss for batch 1246 : 0.10790036618709564\n",
      "Training loss for batch 1247 : 0.20271341502666473\n",
      "Training loss for batch 1248 : 0.011457731947302818\n",
      "Training loss for batch 1249 : 0.24643053114414215\n",
      "Training loss for batch 1250 : 0.03666716068983078\n",
      "Training loss for batch 1251 : 0.2034289538860321\n",
      "Training loss for batch 1252 : 0.18589991331100464\n",
      "Training loss for batch 1253 : 0.07342040538787842\n",
      "Training loss for batch 1254 : 0.2523933947086334\n",
      "Training loss for batch 1255 : 0.07458999752998352\n",
      "Training loss for batch 1256 : 0.1084502711892128\n",
      "Training loss for batch 1257 : 0.1177486702799797\n",
      "Training loss for batch 1258 : 0.3338356912136078\n",
      "Training loss for batch 1259 : 0.05348958447575569\n",
      "Training loss for batch 1260 : 0.381064236164093\n",
      "Training loss for batch 1261 : 0.08314385265111923\n",
      "Training loss for batch 1262 : 0.2621614933013916\n",
      "Training loss for batch 1263 : 0.1212097704410553\n",
      "Training loss for batch 1264 : 0.06297659873962402\n",
      "Training loss for batch 1265 : 0.0601738765835762\n",
      "Training loss for batch 1266 : 0.010240010917186737\n",
      "Training loss for batch 1267 : 0.22825391590595245\n",
      "Training loss for batch 1268 : 0.05101697891950607\n",
      "Training loss for batch 1269 : 0.13151811063289642\n",
      "Training loss for batch 1270 : 0.21988339722156525\n",
      "Training loss for batch 1271 : 0.1835586428642273\n",
      "Training loss for batch 1272 : 0.20926007628440857\n",
      "Training loss for batch 1273 : 0.18914374709129333\n",
      "Training loss for batch 1274 : 0.08026836812496185\n",
      "Training loss for batch 1275 : 0.2570752501487732\n",
      "Training loss for batch 1276 : 0.27275973558425903\n",
      "Training loss for batch 1277 : 0.1128852516412735\n",
      "Training loss for batch 1278 : 0.08451025187969208\n",
      "Training loss for batch 1279 : 0.03770647197961807\n",
      "Training loss for batch 1280 : 0.16508133709430695\n",
      "Training loss for batch 1281 : 0.16327159106731415\n",
      "Training loss for batch 1282 : 0.11306656897068024\n",
      "Training loss for batch 1283 : 0.11095432937145233\n",
      "Training loss for batch 1284 : 0.1225651428103447\n",
      "Training loss for batch 1285 : 0.09040150046348572\n",
      "Training loss for batch 1286 : 0.20808163285255432\n",
      "Training loss for batch 1287 : 0.2821912467479706\n",
      "Training loss for batch 1288 : 0.11376205831766129\n",
      "Training loss for batch 1289 : 0.1322476863861084\n",
      "Training loss for batch 1290 : 0.1662435233592987\n",
      "Training loss for batch 1291 : 0.112645223736763\n",
      "Training loss for batch 1292 : 0.08055444061756134\n",
      "Training loss for batch 1293 : 0.02601766586303711\n",
      "Training loss for batch 1294 : 0.09066534787416458\n",
      "Training loss for batch 1295 : 0.034506749361753464\n",
      "Training loss for batch 1296 : 0.05393850430846214\n",
      "Training loss for batch 1297 : 0.0024794391356408596\n",
      "Training loss for batch 1298 : 0.1706286519765854\n",
      "Training loss for batch 1299 : 0.32872599363327026\n",
      "Training loss for batch 1300 : 0.09209051728248596\n",
      "Training loss for batch 1301 : 0.40527865290641785\n",
      "Training loss for batch 1302 : 0.024791883304715157\n",
      "Training loss for batch 1303 : 0.31323373317718506\n",
      "Training loss for batch 1304 : 0.09514649957418442\n",
      "Training loss for batch 1305 : 0.09266871213912964\n",
      "Training loss for batch 1306 : 0.2579532265663147\n",
      "Training loss for batch 1307 : 0.052858635783195496\n",
      "Training loss for batch 1308 : 0.07523755729198456\n",
      "Training loss for batch 1309 : 0.006209592334926128\n",
      "Training loss for batch 1310 : 0.133443683385849\n",
      "Training loss for batch 1311 : 0.3223886489868164\n",
      "Training loss for batch 1312 : 0.07335597276687622\n",
      "Training loss for batch 1313 : 0.09067119657993317\n",
      "Training loss for batch 1314 : 0.024680064991116524\n",
      "Training loss for batch 1315 : 0.16915182769298553\n",
      "Training loss for batch 1316 : 0.10015664249658585\n",
      "Training loss for batch 1317 : 0.11306874454021454\n",
      "Training loss for batch 1318 : 0.11214665323495865\n",
      "Training loss for batch 1319 : 0.14209173619747162\n",
      "Training loss for batch 1320 : 0.1653675138950348\n",
      "Training loss for batch 1321 : 0.12554065883159637\n",
      "Training loss for batch 1322 : 0.11205262690782547\n",
      "Training loss for batch 1323 : 0.10446186363697052\n",
      "Training loss for batch 1324 : 0.18443690240383148\n",
      "Training loss for batch 1325 : 0.03195198252797127\n",
      "Training loss for batch 1326 : 0.12208901345729828\n",
      "Training loss for batch 1327 : 0.05623893812298775\n",
      "Training loss for batch 1328 : 0.09284140169620514\n",
      "Training loss for batch 1329 : 0.4656386375427246\n",
      "Training loss for batch 1330 : 0.1812727153301239\n",
      "Training loss for batch 1331 : 0.1095162108540535\n",
      "Training loss for batch 1332 : 0.10410809516906738\n",
      "Training loss for batch 1333 : 0.13657625019550323\n",
      "Training loss for batch 1334 : 0.07539642602205276\n",
      "Training loss for batch 1335 : 0.07862360030412674\n",
      "Training loss for batch 1336 : 0.0\n",
      "Training loss for batch 1337 : 0.02609512023627758\n",
      "Training loss for batch 1338 : 0.08144722133874893\n",
      "Training loss for batch 1339 : 0.027754873037338257\n",
      "Training loss for batch 1340 : 0.062432192265987396\n",
      "Training loss for batch 1341 : 0.11087312549352646\n",
      "Training loss for batch 1342 : 0.012894472107291222\n",
      "Training loss for batch 1343 : 0.18145352602005005\n",
      "Training loss for batch 1344 : 0.05667520686984062\n",
      "Training loss for batch 1345 : 0.1421017199754715\n",
      "Training loss for batch 1346 : 0.037165869027376175\n",
      "Training loss for batch 1347 : 0.23405244946479797\n",
      "Training loss for batch 1348 : 0.12170905619859695\n",
      "Training loss for batch 1349 : 0.21026058495044708\n",
      "Training loss for batch 1350 : 0.14771263301372528\n",
      "Training loss for batch 1351 : 0.18789266049861908\n",
      "Training loss for batch 1352 : 0.12973973155021667\n",
      "Training loss for batch 1353 : 0.21721188724040985\n",
      "Training loss for batch 1354 : 0.4855024218559265\n",
      "Training loss for batch 1355 : 0.11498915404081345\n",
      "Training loss for batch 1356 : 0.04173527657985687\n",
      "Training loss for batch 1357 : 0.13020716607570648\n",
      "Training loss for batch 1358 : 0.12026908248662949\n",
      "Training loss for batch 1359 : 0.08536527305841446\n",
      "Training loss for batch 1360 : 0.05296429991722107\n",
      "Training loss for batch 1361 : 0.1839616894721985\n",
      "Training loss for batch 1362 : 0.39749616384506226\n",
      "Training loss for batch 1363 : 0.23013560473918915\n",
      "Training loss for batch 1364 : 0.06034271419048309\n",
      "Training loss for batch 1365 : 0.12226740270853043\n",
      "Training loss for batch 1366 : 0.034965742379426956\n",
      "Training loss for batch 1367 : 0.0003170569834765047\n",
      "Training loss for batch 1368 : 0.06330409646034241\n",
      "Training loss for batch 1369 : 0.2971721291542053\n",
      "Training loss for batch 1370 : 0.46462422609329224\n",
      "Training loss for batch 1371 : 0.013310616835951805\n",
      "Training loss for batch 1372 : 0.1051713079214096\n",
      "Training loss for batch 1373 : 0.17776140570640564\n",
      "Training loss for batch 1374 : 0.36201247572898865\n",
      "Training loss for batch 1375 : 0.1506454050540924\n",
      "Training loss for batch 1376 : 0.35519838333129883\n",
      "Training loss for batch 1377 : 0.0020763296633958817\n",
      "Training loss for batch 1378 : 0.5299131274223328\n",
      "Training loss for batch 1379 : 0.2867208421230316\n",
      "Training loss for batch 1380 : 0.0\n",
      "Training loss for batch 1381 : 0.07598038017749786\n",
      "Training loss for batch 1382 : 0.06190512329339981\n",
      "Training loss for batch 1383 : 0.1617230474948883\n",
      "Training loss for batch 1384 : 0.08796478062868118\n",
      "Training loss for batch 1385 : 0.3017587661743164\n",
      "Training loss for batch 1386 : 0.14707660675048828\n",
      "Training loss for batch 1387 : 0.2690347135066986\n",
      "Training loss for batch 1388 : 0.015605481341481209\n",
      "Training loss for batch 1389 : 0.2345937043428421\n",
      "Training loss for batch 1390 : 0.0768367275595665\n",
      "Training loss for batch 1391 : 0.10501498728990555\n",
      "Training loss for batch 1392 : 0.046870309859514236\n",
      "Training loss for batch 1393 : 0.14121463894844055\n",
      "Training loss for batch 1394 : 0.2334061861038208\n",
      "Training loss for batch 1395 : 0.0\n",
      "Training loss for batch 1396 : 0.07049738615751266\n",
      "Training loss for batch 1397 : 0.26714903116226196\n",
      "Training loss for batch 1398 : 0.1929583102464676\n",
      "Training loss for batch 1399 : 0.012787153013050556\n",
      "Training loss for batch 1400 : 0.0845446065068245\n",
      "Training loss for batch 1401 : 0.23570923507213593\n",
      "Training loss for batch 1402 : 0.28601592779159546\n",
      "Training loss for batch 1403 : 0.06357777863740921\n",
      "Training loss for batch 1404 : 0.012952921912074089\n",
      "Training loss for batch 1405 : 0.35602816939353943\n",
      "Training loss for batch 1406 : 0.18425361812114716\n",
      "Training loss for batch 1407 : 0.3262024223804474\n",
      "Training loss for batch 1408 : 0.2228611409664154\n",
      "Training loss for batch 1409 : 0.14411723613739014\n",
      "Training loss for batch 1410 : 0.24530035257339478\n",
      "Training loss for batch 1411 : 0.02667882665991783\n",
      "Training loss for batch 1412 : 0.1500507891178131\n",
      "Training loss for batch 1413 : 0.08974164724349976\n",
      "Training loss for batch 1414 : 0.0767023041844368\n",
      "Training loss for batch 1415 : 0.19900378584861755\n",
      "Training loss for batch 1416 : 0.2559753656387329\n",
      "Training loss for batch 1417 : 0.004823477938771248\n",
      "Training loss for batch 1418 : 0.016899507492780685\n",
      "Training loss for batch 1419 : 0.09047859907150269\n",
      "Training loss for batch 1420 : 0.0835651159286499\n",
      "Training loss for batch 1421 : 0.10512934625148773\n",
      "Training loss for batch 1422 : 0.2231048345565796\n",
      "Training loss for batch 1423 : 0.2608252465724945\n",
      "Training loss for batch 1424 : 0.0793011337518692\n",
      "Training loss for batch 1425 : 0.1218014732003212\n",
      "Training loss for batch 1426 : 0.2469581514596939\n",
      "Training loss for batch 1427 : 0.10392250865697861\n",
      "Training loss for batch 1428 : 0.11522111296653748\n",
      "Training loss for batch 1429 : 0.16620299220085144\n",
      "Training loss for batch 1430 : 0.11624347418546677\n",
      "Training loss for batch 1431 : 0.0638766661286354\n",
      "Training loss for batch 1432 : 0.05849986895918846\n",
      "Training loss for batch 1433 : 0.1859073042869568\n",
      "Training loss for batch 1434 : 0.07629449665546417\n",
      "Training loss for batch 1435 : 0.039487916976213455\n",
      "Training loss for batch 1436 : 0.03270186483860016\n",
      "Training loss for batch 1437 : 0.1449112892150879\n",
      "Training loss for batch 1438 : 0.2635555863380432\n",
      "Training loss for batch 1439 : 0.014152748510241508\n",
      "Training loss for batch 1440 : 0.19958285987377167\n",
      "Training loss for batch 1441 : 0.044368114322423935\n",
      "Training loss for batch 1442 : 0.09034234285354614\n",
      "Training loss for batch 1443 : 0.14229081571102142\n",
      "Training loss for batch 1444 : 0.0719679743051529\n",
      "Training loss for batch 1445 : 0.037837401032447815\n",
      "Training loss for batch 1446 : 0.025343148037791252\n",
      "Training loss for batch 1447 : 0.024676058441400528\n",
      "Training loss for batch 1448 : 0.13523107767105103\n",
      "Training loss for batch 1449 : 0.0009825329761952162\n",
      "Training loss for batch 1450 : 0.034359484910964966\n",
      "Training loss for batch 1451 : 0.07766693085432053\n",
      "Training loss for batch 1452 : 0.06142817437648773\n",
      "Training loss for batch 1453 : 0.07131984829902649\n",
      "Training loss for batch 1454 : 0.0927414521574974\n",
      "Training loss for batch 1455 : 0.18876899778842926\n",
      "Training loss for batch 1456 : 0.20429594814777374\n",
      "Training loss for batch 1457 : 0.07691682130098343\n",
      "Training loss for batch 1458 : 0.2180263102054596\n",
      "Training loss for batch 1459 : 0.2720624804496765\n",
      "Training loss for batch 1460 : 0.2611794173717499\n",
      "Training loss for batch 1461 : 0.041307687759399414\n",
      "Training loss for batch 1462 : 0.03337598964571953\n",
      "Training loss for batch 1463 : 0.19922880828380585\n",
      "Training loss for batch 1464 : 0.10800176858901978\n",
      "Training loss for batch 1465 : 0.011885413900017738\n",
      "Training loss for batch 1466 : 0.15966498851776123\n",
      "Training loss for batch 1467 : 0.0\n",
      "Training loss for batch 1468 : 0.09167585521936417\n",
      "Training loss for batch 1469 : 0.1132584810256958\n",
      "Training loss for batch 1470 : 0.3148737847805023\n",
      "Training loss for batch 1471 : 0.10482523590326309\n",
      "Training loss for batch 1472 : 0.13223010301589966\n",
      "Training loss for batch 1473 : 0.15725666284561157\n",
      "Training loss for batch 1474 : 0.2814690172672272\n",
      "Training loss for batch 1475 : 0.007342584431171417\n",
      "Training loss for batch 1476 : 0.13947562873363495\n",
      "Training loss for batch 1477 : 0.08184151351451874\n",
      "Training loss for batch 1478 : 0.29361748695373535\n",
      "Training loss for batch 1479 : 0.2259492129087448\n",
      "Training loss for batch 1480 : 0.28606081008911133\n",
      "Training loss for batch 1481 : 0.05961116775870323\n",
      "Training loss for batch 1482 : 0.0425080880522728\n",
      "Training loss for batch 1483 : 0.07481276988983154\n",
      "Training loss for batch 1484 : 0.14368319511413574\n",
      "Training loss for batch 1485 : 0.2133745402097702\n",
      "Training loss for batch 1486 : 0.06760212033987045\n",
      "Training loss for batch 1487 : 0.049603987485170364\n",
      "Training loss for batch 1488 : 0.0592888779938221\n",
      "Training loss for batch 1489 : 0.016074638813734055\n",
      "Training loss for batch 1490 : 0.1298557072877884\n",
      "Training loss for batch 1491 : 0.15772250294685364\n",
      "Training loss for batch 1492 : 0.14667680859565735\n",
      "Training loss for batch 1493 : 0.005166556220501661\n",
      "Training loss for batch 1494 : 0.09648413956165314\n",
      "Training loss for batch 1495 : 0.3116791844367981\n",
      "Training loss for batch 1496 : 0.04270391911268234\n",
      "Training loss for batch 1497 : 0.435579776763916\n",
      "Training loss for batch 1498 : 0.3720138669013977\n",
      "Training loss for batch 1499 : 0.10481275618076324\n",
      "Training loss for batch 1500 : 0.07874695956707001\n",
      "Training loss for batch 1501 : 0.18640290200710297\n",
      "Training loss for batch 1502 : 0.24698695540428162\n",
      "Training loss for batch 1503 : 0.05545874312520027\n",
      "Training loss for batch 1504 : 0.06944002211093903\n",
      "Training loss for batch 1505 : 0.02621975541114807\n",
      "Training loss for batch 1506 : 0.6281827688217163\n",
      "Training loss for batch 1507 : 0.03326903283596039\n",
      "Training loss for batch 1508 : 0.01751520484685898\n",
      "Training loss for batch 1509 : 0.17604254186153412\n",
      "Training loss for batch 1510 : 0.23553155362606049\n",
      "Training loss for batch 1511 : 0.12671518325805664\n",
      "Training loss for batch 1512 : 0.2019953429698944\n",
      "Training loss for batch 1513 : 0.026967346668243408\n",
      "Training loss for batch 1514 : 0.4532472789287567\n",
      "Training loss for batch 1515 : 0.03456241264939308\n",
      "Training loss for batch 1516 : 0.13942235708236694\n",
      "Training loss for batch 1517 : 0.13430136442184448\n",
      "Training loss for batch 1518 : 0.179259791970253\n",
      "Training loss for batch 1519 : 0.04065203666687012\n",
      "Training loss for batch 1520 : 0.05960974469780922\n",
      "Training loss for batch 1521 : 0.1497027575969696\n",
      "Training loss for batch 1522 : 0.25866958498954773\n",
      "Training loss for batch 1523 : 0.27729207277297974\n",
      "Training loss for batch 1524 : 0.03587880730628967\n",
      "Training loss for batch 1525 : 0.08422324061393738\n",
      "Training loss for batch 1526 : 0.3044359087944031\n",
      "Training loss for batch 1527 : 0.13808393478393555\n",
      "Training loss for batch 1528 : 0.04261999577283859\n",
      "Training loss for batch 1529 : 0.030683856457471848\n",
      "Training loss for batch 1530 : 0.26170802116394043\n",
      "Training loss for batch 1531 : 0.028895292431116104\n",
      "Training loss for batch 1532 : 0.04788713902235031\n",
      "Training loss for batch 1533 : 0.043269749730825424\n",
      "Training loss for batch 1534 : 0.17361074686050415\n",
      "Training loss for batch 1535 : 0.07597576826810837\n",
      "Training loss for batch 1536 : 0.2526768445968628\n",
      "Training loss for batch 1537 : 0.18293966352939606\n",
      "Training loss for batch 1538 : 0.2190144956111908\n",
      "Training loss for batch 1539 : 0.1332351565361023\n",
      "Training loss for batch 1540 : 0.21916674077510834\n",
      "Training loss for batch 1541 : 0.17752639949321747\n",
      "Training loss for batch 1542 : 0.11323778331279755\n",
      "Training loss for batch 1543 : 0.10090572386980057\n",
      "Training loss for batch 1544 : 0.0162658728659153\n",
      "Training loss for batch 1545 : 0.026425451040267944\n",
      "Training loss for batch 1546 : 0.10632935166358948\n",
      "Training loss for batch 1547 : 0.14725054800510406\n",
      "Training loss for batch 1548 : 0.04524554684758186\n",
      "Training loss for batch 1549 : 0.047623973339796066\n",
      "Training loss for batch 1550 : 0.06803622096776962\n",
      "Training loss for batch 1551 : 0.19051434099674225\n",
      "Training loss for batch 1552 : 0.06846895813941956\n",
      "Training loss for batch 1553 : 0.3502956032752991\n",
      "Training loss for batch 1554 : 0.0799524188041687\n",
      "Training loss for batch 1555 : 0.2429443746805191\n",
      "Training loss for batch 1556 : 0.04175551235675812\n",
      "Training loss for batch 1557 : 0.09001558274030685\n",
      "Training loss for batch 1558 : 0.038908202201128006\n",
      "Training loss for batch 1559 : 0.05923696979880333\n",
      "Training loss for batch 1560 : 0.021879345178604126\n",
      "Training loss for batch 1561 : 0.04897937923669815\n",
      "Training loss for batch 1562 : 0.30977052450180054\n",
      "Training loss for batch 1563 : 0.3020201623439789\n",
      "Training loss for batch 1564 : 0.22598449885845184\n",
      "Training loss for batch 1565 : 0.2663056254386902\n",
      "Training loss for batch 1566 : 0.18940171599388123\n",
      "Training loss for batch 1567 : 0.07594878226518631\n",
      "Training loss for batch 1568 : 0.061265841126441956\n",
      "Training loss for batch 1569 : 0.2675122320652008\n",
      "Training loss for batch 1570 : 0.10160050541162491\n",
      "Training loss for batch 1571 : 0.11863607168197632\n",
      "Training loss for batch 1572 : 0.057788748294115067\n",
      "Training loss for batch 1573 : 0.12166441977024078\n",
      "Training loss for batch 1574 : 0.4524347186088562\n",
      "Training loss for batch 1575 : 0.13591022789478302\n",
      "Training loss for batch 1576 : 0.11477451026439667\n",
      "Training loss for batch 1577 : 0.21637719869613647\n",
      "Training loss for batch 1578 : 0.09045243263244629\n",
      "Training loss for batch 1579 : 0.09772975742816925\n",
      "Training loss for batch 1580 : 0.17534343898296356\n",
      "Training loss for batch 1581 : 0.14855548739433289\n",
      "Training loss for batch 1582 : 0.3097946345806122\n",
      "Training loss for batch 1583 : 0.12255791574716568\n",
      "Training loss for batch 1584 : 0.030288606882095337\n",
      "Training loss for batch 1585 : 0.04700050875544548\n",
      "Training loss for batch 1586 : 0.03902861103415489\n",
      "Training loss for batch 1587 : 0.29775992035865784\n",
      "Training loss for batch 1588 : 0.06100139394402504\n",
      "Training loss for batch 1589 : 0.15263773500919342\n",
      "Training loss for batch 1590 : 0.04535955190658569\n",
      "Training loss for batch 1591 : 0.11713466793298721\n",
      "Training loss for batch 1592 : 0.2106747180223465\n",
      "Training loss for batch 1593 : 0.08472023904323578\n",
      "Training loss for batch 1594 : 0.14564169943332672\n",
      "Training loss for batch 1595 : 0.05565650388598442\n",
      "Training loss for batch 1596 : 0.12453296780586243\n",
      "Training loss for batch 1597 : 0.2447219043970108\n",
      "Training loss for batch 1598 : 0.0\n",
      "Training loss for batch 1599 : 0.05406844615936279\n",
      "Training loss for batch 1600 : 0.22598601877689362\n",
      "Training loss for batch 1601 : 0.20535755157470703\n",
      "Training loss for batch 1602 : 0.01748308539390564\n",
      "Training loss for batch 1603 : 0.07171430438756943\n",
      "Training loss for batch 1604 : 0.29259389638900757\n",
      "Training loss for batch 1605 : 0.08126108348369598\n",
      "Training loss for batch 1606 : 0.16888366639614105\n",
      "Training loss for batch 1607 : 0.018900228664278984\n",
      "Training loss for batch 1608 : 0.08123477548360825\n",
      "Training loss for batch 1609 : 0.023779597133398056\n",
      "Training loss for batch 1610 : 0.006860857829451561\n",
      "Training loss for batch 1611 : 0.07781945914030075\n",
      "Training loss for batch 1612 : 0.10464496910572052\n",
      "Training loss for batch 1613 : 0.15170246362686157\n",
      "Training loss for batch 1614 : 0.15562425553798676\n",
      "Training loss for batch 1615 : 0.38003695011138916\n",
      "Training loss for batch 1616 : 0.2089066356420517\n",
      "Training loss for batch 1617 : 0.014941162429749966\n",
      "Training loss for batch 1618 : 0.010115080513060093\n",
      "Training loss for batch 1619 : 0.21722882986068726\n",
      "Training loss for batch 1620 : 0.015991780906915665\n",
      "Training loss for batch 1621 : 0.2590844929218292\n",
      "Training loss for batch 1622 : 0.16272425651550293\n",
      "Training loss for batch 1623 : 0.1875268518924713\n",
      "Training loss for batch 1624 : 0.4061622619628906\n",
      "Training loss for batch 1625 : 0.02418641373515129\n",
      "Training loss for batch 1626 : 0.05926147475838661\n",
      "Training loss for batch 1627 : 0.07107441872358322\n",
      "Training loss for batch 1628 : 0.03113662265241146\n",
      "Training loss for batch 1629 : 0.059594329446554184\n",
      "Training loss for batch 1630 : 0.13609987497329712\n",
      "Training loss for batch 1631 : 0.1100253015756607\n",
      "Training loss for batch 1632 : 0.18463529646396637\n",
      "Training loss for batch 1633 : 0.10193973779678345\n",
      "Training loss for batch 1634 : 0.2082277238368988\n",
      "Training loss for batch 1635 : 0.0243479423224926\n",
      "Training loss for batch 1636 : 0.13557349145412445\n",
      "Training loss for batch 1637 : 0.03715583309531212\n",
      "Training loss for batch 1638 : 0.015690362080931664\n",
      "Training loss for batch 1639 : 0.019740499556064606\n",
      "Training loss for batch 1640 : 0.0\n",
      "Training loss for batch 1641 : 0.025524504482746124\n",
      "Training loss for batch 1642 : 0.025557145476341248\n",
      "Training loss for batch 1643 : 0.010669173672795296\n",
      "Training loss for batch 1644 : 0.06553438305854797\n",
      "Training loss for batch 1645 : 0.052470188587903976\n",
      "Training loss for batch 1646 : 0.3215894103050232\n",
      "Training loss for batch 1647 : 0.1866232007741928\n",
      "Training loss for batch 1648 : 0.054052382707595825\n",
      "Training loss for batch 1649 : 0.1838400661945343\n",
      "Training loss for batch 1650 : 0.1869540512561798\n",
      "Training loss for batch 1651 : 0.2471770942211151\n",
      "Training loss for batch 1652 : 0.009968778118491173\n",
      "Training loss for batch 1653 : 0.31294649839401245\n",
      "Training loss for batch 1654 : 0.16059400141239166\n",
      "Training loss for batch 1655 : 0.07493828982114792\n",
      "Training loss for batch 1656 : 0.15476574003696442\n",
      "Training loss for batch 1657 : 0.07810457795858383\n",
      "Training loss for batch 1658 : 0.06073163449764252\n",
      "Training loss for batch 1659 : 0.15335650742053986\n",
      "Training loss for batch 1660 : 0.03946200758218765\n",
      "Training loss for batch 1661 : 0.02615487575531006\n",
      "Training loss for batch 1662 : 0.16520945727825165\n",
      "Training loss for batch 1663 : 0.022564588114619255\n",
      "Training loss for batch 1664 : 0.0900559052824974\n",
      "Training loss for batch 1665 : 0.016784563660621643\n",
      "Training loss for batch 1666 : 0.16134604811668396\n",
      "Training loss for batch 1667 : 0.06665278226137161\n",
      "Training loss for batch 1668 : 0.3532845377922058\n",
      "Training loss for batch 1669 : 0.19295963644981384\n",
      "Training loss for batch 1670 : 0.23769643902778625\n",
      "Training loss for batch 1671 : 0.10699988156557083\n",
      "Training loss for batch 1672 : 0.10174641013145447\n",
      "Training loss for batch 1673 : 0.04185273498296738\n",
      "Training loss for batch 1674 : 0.06401681900024414\n",
      "Training loss for batch 1675 : 0.034012846648693085\n",
      "Training loss for batch 1676 : 0.07065044343471527\n",
      "Training loss for batch 1677 : 0.01877395063638687\n",
      "Training loss for batch 1678 : 0.15186241269111633\n",
      "Training loss for batch 1679 : 0.046648602932691574\n",
      "Training loss for batch 1680 : 0.12908759713172913\n",
      "Training loss for batch 1681 : 0.038517020642757416\n",
      "Training loss for batch 1682 : 0.15548653900623322\n",
      "Training loss for batch 1683 : 0.20828953385353088\n",
      "Training loss for batch 1684 : 0.08824597299098969\n",
      "Training loss for batch 1685 : 0.0352909080684185\n",
      "Training loss for batch 1686 : 0.22711578011512756\n",
      "Training loss for batch 1687 : 0.011029993183910847\n",
      "Training loss for batch 1688 : 0.23711413145065308\n",
      "Training loss for batch 1689 : 0.025171760469675064\n",
      "Training loss for batch 1690 : 0.08708804845809937\n",
      "Training loss for batch 1691 : 0.12703125178813934\n",
      "Training loss for batch 1692 : 0.40138188004493713\n",
      "Training loss for batch 1693 : 0.08316327631473541\n",
      "Training loss for batch 1694 : 0.17178787291049957\n",
      "Training loss for batch 1695 : 5.2342813432915136e-05\n",
      "Training loss for batch 1696 : 0.051791220903396606\n",
      "Training loss for batch 1697 : 0.1036982387304306\n",
      "Training loss for batch 1698 : 0.432453453540802\n",
      "Training loss for batch 1699 : 0.20889736711978912\n",
      "Training loss for batch 1700 : 0.0356307290494442\n",
      "Training loss for batch 1701 : 0.2990470230579376\n",
      "Training loss for batch 1702 : 0.13325822353363037\n",
      "Training loss for batch 1703 : 0.01772698573768139\n",
      "Training loss for batch 1704 : 0.09688594937324524\n",
      "Training loss for batch 1705 : 0.7719807624816895\n",
      "Training loss for batch 1706 : 0.11689286679029465\n",
      "Training loss for batch 1707 : 0.20658141374588013\n",
      "Training loss for batch 1708 : 0.05849383398890495\n",
      "Training loss for batch 1709 : 0.27241250872612\n",
      "Training loss for batch 1710 : 0.22470855712890625\n",
      "Training loss for batch 1711 : 0.07560216635465622\n",
      "Training loss for batch 1712 : 0.11850791424512863\n",
      "Training loss for batch 1713 : 0.11791400611400604\n",
      "Training loss for batch 1714 : 0.06774646043777466\n",
      "Training loss for batch 1715 : 0.0633353441953659\n",
      "Training loss for batch 1716 : 0.015123724937438965\n",
      "Training loss for batch 1717 : 0.0009718835353851318\n",
      "Training loss for batch 1718 : 0.02420584298670292\n",
      "Training loss for batch 1719 : 0.11780945211648941\n",
      "Training loss for batch 1720 : 0.007354207802563906\n",
      "Training loss for batch 1721 : 0.06733774393796921\n",
      "Training loss for batch 1722 : 0.0\n",
      "Training loss for batch 1723 : 0.03163452446460724\n",
      "Training loss for batch 1724 : 0.024974171072244644\n",
      "Training loss for batch 1725 : 0.04774542152881622\n",
      "Training loss for batch 1726 : 0.056840334087610245\n",
      "Training loss for batch 1727 : 0.055852849036455154\n",
      "Training loss for batch 1728 : 0.10005861520767212\n",
      "Training loss for batch 1729 : 0.42139923572540283\n",
      "Training loss for batch 1730 : 0.0994596853852272\n",
      "Training loss for batch 1731 : 0.184167742729187\n",
      "Training loss for batch 1732 : 0.02617989107966423\n",
      "Training loss for batch 1733 : 0.011714346706867218\n",
      "Training loss for batch 1734 : 0.2864983081817627\n",
      "Training loss for batch 1735 : 0.11370137333869934\n",
      "Training loss for batch 1736 : 0.0766432136297226\n",
      "Training loss for batch 1737 : 0.21561791002750397\n",
      "Training loss for batch 1738 : 0.2243901491165161\n",
      "Training loss for batch 1739 : 0.03536427393555641\n",
      "Training loss for batch 1740 : 0.0054505071602761745\n",
      "Training loss for batch 1741 : 0.05575979873538017\n",
      "Training loss for batch 1742 : 0.03328743577003479\n",
      "Training loss for batch 1743 : 0.09756004065275192\n",
      "Training loss for batch 1744 : 0.002589812036603689\n",
      "Training loss for batch 1745 : 0.30778753757476807\n",
      "Training loss for batch 1746 : 0.11003701388835907\n",
      "Training loss for batch 1747 : 0.05352860689163208\n",
      "Training loss for batch 1748 : 0.22313950955867767\n",
      "Training loss for batch 1749 : 0.2953850030899048\n",
      "Training loss for batch 1750 : 0.3140164017677307\n",
      "Training loss for batch 1751 : 0.07788173854351044\n",
      "Training loss for batch 1752 : 0.19135387241840363\n",
      "Training loss for batch 1753 : 0.059788934886455536\n",
      "Training loss for batch 1754 : 0.03597988188266754\n",
      "Training loss for batch 1755 : 0.049203962087631226\n",
      "Training loss for batch 1756 : 0.21585601568222046\n",
      "Training loss for batch 1757 : 0.10826526582241058\n",
      "Training loss for batch 1758 : 0.1433214694261551\n",
      "Training loss for batch 1759 : 0.16748768091201782\n",
      "Training loss for batch 1760 : 0.10428658127784729\n",
      "Training loss for batch 1761 : 0.2986486852169037\n",
      "Training loss for batch 1762 : 0.2470645159482956\n",
      "Training loss for batch 1763 : -0.00037390567013062537\n",
      "Training loss for batch 1764 : 0.03720381110906601\n",
      "Training loss for batch 1765 : 0.012739310041069984\n",
      "Training loss for batch 1766 : 0.0901431068778038\n",
      "Training loss for batch 1767 : 0.061587199568748474\n",
      "Training loss for batch 1768 : 0.06083538010716438\n",
      "Training loss for batch 1769 : 0.19203485548496246\n",
      "Training loss for batch 1770 : 0.08800294995307922\n",
      "Training loss for batch 1771 : -0.0005933696520514786\n",
      "Training loss for batch 1772 : 0.2001427412033081\n",
      "Training loss for batch 1773 : 0.0061910273507237434\n",
      "Training loss for batch 1774 : 0.06890690326690674\n",
      "Training loss for batch 1775 : 0.13448834419250488\n",
      "Training loss for batch 1776 : 0.1651231050491333\n",
      "Training loss for batch 1777 : 0.03196951001882553\n",
      "Training loss for batch 1778 : 0.16001561284065247\n",
      "Training loss for batch 1779 : 0.15624238550662994\n",
      "Training loss for batch 1780 : 0.025672785937786102\n",
      "Training loss for batch 1781 : 0.023212097585201263\n",
      "Training loss for batch 1782 : 0.3416190445423126\n",
      "Training loss for batch 1783 : 0.2587374448776245\n",
      "Training loss for batch 1784 : 0.16398495435714722\n",
      "Training loss for batch 1785 : 0.03379026800394058\n",
      "Training loss for batch 1786 : 0.09462915360927582\n",
      "Training loss for batch 1787 : 0.6235504746437073\n",
      "Training loss for batch 1788 : 0.06715533137321472\n",
      "Training loss for batch 1789 : 0.14352604746818542\n",
      "Training loss for batch 1790 : 0.39554256200790405\n",
      "Training loss for batch 1791 : 0.07595491409301758\n",
      "Training loss for batch 1792 : 0.1606692224740982\n",
      "Training loss for batch 1793 : 0.20252040028572083\n",
      "Training loss for batch 1794 : 0.05712481960654259\n",
      "Training loss for batch 1795 : 0.0789242833852768\n",
      "Training loss for batch 1796 : 0.08962777256965637\n",
      "Training loss for batch 1797 : 0.1972918063402176\n",
      "Training loss for batch 1798 : 0.07752761989831924\n",
      "Training loss for batch 1799 : 0.08496414870023727\n",
      "Training loss for batch 1800 : 0.1605442762374878\n",
      "Training loss for batch 1801 : 0.1101577952504158\n",
      "Training loss for batch 1802 : 0.07982636988162994\n",
      "Training loss for batch 1803 : 0.1254211664199829\n",
      "Training loss for batch 1804 : 0.19315871596336365\n",
      "Training loss for batch 1805 : 0.23220443725585938\n",
      "Training loss for batch 1806 : 0.034796878695487976\n",
      "Training loss for batch 1807 : 0.2549409568309784\n",
      "Training loss for batch 1808 : 0.20758287608623505\n",
      "Training loss for batch 1809 : 0.010368834249675274\n",
      "Training loss for batch 1810 : 0.14039212465286255\n",
      "Training loss for batch 1811 : 0.171799436211586\n",
      "Training loss for batch 1812 : 0.12214747071266174\n",
      "Training loss for batch 1813 : 0.03790406510233879\n",
      "Training loss for batch 1814 : 0.1726541519165039\n",
      "Training loss for batch 1815 : 0.10820306837558746\n",
      "Training loss for batch 1816 : 0.31538331508636475\n",
      "Training loss for batch 1817 : 0.016846340149641037\n",
      "Training loss for batch 1818 : 0.04318125545978546\n",
      "Training loss for batch 1819 : 0.21153907477855682\n",
      "Training loss for batch 1820 : 0.23972631990909576\n",
      "Training loss for batch 1821 : 0.25345978140830994\n",
      "Training loss for batch 1822 : 0.08809734135866165\n",
      "Training loss for batch 1823 : 0.1718605011701584\n",
      "Training loss for batch 1824 : 0.1218489333987236\n",
      "Training loss for batch 1825 : 0.25196558237075806\n",
      "Training loss for batch 1826 : 0.02174815535545349\n",
      "Training loss for batch 1827 : 0.3225347697734833\n",
      "Training loss for batch 1828 : 0.08480313420295715\n",
      "Training loss for batch 1829 : 0.1278599202632904\n",
      "Training loss for batch 1830 : 0.1384085714817047\n",
      "Training loss for batch 1831 : 0.047346122562885284\n",
      "Training loss for batch 1832 : 0.14113357663154602\n",
      "Training loss for batch 1833 : 0.18519343435764313\n",
      "Training loss for batch 1834 : 0.17303894460201263\n",
      "Training loss for batch 1835 : 0.10029727220535278\n",
      "Training loss for batch 1836 : 0.024045897647738457\n",
      "Training loss for batch 1837 : 0.12448199838399887\n",
      "Training loss for batch 1838 : 0.22777678072452545\n",
      "Training loss for batch 1839 : 0.06843425333499908\n",
      "Training loss for batch 1840 : 0.022088678553700447\n",
      "Training loss for batch 1841 : 0.1190648302435875\n",
      "Training loss for batch 1842 : 0.25490802526474\n",
      "Training loss for batch 1843 : 0.012425239197909832\n",
      "Training loss for batch 1844 : 0.0\n",
      "Training loss for batch 1845 : 0.02501612715423107\n",
      "Training loss for batch 1846 : 0.20621617138385773\n",
      "Training loss for batch 1847 : 0.19944700598716736\n",
      "Training loss for batch 1848 : 0.32958224415779114\n",
      "Training loss for batch 1849 : 0.7313554883003235\n",
      "Training loss for batch 1850 : 0.21053791046142578\n",
      "Training loss for batch 1851 : 0.0037668347358703613\n",
      "Training loss for batch 1852 : 0.08021321147680283\n",
      "Training loss for batch 1853 : 0.06242302805185318\n",
      "Training loss for batch 1854 : 0.15354757010936737\n",
      "Training loss for batch 1855 : 0.04638819769024849\n",
      "Training loss for batch 1856 : 0.2799142301082611\n",
      "Training loss for batch 1857 : 0.08694899827241898\n",
      "Training loss for batch 1858 : 0.22663454711437225\n",
      "Training loss for batch 1859 : 0.16774900257587433\n",
      "Training loss for batch 1860 : 0.003011981723830104\n",
      "Training loss for batch 1861 : 0.1501275897026062\n",
      "Training loss for batch 1862 : 0.13727694749832153\n",
      "Training loss for batch 1863 : 0.35268324613571167\n",
      "Training loss for batch 1864 : 0.21069589257240295\n",
      "Training loss for batch 1865 : 0.11343380808830261\n",
      "Training loss for batch 1866 : 0.15148982405662537\n",
      "Training loss for batch 1867 : 0.047613829374313354\n",
      "Training loss for batch 1868 : 0.2919052541255951\n",
      "Training loss for batch 1869 : 0.06514395028352737\n",
      "Training loss for batch 1870 : 0.2886491119861603\n",
      "Training loss for batch 1871 : 0.02360568568110466\n",
      "Training loss for batch 1872 : 0.2369341254234314\n",
      "Training loss for batch 1873 : 0.5310428738594055\n",
      "Training loss for batch 1874 : 0.12707221508026123\n",
      "Training loss for batch 1875 : 0.09601016342639923\n",
      "Training loss for batch 1876 : 0.23694150149822235\n",
      "Training loss for batch 1877 : 0.2918739914894104\n",
      "Training loss for batch 1878 : 0.009723476134240627\n",
      "Training loss for batch 1879 : 0.07525566220283508\n",
      "Training loss for batch 1880 : 0.0383271761238575\n",
      "Training loss for batch 1881 : 0.24248358607292175\n",
      "Training loss for batch 1882 : 0.0744105726480484\n",
      "Training loss for batch 1883 : 0.19249464571475983\n",
      "Training loss for batch 1884 : 0.21700923144817352\n",
      "Training loss for batch 1885 : 0.1534832864999771\n",
      "Training loss for batch 1886 : 0.033653270453214645\n",
      "Training loss for batch 1887 : 0.011743079870939255\n",
      "Training loss for batch 1888 : 0.33761438727378845\n",
      "Training loss for batch 1889 : 0.20238377153873444\n",
      "Training loss for batch 1890 : 0.06975304335355759\n",
      "Training loss for batch 1891 : 0.006508330814540386\n",
      "Training loss for batch 1892 : 0.12458737939596176\n",
      "Training loss for batch 1893 : 0.1873825043439865\n",
      "Training loss for batch 1894 : 0.21611469984054565\n",
      "Training loss for batch 1895 : 0.08192356675863266\n",
      "Training loss for batch 1896 : 0.3710278272628784\n",
      "Training loss for batch 1897 : 0.10797271132469177\n",
      "Training loss for batch 1898 : 0.23514887690544128\n",
      "Training loss for batch 1899 : 0.14622396230697632\n",
      "Training loss for batch 1900 : 0.06396298855543137\n",
      "Training loss for batch 1901 : 0.13268893957138062\n",
      "Training loss for batch 1902 : 0.08103310316801071\n",
      "Training loss for batch 1903 : 0.11182078719139099\n",
      "Training loss for batch 1904 : 0.23957587778568268\n",
      "Training loss for batch 1905 : 0.09279701113700867\n",
      "Training loss for batch 1906 : 0.06034901738166809\n",
      "Training loss for batch 1907 : 0.18027925491333008\n",
      "Training loss for batch 1908 : 0.1372741311788559\n",
      "Training loss for batch 1909 : 0.12603585422039032\n",
      "Training loss for batch 1910 : 0.14400595426559448\n",
      "Training loss for batch 1911 : 0.07672060281038284\n",
      "Training loss for batch 1912 : 0.09986145794391632\n",
      "Training loss for batch 1913 : 0.13758426904678345\n",
      "Training loss for batch 1914 : 0.10091967135667801\n",
      "Training loss for batch 1915 : 0.07659001648426056\n",
      "Training loss for batch 1916 : 0.007355054374784231\n",
      "Training loss for batch 1917 : 0.34420347213745117\n",
      "Training loss for batch 1918 : 0.1809629648923874\n",
      "Training loss for batch 1919 : 0.1707637906074524\n",
      "Training loss for batch 1920 : 0.14470139145851135\n",
      "Training loss for batch 1921 : 0.34765350818634033\n",
      "Training loss for batch 1922 : 0.14460137486457825\n",
      "Training loss for batch 1923 : 0.028591224923729897\n",
      "Training loss for batch 1924 : 0.12728503346443176\n",
      "Training loss for batch 1925 : 0.25602713227272034\n",
      "Training loss for batch 1926 : 0.14843949675559998\n",
      "Training loss for batch 1927 : 0.3122718334197998\n",
      "Training loss for batch 1928 : 0.13671372830867767\n",
      "Training loss for batch 1929 : 0.12485422194004059\n",
      "Training loss for batch 1930 : 0.02480396442115307\n",
      "Training loss for batch 1931 : 0.2281813770532608\n",
      "Training loss for batch 1932 : 0.07271049171686172\n",
      "Training loss for batch 1933 : 0.0048741186037659645\n",
      "Training loss for batch 1934 : 0.14311383664608002\n",
      "Training loss for batch 1935 : 0.1306098997592926\n",
      "Training loss for batch 1936 : 0.13808681070804596\n",
      "Training loss for batch 1937 : 0.03130454942584038\n",
      "Training loss for batch 1938 : 0.17498847842216492\n",
      "Training loss for batch 1939 : 0.05533173680305481\n",
      "Training loss for batch 1940 : 0.03414176404476166\n",
      "Training loss for batch 1941 : 0.018093567341566086\n",
      "Training loss for batch 1942 : 0.08050856739282608\n",
      "Training loss for batch 1943 : 0.24589130282402039\n",
      "Training loss for batch 1944 : 0.03626047447323799\n",
      "Training loss for batch 1945 : 0.14979080855846405\n",
      "Training loss for batch 1946 : 0.2821461856365204\n",
      "Training loss for batch 1947 : 0.09971592575311661\n",
      "Training loss for batch 1948 : 0.23522095382213593\n",
      "Training loss for batch 1949 : 0.2762359082698822\n",
      "Training loss for batch 1950 : 0.04001383110880852\n",
      "Training loss for batch 1951 : 0.02580433525145054\n",
      "Training loss for batch 1952 : 0.004817361943423748\n",
      "Training loss for batch 1953 : 0.013959165662527084\n",
      "Training loss for batch 1954 : 0.08713595569133759\n",
      "Training loss for batch 1955 : 0.10294043272733688\n",
      "Training loss for batch 1956 : 0.10454872250556946\n",
      "Training loss for batch 1957 : 0.11830087006092072\n",
      "Training loss for batch 1958 : 0.18066652119159698\n",
      "Training loss for batch 1959 : 0.21607911586761475\n",
      "Training loss for batch 1960 : 0.0\n",
      "Training loss for batch 1961 : 0.06356531381607056\n",
      "Training loss for batch 1962 : 0.06179654225707054\n",
      "Training loss for batch 1963 : 0.005657087080180645\n",
      "Training loss for batch 1964 : 0.4110517203807831\n",
      "Training loss for batch 1965 : 0.1376124918460846\n",
      "Training loss for batch 1966 : 0.1416381448507309\n",
      "Training loss for batch 1967 : 0.1633957028388977\n",
      "Training loss for batch 1968 : 0.04701387137174606\n",
      "Training loss for batch 1969 : 0.27798083424568176\n",
      "Training loss for batch 1970 : 0.36670923233032227\n",
      "Training loss for batch 1971 : 0.013283520936965942\n",
      "Training loss for batch 1972 : 0.010357854887843132\n",
      "Training loss for batch 1973 : 0.14275775849819183\n",
      "Training loss for batch 1974 : 0.031296972185373306\n",
      "Training loss for batch 1975 : 0.05627049505710602\n",
      "Training loss for batch 1976 : 0.3222443461418152\n",
      "Training loss for batch 1977 : 0.07284791022539139\n",
      "Training loss for batch 1978 : 0.2481166124343872\n",
      "Training loss for batch 1979 : 0.1184145137667656\n",
      "Training loss for batch 1980 : 0.12415836751461029\n",
      "Training loss for batch 1981 : 0.2406625598669052\n",
      "Training loss for batch 1982 : 0.0835801512002945\n",
      "Training loss for batch 1983 : 0.16009043157100677\n",
      "Training loss for batch 1984 : 0.2050885707139969\n",
      "Training loss for batch 1985 : 0.057384148240089417\n",
      "Training loss for batch 1986 : 0.4917290508747101\n",
      "Training loss for batch 1987 : 0.06792183965444565\n",
      "Training loss for batch 1988 : 0.325921893119812\n",
      "Training loss for batch 1989 : 0.16559772193431854\n",
      "Training loss for batch 1990 : 0.0365636944770813\n",
      "Training loss for batch 1991 : 0.384358674287796\n",
      "Training loss for batch 1992 : 0.15082991123199463\n",
      "Training loss for batch 1993 : 0.23071327805519104\n",
      "Training loss for batch 1994 : 0.38610243797302246\n",
      "Training loss for batch 1995 : 0.20260731875896454\n",
      "Training loss for batch 1996 : 0.5615891218185425\n",
      "Training loss for batch 1997 : 0.07059529423713684\n",
      "Training loss for batch 1998 : 0.045165833085775375\n",
      "Training loss for batch 1999 : 0.12117219716310501\n",
      "Training loss for batch 2000 : 0.16738197207450867\n",
      "Training loss for batch 2001 : 0.16732807457447052\n",
      "Training loss for batch 2002 : 0.009678713046014309\n",
      "Training loss for batch 2003 : 0.13157279789447784\n",
      "Training loss for batch 2004 : 0.3999032974243164\n",
      "Training loss for batch 2005 : 0.09708967059850693\n",
      "Training loss for batch 2006 : 0.01126580499112606\n",
      "Training loss for batch 2007 : 0.0731738731265068\n",
      "Training loss for batch 2008 : 0.21970361471176147\n",
      "Training loss for batch 2009 : 0.11717908829450607\n",
      "Training loss for batch 2010 : 0.21541157364845276\n",
      "Training loss for batch 2011 : 0.14564208686351776\n",
      "Training loss for batch 2012 : 0.08296056091785431\n",
      "Training loss for batch 2013 : 0.2925306558609009\n",
      "Training loss for batch 2014 : 0.28146103024482727\n",
      "Training loss for batch 2015 : 0.2509443461894989\n",
      "Training loss for batch 2016 : 0.1916317343711853\n",
      "Training loss for batch 2017 : 0.0698048323392868\n",
      "Training loss for batch 2018 : 0.05876287445425987\n",
      "Training loss for batch 2019 : 0.028212469071149826\n",
      "Training loss for batch 2020 : 0.11441051959991455\n",
      "Training loss for batch 2021 : 0.22468313574790955\n",
      "Training loss for batch 2022 : 0.23958534002304077\n",
      "Training loss for batch 2023 : 0.007674328051507473\n",
      "Training loss for batch 2024 : 0.06057756394147873\n",
      "Training loss for batch 2025 : 0.0018044710159301758\n",
      "Training loss for batch 2026 : 0.02647930011153221\n",
      "Training loss for batch 2027 : 0.0646415576338768\n",
      "Training loss for batch 2028 : 0.09392817318439484\n",
      "Training loss for batch 2029 : 0.29845839738845825\n",
      "Training loss for batch 2030 : 0.06571649760007858\n",
      "Training loss for batch 2031 : 0.25499850511550903\n",
      "Training loss for batch 2032 : 0.04948143661022186\n",
      "Training loss for batch 2033 : 0.11400635540485382\n",
      "Training loss for batch 2034 : 0.43170812726020813\n",
      "Training loss for batch 2035 : 0.49514809250831604\n",
      "Training loss for batch 2036 : 0.1298951953649521\n",
      "Training loss for batch 2037 : 0.3807181715965271\n",
      "Training loss for batch 2038 : 0.05417875200510025\n",
      "Training loss for batch 2039 : 0.003827492706477642\n",
      "Training loss for batch 2040 : 0.2582324743270874\n",
      "Training loss for batch 2041 : 0.04506335407495499\n",
      "Training loss for batch 2042 : 0.03191056102514267\n",
      "Training loss for batch 2043 : 0.1255396604537964\n",
      "Training loss for batch 2044 : 0.025867871940135956\n",
      "Training loss for batch 2045 : 0.3221854865550995\n",
      "Training loss for batch 2046 : 0.03939099237322807\n",
      "Training loss for batch 2047 : 0.04827301204204559\n",
      "Training loss for batch 2048 : 0.03468872979283333\n",
      "Training loss for batch 2049 : 0.1298610270023346\n",
      "Training loss for batch 2050 : 0.06512577086687088\n",
      "Training loss for batch 2051 : 0.21298177540302277\n",
      "Training loss for batch 2052 : 0.06806265562772751\n",
      "Training loss for batch 2053 : 0.07376836240291595\n",
      "Training loss for batch 2054 : 0.09594134241342545\n",
      "Training loss for batch 2055 : 0.026859164237976074\n",
      "Training loss for batch 2056 : 0.26201534271240234\n",
      "Training loss for batch 2057 : 0.2672151029109955\n",
      "Training loss for batch 2058 : 0.3716529905796051\n",
      "Training loss for batch 2059 : 0.05780613049864769\n",
      "Training loss for batch 2060 : 0.10107268393039703\n",
      "Training loss for batch 2061 : 0.03480299562215805\n",
      "Training loss for batch 2062 : 0.2871301770210266\n",
      "Training loss for batch 2063 : 0.24950480461120605\n",
      "Training loss for batch 2064 : 0.09974513947963715\n",
      "Training loss for batch 2065 : 0.0327199250459671\n",
      "Training loss for batch 2066 : 0.16725608706474304\n",
      "Training loss for batch 2067 : 0.1442023515701294\n",
      "Training loss for batch 2068 : 0.17065124213695526\n",
      "Training loss for batch 2069 : 0.28258490562438965\n",
      "Training loss for batch 2070 : 0.32694360613822937\n",
      "Training loss for batch 2071 : 0.0932512879371643\n",
      "Training loss for batch 2072 : 0.006997790187597275\n",
      "Training loss for batch 2073 : 0.08078279346227646\n",
      "Training loss for batch 2074 : 0.24484947323799133\n",
      "Training loss for batch 2075 : 0.08915597200393677\n",
      "Training loss for batch 2076 : 0.10809704661369324\n",
      "Training loss for batch 2077 : 0.04949168115854263\n",
      "Training loss for batch 2078 : 0.09886445105075836\n",
      "Training loss for batch 2079 : 0.19104944169521332\n",
      "Training loss for batch 2080 : 0.041840553283691406\n",
      "Training loss for batch 2081 : 0.10882166773080826\n",
      "Training loss for batch 2082 : 0.12204089015722275\n",
      "Training loss for batch 2083 : 0.1382182389497757\n",
      "Training loss for batch 2084 : 0.03086760826408863\n",
      "Training loss for batch 2085 : 0.06609556823968887\n",
      "Training loss for batch 2086 : 0.06388635188341141\n",
      "Training loss for batch 2087 : 0.10119513422250748\n",
      "Training loss for batch 2088 : 0.025974305346608162\n",
      "Training loss for batch 2089 : 0.25320932269096375\n",
      "Training loss for batch 2090 : 0.0\n",
      "Training loss for batch 2091 : 0.115101158618927\n",
      "Training loss for batch 2092 : 0.31537747383117676\n",
      "Training loss for batch 2093 : 0.10234227776527405\n",
      "Training loss for batch 2094 : 0.22830694913864136\n",
      "Training loss for batch 2095 : 0.027124924585223198\n",
      "Training loss for batch 2096 : 0.09283452481031418\n",
      "Training loss for batch 2097 : 0.4929512143135071\n",
      "Training loss for batch 2098 : 0.09653054922819138\n",
      "Training loss for batch 2099 : 0.14620713889598846\n",
      "Training loss for batch 2100 : 0.2011638730764389\n",
      "Training loss for batch 2101 : 0.2826232314109802\n",
      "Training loss for batch 2102 : 0.08575185388326645\n",
      "Training loss for batch 2103 : 0.15163946151733398\n",
      "Training loss for batch 2104 : 0.10368523746728897\n",
      "Training loss for batch 2105 : 0.08996021747589111\n",
      "Training loss for batch 2106 : 0.08473110944032669\n",
      "Training loss for batch 2107 : 0.18574056029319763\n",
      "Training loss for batch 2108 : 0.1054292693734169\n",
      "Training loss for batch 2109 : 0.11006361246109009\n",
      "Training loss for batch 2110 : 0.25159716606140137\n",
      "Training loss for batch 2111 : 0.22571967542171478\n",
      "Training loss for batch 2112 : 0.43757522106170654\n",
      "Training loss for batch 2113 : 0.013639370910823345\n",
      "Training loss for batch 2114 : 0.003273199312388897\n",
      "Training loss for batch 2115 : 0.2674192786216736\n",
      "Training loss for batch 2116 : 0.1328950822353363\n",
      "Training loss for batch 2117 : 0.03644762560725212\n",
      "Training loss for batch 2118 : 0.20472444593906403\n",
      "Training loss for batch 2119 : 0.3032243251800537\n",
      "Training loss for batch 2120 : 0.32504016160964966\n",
      "Training loss for batch 2121 : 0.39914363622665405\n",
      "Training loss for batch 2122 : 0.08009698241949081\n",
      "Training loss for batch 2123 : 0.2798202335834503\n",
      "Training loss for batch 2124 : 0.28955233097076416\n",
      "Training loss for batch 2125 : 0.06623344868421555\n",
      "Training loss for batch 2126 : -0.0006609205738641322\n",
      "Training loss for batch 2127 : 0.4132130742073059\n",
      "Training loss for batch 2128 : 0.2799636721611023\n",
      "Training loss for batch 2129 : 0.24251094460487366\n",
      "Training loss for batch 2130 : 0.1390741467475891\n",
      "Training loss for batch 2131 : 0.33319762349128723\n",
      "Training loss for batch 2132 : 0.04812819883227348\n",
      "Training loss for batch 2133 : 0.10673029720783234\n",
      "Training loss for batch 2134 : 0.16805441677570343\n",
      "Training loss for batch 2135 : 0.019072189927101135\n",
      "Training loss for batch 2136 : 0.1619100272655487\n",
      "Training loss for batch 2137 : 0.18007943034172058\n",
      "Training loss for batch 2138 : 0.2744167149066925\n",
      "Training loss for batch 2139 : 0.014550084248185158\n",
      "Training loss for batch 2140 : 0.40161293745040894\n",
      "Training loss for batch 2141 : 0.20587444305419922\n",
      "Training loss for batch 2142 : 0.3267560601234436\n",
      "Training loss for batch 2143 : 0.2684863209724426\n",
      "Training loss for batch 2144 : 0.3805999755859375\n",
      "Training loss for batch 2145 : 0.192602276802063\n",
      "Training loss for batch 2146 : 0.03306145966053009\n",
      "Training loss for batch 2147 : 0.3089192509651184\n",
      "Training loss for batch 2148 : 0.025802701711654663\n",
      "Training loss for batch 2149 : 0.0015230675926432014\n",
      "Training loss for batch 2150 : 0.04949727654457092\n",
      "Training loss for batch 2151 : 0.1097412109375\n",
      "Training loss for batch 2152 : 0.07176155596971512\n",
      "Training loss for batch 2153 : 0.04033329710364342\n",
      "Training loss for batch 2154 : 0.20325206220149994\n",
      "Training loss for batch 2155 : 0.111493781208992\n",
      "Training loss for batch 2156 : 0.12130864709615707\n",
      "Training loss for batch 2157 : 0.24106284976005554\n",
      "Training loss for batch 2158 : 0.06172585487365723\n",
      "Training loss for batch 2159 : 0.0607786551117897\n",
      "Training loss for batch 2160 : 0.2600932717323303\n",
      "Training loss for batch 2161 : 0.2090296745300293\n",
      "Training loss for batch 2162 : 0.07313540577888489\n",
      "Training loss for batch 2163 : 0.019436778500676155\n",
      "Training loss for batch 2164 : 0.3699783384799957\n",
      "Training loss for batch 2165 : 0.027258198708295822\n",
      "Training loss for batch 2166 : 0.23316940665245056\n",
      "Training loss for batch 2167 : 0.03273880109190941\n",
      "Training loss for batch 2168 : 0.06454753130674362\n",
      "Training loss for batch 2169 : 0.12571682035923004\n",
      "Training loss for batch 2170 : 0.26678982377052307\n",
      "Training loss for batch 2171 : 0.1938348263502121\n",
      "Training loss for batch 2172 : 0.24343815445899963\n",
      "Training loss for batch 2173 : 0.2643671929836273\n",
      "Training loss for batch 2174 : 0.03143236041069031\n",
      "Training loss for batch 2175 : 0.07290945202112198\n",
      "Training loss for batch 2176 : 0.012426924891769886\n",
      "Training loss for batch 2177 : 0.027941197156906128\n",
      "Training loss for batch 2178 : 0.15932980179786682\n",
      "Training loss for batch 2179 : 0.11120331287384033\n",
      "Training loss for batch 2180 : 0.7398698925971985\n",
      "Training loss for batch 2181 : 0.11409242451190948\n",
      "Training loss for batch 2182 : 0.016992302611470222\n",
      "Training loss for batch 2183 : 0.06377565860748291\n",
      "Training loss for batch 2184 : 0.24965018033981323\n",
      "Training loss for batch 2185 : 0.07307558506727219\n",
      "Training loss for batch 2186 : 0.3665463924407959\n",
      "Training loss for batch 2187 : 0.10408449172973633\n",
      "Training loss for batch 2188 : 0.13027594983577728\n",
      "Training loss for batch 2189 : 0.03668038174510002\n",
      "Training loss for batch 2190 : 0.13228629529476166\n",
      "Training loss for batch 2191 : 0.1670454740524292\n",
      "Training loss for batch 2192 : 0.054155658930540085\n",
      "Training loss for batch 2193 : 0.11795151978731155\n",
      "Training loss for batch 2194 : 0.022183844819664955\n",
      "Training loss for batch 2195 : 0.23055757582187653\n",
      "Training loss for batch 2196 : 0.1524486392736435\n",
      "Training loss for batch 2197 : 0.11442256718873978\n",
      "Training loss for batch 2198 : 0.173405721783638\n",
      "Training loss for batch 2199 : 0.3087853789329529\n",
      "Training loss for batch 2200 : 0.12188410758972168\n",
      "Training loss for batch 2201 : 0.3225642144680023\n",
      "Training loss for batch 2202 : 0.13439202308654785\n",
      "Training loss for batch 2203 : 0.16514278948307037\n",
      "Training loss for batch 2204 : 0.1054336428642273\n",
      "Training loss for batch 2205 : 0.12033708393573761\n",
      "Training loss for batch 2206 : 0.17191269993782043\n",
      "Training loss for batch 2207 : 0.09733123332262039\n",
      "Training loss for batch 2208 : 0.23644675314426422\n",
      "Training loss for batch 2209 : 0.07544055581092834\n",
      "Training loss for batch 2210 : 0.05290580168366432\n",
      "Training loss for batch 2211 : 0.10829436779022217\n",
      "Training loss for batch 2212 : 0.1614178717136383\n",
      "Training loss for batch 2213 : 0.07308892905712128\n",
      "Training loss for batch 2214 : 0.3846912086009979\n",
      "Training loss for batch 2215 : 0.3167131543159485\n",
      "Training loss for batch 2216 : 0.05545594170689583\n",
      "Training loss for batch 2217 : 0.07833488285541534\n",
      "Training loss for batch 2218 : 0.23772205412387848\n",
      "Training loss for batch 2219 : 0.1631159484386444\n",
      "Training loss for batch 2220 : 0.14797794818878174\n",
      "Training loss for batch 2221 : 0.10078153014183044\n",
      "Training loss for batch 2222 : 0.1482066959142685\n",
      "Training loss for batch 2223 : 0.2039726823568344\n",
      "Training loss for batch 2224 : 0.12290564924478531\n",
      "Training loss for batch 2225 : 0.007073700428009033\n",
      "Training loss for batch 2226 : 0.13876506686210632\n",
      "Training loss for batch 2227 : 0.2060655802488327\n",
      "Training loss for batch 2228 : 0.19050481915473938\n",
      "Training loss for batch 2229 : 0.02774510346353054\n",
      "Training loss for batch 2230 : 0.08486591279506683\n",
      "Training loss for batch 2231 : 0.29860934615135193\n",
      "Training loss for batch 2232 : 0.2574540376663208\n",
      "Training loss for batch 2233 : 0.318722128868103\n",
      "Training loss for batch 2234 : 0.035602714866399765\n",
      "Training loss for batch 2235 : 0.08952561765909195\n",
      "Training loss for batch 2236 : 0.025577446445822716\n",
      "Training loss for batch 2237 : 0.06411002576351166\n",
      "Training loss for batch 2238 : 0.014468577690422535\n",
      "Training loss for batch 2239 : 0.13536615669727325\n",
      "Training loss for batch 2240 : 0.027765406295657158\n",
      "Training loss for batch 2241 : 0.2541515529155731\n",
      "Training loss for batch 2242 : 0.11922799795866013\n",
      "Training loss for batch 2243 : 0.16297990083694458\n",
      "Training loss for batch 2244 : 0.06885652244091034\n",
      "Training loss for batch 2245 : 0.1700785607099533\n",
      "Training loss for batch 2246 : 0.14663752913475037\n",
      "Training loss for batch 2247 : 0.33697494864463806\n",
      "Training loss for batch 2248 : 0.21687644720077515\n",
      "Training loss for batch 2249 : 0.2435893714427948\n",
      "Training loss for batch 2250 : 0.1379166841506958\n",
      "Training loss for batch 2251 : 0.012985318899154663\n",
      "Training loss for batch 2252 : 0.06809699535369873\n",
      "Training loss for batch 2253 : 0.13279132544994354\n",
      "Training loss for batch 2254 : 0.11794722825288773\n",
      "Training loss for batch 2255 : 0.10677286982536316\n",
      "Training loss for batch 2256 : 0.08156593888998032\n",
      "Training loss for batch 2257 : 0.3048616945743561\n",
      "Training loss for batch 2258 : 0.18221893906593323\n",
      "Training loss for batch 2259 : 0.2402328997850418\n",
      "Training loss for batch 2260 : 0.0028903379570692778\n",
      "Training loss for batch 2261 : 0.062192853540182114\n",
      "Training loss for batch 2262 : 0.27359822392463684\n",
      "Training loss for batch 2263 : 0.022594096139073372\n",
      "Training loss for batch 2264 : 0.19650331139564514\n",
      "Training loss for batch 2265 : 0.036404192447662354\n",
      "Training loss for batch 2266 : 0.21879027783870697\n",
      "Training loss for batch 2267 : 0.12262978404760361\n",
      "Training loss for batch 2268 : 0.05232548341155052\n",
      "Training loss for batch 2269 : 0.096274733543396\n",
      "Training loss for batch 2270 : 0.2878616452217102\n",
      "Training loss for batch 2271 : 0.25048187375068665\n",
      "Training loss for batch 2272 : 0.08139152824878693\n",
      "Training loss for batch 2273 : 0.46206527948379517\n",
      "Training loss for batch 2274 : 0.04024525359272957\n",
      "Training loss for batch 2275 : 0.24225567281246185\n",
      "Training loss for batch 2276 : 0.06558806449174881\n",
      "Training loss for batch 2277 : 0.025237761437892914\n",
      "Training loss for batch 2278 : 0.07836881279945374\n",
      "Training loss for batch 2279 : 0.015951436012983322\n",
      "Training loss for batch 2280 : 0.27783510088920593\n",
      "Training loss for batch 2281 : 0.09662080556154251\n",
      "Training loss for batch 2282 : 0.08065042644739151\n",
      "Training loss for batch 2283 : 0.005755455698817968\n",
      "Training loss for batch 2284 : 0.03442133963108063\n",
      "Training loss for batch 2285 : 0.0\n",
      "Training loss for batch 2286 : 0.08568893373012543\n",
      "Training loss for batch 2287 : 0.04298505187034607\n",
      "Training loss for batch 2288 : 0.086765818297863\n",
      "Training loss for batch 2289 : 0.21085971593856812\n",
      "Training loss for batch 2290 : 0.08787396550178528\n",
      "Training loss for batch 2291 : 0.0\n",
      "Training loss for batch 2292 : 0.11252748966217041\n",
      "Training loss for batch 2293 : 0.4841008186340332\n",
      "Training loss for batch 2294 : 0.1790701150894165\n",
      "Training loss for batch 2295 : 0.25404292345046997\n",
      "Training loss for batch 2296 : 0.016606483608484268\n",
      "Training loss for batch 2297 : 0.08413717150688171\n",
      "Training loss for batch 2298 : 0.1446622908115387\n",
      "Training loss for batch 2299 : 0.34558361768722534\n",
      "Training loss for batch 2300 : 0.08994800597429276\n",
      "Training loss for batch 2301 : 0.05541698634624481\n",
      "Training loss for batch 2302 : 0.01908492110669613\n",
      "Training loss for batch 2303 : 0.17726197838783264\n",
      "Training loss for batch 2304 : 0.06668399274349213\n",
      "Training loss for batch 2305 : 0.15934567153453827\n",
      "Training loss for batch 2306 : 0.25025108456611633\n",
      "Training loss for batch 2307 : 0.07242339104413986\n",
      "Training loss for batch 2308 : 0.024331538006663322\n",
      "Training loss for batch 2309 : 0.22091391682624817\n",
      "Training loss for batch 2310 : 0.1829703003168106\n",
      "Training loss for batch 2311 : 0.39404886960983276\n",
      "Training loss for batch 2312 : 0.18968816101551056\n",
      "Training loss for batch 2313 : 0.1680189073085785\n",
      "Training loss for batch 2314 : 0.08964761346578598\n",
      "Training loss for batch 2315 : 0.18951144814491272\n",
      "Training loss for batch 2316 : 0.362467885017395\n",
      "Training loss for batch 2317 : 0.19503581523895264\n",
      "Training loss for batch 2318 : 0.21956540644168854\n",
      "Training loss for batch 2319 : 0.3328990042209625\n",
      "Training loss for batch 2320 : 0.10290280729532242\n",
      "Training loss for batch 2321 : 0.10702670365571976\n",
      "Training loss for batch 2322 : 0.0066114068031311035\n",
      "Training loss for batch 2323 : 0.01038038358092308\n",
      "Training loss for batch 2324 : 0.21410714089870453\n",
      "Training loss for batch 2325 : 0.07686425745487213\n",
      "Training loss for batch 2326 : 0.08253853023052216\n",
      "Training loss for batch 2327 : 0.15311051905155182\n",
      "Training loss for batch 2328 : 0.010737454518675804\n",
      "Training loss for batch 2329 : 0.006502494681626558\n",
      "Training loss for batch 2330 : 0.042843908071517944\n",
      "Training loss for batch 2331 : 0.12784327566623688\n",
      "Training loss for batch 2332 : 0.44415009021759033\n",
      "Training loss for batch 2333 : 0.0005899469251744449\n",
      "Training loss for batch 2334 : 0.01969107985496521\n",
      "Training loss for batch 2335 : 0.025316914543509483\n",
      "Training loss for batch 2336 : 0.03717248514294624\n",
      "Training loss for batch 2337 : 0.04179316759109497\n",
      "Training loss for batch 2338 : 0.0757487341761589\n",
      "Training loss for batch 2339 : 0.09065456688404083\n",
      "Training loss for batch 2340 : 0.36601656675338745\n",
      "Training loss for batch 2341 : 0.06374262273311615\n",
      "Training loss for batch 2342 : 0.06846314668655396\n",
      "Training loss for batch 2343 : 0.042286764830350876\n",
      "Training loss for batch 2344 : 0.250723272562027\n",
      "Training loss for batch 2345 : 0.0939306914806366\n",
      "Training loss for batch 2346 : 0.06739071756601334\n",
      "Training loss for batch 2347 : 0.037045031785964966\n",
      "Training loss for batch 2348 : 0.3211335241794586\n",
      "Training loss for batch 2349 : 0.24994853138923645\n",
      "Training loss for batch 2350 : 0.5501067042350769\n",
      "Training loss for batch 2351 : 0.17323465645313263\n",
      "Training loss for batch 2352 : 0.1690143346786499\n",
      "Training loss for batch 2353 : 0.1635279506444931\n",
      "Training loss for batch 2354 : 0.19866301119327545\n",
      "Training loss for batch 2355 : 0.05374860763549805\n",
      "Training loss for batch 2356 : 0.02211710251867771\n",
      "Training loss for batch 2357 : 0.047679271548986435\n",
      "Training loss for batch 2358 : 0.25087833404541016\n",
      "Training loss for batch 2359 : 0.33158406615257263\n",
      "Training loss for batch 2360 : 0.13425415754318237\n",
      "Training loss for batch 2361 : 0.0937674269080162\n",
      "Training loss for batch 2362 : 0.17715682089328766\n",
      "Training loss for batch 2363 : 0.055427007377147675\n",
      "Training loss for batch 2364 : 0.2667138874530792\n",
      "Training loss for batch 2365 : 0.15787161886692047\n",
      "Training loss for batch 2366 : 0.10992089658975601\n",
      "Training loss for batch 2367 : 0.31807252764701843\n",
      "Training loss for batch 2368 : 0.029013464227318764\n",
      "Training loss for batch 2369 : 0.010435361415147781\n",
      "Training loss for batch 2370 : 0.2132478654384613\n",
      "Training loss for batch 2371 : 0.0\n",
      "Training loss for batch 2372 : 0.3244977295398712\n",
      "Training loss for batch 2373 : 0.10062993317842484\n",
      "Training loss for batch 2374 : 0.06506134569644928\n",
      "Training loss for batch 2375 : 0.1645490527153015\n",
      "Training loss for batch 2376 : 0.20012104511260986\n",
      "Training loss for batch 2377 : 0.20258302986621857\n",
      "Training loss for batch 2378 : 0.04247000813484192\n",
      "Training loss for batch 2379 : 0.052166298031806946\n",
      "Training loss for batch 2380 : 0.1582682728767395\n",
      "Training loss for batch 2381 : 0.2672257423400879\n",
      "Training loss for batch 2382 : 0.2580234706401825\n",
      "Training loss for batch 2383 : 0.17363618314266205\n",
      "Training loss for batch 2384 : 0.0023638010025024414\n",
      "Training loss for batch 2385 : 0.2755178213119507\n",
      "Training loss for batch 2386 : 0.1532088965177536\n",
      "Training loss for batch 2387 : 0.23019559681415558\n",
      "Training loss for batch 2388 : 0.06452379375696182\n",
      "Training loss for batch 2389 : 0.0376439206302166\n",
      "Training loss for batch 2390 : 0.031283553689718246\n",
      "Training loss for batch 2391 : 0.013011325150728226\n",
      "Training loss for batch 2392 : 0.15905974805355072\n",
      "Training loss for batch 2393 : 0.029479365795850754\n",
      "Training loss for batch 2394 : 0.12117359042167664\n",
      "Training loss for batch 2395 : 0.20573341846466064\n",
      "Training loss for batch 2396 : 0.08391302078962326\n",
      "Training loss for batch 2397 : 0.31458738446235657\n",
      "Training loss for batch 2398 : 0.3370641767978668\n",
      "Training loss for batch 2399 : 0.08601152151823044\n",
      "Training loss for batch 2400 : 0.06372654438018799\n",
      "Training loss for batch 2401 : 0.29873961210250854\n",
      "Training loss for batch 2402 : 0.0\n",
      "Training loss for batch 2403 : 0.17834006249904633\n",
      "Training loss for batch 2404 : 0.1820596307516098\n",
      "Training loss for batch 2405 : 0.3563401699066162\n",
      "Training loss for batch 2406 : 0.33666855096817017\n",
      "Training loss for batch 2407 : 0.2144918441772461\n",
      "Training loss for batch 2408 : 0.07190977036952972\n",
      "Training loss for batch 2409 : 0.10134685039520264\n",
      "Training loss for batch 2410 : 0.0873975157737732\n",
      "Training loss for batch 2411 : 0.03400539979338646\n",
      "Training loss for batch 2412 : 0.6227685213088989\n",
      "Training loss for batch 2413 : 0.03568529710173607\n",
      "Training loss for batch 2414 : 0.035860154777765274\n",
      "Training loss for batch 2415 : 0.08176595717668533\n",
      "Training loss for batch 2416 : 0.18008574843406677\n",
      "Training loss for batch 2417 : 0.140328049659729\n",
      "Training loss for batch 2418 : 0.11616964638233185\n",
      "Training loss for batch 2419 : 0.26835304498672485\n",
      "Training loss for batch 2420 : 0.018345430493354797\n",
      "Training loss for batch 2421 : 0.09215027838945389\n",
      "Training loss for batch 2422 : 0.12885569036006927\n",
      "Training loss for batch 2423 : 0.12249843776226044\n",
      "Training loss for batch 2424 : 0.5026086568832397\n",
      "Training loss for batch 2425 : 0.04367785155773163\n",
      "Training loss for batch 2426 : 0.23045000433921814\n",
      "Training loss for batch 2427 : 0.13952910900115967\n",
      "Training loss for batch 2428 : 0.3092409670352936\n",
      "Training loss for batch 2429 : 0.2585153877735138\n",
      "Training loss for batch 2430 : 0.08691064268350601\n",
      "Training loss for batch 2431 : 0.015659278258681297\n",
      "Training loss for batch 2432 : 0.34501540660858154\n",
      "Training loss for batch 2433 : 0.1502271443605423\n",
      "Training loss for batch 2434 : 0.08008941262960434\n",
      "Training loss for batch 2435 : 0.1825491338968277\n",
      "Training loss for batch 2436 : 0.11955026537179947\n",
      "Training loss for batch 2437 : 0.1474369466304779\n",
      "Training loss for batch 2438 : 0.0925733745098114\n",
      "Training loss for batch 2439 : 0.08219581842422485\n",
      "Training loss for batch 2440 : 0.21918784081935883\n",
      "Training loss for batch 2441 : 0.07804899662733078\n",
      "Training loss for batch 2442 : 0.0648033544421196\n",
      "Training loss for batch 2443 : 0.40864914655685425\n",
      "Training loss for batch 2444 : 0.20218726992607117\n",
      "Training loss for batch 2445 : 0.22039243578910828\n",
      "Training loss for batch 2446 : 0.10261762887239456\n",
      "Training loss for batch 2447 : 0.25102823972702026\n",
      "Training loss for batch 2448 : 0.2533506453037262\n",
      "Training loss for batch 2449 : 0.17576831579208374\n",
      "Training loss for batch 2450 : 0.0\n",
      "Training loss for batch 2451 : 0.06040845066308975\n",
      "Training loss for batch 2452 : 0.08508928120136261\n",
      "Training loss for batch 2453 : 0.17767320573329926\n",
      "Training loss for batch 2454 : 0.036602944135665894\n",
      "Training loss for batch 2455 : 0.32791125774383545\n",
      "Training loss for batch 2456 : 0.0035113494377583265\n",
      "Training loss for batch 2457 : 0.17527100443840027\n",
      "Training loss for batch 2458 : 0.11133434623479843\n",
      "Training loss for batch 2459 : 0.19990110397338867\n",
      "Training loss for batch 2460 : 0.11737648397684097\n",
      "Training loss for batch 2461 : 0.07284696400165558\n",
      "Training loss for batch 2462 : 0.13440175354480743\n",
      "Training loss for batch 2463 : 0.08752794563770294\n",
      "Training loss for batch 2464 : 0.01939968205988407\n",
      "Training loss for batch 2465 : 0.31279295682907104\n",
      "Training loss for batch 2466 : 0.1918429583311081\n",
      "Training loss for batch 2467 : 0.23664630949497223\n",
      "Training loss for batch 2468 : 0.10736766457557678\n",
      "Training loss for batch 2469 : 0.23935973644256592\n",
      "Training loss for batch 2470 : 0.06620661169290543\n",
      "Training loss for batch 2471 : 0.3862379789352417\n",
      "Training loss for batch 2472 : 0.08442438393831253\n",
      "Training loss for batch 2473 : 0.2670276165008545\n",
      "Training loss for batch 2474 : 0.12037302553653717\n",
      "Training loss for batch 2475 : 0.0937422439455986\n",
      "Training loss for batch 2476 : 0.3516193628311157\n",
      "Training loss for batch 2477 : 0.05372455716133118\n",
      "Training loss for batch 2478 : 0.3735608756542206\n",
      "Training loss for batch 2479 : 0.20487087965011597\n",
      "Training loss for batch 2480 : 0.0552479550242424\n",
      "Training loss for batch 2481 : 0.1910935342311859\n",
      "Training loss for batch 2482 : 0.09585028141736984\n",
      "Training loss for batch 2483 : 0.10208997130393982\n",
      "Training loss for batch 2484 : 0.13002201914787292\n",
      "Training loss for batch 2485 : 0.21167439222335815\n",
      "Training loss for batch 2486 : 0.10917044430971146\n",
      "Training loss for batch 2487 : 0.12246081978082657\n",
      "Training loss for batch 2488 : 0.1412404477596283\n",
      "Training loss for batch 2489 : 0.08595366030931473\n",
      "Training loss for batch 2490 : -0.0008337005274370313\n",
      "Training loss for batch 2491 : 0.26804983615875244\n",
      "Training loss for batch 2492 : 0.13498345017433167\n",
      "Training loss for batch 2493 : 0.06107287108898163\n",
      "Training loss for batch 2494 : 0.0\n",
      "Training loss for batch 2495 : 0.23102468252182007\n",
      "Training loss for batch 2496 : 0.3058801591396332\n",
      "Training loss for batch 2497 : 0.2551755905151367\n",
      "Training loss for batch 2498 : 0.19049815833568573\n",
      "Training loss for batch 2499 : 0.06996260583400726\n",
      "Training loss for batch 2500 : 0.08940999954938889\n",
      "Training loss for batch 2501 : 0.189852774143219\n",
      "Training loss for batch 2502 : 0.05051114782691002\n",
      "Training loss for batch 2503 : 0.13916587829589844\n",
      "Training loss for batch 2504 : 0.16153748333454132\n",
      "Training loss for batch 2505 : 0.2218739241361618\n",
      "Training loss for batch 2506 : 0.023663237690925598\n",
      "Training loss for batch 2507 : 0.14183837175369263\n",
      "Training loss for batch 2508 : 0.1742526739835739\n",
      "Training loss for batch 2509 : 0.5211327075958252\n",
      "Training loss for batch 2510 : 0.10442725569009781\n",
      "Training loss for batch 2511 : 0.07250753790140152\n",
      "Training loss for batch 2512 : 0.36572831869125366\n",
      "Training loss for batch 2513 : 0.15927743911743164\n",
      "Training loss for batch 2514 : 0.09203389286994934\n",
      "Training loss for batch 2515 : 0.10249365121126175\n",
      "Training loss for batch 2516 : 0.2581133544445038\n",
      "Training loss for batch 2517 : 0.0855247750878334\n",
      "Training loss for batch 2518 : 0.39927905797958374\n",
      "Training loss for batch 2519 : 0.10628729313611984\n",
      "Training loss for batch 2520 : 0.15129050612449646\n",
      "Training loss for batch 2521 : 0.14509384334087372\n",
      "Training loss for batch 2522 : 0.06600011140108109\n",
      "Training loss for batch 2523 : 0.16805538535118103\n",
      "Training loss for batch 2524 : 0.1844640076160431\n",
      "Training loss for batch 2525 : 0.02523694559931755\n",
      "Training loss for batch 2526 : 0.1482500433921814\n",
      "Training loss for batch 2527 : 0.18929411470890045\n",
      "Training loss for batch 2528 : 0.14875532686710358\n",
      "Training loss for batch 2529 : 0.2618752419948578\n",
      "Training loss for batch 2530 : 0.002781480550765991\n",
      "Training loss for batch 2531 : 0.19456537067890167\n",
      "Training loss for batch 2532 : 0.13621383905410767\n",
      "Training loss for batch 2533 : 0.06729404628276825\n",
      "Training loss for batch 2534 : 0.08224907517433167\n",
      "Training loss for batch 2535 : 0.020991604775190353\n",
      "Training loss for batch 2536 : 0.05798598378896713\n",
      "Training loss for batch 2537 : 0.12540939450263977\n",
      "Training loss for batch 2538 : 0.05737266689538956\n",
      "Training loss for batch 2539 : 0.042383842170238495\n",
      "Training loss for batch 2540 : 0.21955013275146484\n",
      "Training loss for batch 2541 : 0.13826648890972137\n",
      "Training loss for batch 2542 : 0.257185697555542\n",
      "Training loss for batch 2543 : 0.0978349819779396\n",
      "Training loss for batch 2544 : 0.011159280315041542\n",
      "Training loss for batch 2545 : 0.16006159782409668\n",
      "Training loss for batch 2546 : 0.30371299386024475\n",
      "Training loss for batch 2547 : 0.08003947883844376\n",
      "Training loss for batch 2548 : 0.4297964572906494\n",
      "Training loss for batch 2549 : 0.160828098654747\n",
      "Training loss for batch 2550 : 0.26813656091690063\n",
      "Training loss for batch 2551 : 0.12264996021986008\n",
      "Training loss for batch 2552 : 0.10339664667844772\n",
      "Training loss for batch 2553 : 0.25909698009490967\n",
      "Training loss for batch 2554 : 0.2606702148914337\n",
      "Training loss for batch 2555 : 0.08231307566165924\n",
      "Training loss for batch 2556 : 0.1051827073097229\n",
      "Training loss for batch 2557 : 0.0365719199180603\n",
      "Training loss for batch 2558 : 0.16672062873840332\n",
      "Training loss for batch 2559 : 0.11520783603191376\n",
      "Training loss for batch 2560 : 0.20212291181087494\n",
      "Training loss for batch 2561 : 0.09990458190441132\n",
      "Training loss for batch 2562 : 0.0661005899310112\n",
      "Training loss for batch 2563 : 0.0739179402589798\n",
      "Training loss for batch 2564 : 0.29088813066482544\n",
      "Training loss for batch 2565 : 0.031166931614279747\n",
      "Training loss for batch 2566 : 0.18280838429927826\n",
      "Training loss for batch 2567 : 0.04817872866988182\n",
      "Training loss for batch 2568 : 0.2913982570171356\n",
      "Training loss for batch 2569 : 0.0736747607588768\n",
      "Training loss for batch 2570 : 0.11514948308467865\n",
      "Training loss for batch 2571 : 0.24867811799049377\n",
      "Training loss for batch 2572 : 0.08077029138803482\n",
      "Training loss for batch 2573 : 0.12931819260120392\n",
      "Training loss for batch 2574 : 0.0692160427570343\n",
      "Training loss for batch 2575 : 0.28603339195251465\n",
      "Training loss for batch 2576 : 0.22319109737873077\n",
      "Training loss for batch 2577 : 0.5505193471908569\n",
      "Training loss for batch 2578 : 0.41483137011528015\n",
      "Training loss for batch 2579 : 0.09625120460987091\n",
      "Training loss for batch 2580 : 0.047582224011421204\n",
      "Training loss for batch 2581 : 0.21164235472679138\n",
      "Training loss for batch 2582 : 0.18918991088867188\n",
      "Training loss for batch 2583 : 0.32036787271499634\n",
      "Training loss for batch 2584 : 0.06646206974983215\n",
      "Training loss for batch 2585 : 0.23133333027362823\n",
      "Training loss for batch 2586 : 0.15685442090034485\n",
      "Training loss for batch 2587 : 0.12159162014722824\n",
      "Training loss for batch 2588 : 0.22041110694408417\n",
      "Training loss for batch 2589 : 0.39036959409713745\n",
      "Training loss for batch 2590 : 0.136141836643219\n",
      "Training loss for batch 2591 : 0.0\n",
      "Training loss for batch 2592 : 0.0033958288840949535\n",
      "Training loss for batch 2593 : 0.11394433677196503\n",
      "Training loss for batch 2594 : 0.13537926971912384\n",
      "Training loss for batch 2595 : 0.0\n",
      "Training loss for batch 2596 : 0.1290380209684372\n",
      "Training loss for batch 2597 : -0.0019530684221535921\n",
      "Training loss for batch 2598 : 0.08349252492189407\n",
      "Training loss for batch 2599 : 0.17408563196659088\n",
      "Training loss for batch 2600 : 0.0888887494802475\n",
      "Training loss for batch 2601 : 0.053275514394044876\n",
      "Training loss for batch 2602 : 0.2861291170120239\n",
      "Training loss for batch 2603 : 0.12074581533670425\n",
      "Training loss for batch 2604 : 0.07287607342004776\n",
      "Training loss for batch 2605 : 0.15814727544784546\n",
      "Training loss for batch 2606 : 0.09357236325740814\n",
      "Training loss for batch 2607 : 0.27975985407829285\n",
      "Training loss for batch 2608 : 0.16571515798568726\n",
      "Training loss for batch 2609 : 0.05119915306568146\n",
      "Training loss for batch 2610 : 0.017280539497733116\n",
      "Training loss for batch 2611 : 0.07092568278312683\n",
      "Training loss for batch 2612 : 0.22675737738609314\n",
      "Training loss for batch 2613 : 0.18543550372123718\n",
      "Training loss for batch 2614 : 0.05385950207710266\n",
      "Training loss for batch 2615 : 0.33076348900794983\n",
      "Training loss for batch 2616 : 0.11180068552494049\n",
      "Training loss for batch 2617 : 0.05206692963838577\n",
      "Training loss for batch 2618 : 0.2148902267217636\n",
      "Training loss for batch 2619 : 0.029768049716949463\n",
      "Training loss for batch 2620 : 0.03231734037399292\n",
      "Training loss for batch 2621 : 0.083351269364357\n",
      "Training loss for batch 2622 : 0.3734003007411957\n",
      "Training loss for batch 2623 : 0.19308094680309296\n",
      "Training loss for batch 2624 : -0.00015674381575081497\n",
      "Training loss for batch 2625 : 0.06058843061327934\n",
      "Training loss for batch 2626 : 0.12381613254547119\n",
      "Training loss for batch 2627 : 0.12948302924633026\n",
      "Training loss for batch 2628 : 0.32839736342430115\n",
      "Training loss for batch 2629 : 0.15549051761627197\n",
      "Training loss for batch 2630 : 0.06607156991958618\n",
      "Training loss for batch 2631 : 0.10489608347415924\n",
      "Training loss for batch 2632 : 0.014071127399802208\n",
      "Training loss for batch 2633 : 0.3498491942882538\n",
      "Training loss for batch 2634 : 0.006948568392544985\n",
      "Training loss for batch 2635 : 0.21517662703990936\n",
      "Training loss for batch 2636 : 0.12677064538002014\n",
      "Training loss for batch 2637 : 0.10513992607593536\n",
      "Training loss for batch 2638 : 0.2248769849538803\n",
      "Training loss for batch 2639 : 0.1390911489725113\n",
      "Training loss for batch 2640 : 0.040487900376319885\n",
      "Training loss for batch 2641 : 0.0786987766623497\n",
      "Training loss for batch 2642 : 0.12407064437866211\n",
      "Training loss for batch 2643 : 0.03696746006608009\n",
      "Training loss for batch 2644 : 0.012719649821519852\n",
      "Training loss for batch 2645 : 0.05414491146802902\n",
      "Training loss for batch 2646 : 0.19850501418113708\n",
      "Training loss for batch 2647 : 0.10854124277830124\n",
      "Training loss for batch 2648 : 0.046213239431381226\n",
      "Training loss for batch 2649 : 0.06588520109653473\n",
      "Training loss for batch 2650 : 0.17656144499778748\n",
      "Training loss for batch 2651 : 0.07295867800712585\n",
      "Training loss for batch 2652 : 0.05493740364909172\n",
      "Training loss for batch 2653 : 0.24914151430130005\n",
      "Training loss for batch 2654 : 0.2223484218120575\n",
      "Training loss for batch 2655 : 0.17335443198680878\n",
      "Training loss for batch 2656 : 0.2655520439147949\n",
      "Training loss for batch 2657 : 0.09250298887491226\n",
      "Training loss for batch 2658 : 0.04414628446102142\n",
      "Training loss for batch 2659 : 0.11739031225442886\n",
      "Training loss for batch 2660 : 0.08215591311454773\n",
      "Training loss for batch 2661 : 0.12048905342817307\n",
      "Training loss for batch 2662 : 0.03627999499440193\n",
      "Training loss for batch 2663 : 0.04582488536834717\n",
      "Training loss for batch 2664 : 0.1377233862876892\n",
      "Training loss for batch 2665 : 0.16710413992404938\n",
      "Training loss for batch 2666 : 0.1912476122379303\n",
      "Training loss for batch 2667 : 0.09453579783439636\n",
      "Training loss for batch 2668 : 0.10728777945041656\n",
      "Training loss for batch 2669 : 0.3983955979347229\n",
      "Training loss for batch 2670 : 0.10342894494533539\n",
      "Training loss for batch 2671 : 0.06361091136932373\n",
      "Training loss for batch 2672 : 0.006089001893997192\n",
      "Training loss for batch 2673 : 0.0\n",
      "Training loss for batch 2674 : 0.4200115501880646\n",
      "Training loss for batch 2675 : 0.13260968029499054\n",
      "Training loss for batch 2676 : 0.24586893618106842\n",
      "Training loss for batch 2677 : 0.294727623462677\n",
      "Training loss for batch 2678 : 0.20783759653568268\n",
      "Training loss for batch 2679 : 0.06186698377132416\n",
      "Training loss for batch 2680 : 0.20871253311634064\n",
      "Training loss for batch 2681 : 0.05118337273597717\n",
      "Training loss for batch 2682 : -0.0015645320527255535\n",
      "Training loss for batch 2683 : 0.1269339770078659\n",
      "Training loss for batch 2684 : 0.10823305696249008\n",
      "Training loss for batch 2685 : 0.09782373160123825\n",
      "Training loss for batch 2686 : 0.18696777522563934\n",
      "Training loss for batch 2687 : 0.18447467684745789\n",
      "Training loss for batch 2688 : 0.4828188121318817\n",
      "Training loss for batch 2689 : 0.16892389953136444\n",
      "Training loss for batch 2690 : 0.3116973638534546\n",
      "Training loss for batch 2691 : 0.0021713937167078257\n",
      "Training loss for batch 2692 : 0.1908000111579895\n",
      "Training loss for batch 2693 : 0.0006892681121826172\n",
      "Training loss for batch 2694 : 0.08226409554481506\n",
      "Training loss for batch 2695 : 0.24459892511367798\n",
      "Training loss for batch 2696 : 0.04043569415807724\n",
      "Training loss for batch 2697 : 0.46065211296081543\n",
      "Training loss for batch 2698 : 0.3168981075286865\n",
      "Training loss for batch 2699 : 0.06925572454929352\n",
      "Training loss for batch 2700 : 0.1041736900806427\n",
      "Training loss for batch 2701 : 0.049861080944538116\n",
      "Training loss for batch 2702 : 0.14730626344680786\n",
      "Training loss for batch 2703 : 0.09724290668964386\n",
      "Training loss for batch 2704 : 0.046066801995038986\n",
      "Training loss for batch 2705 : 0.14722950756549835\n",
      "Training loss for batch 2706 : 0.13725051283836365\n",
      "Training loss for batch 2707 : 0.08400534093379974\n",
      "Training loss for batch 2708 : 0.07162496447563171\n",
      "Training loss for batch 2709 : 0.10391481965780258\n",
      "Training loss for batch 2710 : 0.11640140414237976\n",
      "Training loss for batch 2711 : 0.12468019872903824\n",
      "Training loss for batch 2712 : 0.11635583639144897\n",
      "Training loss for batch 2713 : 0.14125879108905792\n",
      "Training loss for batch 2714 : 0.22716817259788513\n",
      "Training loss for batch 2715 : 0.4358338713645935\n",
      "Training loss for batch 2716 : 0.3402928113937378\n",
      "Training loss for batch 2717 : 0.10367795825004578\n",
      "Training loss for batch 2718 : 0.04105544090270996\n",
      "Training loss for batch 2719 : 0.24377572536468506\n",
      "Training loss for batch 2720 : 0.05661962926387787\n",
      "Training loss for batch 2721 : 0.2358754426240921\n",
      "Training loss for batch 2722 : 0.1979396939277649\n",
      "Training loss for batch 2723 : 0.3189269006252289\n",
      "Training loss for batch 2724 : 0.6480391025543213\n",
      "Training loss for batch 2725 : 0.13013479113578796\n",
      "Training loss for batch 2726 : 0.3100106716156006\n",
      "Training loss for batch 2727 : 0.0625479444861412\n",
      "Training loss for batch 2728 : 0.1645401418209076\n",
      "Training loss for batch 2729 : 0.2715940773487091\n",
      "Training loss for batch 2730 : 0.04435495287179947\n",
      "Training loss for batch 2731 : 0.14880980551242828\n",
      "Training loss for batch 2732 : 0.24665890634059906\n",
      "Training loss for batch 2733 : 0.18740028142929077\n",
      "Training loss for batch 2734 : 0.06132793799042702\n",
      "Training loss for batch 2735 : 0.3320551812648773\n",
      "Training loss for batch 2736 : 0.16947919130325317\n",
      "Training loss for batch 2737 : 0.13439498841762543\n",
      "Training loss for batch 2738 : 0.026370596140623093\n",
      "Training loss for batch 2739 : 0.17200666666030884\n",
      "Training loss for batch 2740 : 0.22300732135772705\n",
      "Training loss for batch 2741 : 0.03386786952614784\n",
      "Training loss for batch 2742 : 0.05633750557899475\n",
      "Training loss for batch 2743 : 0.04615072160959244\n",
      "Training loss for batch 2744 : 0.014694944024085999\n",
      "Training loss for batch 2745 : 0.21147885918617249\n",
      "Training loss for batch 2746 : 0.21111488342285156\n",
      "Training loss for batch 2747 : 0.14401891827583313\n",
      "Training loss for batch 2748 : 0.08314183354377747\n",
      "Training loss for batch 2749 : 0.0967259332537651\n",
      "Training loss for batch 2750 : 0.19272425770759583\n",
      "Training loss for batch 2751 : 0.055473435670137405\n",
      "Training loss for batch 2752 : 0.05855058133602142\n",
      "Training loss for batch 2753 : 0.14661087095737457\n",
      "Training loss for batch 2754 : 0.09031860530376434\n",
      "Training loss for batch 2755 : 0.15489834547042847\n",
      "Training loss for batch 2756 : 0.039178770035505295\n",
      "Training loss for batch 2757 : 0.0419190414249897\n",
      "Training loss for batch 2758 : 0.19074569642543793\n",
      "Training loss for batch 2759 : 0.09071926027536392\n",
      "Training loss for batch 2760 : 0.18492789566516876\n",
      "Training loss for batch 2761 : 0.06281840056180954\n",
      "Training loss for batch 2762 : 0.16634654998779297\n",
      "Training loss for batch 2763 : 0.02160041779279709\n",
      "Training loss for batch 2764 : 0.04966505989432335\n",
      "Training loss for batch 2765 : 0.32546287775039673\n",
      "Training loss for batch 2766 : 0.27740076184272766\n",
      "Training loss for batch 2767 : 0.268585741519928\n",
      "Training loss for batch 2768 : 0.18817463517189026\n",
      "Training loss for batch 2769 : 0.09088057279586792\n",
      "Training loss for batch 2770 : 0.016845496371388435\n",
      "Training loss for batch 2771 : 0.31223928928375244\n",
      "Training loss for batch 2772 : 0.024609334766864777\n",
      "Training loss for batch 2773 : 0.1261099874973297\n",
      "Training loss for batch 2774 : 0.3044857382774353\n",
      "Training loss for batch 2775 : 0.039946526288986206\n",
      "Training loss for batch 2776 : 0.0965256318449974\n",
      "Training loss for batch 2777 : 0.11086702346801758\n",
      "Training loss for batch 2778 : 0.029199302196502686\n",
      "Training loss for batch 2779 : 0.14276042580604553\n",
      "Training loss for batch 2780 : 0.2029843032360077\n",
      "Training loss for batch 2781 : 0.20120802521705627\n",
      "Training loss for batch 2782 : 0.11723244190216064\n",
      "Training loss for batch 2783 : 0.03330758213996887\n",
      "Training loss for batch 2784 : 0.09865626692771912\n",
      "Training loss for batch 2785 : 0.1389114111661911\n",
      "Training loss for batch 2786 : 0.14301690459251404\n",
      "Training loss for batch 2787 : 0.0028183809481561184\n",
      "Training loss for batch 2788 : 0.20707859098911285\n",
      "Training loss for batch 2789 : 0.12482999265193939\n",
      "Training loss for batch 2790 : 0.09300998598337173\n",
      "Training loss for batch 2791 : 0.03863609954714775\n",
      "Training loss for batch 2792 : 0.04891455918550491\n",
      "Training loss for batch 2793 : 0.5960507392883301\n",
      "Training loss for batch 2794 : 0.07449810206890106\n",
      "Training loss for batch 2795 : 0.05121057853102684\n",
      "Training loss for batch 2796 : 0.1646662950515747\n",
      "Training loss for batch 2797 : 0.30561375617980957\n",
      "Training loss for batch 2798 : 0.1594173014163971\n",
      "Training loss for batch 2799 : 0.23325224220752716\n",
      "Training loss for batch 2800 : 0.046965956687927246\n",
      "Training loss for batch 2801 : 0.046837981790304184\n",
      "Training loss for batch 2802 : 0.13297784328460693\n",
      "Training loss for batch 2803 : 0.09283339977264404\n",
      "Training loss for batch 2804 : 0.2511973977088928\n",
      "Training loss for batch 2805 : 0.03271226957440376\n",
      "Training loss for batch 2806 : 0.10999145358800888\n",
      "Training loss for batch 2807 : 0.18933618068695068\n",
      "Training loss for batch 2808 : 0.14094720780849457\n",
      "Training loss for batch 2809 : 0.33229073882102966\n",
      "Training loss for batch 2810 : 0.2943885922431946\n",
      "Training loss for batch 2811 : 0.2650521695613861\n",
      "Training loss for batch 2812 : 0.09838046133518219\n",
      "Training loss for batch 2813 : 0.142563134431839\n",
      "Training loss for batch 2814 : 0.04936537891626358\n",
      "Training loss for batch 2815 : 0.2385134994983673\n",
      "Training loss for batch 2816 : 0.2229015976190567\n",
      "Training loss for batch 2817 : 0.07426460832357407\n",
      "Training loss for batch 2818 : 0.1482754796743393\n",
      "Training loss for batch 2819 : 0.1495356559753418\n",
      "Training loss for batch 2820 : 0.020008664578199387\n",
      "Training loss for batch 2821 : 0.02542426809668541\n",
      "Training loss for batch 2822 : 0.3113318383693695\n",
      "Training loss for batch 2823 : 0.22365328669548035\n",
      "Training loss for batch 2824 : 0.10344413667917252\n",
      "Training loss for batch 2825 : 0.043293584138154984\n",
      "Training loss for batch 2826 : 0.31524133682250977\n",
      "Training loss for batch 2827 : 0.14158739149570465\n",
      "Training loss for batch 2828 : 0.31955209374427795\n",
      "Training loss for batch 2829 : 0.12513689696788788\n",
      "Training loss for batch 2830 : 0.045389559119939804\n",
      "Training loss for batch 2831 : 0.041218750178813934\n",
      "Training loss for batch 2832 : 0.26197442412376404\n",
      "Training loss for batch 2833 : 0.11196744441986084\n",
      "Training loss for batch 2834 : 0.04826038330793381\n",
      "Training loss for batch 2835 : 0.12134198844432831\n",
      "Training loss for batch 2836 : 0.4006911814212799\n",
      "Training loss for batch 2837 : 0.02729831263422966\n",
      "Training loss for batch 2838 : 0.20242679119110107\n",
      "Training loss for batch 2839 : 0.0379713773727417\n",
      "Training loss for batch 2840 : 0.0\n",
      "Training loss for batch 2841 : 0.06037808582186699\n",
      "Training loss for batch 2842 : 0.039374932646751404\n",
      "Training loss for batch 2843 : 0.07336346060037613\n",
      "Training loss for batch 2844 : 0.1906522512435913\n",
      "Training loss for batch 2845 : 0.01849977858364582\n",
      "Training loss for batch 2846 : 0.0\n",
      "Training loss for batch 2847 : 0.07830138504505157\n",
      "Training loss for batch 2848 : 0.032921671867370605\n",
      "Training loss for batch 2849 : 0.2250366359949112\n",
      "Training loss for batch 2850 : 0.1801982969045639\n",
      "Training loss for batch 2851 : 0.04904014244675636\n",
      "Training loss for batch 2852 : 0.12298627942800522\n",
      "Training loss for batch 2853 : 0.27979233860969543\n",
      "Training loss for batch 2854 : 0.12762464582920074\n",
      "Training loss for batch 2855 : 0.1750713437795639\n",
      "Training loss for batch 2856 : 0.007613015361130238\n",
      "Training loss for batch 2857 : 0.19503161311149597\n",
      "Training loss for batch 2858 : 0.14443928003311157\n",
      "Training loss for batch 2859 : 0.056898750364780426\n",
      "Training loss for batch 2860 : 0.3067658841609955\n",
      "Training loss for batch 2861 : 0.1310584396123886\n",
      "Training loss for batch 2862 : 0.06425727903842926\n",
      "Training loss for batch 2863 : 0.06180731579661369\n",
      "Training loss for batch 2864 : 0.21067559719085693\n",
      "Training loss for batch 2865 : 0.19440770149230957\n",
      "Training loss for batch 2866 : 0.03969821333885193\n",
      "Training loss for batch 2867 : 0.17709460854530334\n",
      "Training loss for batch 2868 : 0.05945268273353577\n",
      "Training loss for batch 2869 : 0.009543627500534058\n",
      "Training loss for batch 2870 : 0.01850779540836811\n",
      "Training loss for batch 2871 : 0.1027086079120636\n",
      "Training loss for batch 2872 : 0.19000393152236938\n",
      "Training loss for batch 2873 : 0.02637607604265213\n",
      "Training loss for batch 2874 : 0.04718749597668648\n",
      "Training loss for batch 2875 : 0.06301082670688629\n",
      "Training loss for batch 2876 : 0.05010402202606201\n",
      "Training loss for batch 2877 : 0.22103777527809143\n",
      "Training loss for batch 2878 : 0.00196961872279644\n",
      "Training loss for batch 2879 : 0.393106609582901\n",
      "Training loss for batch 2880 : 0.10483349859714508\n",
      "Training loss for batch 2881 : 0.02864701859652996\n",
      "Training loss for batch 2882 : 0.2165159285068512\n",
      "Training loss for batch 2883 : 0.016321534290909767\n",
      "Training loss for batch 2884 : 0.03617003187537193\n",
      "Training loss for batch 2885 : 0.38897451758384705\n",
      "Training loss for batch 2886 : 0.20842644572257996\n",
      "Training loss for batch 2887 : 0.13607533276081085\n",
      "Training loss for batch 2888 : 0.20684866607189178\n",
      "Training loss for batch 2889 : 0.31082236766815186\n",
      "Training loss for batch 2890 : 0.07306423783302307\n",
      "Training loss for batch 2891 : 0.039550893008708954\n",
      "Training loss for batch 2892 : 0.0876363217830658\n",
      "Training loss for batch 2893 : 0.12295283377170563\n",
      "Training loss for batch 2894 : -0.0015533223049715161\n",
      "Training loss for batch 2895 : 0.07257574051618576\n",
      "Training loss for batch 2896 : 0.1104244664311409\n",
      "Training loss for batch 2897 : 0.11749646812677383\n",
      "Training loss for batch 2898 : 0.13532023131847382\n",
      "Training loss for batch 2899 : 0.06569961458444595\n",
      "Training loss for batch 2900 : 0.2981938123703003\n",
      "Training loss for batch 2901 : 0.21633432805538177\n",
      "Training loss for batch 2902 : 0.21765795350074768\n",
      "Training loss for batch 2903 : 0.08169825375080109\n",
      "Training loss for batch 2904 : 0.3320244252681732\n",
      "Training loss for batch 2905 : 0.1654001772403717\n",
      "Training loss for batch 2906 : 0.09212293475866318\n",
      "Training loss for batch 2907 : 0.19554486870765686\n",
      "Training loss for batch 2908 : 0.11370932310819626\n",
      "Training loss for batch 2909 : 0.11607184261083603\n",
      "Training loss for batch 2910 : 0.12262324243783951\n",
      "Training loss for batch 2911 : 0.13757482171058655\n",
      "Training loss for batch 2912 : 0.24034126102924347\n",
      "Training loss for batch 2913 : 0.13106372952461243\n",
      "Training loss for batch 2914 : 0.17987768352031708\n",
      "Training loss for batch 2915 : 0.15642984211444855\n",
      "Training loss for batch 2916 : 0.12671537697315216\n",
      "Training loss for batch 2917 : 0.16554482281208038\n",
      "Training loss for batch 2918 : 0.1345336139202118\n",
      "Training loss for batch 2919 : 0.007572515867650509\n",
      "Training loss for batch 2920 : 0.15572717785835266\n",
      "Training loss for batch 2921 : 0.12565775215625763\n",
      "Training loss for batch 2922 : 0.01240251213312149\n",
      "Training loss for batch 2923 : 0.08247308433055878\n",
      "Training loss for batch 2924 : 0.1801930069923401\n",
      "Training loss for batch 2925 : 0.16314977407455444\n",
      "Training loss for batch 2926 : 0.016160450875759125\n",
      "Training loss for batch 2927 : 0.10990718752145767\n",
      "Training loss for batch 2928 : 0.08761198073625565\n",
      "Training loss for batch 2929 : 0.09303717315196991\n",
      "Training loss for batch 2930 : 0.08867451548576355\n",
      "Training loss for batch 2931 : 0.19442790746688843\n",
      "Training loss for batch 2932 : 0.1517190933227539\n",
      "Training loss for batch 2933 : 0.19571249186992645\n",
      "Training loss for batch 2934 : 0.12555325031280518\n",
      "Training loss for batch 2935 : 0.07577403634786606\n",
      "Training loss for batch 2936 : 0.12600664794445038\n",
      "Training loss for batch 2937 : 0.07237637042999268\n",
      "Training loss for batch 2938 : 0.07190737128257751\n",
      "Training loss for batch 2939 : 0.08007670938968658\n",
      "Training loss for batch 2940 : 0.27716538310050964\n",
      "Training loss for batch 2941 : 0.04279176518321037\n",
      "Training loss for batch 2942 : 0.3361314535140991\n",
      "Training loss for batch 2943 : 0.32038503885269165\n",
      "Training loss for batch 2944 : 0.0879770815372467\n",
      "Training loss for batch 2945 : 0.3466056287288666\n",
      "Training loss for batch 2946 : 0.1613607555627823\n",
      "Training loss for batch 2947 : 0.017475677654147148\n",
      "Training loss for batch 2948 : 0.2830449938774109\n",
      "Training loss for batch 2949 : 0.01783743128180504\n",
      "Training loss for batch 2950 : 0.10578503459692001\n",
      "Training loss for batch 2951 : 0.067646823823452\n",
      "Training loss for batch 2952 : 0.13750821352005005\n",
      "Training loss for batch 2953 : 0.29641133546829224\n",
      "Training loss for batch 2954 : 0.12938909232616425\n",
      "Training loss for batch 2955 : 0.2522498667240143\n",
      "Training loss for batch 2956 : 0.03048095852136612\n",
      "Training loss for batch 2957 : 0.18090380728244781\n",
      "Training loss for batch 2958 : 0.15631508827209473\n",
      "Training loss for batch 2959 : 0.20245566964149475\n",
      "Training loss for batch 2960 : 0.5869684815406799\n",
      "Training loss for batch 2961 : 0.39788269996643066\n",
      "Training loss for batch 2962 : 0.08637908101081848\n",
      "Training loss for batch 2963 : 0.011717407964169979\n",
      "Training loss for batch 2964 : 0.07111020386219025\n",
      "Training loss for batch 2965 : 0.015427449718117714\n",
      "Training loss for batch 2966 : 0.008578727953135967\n",
      "Training loss for batch 2967 : 0.13202664256095886\n",
      "Training loss for batch 2968 : 0.056231673806905746\n",
      "Training loss for batch 2969 : 0.30066728591918945\n",
      "Training loss for batch 2970 : 0.2093629240989685\n",
      "Training loss for batch 2971 : 0.1470808982849121\n",
      "Training loss for batch 2972 : 0.0666797086596489\n",
      "Training loss for batch 2973 : 0.0\n",
      "Training loss for batch 2974 : 0.24592405557632446\n",
      "Training loss for batch 2975 : 0.11476866900920868\n",
      "Training loss for batch 2976 : 0.17386800050735474\n",
      "Training loss for batch 2977 : 0.020701482892036438\n",
      "Training loss for batch 2978 : 0.08609650284051895\n",
      "Training loss for batch 2979 : 0.8866987824440002\n",
      "Training loss for batch 2980 : 0.1025925725698471\n",
      "Training loss for batch 2981 : 0.42497003078460693\n",
      "Training loss for batch 2982 : 0.2102634757757187\n",
      "Training loss for batch 2983 : 0.07376942783594131\n",
      "Training loss for batch 2984 : 0.10960366576910019\n",
      "Training loss for batch 2985 : 0.12937518954277039\n",
      "Training loss for batch 2986 : 0.16663968563079834\n",
      "Training loss for batch 2987 : 0.02516913041472435\n",
      "Training loss for batch 2988 : 0.25223425030708313\n",
      "Training loss for batch 2989 : 0.20705130696296692\n",
      "Training loss for batch 2990 : 0.04854806885123253\n",
      "Training loss for batch 2991 : 0.1075754463672638\n",
      "Training loss for batch 2992 : 0.2076854556798935\n",
      "Training loss for batch 2993 : 0.019216284155845642\n",
      "Training loss for batch 2994 : 0.26556825637817383\n",
      "Training loss for batch 2995 : 0.2953340411186218\n",
      "Training loss for batch 2996 : 0.0987989604473114\n",
      "Training loss for batch 2997 : 0.04501863941550255\n",
      "Training loss for batch 2998 : 0.18319810926914215\n",
      "Training loss for batch 2999 : 0.3353114426136017\n",
      "Training loss for batch 3000 : 0.17042876780033112\n",
      "Training loss for batch 3001 : 0.013915988616645336\n",
      "Training loss for batch 3002 : -0.0008564990712329745\n",
      "Training loss for batch 3003 : 0.09657630324363708\n",
      "Training loss for batch 3004 : 0.07139956206083298\n",
      "Training loss for batch 3005 : 0.046157628297805786\n",
      "Training loss for batch 3006 : 0.2003905177116394\n",
      "Training loss for batch 3007 : 0.13584885001182556\n",
      "Training loss for batch 3008 : 0.10706790536642075\n",
      "Training loss for batch 3009 : 0.02804075926542282\n",
      "Training loss for batch 3010 : 0.023589203134179115\n",
      "Training loss for batch 3011 : 0.11327731609344482\n",
      "Training loss for batch 3012 : 0.12068133801221848\n",
      "Training loss for batch 3013 : 0.09304331243038177\n",
      "Training loss for batch 3014 : 0.016741137951612473\n",
      "Training loss for batch 3015 : 0.2557731866836548\n",
      "Training loss for batch 3016 : 0.3957259953022003\n",
      "Training loss for batch 3017 : 0.3753631114959717\n",
      "Training loss for batch 3018 : 0.05353032797574997\n",
      "Training loss for batch 3019 : 0.20752106606960297\n",
      "Training loss for batch 3020 : 0.06851203739643097\n",
      "Training loss for batch 3021 : 0.08637513965368271\n",
      "Training loss for batch 3022 : 0.060896437615156174\n",
      "Training loss for batch 3023 : 0.12462250888347626\n",
      "Training loss for batch 3024 : 0.10815031826496124\n",
      "Training loss for batch 3025 : 0.1637817919254303\n",
      "Training loss for batch 3026 : 0.1761116236448288\n",
      "Training loss for batch 3027 : 0.11458497494459152\n",
      "Training loss for batch 3028 : 0.08061940968036652\n",
      "Training loss for batch 3029 : 0.28580647706985474\n",
      "Training loss for batch 3030 : 0.12897278368473053\n",
      "Training loss for batch 3031 : 0.413944810628891\n",
      "Training loss for batch 3032 : 0.0562233068048954\n",
      "Training loss for batch 3033 : 0.04198712483048439\n",
      "Training loss for batch 3034 : 0.0855775773525238\n",
      "Training loss for batch 3035 : 0.11195266246795654\n",
      "Training loss for batch 3036 : 0.19732744991779327\n",
      "Training loss for batch 3037 : 0.3710395395755768\n",
      "Training loss for batch 3038 : 0.126836359500885\n",
      "Training loss for batch 3039 : 0.03481695055961609\n",
      "Training loss for batch 3040 : 0.3076338469982147\n",
      "Training loss for batch 3041 : 0.039437174797058105\n",
      "Training loss for batch 3042 : 0.1242867186665535\n",
      "Training loss for batch 3043 : 0.19328223168849945\n",
      "Training loss for batch 3044 : 0.23112371563911438\n",
      "Training loss for batch 3045 : 0.017709597945213318\n",
      "Training loss for batch 3046 : 0.3442285656929016\n",
      "Training loss for batch 3047 : 0.104472815990448\n",
      "Training loss for batch 3048 : 0.0717044398188591\n",
      "Training loss for batch 3049 : 0.19832175970077515\n",
      "Training loss for batch 3050 : 0.6455423831939697\n",
      "Training loss for batch 3051 : 0.2254069596529007\n",
      "Training loss for batch 3052 : 0.6775593757629395\n",
      "Training loss for batch 3053 : 0.06454623490571976\n",
      "Training loss for batch 3054 : 0.020290371030569077\n",
      "Training loss for batch 3055 : 0.14197318255901337\n",
      "Training loss for batch 3056 : 0.07635237276554108\n",
      "Training loss for batch 3057 : 0.025047456845641136\n",
      "Training loss for batch 3058 : 0.07003287225961685\n",
      "Training loss for batch 3059 : 0.02155781351029873\n",
      "Training loss for batch 3060 : 0.45660102367401123\n",
      "Training loss for batch 3061 : 0.10388980060815811\n",
      "Training loss for batch 3062 : 0.08741462230682373\n",
      "Training loss for batch 3063 : 0.2750818133354187\n",
      "Training loss for batch 3064 : 0.3244917392730713\n",
      "Training loss for batch 3065 : 0.07424502074718475\n",
      "Training loss for batch 3066 : 0.06910331547260284\n",
      "Training loss for batch 3067 : 0.2132740467786789\n",
      "Training loss for batch 3068 : 0.4025063216686249\n",
      "Training loss for batch 3069 : 0.23227939009666443\n",
      "Training loss for batch 3070 : 0.10076066851615906\n",
      "Training loss for batch 3071 : 0.1054602861404419\n",
      "Training loss for batch 3072 : 0.014127145521342754\n",
      "Training loss for batch 3073 : 0.02725563757121563\n",
      "Training loss for batch 3074 : 0.08072719722986221\n",
      "Training loss for batch 3075 : 0.16922208666801453\n",
      "Training loss for batch 3076 : 0.3060164153575897\n",
      "Training loss for batch 3077 : 0.1254480481147766\n",
      "Training loss for batch 3078 : 0.126914843916893\n",
      "Training loss for batch 3079 : 0.09920596331357956\n",
      "Training loss for batch 3080 : 0.0397719107568264\n",
      "Training loss for batch 3081 : 0.13836820423603058\n",
      "Training loss for batch 3082 : 0.32757502794265747\n",
      "Training loss for batch 3083 : 0.2604352831840515\n",
      "Training loss for batch 3084 : 0.14798027276992798\n",
      "Training loss for batch 3085 : 0.10962976515293121\n",
      "Training loss for batch 3086 : 0.2137533575296402\n",
      "Training loss for batch 3087 : 0.025439366698265076\n",
      "Training loss for batch 3088 : 0.04974471032619476\n",
      "Training loss for batch 3089 : 0.3379240036010742\n",
      "Training loss for batch 3090 : 0.015283625572919846\n",
      "Training loss for batch 3091 : 0.24434509873390198\n",
      "Training loss for batch 3092 : 0.027696043252944946\n",
      "Training loss for batch 3093 : 0.17070560157299042\n",
      "Training loss for batch 3094 : 0.13089801371097565\n",
      "Training loss for batch 3095 : 0.04225553944706917\n",
      "Training loss for batch 3096 : 0.09736235439777374\n",
      "Training loss for batch 3097 : 0.06675408035516739\n",
      "Training loss for batch 3098 : 0.3057510256767273\n",
      "Training loss for batch 3099 : 0.21600322425365448\n",
      "Training loss for batch 3100 : 0.18100318312644958\n",
      "Training loss for batch 3101 : 0.22748436033725739\n",
      "Training loss for batch 3102 : 0.056616269052028656\n",
      "Training loss for batch 3103 : 0.10682392120361328\n",
      "Training loss for batch 3104 : 0.06845291703939438\n",
      "Training loss for batch 3105 : 0.12737780809402466\n",
      "Training loss for batch 3106 : 0.12374673783779144\n",
      "Training loss for batch 3107 : 0.16745074093341827\n",
      "Training loss for batch 3108 : 0.17189013957977295\n",
      "Training loss for batch 3109 : 0.035352956503629684\n",
      "Training loss for batch 3110 : 0.09179745614528656\n",
      "Training loss for batch 3111 : 0.09816557914018631\n",
      "Training loss for batch 3112 : 0.09316136687994003\n",
      "Training loss for batch 3113 : 0.0014401774387806654\n",
      "Training loss for batch 3114 : 0.0249432735145092\n",
      "Training loss for batch 3115 : 0.1565052568912506\n",
      "Training loss for batch 3116 : 0.19152075052261353\n",
      "Training loss for batch 3117 : 0.25326451659202576\n",
      "Training loss for batch 3118 : 0.25504112243652344\n",
      "Training loss for batch 3119 : 0.1967194676399231\n",
      "Training loss for batch 3120 : 0.37823620438575745\n",
      "Training loss for batch 3121 : 0.06550997495651245\n",
      "Training loss for batch 3122 : 0.1958048939704895\n",
      "Training loss for batch 3123 : -0.003749025287106633\n",
      "Training loss for batch 3124 : 0.24571116268634796\n",
      "Training loss for batch 3125 : 0.3350854516029358\n",
      "Training loss for batch 3126 : 0.018805742263793945\n",
      "Training loss for batch 3127 : 0.3471361994743347\n",
      "Training loss for batch 3128 : 0.030407922342419624\n",
      "Training loss for batch 3129 : 0.2761121392250061\n",
      "Training loss for batch 3130 : 0.007731563877314329\n",
      "Training loss for batch 3131 : 0.1246967539191246\n",
      "Training loss for batch 3132 : 0.13197973370552063\n",
      "Training loss for batch 3133 : 0.03468148037791252\n",
      "Training loss for batch 3134 : 0.05266086012125015\n",
      "Training loss for batch 3135 : 0.17790579795837402\n",
      "Training loss for batch 3136 : 0.17362263798713684\n",
      "Training loss for batch 3137 : 0.05960560217499733\n",
      "Training loss for batch 3138 : 0.15501569211483002\n",
      "Training loss for batch 3139 : 0.17765313386917114\n",
      "Training loss for batch 3140 : 0.3173849284648895\n",
      "Training loss for batch 3141 : 0.20991086959838867\n",
      "Training loss for batch 3142 : 0.21278803050518036\n",
      "Training loss for batch 3143 : 0.08700251579284668\n",
      "Training loss for batch 3144 : 0.09118648618459702\n",
      "Training loss for batch 3145 : 0.12994727492332458\n",
      "Training loss for batch 3146 : 0.1318666785955429\n",
      "Training loss for batch 3147 : 0.19146624207496643\n",
      "Training loss for batch 3148 : 0.1430039256811142\n",
      "Training loss for batch 3149 : 0.18936575949192047\n",
      "Training loss for batch 3150 : 0.011985823512077332\n",
      "Training loss for batch 3151 : 0.15511786937713623\n",
      "Training loss for batch 3152 : 0.03531545028090477\n",
      "Training loss for batch 3153 : 0.17076103389263153\n",
      "Training loss for batch 3154 : 0.1254911720752716\n",
      "Training loss for batch 3155 : 0.1394582986831665\n",
      "Training loss for batch 3156 : 0.06997079402208328\n",
      "Training loss for batch 3157 : 0.17219287157058716\n",
      "Training loss for batch 3158 : 0.21612396836280823\n",
      "Training loss for batch 3159 : 0.04407274350523949\n",
      "Training loss for batch 3160 : 0.020052693784236908\n",
      "Training loss for batch 3161 : 0.021095087751746178\n",
      "Training loss for batch 3162 : 0.28379297256469727\n",
      "Training loss for batch 3163 : 0.2686452567577362\n",
      "Training loss for batch 3164 : 0.34194469451904297\n",
      "Training loss for batch 3165 : 0.22808820009231567\n",
      "Training loss for batch 3166 : 0.049508340656757355\n",
      "Training loss for batch 3167 : 0.22210685908794403\n",
      "Training loss for batch 3168 : 0.15769529342651367\n",
      "Training loss for batch 3169 : 0.029747724533081055\n",
      "Training loss for batch 3170 : 0.0142314238473773\n",
      "Training loss for batch 3171 : 0.04722442105412483\n",
      "Training loss for batch 3172 : 0.27833813428878784\n",
      "Training loss for batch 3173 : 0.05837973952293396\n",
      "Training loss for batch 3174 : 0.3808589577674866\n",
      "Training loss for batch 3175 : 0.17281724512577057\n",
      "Training loss for batch 3176 : 0.17582114040851593\n",
      "Training loss for batch 3177 : 0.09746137261390686\n",
      "Training loss for batch 3178 : 0.12436412274837494\n",
      "Training loss for batch 3179 : 0.1355583369731903\n",
      "Training loss for batch 3180 : 0.09294881671667099\n",
      "Training loss for batch 3181 : 0.4528891444206238\n",
      "Training loss for batch 3182 : 0.12542690336704254\n",
      "Training loss for batch 3183 : 0.3321100175380707\n",
      "Training loss for batch 3184 : 0.2871215045452118\n",
      "Training loss for batch 3185 : 0.3633471131324768\n",
      "Training loss for batch 3186 : 0.3338391184806824\n",
      "Training loss for batch 3187 : 0.0024366378784179688\n",
      "Training loss for batch 3188 : 0.08898814022541046\n",
      "Training loss for batch 3189 : 0.12086012959480286\n",
      "Training loss for batch 3190 : 0.15492360293865204\n",
      "Training loss for batch 3191 : 0.1014220342040062\n",
      "Training loss for batch 3192 : 0.11980310827493668\n",
      "Training loss for batch 3193 : 0.07312793284654617\n",
      "Training loss for batch 3194 : 0.1308295726776123\n",
      "Training loss for batch 3195 : 0.18996334075927734\n",
      "Training loss for batch 3196 : 0.4491318166255951\n",
      "Training loss for batch 3197 : 0.1308109611272812\n",
      "Training loss for batch 3198 : 0.19578056037425995\n",
      "Training loss for batch 3199 : 0.05944542959332466\n",
      "Training loss for batch 3200 : 0.20656050741672516\n",
      "Training loss for batch 3201 : 0.22615191340446472\n",
      "Training loss for batch 3202 : 0.06792980432510376\n",
      "Training loss for batch 3203 : 0.182672381401062\n",
      "Training loss for batch 3204 : 0.1203259527683258\n",
      "Training loss for batch 3205 : 0.21740342676639557\n",
      "Training loss for batch 3206 : 0.02827734500169754\n",
      "Training loss for batch 3207 : 0.06505155563354492\n",
      "Training loss for batch 3208 : 0.2846757173538208\n",
      "Training loss for batch 3209 : 0.0\n",
      "Training loss for batch 3210 : 0.20327405631542206\n",
      "Training loss for batch 3211 : 0.19442731142044067\n",
      "Training loss for batch 3212 : 0.05836721137166023\n",
      "Training loss for batch 3213 : 0.04485970735549927\n",
      "Training loss for batch 3214 : 0.006838592234998941\n",
      "Training loss for batch 3215 : 0.02940552681684494\n",
      "Training loss for batch 3216 : 0.04283415153622627\n",
      "Training loss for batch 3217 : 0.07926926016807556\n",
      "Training loss for batch 3218 : 0.18993644416332245\n",
      "Training loss for batch 3219 : 0.20387709140777588\n",
      "Training loss for batch 3220 : 0.27180588245391846\n",
      "Training loss for batch 3221 : 0.12674178183078766\n",
      "Training loss for batch 3222 : 0.12739147245883942\n",
      "Training loss for batch 3223 : 0.05014992132782936\n",
      "Training loss for batch 3224 : 0.14428293704986572\n",
      "Training loss for batch 3225 : 0.054686035960912704\n",
      "Training loss for batch 3226 : 0.40414920449256897\n",
      "Training loss for batch 3227 : 0.21619467437267303\n",
      "Training loss for batch 3228 : 0.01929398626089096\n",
      "Training loss for batch 3229 : 0.22130899131298065\n",
      "Training loss for batch 3230 : 0.12535779178142548\n",
      "Training loss for batch 3231 : 0.028954356908798218\n",
      "Training loss for batch 3232 : 0.01621370203793049\n",
      "Training loss for batch 3233 : 0.47034841775894165\n",
      "Training loss for batch 3234 : 0.08371728658676147\n",
      "Training loss for batch 3235 : 0.06676427274942398\n",
      "Training loss for batch 3236 : 0.027567066252231598\n",
      "Training loss for batch 3237 : 0.148306742310524\n",
      "Training loss for batch 3238 : 0.23873679339885712\n",
      "Training loss for batch 3239 : 0.3405698239803314\n",
      "Training loss for batch 3240 : 0.11149606108665466\n",
      "Training loss for batch 3241 : 0.11661579459905624\n",
      "Training loss for batch 3242 : 0.13991034030914307\n",
      "Training loss for batch 3243 : 0.24246425926685333\n",
      "Training loss for batch 3244 : 0.07654058933258057\n",
      "Training loss for batch 3245 : 0.0287556741386652\n",
      "Training loss for batch 3246 : 0.29076138138771057\n",
      "Training loss for batch 3247 : 0.07476871460676193\n",
      "Training loss for batch 3248 : 0.03941873833537102\n",
      "Training loss for batch 3249 : 0.03198966011404991\n",
      "Training loss for batch 3250 : 0.06251897662878036\n",
      "Training loss for batch 3251 : 0.06937051564455032\n",
      "Training loss for batch 3252 : 0.10983729362487793\n",
      "Training loss for batch 3253 : 0.17922858893871307\n",
      "Training loss for batch 3254 : 0.0\n",
      "Training loss for batch 3255 : 0.16947677731513977\n",
      "Training loss for batch 3256 : 0.1684294492006302\n",
      "Training loss for batch 3257 : 0.22825387120246887\n",
      "Training loss for batch 3258 : 0.07165417075157166\n",
      "Training loss for batch 3259 : 0.05599043518304825\n",
      "Training loss for batch 3260 : 0.09857402741909027\n",
      "Training loss for batch 3261 : 0.3126976788043976\n",
      "Training loss for batch 3262 : 0.056843142956495285\n",
      "Training loss for batch 3263 : 0.2975784242153168\n",
      "Training loss for batch 3264 : 0.07753755897283554\n",
      "Training loss for batch 3265 : 0.1634216010570526\n",
      "Training loss for batch 3266 : 0.13910739123821259\n",
      "Training loss for batch 3267 : 0.08834615349769592\n",
      "Training loss for batch 3268 : 0.189583420753479\n",
      "Training loss for batch 3269 : 0.0896952822804451\n",
      "Training loss for batch 3270 : 0.08998022973537445\n",
      "Training loss for batch 3271 : 0.46659088134765625\n",
      "Training loss for batch 3272 : 0.025782814249396324\n",
      "Training loss for batch 3273 : 0.13486209511756897\n",
      "Training loss for batch 3274 : 0.1476602554321289\n",
      "Training loss for batch 3275 : 0.1487516164779663\n",
      "Training loss for batch 3276 : 0.13412204384803772\n",
      "Training loss for batch 3277 : 0.307181715965271\n",
      "Training loss for batch 3278 : 0.011061115190386772\n",
      "Training loss for batch 3279 : 0.07606257498264313\n",
      "Training loss for batch 3280 : 0.3577125072479248\n",
      "Training loss for batch 3281 : 0.2683378756046295\n",
      "Training loss for batch 3282 : 0.19231200218200684\n",
      "Training loss for batch 3283 : 0.1678554266691208\n",
      "Training loss for batch 3284 : 0.09075327217578888\n",
      "Training loss for batch 3285 : 0.23345603048801422\n",
      "Training loss for batch 3286 : 0.05669402331113815\n",
      "Training loss for batch 3287 : 0.03230225667357445\n",
      "Training loss for batch 3288 : 0.0919809341430664\n",
      "Training loss for batch 3289 : 0.12917515635490417\n",
      "Training loss for batch 3290 : 0.18850596249103546\n",
      "Training loss for batch 3291 : 0.041985828429460526\n",
      "Training loss for batch 3292 : 0.05634555220603943\n",
      "Training loss for batch 3293 : 0.06058884039521217\n",
      "Training loss for batch 3294 : 0.1731456071138382\n",
      "Training loss for batch 3295 : 0.39201903343200684\n",
      "Training loss for batch 3296 : 0.19937138259410858\n",
      "Training loss for batch 3297 : 0.22673659026622772\n",
      "Training loss for batch 3298 : 0.1903257966041565\n",
      "Training loss for batch 3299 : 0.1005164310336113\n",
      "Training loss for batch 3300 : 0.08932708203792572\n",
      "Training loss for batch 3301 : 0.16734905540943146\n",
      "Training loss for batch 3302 : 0.08980041742324829\n",
      "Training loss for batch 3303 : 0.09057179093360901\n",
      "Training loss for batch 3304 : -0.00357171637006104\n",
      "Training loss for batch 3305 : 0.2061733901500702\n",
      "Training loss for batch 3306 : 0.26399216055870056\n",
      "Training loss for batch 3307 : 0.1652611792087555\n",
      "Training loss for batch 3308 : 0.10074444860219955\n",
      "Training loss for batch 3309 : 0.04684516042470932\n",
      "Training loss for batch 3310 : 0.13805589079856873\n",
      "Training loss for batch 3311 : 0.3325902819633484\n",
      "Training loss for batch 3312 : 0.1242232471704483\n",
      "Training loss for batch 3313 : 0.12994730472564697\n",
      "Training loss for batch 3314 : 0.29477155208587646\n",
      "Training loss for batch 3315 : 0.12122606486082077\n",
      "Training loss for batch 3316 : 0.14215292036533356\n",
      "Training loss for batch 3317 : 0.01541208103299141\n",
      "Training loss for batch 3318 : 0.039379216730594635\n",
      "Training loss for batch 3319 : 0.06712932139635086\n",
      "Training loss for batch 3320 : 0.1129845529794693\n",
      "Training loss for batch 3321 : 0.09796527773141861\n",
      "Training loss for batch 3322 : 0.36506280303001404\n",
      "Training loss for batch 3323 : 0.12084654718637466\n",
      "Training loss for batch 3324 : 0.1474342942237854\n",
      "Training loss for batch 3325 : 0.06545531004667282\n",
      "Training loss for batch 3326 : 0.11558246612548828\n",
      "Training loss for batch 3327 : 0.11082224547863007\n",
      "Training loss for batch 3328 : 0.49920520186424255\n",
      "Training loss for batch 3329 : 0.06718374043703079\n",
      "Training loss for batch 3330 : 0.2547927498817444\n",
      "Training loss for batch 3331 : 0.09376133233308792\n",
      "Training loss for batch 3332 : 0.07001224160194397\n",
      "Training loss for batch 3333 : 0.0748809427022934\n",
      "Training loss for batch 3334 : 0.08246675878763199\n",
      "Training loss for batch 3335 : 0.07427794486284256\n",
      "Training loss for batch 3336 : 0.05593036115169525\n",
      "Training loss for batch 3337 : 0.07285565882921219\n",
      "Training loss for batch 3338 : 0.10706937313079834\n",
      "Training loss for batch 3339 : 0.1202077642083168\n",
      "Training loss for batch 3340 : 0.11187906563282013\n",
      "Training loss for batch 3341 : 0.10454022884368896\n",
      "Training loss for batch 3342 : 0.0693083181977272\n",
      "Training loss for batch 3343 : 0.09028571099042892\n",
      "Training loss for batch 3344 : 0.016586225479841232\n",
      "Training loss for batch 3345 : 0.09211147576570511\n",
      "Training loss for batch 3346 : 0.10918424278497696\n",
      "Training loss for batch 3347 : 0.1278415322303772\n",
      "Training loss for batch 3348 : 0.10636824369430542\n",
      "Training loss for batch 3349 : 0.34059464931488037\n",
      "Training loss for batch 3350 : 0.3378003239631653\n",
      "Training loss for batch 3351 : 0.3736811876296997\n",
      "Training loss for batch 3352 : 0.1518927365541458\n",
      "Training loss for batch 3353 : 0.37526318430900574\n",
      "Training loss for batch 3354 : 0.20071378350257874\n",
      "Training loss for batch 3355 : 0.2495376169681549\n",
      "Training loss for batch 3356 : 0.11040325462818146\n",
      "Training loss for batch 3357 : 0.16477739810943604\n",
      "Training loss for batch 3358 : 0.0\n",
      "Training loss for batch 3359 : 0.06662529706954956\n",
      "Training loss for batch 3360 : 0.1413920670747757\n",
      "Training loss for batch 3361 : 0.07987279444932938\n",
      "Training loss for batch 3362 : 0.0011256984435021877\n",
      "Training loss for batch 3363 : 0.2604219615459442\n",
      "Training loss for batch 3364 : 0.060094594955444336\n",
      "Training loss for batch 3365 : 0.2091953009366989\n",
      "Training loss for batch 3366 : 0.015375783666968346\n",
      "Training loss for batch 3367 : 0.053518157452344894\n",
      "Training loss for batch 3368 : 0.14949221909046173\n",
      "Training loss for batch 3369 : 0.23947548866271973\n",
      "Training loss for batch 3370 : 0.2571932077407837\n",
      "Training loss for batch 3371 : 0.07991379499435425\n",
      "Training loss for batch 3372 : 0.5611428618431091\n",
      "Training loss for batch 3373 : 0.11111100018024445\n",
      "Training loss for batch 3374 : 0.1200120821595192\n",
      "Training loss for batch 3375 : 0.0680878609418869\n",
      "Training loss for batch 3376 : 0.1698954552412033\n",
      "Training loss for batch 3377 : 0.25317463278770447\n",
      "Training loss for batch 3378 : 0.1623157411813736\n",
      "Training loss for batch 3379 : 0.10289973765611649\n",
      "Training loss for batch 3380 : 0.16890737414360046\n",
      "Training loss for batch 3381 : 0.35938698053359985\n",
      "Training loss for batch 3382 : 0.30673858523368835\n",
      "Training loss for batch 3383 : 0.18928936123847961\n",
      "Training loss for batch 3384 : 0.1734256148338318\n",
      "Training loss for batch 3385 : 0.19175508618354797\n",
      "Training loss for batch 3386 : 0.19899338483810425\n",
      "Training loss for batch 3387 : 0.2385430932044983\n",
      "Training loss for batch 3388 : 0.2559482753276825\n",
      "Training loss for batch 3389 : 0.06338739395141602\n",
      "Training loss for batch 3390 : 0.028563911095261574\n",
      "Training loss for batch 3391 : 0.0031613975297659636\n",
      "Training loss for batch 3392 : 0.04393034800887108\n",
      "Training loss for batch 3393 : 0.13510842621326447\n",
      "Training loss for batch 3394 : 0.08799099177122116\n",
      "Training loss for batch 3395 : 0.0275246724486351\n",
      "Training loss for batch 3396 : 0.225759357213974\n",
      "Training loss for batch 3397 : 0.13609665632247925\n",
      "Training loss for batch 3398 : 0.048106878995895386\n",
      "Training loss for batch 3399 : 0.1167193353176117\n",
      "Training loss for batch 3400 : 0.14504331350326538\n",
      "Training loss for batch 3401 : 0.3436621427536011\n",
      "Training loss for batch 3402 : 0.1223447248339653\n",
      "Training loss for batch 3403 : 0.08910637348890305\n",
      "Training loss for batch 3404 : 0.026245087385177612\n",
      "Training loss for batch 3405 : 0.16601216793060303\n",
      "Training loss for batch 3406 : 0.264774888753891\n",
      "Training loss for batch 3407 : 0.06190405413508415\n",
      "Training loss for batch 3408 : 0.32086533308029175\n",
      "Training loss for batch 3409 : 0.14489588141441345\n",
      "Training loss for batch 3410 : 0.003141691442579031\n",
      "Training loss for batch 3411 : 0.24824638664722443\n",
      "Training loss for batch 3412 : 0.10635756701231003\n",
      "Training loss for batch 3413 : 0.3000239133834839\n",
      "Training loss for batch 3414 : 0.10427282005548477\n",
      "Training loss for batch 3415 : 0.4234553575515747\n",
      "Training loss for batch 3416 : 0.11305208504199982\n",
      "Training loss for batch 3417 : 0.15595385432243347\n",
      "Training loss for batch 3418 : 0.15863284468650818\n",
      "Training loss for batch 3419 : 0.2731969952583313\n",
      "Training loss for batch 3420 : 0.004054705612361431\n",
      "Training loss for batch 3421 : 0.09394165873527527\n",
      "Training loss for batch 3422 : 0.04807048663496971\n",
      "Training loss for batch 3423 : 0.016369245946407318\n",
      "Training loss for batch 3424 : 0.13291478157043457\n",
      "Training loss for batch 3425 : 0.23116235435009003\n",
      "Training loss for batch 3426 : 0.12091632187366486\n",
      "Training loss for batch 3427 : 0.13209721446037292\n",
      "Training loss for batch 3428 : 0.17398318648338318\n",
      "Training loss for batch 3429 : 0.0013345216866582632\n",
      "Training loss for batch 3430 : 0.09248233586549759\n",
      "Training loss for batch 3431 : 0.046752069145441055\n",
      "Training loss for batch 3432 : 0.35475361347198486\n",
      "Training loss for batch 3433 : 0.10738649219274521\n",
      "Training loss for batch 3434 : 0.2842164933681488\n",
      "Training loss for batch 3435 : 0.13349401950836182\n",
      "Training loss for batch 3436 : 0.1133919507265091\n",
      "Training loss for batch 3437 : 0.05958021059632301\n",
      "Training loss for batch 3438 : 0.42055657505989075\n",
      "Training loss for batch 3439 : 0.1492394655942917\n",
      "Training loss for batch 3440 : 0.3144471347332001\n",
      "Training loss for batch 3441 : 0.14336559176445007\n",
      "Training loss for batch 3442 : 0.1917542964220047\n",
      "Training loss for batch 3443 : 0.17730200290679932\n",
      "Training loss for batch 3444 : 0.1890791952610016\n",
      "Training loss for batch 3445 : 0.1724371761083603\n",
      "Training loss for batch 3446 : 0.1565450131893158\n",
      "Training loss for batch 3447 : 0.09619341045618057\n",
      "Training loss for batch 3448 : 0.06197470426559448\n",
      "Training loss for batch 3449 : 0.10008242726325989\n",
      "Training loss for batch 3450 : 0.29614579677581787\n",
      "Training loss for batch 3451 : 0.1371292620897293\n",
      "Training loss for batch 3452 : 0.06862394511699677\n",
      "Training loss for batch 3453 : 0.1020202562212944\n",
      "Training loss for batch 3454 : 0.1354876309633255\n",
      "Training loss for batch 3455 : 0.08742744475603104\n",
      "Training loss for batch 3456 : 0.22088749706745148\n",
      "Training loss for batch 3457 : 0.058483537286520004\n",
      "Training loss for batch 3458 : 0.38578787446022034\n",
      "Training loss for batch 3459 : 0.2086334526538849\n",
      "Training loss for batch 3460 : 0.1268485188484192\n",
      "Training loss for batch 3461 : 0.1974053978919983\n",
      "Training loss for batch 3462 : 0.30594462156295776\n",
      "Training loss for batch 3463 : 0.24167639017105103\n",
      "Training loss for batch 3464 : 0.24702799320220947\n",
      "Training loss for batch 3465 : 0.3792040944099426\n",
      "Training loss for batch 3466 : 0.03581977263092995\n",
      "Training loss for batch 3467 : 0.054261352866888046\n",
      "Training loss for batch 3468 : 0.0\n",
      "Training loss for batch 3469 : 0.45348572731018066\n",
      "Training loss for batch 3470 : 0.10531006008386612\n",
      "Training loss for batch 3471 : 0.12420590221881866\n",
      "Training loss for batch 3472 : 0.1561937928199768\n",
      "Training loss for batch 3473 : 0.21925556659698486\n",
      "Training loss for batch 3474 : 0.23163220286369324\n",
      "Training loss for batch 3475 : 0.04816260188817978\n",
      "Training loss for batch 3476 : 0.07333944737911224\n",
      "Training loss for batch 3477 : 0.1941743940114975\n",
      "Training loss for batch 3478 : 0.07938522845506668\n",
      "Training loss for batch 3479 : 0.07582450658082962\n",
      "Training loss for batch 3480 : 0.2635236978530884\n",
      "Training loss for batch 3481 : 0.08792778849601746\n",
      "Training loss for batch 3482 : 0.39451128244400024\n",
      "Training loss for batch 3483 : 0.008188605308532715\n",
      "Training loss for batch 3484 : 0.07264520972967148\n",
      "Training loss for batch 3485 : 0.025792621076107025\n",
      "Training loss for batch 3486 : 0.09764847904443741\n",
      "Training loss for batch 3487 : 0.02738412283360958\n",
      "Training loss for batch 3488 : 0.36378213763237\n",
      "Training loss for batch 3489 : 0.06796576827764511\n",
      "Training loss for batch 3490 : 0.033642712980508804\n",
      "Training loss for batch 3491 : 0.03264594078063965\n",
      "Training loss for batch 3492 : 0.007184440270066261\n",
      "Training loss for batch 3493 : 0.01097378134727478\n",
      "Training loss for batch 3494 : 0.16965597867965698\n",
      "Training loss for batch 3495 : 0.05638531222939491\n",
      "Training loss for batch 3496 : 0.27963557839393616\n",
      "Training loss for batch 3497 : 0.11375770717859268\n",
      "Training loss for batch 3498 : 0.05214864760637283\n",
      "Training loss for batch 3499 : 0.22350971400737762\n",
      "Training loss for batch 3500 : 0.10708048194646835\n",
      "Training loss for batch 3501 : 0.2155742049217224\n",
      "Training loss for batch 3502 : 0.0022814671974629164\n",
      "Training loss for batch 3503 : 0.23384055495262146\n",
      "Training loss for batch 3504 : 0.17394769191741943\n",
      "Training loss for batch 3505 : 0.028744401410222054\n",
      "Training loss for batch 3506 : 0.06652196496725082\n",
      "Training loss for batch 3507 : 0.1106765940785408\n",
      "Training loss for batch 3508 : 0.29764312505722046\n",
      "Training loss for batch 3509 : 0.05213366821408272\n",
      "Training loss for batch 3510 : 0.2631251811981201\n",
      "Training loss for batch 3511 : 0.04364572465419769\n",
      "Training loss for batch 3512 : 0.09948118031024933\n",
      "Training loss for batch 3513 : 0.010650834068655968\n",
      "Training loss for batch 3514 : 0.24459600448608398\n",
      "Training loss for batch 3515 : 0.3270760178565979\n",
      "Training loss for batch 3516 : 0.08103519678115845\n",
      "Training loss for batch 3517 : 0.0020591961219906807\n",
      "Training loss for batch 3518 : 0.1201149970293045\n",
      "Training loss for batch 3519 : 0.02314067631959915\n",
      "Training loss for batch 3520 : 0.2228117734193802\n",
      "Training loss for batch 3521 : 0.3371487259864807\n",
      "Training loss for batch 3522 : 0.099422886967659\n",
      "Training loss for batch 3523 : 0.055185940116643906\n",
      "Training loss for batch 3524 : 0.14009319245815277\n",
      "Training loss for batch 3525 : 0.271487295627594\n",
      "Training loss for batch 3526 : 0.10076427459716797\n",
      "Training loss for batch 3527 : 0.09338226914405823\n",
      "Training loss for batch 3528 : 0.13992293179035187\n",
      "Training loss for batch 3529 : 0.042818062007427216\n",
      "Training loss for batch 3530 : 0.1637517362833023\n",
      "Training loss for batch 3531 : 0.10725661367177963\n",
      "Training loss for batch 3532 : 0.19076593220233917\n",
      "Training loss for batch 3533 : 0.08452461659908295\n",
      "Training loss for batch 3534 : 0.16188260912895203\n",
      "Training loss for batch 3535 : 0.3583437204360962\n",
      "Training loss for batch 3536 : 0.22521598637104034\n",
      "Training loss for batch 3537 : 0.1059219017624855\n",
      "Training loss for batch 3538 : 0.010428623296320438\n",
      "Training loss for batch 3539 : 0.058769382536411285\n",
      "Training loss for batch 3540 : 0.061994265764951706\n",
      "Training loss for batch 3541 : 0.10172576457262039\n",
      "Training loss for batch 3542 : 0.19985266029834747\n",
      "Training loss for batch 3543 : 0.2060174196958542\n",
      "Training loss for batch 3544 : 0.043453726917505264\n",
      "Training loss for batch 3545 : 0.20907442271709442\n",
      "Training loss for batch 3546 : 0.2821633517742157\n",
      "Training loss for batch 3547 : 0.3857276141643524\n",
      "Training loss for batch 3548 : 0.025988703593611717\n",
      "Training loss for batch 3549 : 0.13360683619976044\n",
      "Training loss for batch 3550 : 0.18979346752166748\n",
      "Training loss for batch 3551 : 0.0\n",
      "Training loss for batch 3552 : 0.1509152501821518\n",
      "Training loss for batch 3553 : 0.17995552718639374\n",
      "Training loss for batch 3554 : 0.18181531131267548\n",
      "Training loss for batch 3555 : 0.019277602434158325\n",
      "Training loss for batch 3556 : 0.1600893884897232\n",
      "Training loss for batch 3557 : 0.12379401177167892\n",
      "Training loss for batch 3558 : 0.3800395429134369\n",
      "Training loss for batch 3559 : 0.33606112003326416\n",
      "Training loss for batch 3560 : 0.03939871862530708\n",
      "Training loss for batch 3561 : 0.0495435893535614\n",
      "Training loss for batch 3562 : 0.35252270102500916\n",
      "Training loss for batch 3563 : 0.17507682740688324\n",
      "Training loss for batch 3564 : 0.3603645861148834\n",
      "Training loss for batch 3565 : 0.14617198705673218\n",
      "Training loss for batch 3566 : 0.0\n",
      "Training loss for batch 3567 : 0.07331696152687073\n",
      "Training loss for batch 3568 : 0.19157779216766357\n",
      "Training loss for batch 3569 : 0.1545117348432541\n",
      "Training loss for batch 3570 : 0.025606364011764526\n",
      "Training loss for batch 3571 : 0.12588633596897125\n",
      "Training loss for batch 3572 : 0.01495586521923542\n",
      "Training loss for batch 3573 : 0.17446690797805786\n",
      "Training loss for batch 3574 : 0.12197447568178177\n",
      "Training loss for batch 3575 : 0.11912868916988373\n",
      "Training loss for batch 3576 : 0.36890462040901184\n",
      "Training loss for batch 3577 : 0.12070192396640778\n",
      "Training loss for batch 3578 : 0.007208230905234814\n",
      "Training loss for batch 3579 : 0.11041778326034546\n",
      "Training loss for batch 3580 : 0.16902969777584076\n",
      "Training loss for batch 3581 : 0.06658675521612167\n",
      "Training loss for batch 3582 : 0.1178806945681572\n",
      "Training loss for batch 3583 : 0.29025790095329285\n",
      "Training loss for batch 3584 : 0.09006664901971817\n",
      "Training loss for batch 3585 : 0.07721100002527237\n",
      "Training loss for batch 3586 : 0.10917963087558746\n",
      "Training loss for batch 3587 : 0.01862911507487297\n",
      "Training loss for batch 3588 : 0.3759111762046814\n",
      "Training loss for batch 3589 : 0.14620761573314667\n",
      "Training loss for batch 3590 : 0.173555389046669\n",
      "Training loss for batch 3591 : 0.1844118982553482\n",
      "Training loss for batch 3592 : 0.34351328015327454\n",
      "Training loss for batch 3593 : 0.06873630732297897\n",
      "Training loss for batch 3594 : 0.11041226983070374\n",
      "Training loss for batch 3595 : 0.4032834470272064\n",
      "Training loss for batch 3596 : 0.16600319743156433\n",
      "Training loss for batch 3597 : 0.1709652990102768\n",
      "Training loss for batch 3598 : 0.04744851216673851\n",
      "Training loss for batch 3599 : 0.004475404974073172\n",
      "Training loss for batch 3600 : 0.0\n",
      "Training loss for batch 3601 : 0.13065631687641144\n",
      "Training loss for batch 3602 : 0.20519326627254486\n",
      "Training loss for batch 3603 : 0.4066776633262634\n",
      "Training loss for batch 3604 : 0.09814758598804474\n",
      "Training loss for batch 3605 : 0.06608925759792328\n",
      "Training loss for batch 3606 : 0.02561349794268608\n",
      "Training loss for batch 3607 : 0.17041194438934326\n",
      "Training loss for batch 3608 : 0.09680773317813873\n",
      "Training loss for batch 3609 : 0.4184255301952362\n",
      "Training loss for batch 3610 : 0.09971457719802856\n",
      "Training loss for batch 3611 : 0.2250700145959854\n",
      "Training loss for batch 3612 : 0.037084829062223434\n",
      "Training loss for batch 3613 : 0.13653820753097534\n",
      "Training loss for batch 3614 : 0.044096581637859344\n",
      "Training loss for batch 3615 : 0.1159413605928421\n",
      "Training loss for batch 3616 : 0.10247324407100677\n",
      "Training loss for batch 3617 : 0.04743540287017822\n",
      "Training loss for batch 3618 : 0.15759176015853882\n",
      "Training loss for batch 3619 : 0.14454972743988037\n",
      "Training loss for batch 3620 : 0.04545695334672928\n",
      "Training loss for batch 3621 : 0.20465993881225586\n",
      "Training loss for batch 3622 : 0.2066110074520111\n",
      "Training loss for batch 3623 : 0.36017605662345886\n",
      "Training loss for batch 3624 : 0.47504210472106934\n",
      "Training loss for batch 3625 : 0.05311042442917824\n",
      "Training loss for batch 3626 : 0.06598901003599167\n",
      "Training loss for batch 3627 : 0.01626071333885193\n",
      "Training loss for batch 3628 : 0.12950925529003143\n",
      "Training loss for batch 3629 : 0.0460827462375164\n",
      "Training loss for batch 3630 : 0.05007551237940788\n",
      "Training loss for batch 3631 : 0.1734933853149414\n",
      "Training loss for batch 3632 : 0.29620999097824097\n",
      "Training loss for batch 3633 : 0.36624085903167725\n",
      "Training loss for batch 3634 : 0.242093026638031\n",
      "Training loss for batch 3635 : 0.27446141839027405\n",
      "Training loss for batch 3636 : 0.10112158209085464\n",
      "Training loss for batch 3637 : 0.17364856600761414\n",
      "Training loss for batch 3638 : 0.0741729736328125\n",
      "Training loss for batch 3639 : 0.13851487636566162\n",
      "Training loss for batch 3640 : 0.10508304834365845\n",
      "Training loss for batch 3641 : 0.07052918523550034\n",
      "Training loss for batch 3642 : 0.009612040594220161\n",
      "Training loss for batch 3643 : 0.24570243060588837\n",
      "Training loss for batch 3644 : 0.07577808201313019\n",
      "Training loss for batch 3645 : 0.27373838424682617\n",
      "Training loss for batch 3646 : 0.10429511219263077\n",
      "Training loss for batch 3647 : 0.32100826501846313\n",
      "Training loss for batch 3648 : 0.2344171106815338\n",
      "Training loss for batch 3649 : 0.07434999942779541\n",
      "Training loss for batch 3650 : 0.1956971436738968\n",
      "Training loss for batch 3651 : 0.06540750712156296\n",
      "Training loss for batch 3652 : 0.20900267362594604\n",
      "Training loss for batch 3653 : 0.19505105912685394\n",
      "Training loss for batch 3654 : 0.1956857144832611\n",
      "Training loss for batch 3655 : 0.2364276498556137\n",
      "Training loss for batch 3656 : 0.14385175704956055\n",
      "Training loss for batch 3657 : 0.04156019911170006\n",
      "Training loss for batch 3658 : 0.0007154643535614014\n",
      "Training loss for batch 3659 : 0.0915650799870491\n",
      "Training loss for batch 3660 : 0.0918758362531662\n",
      "Training loss for batch 3661 : 0.08379864692687988\n",
      "Training loss for batch 3662 : 0.09502990543842316\n",
      "Training loss for batch 3663 : 0.3416946232318878\n",
      "Training loss for batch 3664 : 0.09996326267719269\n",
      "Training loss for batch 3665 : 0.26438620686531067\n",
      "Training loss for batch 3666 : 0.0725703090429306\n",
      "Training loss for batch 3667 : -7.08454754203558e-05\n",
      "Training loss for batch 3668 : 0.04443477466702461\n",
      "Training loss for batch 3669 : 0.18947045505046844\n",
      "Training loss for batch 3670 : 0.08022092282772064\n",
      "Training loss for batch 3671 : 0.10997335612773895\n",
      "Training loss for batch 3672 : 0.019223574548959732\n",
      "Training loss for batch 3673 : 0.012883216142654419\n",
      "Training loss for batch 3674 : 0.11035320162773132\n",
      "Training loss for batch 3675 : 0.19693735241889954\n",
      "Training loss for batch 3676 : 0.48023390769958496\n",
      "Training loss for batch 3677 : 0.07243601232767105\n",
      "Training loss for batch 3678 : 0.2639237642288208\n",
      "Training loss for batch 3679 : 0.16594982147216797\n",
      "Training loss for batch 3680 : 0.02221962809562683\n",
      "Training loss for batch 3681 : 0.1914655566215515\n",
      "Training loss for batch 3682 : 0.23790591955184937\n",
      "Training loss for batch 3683 : 0.12038372457027435\n",
      "Training loss for batch 3684 : 0.25500863790512085\n",
      "Training loss for batch 3685 : 0.13611699640750885\n",
      "Training loss for batch 3686 : 0.04783656820654869\n",
      "Training loss for batch 3687 : 0.39504069089889526\n",
      "Training loss for batch 3688 : 0.17777274549007416\n",
      "Training loss for batch 3689 : 0.43736743927001953\n",
      "Training loss for batch 3690 : 0.02757965587079525\n",
      "Training loss for batch 3691 : 0.06332522630691528\n",
      "Training loss for batch 3692 : 0.2286425679922104\n",
      "Training loss for batch 3693 : 0.029096409678459167\n",
      "Training loss for batch 3694 : 0.08974646776914597\n",
      "Training loss for batch 3695 : 0.03587964177131653\n",
      "Training loss for batch 3696 : 0.07703341543674469\n",
      "Training loss for batch 3697 : 0.06441110372543335\n",
      "Training loss for batch 3698 : 0.06668372452259064\n",
      "Training loss for batch 3699 : 0.2907869517803192\n",
      "Training loss for batch 3700 : 0.053074561059474945\n",
      "Training loss for batch 3701 : 0.20043374598026276\n",
      "Training loss for batch 3702 : 0.07181169837713242\n",
      "Training loss for batch 3703 : 0.09126629680395126\n",
      "Training loss for batch 3704 : 0.7387672066688538\n",
      "Training loss for batch 3705 : 0.21705660223960876\n",
      "Training loss for batch 3706 : 0.24644632637500763\n",
      "Training loss for batch 3707 : 0.01869678497314453\n",
      "Training loss for batch 3708 : 0.1600688397884369\n",
      "Training loss for batch 3709 : 0.36427682638168335\n",
      "Training loss for batch 3710 : 0.09448886662721634\n",
      "Training loss for batch 3711 : 0.2867164611816406\n",
      "Training loss for batch 3712 : 0.22097550332546234\n",
      "Training loss for batch 3713 : 0.3852997124195099\n",
      "Training loss for batch 3714 : 0.15093910694122314\n",
      "Training loss for batch 3715 : 0.20327641069889069\n",
      "Training loss for batch 3716 : 0.24214574694633484\n",
      "Training loss for batch 3717 : 0.08293556421995163\n",
      "Training loss for batch 3718 : 0.08243958652019501\n",
      "Training loss for batch 3719 : 0.05949103459715843\n",
      "Training loss for batch 3720 : -0.000633420015219599\n",
      "Training loss for batch 3721 : 0.3406919538974762\n",
      "Training loss for batch 3722 : 0.06030840799212456\n",
      "Training loss for batch 3723 : 0.20098772644996643\n",
      "Training loss for batch 3724 : 0.021340344101190567\n",
      "Training loss for batch 3725 : 0.01575635001063347\n",
      "Training loss for batch 3726 : 0.16825702786445618\n",
      "Training loss for batch 3727 : 0.22367000579833984\n",
      "Training loss for batch 3728 : 0.03924257308244705\n",
      "Training loss for batch 3729 : 0.11788056790828705\n",
      "Training loss for batch 3730 : 0.14020836353302002\n",
      "Training loss for batch 3731 : 0.21310463547706604\n",
      "Training loss for batch 3732 : 0.07841469347476959\n",
      "Training loss for batch 3733 : 0.01341276429593563\n",
      "Training loss for batch 3734 : 0.015423586592078209\n",
      "Training loss for batch 3735 : 0.08783046901226044\n",
      "Training loss for batch 3736 : 0.11538304388523102\n",
      "Training loss for batch 3737 : 0.041958391666412354\n",
      "Training loss for batch 3738 : 0.21548423171043396\n",
      "Training loss for batch 3739 : 0.1595279574394226\n",
      "Training loss for batch 3740 : 0.06998471915721893\n",
      "Training loss for batch 3741 : 0.18602174520492554\n",
      "Training loss for batch 3742 : 0.2664394676685333\n",
      "Training loss for batch 3743 : 0.2204064279794693\n",
      "Training loss for batch 3744 : 0.27382412552833557\n",
      "Training loss for batch 3745 : 0.013814404606819153\n",
      "Training loss for batch 3746 : 0.1678646206855774\n",
      "Training loss for batch 3747 : 0.08044777810573578\n",
      "Training loss for batch 3748 : 0.00867412518709898\n",
      "Training loss for batch 3749 : 0.3300270438194275\n",
      "Training loss for batch 3750 : 0.10605834424495697\n",
      "Training loss for batch 3751 : 0.5162086486816406\n",
      "Training loss for batch 3752 : 0.003503958461806178\n",
      "Training loss for batch 3753 : 0.07852433621883392\n",
      "Training loss for batch 3754 : 0.02112448960542679\n",
      "Training loss for batch 3755 : 0.12724559009075165\n",
      "Training loss for batch 3756 : 0.26306217908859253\n",
      "Training loss for batch 3757 : 0.17718079686164856\n",
      "Training loss for batch 3758 : 0.09236982464790344\n",
      "Training loss for batch 3759 : 0.05644112080335617\n",
      "Training loss for batch 3760 : 0.07930465042591095\n",
      "Training loss for batch 3761 : 0.39936965703964233\n",
      "Training loss for batch 3762 : 0.23533286154270172\n",
      "Training loss for batch 3763 : 0.12839190661907196\n",
      "Training loss for batch 3764 : 0.010063539259135723\n",
      "Training loss for batch 3765 : 0.041602592915296555\n",
      "Training loss for batch 3766 : 0.027079330757260323\n",
      "Training loss for batch 3767 : 0.00693506421521306\n",
      "Training loss for batch 3768 : 0.08954508602619171\n",
      "Training loss for batch 3769 : -0.003846132894977927\n",
      "Training loss for batch 3770 : 0.18480144441127777\n",
      "Training loss for batch 3771 : 0.05624890327453613\n",
      "Training loss for batch 3772 : 0.3123316168785095\n",
      "Training loss for batch 3773 : 0.13052032887935638\n",
      "Training loss for batch 3774 : 0.277504026889801\n",
      "Training loss for batch 3775 : 0.05481256544589996\n",
      "Training loss for batch 3776 : 0.06647735834121704\n",
      "Training loss for batch 3777 : 0.028802603483200073\n",
      "Training loss for batch 3778 : 0.17251044511795044\n",
      "Training loss for batch 3779 : 0.29090186953544617\n",
      "Training loss for batch 3780 : 0.10669153928756714\n",
      "Training loss for batch 3781 : 0.22877494990825653\n",
      "Training loss for batch 3782 : 0.20909082889556885\n",
      "Training loss for batch 3783 : 0.18170513212680817\n",
      "Training loss for batch 3784 : 0.2694517970085144\n",
      "Training loss for batch 3785 : 0.04213542863726616\n",
      "Training loss for batch 3786 : 0.12300864607095718\n",
      "Training loss for batch 3787 : 0.37888991832733154\n",
      "Training loss for batch 3788 : 0.1708364486694336\n",
      "Training loss for batch 3789 : 0.33443525433540344\n",
      "Training loss for batch 3790 : 0.08371928334236145\n",
      "Training loss for batch 3791 : 0.06513991206884384\n",
      "Training loss for batch 3792 : 0.1278950423002243\n",
      "Training loss for batch 3793 : 0.1346944272518158\n",
      "Training loss for batch 3794 : 0.09452027082443237\n",
      "Training loss for batch 3795 : 0.006659199949353933\n",
      "Training loss for batch 3796 : 0.051980823278427124\n",
      "Training loss for batch 3797 : 0.0\n",
      "Training loss for batch 3798 : 0.08807196468114853\n",
      "Training loss for batch 3799 : 0.17911721765995026\n",
      "Training loss for batch 3800 : 0.24153093993663788\n",
      "Training loss for batch 3801 : 0.30045264959335327\n",
      "Training loss for batch 3802 : 0.10654941201210022\n",
      "Training loss for batch 3803 : 0.09964216500520706\n",
      "Training loss for batch 3804 : 0.002451270818710327\n",
      "Training loss for batch 3805 : 0.16241773962974548\n",
      "Training loss for batch 3806 : 0.3460596799850464\n",
      "Training loss for batch 3807 : 0.030384795740246773\n",
      "Training loss for batch 3808 : 0.31054311990737915\n",
      "Training loss for batch 3809 : 0.005635499954223633\n",
      "Training loss for batch 3810 : 0.14943183958530426\n",
      "Training loss for batch 3811 : 0.16560330986976624\n",
      "Training loss for batch 3812 : 0.11765972524881363\n",
      "Training loss for batch 3813 : 0.15409477055072784\n",
      "Training loss for batch 3814 : 0.15795521438121796\n",
      "Training loss for batch 3815 : 0.11045043915510178\n",
      "Training loss for batch 3816 : 0.06710584461688995\n",
      "Training loss for batch 3817 : 0.01813632994890213\n",
      "Training loss for batch 3818 : 0.22620123624801636\n",
      "Training loss for batch 3819 : 0.001047011697664857\n",
      "Training loss for batch 3820 : 0.023166753351688385\n",
      "Training loss for batch 3821 : 0.050267014652490616\n",
      "Training loss for batch 3822 : 0.32752203941345215\n",
      "Training loss for batch 3823 : 0.19510678946971893\n",
      "Training loss for batch 3824 : 0.17519082129001617\n",
      "Training loss for batch 3825 : 0.40021440386772156\n",
      "Training loss for batch 3826 : 0.3230794370174408\n",
      "Training loss for batch 3827 : 0.013530060648918152\n",
      "Training loss for batch 3828 : 0.023395411670207977\n",
      "Training loss for batch 3829 : 0.04104835167527199\n",
      "Training loss for batch 3830 : 0.029353637248277664\n",
      "Training loss for batch 3831 : 0.015371302142739296\n",
      "Training loss for batch 3832 : 0.04255896061658859\n",
      "Training loss for batch 3833 : 0.10368967056274414\n",
      "Training loss for batch 3834 : 0.09901469200849533\n",
      "Training loss for batch 3835 : 0.006639768835157156\n",
      "Training loss for batch 3836 : 0.02743224799633026\n",
      "Training loss for batch 3837 : 0.14912216365337372\n",
      "Training loss for batch 3838 : 0.0760013684630394\n",
      "Training loss for batch 3839 : 0.2442532777786255\n",
      "Training loss for batch 3840 : 0.2616562247276306\n",
      "Training loss for batch 3841 : 0.0778827965259552\n",
      "Training loss for batch 3842 : 0.07295666635036469\n",
      "Training loss for batch 3843 : 0.07837814092636108\n",
      "Training loss for batch 3844 : 0.47665029764175415\n",
      "Training loss for batch 3845 : 0.20960506796836853\n",
      "Training loss for batch 3846 : 0.20711931586265564\n",
      "Training loss for batch 3847 : 0.16658134758472443\n",
      "Training loss for batch 3848 : 0.061375588178634644\n",
      "Training loss for batch 3849 : 0.07767304033041\n",
      "Training loss for batch 3850 : 0.3041996657848358\n",
      "Training loss for batch 3851 : 0.054059915244579315\n",
      "Training loss for batch 3852 : 0.23092293739318848\n",
      "Training loss for batch 3853 : 0.31480085849761963\n",
      "Training loss for batch 3854 : 0.11572159826755524\n",
      "Training loss for batch 3855 : 0.09787072986364365\n",
      "Training loss for batch 3856 : 0.025899115949869156\n",
      "Training loss for batch 3857 : 0.46536901593208313\n",
      "Training loss for batch 3858 : 0.3128809928894043\n",
      "Training loss for batch 3859 : 0.15567882359027863\n",
      "Training loss for batch 3860 : 0.3286219835281372\n",
      "Training loss for batch 3861 : 0.5003869533538818\n",
      "Training loss for batch 3862 : 0.10378829389810562\n",
      "Training loss for batch 3863 : 0.11094754934310913\n",
      "Training loss for batch 3864 : 0.3173144459724426\n",
      "Training loss for batch 3865 : 0.09211292117834091\n",
      "Training loss for batch 3866 : 0.3077411651611328\n",
      "Training loss for batch 3867 : 0.06045397371053696\n",
      "Training loss for batch 3868 : 0.16870614886283875\n",
      "Training loss for batch 3869 : 0.2545357346534729\n",
      "Training loss for batch 3870 : 0.014437589794397354\n",
      "Training loss for batch 3871 : 0.054952964186668396\n",
      "Training loss for batch 3872 : 0.2730475664138794\n",
      "Training loss for batch 3873 : 0.6034546494483948\n",
      "Training loss for batch 3874 : 0.1307629495859146\n",
      "Training loss for batch 3875 : 0.3021566569805145\n",
      "Training loss for batch 3876 : 0.1845984160900116\n",
      "Training loss for batch 3877 : 0.13783808052539825\n",
      "Training loss for batch 3878 : 0.0680873766541481\n",
      "Training loss for batch 3879 : 0.03967894986271858\n",
      "Training loss for batch 3880 : 0.17886270582675934\n",
      "Training loss for batch 3881 : 0.057542651891708374\n",
      "Training loss for batch 3882 : 0.23060527443885803\n",
      "Training loss for batch 3883 : 0.2501099407672882\n",
      "Training loss for batch 3884 : 0.22744305431842804\n",
      "Training loss for batch 3885 : 0.1550571769475937\n",
      "Training loss for batch 3886 : 0.08701297640800476\n",
      "Training loss for batch 3887 : 0.38370272517204285\n",
      "Training loss for batch 3888 : 0.3376973569393158\n",
      "Training loss for batch 3889 : 0.32648172974586487\n",
      "Training loss for batch 3890 : 0.09764103591442108\n",
      "Training loss for batch 3891 : 0.06019541248679161\n",
      "Training loss for batch 3892 : 0.14394108951091766\n",
      "Training loss for batch 3893 : 0.13731229305267334\n",
      "Training loss for batch 3894 : 0.25222572684288025\n",
      "Training loss for batch 3895 : 0.009764134883880615\n",
      "Training loss for batch 3896 : 0.37479910254478455\n",
      "Training loss for batch 3897 : 0.04098990559577942\n",
      "Training loss for batch 3898 : 0.294462651014328\n",
      "Training loss for batch 3899 : 0.024908099323511124\n",
      "Training loss for batch 3900 : 0.08213050663471222\n",
      "Training loss for batch 3901 : 0.11784350872039795\n",
      "Training loss for batch 3902 : 0.0\n",
      "Training loss for batch 3903 : 0.3690512478351593\n",
      "Training loss for batch 3904 : 0.060871466994285583\n",
      "Training loss for batch 3905 : 0.07940729707479477\n",
      "Training loss for batch 3906 : 0.07682979106903076\n",
      "Training loss for batch 3907 : 0.13833841681480408\n",
      "Training loss for batch 3908 : 0.2700410485267639\n",
      "Training loss for batch 3909 : 0.09330219030380249\n",
      "Training loss for batch 3910 : 0.300578773021698\n",
      "Training loss for batch 3911 : 0.04081736505031586\n",
      "Training loss for batch 3912 : 0.008127054199576378\n",
      "Training loss for batch 3913 : 0.0031171785667538643\n",
      "Training loss for batch 3914 : 0.12783165276050568\n",
      "Training loss for batch 3915 : 0.04375024512410164\n",
      "Training loss for batch 3916 : 0.030826084315776825\n",
      "Training loss for batch 3917 : 0.06303630769252777\n",
      "Training loss for batch 3918 : 0.03740065172314644\n",
      "Training loss for batch 3919 : 0.1940004974603653\n",
      "Training loss for batch 3920 : 0.11411929875612259\n",
      "Training loss for batch 3921 : 0.2679721713066101\n",
      "Training loss for batch 3922 : 0.0005360245704650879\n",
      "Training loss for batch 3923 : 0.0\n",
      "Training loss for batch 3924 : 0.0031108050607144833\n",
      "Training loss for batch 3925 : 0.0976330041885376\n",
      "Training loss for batch 3926 : 0.0653829276561737\n",
      "Training loss for batch 3927 : 0.2552846670150757\n",
      "Training loss for batch 3928 : 0.022699756547808647\n",
      "Training loss for batch 3929 : 0.23763422667980194\n",
      "Training loss for batch 3930 : 0.06762821227312088\n",
      "Training loss for batch 3931 : -0.0007714747334830463\n",
      "Training loss for batch 3932 : 0.1369701623916626\n",
      "Training loss for batch 3933 : 0.08025990426540375\n",
      "Training loss for batch 3934 : 0.1207519918680191\n",
      "Training loss for batch 3935 : 0.18461455404758453\n",
      "Training loss for batch 3936 : 0.2139607071876526\n",
      "Training loss for batch 3937 : 0.0\n",
      "Training loss for batch 3938 : 0.06022007018327713\n",
      "Training loss for batch 3939 : 0.13399086892604828\n",
      "Training loss for batch 3940 : 0.15711984038352966\n",
      "Training loss for batch 3941 : 0.1175682321190834\n",
      "Training loss for batch 3942 : 0.14901849627494812\n",
      "Training loss for batch 3943 : 0.012001743540167809\n",
      "Training loss for batch 3944 : 0.08020574599504471\n",
      "Training loss for batch 3945 : 0.06201742962002754\n",
      "Training loss for batch 3946 : 0.03389093279838562\n",
      "Training loss for batch 3947 : 0.055327385663986206\n",
      "Training loss for batch 3948 : 0.3322911262512207\n",
      "Training loss for batch 3949 : 0.019958065822720528\n",
      "Training loss for batch 3950 : 0.24008002877235413\n",
      "Training loss for batch 3951 : 0.022228345274925232\n",
      "Training loss for batch 3952 : 0.13200606405735016\n",
      "Training loss for batch 3953 : 0.25546130537986755\n",
      "Training loss for batch 3954 : 0.2226213812828064\n",
      "Training loss for batch 3955 : 0.14528107643127441\n",
      "Training loss for batch 3956 : 0.210346981883049\n",
      "Training loss for batch 3957 : 0.09159061312675476\n",
      "Training loss for batch 3958 : 0.32111796736717224\n",
      "Training loss for batch 3959 : 0.33938729763031006\n",
      "Training loss for batch 3960 : 0.3533831238746643\n",
      "Training loss for batch 3961 : 0.2731698453426361\n",
      "Training loss for batch 3962 : 0.4007357954978943\n",
      "Training loss for batch 3963 : 0.17787370085716248\n",
      "Training loss for batch 3964 : 0.063792884349823\n",
      "Training loss for batch 3965 : 0.17263886332511902\n",
      "Training loss for batch 3966 : 0.003908889833837748\n",
      "Training loss for batch 3967 : 0.0722116157412529\n",
      "Training loss for batch 3968 : 0.021207118406891823\n",
      "Training loss for batch 3969 : 0.28811559081077576\n",
      "Training loss for batch 3970 : 0.29445743560791016\n",
      "Training loss for batch 3971 : 0.09714876860380173\n",
      "Training loss for batch 3972 : 0.26689714193344116\n",
      "Training loss for batch 3973 : 0.09926103055477142\n",
      "Training loss for batch 3974 : 0.07956661283969879\n",
      "Training loss for batch 3975 : 0.3845777213573456\n",
      "Training loss for batch 3976 : 0.021074622869491577\n",
      "Training loss for batch 3977 : 0.12699466943740845\n",
      "Training loss for batch 3978 : 0.004416948184370995\n",
      "Training loss for batch 3979 : 0.045616280287504196\n",
      "Training loss for batch 3980 : 0.17381639778614044\n",
      "Training loss for batch 3981 : 0.07835642993450165\n",
      "Training loss for batch 3982 : 0.07918775081634521\n",
      "Training loss for batch 3983 : 0.10299837589263916\n",
      "Training loss for batch 3984 : 0.12074904143810272\n",
      "Training loss for batch 3985 : 0.35626935958862305\n",
      "Training loss for batch 3986 : 0.20324955880641937\n",
      "Training loss for batch 3987 : 0.2673393487930298\n",
      "Training loss for batch 3988 : 0.3042812943458557\n",
      "Training loss for batch 3989 : 0.12126302719116211\n",
      "Training loss for batch 3990 : 0.11219862103462219\n",
      "Training loss for batch 3991 : 0.10091295093297958\n",
      "Training loss for batch 3992 : 0.04377484321594238\n",
      "Training loss for batch 3993 : 0.0687166079878807\n",
      "Training loss for batch 3994 : 0.1087503507733345\n",
      "Training loss for batch 3995 : 0.09776295721530914\n",
      "Training loss for batch 3996 : 0.2590194344520569\n",
      "Training loss for batch 3997 : 0.20682083070278168\n",
      "Training loss for batch 3998 : 0.11805819720029831\n",
      "Training loss for batch 3999 : 0.05787299573421478\n",
      "Training loss for batch 4000 : 0.2382085919380188\n",
      "Training loss for batch 4001 : 0.14913028478622437\n",
      "Training loss for batch 4002 : 0.26584696769714355\n",
      "Training loss for batch 4003 : 0.0071685947477817535\n",
      "Training loss for batch 4004 : 0.10900836437940598\n",
      "Training loss for batch 4005 : 0.06168041005730629\n",
      "Training loss for batch 4006 : 0.03065132349729538\n",
      "Training loss for batch 4007 : 0.21315515041351318\n",
      "Training loss for batch 4008 : 0.1261618733406067\n",
      "Training loss for batch 4009 : 0.10646802932024002\n",
      "Training loss for batch 4010 : 0.08633646368980408\n",
      "Training loss for batch 4011 : 0.0025769274216145277\n",
      "Training loss for batch 4012 : 0.006965826265513897\n",
      "Training loss for batch 4013 : 0.12887734174728394\n",
      "Training loss for batch 4014 : 0.09127894788980484\n",
      "Training loss for batch 4015 : 0.02803393080830574\n",
      "Training loss for batch 4016 : 0.031848639249801636\n",
      "Training loss for batch 4017 : 0.316862016916275\n",
      "Training loss for batch 4018 : 0.05189700424671173\n",
      "Training loss for batch 4019 : 0.12838906049728394\n",
      "Training loss for batch 4020 : 0.05997788906097412\n",
      "Training loss for batch 4021 : 0.033650096505880356\n",
      "Training loss for batch 4022 : 0.07496359944343567\n",
      "Training loss for batch 4023 : 0.01315060630440712\n",
      "Training loss for batch 4024 : 0.1243717148900032\n",
      "Training loss for batch 4025 : 0.21212543547153473\n",
      "Training loss for batch 4026 : 0.07313306629657745\n",
      "Training loss for batch 4027 : 0.12888003885746002\n",
      "Training loss for batch 4028 : 0.20276691019535065\n",
      "Training loss for batch 4029 : 0.36378732323646545\n",
      "Training loss for batch 4030 : 0.07127657532691956\n",
      "Training loss for batch 4031 : 0.24663205444812775\n",
      "Training loss for batch 4032 : 0.05522233992815018\n",
      "Training loss for batch 4033 : 0.19114404916763306\n",
      "Training loss for batch 4034 : 0.015038741752505302\n",
      "Training loss for batch 4035 : 0.02477050945162773\n",
      "Training loss for batch 4036 : 0.11245163530111313\n",
      "Training loss for batch 4037 : 0.04358173534274101\n",
      "Training loss for batch 4038 : 0.218320831656456\n",
      "Training loss for batch 4039 : 0.22916944324970245\n",
      "Training loss for batch 4040 : 0.055055584758520126\n",
      "Training loss for batch 4041 : 0.3014007806777954\n",
      "Training loss for batch 4042 : -0.0006721317768096924\n",
      "Training loss for batch 4043 : 0.04945392161607742\n",
      "Training loss for batch 4044 : 0.14116226136684418\n",
      "Training loss for batch 4045 : 0.125647634267807\n",
      "Training loss for batch 4046 : 0.039006538689136505\n",
      "Training loss for batch 4047 : 0.11131583154201508\n",
      "Training loss for batch 4048 : 0.514306366443634\n",
      "Training loss for batch 4049 : 0.19799286127090454\n",
      "Training loss for batch 4050 : 0.009862959384918213\n",
      "Training loss for batch 4051 : 0.3704834580421448\n",
      "Training loss for batch 4052 : 0.29789090156555176\n",
      "Training loss for batch 4053 : 0.0\n",
      "Training loss for batch 4054 : 0.09411510825157166\n",
      "Training loss for batch 4055 : 0.2192351371049881\n",
      "Training loss for batch 4056 : 0.20503228902816772\n",
      "Training loss for batch 4057 : 0.12653705477714539\n",
      "Training loss for batch 4058 : 0.060523126274347305\n",
      "Training loss for batch 4059 : 0.04197566211223602\n",
      "Training loss for batch 4060 : 0.035207681357860565\n",
      "Training loss for batch 4061 : -0.0011149062775075436\n",
      "Training loss for batch 4062 : 0.15990453958511353\n",
      "Training loss for batch 4063 : 0.2955147922039032\n",
      "Training loss for batch 4064 : 0.22342033684253693\n",
      "Training loss for batch 4065 : 0.09629105031490326\n",
      "Training loss for batch 4066 : 0.2753557860851288\n",
      "Training loss for batch 4067 : 0.038605980575084686\n",
      "Training loss for batch 4068 : 0.25244462490081787\n",
      "Training loss for batch 4069 : 0.08463360369205475\n",
      "Training loss for batch 4070 : 0.09803561121225357\n",
      "Training loss for batch 4071 : 0.037221673876047134\n",
      "Training loss for batch 4072 : 0.3980293571949005\n",
      "Training loss for batch 4073 : 0.2647247016429901\n",
      "Training loss for batch 4074 : 0.17827966809272766\n",
      "Training loss for batch 4075 : 0.06923165172338486\n",
      "Training loss for batch 4076 : 0.2661859393119812\n",
      "Training loss for batch 4077 : 0.359948992729187\n",
      "Training loss for batch 4078 : 0.06637060642242432\n",
      "Training loss for batch 4079 : 0.10510443896055222\n",
      "Training loss for batch 4080 : 0.10015864670276642\n",
      "Training loss for batch 4081 : 0.07640063762664795\n",
      "Training loss for batch 4082 : 0.07193247973918915\n",
      "Training loss for batch 4083 : 0.2203095257282257\n",
      "Training loss for batch 4084 : 0.11869848519563675\n",
      "Training loss for batch 4085 : 0.0\n",
      "Training loss for batch 4086 : 0.26166245341300964\n",
      "Training loss for batch 4087 : 0.17620952427387238\n",
      "Training loss for batch 4088 : 0.14220589399337769\n",
      "Training loss for batch 4089 : 0.160562664270401\n",
      "Training loss for batch 4090 : 0.032499127089977264\n",
      "Training loss for batch 4091 : 0.15732882916927338\n",
      "Training loss for batch 4092 : 0.19166144728660583\n",
      "Training loss for batch 4093 : 0.16456586122512817\n",
      "Training loss for batch 4094 : 0.03308946639299393\n",
      "Training loss for batch 4095 : 0.3543650507926941\n",
      "Training loss for batch 4096 : 0.003995905630290508\n",
      "Training loss for batch 4097 : 0.15527185797691345\n",
      "Training loss for batch 4098 : 0.2774999737739563\n",
      "Training loss for batch 4099 : 0.19672821462154388\n",
      "Training loss for batch 4100 : 0.3091657757759094\n",
      "Training loss for batch 4101 : 0.08108414709568024\n",
      "Training loss for batch 4102 : 0.19082586467266083\n",
      "Training loss for batch 4103 : 0.07127352803945541\n",
      "Training loss for batch 4104 : 0.11755119264125824\n",
      "Training loss for batch 4105 : 0.08071502298116684\n",
      "Training loss for batch 4106 : 0.05530165508389473\n",
      "Training loss for batch 4107 : 0.13785812258720398\n",
      "Training loss for batch 4108 : 0.013964831829071045\n",
      "Training loss for batch 4109 : 0.009629597887396812\n",
      "Training loss for batch 4110 : 0.03373287618160248\n",
      "Training loss for batch 4111 : 0.16195178031921387\n",
      "Training loss for batch 4112 : 0.2612893283367157\n",
      "Training loss for batch 4113 : 0.37214696407318115\n",
      "Training loss for batch 4114 : 0.13473722338676453\n",
      "Training loss for batch 4115 : 0.040044818073511124\n",
      "Training loss for batch 4116 : 0.10620833933353424\n",
      "Training loss for batch 4117 : 0.15929493308067322\n",
      "Training loss for batch 4118 : 0.25623810291290283\n",
      "Training loss for batch 4119 : 0.05841248482465744\n",
      "Training loss for batch 4120 : 0.052201710641384125\n",
      "Training loss for batch 4121 : 0.04867614060640335\n",
      "Training loss for batch 4122 : 0.03374204412102699\n",
      "Training loss for batch 4123 : 0.03947366029024124\n",
      "Training loss for batch 4124 : 0.036356370896101\n",
      "Training loss for batch 4125 : 0.014208515174686909\n",
      "Training loss for batch 4126 : 0.01576877571642399\n",
      "Training loss for batch 4127 : 0.11576804518699646\n",
      "Training loss for batch 4128 : 0.012557145208120346\n",
      "Training loss for batch 4129 : 0.021303068846464157\n",
      "Training loss for batch 4130 : 0.20396973192691803\n",
      "Training loss for batch 4131 : 0.2655995190143585\n",
      "Training loss for batch 4132 : 0.08364488929510117\n",
      "Training loss for batch 4133 : 0.10341093689203262\n",
      "Training loss for batch 4134 : 0.1157694160938263\n",
      "Training loss for batch 4135 : 0.13181033730506897\n",
      "Training loss for batch 4136 : 0.2204555869102478\n",
      "Training loss for batch 4137 : 0.22710925340652466\n",
      "Training loss for batch 4138 : 0.35947051644325256\n",
      "Training loss for batch 4139 : 0.17467249929904938\n",
      "Training loss for batch 4140 : 0.2358388751745224\n",
      "Training loss for batch 4141 : 0.15389138460159302\n",
      "Training loss for batch 4142 : 0.09453566372394562\n",
      "Training loss for batch 4143 : 0.16543513536453247\n",
      "Training loss for batch 4144 : 0.17175965011119843\n",
      "Training loss for batch 4145 : 0.11612149327993393\n",
      "Training loss for batch 4146 : 0.06131560355424881\n",
      "Training loss for batch 4147 : 0.17453549802303314\n",
      "Training loss for batch 4148 : 0.17013144493103027\n",
      "Training loss for batch 4149 : 0.1514289379119873\n",
      "Training loss for batch 4150 : 0.24060407280921936\n",
      "Training loss for batch 4151 : 0.012047349475324154\n",
      "Training loss for batch 4152 : 0.02296452969312668\n",
      "Training loss for batch 4153 : 0.2961209714412689\n",
      "Training loss for batch 4154 : 0.07823564857244492\n",
      "Training loss for batch 4155 : 0.007079824805259705\n",
      "Training loss for batch 4156 : 0.17403337359428406\n",
      "Training loss for batch 4157 : 0.1485820710659027\n",
      "Training loss for batch 4158 : 0.03653610125184059\n",
      "Training loss for batch 4159 : 0.02514554187655449\n",
      "Training loss for batch 4160 : 0.25437644124031067\n",
      "Training loss for batch 4161 : 0.2178957462310791\n",
      "Training loss for batch 4162 : 0.05380814149975777\n",
      "Training loss for batch 4163 : 0.056469179689884186\n",
      "Training loss for batch 4164 : 0.0317176915705204\n",
      "Training loss for batch 4165 : 0.11102823913097382\n",
      "Training loss for batch 4166 : 0.004048466682434082\n",
      "Training loss for batch 4167 : 0.16556859016418457\n",
      "Training loss for batch 4168 : 0.18387040495872498\n",
      "Training loss for batch 4169 : 0.08203094452619553\n",
      "Training loss for batch 4170 : 0.052617959678173065\n",
      "Training loss for batch 4171 : 0.032739024609327316\n",
      "Training loss for batch 4172 : 0.31506308913230896\n",
      "Training loss for batch 4173 : 0.0665646567940712\n",
      "Training loss for batch 4174 : 0.018675625324249268\n",
      "Training loss for batch 4175 : 0.028639351949095726\n",
      "Training loss for batch 4176 : 0.10981274396181107\n",
      "Training loss for batch 4177 : 0.11170793324708939\n",
      "Training loss for batch 4178 : 0.08929053694009781\n",
      "Training loss for batch 4179 : 0.280166894197464\n",
      "Training loss for batch 4180 : 0.11784398555755615\n",
      "Training loss for batch 4181 : 0.19816923141479492\n",
      "Training loss for batch 4182 : 0.171286940574646\n",
      "Training loss for batch 4183 : 0.13449673354625702\n",
      "Training loss for batch 4184 : 0.0896679237484932\n",
      "Training loss for batch 4185 : 0.13692423701286316\n",
      "Training loss for batch 4186 : 0.12739089131355286\n",
      "Training loss for batch 4187 : -0.0025468056555837393\n",
      "Training loss for batch 4188 : 0.328021764755249\n",
      "Training loss for batch 4189 : 0.11552406847476959\n",
      "Training loss for batch 4190 : 0.16323961317539215\n",
      "Training loss for batch 4191 : 0.011679723858833313\n",
      "Training loss for batch 4192 : 0.08232275396585464\n",
      "Training loss for batch 4193 : -0.0001806422951631248\n",
      "Training loss for batch 4194 : 0.20427590608596802\n",
      "Training loss for batch 4195 : 0.21735495328903198\n",
      "Training loss for batch 4196 : 0.09813538193702698\n",
      "Training loss for batch 4197 : 0.20129212737083435\n",
      "Training loss for batch 4198 : 0.11726078391075134\n",
      "Training loss for batch 4199 : 0.10918644815683365\n",
      "Training loss for batch 4200 : 0.2171230912208557\n",
      "Training loss for batch 4201 : 0.032319024205207825\n",
      "Training loss for batch 4202 : 0.10468075424432755\n",
      "Training loss for batch 4203 : 0.007588221225887537\n",
      "Training loss for batch 4204 : 0.021468674764037132\n",
      "Training loss for batch 4205 : 0.06515604257583618\n",
      "Training loss for batch 4206 : 0.03922685235738754\n",
      "Training loss for batch 4207 : 0.018815260380506516\n",
      "Training loss for batch 4208 : -0.00015906192129477859\n",
      "Training loss for batch 4209 : 0.050324492156505585\n",
      "Training loss for batch 4210 : 0.0790768563747406\n",
      "Training loss for batch 4211 : 0.01594935730099678\n",
      "Training loss for batch 4212 : 0.15576907992362976\n",
      "Training loss for batch 4213 : 0.1646551936864853\n",
      "Training loss for batch 4214 : 0.2516290843486786\n",
      "Training loss for batch 4215 : 0.13900479674339294\n",
      "Training loss for batch 4216 : 0.21121010184288025\n",
      "Training loss for batch 4217 : 0.04740782827138901\n",
      "Training loss for batch 4218 : 0.28221988677978516\n",
      "Training loss for batch 4219 : 0.05335491523146629\n",
      "Training loss for batch 4220 : 0.15349777042865753\n",
      "Training loss for batch 4221 : 0.22235515713691711\n",
      "Training loss for batch 4222 : 0.14790859818458557\n",
      "Training loss for batch 4223 : 0.21269962191581726\n",
      "Training loss for batch 4224 : 0.296403169631958\n",
      "Training loss for batch 4225 : 0.06612356007099152\n",
      "Training loss for batch 4226 : 0.4072860777378082\n",
      "Training loss for batch 4227 : 0.3674885332584381\n",
      "Training loss for batch 4228 : 0.14194069802761078\n",
      "Training loss for batch 4229 : 0.029297368600964546\n",
      "Training loss for batch 4230 : 0.07307766377925873\n",
      "Training loss for batch 4231 : 0.36511099338531494\n",
      "Training loss for batch 4232 : 0.08093661814928055\n",
      "Training loss for batch 4233 : 0.09972431510686874\n",
      "Training loss for batch 4234 : 0.17630261182785034\n",
      "Training loss for batch 4235 : 0.009774956852197647\n",
      "Training loss for batch 4236 : 0.1704455018043518\n",
      "Training loss for batch 4237 : 0.11318948864936829\n",
      "Training loss for batch 4238 : 0.22308538854122162\n",
      "Training loss for batch 4239 : 0.12236073613166809\n",
      "Training loss for batch 4240 : 0.013516832143068314\n",
      "Training loss for batch 4241 : 0.07808313518762589\n",
      "Training loss for batch 4242 : 0.0028316876851022243\n",
      "Training loss for batch 4243 : 0.017505014315247536\n",
      "Training loss for batch 4244 : 0.09097909182310104\n",
      "Training loss for batch 4245 : 0.15463143587112427\n",
      "Training loss for batch 4246 : 0.12670274078845978\n",
      "Training loss for batch 4247 : 0.002959167119115591\n",
      "Training loss for batch 4248 : 0.20814916491508484\n",
      "Training loss for batch 4249 : 0.20165245234966278\n",
      "Training loss for batch 4250 : 0.40558603405952454\n",
      "Training loss for batch 4251 : 0.028066130355000496\n",
      "Training loss for batch 4252 : 0.09110204130411148\n",
      "Training loss for batch 4253 : 0.3589744567871094\n",
      "Training loss for batch 4254 : 0.058113209903240204\n",
      "Training loss for batch 4255 : 0.1035110279917717\n",
      "Training loss for batch 4256 : 0.2630969285964966\n",
      "Training loss for batch 4257 : 0.1354794204235077\n",
      "Training loss for batch 4258 : 0.10039790719747543\n",
      "Training loss for batch 4259 : 0.07347707450389862\n",
      "Training loss for batch 4260 : 0.3978610038757324\n",
      "Training loss for batch 4261 : 0.009479866363108158\n",
      "Training loss for batch 4262 : 0.16053779423236847\n",
      "Training loss for batch 4263 : 0.2988595962524414\n",
      "Training loss for batch 4264 : 0.385324090719223\n",
      "Training loss for batch 4265 : 0.1470722109079361\n",
      "Training loss for batch 4266 : 0.027831396088004112\n",
      "Training loss for batch 4267 : 0.48840606212615967\n",
      "Training loss for batch 4268 : 0.22990119457244873\n",
      "Training loss for batch 4269 : 0.031111638993024826\n",
      "Training loss for batch 4270 : 0.26499465107917786\n",
      "Training loss for batch 4271 : 0.058377962559461594\n",
      "Training loss for batch 4272 : 0.24098752439022064\n",
      "Training loss for batch 4273 : 0.04490916058421135\n",
      "Training loss for batch 4274 : 0.46548163890838623\n",
      "Training loss for batch 4275 : 0.17507034540176392\n",
      "Training loss for batch 4276 : 0.323775976896286\n",
      "Training loss for batch 4277 : 0.08739301562309265\n",
      "Training loss for batch 4278 : 0.020449386909604073\n",
      "Training loss for batch 4279 : 0.06631743907928467\n",
      "Training loss for batch 4280 : 0.008246551267802715\n",
      "Training loss for batch 4281 : 0.3130415976047516\n",
      "Training loss for batch 4282 : 0.09643781185150146\n",
      "Training loss for batch 4283 : 0.011317282915115356\n",
      "Training loss for batch 4284 : 0.17165380716323853\n",
      "Training loss for batch 4285 : 0.11826842278242111\n",
      "Training loss for batch 4286 : 0.008249755017459393\n",
      "Training loss for batch 4287 : 0.20355090498924255\n",
      "Training loss for batch 4288 : 0.09692099690437317\n",
      "Training loss for batch 4289 : 0.050578273832798004\n",
      "Training loss for batch 4290 : 0.08967980742454529\n",
      "Training loss for batch 4291 : 0.10744701325893402\n",
      "Training loss for batch 4292 : 0.14711764454841614\n",
      "Training loss for batch 4293 : 0.37548956274986267\n",
      "Training loss for batch 4294 : 0.016179606318473816\n",
      "Training loss for batch 4295 : 0.10905001312494278\n",
      "Training loss for batch 4296 : 0.1417909413576126\n",
      "Training loss for batch 4297 : 0.12705419957637787\n",
      "Training loss for batch 4298 : 0.29863277077674866\n",
      "Training loss for batch 4299 : 0.00542457913979888\n",
      "Training loss for batch 4300 : 0.1169232651591301\n",
      "Training loss for batch 4301 : 0.10098319500684738\n",
      "Training loss for batch 4302 : 0.16496852040290833\n",
      "Training loss for batch 4303 : 0.02786826156079769\n",
      "Training loss for batch 4304 : 0.14859217405319214\n",
      "Training loss for batch 4305 : 0.03450172394514084\n",
      "Training loss for batch 4306 : 0.10307759791612625\n",
      "Training loss for batch 4307 : 0.36255118250846863\n",
      "Training loss for batch 4308 : 0.24192506074905396\n",
      "Training loss for batch 4309 : 0.17028838396072388\n",
      "Training loss for batch 4310 : 0.4402633309364319\n",
      "Training loss for batch 4311 : 0.08968532830476761\n",
      "Training loss for batch 4312 : 0.2783222198486328\n",
      "Training loss for batch 4313 : 0.05427176505327225\n",
      "Training loss for batch 4314 : 0.18991614878177643\n",
      "Training loss for batch 4315 : 0.23782901465892792\n",
      "Training loss for batch 4316 : 0.3373189866542816\n",
      "Training loss for batch 4317 : 0.3565576672554016\n",
      "Training loss for batch 4318 : 0.34867119789123535\n",
      "Training loss for batch 4319 : 0.1908881664276123\n",
      "Training loss for batch 4320 : 0.1297837793827057\n",
      "Training loss for batch 4321 : 0.06988103687763214\n",
      "Training loss for batch 4322 : 0.036735787987709045\n",
      "Training loss for batch 4323 : 0.19939447939395905\n",
      "Training loss for batch 4324 : 0.24918611347675323\n",
      "Training loss for batch 4325 : 0.06248285621404648\n",
      "Training loss for batch 4326 : 0.09763380140066147\n",
      "Training loss for batch 4327 : 0.11068656295537949\n",
      "Training loss for batch 4328 : 0.2212127298116684\n",
      "Training loss for batch 4329 : 0.02609923481941223\n",
      "Training loss for batch 4330 : 0.09755199402570724\n",
      "Training loss for batch 4331 : 0.3006182312965393\n",
      "Training loss for batch 4332 : 0.05315446853637695\n",
      "Training loss for batch 4333 : 0.4971961975097656\n",
      "Training loss for batch 4334 : 0.045972153544425964\n",
      "Training loss for batch 4335 : 0.08778150379657745\n",
      "Training loss for batch 4336 : 0.22063550353050232\n",
      "Training loss for batch 4337 : 0.030157199129462242\n",
      "Training loss for batch 4338 : 0.2412518858909607\n",
      "Training loss for batch 4339 : 0.07682464271783829\n",
      "Training loss for batch 4340 : 0.015690583735704422\n",
      "Training loss for batch 4341 : 0.2514350712299347\n",
      "Training loss for batch 4342 : 0.06746464222669601\n",
      "Training loss for batch 4343 : 0.31948143243789673\n",
      "Training loss for batch 4344 : 0.14507104456424713\n",
      "Training loss for batch 4345 : -0.004283018410205841\n",
      "Training loss for batch 4346 : 0.11321697384119034\n",
      "Training loss for batch 4347 : 0.3320004642009735\n",
      "Training loss for batch 4348 : 0.23746541142463684\n",
      "Training loss for batch 4349 : 0.30550119280815125\n",
      "Training loss for batch 4350 : 0.025064684450626373\n",
      "Training loss for batch 4351 : 0.2696659564971924\n",
      "Training loss for batch 4352 : 0.14210885763168335\n",
      "Training loss for batch 4353 : 0.08841202408075333\n",
      "Training loss for batch 4354 : 0.10330222547054291\n",
      "Training loss for batch 4355 : 0.15560458600521088\n",
      "Training loss for batch 4356 : 0.03519783541560173\n",
      "Training loss for batch 4357 : 0.022439638152718544\n",
      "Training loss for batch 4358 : 0.2564000189304352\n",
      "Training loss for batch 4359 : 0.15863202512264252\n",
      "Training loss for batch 4360 : 0.08328817784786224\n",
      "Training loss for batch 4361 : 0.446744829416275\n",
      "Training loss for batch 4362 : 0.16282670199871063\n",
      "Training loss for batch 4363 : 0.08794281631708145\n",
      "Training loss for batch 4364 : 0.21341875195503235\n",
      "Training loss for batch 4365 : 0.05205519124865532\n",
      "Training loss for batch 4366 : 0.3606889843940735\n",
      "Training loss for batch 4367 : 0.0835375189781189\n",
      "Training loss for batch 4368 : 0.3393906354904175\n",
      "Training loss for batch 4369 : 0.18292108178138733\n",
      "Training loss for batch 4370 : 0.1757861077785492\n",
      "Training loss for batch 4371 : 0.18113911151885986\n",
      "Training loss for batch 4372 : 0.44105827808380127\n",
      "Training loss for batch 4373 : 0.04203158617019653\n",
      "Training loss for batch 4374 : 0.19668911397457123\n",
      "Training loss for batch 4375 : 0.009697981178760529\n",
      "Training loss for batch 4376 : 0.38114693760871887\n",
      "Training loss for batch 4377 : 0.33153000473976135\n",
      "Training loss for batch 4378 : 0.05722180753946304\n",
      "Training loss for batch 4379 : 0.09277214109897614\n",
      "Training loss for batch 4380 : 0.10068721324205399\n",
      "Training loss for batch 4381 : 0.10534293204545975\n",
      "Training loss for batch 4382 : 0.13203570246696472\n",
      "Training loss for batch 4383 : 0.04193022847175598\n",
      "Training loss for batch 4384 : 0.00545280147343874\n",
      "Training loss for batch 4385 : 0.09061696380376816\n",
      "Training loss for batch 4386 : 0.2857770323753357\n",
      "Training loss for batch 4387 : 0.14674195647239685\n",
      "Training loss for batch 4388 : 0.403883695602417\n",
      "Training loss for batch 4389 : 0.15258951485157013\n",
      "Training loss for batch 4390 : 0.09558943659067154\n",
      "Training loss for batch 4391 : 0.1383926421403885\n",
      "Training loss for batch 4392 : 0.5388489961624146\n",
      "Training loss for batch 4393 : 0.05883296579122543\n",
      "Training loss for batch 4394 : 0.04274716600775719\n",
      "Training loss for batch 4395 : 0.15683944523334503\n",
      "Training loss for batch 4396 : 0.05753402039408684\n",
      "Training loss for batch 4397 : 0.184679314494133\n",
      "Training loss for batch 4398 : 0.29290521144866943\n",
      "Training loss for batch 4399 : 0.07083679735660553\n",
      "Training loss for batch 4400 : 0.272758811712265\n",
      "Training loss for batch 4401 : 0.11071859300136566\n",
      "Training loss for batch 4402 : 0.1657278835773468\n",
      "Training loss for batch 4403 : 0.031078189611434937\n",
      "Training loss for batch 4404 : 0.30700528621673584\n",
      "Training loss for batch 4405 : 0.16027528047561646\n",
      "Training loss for batch 4406 : 0.3082054853439331\n",
      "Training loss for batch 4407 : 0.1690056025981903\n",
      "Training loss for batch 4408 : 0.21469232439994812\n",
      "Training loss for batch 4409 : 0.060470983386039734\n",
      "Training loss for batch 4410 : 0.037209201604127884\n",
      "Training loss for batch 4411 : 0.1070886179804802\n",
      "Training loss for batch 4412 : 0.15500371158123016\n",
      "Training loss for batch 4413 : 0.017688855528831482\n",
      "Training loss for batch 4414 : 0.2522056996822357\n",
      "Training loss for batch 4415 : 0.07056178897619247\n",
      "Training loss for batch 4416 : 0.030804645270109177\n",
      "Training loss for batch 4417 : 0.034283172339200974\n",
      "Training loss for batch 4418 : 0.009855988435447216\n",
      "Training loss for batch 4419 : 0.035490117967128754\n",
      "Training loss for batch 4420 : 0.1989998072385788\n",
      "Training loss for batch 4421 : 0.30387189984321594\n",
      "Training loss for batch 4422 : 0.05662733316421509\n",
      "Training loss for batch 4423 : 0.05179585516452789\n",
      "Training loss for batch 4424 : 0.16067475080490112\n",
      "Training loss for batch 4425 : 0.159190371632576\n",
      "Training loss for batch 4426 : 0.084043949842453\n",
      "Training loss for batch 4427 : 0.08784281462430954\n",
      "Training loss for batch 4428 : 0.19161951541900635\n",
      "Training loss for batch 4429 : -0.0023734518326818943\n",
      "Training loss for batch 4430 : -0.0024030576460063457\n",
      "Training loss for batch 4431 : 0.15647563338279724\n",
      "Training loss for batch 4432 : 0.12944890558719635\n",
      "Training loss for batch 4433 : 0.32669761776924133\n",
      "Training loss for batch 4434 : 0.02030763030052185\n",
      "Training loss for batch 4435 : 0.1282997876405716\n",
      "Training loss for batch 4436 : 0.2120157778263092\n",
      "Training loss for batch 4437 : 0.20321112871170044\n",
      "Training loss for batch 4438 : 0.1684214472770691\n",
      "Training loss for batch 4439 : 0.06663933396339417\n",
      "Training loss for batch 4440 : 0.07177384197711945\n",
      "Training loss for batch 4441 : 0.05469568073749542\n",
      "Training loss for batch 4442 : 0.20981164276599884\n",
      "Training loss for batch 4443 : 0.16462598741054535\n",
      "Training loss for batch 4444 : 0.04384854435920715\n",
      "Training loss for batch 4445 : 0.05898267775774002\n",
      "Training loss for batch 4446 : 0.032550714910030365\n",
      "Training loss for batch 4447 : 0.16774892807006836\n",
      "Training loss for batch 4448 : 0.019586866721510887\n",
      "Training loss for batch 4449 : 0.2577921152114868\n",
      "Training loss for batch 4450 : 0.15319481492042542\n",
      "Training loss for batch 4451 : 0.18388701975345612\n",
      "Training loss for batch 4452 : 0.14846967160701752\n",
      "Training loss for batch 4453 : 0.16120314598083496\n",
      "Training loss for batch 4454 : 0.1916632354259491\n",
      "Training loss for batch 4455 : 0.11686162650585175\n",
      "Training loss for batch 4456 : 0.10195320844650269\n",
      "Training loss for batch 4457 : 0.1545836478471756\n",
      "Training loss for batch 4458 : 0.06245586276054382\n",
      "Training loss for batch 4459 : 0.15313121676445007\n",
      "Training loss for batch 4460 : 0.06566452980041504\n",
      "Training loss for batch 4461 : 0.2982853055000305\n",
      "Training loss for batch 4462 : 0.23050999641418457\n",
      "Training loss for batch 4463 : 0.1455581933259964\n",
      "Training loss for batch 4464 : 0.2063809484243393\n",
      "Training loss for batch 4465 : 0.11871489882469177\n",
      "Training loss for batch 4466 : 0.05981740728020668\n",
      "Training loss for batch 4467 : 0.05451451987028122\n",
      "Training loss for batch 4468 : 0.28207796812057495\n",
      "Training loss for batch 4469 : 0.026527708396315575\n",
      "Training loss for batch 4470 : 0.12618649005889893\n",
      "Training loss for batch 4471 : 0.27660882472991943\n",
      "Training loss for batch 4472 : 0.10693951696157455\n",
      "Training loss for batch 4473 : 0.0969114825129509\n",
      "Training loss for batch 4474 : 0.0668976679444313\n",
      "Training loss for batch 4475 : 0.12780359387397766\n",
      "Training loss for batch 4476 : 0.16272127628326416\n",
      "Training loss for batch 4477 : 0.10366981476545334\n",
      "Training loss for batch 4478 : 0.20442241430282593\n",
      "Training loss for batch 4479 : 0.005706647876650095\n",
      "Training loss for batch 4480 : 0.018637094646692276\n",
      "Training loss for batch 4481 : 0.053343310952186584\n",
      "Training loss for batch 4482 : 0.03387868404388428\n",
      "Training loss for batch 4483 : 0.3017956018447876\n",
      "Training loss for batch 4484 : 0.17294378578662872\n",
      "Training loss for batch 4485 : 0.19153068959712982\n",
      "Training loss for batch 4486 : 0.11509287357330322\n",
      "Training loss for batch 4487 : 0.12959633767604828\n",
      "Training loss for batch 4488 : 0.2438940405845642\n",
      "Training loss for batch 4489 : 0.0819980651140213\n",
      "Training loss for batch 4490 : 0.020650334656238556\n",
      "Training loss for batch 4491 : 0.2570897340774536\n",
      "Training loss for batch 4492 : 0.2743571400642395\n",
      "Training loss for batch 4493 : 0.033911798149347305\n",
      "Training loss for batch 4494 : 0.15985897183418274\n",
      "Training loss for batch 4495 : 0.16760894656181335\n",
      "Training loss for batch 4496 : 0.024655211716890335\n",
      "Training loss for batch 4497 : 0.017872536554932594\n",
      "Training loss for batch 4498 : 0.8062610626220703\n",
      "Training loss for batch 4499 : 0.025810930877923965\n",
      "Training loss for batch 4500 : 0.20870719850063324\n",
      "Training loss for batch 4501 : 0.22928203642368317\n",
      "Training loss for batch 4502 : 0.2819628417491913\n",
      "Training loss for batch 4503 : 0.11369715631008148\n",
      "Training loss for batch 4504 : 0.026355993002653122\n",
      "Training loss for batch 4505 : 0.29393303394317627\n",
      "Training loss for batch 4506 : 0.13401548564434052\n",
      "Training loss for batch 4507 : 0.2539655268192291\n",
      "Training loss for batch 4508 : 0.01442448329180479\n",
      "Training loss for batch 4509 : 0.1357067972421646\n",
      "Training loss for batch 4510 : 0.026105593889951706\n",
      "Training loss for batch 4511 : 0.09536220878362656\n",
      "Training loss for batch 4512 : 0.025202590972185135\n",
      "Training loss for batch 4513 : 0.05891066789627075\n",
      "Training loss for batch 4514 : 0.23582126200199127\n",
      "Training loss for batch 4515 : 0.030248062685132027\n",
      "Training loss for batch 4516 : 0.009132467210292816\n",
      "Training loss for batch 4517 : 0.18840432167053223\n",
      "Training loss for batch 4518 : 0.42057347297668457\n",
      "Training loss for batch 4519 : 0.1633455604314804\n",
      "Training loss for batch 4520 : 0.1474524736404419\n",
      "Training loss for batch 4521 : 0.2597150206565857\n",
      "Training loss for batch 4522 : 0.1534249186515808\n",
      "Training loss for batch 4523 : 0.030997078865766525\n",
      "Training loss for batch 4524 : 0.0\n",
      "Training loss for batch 4525 : 0.1156327873468399\n",
      "Training loss for batch 4526 : 0.05311959236860275\n",
      "Training loss for batch 4527 : 0.11587227880954742\n",
      "Training loss for batch 4528 : 0.09065384417772293\n",
      "Training loss for batch 4529 : 0.22052550315856934\n",
      "Training loss for batch 4530 : 0.05972204729914665\n",
      "Training loss for batch 4531 : 0.05595846474170685\n",
      "Training loss for batch 4532 : 0.03959348425269127\n",
      "Training loss for batch 4533 : 0.15120968222618103\n",
      "Training loss for batch 4534 : 0.028324810788035393\n",
      "Training loss for batch 4535 : 0.18381959199905396\n",
      "Training loss for batch 4536 : 0.002716412302106619\n",
      "Training loss for batch 4537 : 0.03306620940566063\n",
      "Training loss for batch 4538 : 0.24134385585784912\n",
      "Training loss for batch 4539 : 0.0\n",
      "Training loss for batch 4540 : 0.43658876419067383\n",
      "Training loss for batch 4541 : 0.0\n",
      "Training loss for batch 4542 : 0.015403121709823608\n",
      "Training loss for batch 4543 : 0.21903526782989502\n",
      "Training loss for batch 4544 : 0.014504640363156796\n",
      "Training loss for batch 4545 : 0.38709062337875366\n",
      "Training loss for batch 4546 : 0.15289372205734253\n",
      "Training loss for batch 4547 : 0.1665171980857849\n",
      "Training loss for batch 4548 : 0.10959770530462265\n",
      "Training loss for batch 4549 : 0.25478917360305786\n",
      "Training loss for batch 4550 : 0.047943513840436935\n",
      "Training loss for batch 4551 : 0.01425404567271471\n",
      "Training loss for batch 4552 : 0.2603171169757843\n",
      "Training loss for batch 4553 : 0.08366627246141434\n",
      "Training loss for batch 4554 : 0.2503592371940613\n",
      "Training loss for batch 4555 : 0.26570984721183777\n",
      "Training loss for batch 4556 : 0.5131600499153137\n",
      "Training loss for batch 4557 : 0.051816053688526154\n",
      "Training loss for batch 4558 : 0.08038949966430664\n",
      "Training loss for batch 4559 : 0.10580181330442429\n",
      "Training loss for batch 4560 : 0.1206015795469284\n",
      "Training loss for batch 4561 : 0.0917925164103508\n",
      "Training loss for batch 4562 : 0.0875445231795311\n",
      "Training loss for batch 4563 : 0.07980169355869293\n",
      "Training loss for batch 4564 : 0.01083249133080244\n",
      "Training loss for batch 4565 : 0.31481340527534485\n",
      "Training loss for batch 4566 : 0.05281210318207741\n",
      "Training loss for batch 4567 : 0.09849655628204346\n",
      "Training loss for batch 4568 : 0.15996302664279938\n",
      "Training loss for batch 4569 : 0.13534726202487946\n",
      "Training loss for batch 4570 : 0.21830466389656067\n",
      "Training loss for batch 4571 : 0.3489413559436798\n",
      "Training loss for batch 4572 : 0.10621996223926544\n",
      "Training loss for batch 4573 : 0.005742046516388655\n",
      "Training loss for batch 4574 : 0.33014917373657227\n",
      "Training loss for batch 4575 : 0.07142144441604614\n",
      "Training loss for batch 4576 : 0.022362153977155685\n",
      "Training loss for batch 4577 : 0.3987204432487488\n",
      "Training loss for batch 4578 : 0.23891055583953857\n",
      "Training loss for batch 4579 : 0.14814439415931702\n",
      "Training loss for batch 4580 : 0.21174296736717224\n",
      "Training loss for batch 4581 : 0.1135115772485733\n",
      "Training loss for batch 4582 : 0.20100919902324677\n",
      "Training loss for batch 4583 : 0.2704017758369446\n",
      "Training loss for batch 4584 : 0.20149670541286469\n",
      "Training loss for batch 4585 : 0.038859426975250244\n",
      "Training loss for batch 4586 : 0.147085040807724\n",
      "Training loss for batch 4587 : 0.15994109213352203\n",
      "Training loss for batch 4588 : 0.10247410088777542\n",
      "Training loss for batch 4589 : 0.09330087900161743\n",
      "Training loss for batch 4590 : 0.041567347943782806\n",
      "Training loss for batch 4591 : 0.12665045261383057\n",
      "Training loss for batch 4592 : 0.13889379799365997\n",
      "Training loss for batch 4593 : 0.0906275063753128\n",
      "Training loss for batch 4594 : 0.23567943274974823\n",
      "Training loss for batch 4595 : 0.07223939895629883\n",
      "Training loss for batch 4596 : 0.10994593054056168\n",
      "Training loss for batch 4597 : 0.08081484586000443\n",
      "Training loss for batch 4598 : 0.14108024537563324\n",
      "Training loss for batch 4599 : 0.06211906671524048\n",
      "Training loss for batch 4600 : 0.1626189649105072\n",
      "Training loss for batch 4601 : 0.017435535788536072\n",
      "Training loss for batch 4602 : 0.15269184112548828\n",
      "Training loss for batch 4603 : 0.11716268211603165\n",
      "Training loss for batch 4604 : 0.09011751413345337\n",
      "Training loss for batch 4605 : 0.012530352920293808\n",
      "Training loss for batch 4606 : 0.2809930145740509\n",
      "Training loss for batch 4607 : 0.23520538210868835\n",
      "Training loss for batch 4608 : 0.23547326028347015\n",
      "Training loss for batch 4609 : 0.16480475664138794\n",
      "Training loss for batch 4610 : 0.1794872134923935\n",
      "Training loss for batch 4611 : 0.2660689949989319\n",
      "Training loss for batch 4612 : 0.11934170126914978\n",
      "Training loss for batch 4613 : 0.04618312418460846\n",
      "Training loss for batch 4614 : 0.075352743268013\n",
      "Training loss for batch 4615 : 0.196690171957016\n",
      "Training loss for batch 4616 : 0.11598590016365051\n",
      "Training loss for batch 4617 : 0.2336011379957199\n",
      "Training loss for batch 4618 : 0.03952253609895706\n",
      "Training loss for batch 4619 : 0.13385668396949768\n",
      "Training loss for batch 4620 : 0.11730020493268967\n",
      "Training loss for batch 4621 : 0.35774025321006775\n",
      "Training loss for batch 4622 : 0.11168033629655838\n",
      "Training loss for batch 4623 : 0.2513963282108307\n",
      "Training loss for batch 4624 : 0.11312375217676163\n",
      "Training loss for batch 4625 : 0.23259997367858887\n",
      "Training loss for batch 4626 : 0.23159098625183105\n",
      "Training loss for batch 4627 : 0.23856012523174286\n",
      "Training loss for batch 4628 : 0.028018899261951447\n",
      "Training loss for batch 4629 : 0.05117115005850792\n",
      "Training loss for batch 4630 : 0.017409473657608032\n",
      "Training loss for batch 4631 : 0.16228973865509033\n",
      "Training loss for batch 4632 : 0.1266670525074005\n",
      "Training loss for batch 4633 : 0.10249585658311844\n",
      "Training loss for batch 4634 : 0.12946340441703796\n",
      "Training loss for batch 4635 : 0.22559389472007751\n",
      "Training loss for batch 4636 : 0.22692835330963135\n",
      "Training loss for batch 4637 : 0.009889903478324413\n",
      "Training loss for batch 4638 : 0.13646700978279114\n",
      "Training loss for batch 4639 : 0.12833718955516815\n",
      "Training loss for batch 4640 : 0.23718005418777466\n",
      "Training loss for batch 4641 : 0.13988453149795532\n",
      "Training loss for batch 4642 : 0.3000887930393219\n",
      "Training loss for batch 4643 : 0.36689242720603943\n",
      "Training loss for batch 4644 : 0.05964599549770355\n",
      "Training loss for batch 4645 : 0.022168727591633797\n",
      "Training loss for batch 4646 : 0.19712093472480774\n",
      "Training loss for batch 4647 : 0.07068659365177155\n",
      "Training loss for batch 4648 : 0.10041403770446777\n",
      "Training loss for batch 4649 : 0.29779282212257385\n",
      "Training loss for batch 4650 : 0.0377366840839386\n",
      "Training loss for batch 4651 : 0.19150327146053314\n",
      "Training loss for batch 4652 : 0.06455865502357483\n",
      "Training loss for batch 4653 : 0.23368270695209503\n",
      "Training loss for batch 4654 : 0.24910782277584076\n",
      "Training loss for batch 4655 : 0.1625274121761322\n",
      "Training loss for batch 4656 : 0.1443047821521759\n",
      "Training loss for batch 4657 : 0.07627196609973907\n",
      "Training loss for batch 4658 : 0.23336857557296753\n",
      "Training loss for batch 4659 : 0.04605955258011818\n",
      "Training loss for batch 4660 : 0.2807362675666809\n",
      "Training loss for batch 4661 : 0.19862757623195648\n",
      "Training loss for batch 4662 : 0.10560879111289978\n",
      "Training loss for batch 4663 : 0.06692177057266235\n",
      "Training loss for batch 4664 : 0.07604315131902695\n",
      "Training loss for batch 4665 : 0.018446583300828934\n",
      "Training loss for batch 4666 : 0.0074545941315591335\n",
      "Training loss for batch 4667 : 0.1838708519935608\n",
      "Training loss for batch 4668 : 0.09511534124612808\n",
      "Training loss for batch 4669 : 0.06679783016443253\n",
      "Training loss for batch 4670 : 0.1472722887992859\n",
      "Training loss for batch 4671 : 0.0001603814889676869\n",
      "Training loss for batch 4672 : 0.01472461223602295\n",
      "Training loss for batch 4673 : 0.22838902473449707\n",
      "Training loss for batch 4674 : 0.009178714826703072\n",
      "Training loss for batch 4675 : 0.1645597219467163\n",
      "Training loss for batch 4676 : 0.09612086415290833\n",
      "Training loss for batch 4677 : 0.21879954636096954\n",
      "Training loss for batch 4678 : 0.22994022071361542\n",
      "Training loss for batch 4679 : 0.0\n",
      "Training loss for batch 4680 : 0.3242478668689728\n",
      "Training loss for batch 4681 : 0.1411663442850113\n",
      "Training loss for batch 4682 : 0.13160504400730133\n",
      "Training loss for batch 4683 : 0.055437490344047546\n",
      "Training loss for batch 4684 : 0.10598107427358627\n",
      "Training loss for batch 4685 : 0.07409095764160156\n",
      "Training loss for batch 4686 : 0.17442874610424042\n",
      "Training loss for batch 4687 : 0.16585715115070343\n",
      "Training loss for batch 4688 : 0.3145639896392822\n",
      "Training loss for batch 4689 : 0.11420846730470657\n",
      "Training loss for batch 4690 : 0.29293394088745117\n",
      "Training loss for batch 4691 : 0.09828581660985947\n",
      "Training loss for batch 4692 : 0.26990145444869995\n",
      "Training loss for batch 4693 : 0.08098546415567398\n",
      "Training loss for batch 4694 : 0.36584949493408203\n",
      "Training loss for batch 4695 : 0.26590216159820557\n",
      "Training loss for batch 4696 : 0.05898745730519295\n",
      "Training loss for batch 4697 : 0.24233612418174744\n",
      "Training loss for batch 4698 : 0.15865005552768707\n",
      "Training loss for batch 4699 : 0.2357410192489624\n",
      "Training loss for batch 4700 : 0.10593539476394653\n",
      "Training loss for batch 4701 : 0.04268981143832207\n",
      "Training loss for batch 4702 : 0.026406852528452873\n",
      "Training loss for batch 4703 : 0.01674763485789299\n",
      "Training loss for batch 4704 : 0.14544281363487244\n",
      "Training loss for batch 4705 : 0.19996686279773712\n",
      "Training loss for batch 4706 : 0.035511717200279236\n",
      "Training loss for batch 4707 : 0.10009688138961792\n",
      "Training loss for batch 4708 : 0.0\n",
      "Training loss for batch 4709 : 0.06587470322847366\n",
      "Training loss for batch 4710 : 0.16778220236301422\n",
      "Training loss for batch 4711 : 0.24889421463012695\n",
      "Training loss for batch 4712 : 0.2715030014514923\n",
      "Training loss for batch 4713 : 0.4277504086494446\n",
      "Training loss for batch 4714 : 0.03906261920928955\n",
      "Training loss for batch 4715 : 0.13977289199829102\n",
      "Training loss for batch 4716 : 0.2627979815006256\n",
      "Training loss for batch 4717 : 0.20385564863681793\n",
      "Training loss for batch 4718 : 0.012509982101619244\n",
      "Training loss for batch 4719 : 0.20645225048065186\n",
      "Training loss for batch 4720 : 0.03576553985476494\n",
      "Training loss for batch 4721 : 0.17906366288661957\n",
      "Training loss for batch 4722 : 0.04428315907716751\n",
      "Training loss for batch 4723 : 0.12694485485553741\n",
      "Training loss for batch 4724 : 0.035631462931632996\n",
      "Training loss for batch 4725 : 0.0798216387629509\n",
      "Training loss for batch 4726 : 0.017906343564391136\n",
      "Training loss for batch 4727 : 0.2530405521392822\n",
      "Training loss for batch 4728 : 0.19075018167495728\n",
      "Training loss for batch 4729 : 0.0968657061457634\n",
      "Training loss for batch 4730 : 0.18594877421855927\n",
      "Training loss for batch 4731 : 0.3761417269706726\n",
      "Training loss for batch 4732 : 0.2736981213092804\n",
      "Training loss for batch 4733 : 0.06749245524406433\n",
      "Training loss for batch 4734 : 0.055203069001436234\n",
      "Training loss for batch 4735 : 0.2898860275745392\n",
      "Training loss for batch 4736 : 0.19376902282238007\n",
      "Training loss for batch 4737 : 0.32736170291900635\n",
      "Training loss for batch 4738 : 0.047141823917627335\n",
      "Training loss for batch 4739 : 0.024084361270070076\n",
      "Training loss for batch 4740 : 0.2083948254585266\n",
      "Training loss for batch 4741 : 0.16336289048194885\n",
      "Training loss for batch 4742 : 0.08065558969974518\n",
      "Training loss for batch 4743 : 0.22277313470840454\n",
      "Training loss for batch 4744 : 0.114239402115345\n",
      "Training loss for batch 4745 : 0.37088751792907715\n",
      "Training loss for batch 4746 : 0.013640966266393661\n",
      "Training loss for batch 4747 : 0.13063447177410126\n",
      "Training loss for batch 4748 : 0.04244408756494522\n",
      "Training loss for batch 4749 : 0.19163547456264496\n",
      "Training loss for batch 4750 : 0.0460105799138546\n",
      "Training loss for batch 4751 : 0.07721451669931412\n",
      "Training loss for batch 4752 : 0.06067371740937233\n",
      "Training loss for batch 4753 : 0.12191129475831985\n",
      "Training loss for batch 4754 : 0.14676377177238464\n",
      "Training loss for batch 4755 : 0.13767562806606293\n",
      "Training loss for batch 4756 : 0.054604072123765945\n",
      "Training loss for batch 4757 : 0.2542814314365387\n",
      "Training loss for batch 4758 : 0.1261758804321289\n",
      "Training loss for batch 4759 : 0.03575723245739937\n",
      "Training loss for batch 4760 : 0.26279085874557495\n",
      "Training loss for batch 4761 : 0.056293830275535583\n",
      "Training loss for batch 4762 : 0.18627704679965973\n",
      "Training loss for batch 4763 : 0.24701985716819763\n",
      "Training loss for batch 4764 : 0.29555416107177734\n",
      "Training loss for batch 4765 : 0.1472388207912445\n",
      "Training loss for batch 4766 : 0.3030048906803131\n",
      "Training loss for batch 4767 : 0.20177048444747925\n",
      "Training loss for batch 4768 : 0.030296199023723602\n",
      "Training loss for batch 4769 : 0.014819914475083351\n",
      "Training loss for batch 4770 : 0.18480347096920013\n",
      "Training loss for batch 4771 : 0.3891022801399231\n",
      "Training loss for batch 4772 : 0.1801214963197708\n",
      "Training loss for batch 4773 : 0.14513570070266724\n",
      "Training loss for batch 4774 : 0.2690507471561432\n",
      "Training loss for batch 4775 : 0.2408590018749237\n",
      "Training loss for batch 4776 : 0.34539663791656494\n",
      "Training loss for batch 4777 : 0.1362314075231552\n",
      "Training loss for batch 4778 : 0.35257676243782043\n",
      "Training loss for batch 4779 : 0.19365622103214264\n",
      "Training loss for batch 4780 : 0.3050975203514099\n",
      "Training loss for batch 4781 : 0.04206543043255806\n",
      "Training loss for batch 4782 : 0.1329512745141983\n",
      "Training loss for batch 4783 : 0.1939868927001953\n",
      "Training loss for batch 4784 : 0.08185962587594986\n",
      "Training loss for batch 4785 : 0.3283490836620331\n",
      "Training loss for batch 4786 : -0.0013592703035101295\n",
      "Training loss for batch 4787 : 0.07464416325092316\n",
      "Training loss for batch 4788 : 0.14757893979549408\n",
      "Training loss for batch 4789 : 0.17266373336315155\n",
      "Training loss for batch 4790 : -0.00017946353182196617\n",
      "Training loss for batch 4791 : 0.011171480640769005\n",
      "Training loss for batch 4792 : 0.13766445219516754\n",
      "Training loss for batch 4793 : 0.21704436838626862\n",
      "Training loss for batch 4794 : 0.11634066700935364\n",
      "Training loss for batch 4795 : 0.11208876967430115\n",
      "Training loss for batch 4796 : 0.2729969620704651\n",
      "Training loss for batch 4797 : 0.09047257900238037\n",
      "Training loss for batch 4798 : 0.14117559790611267\n",
      "Training loss for batch 4799 : 0.16764125227928162\n",
      "Training loss for batch 4800 : 0.13355402648448944\n",
      "Training loss for batch 4801 : 0.031056221574544907\n",
      "Training loss for batch 4802 : 0.25807076692581177\n",
      "Training loss for batch 4803 : 0.06272804737091064\n",
      "Training loss for batch 4804 : 0.034173451364040375\n",
      "Training loss for batch 4805 : 0.06832344084978104\n",
      "Training loss for batch 4806 : 0.18065893650054932\n",
      "Training loss for batch 4807 : 0.14106498658657074\n",
      "Training loss for batch 4808 : 0.051495522260665894\n",
      "Training loss for batch 4809 : 0.15410521626472473\n",
      "Training loss for batch 4810 : 0.15929724276065826\n",
      "Training loss for batch 4811 : 0.16447007656097412\n",
      "Training loss for batch 4812 : 0.04998544603586197\n",
      "Training loss for batch 4813 : 0.31791120767593384\n",
      "Training loss for batch 4814 : 0.2163684070110321\n",
      "Training loss for batch 4815 : 0.07970243692398071\n",
      "Training loss for batch 4816 : 0.12806710600852966\n",
      "Training loss for batch 4817 : 0.10249362140893936\n",
      "Training loss for batch 4818 : 0.29264119267463684\n",
      "Training loss for batch 4819 : 0.12161090224981308\n",
      "Training loss for batch 4820 : 0.07360329478979111\n",
      "Training loss for batch 4821 : 0.028858624398708344\n",
      "Training loss for batch 4822 : 0.2681712806224823\n",
      "Training loss for batch 4823 : 0.2109871506690979\n",
      "Training loss for batch 4824 : 0.11300671100616455\n",
      "Training loss for batch 4825 : 0.20082825422286987\n",
      "Training loss for batch 4826 : 0.1683923453092575\n",
      "Training loss for batch 4827 : 0.0\n",
      "Training loss for batch 4828 : 0.03232654929161072\n",
      "Training loss for batch 4829 : 0.15262697637081146\n",
      "Training loss for batch 4830 : 0.047701191157102585\n",
      "Training loss for batch 4831 : 0.1072552502155304\n",
      "Training loss for batch 4832 : 0.16916640102863312\n",
      "Training loss for batch 4833 : 0.1726887822151184\n",
      "Training loss for batch 4834 : 0.15862679481506348\n",
      "Training loss for batch 4835 : 0.21957537531852722\n",
      "Training loss for batch 4836 : 0.018899906426668167\n",
      "Training loss for batch 4837 : 0.08074022829532623\n",
      "Training loss for batch 4838 : 0.09235568344593048\n",
      "Training loss for batch 4839 : 0.022972632199525833\n",
      "Training loss for batch 4840 : 0.019003182649612427\n",
      "Training loss for batch 4841 : 0.39019912481307983\n",
      "Training loss for batch 4842 : 0.1741766780614853\n",
      "Training loss for batch 4843 : 0.2170671820640564\n",
      "Training loss for batch 4844 : 0.19409528374671936\n",
      "Training loss for batch 4845 : 0.04900239780545235\n",
      "Training loss for batch 4846 : 0.1771496683359146\n",
      "Training loss for batch 4847 : 0.18126679956912994\n",
      "Training loss for batch 4848 : 0.2554001808166504\n",
      "Training loss for batch 4849 : 0.08698993921279907\n",
      "Training loss for batch 4850 : 0.22641445696353912\n",
      "Training loss for batch 4851 : 0.055614303797483444\n",
      "Training loss for batch 4852 : 0.09532254934310913\n",
      "Training loss for batch 4853 : 0.15151293575763702\n",
      "Training loss for batch 4854 : 0.0308864526450634\n",
      "Training loss for batch 4855 : 0.032592758536338806\n",
      "Training loss for batch 4856 : 0.15059658885002136\n",
      "Training loss for batch 4857 : 0.10821961611509323\n",
      "Training loss for batch 4858 : 0.11025742441415787\n",
      "Training loss for batch 4859 : 0.23287710547447205\n",
      "Training loss for batch 4860 : 0.09927980601787567\n",
      "Training loss for batch 4861 : 0.1406269520521164\n",
      "Training loss for batch 4862 : 0.07341085374355316\n",
      "Training loss for batch 4863 : 0.09262973070144653\n",
      "Training loss for batch 4864 : 0.21230533719062805\n",
      "Training loss for batch 4865 : 0.010468274354934692\n",
      "Training loss for batch 4866 : 0.20593316853046417\n",
      "Training loss for batch 4867 : 0.2683776319026947\n",
      "Training loss for batch 4868 : 0.23021307587623596\n",
      "Training loss for batch 4869 : 0.06679470092058182\n",
      "Training loss for batch 4870 : 0.02405562438070774\n",
      "Training loss for batch 4871 : 0.2893046438694\n",
      "Training loss for batch 4872 : 0.2664267420768738\n",
      "Training loss for batch 4873 : 0.10328911989927292\n",
      "Training loss for batch 4874 : 0.17342202365398407\n",
      "Training loss for batch 4875 : 0.07392597943544388\n",
      "Training loss for batch 4876 : 0.3399263322353363\n",
      "Training loss for batch 4877 : 0.14497756958007812\n",
      "Training loss for batch 4878 : 0.0\n",
      "Training loss for batch 4879 : 0.17406703531742096\n",
      "Training loss for batch 4880 : 0.19708774983882904\n",
      "Training loss for batch 4881 : 0.2649901509284973\n",
      "Training loss for batch 4882 : 0.09332232922315598\n",
      "Training loss for batch 4883 : 0.17341552674770355\n",
      "Training loss for batch 4884 : 0.12168757617473602\n",
      "Training loss for batch 4885 : 0.13041260838508606\n",
      "Training loss for batch 4886 : 0.07876206934452057\n",
      "Training loss for batch 4887 : 0.12275823205709457\n",
      "Training loss for batch 4888 : 0.16434213519096375\n",
      "Training loss for batch 4889 : 0.26715195178985596\n",
      "Training loss for batch 4890 : 0.1051064059138298\n",
      "Training loss for batch 4891 : 0.004272212740033865\n",
      "Training loss for batch 4892 : 0.0736762136220932\n",
      "Training loss for batch 4893 : 0.05277477204799652\n",
      "Training loss for batch 4894 : 0.13850417733192444\n",
      "Training loss for batch 4895 : 0.12903790175914764\n",
      "Training loss for batch 4896 : 0.21645118296146393\n",
      "Training loss for batch 4897 : 0.2388419806957245\n",
      "Training loss for batch 4898 : 0.05490648001432419\n",
      "Training loss for batch 4899 : 0.16646140813827515\n",
      "Training loss for batch 4900 : 0.14241322875022888\n",
      "Training loss for batch 4901 : 0.2608441114425659\n",
      "Training loss for batch 4902 : 0.04037187993526459\n",
      "Training loss for batch 4903 : 0.032365601509809494\n",
      "Training loss for batch 4904 : 0.0\n",
      "Training loss for batch 4905 : 0.1409236192703247\n",
      "Training loss for batch 4906 : 0.1761554330587387\n",
      "Training loss for batch 4907 : 0.2148333489894867\n",
      "Training loss for batch 4908 : 0.255575954914093\n",
      "Training loss for batch 4909 : 0.24545824527740479\n",
      "Training loss for batch 4910 : 0.1634947657585144\n",
      "Training loss for batch 4911 : 0.16529588401317596\n",
      "Training loss for batch 4912 : 0.11721976101398468\n",
      "Training loss for batch 4913 : 0.09071280062198639\n",
      "Training loss for batch 4914 : 0.06315307319164276\n",
      "Training loss for batch 4915 : 0.011402340605854988\n",
      "Training loss for batch 4916 : 0.1121828556060791\n",
      "Training loss for batch 4917 : 0.15930970013141632\n",
      "Training loss for batch 4918 : 0.12808069586753845\n",
      "Training loss for batch 4919 : 0.15515746176242828\n",
      "Training loss for batch 4920 : 0.026553314179182053\n",
      "Training loss for batch 4921 : 0.06424076855182648\n",
      "Training loss for batch 4922 : 0.11161626875400543\n",
      "Training loss for batch 4923 : 0.055379174649715424\n",
      "Training loss for batch 4924 : 0.010175103321671486\n",
      "Training loss for batch 4925 : 0.04900112375617027\n",
      "Training loss for batch 4926 : 0.2236243486404419\n",
      "Training loss for batch 4927 : 0.05636700615286827\n",
      "Training loss for batch 4928 : 0.18934400379657745\n",
      "Training loss for batch 4929 : 0.13797198235988617\n",
      "Training loss for batch 4930 : 0.1256868541240692\n",
      "Training loss for batch 4931 : 0.07604900747537613\n",
      "Training loss for batch 4932 : 0.11178497225046158\n",
      "Training loss for batch 4933 : 0.13245832920074463\n",
      "Training loss for batch 4934 : 0.10594833642244339\n",
      "Training loss for batch 4935 : 0.08931524306535721\n",
      "Training loss for batch 4936 : 0.056081611663103104\n",
      "Training loss for batch 4937 : 0.20977133512496948\n",
      "Training loss for batch 4938 : 0.03919893875718117\n",
      "Training loss for batch 4939 : 0.1845265030860901\n",
      "Training loss for batch 4940 : 0.04536893963813782\n",
      "Training loss for batch 4941 : 0.10751320421695709\n",
      "Training loss for batch 4942 : 0.08728524297475815\n",
      "Training loss for batch 4943 : 0.017160575836896896\n",
      "Training loss for batch 4944 : 0.026325292885303497\n",
      "Training loss for batch 4945 : 0.024244707077741623\n",
      "Training loss for batch 4946 : 0.07229190319776535\n",
      "Training loss for batch 4947 : 0.061257682740688324\n",
      "Training loss for batch 4948 : 0.21009334921836853\n",
      "Training loss for batch 4949 : 0.2419835478067398\n",
      "Training loss for batch 4950 : 0.049506545066833496\n",
      "Training loss for batch 4951 : 0.3406420648097992\n",
      "Training loss for batch 4952 : 0.17928488552570343\n",
      "Training loss for batch 4953 : 0.016501031816005707\n",
      "Training loss for batch 4954 : 0.049985285848379135\n",
      "Training loss for batch 4955 : 0.14037534594535828\n",
      "Training loss for batch 4956 : 0.11356345564126968\n",
      "Training loss for batch 4957 : 0.12260597944259644\n",
      "Training loss for batch 4958 : 0.08481389284133911\n",
      "Training loss for batch 4959 : 0.17137037217617035\n",
      "Training loss for batch 4960 : 0.060747839510440826\n",
      "Training loss for batch 4961 : 0.0\n",
      "Training loss for batch 4962 : 0.13862384855747223\n",
      "Training loss for batch 4963 : 0.23534142971038818\n",
      "Training loss for batch 4964 : 0.0491180494427681\n",
      "Training loss for batch 4965 : 0.06913572549819946\n",
      "Training loss for batch 4966 : 0.14378787577152252\n",
      "Training loss for batch 4967 : 0.254681795835495\n",
      "Training loss for batch 4968 : 0.36591818928718567\n",
      "Training loss for batch 4969 : 0.06264897435903549\n",
      "Training loss for batch 4970 : 0.26876094937324524\n",
      "Training loss for batch 4971 : 0.21227267384529114\n",
      "Training loss for batch 4972 : 0.04792580008506775\n",
      "Training loss for batch 4973 : 0.028385281562805176\n",
      "Training loss for batch 4974 : 0.09140489995479584\n",
      "Training loss for batch 4975 : 0.29343321919441223\n",
      "Training loss for batch 4976 : 0.13698266446590424\n",
      "Training loss for batch 4977 : 0.36630234122276306\n",
      "Training loss for batch 4978 : 0.04350440949201584\n",
      "Training loss for batch 4979 : 0.10541407018899918\n",
      "Training loss for batch 4980 : 0.11789623647928238\n",
      "Training loss for batch 4981 : 0.03760503605008125\n",
      "Training loss for batch 4982 : 0.24447007477283478\n",
      "Training loss for batch 4983 : 0.09079603850841522\n",
      "Training loss for batch 4984 : 0.015741288661956787\n",
      "Training loss for batch 4985 : 0.2649461030960083\n",
      "Training loss for batch 4986 : 0.17537394165992737\n",
      "Training loss for batch 4987 : 0.17166846990585327\n",
      "Training loss for batch 4988 : 0.10269710421562195\n",
      "Training loss for batch 4989 : 0.25994443893432617\n",
      "Training loss for batch 4990 : 0.21255069971084595\n",
      "Training loss for batch 4991 : 0.12415257096290588\n",
      "Training loss for batch 4992 : 0.09059742838144302\n",
      "Training loss for batch 4993 : 0.0\n",
      "Training loss for batch 4994 : 0.11444597691297531\n",
      "Training loss for batch 4995 : 0.027926307171583176\n",
      "Training loss for batch 4996 : 0.0830216184258461\n",
      "Training loss for batch 4997 : 0.12839271128177643\n",
      "Training loss for batch 4998 : 0.03840890899300575\n",
      "Training loss for batch 4999 : 0.28120601177215576\n",
      "Training loss for batch 5000 : 0.3083307147026062\n",
      "Training loss for batch 5001 : 0.049633923918008804\n",
      "Training loss for batch 5002 : 0.04888954758644104\n",
      "Training loss for batch 5003 : 0.33844003081321716\n",
      "Training loss for batch 5004 : 0.2786484360694885\n",
      "Training loss for batch 5005 : 0.3320351839065552\n",
      "Training loss for batch 5006 : 0.1427888572216034\n",
      "Training loss for batch 5007 : 0.16001452505588531\n",
      "Training loss for batch 5008 : 0.40390464663505554\n",
      "Training loss for batch 5009 : 0.008958030492067337\n",
      "Training loss for batch 5010 : 0.0782598927617073\n",
      "Training loss for batch 5011 : 0.3315450847148895\n",
      "Training loss for batch 5012 : 0.22050483524799347\n",
      "Training loss for batch 5013 : 0.03209013119339943\n",
      "Training loss for batch 5014 : 0.02864740788936615\n",
      "Training loss for batch 5015 : 0.13417480885982513\n",
      "Training loss for batch 5016 : 0.1584998220205307\n",
      "Training loss for batch 5017 : 0.07236678898334503\n",
      "Training loss for batch 5018 : 0.04001756012439728\n",
      "Training loss for batch 5019 : 0.025319863110780716\n",
      "Training loss for batch 5020 : 0.22261379659175873\n",
      "Training loss for batch 5021 : 0.04033990204334259\n",
      "Training loss for batch 5022 : 0.14012496173381805\n",
      "Training loss for batch 5023 : 0.4491364657878876\n",
      "Training loss for batch 5024 : 0.2853441536426544\n",
      "Training loss for batch 5025 : 0.15588639676570892\n",
      "Training loss for batch 5026 : 0.08031805604696274\n",
      "Training loss for batch 5027 : 0.11236555129289627\n",
      "Training loss for batch 5028 : 0.14440982043743134\n",
      "Training loss for batch 5029 : 0.11012013256549835\n",
      "Training loss for batch 5030 : 0.16240374743938446\n",
      "Training loss for batch 5031 : 0.09191709756851196\n",
      "Training loss for batch 5032 : 0.031448617577552795\n",
      "Training loss for batch 5033 : 0.3050723671913147\n",
      "Training loss for batch 5034 : 0.2425469160079956\n",
      "Training loss for batch 5035 : 0.03349054604768753\n",
      "Training loss for batch 5036 : 0.19873002171516418\n",
      "Training loss for batch 5037 : 0.1042899340391159\n",
      "Training loss for batch 5038 : 0.13633759319782257\n",
      "Training loss for batch 5039 : 0.21325893700122833\n",
      "Training loss for batch 5040 : 0.20017151534557343\n",
      "Training loss for batch 5041 : 0.10033490508794785\n",
      "Training loss for batch 5042 : 0.0969746857881546\n",
      "Training loss for batch 5043 : 0.21595987677574158\n",
      "Training loss for batch 5044 : 0.018937816843390465\n",
      "Training loss for batch 5045 : 0.2658192217350006\n",
      "Training loss for batch 5046 : 0.06327619403600693\n",
      "Training loss for batch 5047 : 0.10266706347465515\n",
      "Training loss for batch 5048 : 0.3237208127975464\n",
      "Training loss for batch 5049 : 0.08957862108945847\n",
      "Training loss for batch 5050 : 0.42542871832847595\n",
      "Training loss for batch 5051 : 0.21824872493743896\n",
      "Training loss for batch 5052 : 0.14742647111415863\n",
      "Training loss for batch 5053 : 0.15233252942562103\n",
      "Training loss for batch 5054 : 0.15542875230312347\n",
      "Training loss for batch 5055 : 0.20977365970611572\n",
      "Training loss for batch 5056 : 0.12680602073669434\n",
      "Training loss for batch 5057 : 0.10662560909986496\n",
      "Training loss for batch 5058 : 0.12901407480239868\n",
      "Training loss for batch 5059 : 0.0952768623828888\n",
      "Training loss for batch 5060 : 0.18761447072029114\n",
      "Training loss for batch 5061 : 0.19802437722682953\n",
      "Training loss for batch 5062 : 0.0\n",
      "Training loss for batch 5063 : 0.021421056240797043\n",
      "Training loss for batch 5064 : 0.38774919509887695\n",
      "Training loss for batch 5065 : 0.22041210532188416\n",
      "Training loss for batch 5066 : 0.061451539397239685\n",
      "Training loss for batch 5067 : 0.08353115618228912\n",
      "Training loss for batch 5068 : 0.24252066016197205\n",
      "Training loss for batch 5069 : 0.17514051496982574\n",
      "Training loss for batch 5070 : 0.08980163186788559\n",
      "Training loss for batch 5071 : 0.37963438034057617\n",
      "Training loss for batch 5072 : 0.10915431380271912\n",
      "Training loss for batch 5073 : 0.0023376760073006153\n",
      "Training loss for batch 5074 : 0.02366737462580204\n",
      "Training loss for batch 5075 : 0.0621134452521801\n",
      "Training loss for batch 5076 : 0.21544335782527924\n",
      "Training loss for batch 5077 : 0.028137825429439545\n",
      "Training loss for batch 5078 : 0.14598867297172546\n",
      "Training loss for batch 5079 : 0.09955237805843353\n",
      "Training loss for batch 5080 : 0.2888375222682953\n",
      "Training loss for batch 5081 : 0.03855734318494797\n",
      "Training loss for batch 5082 : 0.20967043936252594\n",
      "Training loss for batch 5083 : 0.1617228388786316\n",
      "Training loss for batch 5084 : 0.05265312269330025\n",
      "Training loss for batch 5085 : 0.02187153697013855\n",
      "Training loss for batch 5086 : 0.14947955310344696\n",
      "Training loss for batch 5087 : 0.3724590539932251\n",
      "Training loss for batch 5088 : 0.37384122610092163\n",
      "Training loss for batch 5089 : 0.018383778631687164\n",
      "Training loss for batch 5090 : 0.34667477011680603\n",
      "Training loss for batch 5091 : 0.1784934550523758\n",
      "Training loss for batch 5092 : 0.13910435140132904\n",
      "Training loss for batch 5093 : 0.0937078520655632\n",
      "Training loss for batch 5094 : 0.18131226301193237\n",
      "Training loss for batch 5095 : 0.12222326546907425\n",
      "Training loss for batch 5096 : 0.14755713939666748\n",
      "Training loss for batch 5097 : 0.21028171479701996\n",
      "Training loss for batch 5098 : 0.003921836614608765\n",
      "Training loss for batch 5099 : 0.4880494177341461\n",
      "Training loss for batch 5100 : 0.3774644434452057\n",
      "Training loss for batch 5101 : 0.4051016867160797\n",
      "Training loss for batch 5102 : 0.07337083667516708\n",
      "Training loss for batch 5103 : 0.06940488517284393\n",
      "Training loss for batch 5104 : 0.30904120206832886\n",
      "Training loss for batch 5105 : 0.13150446116924286\n",
      "Training loss for batch 5106 : 0.3939021825790405\n",
      "Training loss for batch 5107 : 0.06866966187953949\n",
      "Training loss for batch 5108 : 0.09684521704912186\n",
      "Training loss for batch 5109 : 0.04908907786011696\n",
      "Training loss for batch 5110 : 0.1849777102470398\n",
      "Training loss for batch 5111 : 0.39422234892845154\n",
      "Training loss for batch 5112 : 0.07088906317949295\n",
      "Training loss for batch 5113 : 0.12170790135860443\n",
      "Training loss for batch 5114 : 0.10447067022323608\n",
      "Training loss for batch 5115 : 0.016104578971862793\n",
      "Training loss for batch 5116 : 0.18293675780296326\n",
      "Training loss for batch 5117 : 0.02455296739935875\n",
      "Training loss for batch 5118 : 0.12427057325839996\n",
      "Training loss for batch 5119 : 0.05902305617928505\n",
      "Training loss for batch 5120 : 0.21608670055866241\n",
      "Training loss for batch 5121 : 0.17759883403778076\n",
      "Training loss for batch 5122 : 0.3884115517139435\n",
      "Training loss for batch 5123 : 0.0283031202852726\n",
      "Training loss for batch 5124 : 0.22625331580638885\n",
      "Training loss for batch 5125 : 0.14037151634693146\n",
      "Training loss for batch 5126 : 0.22868071496486664\n",
      "Training loss for batch 5127 : 0.06783851981163025\n",
      "Training loss for batch 5128 : 0.215645432472229\n",
      "Training loss for batch 5129 : 0.18563415110111237\n",
      "Training loss for batch 5130 : 0.1768069714307785\n",
      "Training loss for batch 5131 : 0.03618721291422844\n",
      "Training loss for batch 5132 : 0.03789413720369339\n",
      "Training loss for batch 5133 : 0.1257794052362442\n",
      "Training loss for batch 5134 : 0.32038772106170654\n",
      "Training loss for batch 5135 : 0.07665940374135971\n",
      "Training loss for batch 5136 : 0.10517504066228867\n",
      "Training loss for batch 5137 : 0.17752385139465332\n",
      "Training loss for batch 5138 : 0.1890222579240799\n",
      "Training loss for batch 5139 : 0.20850993692874908\n",
      "Training loss for batch 5140 : 0.1278678923845291\n",
      "Training loss for batch 5141 : 0.11165351420640945\n",
      "Training loss for batch 5142 : 0.1042838841676712\n",
      "Training loss for batch 5143 : 0.18792444467544556\n",
      "Training loss for batch 5144 : 0.1968948394060135\n",
      "Training loss for batch 5145 : 0.36653897166252136\n",
      "Training loss for batch 5146 : 0.018838506191968918\n",
      "Training loss for batch 5147 : 0.16310428082942963\n",
      "Training loss for batch 5148 : 0.054535310715436935\n",
      "Training loss for batch 5149 : 0.07032129168510437\n",
      "Training loss for batch 5150 : 0.20460131764411926\n",
      "Training loss for batch 5151 : 0.034826621413230896\n",
      "Training loss for batch 5152 : 0.13983774185180664\n",
      "Training loss for batch 5153 : 0.0\n",
      "Training loss for batch 5154 : 0.015289285220205784\n",
      "Training loss for batch 5155 : 0.08420060575008392\n",
      "Training loss for batch 5156 : 0.08859869837760925\n",
      "Training loss for batch 5157 : 0.17499308288097382\n",
      "Training loss for batch 5158 : 0.13145966827869415\n",
      "Training loss for batch 5159 : 0.16132093966007233\n",
      "Training loss for batch 5160 : 0.045294880867004395\n",
      "Training loss for batch 5161 : 0.2000918835401535\n",
      "Training loss for batch 5162 : 0.0009423792362213135\n",
      "Training loss for batch 5163 : 0.13755962252616882\n",
      "Training loss for batch 5164 : 0.11421697586774826\n",
      "Training loss for batch 5165 : 0.1376497596502304\n",
      "Training loss for batch 5166 : 0.0983971580862999\n",
      "Training loss for batch 5167 : 0.07148811966180801\n",
      "Training loss for batch 5168 : 0.15451931953430176\n",
      "Training loss for batch 5169 : 0.5812119245529175\n",
      "Training loss for batch 5170 : 0.1080046221613884\n",
      "Training loss for batch 5171 : 0.12430190294981003\n",
      "Training loss for batch 5172 : 0.23629803955554962\n",
      "Training loss for batch 5173 : 0.2768210172653198\n",
      "Training loss for batch 5174 : 0.0737953707575798\n",
      "Training loss for batch 5175 : 0.09092367440462112\n",
      "Training loss for batch 5176 : 0.2327681928873062\n",
      "Training loss for batch 5177 : 0.12407412379980087\n",
      "Training loss for batch 5178 : 0.22503161430358887\n",
      "Training loss for batch 5179 : 0.16558757424354553\n",
      "Training loss for batch 5180 : 0.08816034346818924\n",
      "Training loss for batch 5181 : 0.18796280026435852\n",
      "Training loss for batch 5182 : 0.014429190196096897\n",
      "Training loss for batch 5183 : 0.10847727954387665\n",
      "Training loss for batch 5184 : 0.09972408413887024\n",
      "Training loss for batch 5185 : 0.07799297571182251\n",
      "Training loss for batch 5186 : 0.17079856991767883\n",
      "Training loss for batch 5187 : 0.12917903065681458\n",
      "Training loss for batch 5188 : 0.5056901574134827\n",
      "Training loss for batch 5189 : 0.04792577773332596\n",
      "Training loss for batch 5190 : 0.10858859121799469\n",
      "Training loss for batch 5191 : 0.19627097249031067\n",
      "Training loss for batch 5192 : 0.03348135948181152\n",
      "Training loss for batch 5193 : 0.13791349530220032\n",
      "Training loss for batch 5194 : 0.05965495854616165\n",
      "Training loss for batch 5195 : 0.008102559484541416\n",
      "Training loss for batch 5196 : 0.2618681490421295\n",
      "Training loss for batch 5197 : 0.12486318498849869\n",
      "Training loss for batch 5198 : 0.09558779001235962\n",
      "Training loss for batch 5199 : 0.005690238904207945\n",
      "Training loss for batch 5200 : 0.22201138734817505\n",
      "Training loss for batch 5201 : 0.28277358412742615\n",
      "Training loss for batch 5202 : 0.08942021429538727\n",
      "Training loss for batch 5203 : 0.256719708442688\n",
      "Training loss for batch 5204 : 0.2956770658493042\n",
      "Training loss for batch 5205 : 0.14462262392044067\n",
      "Training loss for batch 5206 : 0.24454621970653534\n",
      "Training loss for batch 5207 : 0.08536261320114136\n",
      "Training loss for batch 5208 : 0.190837562084198\n",
      "Training loss for batch 5209 : 0.17565014958381653\n",
      "Training loss for batch 5210 : 0.0855197086930275\n",
      "Training loss for batch 5211 : 0.26378771662712097\n",
      "Training loss for batch 5212 : 0.02219357341527939\n",
      "Training loss for batch 5213 : 0.05860976502299309\n",
      "Training loss for batch 5214 : 0.17747583985328674\n",
      "Training loss for batch 5215 : 0.16781307756900787\n",
      "Training loss for batch 5216 : 0.14664380252361298\n",
      "Training loss for batch 5217 : 0.26585888862609863\n",
      "Training loss for batch 5218 : 0.19398649036884308\n",
      "Training loss for batch 5219 : 0.24651220440864563\n",
      "Training loss for batch 5220 : 0.018557433038949966\n",
      "Training loss for batch 5221 : 0.03285020589828491\n",
      "Training loss for batch 5222 : 0.4019792377948761\n",
      "Training loss for batch 5223 : 0.09288201481103897\n",
      "Training loss for batch 5224 : 0.020141582936048508\n",
      "Training loss for batch 5225 : 0.2390124350786209\n",
      "Training loss for batch 5226 : 0.0691847950220108\n",
      "Training loss for batch 5227 : 0.06384649872779846\n",
      "Training loss for batch 5228 : 0.0692787691950798\n",
      "Training loss for batch 5229 : 0.12207706272602081\n",
      "Training loss for batch 5230 : 0.10442869365215302\n",
      "Training loss for batch 5231 : 0.05510396510362625\n",
      "Training loss for batch 5232 : 0.03407258167862892\n",
      "Training loss for batch 5233 : 0.07989909499883652\n",
      "Training loss for batch 5234 : 0.07590342313051224\n",
      "Training loss for batch 5235 : 0.3189217746257782\n",
      "Training loss for batch 5236 : 0.2711890935897827\n",
      "Training loss for batch 5237 : 0.17565877735614777\n",
      "Training loss for batch 5238 : 0.07838603854179382\n",
      "Training loss for batch 5239 : 0.05217916518449783\n",
      "Training loss for batch 5240 : 0.24966345727443695\n",
      "Training loss for batch 5241 : 0.09490939229726791\n",
      "Training loss for batch 5242 : 0.055729079991579056\n",
      "Training loss for batch 5243 : 0.15378135442733765\n",
      "Training loss for batch 5244 : 0.13531959056854248\n",
      "Training loss for batch 5245 : 0.1676994413137436\n",
      "Training loss for batch 5246 : 0.15534651279449463\n",
      "Training loss for batch 5247 : 0.24722811579704285\n",
      "Training loss for batch 5248 : 0.10710453987121582\n",
      "Training loss for batch 5249 : 0.25723204016685486\n",
      "Training loss for batch 5250 : 0.13781249523162842\n",
      "Training loss for batch 5251 : 0.004832655191421509\n",
      "Training loss for batch 5252 : 0.05482659861445427\n",
      "Training loss for batch 5253 : 0.21137401461601257\n",
      "Training loss for batch 5254 : 0.039086852222681046\n",
      "Training loss for batch 5255 : 0.04133974760770798\n",
      "Training loss for batch 5256 : 0.26420825719833374\n",
      "Training loss for batch 5257 : 0.009771740064024925\n",
      "Training loss for batch 5258 : 0.04131128638982773\n",
      "Training loss for batch 5259 : 0.26223647594451904\n",
      "Training loss for batch 5260 : 0.06132802367210388\n",
      "Training loss for batch 5261 : 0.37227314710617065\n",
      "Training loss for batch 5262 : 0.1870347559452057\n",
      "Training loss for batch 5263 : 0.10287930071353912\n",
      "Training loss for batch 5264 : 0.17965033650398254\n",
      "Training loss for batch 5265 : 0.06720087677240372\n",
      "Training loss for batch 5266 : 0.21701034903526306\n",
      "Training loss for batch 5267 : 0.2164345681667328\n",
      "Training loss for batch 5268 : 0.09737515449523926\n",
      "Training loss for batch 5269 : 0.08012829720973969\n",
      "Training loss for batch 5270 : 0.14678747951984406\n",
      "Training loss for batch 5271 : 0.09229663014411926\n",
      "Training loss for batch 5272 : 0.014583016745746136\n",
      "Training loss for batch 5273 : 0.02701655775308609\n",
      "Training loss for batch 5274 : 0.15210828185081482\n",
      "Training loss for batch 5275 : 0.22804655134677887\n",
      "Training loss for batch 5276 : 0.1635991483926773\n",
      "Training loss for batch 5277 : 0.33802318572998047\n",
      "Training loss for batch 5278 : 0.339608371257782\n",
      "Training loss for batch 5279 : 0.1648608297109604\n",
      "Training loss for batch 5280 : 0.08010242879390717\n",
      "Training loss for batch 5281 : 0.06968896090984344\n",
      "Training loss for batch 5282 : 0.01785297319293022\n",
      "Training loss for batch 5283 : 0.05704197287559509\n",
      "Training loss for batch 5284 : 0.12724092602729797\n",
      "Training loss for batch 5285 : 0.1471600979566574\n",
      "Training loss for batch 5286 : 0.025929396972060204\n",
      "Training loss for batch 5287 : 0.29173368215560913\n",
      "Training loss for batch 5288 : 0.2582464814186096\n",
      "Training loss for batch 5289 : 0.019950315356254578\n",
      "Training loss for batch 5290 : 0.26387566328048706\n",
      "Training loss for batch 5291 : 0.01720409467816353\n",
      "Training loss for batch 5292 : 0.06496331095695496\n",
      "Training loss for batch 5293 : 0.17111168801784515\n",
      "Training loss for batch 5294 : 0.07133996486663818\n",
      "Training loss for batch 5295 : 0.030823830515146255\n",
      "Training loss for batch 5296 : 0.14969493448734283\n",
      "Training loss for batch 5297 : 0.06309475004673004\n",
      "Training loss for batch 5298 : 0.09636767208576202\n",
      "Training loss for batch 5299 : 0.33154964447021484\n",
      "Training loss for batch 5300 : 0.19123704731464386\n",
      "Training loss for batch 5301 : 0.056449584662914276\n",
      "Training loss for batch 5302 : 0.03769460693001747\n",
      "Training loss for batch 5303 : 0.3307252526283264\n",
      "Training loss for batch 5304 : 0.15596210956573486\n",
      "Training loss for batch 5305 : 0.05710601434111595\n",
      "Training loss for batch 5306 : 0.19522671401500702\n",
      "Training loss for batch 5307 : 0.2797401249408722\n",
      "Training loss for batch 5308 : 0.09956086426973343\n",
      "Training loss for batch 5309 : 0.27943089604377747\n",
      "Training loss for batch 5310 : 0.0\n",
      "Training loss for batch 5311 : 0.297579288482666\n",
      "Training loss for batch 5312 : 0.23081403970718384\n",
      "Training loss for batch 5313 : 0.1401398926973343\n",
      "Training loss for batch 5314 : 0.06484933942556381\n",
      "Training loss for batch 5315 : 0.2166643589735031\n",
      "Training loss for batch 5316 : 0.04330082982778549\n",
      "Training loss for batch 5317 : 0.018442749977111816\n",
      "Training loss for batch 5318 : 0.4320791959762573\n",
      "Training loss for batch 5319 : 0.15658673644065857\n",
      "Training loss for batch 5320 : 0.057305872440338135\n",
      "Training loss for batch 5321 : 0.01473819836974144\n",
      "Training loss for batch 5322 : 0.061963729560375214\n",
      "Training loss for batch 5323 : 0.13165661692619324\n",
      "Training loss for batch 5324 : 0.14398711919784546\n",
      "Training loss for batch 5325 : 0.04477246478199959\n",
      "Training loss for batch 5326 : 0.08946509659290314\n",
      "Training loss for batch 5327 : 0.23519004881381989\n",
      "Training loss for batch 5328 : 0.048389982432127\n",
      "Training loss for batch 5329 : 0.13965480029582977\n",
      "Training loss for batch 5330 : 0.2954443097114563\n",
      "Training loss for batch 5331 : 0.13516169786453247\n",
      "Training loss for batch 5332 : 0.11933185905218124\n",
      "Training loss for batch 5333 : 0.20229825377464294\n",
      "Training loss for batch 5334 : 0.1627257764339447\n",
      "Training loss for batch 5335 : 0.047521039843559265\n",
      "Training loss for batch 5336 : 0.03618090972304344\n",
      "Training loss for batch 5337 : 0.28400588035583496\n",
      "Training loss for batch 5338 : 0.28526178002357483\n",
      "Training loss for batch 5339 : 0.0880630686879158\n",
      "Training loss for batch 5340 : 0.31459057331085205\n",
      "Training loss for batch 5341 : 0.006131857633590698\n",
      "Training loss for batch 5342 : 0.3151341676712036\n",
      "Training loss for batch 5343 : 0.1307222694158554\n",
      "Training loss for batch 5344 : 0.22603626549243927\n",
      "Training loss for batch 5345 : 0.17209790647029877\n",
      "Training loss for batch 5346 : 0.35076674818992615\n",
      "Training loss for batch 5347 : 0.06604860723018646\n",
      "Training loss for batch 5348 : 0.1345636397600174\n",
      "Training loss for batch 5349 : 0.05144612491130829\n",
      "Training loss for batch 5350 : 0.08096890896558762\n",
      "Training loss for batch 5351 : 0.0091389250010252\n",
      "Training loss for batch 5352 : 0.1648576557636261\n",
      "Training loss for batch 5353 : 0.05333460494875908\n",
      "Training loss for batch 5354 : 0.09860294312238693\n",
      "Training loss for batch 5355 : 0.16469070315361023\n",
      "Training loss for batch 5356 : 0.09245622903108597\n",
      "Training loss for batch 5357 : 0.22443275153636932\n",
      "Training loss for batch 5358 : 0.1339997500181198\n",
      "Training loss for batch 5359 : 0.13805018365383148\n",
      "Training loss for batch 5360 : 0.2069975882768631\n",
      "Training loss for batch 5361 : 0.13708072900772095\n",
      "Training loss for batch 5362 : 0.20344285666942596\n",
      "Training loss for batch 5363 : 0.1116374060511589\n",
      "Training loss for batch 5364 : 0.17360351979732513\n",
      "Training loss for batch 5365 : 0.08835159242153168\n",
      "Training loss for batch 5366 : 0.06767836958169937\n",
      "Training loss for batch 5367 : 0.17024417221546173\n",
      "Training loss for batch 5368 : 0.1612953245639801\n",
      "Training loss for batch 5369 : 0.21469886600971222\n",
      "Training loss for batch 5370 : 0.324194997549057\n",
      "Training loss for batch 5371 : 0.0447932630777359\n",
      "Training loss for batch 5372 : 0.0\n",
      "Training loss for batch 5373 : 0.1733545958995819\n",
      "Training loss for batch 5374 : 0.29553061723709106\n",
      "Training loss for batch 5375 : 0.08694286644458771\n",
      "Training loss for batch 5376 : 0.058220766484737396\n",
      "Training loss for batch 5377 : 0.1856282502412796\n",
      "Training loss for batch 5378 : 0.027407994493842125\n",
      "Training loss for batch 5379 : 0.31061533093452454\n",
      "Training loss for batch 5380 : 0.2156020998954773\n",
      "Training loss for batch 5381 : 0.1813650131225586\n",
      "Training loss for batch 5382 : 0.5343981981277466\n",
      "Training loss for batch 5383 : 0.07746682316064835\n",
      "Training loss for batch 5384 : 0.1773688942193985\n",
      "Training loss for batch 5385 : 0.18291205167770386\n",
      "Training loss for batch 5386 : 0.2921043634414673\n",
      "Training loss for batch 5387 : 0.08307304978370667\n",
      "Training loss for batch 5388 : 0.32559216022491455\n",
      "Training loss for batch 5389 : 0.24969346821308136\n",
      "Training loss for batch 5390 : 0.2716049253940582\n",
      "Training loss for batch 5391 : 0.15919435024261475\n",
      "Training loss for batch 5392 : 0.010903600603342056\n",
      "Training loss for batch 5393 : 0.10947132110595703\n",
      "Training loss for batch 5394 : 0.05865051969885826\n",
      "Training loss for batch 5395 : 0.0450906828045845\n",
      "Training loss for batch 5396 : 0.11662328988313675\n",
      "Training loss for batch 5397 : 0.09350545704364777\n",
      "Training loss for batch 5398 : 0.3331749439239502\n",
      "Training loss for batch 5399 : 0.30554211139678955\n",
      "Training loss for batch 5400 : 0.49238789081573486\n",
      "Training loss for batch 5401 : 0.11527010798454285\n",
      "Training loss for batch 5402 : 0.1018650084733963\n",
      "Training loss for batch 5403 : 0.22742918133735657\n",
      "Training loss for batch 5404 : 0.06441222131252289\n",
      "Training loss for batch 5405 : 0.06769280135631561\n",
      "Training loss for batch 5406 : 0.20640304684638977\n",
      "Training loss for batch 5407 : 0.006416767369955778\n",
      "Training loss for batch 5408 : 0.30933690071105957\n",
      "Training loss for batch 5409 : 0.09878744930028915\n",
      "Training loss for batch 5410 : 0.08022710680961609\n",
      "Training loss for batch 5411 : 0.15403234958648682\n",
      "Training loss for batch 5412 : 0.3311980366706848\n",
      "Training loss for batch 5413 : 0.3219485580921173\n",
      "Training loss for batch 5414 : 0.4981817305088043\n",
      "Training loss for batch 5415 : 0.16320431232452393\n",
      "Training loss for batch 5416 : 0.0067888470366597176\n",
      "Training loss for batch 5417 : 0.10147494822740555\n",
      "Training loss for batch 5418 : 0.10606662929058075\n",
      "Training loss for batch 5419 : 0.1899223029613495\n",
      "Training loss for batch 5420 : 0.018291467800736427\n",
      "Training loss for batch 5421 : 0.08025822043418884\n",
      "Training loss for batch 5422 : 0.0\n",
      "Training loss for batch 5423 : 0.30018892884254456\n",
      "Training loss for batch 5424 : 0.007100217510014772\n",
      "Training loss for batch 5425 : 0.20313847064971924\n",
      "Training loss for batch 5426 : 0.14913514256477356\n",
      "Training loss for batch 5427 : 0.1010742262005806\n",
      "Training loss for batch 5428 : 0.06616578996181488\n",
      "Training loss for batch 5429 : 0.1029081866145134\n",
      "Training loss for batch 5430 : 0.10711223632097244\n",
      "Training loss for batch 5431 : 0.06858209520578384\n",
      "Training loss for batch 5432 : 0.27518582344055176\n",
      "Training loss for batch 5433 : 0.40316128730773926\n",
      "Training loss for batch 5434 : 0.24038667976856232\n",
      "Training loss for batch 5435 : 0.11537901312112808\n",
      "Training loss for batch 5436 : 0.09606257826089859\n",
      "Training loss for batch 5437 : 0.10722170770168304\n",
      "Training loss for batch 5438 : 0.14488017559051514\n",
      "Training loss for batch 5439 : 0.04712393134832382\n",
      "Training loss for batch 5440 : 0.02826039306819439\n",
      "Training loss for batch 5441 : 0.031638503074645996\n",
      "Training loss for batch 5442 : 0.19671638309955597\n",
      "Training loss for batch 5443 : 0.09332288056612015\n",
      "Training loss for batch 5444 : 0.3930005133152008\n",
      "Training loss for batch 5445 : 0.07662435621023178\n",
      "Training loss for batch 5446 : 0.010534779168665409\n",
      "Training loss for batch 5447 : 0.3151075839996338\n",
      "Training loss for batch 5448 : 0.13564027845859528\n",
      "Training loss for batch 5449 : 0.26725882291793823\n",
      "Training loss for batch 5450 : 0.24227236211299896\n",
      "Training loss for batch 5451 : 0.07503817230463028\n",
      "Training loss for batch 5452 : 0.22760410606861115\n",
      "Training loss for batch 5453 : 0.19222453236579895\n",
      "Training loss for batch 5454 : 0.04878678917884827\n",
      "Training loss for batch 5455 : 0.06607155501842499\n",
      "Training loss for batch 5456 : 0.07338136434555054\n",
      "Training loss for batch 5457 : 0.0129803866147995\n",
      "Training loss for batch 5458 : 0.19543276727199554\n",
      "Training loss for batch 5459 : 0.17878684401512146\n",
      "Training loss for batch 5460 : 0.16731896996498108\n",
      "Training loss for batch 5461 : 0.13392691314220428\n",
      "Training loss for batch 5462 : 0.19128017127513885\n",
      "Training loss for batch 5463 : 0.08034422993659973\n",
      "Training loss for batch 5464 : 0.16327887773513794\n",
      "Training loss for batch 5465 : 0.2599581182003021\n",
      "Training loss for batch 5466 : 0.10747314244508743\n",
      "Training loss for batch 5467 : 0.2085074484348297\n",
      "Training loss for batch 5468 : 0.36971789598464966\n",
      "Training loss for batch 5469 : 0.23771226406097412\n",
      "Training loss for batch 5470 : 0.33608052134513855\n",
      "Training loss for batch 5471 : 0.05972115695476532\n",
      "Training loss for batch 5472 : 0.31436023116111755\n",
      "Training loss for batch 5473 : 0.33063703775405884\n",
      "Training loss for batch 5474 : 0.015110906213521957\n",
      "Training loss for batch 5475 : 0.129359632730484\n",
      "Training loss for batch 5476 : 0.3208964765071869\n",
      "Training loss for batch 5477 : 0.046241555362939835\n",
      "Training loss for batch 5478 : 0.07335307449102402\n",
      "Training loss for batch 5479 : 0.2531360387802124\n",
      "Training loss for batch 5480 : 0.31014829874038696\n",
      "Training loss for batch 5481 : 0.08994773775339127\n",
      "Training loss for batch 5482 : 0.05523689091205597\n",
      "Training loss for batch 5483 : 0.0826001912355423\n",
      "Training loss for batch 5484 : 0.17466817796230316\n",
      "Training loss for batch 5485 : 0.2665346562862396\n",
      "Training loss for batch 5486 : 0.06548658013343811\n",
      "Training loss for batch 5487 : 0.22629232704639435\n",
      "Training loss for batch 5488 : 0.06827034056186676\n",
      "Training loss for batch 5489 : 0.22906363010406494\n",
      "Training loss for batch 5490 : 0.00026391944265924394\n",
      "Training loss for batch 5491 : 0.16349978744983673\n",
      "Training loss for batch 5492 : 0.06755879521369934\n",
      "Training loss for batch 5493 : 0.0953199565410614\n",
      "Training loss for batch 5494 : 0.11383996903896332\n",
      "Training loss for batch 5495 : 0.2320655882358551\n",
      "Training loss for batch 5496 : 0.44851645827293396\n",
      "Training loss for batch 5497 : 0.014801458455622196\n",
      "Training loss for batch 5498 : 0.2632758319377899\n",
      "Training loss for batch 5499 : 0.0671338438987732\n",
      "Training loss for batch 5500 : 0.07287527620792389\n",
      "Training loss for batch 5501 : 0.20691978931427002\n",
      "Training loss for batch 5502 : 0.0823814794421196\n",
      "Training loss for batch 5503 : 0.2934930920600891\n",
      "Training loss for batch 5504 : 0.11473239213228226\n",
      "Training loss for batch 5505 : 0.3322969377040863\n",
      "Training loss for batch 5506 : 0.19164785742759705\n",
      "Training loss for batch 5507 : 0.14865632355213165\n",
      "Training loss for batch 5508 : 0.179579496383667\n",
      "Training loss for batch 5509 : 0.03789764642715454\n",
      "Training loss for batch 5510 : 0.1759587526321411\n",
      "Training loss for batch 5511 : 0.012690836563706398\n",
      "Training loss for batch 5512 : 0.17293401062488556\n",
      "Training loss for batch 5513 : 0.11427848041057587\n",
      "Training loss for batch 5514 : 0.3228107988834381\n",
      "Training loss for batch 5515 : 0.22799676656723022\n",
      "Training loss for batch 5516 : 0.1630878746509552\n",
      "Training loss for batch 5517 : 0.22278264164924622\n",
      "Training loss for batch 5518 : 0.17645110189914703\n",
      "Training loss for batch 5519 : 0.09020667523145676\n",
      "Training loss for batch 5520 : 0.38775360584259033\n",
      "Training loss for batch 5521 : 0.2779998481273651\n",
      "Training loss for batch 5522 : 0.0860302597284317\n",
      "Training loss for batch 5523 : 0.09633256494998932\n",
      "Training loss for batch 5524 : 0.11037386208772659\n",
      "Training loss for batch 5525 : 0.13937386870384216\n",
      "Training loss for batch 5526 : 0.14008250832557678\n",
      "Training loss for batch 5527 : 0.04894106835126877\n",
      "Training loss for batch 5528 : 0.23636767268180847\n",
      "Training loss for batch 5529 : 0.11838947236537933\n",
      "Training loss for batch 5530 : 0.018660441040992737\n",
      "Training loss for batch 5531 : 0.04537082836031914\n",
      "Training loss for batch 5532 : 0.3009563684463501\n",
      "Training loss for batch 5533 : 0.05937168747186661\n",
      "Training loss for batch 5534 : 0.3950512707233429\n",
      "Training loss for batch 5535 : 0.0598660446703434\n",
      "Training loss for batch 5536 : 0.07340160012245178\n",
      "Training loss for batch 5537 : 0.06556031852960587\n",
      "Training loss for batch 5538 : 0.2746758460998535\n",
      "Training loss for batch 5539 : 0.04701852798461914\n",
      "Training loss for batch 5540 : 0.02020721510052681\n",
      "Training loss for batch 5541 : 0.07126875221729279\n",
      "Training loss for batch 5542 : 0.16680920124053955\n",
      "Training loss for batch 5543 : 0.028807491064071655\n",
      "Training loss for batch 5544 : 0.14165067672729492\n",
      "Training loss for batch 5545 : 0.13839374482631683\n",
      "Training loss for batch 5546 : 0.0015037556877359748\n",
      "Training loss for batch 5547 : 0.09812702238559723\n",
      "Training loss for batch 5548 : 0.28482121229171753\n",
      "Training loss for batch 5549 : 0.12914371490478516\n",
      "Training loss for batch 5550 : 0.21549202501773834\n",
      "Training loss for batch 5551 : 0.14298184216022491\n",
      "Training loss for batch 5552 : 0.20547492802143097\n",
      "Training loss for batch 5553 : 0.10111917555332184\n",
      "Training loss for batch 5554 : 0.20353612303733826\n",
      "Training loss for batch 5555 : 0.17191912233829498\n",
      "Training loss for batch 5556 : 0.23609678447246552\n",
      "Training loss for batch 5557 : 0.034120216965675354\n",
      "Training loss for batch 5558 : 0.1537608802318573\n",
      "Training loss for batch 5559 : 0.08778811246156693\n",
      "Training loss for batch 5560 : 0.1801498532295227\n",
      "Training loss for batch 5561 : 0.13346925377845764\n",
      "Training loss for batch 5562 : 0.025683347135782242\n",
      "Training loss for batch 5563 : 0.2528079152107239\n",
      "Training loss for batch 5564 : 0.3210320770740509\n",
      "Training loss for batch 5565 : 0.014625223353505135\n",
      "Training loss for batch 5566 : 0.10101443529129028\n",
      "Training loss for batch 5567 : 0.18421277403831482\n",
      "Training loss for batch 5568 : 0.26156413555145264\n",
      "Training loss for batch 5569 : 0.016105622053146362\n",
      "Training loss for batch 5570 : 0.1493774652481079\n",
      "Training loss for batch 5571 : 0.17721354961395264\n",
      "Training loss for batch 5572 : 0.4690523147583008\n",
      "Training loss for batch 5573 : 0.28130918741226196\n",
      "Training loss for batch 5574 : 0.020567836239933968\n",
      "Training loss for batch 5575 : 0.0\n",
      "Training loss for batch 5576 : 0.012029552832245827\n",
      "Training loss for batch 5577 : 0.06319701671600342\n",
      "Training loss for batch 5578 : 0.04578660428524017\n",
      "Training loss for batch 5579 : 0.30151984095573425\n",
      "Training loss for batch 5580 : 0.156207874417305\n",
      "Training loss for batch 5581 : 0.03383282199501991\n",
      "Training loss for batch 5582 : 0.21001380681991577\n",
      "Training loss for batch 5583 : 0.1631869375705719\n",
      "Training loss for batch 5584 : 0.05269969627261162\n",
      "Training loss for batch 5585 : 0.23458492755889893\n",
      "Training loss for batch 5586 : 0.09606388211250305\n",
      "Training loss for batch 5587 : 0.1369193196296692\n",
      "Training loss for batch 5588 : 0.09318654984235764\n",
      "Training loss for batch 5589 : 0.4051397740840912\n",
      "Training loss for batch 5590 : 0.0491633415222168\n",
      "Training loss for batch 5591 : 0.05946674942970276\n",
      "Training loss for batch 5592 : 0.19695279002189636\n",
      "Training loss for batch 5593 : 0.08567622303962708\n",
      "Training loss for batch 5594 : 0.19428512454032898\n",
      "Training loss for batch 5595 : 0.17001336812973022\n",
      "Training loss for batch 5596 : 0.17650310695171356\n",
      "Training loss for batch 5597 : 0.18366235494613647\n",
      "Training loss for batch 5598 : 0.03291952237486839\n",
      "Training loss for batch 5599 : 0.01705748401582241\n",
      "Training loss for batch 5600 : 0.19612820446491241\n",
      "Training loss for batch 5601 : 0.1699569672346115\n",
      "Training loss for batch 5602 : 0.14215898513793945\n",
      "Training loss for batch 5603 : 0.17931406199932098\n",
      "Training loss for batch 5604 : 0.2858234643936157\n",
      "Training loss for batch 5605 : 0.22246573865413666\n",
      "Training loss for batch 5606 : 0.0035985629074275494\n",
      "Training loss for batch 5607 : 0.15111885964870453\n",
      "Training loss for batch 5608 : 0.013923052698373795\n",
      "Training loss for batch 5609 : 0.13459694385528564\n",
      "Training loss for batch 5610 : 0.0450543537735939\n",
      "Training loss for batch 5611 : 0.16292858123779297\n",
      "Training loss for batch 5612 : 0.21151481568813324\n",
      "Training loss for batch 5613 : 0.0\n",
      "Training loss for batch 5614 : 0.2703666090965271\n",
      "Training loss for batch 5615 : 0.12196772545576096\n",
      "Training loss for batch 5616 : 0.23047932982444763\n",
      "Training loss for batch 5617 : 0.10785248875617981\n",
      "Training loss for batch 5618 : 0.18351511657238007\n",
      "Training loss for batch 5619 : 0.0878075361251831\n",
      "Training loss for batch 5620 : 0.022663768380880356\n",
      "Training loss for batch 5621 : 0.40298792719841003\n",
      "Training loss for batch 5622 : 0.1888158619403839\n",
      "Training loss for batch 5623 : 0.17056192457675934\n",
      "Training loss for batch 5624 : 0.15357767045497894\n",
      "Training loss for batch 5625 : 0.25568708777427673\n",
      "Training loss for batch 5626 : 0.24688702821731567\n",
      "Training loss for batch 5627 : 0.33551672101020813\n",
      "Training loss for batch 5628 : 0.10729370266199112\n",
      "Training loss for batch 5629 : 0.10488623380661011\n",
      "Training loss for batch 5630 : 0.19417515397071838\n",
      "Training loss for batch 5631 : 0.16101498901844025\n",
      "Training loss for batch 5632 : 0.11831828951835632\n",
      "Training loss for batch 5633 : 0.09337671846151352\n",
      "Training loss for batch 5634 : 0.2052241712808609\n",
      "Training loss for batch 5635 : 0.21185830235481262\n",
      "Training loss for batch 5636 : 0.17492635548114777\n",
      "Training loss for batch 5637 : 0.06914836168289185\n",
      "Training loss for batch 5638 : 0.13088010251522064\n",
      "Training loss for batch 5639 : 0.28879234194755554\n",
      "Training loss for batch 5640 : 0.11515724658966064\n",
      "Training loss for batch 5641 : 0.25410330295562744\n",
      "Training loss for batch 5642 : 0.28407061100006104\n",
      "Training loss for batch 5643 : 0.06631132960319519\n",
      "Training loss for batch 5644 : 0.11122895032167435\n",
      "Training loss for batch 5645 : 0.05494921654462814\n",
      "Training loss for batch 5646 : 0.332203209400177\n",
      "Training loss for batch 5647 : 0.25839266180992126\n",
      "Training loss for batch 5648 : 0.22775566577911377\n",
      "Training loss for batch 5649 : 0.08431533724069595\n",
      "Training loss for batch 5650 : 0.04659752547740936\n",
      "Training loss for batch 5651 : 0.07180128991603851\n",
      "Training loss for batch 5652 : 0.0006958941812627017\n",
      "Training loss for batch 5653 : 0.10448061674833298\n",
      "Training loss for batch 5654 : 0.024216020479798317\n",
      "Training loss for batch 5655 : 0.03193669766187668\n",
      "Training loss for batch 5656 : 0.25387871265411377\n",
      "Training loss for batch 5657 : 0.2586902976036072\n",
      "Training loss for batch 5658 : 0.0839766263961792\n",
      "Training loss for batch 5659 : 0.19215674698352814\n",
      "Training loss for batch 5660 : 0.1706773340702057\n",
      "Training loss for batch 5661 : 0.1225610226392746\n",
      "Training loss for batch 5662 : 0.18168708682060242\n",
      "Training loss for batch 5663 : 0.0860043615102768\n",
      "Training loss for batch 5664 : 0.004853864666074514\n",
      "Training loss for batch 5665 : 0.058486588299274445\n",
      "Training loss for batch 5666 : 0.0910240039229393\n",
      "Training loss for batch 5667 : 0.2612113356590271\n",
      "Training loss for batch 5668 : 0.3082807958126068\n",
      "Training loss for batch 5669 : 0.027457237243652344\n",
      "Training loss for batch 5670 : 0.05637861788272858\n",
      "Training loss for batch 5671 : 0.07703109085559845\n",
      "Training loss for batch 5672 : 0.03785289451479912\n",
      "Training loss for batch 5673 : 0.10589428246021271\n",
      "Training loss for batch 5674 : 0.08468660712242126\n",
      "Training loss for batch 5675 : 0.04050213843584061\n",
      "Training loss for batch 5676 : 0.25314897298812866\n",
      "Training loss for batch 5677 : 0.10277281701564789\n",
      "Training loss for batch 5678 : 0.14562031626701355\n",
      "Training loss for batch 5679 : 0.28668320178985596\n",
      "Training loss for batch 5680 : 0.0621754489839077\n",
      "Training loss for batch 5681 : -0.001631881925277412\n",
      "Training loss for batch 5682 : 0.053400296717882156\n",
      "Training loss for batch 5683 : 0.0053844451904296875\n",
      "Training loss for batch 5684 : 0.09698095917701721\n",
      "Training loss for batch 5685 : 0.0850604698061943\n",
      "Training loss for batch 5686 : 0.11477843672037125\n",
      "Training loss for batch 5687 : 0.18243038654327393\n",
      "Training loss for batch 5688 : 0.09942064434289932\n",
      "Training loss for batch 5689 : 0.006035074591636658\n",
      "Training loss for batch 5690 : 0.21192127466201782\n",
      "Training loss for batch 5691 : 0.06106003001332283\n",
      "Training loss for batch 5692 : 0.012094620615243912\n",
      "Training loss for batch 5693 : 0.04224666953086853\n",
      "Training loss for batch 5694 : 0.34067413210868835\n",
      "Training loss for batch 5695 : 0.30087077617645264\n",
      "Training loss for batch 5696 : 0.16008877754211426\n",
      "Training loss for batch 5697 : -0.0006278649671003222\n",
      "Training loss for batch 5698 : 0.23078955709934235\n",
      "Training loss for batch 5699 : 0.040155891329050064\n",
      "Training loss for batch 5700 : 0.018100693821907043\n",
      "Training loss for batch 5701 : 0.04244743287563324\n",
      "Training loss for batch 5702 : 0.18358248472213745\n",
      "Training loss for batch 5703 : 0.06628648936748505\n",
      "Training loss for batch 5704 : 0.07677866518497467\n",
      "Training loss for batch 5705 : 0.00880669616162777\n",
      "Training loss for batch 5706 : 0.20107007026672363\n",
      "Training loss for batch 5707 : 0.27789121866226196\n",
      "Training loss for batch 5708 : 0.05819954723119736\n",
      "Training loss for batch 5709 : 0.1265546828508377\n",
      "Training loss for batch 5710 : -0.00323099154047668\n",
      "Training loss for batch 5711 : 0.05815519765019417\n",
      "Training loss for batch 5712 : 0.079374760389328\n",
      "Training loss for batch 5713 : 0.3030599057674408\n",
      "Training loss for batch 5714 : 0.04145130515098572\n",
      "Training loss for batch 5715 : 0.4481950104236603\n",
      "Training loss for batch 5716 : 0.04450409114360809\n",
      "Training loss for batch 5717 : 0.2379322052001953\n",
      "Training loss for batch 5718 : 0.16841387748718262\n",
      "Training loss for batch 5719 : 0.20429491996765137\n",
      "Training loss for batch 5720 : 0.11091958731412888\n",
      "Training loss for batch 5721 : 0.029500506818294525\n",
      "Training loss for batch 5722 : 0.08535439521074295\n",
      "Training loss for batch 5723 : 0.15092171728610992\n",
      "Training loss for batch 5724 : 0.23007789254188538\n",
      "Training loss for batch 5725 : 0.10669045895338058\n",
      "Training loss for batch 5726 : 0.12708885967731476\n",
      "Training loss for batch 5727 : 0.06846404075622559\n",
      "Training loss for batch 5728 : 0.044398609548807144\n",
      "Training loss for batch 5729 : 0.3919333815574646\n",
      "Training loss for batch 5730 : 0.027511734515428543\n",
      "Training loss for batch 5731 : 0.06748885661363602\n",
      "Training loss for batch 5732 : 0.198635533452034\n",
      "Training loss for batch 5733 : 0.1638796627521515\n",
      "Training loss for batch 5734 : 0.19419428706169128\n",
      "Training loss for batch 5735 : 0.19199250638484955\n",
      "Training loss for batch 5736 : 0.5392612218856812\n",
      "Training loss for batch 5737 : 0.22885587811470032\n",
      "Training loss for batch 5738 : 0.08403761684894562\n",
      "Training loss for batch 5739 : 0.09110954403877258\n",
      "Training loss for batch 5740 : 0.03480810299515724\n",
      "Training loss for batch 5741 : 0.08848503977060318\n",
      "Training loss for batch 5742 : 0.2090023159980774\n",
      "Training loss for batch 5743 : 0.04862041398882866\n",
      "Training loss for batch 5744 : 0.23442184925079346\n",
      "Training loss for batch 5745 : 0.415211945772171\n",
      "Training loss for batch 5746 : 0.08679963648319244\n",
      "Training loss for batch 5747 : 0.022670742124319077\n",
      "Training loss for batch 5748 : 0.15151214599609375\n",
      "Training loss for batch 5749 : 0.08719637989997864\n",
      "Training loss for batch 5750 : 0.3109854459762573\n",
      "Training loss for batch 5751 : 0.07234226167201996\n",
      "Training loss for batch 5752 : 0.0352434441447258\n",
      "Training loss for batch 5753 : 0.21804337203502655\n",
      "Training loss for batch 5754 : 0.11347033083438873\n",
      "Training loss for batch 5755 : 0.1905374825000763\n",
      "Training loss for batch 5756 : 0.29348522424697876\n",
      "Training loss for batch 5757 : 0.0950021967291832\n",
      "Training loss for batch 5758 : 0.1894076019525528\n",
      "Training loss for batch 5759 : 0.01519610546529293\n",
      "Training loss for batch 5760 : 0.30433911085128784\n",
      "Training loss for batch 5761 : 0.1765160858631134\n",
      "Training loss for batch 5762 : 0.12240525335073471\n",
      "Training loss for batch 5763 : 0.1688251793384552\n",
      "Training loss for batch 5764 : 0.3396599590778351\n",
      "Training loss for batch 5765 : 0.19603046774864197\n",
      "Training loss for batch 5766 : 0.002125769853591919\n",
      "Training loss for batch 5767 : 0.07412435114383698\n",
      "Training loss for batch 5768 : 0.17928579449653625\n",
      "Training loss for batch 5769 : 0.23500989377498627\n",
      "Training loss for batch 5770 : 0.06341810524463654\n",
      "Training loss for batch 5771 : 0.08392030745744705\n",
      "Training loss for batch 5772 : 0.00434306263923645\n",
      "Training loss for batch 5773 : 0.12296838313341141\n",
      "Training loss for batch 5774 : 0.09182460606098175\n",
      "Training loss for batch 5775 : 0.16719649732112885\n",
      "Training loss for batch 5776 : 0.06154739856719971\n",
      "Training loss for batch 5777 : 0.18693187832832336\n",
      "Training loss for batch 5778 : 0.02734539285302162\n",
      "Training loss for batch 5779 : 0.03078962117433548\n",
      "Training loss for batch 5780 : 0.1425781548023224\n",
      "Training loss for batch 5781 : 0.19877272844314575\n",
      "Training loss for batch 5782 : 0.005945364944636822\n",
      "Training loss for batch 5783 : 0.04119434207677841\n",
      "Training loss for batch 5784 : 0.29903069138526917\n",
      "Training loss for batch 5785 : 0.12811116874217987\n",
      "Training loss for batch 5786 : 0.23412376642227173\n",
      "Training loss for batch 5787 : 0.160089373588562\n",
      "Training loss for batch 5788 : 0.09117548912763596\n",
      "Training loss for batch 5789 : 0.318562388420105\n",
      "Training loss for batch 5790 : 0.012747736647725105\n",
      "Training loss for batch 5791 : 0.010050254873931408\n",
      "Training loss for batch 5792 : 0.005821794271469116\n",
      "Training loss for batch 5793 : 0.05062318965792656\n",
      "Training loss for batch 5794 : 0.006640302948653698\n",
      "Training loss for batch 5795 : 0.1433066576719284\n",
      "Training loss for batch 5796 : 0.042271293699741364\n",
      "Training loss for batch 5797 : 0.2230968475341797\n",
      "Training loss for batch 5798 : 0.09882631152868271\n",
      "Training loss for batch 5799 : 0.1701509952545166\n",
      "Training loss for batch 5800 : 0.3063049912452698\n",
      "Training loss for batch 5801 : 0.17779527604579926\n",
      "Training loss for batch 5802 : 0.5822311639785767\n",
      "Training loss for batch 5803 : 0.014235561713576317\n",
      "Training loss for batch 5804 : 0.20268362760543823\n",
      "Training loss for batch 5805 : 0.11657079309225082\n",
      "Training loss for batch 5806 : 0.1469826102256775\n",
      "Training loss for batch 5807 : 0.056659772992134094\n",
      "Training loss for batch 5808 : 0.09446852654218674\n",
      "Training loss for batch 5809 : 0.330580472946167\n",
      "Training loss for batch 5810 : 0.08545810729265213\n",
      "Training loss for batch 5811 : 0.12330885231494904\n",
      "Training loss for batch 5812 : 0.17938558757305145\n",
      "Training loss for batch 5813 : 0.23591062426567078\n",
      "Training loss for batch 5814 : 0.22194159030914307\n",
      "Training loss for batch 5815 : 0.29958242177963257\n",
      "Training loss for batch 5816 : 0.02019418403506279\n",
      "Training loss for batch 5817 : 0.4303252398967743\n",
      "Training loss for batch 5818 : 0.03412487357854843\n",
      "Training loss for batch 5819 : 0.030233535915613174\n",
      "Training loss for batch 5820 : 0.0028673510532826185\n",
      "Training loss for batch 5821 : 0.059856414794921875\n",
      "Training loss for batch 5822 : 0.1646655797958374\n",
      "Training loss for batch 5823 : 0.011226902715861797\n",
      "Training loss for batch 5824 : 0.17149607837200165\n",
      "Training loss for batch 5825 : -0.0023141209967434406\n",
      "Training loss for batch 5826 : 0.3248642385005951\n",
      "Training loss for batch 5827 : 0.0\n",
      "Training loss for batch 5828 : 0.17262862622737885\n",
      "Training loss for batch 5829 : 0.20245803892612457\n",
      "Training loss for batch 5830 : 0.283058762550354\n",
      "Training loss for batch 5831 : 0.14867395162582397\n",
      "Training loss for batch 5832 : 0.11124527454376221\n",
      "Training loss for batch 5833 : 0.31442540884017944\n",
      "Training loss for batch 5834 : 0.12901650369167328\n",
      "Training loss for batch 5835 : 0.4957309365272522\n",
      "Training loss for batch 5836 : 0.11091326177120209\n",
      "Training loss for batch 5837 : 0.04112149029970169\n",
      "Training loss for batch 5838 : 0.07152421027421951\n",
      "Training loss for batch 5839 : 0.005553877912461758\n",
      "Training loss for batch 5840 : 0.026284391060471535\n",
      "Training loss for batch 5841 : 0.1370840072631836\n",
      "Training loss for batch 5842 : 0.017558950930833817\n",
      "Training loss for batch 5843 : 0.2713514268398285\n",
      "Training loss for batch 5844 : 0.3831256031990051\n",
      "Training loss for batch 5845 : 0.3901820182800293\n",
      "Training loss for batch 5846 : 0.11070915311574936\n",
      "Training loss for batch 5847 : 0.08886541426181793\n",
      "Training loss for batch 5848 : 0.45315656065940857\n",
      "Training loss for batch 5849 : 0.12304405868053436\n",
      "Training loss for batch 5850 : 0.12919658422470093\n",
      "Training loss for batch 5851 : 0.19602958858013153\n",
      "Training loss for batch 5852 : 0.08149928599596024\n",
      "Training loss for batch 5853 : 0.17910361289978027\n",
      "Training loss for batch 5854 : 0.14158867299556732\n",
      "Training loss for batch 5855 : 0.09367580711841583\n",
      "Training loss for batch 5856 : 0.013886882923543453\n",
      "Training loss for batch 5857 : 0.03889162465929985\n",
      "Training loss for batch 5858 : 0.5371609330177307\n",
      "Training loss for batch 5859 : 0.3195667564868927\n",
      "Training loss for batch 5860 : 0.33340680599212646\n",
      "Training loss for batch 5861 : 0.2596907913684845\n",
      "Training loss for batch 5862 : 0.16617517173290253\n",
      "Training loss for batch 5863 : 0.07576296478509903\n",
      "Training loss for batch 5864 : 0.10840969532728195\n",
      "Training loss for batch 5865 : 0.05988448113203049\n",
      "Training loss for batch 5866 : 0.436691015958786\n",
      "Training loss for batch 5867 : 0.139886736869812\n",
      "Training loss for batch 5868 : 0.6127607226371765\n",
      "Training loss for batch 5869 : 0.11948132514953613\n",
      "Training loss for batch 5870 : 0.2721005976200104\n",
      "Training loss for batch 5871 : 0.1154460459947586\n",
      "Training loss for batch 5872 : 0.10682976245880127\n",
      "Training loss for batch 5873 : 0.20385542511940002\n",
      "Training loss for batch 5874 : 0.15714259445667267\n",
      "Training loss for batch 5875 : 0.12224790453910828\n",
      "Training loss for batch 5876 : 0.03711821883916855\n",
      "Training loss for batch 5877 : 0.05697283148765564\n",
      "Training loss for batch 5878 : 0.22648262977600098\n",
      "Training loss for batch 5879 : 0.09079568833112717\n",
      "Training loss for batch 5880 : 0.04994041472673416\n",
      "Training loss for batch 5881 : 0.046722687780857086\n",
      "Training loss for batch 5882 : 0.16433432698249817\n",
      "Training loss for batch 5883 : 0.04150322452187538\n",
      "Training loss for batch 5884 : 0.002813786268234253\n",
      "Training loss for batch 5885 : 0.20728760957717896\n",
      "Training loss for batch 5886 : 0.08967285603284836\n",
      "Training loss for batch 5887 : 0.05220269784331322\n",
      "Training loss for batch 5888 : 0.015508951619267464\n",
      "Training loss for batch 5889 : 0.1663077026605606\n",
      "Training loss for batch 5890 : 0.291628360748291\n",
      "Training loss for batch 5891 : 0.16497041285037994\n",
      "Training loss for batch 5892 : -0.0006283670663833618\n",
      "Training loss for batch 5893 : 0.1397150754928589\n",
      "Training loss for batch 5894 : 0.20503337681293488\n",
      "Training loss for batch 5895 : 0.044052328914403915\n",
      "Training loss for batch 5896 : 0.033729277551174164\n",
      "Training loss for batch 5897 : 0.3680185079574585\n",
      "Training loss for batch 5898 : 0.12913504242897034\n",
      "Training loss for batch 5899 : 0.09264776110649109\n",
      "Training loss for batch 5900 : 0.15185344219207764\n",
      "Training loss for batch 5901 : 0.08949758857488632\n",
      "Training loss for batch 5902 : 0.25064343214035034\n",
      "Training loss for batch 5903 : 0.09722720086574554\n",
      "Training loss for batch 5904 : 0.22876188158988953\n",
      "Training loss for batch 5905 : 0.17834557592868805\n",
      "Training loss for batch 5906 : 0.05423307791352272\n",
      "Training loss for batch 5907 : 0.1522434800863266\n",
      "Training loss for batch 5908 : 0.2991030216217041\n",
      "Training loss for batch 5909 : 0.22258086502552032\n",
      "Training loss for batch 5910 : 0.12196436524391174\n",
      "Training loss for batch 5911 : 0.10635633766651154\n",
      "Training loss for batch 5912 : 0.029672710224986076\n",
      "Training loss for batch 5913 : 0.10976597666740417\n",
      "Training loss for batch 5914 : 0.12274853140115738\n",
      "Training loss for batch 5915 : 0.14017991721630096\n",
      "Training loss for batch 5916 : 0.06414201855659485\n",
      "Training loss for batch 5917 : 0.05387198552489281\n",
      "Training loss for batch 5918 : 0.007293482776731253\n",
      "Training loss for batch 5919 : 0.21318545937538147\n",
      "Training loss for batch 5920 : 0.10231911391019821\n",
      "Training loss for batch 5921 : 0.20057153701782227\n",
      "Training loss for batch 5922 : 0.08776514232158661\n",
      "Training loss for batch 5923 : 0.06715622544288635\n",
      "Training loss for batch 5924 : 0.22649367153644562\n",
      "Training loss for batch 5925 : 0.3385305404663086\n",
      "Training loss for batch 5926 : 0.1687365174293518\n",
      "Training loss for batch 5927 : 0.23518703877925873\n",
      "Training loss for batch 5928 : 0.1882004290819168\n",
      "Training loss for batch 5929 : 0.1168542206287384\n",
      "Training loss for batch 5930 : 0.08940109610557556\n",
      "Training loss for batch 5931 : 0.04072459042072296\n",
      "Training loss for batch 5932 : 0.20619653165340424\n",
      "Training loss for batch 5933 : 0.0728008970618248\n",
      "Training loss for batch 5934 : 0.037953756749629974\n",
      "Training loss for batch 5935 : 0.32242918014526367\n",
      "Training loss for batch 5936 : 0.13456496596336365\n",
      "Training loss for batch 5937 : 0.03632599860429764\n",
      "Training loss for batch 5938 : 0.06248779594898224\n",
      "Training loss for batch 5939 : 0.2618688941001892\n",
      "Training loss for batch 5940 : 0.13296404480934143\n",
      "Training loss for batch 5941 : 0.3624540865421295\n",
      "Training loss for batch 5942 : 0.1633513867855072\n",
      "Training loss for batch 5943 : 0.011966001242399216\n",
      "Training loss for batch 5944 : 0.002643299987539649\n",
      "Training loss for batch 5945 : 0.1395173966884613\n",
      "Training loss for batch 5946 : 0.16610071063041687\n",
      "Training loss for batch 5947 : 0.10714025050401688\n",
      "Training loss for batch 5948 : 0.2661808133125305\n",
      "Training loss for batch 5949 : 0.22813542187213898\n",
      "Training loss for batch 5950 : 0.40628138184547424\n",
      "Training loss for batch 5951 : 0.05804358795285225\n",
      "Training loss for batch 5952 : 0.07831794023513794\n",
      "Training loss for batch 5953 : 0.41076070070266724\n",
      "Training loss for batch 5954 : -0.001370723475702107\n",
      "Training loss for batch 5955 : 0.09761928021907806\n",
      "Training loss for batch 5956 : 0.32193371653556824\n",
      "Training loss for batch 5957 : 0.15659958124160767\n",
      "Training loss for batch 5958 : 0.2572791874408722\n",
      "Training loss for batch 5959 : 0.4744108021259308\n",
      "Training loss for batch 5960 : 0.2039152830839157\n",
      "Training loss for batch 5961 : 0.14387008547782898\n",
      "Training loss for batch 5962 : 0.1953406035900116\n",
      "Training loss for batch 5963 : 0.04667828977108002\n",
      "Training loss for batch 5964 : 0.12044350057840347\n",
      "Training loss for batch 5965 : 0.239903062582016\n",
      "Training loss for batch 5966 : 0.19571807980537415\n",
      "Training loss for batch 5967 : 0.042790256440639496\n",
      "Training loss for batch 5968 : 0.024282606318593025\n",
      "Training loss for batch 5969 : 0.18513649702072144\n",
      "Training loss for batch 5970 : 0.06027507409453392\n",
      "Training loss for batch 5971 : 0.14875245094299316\n",
      "Training loss for batch 5972 : 0.037990689277648926\n",
      "Training loss for batch 5973 : 0.09677848219871521\n",
      "Training loss for batch 5974 : 0.33318424224853516\n",
      "Training loss for batch 5975 : 0.28065821528434753\n",
      "Training loss for batch 5976 : 0.08817920088768005\n",
      "Training loss for batch 5977 : 0.18549294769763947\n",
      "Training loss for batch 5978 : 0.06187494844198227\n",
      "Training loss for batch 5979 : 0.07995893061161041\n",
      "Training loss for batch 5980 : 0.10268034040927887\n",
      "Training loss for batch 5981 : 0.3014596998691559\n",
      "Training loss for batch 5982 : 0.2938500642776489\n",
      "Training loss for batch 5983 : 0.020655415952205658\n",
      "Training loss for batch 5984 : 0.0\n",
      "Training loss for batch 5985 : 0.11440298706293106\n",
      "Training loss for batch 5986 : 0.018868517130613327\n",
      "Training loss for batch 5987 : 0.008796760812401772\n",
      "Training loss for batch 5988 : 0.10972079634666443\n",
      "Training loss for batch 5989 : 0.13159865140914917\n",
      "Training loss for batch 5990 : 0.13330484926700592\n",
      "Training loss for batch 5991 : 0.35264313220977783\n",
      "Training loss for batch 5992 : 0.11198871582746506\n",
      "Training loss for batch 5993 : 0.23887914419174194\n",
      "Training loss for batch 5994 : 0.13286687433719635\n",
      "Training loss for batch 5995 : 0.23057180643081665\n",
      "Training loss for batch 5996 : 0.03677499666810036\n",
      "Training loss for batch 5997 : 0.2386556714773178\n",
      "Training loss for batch 5998 : 0.2783096432685852\n",
      "Training loss for batch 5999 : 0.04513842612504959\n",
      "Training loss for batch 6000 : 0.24572120606899261\n",
      "Training loss for batch 6001 : 0.04332268610596657\n",
      "Training loss for batch 6002 : 0.1534723937511444\n",
      "Training loss for batch 6003 : 0.1575574278831482\n",
      "Training loss for batch 6004 : 0.11797812581062317\n",
      "Training loss for batch 6005 : 0.060075193643569946\n",
      "Training loss for batch 6006 : 0.08362780511379242\n",
      "Training loss for batch 6007 : 0.039332322776317596\n",
      "Training loss for batch 6008 : 0.24601750075817108\n",
      "Training loss for batch 6009 : 0.040954843163490295\n",
      "Training loss for batch 6010 : 0.018126117065548897\n",
      "Training loss for batch 6011 : 0.11403562873601913\n",
      "Training loss for batch 6012 : 0.17349235713481903\n",
      "Training loss for batch 6013 : 0.28368252515792847\n",
      "Training loss for batch 6014 : 0.145805224776268\n",
      "Training loss for batch 6015 : 0.05299369618296623\n",
      "Training loss for batch 6016 : 0.06022300943732262\n",
      "Training loss for batch 6017 : 0.3916078209877014\n",
      "Training loss for batch 6018 : 0.19638848304748535\n",
      "Training loss for batch 6019 : 0.6070409417152405\n",
      "Training loss for batch 6020 : 0.5332138538360596\n",
      "Training loss for batch 6021 : 0.21047577261924744\n",
      "Training loss for batch 6022 : 0.07813939452171326\n",
      "Training loss for batch 6023 : 0.2504555284976959\n",
      "Training loss for batch 6024 : 0.22129729390144348\n",
      "Training loss for batch 6025 : 0.05939971283078194\n",
      "Training loss for batch 6026 : 0.3108680844306946\n",
      "Training loss for batch 6027 : 0.06783507019281387\n",
      "Training loss for batch 6028 : 0.09176661819219589\n",
      "Training loss for batch 6029 : 0.10790984332561493\n",
      "Training loss for batch 6030 : 0.11673995852470398\n",
      "Training loss for batch 6031 : 0.014996413141489029\n",
      "Training loss for batch 6032 : 0.10895173996686935\n",
      "Training loss for batch 6033 : 0.030115902423858643\n",
      "Training loss for batch 6034 : 0.27173104882240295\n",
      "Training loss for batch 6035 : 0.017940431833267212\n",
      "Training loss for batch 6036 : 0.2002934217453003\n",
      "Training loss for batch 6037 : 0.0016904803924262524\n",
      "Training loss for batch 6038 : 0.14472262561321259\n",
      "Training loss for batch 6039 : 0.14653052389621735\n",
      "Training loss for batch 6040 : 0.01940343715250492\n",
      "Training loss for batch 6041 : 0.14012150466442108\n",
      "Training loss for batch 6042 : 0.10276901721954346\n",
      "Training loss for batch 6043 : 0.07056410610675812\n",
      "Training loss for batch 6044 : 0.05596635490655899\n",
      "Training loss for batch 6045 : 0.07350148260593414\n",
      "Training loss for batch 6046 : 0.07818886637687683\n",
      "Training loss for batch 6047 : 0.15055978298187256\n",
      "Training loss for batch 6048 : 0.14569520950317383\n",
      "Training loss for batch 6049 : 0.09032709896564484\n",
      "Training loss for batch 6050 : 0.10917112231254578\n",
      "Training loss for batch 6051 : 0.1259167641401291\n",
      "Training loss for batch 6052 : 0.19633221626281738\n",
      "Training loss for batch 6053 : 0.08568239212036133\n",
      "Training loss for batch 6054 : 0.17803718149662018\n",
      "Training loss for batch 6055 : 0.37191054224967957\n",
      "Training loss for batch 6056 : 0.20643772184848785\n",
      "Training loss for batch 6057 : 0.0639113113284111\n",
      "Training loss for batch 6058 : 0.20361313223838806\n",
      "Training loss for batch 6059 : 0.07930979877710342\n",
      "Training loss for batch 6060 : 0.08740723878145218\n",
      "Training loss for batch 6061 : 0.06036517769098282\n",
      "Training loss for batch 6062 : 0.6232965588569641\n",
      "Training loss for batch 6063 : 0.13237111270427704\n",
      "Training loss for batch 6064 : 0.03239523991942406\n",
      "Training loss for batch 6065 : 0.18781930208206177\n",
      "Training loss for batch 6066 : 0.19410715997219086\n",
      "Training loss for batch 6067 : 0.216440811753273\n",
      "Training loss for batch 6068 : 0.6120255589485168\n",
      "Training loss for batch 6069 : 0.058956775814294815\n",
      "Training loss for batch 6070 : 0.25283002853393555\n",
      "Training loss for batch 6071 : 0.02482309751212597\n",
      "Training loss for batch 6072 : 0.3396551311016083\n",
      "Training loss for batch 6073 : 0.06435840576887131\n",
      "Training loss for batch 6074 : 0.0785352885723114\n",
      "Training loss for batch 6075 : 0.2148195058107376\n",
      "Training loss for batch 6076 : 0.3115529417991638\n",
      "Training loss for batch 6077 : 0.18173539638519287\n",
      "Training loss for batch 6078 : 0.0\n",
      "Training loss for batch 6079 : 0.020914467051625252\n",
      "Training loss for batch 6080 : 0.10274501144886017\n",
      "Training loss for batch 6081 : 0.20764636993408203\n",
      "Training loss for batch 6082 : 0.21551233530044556\n",
      "Training loss for batch 6083 : 0.17657162249088287\n",
      "Training loss for batch 6084 : 0.11471472680568695\n",
      "Training loss for batch 6085 : 0.19308313727378845\n",
      "Training loss for batch 6086 : 0.21134987473487854\n",
      "Training loss for batch 6087 : 0.04391655698418617\n",
      "Training loss for batch 6088 : 0.046230465173721313\n",
      "Training loss for batch 6089 : 0.13921739161014557\n",
      "Training loss for batch 6090 : 0.39702755212783813\n",
      "Training loss for batch 6091 : 0.063381128013134\n",
      "Training loss for batch 6092 : 0.014353841543197632\n",
      "Training loss for batch 6093 : 0.20231769979000092\n",
      "Training loss for batch 6094 : 0.05004526302218437\n",
      "Training loss for batch 6095 : 0.21565797924995422\n",
      "Training loss for batch 6096 : 0.020842526108026505\n",
      "Training loss for batch 6097 : 0.03367599844932556\n",
      "Training loss for batch 6098 : 0.16817086935043335\n",
      "Training loss for batch 6099 : 0.1949005126953125\n",
      "Training loss for batch 6100 : 0.14536631107330322\n",
      "Training loss for batch 6101 : 0.061268556863069534\n",
      "Training loss for batch 6102 : 0.09388957917690277\n",
      "Training loss for batch 6103 : 0.1896350234746933\n",
      "Training loss for batch 6104 : 0.026533164083957672\n",
      "Training loss for batch 6105 : 0.02713414654135704\n",
      "Training loss for batch 6106 : 0.06427232176065445\n",
      "Training loss for batch 6107 : 0.31732362508773804\n",
      "Training loss for batch 6108 : 0.09486071765422821\n",
      "Training loss for batch 6109 : 0.1694830060005188\n",
      "Training loss for batch 6110 : 0.02923871949315071\n",
      "Training loss for batch 6111 : 0.053676504641771317\n",
      "Training loss for batch 6112 : 0.12869375944137573\n",
      "Training loss for batch 6113 : 0.369304895401001\n",
      "Training loss for batch 6114 : 0.11401234567165375\n",
      "Training loss for batch 6115 : 0.19869832694530487\n",
      "Training loss for batch 6116 : 0.20371687412261963\n",
      "Training loss for batch 6117 : 0.04288361966609955\n",
      "Training loss for batch 6118 : 0.17814011871814728\n",
      "Training loss for batch 6119 : 0.1421949714422226\n",
      "Training loss for batch 6120 : 0.08468622714281082\n",
      "Training loss for batch 6121 : 0.22246482968330383\n",
      "Training loss for batch 6122 : 0.05196013301610947\n",
      "Training loss for batch 6123 : 0.09602470695972443\n",
      "Training loss for batch 6124 : 0.0014373462181538343\n",
      "Training loss for batch 6125 : 0.07216782867908478\n",
      "Training loss for batch 6126 : 0.07719337940216064\n",
      "Training loss for batch 6127 : 0.2955211400985718\n",
      "Training loss for batch 6128 : 0.13624298572540283\n",
      "Training loss for batch 6129 : 0.294454425573349\n",
      "Training loss for batch 6130 : 0.25945115089416504\n",
      "Training loss for batch 6131 : 0.1695968210697174\n",
      "Training loss for batch 6132 : 0.1117582619190216\n",
      "Training loss for batch 6133 : 0.2607353627681732\n",
      "Training loss for batch 6134 : 0.24619907140731812\n",
      "Training loss for batch 6135 : 0.2656378149986267\n",
      "Training loss for batch 6136 : 0.17789770662784576\n",
      "Training loss for batch 6137 : 0.10263694077730179\n",
      "Training loss for batch 6138 : 0.2819823920726776\n",
      "Training loss for batch 6139 : 0.2663605213165283\n",
      "Training loss for batch 6140 : 0.09566634148359299\n",
      "Training loss for batch 6141 : 0.05143003910779953\n",
      "Training loss for batch 6142 : 0.14360156655311584\n",
      "Training loss for batch 6143 : 0.01387349795550108\n",
      "Training loss for batch 6144 : 0.11021962016820908\n",
      "Training loss for batch 6145 : 0.12032626569271088\n",
      "Training loss for batch 6146 : 0.016972830519080162\n",
      "Training loss for batch 6147 : 0.06890217959880829\n",
      "Training loss for batch 6148 : 0.17782533168792725\n",
      "Training loss for batch 6149 : 0.2100115269422531\n",
      "Training loss for batch 6150 : 0.12826848030090332\n",
      "Training loss for batch 6151 : 0.11565224826335907\n",
      "Training loss for batch 6152 : 0.09754717350006104\n",
      "Training loss for batch 6153 : 0.3238019645214081\n",
      "Training loss for batch 6154 : 0.2630510926246643\n",
      "Training loss for batch 6155 : 0.0545058511197567\n",
      "Training loss for batch 6156 : 0.25236526131629944\n",
      "Training loss for batch 6157 : 0.2350505143404007\n",
      "Training loss for batch 6158 : 0.011008627712726593\n",
      "Training loss for batch 6159 : 0.1868440955877304\n",
      "Training loss for batch 6160 : 0.0730561837553978\n",
      "Training loss for batch 6161 : 0.02978808619081974\n",
      "Training loss for batch 6162 : 0.35191792249679565\n",
      "Training loss for batch 6163 : 0.432288259267807\n",
      "Training loss for batch 6164 : 0.12968266010284424\n",
      "Training loss for batch 6165 : 0.0835316926240921\n",
      "Training loss for batch 6166 : 0.062346551567316055\n",
      "Training loss for batch 6167 : 0.05103765428066254\n",
      "Training loss for batch 6168 : 0.037996187806129456\n",
      "Training loss for batch 6169 : 0.04710996150970459\n",
      "Training loss for batch 6170 : 0.3193051218986511\n",
      "Training loss for batch 6171 : 0.18171577155590057\n",
      "Training loss for batch 6172 : 0.2745981216430664\n",
      "Training loss for batch 6173 : 0.30337151885032654\n",
      "Training loss for batch 6174 : 0.029071377590298653\n",
      "Training loss for batch 6175 : 0.1199372410774231\n",
      "Training loss for batch 6176 : 0.08152876049280167\n",
      "Training loss for batch 6177 : 0.20070019364356995\n",
      "Training loss for batch 6178 : 0.05020607262849808\n",
      "Training loss for batch 6179 : 0.22305811941623688\n",
      "Training loss for batch 6180 : 0.12768889963626862\n",
      "Training loss for batch 6181 : 0.23864901065826416\n",
      "Training loss for batch 6182 : 0.356514036655426\n",
      "Training loss for batch 6183 : 0.35630813241004944\n",
      "Training loss for batch 6184 : 0.20396678149700165\n",
      "Training loss for batch 6185 : 0.36793848872184753\n",
      "Training loss for batch 6186 : 0.05543994531035423\n",
      "Training loss for batch 6187 : 0.23421147465705872\n",
      "Training loss for batch 6188 : 0.2335008829832077\n",
      "Training loss for batch 6189 : 0.11223912984132767\n",
      "Training loss for batch 6190 : 0.1662454605102539\n",
      "Training loss for batch 6191 : 0.08972970396280289\n",
      "Training loss for batch 6192 : 0.12067484855651855\n",
      "Training loss for batch 6193 : 0.12269088625907898\n",
      "Training loss for batch 6194 : 0.20595332980155945\n",
      "Training loss for batch 6195 : 0.03433000668883324\n",
      "Training loss for batch 6196 : 0.20207111537456512\n",
      "Training loss for batch 6197 : 0.13978612422943115\n",
      "Training loss for batch 6198 : 0.0\n",
      "Training loss for batch 6199 : 0.2276679128408432\n",
      "Training loss for batch 6200 : 0.10201982408761978\n",
      "Training loss for batch 6201 : 0.3236576318740845\n",
      "Training loss for batch 6202 : 0.2518521845340729\n",
      "Training loss for batch 6203 : 0.15171170234680176\n",
      "Training loss for batch 6204 : 0.04832305759191513\n",
      "Training loss for batch 6205 : 0.13337811827659607\n",
      "Training loss for batch 6206 : 0.019566591829061508\n",
      "Training loss for batch 6207 : 0.0682247206568718\n",
      "Training loss for batch 6208 : 0.15295211970806122\n",
      "Training loss for batch 6209 : 0.2242589145898819\n",
      "Training loss for batch 6210 : 0.16781151294708252\n",
      "Training loss for batch 6211 : 0.12061910331249237\n",
      "Training loss for batch 6212 : 0.29638203978538513\n",
      "Training loss for batch 6213 : 0.27059489488601685\n",
      "Training loss for batch 6214 : 0.07252682745456696\n",
      "Training loss for batch 6215 : 0.2439298927783966\n",
      "Training loss for batch 6216 : 0.09661398082971573\n",
      "Training loss for batch 6217 : 0.15182636678218842\n",
      "Training loss for batch 6218 : 0.41755571961402893\n",
      "Training loss for batch 6219 : 0.03806488215923309\n",
      "Training loss for batch 6220 : 0.06110989302396774\n",
      "Training loss for batch 6221 : 0.004476676695048809\n",
      "Training loss for batch 6222 : 0.22281983494758606\n",
      "Training loss for batch 6223 : 0.10553006082773209\n",
      "Training loss for batch 6224 : 0.09942024201154709\n",
      "Training loss for batch 6225 : 0.1735478639602661\n",
      "Training loss for batch 6226 : 0.16021297872066498\n",
      "Training loss for batch 6227 : 0.27860650420188904\n",
      "Training loss for batch 6228 : 0.04813050478696823\n",
      "Training loss for batch 6229 : 0.08159422874450684\n",
      "Training loss for batch 6230 : 0.04881425201892853\n",
      "Training loss for batch 6231 : 0.17144355177879333\n",
      "Training loss for batch 6232 : 0.5509051084518433\n",
      "Training loss for batch 6233 : 0.231117844581604\n",
      "Training loss for batch 6234 : 0.23866595327854156\n",
      "Training loss for batch 6235 : 0.013892542570829391\n",
      "Training loss for batch 6236 : 0.014626463875174522\n",
      "Training loss for batch 6237 : 0.14282983541488647\n",
      "Training loss for batch 6238 : 0.11256372183561325\n",
      "Training loss for batch 6239 : 0.12069524824619293\n",
      "Training loss for batch 6240 : 0.05291982740163803\n",
      "Training loss for batch 6241 : 0.13487303256988525\n",
      "Training loss for batch 6242 : 0.0707625150680542\n",
      "Training loss for batch 6243 : 0.08213534951210022\n",
      "Training loss for batch 6244 : 0.23259219527244568\n",
      "Training loss for batch 6245 : 0.2620670795440674\n",
      "Training loss for batch 6246 : 0.006704824045300484\n",
      "Training loss for batch 6247 : 0.022706326097249985\n",
      "Training loss for batch 6248 : 0.07281290739774704\n",
      "Training loss for batch 6249 : 0.0700603798031807\n",
      "Training loss for batch 6250 : 0.10117171704769135\n",
      "Training loss for batch 6251 : 0.39020276069641113\n",
      "Training loss for batch 6252 : 0.34330815076828003\n",
      "Training loss for batch 6253 : 0.13703233003616333\n",
      "Training loss for batch 6254 : 0.09914290904998779\n",
      "Training loss for batch 6255 : 0.22640299797058105\n",
      "Training loss for batch 6256 : 0.16660349071025848\n",
      "Training loss for batch 6257 : 0.1396815925836563\n",
      "Training loss for batch 6258 : 0.41681793332099915\n",
      "Training loss for batch 6259 : 0.08345570415258408\n",
      "Training loss for batch 6260 : 0.15031281113624573\n",
      "Training loss for batch 6261 : 0.2175876647233963\n",
      "Training loss for batch 6262 : 0.009888063184916973\n",
      "Training loss for batch 6263 : 0.09965315461158752\n",
      "Training loss for batch 6264 : 0.07752646505832672\n",
      "Training loss for batch 6265 : 0.16600017249584198\n",
      "Training loss for batch 6266 : 0.14841578900814056\n",
      "Training loss for batch 6267 : 0.3105577826499939\n",
      "Training loss for batch 6268 : 0.07407359778881073\n",
      "Training loss for batch 6269 : 0.1103198304772377\n",
      "Training loss for batch 6270 : 0.0002898077364079654\n",
      "Training loss for batch 6271 : 0.0402163602411747\n",
      "Training loss for batch 6272 : 0.34365567564964294\n",
      "Training loss for batch 6273 : 0.02564328908920288\n",
      "Training loss for batch 6274 : 0.10927609354257584\n",
      "Training loss for batch 6275 : 0.060656655579805374\n",
      "Training loss for batch 6276 : 0.27582088112831116\n",
      "Training loss for batch 6277 : 0.3707900941371918\n",
      "Training loss for batch 6278 : 0.16820171475410461\n",
      "Training loss for batch 6279 : 0.06978164613246918\n",
      "Training loss for batch 6280 : 0.09706337004899979\n",
      "Training loss for batch 6281 : 0.304495245218277\n",
      "Training loss for batch 6282 : 0.3350905478000641\n",
      "Training loss for batch 6283 : 0.14756843447685242\n",
      "Training loss for batch 6284 : 0.01936190389096737\n",
      "Training loss for batch 6285 : 0.30220699310302734\n",
      "Training loss for batch 6286 : 0.3329125940799713\n",
      "Training loss for batch 6287 : 0.34180498123168945\n",
      "Training loss for batch 6288 : 0.1498115360736847\n",
      "Training loss for batch 6289 : 0.3294662833213806\n",
      "Training loss for batch 6290 : 0.09155865758657455\n",
      "Training loss for batch 6291 : -0.0004182992270216346\n",
      "Training loss for batch 6292 : 0.12997382879257202\n",
      "Training loss for batch 6293 : 0.09416539967060089\n",
      "Training loss for batch 6294 : 0.08081585168838501\n",
      "Training loss for batch 6295 : 0.11577282845973969\n",
      "Training loss for batch 6296 : 0.03942051902413368\n",
      "Training loss for batch 6297 : 0.07302431762218475\n",
      "Training loss for batch 6298 : 0.03963411971926689\n",
      "Training loss for batch 6299 : 0.017955100163817406\n",
      "Training loss for batch 6300 : 0.14882291853427887\n",
      "Training loss for batch 6301 : 0.009042717516422272\n",
      "Training loss for batch 6302 : 0.33257386088371277\n",
      "Training loss for batch 6303 : 0.09418192505836487\n",
      "Training loss for batch 6304 : 0.15438386797904968\n",
      "Training loss for batch 6305 : 0.10740058124065399\n",
      "Training loss for batch 6306 : 0.052662745118141174\n",
      "Training loss for batch 6307 : 0.21518559753894806\n",
      "Training loss for batch 6308 : 0.15547136962413788\n",
      "Training loss for batch 6309 : 0.06005236133933067\n",
      "Training loss for batch 6310 : 0.20134809613227844\n",
      "Training loss for batch 6311 : 0.16620351374149323\n",
      "Training loss for batch 6312 : 0.12606556713581085\n",
      "Training loss for batch 6313 : 0.34801775217056274\n",
      "Training loss for batch 6314 : 0.07023926079273224\n",
      "Training loss for batch 6315 : 0.6115260124206543\n",
      "Training loss for batch 6316 : 0.036676086485385895\n",
      "Training loss for batch 6317 : 0.13542506098747253\n",
      "Training loss for batch 6318 : 0.05526643991470337\n",
      "Training loss for batch 6319 : 0.10016337037086487\n",
      "Training loss for batch 6320 : 0.06666838377714157\n",
      "Training loss for batch 6321 : 0.3049849271774292\n",
      "Training loss for batch 6322 : 0.16679422557353973\n",
      "Training loss for batch 6323 : 0.3140808939933777\n",
      "Training loss for batch 6324 : 0.08033189177513123\n",
      "Training loss for batch 6325 : 0.02959342673420906\n",
      "Training loss for batch 6326 : 0.12291806936264038\n",
      "Training loss for batch 6327 : 0.25567853450775146\n",
      "Training loss for batch 6328 : 0.1171124279499054\n",
      "Training loss for batch 6329 : 0.0429844968020916\n",
      "Training loss for batch 6330 : 0.1159614622592926\n",
      "Training loss for batch 6331 : 0.017676115036010742\n",
      "Training loss for batch 6332 : 0.1309281587600708\n",
      "Training loss for batch 6333 : 0.25113311409950256\n",
      "Training loss for batch 6334 : 0.16969428956508636\n",
      "Training loss for batch 6335 : 0.11334256827831268\n",
      "Training loss for batch 6336 : 0.016738658770918846\n",
      "Training loss for batch 6337 : 0.13447371125221252\n",
      "Training loss for batch 6338 : 0.30512914061546326\n",
      "Training loss for batch 6339 : 0.1201939731836319\n",
      "Training loss for batch 6340 : 0.1890403926372528\n",
      "Training loss for batch 6341 : 0.15724880993366241\n",
      "Training loss for batch 6342 : 0.056018002331256866\n",
      "Training loss for batch 6343 : 0.10152159631252289\n",
      "Training loss for batch 6344 : 0.054385751485824585\n",
      "Training loss for batch 6345 : 0.10870849341154099\n",
      "Training loss for batch 6346 : 0.11701762676239014\n",
      "Training loss for batch 6347 : 0.16840359568595886\n",
      "Training loss for batch 6348 : 0.3213719129562378\n",
      "Training loss for batch 6349 : 0.2819842994213104\n",
      "Training loss for batch 6350 : 0.15526819229125977\n",
      "Training loss for batch 6351 : 0.1685570478439331\n",
      "Training loss for batch 6352 : 0.19374725222587585\n",
      "Training loss for batch 6353 : 0.08113408088684082\n",
      "Training loss for batch 6354 : 0.219130739569664\n",
      "Training loss for batch 6355 : 0.15965746343135834\n",
      "Training loss for batch 6356 : 0.026895521208643913\n",
      "Training loss for batch 6357 : 0.17010028660297394\n",
      "Training loss for batch 6358 : 0.08661238104104996\n",
      "Training loss for batch 6359 : 0.06482650339603424\n",
      "Training loss for batch 6360 : 0.22527232766151428\n",
      "Training loss for batch 6361 : 0.05655748397111893\n",
      "Training loss for batch 6362 : 0.0\n",
      "Training loss for batch 6363 : 0.042148247361183167\n",
      "Training loss for batch 6364 : 0.30711859464645386\n",
      "Training loss for batch 6365 : 0.12033452093601227\n",
      "Training loss for batch 6366 : 0.15429121255874634\n",
      "Training loss for batch 6367 : 0.041660409420728683\n",
      "Training loss for batch 6368 : 0.04169567674398422\n",
      "Training loss for batch 6369 : 0.03517705947160721\n",
      "Training loss for batch 6370 : 0.06285611540079117\n",
      "Training loss for batch 6371 : 0.1735173612833023\n",
      "Training loss for batch 6372 : 0.21852287650108337\n",
      "Training loss for batch 6373 : 0.09216788411140442\n",
      "Training loss for batch 6374 : 0.2919600009918213\n",
      "Training loss for batch 6375 : 0.20835243165493011\n",
      "Training loss for batch 6376 : 0.33752626180648804\n",
      "Training loss for batch 6377 : 0.05359097942709923\n",
      "Training loss for batch 6378 : 0.041375815868377686\n",
      "Training loss for batch 6379 : 0.2695890963077545\n",
      "Training loss for batch 6380 : 0.06763957440853119\n",
      "Training loss for batch 6381 : 0.3274909257888794\n",
      "Training loss for batch 6382 : 0.024184148758649826\n",
      "Training loss for batch 6383 : 0.39387810230255127\n",
      "Training loss for batch 6384 : 0.10951212793588638\n",
      "Training loss for batch 6385 : 0.16420014202594757\n",
      "Training loss for batch 6386 : 0.2907290458679199\n",
      "Training loss for batch 6387 : 0.09547452628612518\n",
      "Training loss for batch 6388 : -0.0020027034915983677\n",
      "Training loss for batch 6389 : 0.0\n",
      "Training loss for batch 6390 : 0.08848489820957184\n",
      "Training loss for batch 6391 : 0.11149498075246811\n",
      "Training loss for batch 6392 : 0.3018009662628174\n",
      "Training loss for batch 6393 : 0.41183143854141235\n",
      "Training loss for batch 6394 : 0.18022111058235168\n",
      "Training loss for batch 6395 : 0.09200119972229004\n",
      "Training loss for batch 6396 : 0.06717287749052048\n",
      "Training loss for batch 6397 : 0.2996397614479065\n",
      "Training loss for batch 6398 : 0.06618531793355942\n",
      "Training loss for batch 6399 : 0.1645289808511734\n",
      "Training loss for batch 6400 : 0.09044334292411804\n",
      "Training loss for batch 6401 : 0.033284224569797516\n",
      "Training loss for batch 6402 : 0.19138428568840027\n",
      "Training loss for batch 6403 : 0.006440520286560059\n",
      "Training loss for batch 6404 : 0.17522327601909637\n",
      "Training loss for batch 6405 : 0.23538431525230408\n",
      "Training loss for batch 6406 : 0.21612557768821716\n",
      "Training loss for batch 6407 : 0.0767553299665451\n",
      "Training loss for batch 6408 : 0.5111497044563293\n",
      "Training loss for batch 6409 : 0.18561837077140808\n",
      "Training loss for batch 6410 : 0.4495520293712616\n",
      "Training loss for batch 6411 : 0.18036454916000366\n",
      "Training loss for batch 6412 : 0.004630973096936941\n",
      "Training loss for batch 6413 : 0.051514752209186554\n",
      "Training loss for batch 6414 : 0.10962983965873718\n",
      "Training loss for batch 6415 : 0.018523193895816803\n",
      "Training loss for batch 6416 : 0.12474051862955093\n",
      "Training loss for batch 6417 : 0.05440312623977661\n",
      "Training loss for batch 6418 : 0.041338227689266205\n",
      "Training loss for batch 6419 : 0.05745088309049606\n",
      "Training loss for batch 6420 : 0.034632209688425064\n",
      "Training loss for batch 6421 : 0.09928561747074127\n",
      "Training loss for batch 6422 : 0.18526528775691986\n",
      "Training loss for batch 6423 : 0.15929988026618958\n",
      "Training loss for batch 6424 : 0.061987653374671936\n",
      "Training loss for batch 6425 : 0.09999608993530273\n",
      "Training loss for batch 6426 : 0.1559782326221466\n",
      "Training loss for batch 6427 : 0.10034815967082977\n",
      "Training loss for batch 6428 : 0.05837923288345337\n",
      "Training loss for batch 6429 : 0.05153617262840271\n",
      "Training loss for batch 6430 : 0.047924526035785675\n",
      "Training loss for batch 6431 : 0.06627784669399261\n",
      "Training loss for batch 6432 : 0.2672584652900696\n",
      "Training loss for batch 6433 : 0.06922708451747894\n",
      "Training loss for batch 6434 : 0.16685685515403748\n",
      "Training loss for batch 6435 : 0.06636450439691544\n",
      "Training loss for batch 6436 : 0.12464683502912521\n",
      "Training loss for batch 6437 : 0.1340814232826233\n",
      "Training loss for batch 6438 : 0.29720109701156616\n",
      "Training loss for batch 6439 : 0.061152249574661255\n",
      "Training loss for batch 6440 : 0.20600873231887817\n",
      "Training loss for batch 6441 : 0.05031909793615341\n",
      "Training loss for batch 6442 : 0.052713412791490555\n",
      "Training loss for batch 6443 : 0.2525038719177246\n",
      "Training loss for batch 6444 : 0.11998017877340317\n",
      "Training loss for batch 6445 : 0.17715752124786377\n",
      "Training loss for batch 6446 : 0.07639007270336151\n",
      "Training loss for batch 6447 : 0.055818650871515274\n",
      "Training loss for batch 6448 : 0.1842832863330841\n",
      "Training loss for batch 6449 : 0.303584486246109\n",
      "Training loss for batch 6450 : 0.12071231007575989\n",
      "Training loss for batch 6451 : 0.20934705436229706\n",
      "Training loss for batch 6452 : 0.0\n",
      "Training loss for batch 6453 : 0.012856589630246162\n",
      "Training loss for batch 6454 : 0.14209692180156708\n",
      "Training loss for batch 6455 : 0.18506376445293427\n",
      "Training loss for batch 6456 : 0.07309304922819138\n",
      "Training loss for batch 6457 : 0.13073761761188507\n",
      "Training loss for batch 6458 : 0.11296471953392029\n",
      "Training loss for batch 6459 : 0.34353765845298767\n",
      "Training loss for batch 6460 : 0.05478385463356972\n",
      "Training loss for batch 6461 : 0.09816033393144608\n",
      "Training loss for batch 6462 : 0.008362730965018272\n",
      "Training loss for batch 6463 : 0.014146233908832073\n",
      "Training loss for batch 6464 : 0.12646202743053436\n",
      "Training loss for batch 6465 : 0.14707526564598083\n",
      "Training loss for batch 6466 : 0.21393665671348572\n",
      "Training loss for batch 6467 : 0.19549217820167542\n",
      "Training loss for batch 6468 : 0.06186741590499878\n",
      "Training loss for batch 6469 : 0.13018523156642914\n",
      "Training loss for batch 6470 : 0.14148478209972382\n",
      "Training loss for batch 6471 : 0.06885698437690735\n",
      "Training loss for batch 6472 : -0.004154697060585022\n",
      "Training loss for batch 6473 : 0.08969561755657196\n",
      "Training loss for batch 6474 : 0.21154844760894775\n",
      "Training loss for batch 6475 : 0.24145102500915527\n",
      "Training loss for batch 6476 : 0.313509076833725\n",
      "Training loss for batch 6477 : 0.3152362108230591\n",
      "Training loss for batch 6478 : 0.0887017548084259\n",
      "Training loss for batch 6479 : 0.056420937180519104\n",
      "Training loss for batch 6480 : 0.2967206537723541\n",
      "Training loss for batch 6481 : 0.15555648505687714\n",
      "Training loss for batch 6482 : 0.19851163029670715\n",
      "Training loss for batch 6483 : 0.04871171712875366\n",
      "Training loss for batch 6484 : 0.5167322754859924\n",
      "Training loss for batch 6485 : 0.09419149160385132\n",
      "Training loss for batch 6486 : 0.2750823497772217\n",
      "Training loss for batch 6487 : 0.03962734341621399\n",
      "Training loss for batch 6488 : 0.04274033382534981\n",
      "Training loss for batch 6489 : 0.2339700162410736\n",
      "Training loss for batch 6490 : 0.18875384330749512\n",
      "Training loss for batch 6491 : 0.07684127241373062\n",
      "Training loss for batch 6492 : 0.39553067088127136\n",
      "Training loss for batch 6493 : 0.17232006788253784\n",
      "Training loss for batch 6494 : 0.5607466101646423\n",
      "Training loss for batch 6495 : 0.0016950666904449463\n",
      "Training loss for batch 6496 : 0.3945699632167816\n",
      "Training loss for batch 6497 : 0.2588888704776764\n",
      "Training loss for batch 6498 : 0.06025827303528786\n",
      "Training loss for batch 6499 : 0.07469640672206879\n",
      "Training loss for batch 6500 : 0.05745313689112663\n",
      "Training loss for batch 6501 : 0.0546683669090271\n",
      "Training loss for batch 6502 : 0.08294669538736343\n",
      "Training loss for batch 6503 : 0.20452316105365753\n",
      "Training loss for batch 6504 : 0.11567001789808273\n",
      "Training loss for batch 6505 : 0.1534591168165207\n",
      "Training loss for batch 6506 : 0.14958103001117706\n",
      "Training loss for batch 6507 : 0.13595116138458252\n",
      "Training loss for batch 6508 : 0.2648133337497711\n",
      "Training loss for batch 6509 : 0.14845310151576996\n",
      "Training loss for batch 6510 : 0.018695732578635216\n",
      "Training loss for batch 6511 : 0.1995604932308197\n",
      "Training loss for batch 6512 : 0.0\n",
      "Training loss for batch 6513 : 0.16421550512313843\n",
      "Training loss for batch 6514 : 0.15722131729125977\n",
      "Training loss for batch 6515 : 0.21883121132850647\n",
      "Training loss for batch 6516 : 0.1468309760093689\n",
      "Training loss for batch 6517 : 0.0591505691409111\n",
      "Training loss for batch 6518 : 0.2307264357805252\n",
      "Training loss for batch 6519 : 0.01327125821262598\n",
      "Training loss for batch 6520 : 0.306767076253891\n",
      "Training loss for batch 6521 : 0.07664571702480316\n",
      "Training loss for batch 6522 : 0.2162245810031891\n",
      "Training loss for batch 6523 : 0.018944840878248215\n",
      "Training loss for batch 6524 : 0.3254011273384094\n",
      "Training loss for batch 6525 : 0.07747720927000046\n",
      "Training loss for batch 6526 : 0.03987481817603111\n",
      "Training loss for batch 6527 : 0.02694583125412464\n",
      "Training loss for batch 6528 : 0.05275820195674896\n",
      "Training loss for batch 6529 : 0.16914591193199158\n",
      "Training loss for batch 6530 : 0.11576463282108307\n",
      "Training loss for batch 6531 : 0.025240670889616013\n",
      "Training loss for batch 6532 : 0.13061225414276123\n",
      "Training loss for batch 6533 : 0.10307446867227554\n",
      "Training loss for batch 6534 : 0.3121240735054016\n",
      "Training loss for batch 6535 : 0.2050633430480957\n",
      "Training loss for batch 6536 : 0.1718919277191162\n",
      "Training loss for batch 6537 : 0.2395876795053482\n",
      "Training loss for batch 6538 : 0.22451834380626678\n",
      "Training loss for batch 6539 : 0.35085880756378174\n",
      "Training loss for batch 6540 : 0.26812314987182617\n",
      "Training loss for batch 6541 : 0.2451280802488327\n",
      "Training loss for batch 6542 : 0.32396769523620605\n",
      "Training loss for batch 6543 : 0.21819493174552917\n",
      "Training loss for batch 6544 : 0.04298056662082672\n",
      "Training loss for batch 6545 : 0.04716240614652634\n",
      "Training loss for batch 6546 : 0.17940978705883026\n",
      "Training loss for batch 6547 : 0.1403300017118454\n",
      "Training loss for batch 6548 : 0.08907278627157211\n",
      "Training loss for batch 6549 : 0.05863732472062111\n",
      "Training loss for batch 6550 : 0.29197144508361816\n",
      "Training loss for batch 6551 : 0.03215045854449272\n",
      "Training loss for batch 6552 : 0.0\n",
      "Training loss for batch 6553 : 0.060493018478155136\n",
      "Training loss for batch 6554 : 0.3352162837982178\n",
      "Training loss for batch 6555 : 0.34324368834495544\n",
      "Training loss for batch 6556 : 0.08638906478881836\n",
      "Training loss for batch 6557 : 0.189723938703537\n",
      "Training loss for batch 6558 : 0.03929338604211807\n",
      "Training loss for batch 6559 : 0.22287461161613464\n",
      "Training loss for batch 6560 : 0.13867437839508057\n",
      "Training loss for batch 6561 : 0.03860762342810631\n",
      "Training loss for batch 6562 : 0.19414719939231873\n",
      "Training loss for batch 6563 : 0.19459648430347443\n",
      "Training loss for batch 6564 : 0.11654719710350037\n",
      "Training loss for batch 6565 : 0.021374410018324852\n",
      "Training loss for batch 6566 : 0.11990149319171906\n",
      "Training loss for batch 6567 : 0.22063225507736206\n",
      "Training loss for batch 6568 : 0.13034117221832275\n",
      "Training loss for batch 6569 : -7.928162813186646e-05\n",
      "Training loss for batch 6570 : 0.1399824172258377\n",
      "Training loss for batch 6571 : 0.15310437977313995\n",
      "Training loss for batch 6572 : 0.15869015455245972\n",
      "Training loss for batch 6573 : 0.38491711020469666\n",
      "Training loss for batch 6574 : 0.1939479410648346\n",
      "Training loss for batch 6575 : 0.1421949714422226\n",
      "Training loss for batch 6576 : 0.16135936975479126\n",
      "Training loss for batch 6577 : 0.3884424567222595\n",
      "Training loss for batch 6578 : 0.015210866928100586\n",
      "Training loss for batch 6579 : 0.09403779357671738\n",
      "Training loss for batch 6580 : 0.1876893788576126\n",
      "Training loss for batch 6581 : 0.0641527995467186\n",
      "Training loss for batch 6582 : 0.06469745934009552\n",
      "Training loss for batch 6583 : 0.15292280912399292\n",
      "Training loss for batch 6584 : 0.04445267096161842\n",
      "Training loss for batch 6585 : 0.1586448848247528\n",
      "Training loss for batch 6586 : 0.11756624281406403\n",
      "Training loss for batch 6587 : 0.041576776653528214\n",
      "Training loss for batch 6588 : 0.09147635847330093\n",
      "Training loss for batch 6589 : 0.07322084903717041\n",
      "Training loss for batch 6590 : 0.07962968945503235\n",
      "Training loss for batch 6591 : 0.19245749711990356\n",
      "Training loss for batch 6592 : 0.09524257481098175\n",
      "Training loss for batch 6593 : 0.21631737053394318\n",
      "Training loss for batch 6594 : 0.18844036757946014\n",
      "Training loss for batch 6595 : 0.009295755997300148\n",
      "Training loss for batch 6596 : 0.0715237632393837\n",
      "Training loss for batch 6597 : 0.328819215297699\n",
      "Training loss for batch 6598 : 0.11905816197395325\n",
      "Training loss for batch 6599 : 0.410429984331131\n",
      "Training loss for batch 6600 : 0.16196802258491516\n",
      "Training loss for batch 6601 : 0.02037510834634304\n",
      "Training loss for batch 6602 : 0.11043274402618408\n",
      "Training loss for batch 6603 : 0.00014238758012652397\n",
      "Training loss for batch 6604 : 0.09734547883272171\n",
      "Training loss for batch 6605 : 0.14272676408290863\n",
      "Training loss for batch 6606 : 0.2878904640674591\n",
      "Training loss for batch 6607 : 0.040149323642253876\n",
      "Training loss for batch 6608 : 0.162833571434021\n",
      "Training loss for batch 6609 : 0.22343052923679352\n",
      "Training loss for batch 6610 : 0.0037438671570271254\n",
      "Training loss for batch 6611 : 0.013941179029643536\n",
      "Training loss for batch 6612 : 0.10471425205469131\n",
      "Training loss for batch 6613 : 0.04203702136874199\n",
      "Training loss for batch 6614 : 0.054548002779483795\n",
      "Training loss for batch 6615 : 0.1802685558795929\n",
      "Training loss for batch 6616 : 0.2190762609243393\n",
      "Training loss for batch 6617 : 0.2721385657787323\n",
      "Training loss for batch 6618 : 0.04162571206688881\n",
      "Training loss for batch 6619 : 0.0015533986734226346\n",
      "Training loss for batch 6620 : 0.10044865310192108\n",
      "Training loss for batch 6621 : 0.02859554812312126\n",
      "Training loss for batch 6622 : 0.19475606083869934\n",
      "Training loss for batch 6623 : 0.07213825732469559\n",
      "Training loss for batch 6624 : 0.08018917590379715\n",
      "Training loss for batch 6625 : 0.14753149449825287\n",
      "Training loss for batch 6626 : 0.16256898641586304\n",
      "Training loss for batch 6627 : 0.0\n",
      "Training loss for batch 6628 : 0.15561316907405853\n",
      "Training loss for batch 6629 : 0.14758320152759552\n",
      "Training loss for batch 6630 : 0.10933949053287506\n",
      "Training loss for batch 6631 : 0.06320663541555405\n",
      "Training loss for batch 6632 : 0.048989348113536835\n",
      "Training loss for batch 6633 : 0.11211374402046204\n",
      "Training loss for batch 6634 : 0.03392872214317322\n",
      "Training loss for batch 6635 : 0.30889570713043213\n",
      "Training loss for batch 6636 : 0.028626637533307076\n",
      "Training loss for batch 6637 : 0.04804578796029091\n",
      "Training loss for batch 6638 : 0.014823154546320438\n",
      "Training loss for batch 6639 : 0.021689070388674736\n",
      "Training loss for batch 6640 : 0.25572243332862854\n",
      "Training loss for batch 6641 : 0.05741479992866516\n",
      "Training loss for batch 6642 : 0.18968810141086578\n",
      "Training loss for batch 6643 : 0.18463578820228577\n",
      "Training loss for batch 6644 : 0.05508096516132355\n",
      "Training loss for batch 6645 : 0.22206835448741913\n",
      "Training loss for batch 6646 : 0.30402567982673645\n",
      "Training loss for batch 6647 : 0.12158211320638657\n",
      "Training loss for batch 6648 : 0.09606335312128067\n",
      "Training loss for batch 6649 : 0.08774526417255402\n",
      "Training loss for batch 6650 : 0.19011123478412628\n",
      "Training loss for batch 6651 : 0.25208044052124023\n",
      "Training loss for batch 6652 : 0.13221123814582825\n",
      "Training loss for batch 6653 : 0.035164911299943924\n",
      "Training loss for batch 6654 : 0.05934927985072136\n",
      "Training loss for batch 6655 : 0.12848976254463196\n",
      "Training loss for batch 6656 : 0.15286801755428314\n",
      "Training loss for batch 6657 : 0.17655164003372192\n",
      "Training loss for batch 6658 : 0.2714262306690216\n",
      "Training loss for batch 6659 : 0.3628113269805908\n",
      "Training loss for batch 6660 : 0.3422433137893677\n",
      "Training loss for batch 6661 : 0.08637341856956482\n",
      "Training loss for batch 6662 : 0.3232274651527405\n",
      "Training loss for batch 6663 : 0.013521905988454819\n",
      "Training loss for batch 6664 : 0.05895769223570824\n",
      "Training loss for batch 6665 : 0.01409361232072115\n",
      "Training loss for batch 6666 : 0.02421356365084648\n",
      "Training loss for batch 6667 : 0.18830615282058716\n",
      "Training loss for batch 6668 : 0.2651885151863098\n",
      "Training loss for batch 6669 : 0.05999588221311569\n",
      "Training loss for batch 6670 : 0.16054221987724304\n",
      "Training loss for batch 6671 : 0.04000105708837509\n",
      "Training loss for batch 6672 : 0.11396859586238861\n",
      "Training loss for batch 6673 : 0.1007448360323906\n",
      "Training loss for batch 6674 : 0.08842253684997559\n",
      "Training loss for batch 6675 : 0.21361888945102692\n",
      "Training loss for batch 6676 : 0.1595572829246521\n",
      "Training loss for batch 6677 : 0.12612363696098328\n",
      "Training loss for batch 6678 : 0.07258924841880798\n",
      "Training loss for batch 6679 : 0.23886437714099884\n",
      "Training loss for batch 6680 : 0.1553739309310913\n",
      "Training loss for batch 6681 : 0.026922011747956276\n",
      "Training loss for batch 6682 : 0.16087624430656433\n",
      "Training loss for batch 6683 : 0.02174912579357624\n",
      "Training loss for batch 6684 : 0.1555531769990921\n",
      "Training loss for batch 6685 : 0.0743265375494957\n",
      "Training loss for batch 6686 : 0.010004044510424137\n",
      "Training loss for batch 6687 : 0.11286425590515137\n",
      "Training loss for batch 6688 : 0.28137901425361633\n",
      "Training loss for batch 6689 : 0.36323127150535583\n",
      "Training loss for batch 6690 : 0.12649260461330414\n",
      "Training loss for batch 6691 : 0.07868985086679459\n",
      "Training loss for batch 6692 : 0.04607939347624779\n",
      "Training loss for batch 6693 : 0.07060213387012482\n",
      "Training loss for batch 6694 : 0.23690786957740784\n",
      "Training loss for batch 6695 : 0.08669698983430862\n",
      "Training loss for batch 6696 : 0.04270167648792267\n",
      "Training loss for batch 6697 : 0.1473025381565094\n",
      "Training loss for batch 6698 : 0.10322079807519913\n",
      "Training loss for batch 6699 : 0.012948528863489628\n",
      "Training loss for batch 6700 : 0.14311754703521729\n",
      "Training loss for batch 6701 : 0.09267362952232361\n",
      "Training loss for batch 6702 : 0.2762284576892853\n",
      "Training loss for batch 6703 : 0.24245598912239075\n",
      "Training loss for batch 6704 : 0.2065684050321579\n",
      "Training loss for batch 6705 : 0.09356002509593964\n",
      "Training loss for batch 6706 : 0.15036536753177643\n",
      "Training loss for batch 6707 : 0.4183933436870575\n",
      "Training loss for batch 6708 : 0.31424954533576965\n",
      "Training loss for batch 6709 : 0.23050642013549805\n",
      "Training loss for batch 6710 : 0.0016293327789753675\n",
      "Training loss for batch 6711 : 0.5258933305740356\n",
      "Training loss for batch 6712 : 0.02183532901108265\n",
      "Training loss for batch 6713 : 0.08657027781009674\n",
      "Training loss for batch 6714 : 0.10763756185770035\n",
      "Training loss for batch 6715 : 0.0\n",
      "Training loss for batch 6716 : 0.0013383873738348484\n",
      "Training loss for batch 6717 : 0.11023750901222229\n",
      "Training loss for batch 6718 : 0.09662983566522598\n",
      "Training loss for batch 6719 : 0.0007882416248321533\n",
      "Training loss for batch 6720 : 0.12341837584972382\n",
      "Training loss for batch 6721 : 0.23194332420825958\n",
      "Training loss for batch 6722 : 0.25966954231262207\n",
      "Training loss for batch 6723 : 0.058219172060489655\n",
      "Training loss for batch 6724 : 0.2531743049621582\n",
      "Training loss for batch 6725 : 0.1490604430437088\n",
      "Training loss for batch 6726 : 0.288741797208786\n",
      "Training loss for batch 6727 : 0.35898303985595703\n",
      "Training loss for batch 6728 : 0.1900167614221573\n",
      "Training loss for batch 6729 : 0.06965857744216919\n",
      "Training loss for batch 6730 : 0.03658192977309227\n",
      "Training loss for batch 6731 : 0.20242315530776978\n",
      "Training loss for batch 6732 : 0.19297200441360474\n",
      "Training loss for batch 6733 : 0.11728890240192413\n",
      "Training loss for batch 6734 : 0.0803433209657669\n",
      "Training loss for batch 6735 : 0.04675711691379547\n",
      "Training loss for batch 6736 : 0.05014275014400482\n",
      "Training loss for batch 6737 : 0.10223362594842911\n",
      "Training loss for batch 6738 : 0.05521071329712868\n",
      "Training loss for batch 6739 : 0.24200545251369476\n",
      "Training loss for batch 6740 : 0.15022031962871552\n",
      "Training loss for batch 6741 : 0.031211989000439644\n",
      "Training loss for batch 6742 : 0.028698813170194626\n",
      "Training loss for batch 6743 : 0.23453257977962494\n",
      "Training loss for batch 6744 : 0.1018223762512207\n",
      "Training loss for batch 6745 : -0.0010690020862966776\n",
      "Training loss for batch 6746 : 0.06458840519189835\n",
      "Training loss for batch 6747 : 0.0\n",
      "Training loss for batch 6748 : 0.06812171638011932\n",
      "Training loss for batch 6749 : 0.2091480791568756\n",
      "Training loss for batch 6750 : 0.12389573454856873\n",
      "Training loss for batch 6751 : 0.06429572403430939\n",
      "Training loss for batch 6752 : 0.0\n",
      "Training loss for batch 6753 : 0.16220273077487946\n",
      "Training loss for batch 6754 : 0.2889384627342224\n",
      "Training loss for batch 6755 : 0.07755351066589355\n",
      "Training loss for batch 6756 : 0.26011282205581665\n",
      "Training loss for batch 6757 : 0.14101199805736542\n",
      "Training loss for batch 6758 : 0.016546307131648064\n",
      "Training loss for batch 6759 : 0.013825327157974243\n",
      "Training loss for batch 6760 : 0.06736437976360321\n",
      "Training loss for batch 6761 : 0.0939192995429039\n",
      "Training loss for batch 6762 : 0.30056020617485046\n",
      "Training loss for batch 6763 : 0.33550286293029785\n",
      "Training loss for batch 6764 : 0.07569736242294312\n",
      "Training loss for batch 6765 : 0.25230342149734497\n",
      "Training loss for batch 6766 : 0.038245994597673416\n",
      "Training loss for batch 6767 : 0.03920730575919151\n",
      "Training loss for batch 6768 : 0.23060797154903412\n",
      "Training loss for batch 6769 : 0.01069958321750164\n",
      "Training loss for batch 6770 : 0.45757758617401123\n",
      "Training loss for batch 6771 : 0.26366114616394043\n",
      "Training loss for batch 6772 : 0.24506698548793793\n",
      "Training loss for batch 6773 : 0.05579013004899025\n",
      "Training loss for batch 6774 : 0.05009349435567856\n",
      "Training loss for batch 6775 : 0.006058305501937866\n",
      "Training loss for batch 6776 : 0.2613879442214966\n",
      "Training loss for batch 6777 : 0.0\n",
      "Training loss for batch 6778 : 0.14854685962200165\n",
      "Training loss for batch 6779 : 0.1939687579870224\n",
      "Training loss for batch 6780 : 0.0598759688436985\n",
      "Training loss for batch 6781 : 0.1471555083990097\n",
      "Training loss for batch 6782 : 0.12706804275512695\n",
      "Training loss for batch 6783 : 0.1442830115556717\n",
      "Training loss for batch 6784 : 0.18434420228004456\n",
      "Training loss for batch 6785 : 0.035161010921001434\n",
      "Training loss for batch 6786 : 0.02407842129468918\n",
      "Training loss for batch 6787 : 0.2719026803970337\n",
      "Training loss for batch 6788 : 0.01678238809108734\n",
      "Training loss for batch 6789 : 0.07146605849266052\n",
      "Training loss for batch 6790 : 0.30361613631248474\n",
      "Training loss for batch 6791 : 0.27859392762184143\n",
      "Training loss for batch 6792 : 0.04545878618955612\n",
      "Training loss for batch 6793 : 0.04729337990283966\n",
      "Training loss for batch 6794 : 0.21920713782310486\n",
      "Training loss for batch 6795 : 0.1620599329471588\n",
      "Training loss for batch 6796 : 0.2737840414047241\n",
      "Training loss for batch 6797 : 0.28319650888442993\n",
      "Training loss for batch 6798 : 0.0076681082136929035\n",
      "Training loss for batch 6799 : 0.08742383122444153\n",
      "Training loss for batch 6800 : 0.07056090980768204\n",
      "Training loss for batch 6801 : 0.2928253710269928\n",
      "Training loss for batch 6802 : 0.1245356947183609\n",
      "Training loss for batch 6803 : 0.167580246925354\n",
      "Training loss for batch 6804 : 0.2752918601036072\n",
      "Training loss for batch 6805 : 0.2870069444179535\n",
      "Training loss for batch 6806 : 0.23938357830047607\n",
      "Training loss for batch 6807 : 0.006396681070327759\n",
      "Training loss for batch 6808 : 0.06462325900793076\n",
      "Training loss for batch 6809 : 0.003532459493726492\n",
      "Training loss for batch 6810 : 0.19504933059215546\n",
      "Training loss for batch 6811 : 0.0461774617433548\n",
      "Training loss for batch 6812 : 0.22527506947517395\n",
      "Training loss for batch 6813 : 0.06469792127609253\n",
      "Training loss for batch 6814 : 0.08777906745672226\n",
      "Training loss for batch 6815 : 0.02390584722161293\n",
      "Training loss for batch 6816 : 0.2955399751663208\n",
      "Training loss for batch 6817 : 0.15972843766212463\n",
      "Training loss for batch 6818 : 0.05653582513332367\n",
      "Training loss for batch 6819 : 0.25720369815826416\n",
      "Training loss for batch 6820 : 0.013078782707452774\n",
      "Training loss for batch 6821 : 0.10598823428153992\n",
      "Training loss for batch 6822 : 0.0923393964767456\n",
      "Training loss for batch 6823 : 0.06865403801202774\n",
      "Training loss for batch 6824 : 0.2066047191619873\n",
      "Training loss for batch 6825 : 0.1104976087808609\n",
      "Training loss for batch 6826 : 0.2094453126192093\n",
      "Training loss for batch 6827 : 0.20208801329135895\n",
      "Training loss for batch 6828 : 0.10700679570436478\n",
      "Training loss for batch 6829 : 0.08510912954807281\n",
      "Training loss for batch 6830 : 0.05580409616231918\n",
      "Training loss for batch 6831 : 0.2327912449836731\n",
      "Training loss for batch 6832 : 0.08932103216648102\n",
      "Training loss for batch 6833 : 0.13075624406337738\n",
      "Training loss for batch 6834 : 0.007029971573501825\n",
      "Training loss for batch 6835 : 0.23536035418510437\n",
      "Training loss for batch 6836 : -0.0007398693705908954\n",
      "Training loss for batch 6837 : 0.14590483903884888\n",
      "Training loss for batch 6838 : 0.12274567782878876\n",
      "Training loss for batch 6839 : 0.17189288139343262\n",
      "Training loss for batch 6840 : 0.029704617336392403\n",
      "Training loss for batch 6841 : 0.1745174676179886\n",
      "Training loss for batch 6842 : 0.21241778135299683\n",
      "Training loss for batch 6843 : 0.009907823987305164\n",
      "Training loss for batch 6844 : 0.041688479483127594\n",
      "Training loss for batch 6845 : 0.30542582273483276\n",
      "Training loss for batch 6846 : 0.10998519510030746\n",
      "Training loss for batch 6847 : 0.08090545982122421\n",
      "Training loss for batch 6848 : 0.05753085017204285\n",
      "Training loss for batch 6849 : 0.09033761918544769\n",
      "Training loss for batch 6850 : 0.07316944003105164\n",
      "Training loss for batch 6851 : 0.14093026518821716\n",
      "Training loss for batch 6852 : 0.13268734514713287\n",
      "Training loss for batch 6853 : 0.0501309297978878\n",
      "Training loss for batch 6854 : 0.11244498938322067\n",
      "Training loss for batch 6855 : 0.4528827369213104\n",
      "Training loss for batch 6856 : 0.06636665016412735\n",
      "Training loss for batch 6857 : 0.05187561735510826\n",
      "Training loss for batch 6858 : 0.006170675158500671\n",
      "Training loss for batch 6859 : 0.16909955441951752\n",
      "Training loss for batch 6860 : 0.36454662680625916\n",
      "Training loss for batch 6861 : 0.08459048718214035\n",
      "Training loss for batch 6862 : 0.0967247486114502\n",
      "Training loss for batch 6863 : 0.14467599987983704\n",
      "Training loss for batch 6864 : 0.21383832395076752\n",
      "Training loss for batch 6865 : 0.38960909843444824\n",
      "Training loss for batch 6866 : 0.0299462229013443\n",
      "Training loss for batch 6867 : 0.0935797318816185\n",
      "Training loss for batch 6868 : 0.11679388582706451\n",
      "Training loss for batch 6869 : 0.010408297181129456\n",
      "Training loss for batch 6870 : 0.4092584550380707\n",
      "Training loss for batch 6871 : 0.220703586935997\n",
      "Training loss for batch 6872 : 0.3273181915283203\n",
      "Training loss for batch 6873 : 0.1477476805448532\n",
      "Training loss for batch 6874 : 0.08300907909870148\n",
      "Training loss for batch 6875 : 0.14781299233436584\n",
      "Training loss for batch 6876 : 0.0818350687623024\n",
      "Training loss for batch 6877 : 0.09158336371183395\n",
      "Training loss for batch 6878 : 0.04998661205172539\n",
      "Training loss for batch 6879 : 0.04098096862435341\n",
      "Training loss for batch 6880 : 0.32007065415382385\n",
      "Training loss for batch 6881 : 0.01892484538257122\n",
      "Training loss for batch 6882 : 0.15119831264019012\n",
      "Training loss for batch 6883 : 0.3293951749801636\n",
      "Training loss for batch 6884 : 0.08195440471172333\n",
      "Training loss for batch 6885 : 0.2730092406272888\n",
      "Training loss for batch 6886 : 0.21680951118469238\n",
      "Training loss for batch 6887 : 0.359659880399704\n",
      "Training loss for batch 6888 : 0.24530556797981262\n",
      "Training loss for batch 6889 : 0.017198197543621063\n",
      "Training loss for batch 6890 : 0.19154714047908783\n",
      "Training loss for batch 6891 : 0.10251573473215103\n",
      "Training loss for batch 6892 : 0.08181136846542358\n",
      "Training loss for batch 6893 : 0.04155363142490387\n",
      "Training loss for batch 6894 : 0.031399697065353394\n",
      "Training loss for batch 6895 : 0.12425243854522705\n",
      "Training loss for batch 6896 : 0.2093719094991684\n",
      "Training loss for batch 6897 : 0.1286393254995346\n",
      "Training loss for batch 6898 : 0.26199212670326233\n",
      "Training loss for batch 6899 : 0.16161838173866272\n",
      "Training loss for batch 6900 : 0.0\n",
      "Training loss for batch 6901 : 0.055865444242954254\n",
      "Training loss for batch 6902 : -0.00129462662152946\n",
      "Training loss for batch 6903 : 0.31956788897514343\n",
      "Training loss for batch 6904 : 0.2275322824716568\n",
      "Training loss for batch 6905 : 0.1651836782693863\n",
      "Training loss for batch 6906 : 0.029869547113776207\n",
      "Training loss for batch 6907 : 0.060436006635427475\n",
      "Training loss for batch 6908 : 0.4410112202167511\n",
      "Training loss for batch 6909 : 0.18737488985061646\n",
      "Training loss for batch 6910 : 0.0027934610843658447\n",
      "Training loss for batch 6911 : 0.04443242400884628\n",
      "Training loss for batch 6912 : 0.07087439298629761\n",
      "Training loss for batch 6913 : 0.1828509122133255\n",
      "Training loss for batch 6914 : 0.04328884929418564\n",
      "Training loss for batch 6915 : 0.18374787271022797\n",
      "Training loss for batch 6916 : 0.05777072161436081\n",
      "Training loss for batch 6917 : 0.028143316507339478\n",
      "Training loss for batch 6918 : 0.020117588341236115\n",
      "Training loss for batch 6919 : 0.1519041657447815\n",
      "Training loss for batch 6920 : 0.2553083896636963\n",
      "Training loss for batch 6921 : 0.14470764994621277\n",
      "Training loss for batch 6922 : 0.40512987971305847\n",
      "Training loss for batch 6923 : 0.00995651539415121\n",
      "Training loss for batch 6924 : 0.13786721229553223\n",
      "Training loss for batch 6925 : 0.0883273333311081\n",
      "Training loss for batch 6926 : 0.2790130376815796\n",
      "Training loss for batch 6927 : 0.03228739649057388\n",
      "Training loss for batch 6928 : 0.11464196443557739\n",
      "Training loss for batch 6929 : 0.22305159270763397\n",
      "Training loss for batch 6930 : 0.03789633512496948\n",
      "Training loss for batch 6931 : 0.30229640007019043\n",
      "Training loss for batch 6932 : 0.13780467212200165\n",
      "Training loss for batch 6933 : 0.35679998993873596\n",
      "Training loss for batch 6934 : 0.20778357982635498\n",
      "Training loss for batch 6935 : 0.14127963781356812\n",
      "Training loss for batch 6936 : 0.08211430162191391\n",
      "Training loss for batch 6937 : 0.1168680191040039\n",
      "Training loss for batch 6938 : 0.24425002932548523\n",
      "Training loss for batch 6939 : 0.0009253224125131965\n",
      "Training loss for batch 6940 : 0.18970908224582672\n",
      "Training loss for batch 6941 : 0.2579350173473358\n",
      "Training loss for batch 6942 : 0.13204893469810486\n",
      "Training loss for batch 6943 : 0.01240837574005127\n",
      "Training loss for batch 6944 : 0.04654770344495773\n",
      "Training loss for batch 6945 : 0.09237255901098251\n",
      "Training loss for batch 6946 : 0.24268004298210144\n",
      "Training loss for batch 6947 : 0.17166799306869507\n",
      "Training loss for batch 6948 : 0.10148417204618454\n",
      "Training loss for batch 6949 : 0.01127837598323822\n",
      "Training loss for batch 6950 : 0.2842360734939575\n",
      "Training loss for batch 6951 : 0.26490017771720886\n",
      "Training loss for batch 6952 : 0.3400871157646179\n",
      "Training loss for batch 6953 : 0.18324914574623108\n",
      "Training loss for batch 6954 : 0.035216670483350754\n",
      "Training loss for batch 6955 : 0.237473264336586\n",
      "Training loss for batch 6956 : 0.3195550739765167\n",
      "Training loss for batch 6957 : 0.01208442635834217\n",
      "Training loss for batch 6958 : 0.09406860917806625\n",
      "Training loss for batch 6959 : 0.3096238374710083\n",
      "Training loss for batch 6960 : 0.12863509356975555\n",
      "Training loss for batch 6961 : 0.11296036839485168\n",
      "Training loss for batch 6962 : 0.0005089184851385653\n",
      "Training loss for batch 6963 : 0.061451949179172516\n",
      "Training loss for batch 6964 : 0.002289633033797145\n",
      "Training loss for batch 6965 : 0.3065466582775116\n",
      "Training loss for batch 6966 : 0.10387314110994339\n",
      "Training loss for batch 6967 : 0.011147390119731426\n",
      "Training loss for batch 6968 : 0.07754725962877274\n",
      "Training loss for batch 6969 : 0.2035803198814392\n",
      "Training loss for batch 6970 : 0.3148549199104309\n",
      "Training loss for batch 6971 : 0.3226558268070221\n",
      "Training loss for batch 6972 : 0.10411598533391953\n",
      "Training loss for batch 6973 : 0.14335446059703827\n",
      "Training loss for batch 6974 : 0.017277734354138374\n",
      "Training loss for batch 6975 : 0.06919687986373901\n",
      "Training loss for batch 6976 : 0.07998084276914597\n",
      "Training loss for batch 6977 : 0.2896973788738251\n",
      "Training loss for batch 6978 : 0.10833316296339035\n",
      "Training loss for batch 6979 : 0.0\n",
      "Training loss for batch 6980 : 0.12900933623313904\n",
      "Training loss for batch 6981 : 0.015430326573550701\n",
      "Training loss for batch 6982 : 0.2462794929742813\n",
      "Training loss for batch 6983 : 0.03237345442175865\n",
      "Training loss for batch 6984 : 0.14155688881874084\n",
      "Training loss for batch 6985 : 0.1322968751192093\n",
      "Training loss for batch 6986 : 0.16676123440265656\n",
      "Training loss for batch 6987 : 0.09365685284137726\n",
      "Training loss for batch 6988 : 0.27968108654022217\n",
      "Training loss for batch 6989 : 0.2487890124320984\n",
      "Training loss for batch 6990 : 0.05648456886410713\n",
      "Training loss for batch 6991 : 0.024258863180875778\n",
      "Training loss for batch 6992 : 0.23136036098003387\n",
      "Training loss for batch 6993 : 0.47331902384757996\n",
      "Training loss for batch 6994 : 0.18688063323497772\n",
      "Training loss for batch 6995 : 0.31331026554107666\n",
      "Training loss for batch 6996 : 0.019705580547451973\n",
      "Training loss for batch 6997 : 0.05930003523826599\n",
      "Training loss for batch 6998 : 0.027112647891044617\n",
      "Training loss for batch 6999 : 0.008389447815716267\n",
      "Training loss for batch 7000 : 0.03467816859483719\n",
      "Training loss for batch 7001 : 0.17529021203517914\n",
      "Training loss for batch 7002 : 0.3444809317588806\n",
      "Training loss for batch 7003 : 0.3776441514492035\n",
      "Training loss for batch 7004 : 0.00641109561547637\n",
      "Training loss for batch 7005 : 0.056514084339141846\n",
      "Training loss for batch 7006 : 0.14667588472366333\n",
      "Training loss for batch 7007 : 0.010390400886535645\n",
      "Training loss for batch 7008 : 0.053372371941804886\n",
      "Training loss for batch 7009 : 0.04430680349469185\n",
      "Training loss for batch 7010 : 0.07929090410470963\n",
      "Training loss for batch 7011 : 0.038639944046735764\n",
      "Training loss for batch 7012 : 0.07399485260248184\n",
      "Training loss for batch 7013 : 0.012937946245074272\n",
      "Training loss for batch 7014 : 0.16410629451274872\n",
      "Training loss for batch 7015 : 0.0711384117603302\n",
      "Training loss for batch 7016 : 0.301996111869812\n",
      "Training loss for batch 7017 : 0.1028931513428688\n",
      "Training loss for batch 7018 : 0.04275241866707802\n",
      "Training loss for batch 7019 : 0.018986616283655167\n",
      "Training loss for batch 7020 : 0.013957759365439415\n",
      "Training loss for batch 7021 : 0.000996152637526393\n",
      "Training loss for batch 7022 : 0.005808780901134014\n",
      "Training loss for batch 7023 : 0.20615878701210022\n",
      "Training loss for batch 7024 : 0.049378909170627594\n",
      "Training loss for batch 7025 : 0.48311471939086914\n",
      "Training loss for batch 7026 : 0.11399497091770172\n",
      "Training loss for batch 7027 : 0.012872676365077496\n",
      "Training loss for batch 7028 : 0.03381983935832977\n",
      "Training loss for batch 7029 : 0.06857439130544662\n",
      "Training loss for batch 7030 : 0.28857484459877014\n",
      "Training loss for batch 7031 : 0.12253350019454956\n",
      "Training loss for batch 7032 : 0.27537286281585693\n",
      "Training loss for batch 7033 : 0.1873452365398407\n",
      "Training loss for batch 7034 : 0.12479846179485321\n",
      "Training loss for batch 7035 : 0.4372090697288513\n",
      "Training loss for batch 7036 : 0.0\n",
      "Training loss for batch 7037 : 0.15889297425746918\n",
      "Training loss for batch 7038 : 0.09812373667955399\n",
      "Training loss for batch 7039 : 0.2676963806152344\n",
      "Training loss for batch 7040 : 0.39197516441345215\n",
      "Training loss for batch 7041 : 0.12880465388298035\n",
      "Training loss for batch 7042 : 0.0\n",
      "Training loss for batch 7043 : 0.048209913074970245\n",
      "Training loss for batch 7044 : 0.12240029126405716\n",
      "Training loss for batch 7045 : 0.2058483064174652\n",
      "Training loss for batch 7046 : 0.17990627884864807\n",
      "Training loss for batch 7047 : 0.04469060152769089\n",
      "Training loss for batch 7048 : 0.16210000216960907\n",
      "Training loss for batch 7049 : 0.034621693193912506\n",
      "Training loss for batch 7050 : 0.12549389898777008\n",
      "Training loss for batch 7051 : 0.0\n",
      "Training loss for batch 7052 : 0.2860749065876007\n",
      "Training loss for batch 7053 : 0.06536591053009033\n",
      "Training loss for batch 7054 : 0.38559433817863464\n",
      "Training loss for batch 7055 : 0.43924176692962646\n",
      "Training loss for batch 7056 : 0.25771719217300415\n",
      "Training loss for batch 7057 : 0.18234966695308685\n",
      "Training loss for batch 7058 : 0.19082941114902496\n",
      "Training loss for batch 7059 : 0.18592587113380432\n",
      "Training loss for batch 7060 : 0.49451106786727905\n",
      "Training loss for batch 7061 : 0.08339414745569229\n",
      "Training loss for batch 7062 : 0.06213700771331787\n",
      "Training loss for batch 7063 : 0.2532091736793518\n",
      "Training loss for batch 7064 : 0.009467761032283306\n",
      "Training loss for batch 7065 : 0.4752174913883209\n",
      "Training loss for batch 7066 : 0.008850633166730404\n",
      "Training loss for batch 7067 : 0.14953643083572388\n",
      "Training loss for batch 7068 : 0.2271338552236557\n",
      "Training loss for batch 7069 : 0.6122835874557495\n",
      "Training loss for batch 7070 : 0.07492829859256744\n",
      "Training loss for batch 7071 : 0.2865748405456543\n",
      "Training loss for batch 7072 : 0.046375274658203125\n",
      "Training loss for batch 7073 : 0.22773493826389313\n",
      "Training loss for batch 7074 : 0.05187157168984413\n",
      "Training loss for batch 7075 : 0.3133545517921448\n",
      "Training loss for batch 7076 : 0.19023708999156952\n",
      "Training loss for batch 7077 : 0.07492236793041229\n",
      "Training loss for batch 7078 : 0.14602085947990417\n",
      "Training loss for batch 7079 : 0.20028981566429138\n",
      "Training loss for batch 7080 : 0.26417046785354614\n",
      "Training loss for batch 7081 : 0.24451583623886108\n",
      "Training loss for batch 7082 : 0.1269899159669876\n",
      "Training loss for batch 7083 : 0.13656023144721985\n",
      "Training loss for batch 7084 : 0.222826287150383\n",
      "Training loss for batch 7085 : 0.1932147592306137\n",
      "Training loss for batch 7086 : 0.3579809069633484\n",
      "Training loss for batch 7087 : 0.10370489954948425\n",
      "Training loss for batch 7088 : 0.0936834067106247\n",
      "Training loss for batch 7089 : 0.2650258243083954\n",
      "Training loss for batch 7090 : 0.1596737653017044\n",
      "Training loss for batch 7091 : 0.3991154432296753\n",
      "Training loss for batch 7092 : 0.048546288162469864\n",
      "Training loss for batch 7093 : 0.14736109972000122\n",
      "Training loss for batch 7094 : 0.19352534413337708\n",
      "Training loss for batch 7095 : 0.14056068658828735\n",
      "Training loss for batch 7096 : 0.14598195254802704\n",
      "Training loss for batch 7097 : 0.02055303379893303\n",
      "Training loss for batch 7098 : 0.03301551938056946\n",
      "Training loss for batch 7099 : 0.07245205342769623\n",
      "Training loss for batch 7100 : 0.19945815205574036\n",
      "Training loss for batch 7101 : 0.1909986138343811\n",
      "Training loss for batch 7102 : 0.1822577863931656\n",
      "Training loss for batch 7103 : 0.3236902952194214\n",
      "Training loss for batch 7104 : 0.1368844360113144\n",
      "Training loss for batch 7105 : 0.11123678088188171\n",
      "Training loss for batch 7106 : 0.1226852685213089\n",
      "Training loss for batch 7107 : 0.03141311556100845\n",
      "Training loss for batch 7108 : 0.0010078375926241279\n",
      "Training loss for batch 7109 : 0.03450953960418701\n",
      "Training loss for batch 7110 : 0.05671411380171776\n",
      "Training loss for batch 7111 : 0.0\n",
      "Training loss for batch 7112 : 0.1788514256477356\n",
      "Training loss for batch 7113 : 0.3898901045322418\n",
      "Training loss for batch 7114 : 0.12433762103319168\n",
      "Training loss for batch 7115 : 0.3800361752510071\n",
      "Training loss for batch 7116 : 0.08779703080654144\n",
      "Training loss for batch 7117 : 0.09734432399272919\n",
      "Training loss for batch 7118 : 0.38308823108673096\n",
      "Training loss for batch 7119 : 0.796997606754303\n",
      "Training loss for batch 7120 : 0.0396832637488842\n",
      "Training loss for batch 7121 : 0.3426457643508911\n",
      "Training loss for batch 7122 : 0.2512570917606354\n",
      "Training loss for batch 7123 : 0.06801759451627731\n",
      "Training loss for batch 7124 : 0.11233639717102051\n",
      "Training loss for batch 7125 : 0.09920566529035568\n",
      "Training loss for batch 7126 : 0.011236986145377159\n",
      "Training loss for batch 7127 : 0.2834407389163971\n",
      "Training loss for batch 7128 : 0.06879807263612747\n",
      "Training loss for batch 7129 : 0.31075483560562134\n",
      "Training loss for batch 7130 : 0.25395750999450684\n",
      "Training loss for batch 7131 : 0.02263815701007843\n",
      "Training loss for batch 7132 : 0.11556373536586761\n",
      "Training loss for batch 7133 : 0.27009180188179016\n",
      "Training loss for batch 7134 : 0.02103450894355774\n",
      "Training loss for batch 7135 : 0.15997515618801117\n",
      "Training loss for batch 7136 : 0.03723775967955589\n",
      "Training loss for batch 7137 : 0.1332956701517105\n",
      "Training loss for batch 7138 : 0.05384445935487747\n",
      "Training loss for batch 7139 : 0.24736785888671875\n",
      "Training loss for batch 7140 : 0.47774526476860046\n",
      "Training loss for batch 7141 : 0.12066353112459183\n",
      "Training loss for batch 7142 : 0.3541485071182251\n",
      "Training loss for batch 7143 : 0.25122275948524475\n",
      "Training loss for batch 7144 : 0.10078845173120499\n",
      "Training loss for batch 7145 : 0.05839959904551506\n",
      "Training loss for batch 7146 : 0.2156568169593811\n",
      "Training loss for batch 7147 : 0.057616837322711945\n",
      "Training loss for batch 7148 : 0.15292231738567352\n",
      "Training loss for batch 7149 : 0.246150940656662\n",
      "Training loss for batch 7150 : 0.054296497255563736\n",
      "Training loss for batch 7151 : 0.0631122887134552\n",
      "Training loss for batch 7152 : 0.09830721467733383\n",
      "Training loss for batch 7153 : 0.14843502640724182\n",
      "Training loss for batch 7154 : 0.05822206288576126\n",
      "Training loss for batch 7155 : 0.12570524215698242\n",
      "Training loss for batch 7156 : 0.03326783329248428\n",
      "Training loss for batch 7157 : 0.1611088067293167\n",
      "Training loss for batch 7158 : 0.13445881009101868\n",
      "Training loss for batch 7159 : 0.2535603642463684\n",
      "Training loss for batch 7160 : 0.11264362931251526\n",
      "Training loss for batch 7161 : 0.03393224999308586\n",
      "Training loss for batch 7162 : 0.2811782956123352\n",
      "Training loss for batch 7163 : 0.07358243316411972\n",
      "Training loss for batch 7164 : 0.0\n",
      "Training loss for batch 7165 : 0.03702319785952568\n",
      "Training loss for batch 7166 : 0.14629538357257843\n",
      "Training loss for batch 7167 : 0.014910519123077393\n",
      "Training loss for batch 7168 : 0.13376562297344208\n",
      "Training loss for batch 7169 : 0.15859480202198029\n",
      "Training loss for batch 7170 : 0.2053166627883911\n",
      "Training loss for batch 7171 : 0.03668387606739998\n",
      "Training loss for batch 7172 : 0.15535546839237213\n",
      "Training loss for batch 7173 : -0.0010718320263549685\n",
      "Training loss for batch 7174 : -0.0004833089478779584\n",
      "Training loss for batch 7175 : 0.02887781336903572\n",
      "Training loss for batch 7176 : 0.07763399928808212\n",
      "Training loss for batch 7177 : 0.0857205018401146\n",
      "Training loss for batch 7178 : 0.13631129264831543\n",
      "Training loss for batch 7179 : 0.24606789648532867\n",
      "Training loss for batch 7180 : 0.2481050193309784\n",
      "Training loss for batch 7181 : 0.014865797013044357\n",
      "Training loss for batch 7182 : 0.15822504460811615\n",
      "Training loss for batch 7183 : 0.17174631357192993\n",
      "Training loss for batch 7184 : 0.1558350771665573\n",
      "Training loss for batch 7185 : 0.2452334314584732\n",
      "Training loss for batch 7186 : 0.2680792212486267\n",
      "Training loss for batch 7187 : 0.06978971511125565\n",
      "Training loss for batch 7188 : 0.2342732846736908\n",
      "Training loss for batch 7189 : 0.20687630772590637\n",
      "Training loss for batch 7190 : 0.18251284956932068\n",
      "Training loss for batch 7191 : 0.1836399883031845\n",
      "Training loss for batch 7192 : 0.32250842452049255\n",
      "Training loss for batch 7193 : 0.1287369728088379\n",
      "Training loss for batch 7194 : 0.10268847644329071\n",
      "Training loss for batch 7195 : 0.053363312035799026\n",
      "Training loss for batch 7196 : 0.16509592533111572\n",
      "Training loss for batch 7197 : 0.02252589724957943\n",
      "Training loss for batch 7198 : 0.1792398989200592\n",
      "Training loss for batch 7199 : 0.05124186724424362\n",
      "Training loss for batch 7200 : 0.17953120172023773\n",
      "Training loss for batch 7201 : 0.1262134611606598\n",
      "Training loss for batch 7202 : 0.22586177289485931\n",
      "Training loss for batch 7203 : 0.43733933568000793\n",
      "Training loss for batch 7204 : 0.041529182344675064\n",
      "Training loss for batch 7205 : 0.15153203904628754\n",
      "Training loss for batch 7206 : 0.0864836797118187\n",
      "Training loss for batch 7207 : 0.1919858306646347\n",
      "Training loss for batch 7208 : 0.12897801399230957\n",
      "Training loss for batch 7209 : 0.12631671130657196\n",
      "Training loss for batch 7210 : 0.03972207009792328\n",
      "Training loss for batch 7211 : 0.19478051364421844\n",
      "Training loss for batch 7212 : 0.2683338522911072\n",
      "Training loss for batch 7213 : 0.5557115077972412\n",
      "Training loss for batch 7214 : 0.21338675916194916\n",
      "Training loss for batch 7215 : 0.06736993044614792\n",
      "Training loss for batch 7216 : 0.013712337240576744\n",
      "Training loss for batch 7217 : 0.013659179210662842\n",
      "Training loss for batch 7218 : 0.14526113867759705\n",
      "Training loss for batch 7219 : 0.1286633163690567\n",
      "Training loss for batch 7220 : 0.3302971124649048\n",
      "Training loss for batch 7221 : 0.0065498556941747665\n",
      "Training loss for batch 7222 : 0.1413034051656723\n",
      "Training loss for batch 7223 : 0.23248720169067383\n",
      "Training loss for batch 7224 : 0.04728115350008011\n",
      "Training loss for batch 7225 : 0.2538018226623535\n",
      "Training loss for batch 7226 : 0.1106371060013771\n",
      "Training loss for batch 7227 : 0.04061306267976761\n",
      "Training loss for batch 7228 : 0.04606598615646362\n",
      "Training loss for batch 7229 : 0.16334760189056396\n",
      "Training loss for batch 7230 : 0.05917961895465851\n",
      "Training loss for batch 7231 : 0.24446144700050354\n",
      "Training loss for batch 7232 : 0.05414711311459541\n",
      "Training loss for batch 7233 : 0.03789472579956055\n",
      "Training loss for batch 7234 : 0.2167816311120987\n",
      "Training loss for batch 7235 : 0.10373254865407944\n",
      "Training loss for batch 7236 : 0.2728058695793152\n",
      "Training loss for batch 7237 : 0.21490314602851868\n",
      "Training loss for batch 7238 : 0.134891539812088\n",
      "Training loss for batch 7239 : 0.012370447628200054\n",
      "Training loss for batch 7240 : 0.2008192241191864\n",
      "Training loss for batch 7241 : 0.033735960721969604\n",
      "Training loss for batch 7242 : 0.17074377834796906\n",
      "Training loss for batch 7243 : -0.00011497226660139859\n",
      "Training loss for batch 7244 : 0.516862690448761\n",
      "Training loss for batch 7245 : 0.07541313767433167\n",
      "Training loss for batch 7246 : 0.1957642138004303\n",
      "Training loss for batch 7247 : 0.3756431043148041\n",
      "Training loss for batch 7248 : 0.07772497087717056\n",
      "Training loss for batch 7249 : 0.00012147519737482071\n",
      "Training loss for batch 7250 : 0.3452951908111572\n",
      "Training loss for batch 7251 : 0.06144360825419426\n",
      "Training loss for batch 7252 : 0.1519629806280136\n",
      "Training loss for batch 7253 : 0.07723452895879745\n",
      "Training loss for batch 7254 : 0.17091208696365356\n",
      "Training loss for batch 7255 : 0.0785798728466034\n",
      "Training loss for batch 7256 : 0.04635193943977356\n",
      "Training loss for batch 7257 : 0.3636908531188965\n",
      "Training loss for batch 7258 : 0.02503601834177971\n",
      "Training loss for batch 7259 : 0.14586828649044037\n",
      "Training loss for batch 7260 : 0.3097454309463501\n",
      "Training loss for batch 7261 : 0.29586198925971985\n",
      "Training loss for batch 7262 : 0.034899186342954636\n",
      "Training loss for batch 7263 : 0.19919663667678833\n",
      "Training loss for batch 7264 : 0.12440745532512665\n",
      "Training loss for batch 7265 : 0.0827474296092987\n",
      "Training loss for batch 7266 : 0.04697590321302414\n",
      "Training loss for batch 7267 : 0.28935742378234863\n",
      "Training loss for batch 7268 : 0.028026610612869263\n",
      "Training loss for batch 7269 : 0.03756219521164894\n",
      "Training loss for batch 7270 : 0.0796092078089714\n",
      "Training loss for batch 7271 : 0.09936054050922394\n",
      "Training loss for batch 7272 : 0.041219085454940796\n",
      "Training loss for batch 7273 : 0.23198527097702026\n",
      "Training loss for batch 7274 : 0.13042747974395752\n",
      "Training loss for batch 7275 : 0.12133869528770447\n",
      "Training loss for batch 7276 : 0.44489529728889465\n",
      "Training loss for batch 7277 : 0.2842654287815094\n",
      "Training loss for batch 7278 : 0.047385066747665405\n",
      "Training loss for batch 7279 : 0.18387597799301147\n",
      "Training loss for batch 7280 : 0.02524099498987198\n",
      "Training loss for batch 7281 : 0.058773159980773926\n",
      "Training loss for batch 7282 : 0.2062377631664276\n",
      "Training loss for batch 7283 : 0.13230100274085999\n",
      "Training loss for batch 7284 : 0.23038321733474731\n",
      "Training loss for batch 7285 : 0.06947912275791168\n",
      "Training loss for batch 7286 : -0.0013235381338745356\n",
      "Training loss for batch 7287 : -0.0019017379963770509\n",
      "Training loss for batch 7288 : 0.1571774184703827\n",
      "Training loss for batch 7289 : 0.23849797248840332\n",
      "Training loss for batch 7290 : 0.02444032020866871\n",
      "Training loss for batch 7291 : 0.0645560473203659\n",
      "Training loss for batch 7292 : 0.23032677173614502\n",
      "Training loss for batch 7293 : 0.10806390643119812\n",
      "Training loss for batch 7294 : 0.027274759486317635\n",
      "Training loss for batch 7295 : 0.07999330759048462\n",
      "Training loss for batch 7296 : 0.2456989586353302\n",
      "Training loss for batch 7297 : 0.3925210237503052\n",
      "Training loss for batch 7298 : 0.22688159346580505\n",
      "Training loss for batch 7299 : 0.1742769479751587\n",
      "Training loss for batch 7300 : 0.04100673273205757\n",
      "Training loss for batch 7301 : 0.285959392786026\n",
      "Training loss for batch 7302 : 0.24124076962471008\n",
      "Training loss for batch 7303 : 0.2562633752822876\n",
      "Training loss for batch 7304 : 0.001829852699302137\n",
      "Training loss for batch 7305 : 0.2048315405845642\n",
      "Training loss for batch 7306 : 0.19592252373695374\n",
      "Training loss for batch 7307 : 0.03685805946588516\n",
      "Training loss for batch 7308 : 0.17808109521865845\n",
      "Training loss for batch 7309 : 0.21526627242565155\n",
      "Training loss for batch 7310 : 0.008802424184978008\n",
      "Training loss for batch 7311 : 0.09247174859046936\n",
      "Training loss for batch 7312 : 0.13996781408786774\n",
      "Training loss for batch 7313 : 0.2179396152496338\n",
      "Training loss for batch 7314 : 0.20212414860725403\n",
      "Training loss for batch 7315 : 0.15473392605781555\n",
      "Training loss for batch 7316 : 0.2820883095264435\n",
      "Training loss for batch 7317 : 0.021507490426301956\n",
      "Training loss for batch 7318 : 0.003285700920969248\n",
      "Training loss for batch 7319 : 0.06392771005630493\n",
      "Training loss for batch 7320 : 0.19916851818561554\n",
      "Training loss for batch 7321 : 0.3616248369216919\n",
      "Training loss for batch 7322 : 0.056277722120285034\n",
      "Training loss for batch 7323 : 0.28871309757232666\n",
      "Training loss for batch 7324 : 0.0635053962469101\n",
      "Training loss for batch 7325 : 0.09801869094371796\n",
      "Training loss for batch 7326 : 0.32574203610420227\n",
      "Training loss for batch 7327 : 0.10439988970756531\n",
      "Training loss for batch 7328 : 0.025583280250430107\n",
      "Training loss for batch 7329 : 0.15464168787002563\n",
      "Training loss for batch 7330 : 0.3102978765964508\n",
      "Training loss for batch 7331 : 0.03167093172669411\n",
      "Training loss for batch 7332 : 0.10730766505002975\n",
      "Training loss for batch 7333 : 0.3366714417934418\n",
      "Training loss for batch 7334 : 0.0037470986135303974\n",
      "Training loss for batch 7335 : 0.2843436896800995\n",
      "Training loss for batch 7336 : 0.14434200525283813\n",
      "Training loss for batch 7337 : 0.20951777696609497\n",
      "Training loss for batch 7338 : 0.04828937351703644\n",
      "Training loss for batch 7339 : 0.15175235271453857\n",
      "Training loss for batch 7340 : 0.2452497035264969\n",
      "Training loss for batch 7341 : 0.08452517539262772\n",
      "Training loss for batch 7342 : 0.059367962181568146\n",
      "Training loss for batch 7343 : 0.0024616520386189222\n",
      "Training loss for batch 7344 : 0.24828428030014038\n",
      "Training loss for batch 7345 : 0.1870504766702652\n",
      "Training loss for batch 7346 : 0.01537327654659748\n",
      "Training loss for batch 7347 : 0.31514763832092285\n",
      "Training loss for batch 7348 : 0.20100997388362885\n",
      "Training loss for batch 7349 : 0.3441826105117798\n",
      "Training loss for batch 7350 : 0.342407763004303\n",
      "Training loss for batch 7351 : 0.17556944489479065\n",
      "Training loss for batch 7352 : 0.09664404392242432\n",
      "Training loss for batch 7353 : 0.014691800810396671\n",
      "Training loss for batch 7354 : 0.3710314631462097\n",
      "Training loss for batch 7355 : 0.10732726752758026\n",
      "Training loss for batch 7356 : 0.0837169885635376\n",
      "Training loss for batch 7357 : 0.0\n",
      "Training loss for batch 7358 : 0.2130364626646042\n",
      "Training loss for batch 7359 : 0.13435085117816925\n",
      "Training loss for batch 7360 : 0.032298699021339417\n",
      "Training loss for batch 7361 : 0.030410295352339745\n",
      "Training loss for batch 7362 : 0.09211038798093796\n",
      "Training loss for batch 7363 : 0.027258872985839844\n",
      "Training loss for batch 7364 : 0.28967931866645813\n",
      "Training loss for batch 7365 : 0.4086756408214569\n",
      "Training loss for batch 7366 : 0.10118730366230011\n",
      "Training loss for batch 7367 : 0.23810739815235138\n",
      "Training loss for batch 7368 : 0.28363561630249023\n",
      "Training loss for batch 7369 : 0.07627461105585098\n",
      "Training loss for batch 7370 : 0.018555037677288055\n",
      "Training loss for batch 7371 : 0.18693386018276215\n",
      "Training loss for batch 7372 : 0.1117136999964714\n",
      "Training loss for batch 7373 : 0.16898401081562042\n",
      "Training loss for batch 7374 : 0.1903693974018097\n",
      "Training loss for batch 7375 : 0.07791558653116226\n",
      "Training loss for batch 7376 : 0.070404052734375\n",
      "Training loss for batch 7377 : 0.0981823280453682\n",
      "Training loss for batch 7378 : 0.010477440431714058\n",
      "Training loss for batch 7379 : 0.0003992603160440922\n",
      "Training loss for batch 7380 : 0.06777967512607574\n",
      "Training loss for batch 7381 : 0.19434891641139984\n",
      "Training loss for batch 7382 : 0.1650198996067047\n",
      "Training loss for batch 7383 : 0.17151793837547302\n",
      "Training loss for batch 7384 : 0.11690777540206909\n",
      "Training loss for batch 7385 : 0.15777213871479034\n",
      "Training loss for batch 7386 : 0.05335129052400589\n",
      "Training loss for batch 7387 : 0.053837038576602936\n",
      "Training loss for batch 7388 : 0.25893881916999817\n",
      "Training loss for batch 7389 : 0.12300586700439453\n",
      "Training loss for batch 7390 : 0.22342270612716675\n",
      "Training loss for batch 7391 : 0.05676979944109917\n",
      "Training loss for batch 7392 : 0.07933823019266129\n",
      "Training loss for batch 7393 : 0.33220845460891724\n",
      "Training loss for batch 7394 : 0.11341497302055359\n",
      "Training loss for batch 7395 : 0.01705193519592285\n",
      "Training loss for batch 7396 : 0.17877693474292755\n",
      "Training loss for batch 7397 : 0.07579954713582993\n",
      "Training loss for batch 7398 : 0.09786457568407059\n",
      "Training loss for batch 7399 : 0.05408492684364319\n",
      "Training loss for batch 7400 : 0.014360904693603516\n",
      "Training loss for batch 7401 : 0.24537676572799683\n",
      "Training loss for batch 7402 : 0.050844211131334305\n",
      "Training loss for batch 7403 : 0.03909517452120781\n",
      "Training loss for batch 7404 : 0.07670494168996811\n",
      "Training loss for batch 7405 : 0.1993722915649414\n",
      "Training loss for batch 7406 : 0.13256224989891052\n",
      "Training loss for batch 7407 : 0.30408453941345215\n",
      "Training loss for batch 7408 : 0.014119794592261314\n",
      "Training loss for batch 7409 : 0.16782255470752716\n",
      "Training loss for batch 7410 : 0.09505366533994675\n",
      "Training loss for batch 7411 : 0.13937287032604218\n",
      "Training loss for batch 7412 : 0.023691488429903984\n",
      "Training loss for batch 7413 : 0.034273624420166016\n",
      "Training loss for batch 7414 : 0.32769036293029785\n",
      "Training loss for batch 7415 : 0.1791553646326065\n",
      "Training loss for batch 7416 : 0.013422101736068726\n",
      "Training loss for batch 7417 : 0.1736689656972885\n",
      "Training loss for batch 7418 : 0.4762851297855377\n",
      "Training loss for batch 7419 : 0.04406173154711723\n",
      "Training loss for batch 7420 : 0.11324182897806168\n",
      "Training loss for batch 7421 : 0.021432489156723022\n",
      "Training loss for batch 7422 : 0.07040137052536011\n",
      "Training loss for batch 7423 : 0.09281489253044128\n",
      "Training loss for batch 7424 : 0.5584906935691833\n",
      "Training loss for batch 7425 : 0.27542251348495483\n",
      "Training loss for batch 7426 : 0.21071094274520874\n",
      "Training loss for batch 7427 : 0.01751558855175972\n",
      "Training loss for batch 7428 : 0.0928381159901619\n",
      "Training loss for batch 7429 : 0.029785487800836563\n",
      "Training loss for batch 7430 : 0.21788239479064941\n",
      "Training loss for batch 7431 : 0.0014530248008668423\n",
      "Training loss for batch 7432 : 0.08012881875038147\n",
      "Training loss for batch 7433 : 0.20677095651626587\n",
      "Training loss for batch 7434 : 0.013152437284588814\n",
      "Training loss for batch 7435 : 0.33803901076316833\n",
      "Training loss for batch 7436 : 0.14552737772464752\n",
      "Training loss for batch 7437 : 0.09757845848798752\n",
      "Training loss for batch 7438 : 0.04340501129627228\n",
      "Training loss for batch 7439 : 0.22158166766166687\n",
      "Training loss for batch 7440 : 0.1762896478176117\n",
      "Training loss for batch 7441 : 0.029654037207365036\n",
      "Training loss for batch 7442 : 0.2069745659828186\n",
      "Training loss for batch 7443 : 0.19637927412986755\n",
      "Training loss for batch 7444 : 0.05371247977018356\n",
      "Training loss for batch 7445 : 0.4175095558166504\n",
      "Training loss for batch 7446 : 0.1160183995962143\n",
      "Training loss for batch 7447 : 0.2706984877586365\n",
      "Training loss for batch 7448 : 0.10989895462989807\n",
      "Training loss for batch 7449 : 0.13688436150550842\n",
      "Training loss for batch 7450 : 0.19771137833595276\n",
      "Training loss for batch 7451 : 0.05864391103386879\n",
      "Training loss for batch 7452 : 0.10107384622097015\n",
      "Training loss for batch 7453 : 0.24608781933784485\n",
      "Training loss for batch 7454 : 0.12169617414474487\n",
      "Training loss for batch 7455 : 0.46643951535224915\n",
      "Training loss for batch 7456 : 0.036867525428533554\n",
      "Training loss for batch 7457 : 0.15767532587051392\n",
      "Training loss for batch 7458 : 0.26641443371772766\n",
      "Training loss for batch 7459 : 0.20915813744068146\n",
      "Training loss for batch 7460 : 0.28899380564689636\n",
      "Training loss for batch 7461 : 0.34451138973236084\n",
      "Training loss for batch 7462 : 0.06659083068370819\n",
      "Training loss for batch 7463 : 0.04246044158935547\n",
      "Training loss for batch 7464 : 0.09915713220834732\n",
      "Training loss for batch 7465 : 0.22050267457962036\n",
      "Training loss for batch 7466 : 0.07268097996711731\n",
      "Training loss for batch 7467 : 0.10795080661773682\n",
      "Training loss for batch 7468 : 0.15997247397899628\n",
      "Training loss for batch 7469 : 0.14394190907478333\n",
      "Training loss for batch 7470 : 0.020073898136615753\n",
      "Training loss for batch 7471 : 0.3595978617668152\n",
      "Training loss for batch 7472 : 0.1328759342432022\n",
      "Training loss for batch 7473 : 0.09059476107358932\n",
      "Training loss for batch 7474 : 0.0545538067817688\n",
      "Training loss for batch 7475 : 0.08482624590396881\n",
      "Training loss for batch 7476 : 0.25526976585388184\n",
      "Training loss for batch 7477 : 0.091072678565979\n",
      "Training loss for batch 7478 : 0.24312707781791687\n",
      "Training loss for batch 7479 : 0.1249825730919838\n",
      "Training loss for batch 7480 : 0.13560213148593903\n",
      "Training loss for batch 7481 : 0.01550039742141962\n",
      "Training loss for batch 7482 : 0.13283082842826843\n",
      "Training loss for batch 7483 : 0.01692737266421318\n",
      "Training loss for batch 7484 : 0.1905646175146103\n",
      "Training loss for batch 7485 : 0.022118091583251953\n",
      "Training loss for batch 7486 : 0.4646432101726532\n",
      "Training loss for batch 7487 : 0.052805401384830475\n",
      "Training loss for batch 7488 : 0.24003010988235474\n",
      "Training loss for batch 7489 : 0.19150783121585846\n",
      "Training loss for batch 7490 : 0.3371660113334656\n",
      "Training loss for batch 7491 : 0.015200446359813213\n",
      "Training loss for batch 7492 : 0.3767867088317871\n",
      "Training loss for batch 7493 : 0.1802481710910797\n",
      "Training loss for batch 7494 : 0.09194037318229675\n",
      "Training loss for batch 7495 : 0.20618876814842224\n",
      "Training loss for batch 7496 : 0.2835630178451538\n",
      "Training loss for batch 7497 : 0.09071304649114609\n",
      "Training loss for batch 7498 : 0.19803525507450104\n",
      "Training loss for batch 7499 : 0.09668299555778503\n",
      "Training loss for batch 7500 : 0.13378313183784485\n",
      "Training loss for batch 7501 : 0.10107141733169556\n",
      "Training loss for batch 7502 : 0.024847060441970825\n",
      "Training loss for batch 7503 : 0.11703431606292725\n",
      "Training loss for batch 7504 : 0.19021165370941162\n",
      "Training loss for batch 7505 : 0.0608302466571331\n",
      "Training loss for batch 7506 : 0.07604722678661346\n",
      "Training loss for batch 7507 : 0.38959503173828125\n",
      "Training loss for batch 7508 : 0.15923041105270386\n",
      "Training loss for batch 7509 : 0.10943099856376648\n",
      "Training loss for batch 7510 : 0.22891022264957428\n",
      "Training loss for batch 7511 : 0.2361413538455963\n",
      "Training loss for batch 7512 : 0.12654465436935425\n",
      "Training loss for batch 7513 : 0.10336793959140778\n",
      "Training loss for batch 7514 : 0.0173896886408329\n",
      "Training loss for batch 7515 : 0.23046328127384186\n",
      "Training loss for batch 7516 : 0.011950093321502209\n",
      "Training loss for batch 7517 : 0.1864318698644638\n",
      "Training loss for batch 7518 : 0.0997188612818718\n",
      "Training loss for batch 7519 : 0.22233763337135315\n",
      "Training loss for batch 7520 : 0.2583724856376648\n",
      "Training loss for batch 7521 : 0.0033989548683166504\n",
      "Training loss for batch 7522 : 0.041873373091220856\n",
      "Training loss for batch 7523 : 0.29388925433158875\n",
      "Training loss for batch 7524 : 0.20619958639144897\n",
      "Training loss for batch 7525 : 0.08650106936693192\n",
      "Training loss for batch 7526 : 0.13429075479507446\n",
      "Training loss for batch 7527 : 0.1000814437866211\n",
      "Training loss for batch 7528 : 0.09860999882221222\n",
      "Training loss for batch 7529 : 0.059677604585886\n",
      "Training loss for batch 7530 : 0.09266972541809082\n",
      "Training loss for batch 7531 : 0.022636614739894867\n",
      "Training loss for batch 7532 : 0.09542132914066315\n",
      "Training loss for batch 7533 : 0.08943501859903336\n",
      "Training loss for batch 7534 : 0.0302156712859869\n",
      "Training loss for batch 7535 : 0.11749810725450516\n",
      "Training loss for batch 7536 : 0.021714819595217705\n",
      "Training loss for batch 7537 : 0.2131262868642807\n",
      "Training loss for batch 7538 : 0.13678976893424988\n",
      "Training loss for batch 7539 : 0.0013956637121737003\n",
      "Training loss for batch 7540 : 0.08359820395708084\n",
      "Training loss for batch 7541 : 0.3070041537284851\n",
      "Training loss for batch 7542 : 0.03363187611103058\n",
      "Training loss for batch 7543 : 0.3270335793495178\n",
      "Training loss for batch 7544 : 0.33615750074386597\n",
      "Training loss for batch 7545 : 0.14622879028320312\n",
      "Training loss for batch 7546 : 0.08017554134130478\n",
      "Training loss for batch 7547 : 0.21812619268894196\n",
      "Training loss for batch 7548 : 0.04253236949443817\n",
      "Training loss for batch 7549 : 0.2595188617706299\n",
      "Training loss for batch 7550 : -0.0025782245211303234\n",
      "Training loss for batch 7551 : 0.2329167276620865\n",
      "Training loss for batch 7552 : 0.1031489446759224\n",
      "Training loss for batch 7553 : 0.2951672375202179\n",
      "Training loss for batch 7554 : 0.040145691484212875\n",
      "Training loss for batch 7555 : 0.07917383313179016\n",
      "Training loss for batch 7556 : 0.0027650296688079834\n",
      "Training loss for batch 7557 : 0.30705779790878296\n",
      "Training loss for batch 7558 : 0.13983015716075897\n",
      "Training loss for batch 7559 : 0.1487952470779419\n",
      "Training loss for batch 7560 : 0.2747684419155121\n",
      "Training loss for batch 7561 : 0.12104358524084091\n",
      "Training loss for batch 7562 : 0.2166079431772232\n",
      "Training loss for batch 7563 : 0.1969216763973236\n",
      "Training loss for batch 7564 : 0.10846643894910812\n",
      "Training loss for batch 7565 : 0.04645838961005211\n",
      "Training loss for batch 7566 : 0.22446973621845245\n",
      "Training loss for batch 7567 : 0.11917321383953094\n",
      "Training loss for batch 7568 : 0.050397854298353195\n",
      "Training loss for batch 7569 : 0.04069335758686066\n",
      "Training loss for batch 7570 : 0.10326380282640457\n",
      "Training loss for batch 7571 : 0.10693690925836563\n",
      "Training loss for batch 7572 : 0.2089754343032837\n",
      "Training loss for batch 7573 : 0.243571475148201\n",
      "Training loss for batch 7574 : 0.21542468667030334\n",
      "Training loss for batch 7575 : 0.18518006801605225\n",
      "Training loss for batch 7576 : 0.07858188450336456\n",
      "Training loss for batch 7577 : 0.2377627193927765\n",
      "Training loss for batch 7578 : 0.013741451315581799\n",
      "Training loss for batch 7579 : 0.06323449313640594\n",
      "Training loss for batch 7580 : 0.16670997440814972\n",
      "Training loss for batch 7581 : 0.058875247836112976\n",
      "Training loss for batch 7582 : 0.07036705315113068\n",
      "Training loss for batch 7583 : 0.15591253340244293\n",
      "Training loss for batch 7584 : 0.16767877340316772\n",
      "Training loss for batch 7585 : 0.014458130113780499\n",
      "Training loss for batch 7586 : 0.13676095008850098\n",
      "Training loss for batch 7587 : 0.20537996292114258\n",
      "Training loss for batch 7588 : 0.06386128813028336\n",
      "Training loss for batch 7589 : 0.07291214168071747\n",
      "Training loss for batch 7590 : 0.13447290658950806\n",
      "Training loss for batch 7591 : 0.19000855088233948\n",
      "Training loss for batch 7592 : 0.2015371024608612\n",
      "Training loss for batch 7593 : 0.2557809352874756\n",
      "Training loss for batch 7594 : 0.10277792811393738\n",
      "Training loss for batch 7595 : 0.3800380229949951\n",
      "Training loss for batch 7596 : 0.37502720952033997\n",
      "Training loss for batch 7597 : 0.0\n",
      "Training loss for batch 7598 : 0.0029580204281955957\n",
      "Training loss for batch 7599 : 0.06988678872585297\n",
      "Training loss for batch 7600 : 0.17107315361499786\n",
      "Training loss for batch 7601 : 0.21386206150054932\n",
      "Training loss for batch 7602 : 0.159211665391922\n",
      "Training loss for batch 7603 : 0.21867454051971436\n",
      "Training loss for batch 7604 : 0.00046928724623285234\n",
      "Training loss for batch 7605 : 0.19272632896900177\n",
      "Training loss for batch 7606 : 0.11613787710666656\n",
      "Training loss for batch 7607 : 0.3744751214981079\n",
      "Training loss for batch 7608 : 0.11769495159387589\n",
      "Training loss for batch 7609 : 0.25159284472465515\n",
      "Training loss for batch 7610 : 0.26825058460235596\n",
      "Training loss for batch 7611 : 0.02872588485479355\n",
      "Training loss for batch 7612 : 0.3390902280807495\n",
      "Training loss for batch 7613 : 0.16433951258659363\n",
      "Training loss for batch 7614 : 0.11933092027902603\n",
      "Training loss for batch 7615 : 0.0682225227355957\n",
      "Training loss for batch 7616 : 0.14974378049373627\n",
      "Training loss for batch 7617 : 0.3697396516799927\n",
      "Training loss for batch 7618 : 0.3679141402244568\n",
      "Training loss for batch 7619 : 0.31207630038261414\n",
      "Training loss for batch 7620 : 0.24529580771923065\n",
      "Training loss for batch 7621 : 0.009437501430511475\n",
      "Training loss for batch 7622 : 0.03781198710203171\n",
      "Training loss for batch 7623 : 0.2083083689212799\n",
      "Training loss for batch 7624 : 0.19060422480106354\n",
      "Training loss for batch 7625 : 0.33686280250549316\n",
      "Training loss for batch 7626 : 0.3759979009628296\n",
      "Training loss for batch 7627 : 0.1394023299217224\n",
      "Training loss for batch 7628 : 0.02032371796667576\n",
      "Training loss for batch 7629 : 0.06706459075212479\n",
      "Training loss for batch 7630 : 0.15563499927520752\n",
      "Training loss for batch 7631 : 0.05811820924282074\n",
      "Training loss for batch 7632 : 0.0011986494064331055\n",
      "Training loss for batch 7633 : 0.36812639236450195\n",
      "Training loss for batch 7634 : 0.21639558672904968\n",
      "Training loss for batch 7635 : 0.034105461090803146\n",
      "Training loss for batch 7636 : 0.025387918576598167\n",
      "Training loss for batch 7637 : 0.11542996019124985\n",
      "Training loss for batch 7638 : 0.19128374755382538\n",
      "Training loss for batch 7639 : 0.019699538126587868\n",
      "Training loss for batch 7640 : 0.06098019331693649\n",
      "Training loss for batch 7641 : 0.2185518443584442\n",
      "Training loss for batch 7642 : 0.06328702718019485\n",
      "Training loss for batch 7643 : 0.0743880644440651\n",
      "Training loss for batch 7644 : 0.11995116621255875\n",
      "Training loss for batch 7645 : 0.0477156937122345\n",
      "Training loss for batch 7646 : 0.12312720715999603\n",
      "Training loss for batch 7647 : 0.29631584882736206\n",
      "Training loss for batch 7648 : 0.12481348216533661\n",
      "Training loss for batch 7649 : 0.22952109575271606\n",
      "Training loss for batch 7650 : 0.019811559468507767\n",
      "Training loss for batch 7651 : 0.043920163065195084\n",
      "Training loss for batch 7652 : 0.14202585816383362\n",
      "Training loss for batch 7653 : 0.20685359835624695\n",
      "Training loss for batch 7654 : 0.20990712940692902\n",
      "Training loss for batch 7655 : 0.10604918003082275\n",
      "Training loss for batch 7656 : 0.23834365606307983\n",
      "Training loss for batch 7657 : 0.2052178829908371\n",
      "Training loss for batch 7658 : 0.14212389290332794\n",
      "Training loss for batch 7659 : 0.12627574801445007\n",
      "Training loss for batch 7660 : -0.0035499371588230133\n",
      "Training loss for batch 7661 : 0.015422861091792583\n",
      "Training loss for batch 7662 : 0.3156909942626953\n",
      "Training loss for batch 7663 : 0.09593167901039124\n",
      "Training loss for batch 7664 : 0.15819653868675232\n",
      "Training loss for batch 7665 : 0.022353600710630417\n",
      "Training loss for batch 7666 : 0.056715723127126694\n",
      "Training loss for batch 7667 : 0.07967853546142578\n",
      "Training loss for batch 7668 : 0.004773499444127083\n",
      "Training loss for batch 7669 : 0.09354337304830551\n",
      "Training loss for batch 7670 : 0.19844281673431396\n",
      "Training loss for batch 7671 : 0.05426514521241188\n",
      "Training loss for batch 7672 : 0.013486715033650398\n",
      "Training loss for batch 7673 : 0.19119349122047424\n",
      "Training loss for batch 7674 : 0.18928298354148865\n",
      "Training loss for batch 7675 : 0.2519387900829315\n",
      "Training loss for batch 7676 : 0.20954930782318115\n",
      "Training loss for batch 7677 : 0.6962552070617676\n",
      "Training loss for batch 7678 : 0.024986758828163147\n",
      "Training loss for batch 7679 : 0.29236552119255066\n",
      "Training loss for batch 7680 : 0.15201547741889954\n",
      "Training loss for batch 7681 : 0.1770711988210678\n",
      "Training loss for batch 7682 : 0.0031600992660969496\n",
      "Training loss for batch 7683 : 0.5996607542037964\n",
      "Training loss for batch 7684 : 0.11928361654281616\n",
      "Training loss for batch 7685 : 0.016156941652297974\n",
      "Training loss for batch 7686 : 0.3354496657848358\n",
      "Training loss for batch 7687 : 0.476286917924881\n",
      "Training loss for batch 7688 : 0.19323192536830902\n",
      "Training loss for batch 7689 : 0.14460282027721405\n",
      "Training loss for batch 7690 : 0.2502065896987915\n",
      "Training loss for batch 7691 : 0.24670393764972687\n",
      "Training loss for batch 7692 : 0.2408333271741867\n",
      "Training loss for batch 7693 : 0.5641005635261536\n",
      "Training loss for batch 7694 : 0.11734035611152649\n",
      "Training loss for batch 7695 : 0.0693405419588089\n",
      "Training loss for batch 7696 : 0.09798917919397354\n",
      "Training loss for batch 7697 : 0.05592843145132065\n",
      "Training loss for batch 7698 : 0.03852557763457298\n",
      "Training loss for batch 7699 : 0.35279643535614014\n",
      "Training loss for batch 7700 : 0.0\n",
      "Training loss for batch 7701 : 0.2799757719039917\n",
      "Training loss for batch 7702 : 0.19940440356731415\n",
      "Training loss for batch 7703 : 0.3013533651828766\n",
      "Training loss for batch 7704 : 0.09306428581476212\n",
      "Training loss for batch 7705 : 0.12256916612386703\n",
      "Training loss for batch 7706 : 0.09123814105987549\n",
      "Training loss for batch 7707 : 0.06983357667922974\n",
      "Training loss for batch 7708 : 0.27921533584594727\n",
      "Training loss for batch 7709 : 0.04714678227901459\n",
      "Training loss for batch 7710 : 0.2584344148635864\n",
      "Training loss for batch 7711 : 0.0627485066652298\n",
      "Training loss for batch 7712 : 0.1669386476278305\n",
      "Training loss for batch 7713 : 0.11887293308973312\n",
      "Training loss for batch 7714 : 0.23394688963890076\n",
      "Training loss for batch 7715 : 0.16419066488742828\n",
      "Training loss for batch 7716 : 0.018045928329229355\n",
      "Training loss for batch 7717 : 0.08477506041526794\n",
      "Training loss for batch 7718 : 0.34967201948165894\n",
      "Training loss for batch 7719 : 0.3263508677482605\n",
      "Training loss for batch 7720 : 0.3595579266548157\n",
      "Training loss for batch 7721 : 0.16890859603881836\n",
      "Training loss for batch 7722 : 0.18595939874649048\n",
      "Training loss for batch 7723 : 0.25795918703079224\n",
      "Training loss for batch 7724 : 0.11620531231164932\n",
      "Training loss for batch 7725 : 0.10753630846738815\n",
      "Training loss for batch 7726 : 0.028200196102261543\n",
      "Training loss for batch 7727 : 0.03508468344807625\n",
      "Training loss for batch 7728 : 0.06820572912693024\n",
      "Training loss for batch 7729 : 0.2930159568786621\n",
      "Training loss for batch 7730 : 0.33670347929000854\n",
      "Training loss for batch 7731 : 0.13337144255638123\n",
      "Training loss for batch 7732 : 0.12452837079763412\n",
      "Training loss for batch 7733 : 0.08250222355127335\n",
      "Training loss for batch 7734 : 0.2918418347835541\n",
      "Training loss for batch 7735 : 0.08920211344957352\n",
      "Training loss for batch 7736 : 0.011760057881474495\n",
      "Training loss for batch 7737 : 0.12926572561264038\n",
      "Training loss for batch 7738 : 0.07037904858589172\n",
      "Training loss for batch 7739 : 0.13611504435539246\n",
      "Training loss for batch 7740 : 0.17649945616722107\n",
      "Training loss for batch 7741 : 0.1179259642958641\n",
      "Training loss for batch 7742 : 0.04886140674352646\n",
      "Training loss for batch 7743 : 0.6690984964370728\n",
      "Training loss for batch 7744 : 0.11452201753854752\n",
      "Training loss for batch 7745 : 0.14218369126319885\n",
      "Training loss for batch 7746 : 0.24243739247322083\n",
      "Training loss for batch 7747 : 0.00209088739939034\n",
      "Training loss for batch 7748 : 0.25251781940460205\n",
      "Training loss for batch 7749 : 0.10778351128101349\n",
      "Training loss for batch 7750 : 0.2038605660200119\n",
      "Training loss for batch 7751 : 0.06525097042322159\n",
      "Training loss for batch 7752 : 0.36794787645339966\n",
      "Training loss for batch 7753 : 0.06501464545726776\n",
      "Training loss for batch 7754 : 0.10696983337402344\n",
      "Training loss for batch 7755 : 0.21740345656871796\n",
      "Training loss for batch 7756 : 0.02189372293651104\n",
      "Training loss for batch 7757 : 0.004789263010025024\n",
      "Training loss for batch 7758 : 0.8831615447998047\n",
      "Training loss for batch 7759 : 0.19615355134010315\n",
      "Training loss for batch 7760 : 0.21723103523254395\n",
      "Training loss for batch 7761 : 0.16266579926013947\n",
      "Training loss for batch 7762 : 0.010454009287059307\n",
      "Training loss for batch 7763 : 0.2153438925743103\n",
      "Training loss for batch 7764 : 0.4610181152820587\n",
      "Training loss for batch 7765 : 0.10268333554267883\n",
      "Training loss for batch 7766 : 0.050194792449474335\n",
      "Training loss for batch 7767 : 0.04677100107073784\n",
      "Training loss for batch 7768 : 0.25689396262168884\n",
      "Training loss for batch 7769 : 0.09383045136928558\n",
      "Training loss for batch 7770 : 0.21579869091510773\n",
      "Training loss for batch 7771 : 0.12092690169811249\n",
      "Training loss for batch 7772 : 0.17207691073417664\n",
      "Training loss for batch 7773 : 0.07318052649497986\n",
      "Training loss for batch 7774 : 0.2610590159893036\n",
      "Training loss for batch 7775 : 0.19859753549098969\n",
      "Training loss for batch 7776 : 0.17984776198863983\n",
      "Training loss for batch 7777 : 0.06938674300909042\n",
      "Training loss for batch 7778 : 0.13903960585594177\n",
      "Training loss for batch 7779 : 0.10783383250236511\n",
      "Training loss for batch 7780 : 0.11640306562185287\n",
      "Training loss for batch 7781 : 0.2389112263917923\n",
      "Training loss for batch 7782 : 0.1078491359949112\n",
      "Training loss for batch 7783 : 0.10807102918624878\n",
      "Training loss for batch 7784 : 0.304997056722641\n",
      "Training loss for batch 7785 : 0.24978601932525635\n",
      "Training loss for batch 7786 : 0.26798292994499207\n",
      "Training loss for batch 7787 : 0.3669395446777344\n",
      "Training loss for batch 7788 : 0.10270538926124573\n",
      "Training loss for batch 7789 : 0.09508974105119705\n",
      "Training loss for batch 7790 : 0.27830609679222107\n",
      "Training loss for batch 7791 : 0.08515208959579468\n",
      "Training loss for batch 7792 : 0.20781061053276062\n",
      "Training loss for batch 7793 : 0.09485890716314316\n",
      "Training loss for batch 7794 : 0.15927980840206146\n",
      "Training loss for batch 7795 : 0.014179706573486328\n",
      "Training loss for batch 7796 : 0.13183711469173431\n",
      "Training loss for batch 7797 : 0.11762572079896927\n",
      "Training loss for batch 7798 : 0.1041894406080246\n",
      "Training loss for batch 7799 : 0.35123756527900696\n",
      "Training loss for batch 7800 : 0.10141824185848236\n",
      "Training loss for batch 7801 : 0.17952637374401093\n",
      "Training loss for batch 7802 : 0.20237328112125397\n",
      "Training loss for batch 7803 : 0.34429603815078735\n",
      "Training loss for batch 7804 : 0.2929501533508301\n",
      "Training loss for batch 7805 : 0.027524566277861595\n",
      "Training loss for batch 7806 : 0.1271943598985672\n",
      "Training loss for batch 7807 : -0.0005762258660979569\n",
      "Training loss for batch 7808 : 0.03697232902050018\n",
      "Training loss for batch 7809 : 0.38602006435394287\n",
      "Training loss for batch 7810 : 0.04258042573928833\n",
      "Training loss for batch 7811 : 0.08099877834320068\n",
      "Training loss for batch 7812 : 0.06876672804355621\n",
      "Training loss for batch 7813 : 0.04337455704808235\n",
      "Training loss for batch 7814 : 0.15116150677204132\n",
      "Training loss for batch 7815 : 0.06465467810630798\n",
      "Training loss for batch 7816 : 0.14513643085956573\n",
      "Training loss for batch 7817 : 0.08145186305046082\n",
      "Training loss for batch 7818 : 0.07940572500228882\n",
      "Training loss for batch 7819 : 0.18509387969970703\n",
      "Training loss for batch 7820 : 0.047308240085840225\n",
      "Training loss for batch 7821 : 0.3303888142108917\n",
      "Training loss for batch 7822 : 0.17394357919692993\n",
      "Training loss for batch 7823 : 0.040363360196352005\n",
      "Training loss for batch 7824 : 0.21516823768615723\n",
      "Training loss for batch 7825 : 0.19942079484462738\n",
      "Training loss for batch 7826 : 0.03344166278839111\n",
      "Training loss for batch 7827 : 0.2413213849067688\n",
      "Training loss for batch 7828 : 0.0018504639156162739\n",
      "Training loss for batch 7829 : 0.044603072106838226\n",
      "Training loss for batch 7830 : 0.12206646054983139\n",
      "Training loss for batch 7831 : 0.03947911784052849\n",
      "Training loss for batch 7832 : 0.2875669300556183\n",
      "Training loss for batch 7833 : 0.31376516819000244\n",
      "Training loss for batch 7834 : 0.3375101685523987\n",
      "Training loss for batch 7835 : 0.09608078002929688\n",
      "Training loss for batch 7836 : 0.10171650350093842\n",
      "Training loss for batch 7837 : 0.2525348663330078\n",
      "Training loss for batch 7838 : 0.08956238627433777\n",
      "Training loss for batch 7839 : 0.09916464984416962\n",
      "Training loss for batch 7840 : 0.2564030587673187\n",
      "Training loss for batch 7841 : 0.3789832293987274\n",
      "Training loss for batch 7842 : 0.06166374683380127\n",
      "Training loss for batch 7843 : 0.1297551542520523\n",
      "Training loss for batch 7844 : 0.06847264617681503\n",
      "Training loss for batch 7845 : 0.24002781510353088\n",
      "Training loss for batch 7846 : 0.22878926992416382\n",
      "Training loss for batch 7847 : 0.2553921937942505\n",
      "Training loss for batch 7848 : 0.3593158423900604\n",
      "Training loss for batch 7849 : 0.12179560214281082\n",
      "Training loss for batch 7850 : 0.005271649919450283\n",
      "Training loss for batch 7851 : 0.037600740790367126\n",
      "Training loss for batch 7852 : 0.029515892267227173\n",
      "Training loss for batch 7853 : 0.1299303025007248\n",
      "Training loss for batch 7854 : 0.1634223908185959\n",
      "Training loss for batch 7855 : 0.1933826357126236\n",
      "Training loss for batch 7856 : 0.062098316848278046\n",
      "Training loss for batch 7857 : 0.03858160972595215\n",
      "Training loss for batch 7858 : 0.05947878211736679\n",
      "Training loss for batch 7859 : 0.14217858016490936\n",
      "Training loss for batch 7860 : 0.23345008492469788\n",
      "Training loss for batch 7861 : 0.043183065950870514\n",
      "Training loss for batch 7862 : 0.13264985382556915\n",
      "Training loss for batch 7863 : 0.015403291210532188\n",
      "Training loss for batch 7864 : 0.21324798464775085\n",
      "Training loss for batch 7865 : 0.06170859932899475\n",
      "Training loss for batch 7866 : 0.019976526498794556\n",
      "Training loss for batch 7867 : 0.06053979694843292\n",
      "Training loss for batch 7868 : 0.005632566288113594\n",
      "Training loss for batch 7869 : 0.3065991699695587\n",
      "Training loss for batch 7870 : 0.2763015031814575\n",
      "Training loss for batch 7871 : 0.05146736279129982\n",
      "Training loss for batch 7872 : 0.11993421614170074\n",
      "Training loss for batch 7873 : 0.3050422966480255\n",
      "Training loss for batch 7874 : 0.1979665756225586\n",
      "Training loss for batch 7875 : 0.11001786589622498\n",
      "Training loss for batch 7876 : 0.010000774636864662\n",
      "Training loss for batch 7877 : 0.16064593195915222\n",
      "Training loss for batch 7878 : 0.11722426861524582\n",
      "Training loss for batch 7879 : 0.2168603390455246\n",
      "Training loss for batch 7880 : 0.1249680146574974\n",
      "Training loss for batch 7881 : 0.13896936178207397\n",
      "Training loss for batch 7882 : 0.1650272011756897\n",
      "Training loss for batch 7883 : 0.10317739844322205\n",
      "Training loss for batch 7884 : 0.15154120326042175\n",
      "Training loss for batch 7885 : 0.0372261106967926\n",
      "Training loss for batch 7886 : 0.14644357562065125\n",
      "Training loss for batch 7887 : 0.1171918585896492\n",
      "Training loss for batch 7888 : 0.258472740650177\n",
      "Training loss for batch 7889 : 0.1880146861076355\n",
      "Training loss for batch 7890 : 0.34137091040611267\n",
      "Training loss for batch 7891 : 0.2534237205982208\n",
      "Training loss for batch 7892 : 0.28757789731025696\n",
      "Training loss for batch 7893 : 0.04832177609205246\n",
      "Training loss for batch 7894 : 0.2991032600402832\n",
      "Training loss for batch 7895 : 0.46246013045310974\n",
      "Training loss for batch 7896 : 0.019617494195699692\n",
      "Training loss for batch 7897 : 0.1123938038945198\n",
      "Training loss for batch 7898 : 0.31342077255249023\n",
      "Training loss for batch 7899 : 0.1462239921092987\n",
      "Training loss for batch 7900 : 0.007548665627837181\n",
      "Training loss for batch 7901 : 0.08206988126039505\n",
      "Training loss for batch 7902 : 0.3715701997280121\n",
      "Training loss for batch 7903 : 0.14819586277008057\n",
      "Training loss for batch 7904 : 0.0\n",
      "Training loss for batch 7905 : 0.14003877341747284\n",
      "Training loss for batch 7906 : 0.05711480975151062\n",
      "Training loss for batch 7907 : 0.07507019490003586\n",
      "Training loss for batch 7908 : 0.1639418751001358\n",
      "Training loss for batch 7909 : 0.007265579421073198\n",
      "Training loss for batch 7910 : 0.04224054515361786\n",
      "Training loss for batch 7911 : 0.02529320865869522\n",
      "Training loss for batch 7912 : 0.05789501965045929\n",
      "Training loss for batch 7913 : 0.13227489590644836\n",
      "Training loss for batch 7914 : 0.1143941730260849\n",
      "Training loss for batch 7915 : 0.23782625794410706\n",
      "Training loss for batch 7916 : 0.2419058233499527\n",
      "Training loss for batch 7917 : 0.057556331157684326\n",
      "Training loss for batch 7918 : 0.09769925475120544\n",
      "Training loss for batch 7919 : 0.021099427714943886\n",
      "Training loss for batch 7920 : 0.25126537680625916\n",
      "Training loss for batch 7921 : 0.0\n",
      "Training loss for batch 7922 : 0.1589507907629013\n",
      "Training loss for batch 7923 : 0.07667311280965805\n",
      "Training loss for batch 7924 : 0.041454046964645386\n",
      "Training loss for batch 7925 : 0.2723386585712433\n",
      "Training loss for batch 7926 : 0.1125604659318924\n",
      "Training loss for batch 7927 : 0.05855999141931534\n",
      "Training loss for batch 7928 : 0.3231784403324127\n",
      "Training loss for batch 7929 : 0.20844528079032898\n",
      "Training loss for batch 7930 : 0.23820361495018005\n",
      "Training loss for batch 7931 : 0.11072544753551483\n",
      "Training loss for batch 7932 : 0.0013224780559539795\n",
      "Training loss for batch 7933 : 0.04732219874858856\n",
      "Training loss for batch 7934 : 0.0122903510928154\n",
      "Training loss for batch 7935 : 0.11276338249444962\n",
      "Training loss for batch 7936 : 0.18993937969207764\n",
      "Training loss for batch 7937 : 0.08421884477138519\n",
      "Training loss for batch 7938 : 0.2275828719139099\n",
      "Training loss for batch 7939 : 0.03192262351512909\n",
      "Training loss for batch 7940 : 0.019196342676877975\n",
      "Training loss for batch 7941 : 0.3772330582141876\n",
      "Training loss for batch 7942 : 0.016584772616624832\n",
      "Training loss for batch 7943 : 0.10355940461158752\n",
      "Training loss for batch 7944 : 0.06873062998056412\n",
      "Training loss for batch 7945 : 0.10055285692214966\n",
      "Training loss for batch 7946 : 0.028983648866415024\n",
      "Training loss for batch 7947 : 0.03920755907893181\n",
      "Training loss for batch 7948 : -0.0005929106846451759\n",
      "Training loss for batch 7949 : 0.007682709954679012\n",
      "Training loss for batch 7950 : 0.049992695450782776\n",
      "Training loss for batch 7951 : 0.254982590675354\n",
      "Training loss for batch 7952 : 0.08117057383060455\n",
      "Training loss for batch 7953 : 0.127433180809021\n",
      "Training loss for batch 7954 : 0.15526241064071655\n",
      "Training loss for batch 7955 : 0.3507552444934845\n",
      "Training loss for batch 7956 : 0.02873498946428299\n",
      "Training loss for batch 7957 : 0.1235591471195221\n",
      "Training loss for batch 7958 : 0.18595784902572632\n",
      "Training loss for batch 7959 : 0.1085488349199295\n",
      "Training loss for batch 7960 : 0.03952164202928543\n",
      "Training loss for batch 7961 : 0.26185160875320435\n",
      "Training loss for batch 7962 : 0.0011661621974781156\n",
      "Training loss for batch 7963 : 0.05910852923989296\n",
      "Training loss for batch 7964 : 0.07579414546489716\n",
      "Training loss for batch 7965 : 0.023252084851264954\n",
      "Training loss for batch 7966 : 0.6346439719200134\n",
      "Training loss for batch 7967 : 0.07542780786752701\n",
      "Training loss for batch 7968 : 0.19976122677326202\n",
      "Training loss for batch 7969 : 0.05429829657077789\n",
      "Training loss for batch 7970 : 0.053479380905628204\n",
      "Training loss for batch 7971 : 0.3060188889503479\n",
      "Training loss for batch 7972 : 0.18135955929756165\n",
      "Training loss for batch 7973 : 0.03664039447903633\n",
      "Training loss for batch 7974 : 0.020465368404984474\n",
      "Training loss for batch 7975 : 0.6303617358207703\n",
      "Training loss for batch 7976 : 0.31104010343551636\n",
      "Training loss for batch 7977 : 0.26530835032463074\n",
      "Training loss for batch 7978 : 0.1906084418296814\n",
      "Training loss for batch 7979 : 0.04943583160638809\n",
      "Training loss for batch 7980 : 0.10275624692440033\n",
      "Training loss for batch 7981 : 0.056291110813617706\n",
      "Training loss for batch 7982 : 0.13736078143119812\n",
      "Training loss for batch 7983 : 0.1587761491537094\n",
      "Training loss for batch 7984 : 0.24990326166152954\n",
      "Training loss for batch 7985 : 0.2086954116821289\n",
      "Training loss for batch 7986 : 0.29213058948516846\n",
      "Training loss for batch 7987 : 0.1573467254638672\n",
      "Training loss for batch 7988 : 0.12234041094779968\n",
      "Training loss for batch 7989 : 0.31966277956962585\n",
      "Training loss for batch 7990 : 0.21003276109695435\n",
      "Training loss for batch 7991 : 0.38721686601638794\n",
      "Training loss for batch 7992 : 0.040094442665576935\n",
      "Training loss for batch 7993 : 0.20635342597961426\n",
      "Training loss for batch 7994 : 0.13854730129241943\n",
      "Training loss for batch 7995 : 0.017870740965008736\n",
      "Training loss for batch 7996 : 0.05611908435821533\n",
      "Training loss for batch 7997 : 0.1250530481338501\n",
      "Training loss for batch 7998 : 0.2763979136943817\n",
      "Training loss for batch 7999 : 0.059914082288742065\n",
      "Training loss for batch 8000 : 0.08639451861381531\n",
      "Training loss for batch 8001 : 0.1902773231267929\n",
      "Training loss for batch 8002 : 0.11977214366197586\n",
      "Training loss for batch 8003 : 0.7322407364845276\n",
      "Training loss for batch 8004 : 0.14902466535568237\n",
      "Training loss for batch 8005 : 0.4080848693847656\n",
      "Training loss for batch 8006 : 0.06970728933811188\n",
      "Training loss for batch 8007 : 0.188958540558815\n",
      "Training loss for batch 8008 : 0.31132712960243225\n",
      "Training loss for batch 8009 : 0.20819318294525146\n",
      "Training loss for batch 8010 : 0.19215962290763855\n",
      "Training loss for batch 8011 : 0.08233748376369476\n",
      "Training loss for batch 8012 : 0.37838149070739746\n",
      "Training loss for batch 8013 : 0.06515395641326904\n",
      "Training loss for batch 8014 : 0.33271312713623047\n",
      "Training loss for batch 8015 : 0.0\n",
      "Training loss for batch 8016 : 0.00561744999140501\n",
      "Training loss for batch 8017 : 0.32350414991378784\n",
      "Training loss for batch 8018 : 0.1821724772453308\n",
      "Training loss for batch 8019 : 0.20783373713493347\n",
      "Training loss for batch 8020 : 0.0\n",
      "Training loss for batch 8021 : 0.28216665983200073\n",
      "Training loss for batch 8022 : 0.08739412575960159\n",
      "Training loss for batch 8023 : 0.10688462853431702\n",
      "Training loss for batch 8024 : 0.24892772734165192\n",
      "Training loss for batch 8025 : 0.20991069078445435\n",
      "Training loss for batch 8026 : 0.45074331760406494\n",
      "Training loss for batch 8027 : 0.18276163935661316\n",
      "Training loss for batch 8028 : 0.025817926973104477\n",
      "Training loss for batch 8029 : 0.13892611861228943\n",
      "Training loss for batch 8030 : 0.0035002781078219414\n",
      "Training loss for batch 8031 : 0.04410349577665329\n",
      "Training loss for batch 8032 : 0.04146869480609894\n",
      "Training loss for batch 8033 : 0.12101606279611588\n",
      "Training loss for batch 8034 : 0.1482294797897339\n",
      "Training loss for batch 8035 : 0.5694379210472107\n",
      "Training loss for batch 8036 : 0.11326020210981369\n",
      "Training loss for batch 8037 : 0.06718607246875763\n",
      "Training loss for batch 8038 : 0.45345136523246765\n",
      "Training loss for batch 8039 : -0.0004917773185297847\n",
      "Training loss for batch 8040 : 0.1685430258512497\n",
      "Training loss for batch 8041 : 0.12496308982372284\n",
      "Training loss for batch 8042 : 0.18639609217643738\n",
      "Training loss for batch 8043 : 0.10641257464885712\n",
      "Training loss for batch 8044 : 0.2569437325000763\n",
      "Training loss for batch 8045 : 0.01705094240605831\n",
      "Training loss for batch 8046 : 0.06635326892137527\n",
      "Training loss for batch 8047 : 0.1821073740720749\n",
      "Training loss for batch 8048 : 0.13326099514961243\n",
      "Training loss for batch 8049 : 0.3680095076560974\n",
      "Training loss for batch 8050 : 0.077427938580513\n",
      "Training loss for batch 8051 : 0.1365116983652115\n",
      "Training loss for batch 8052 : 0.20461754500865936\n",
      "Training loss for batch 8053 : 0.16118347644805908\n",
      "Training loss for batch 8054 : 0.22785046696662903\n",
      "Training loss for batch 8055 : 0.23336666822433472\n",
      "Training loss for batch 8056 : 0.1403607875108719\n",
      "Training loss for batch 8057 : 0.03592549264431\n",
      "Training loss for batch 8058 : 0.03550401329994202\n",
      "Training loss for batch 8059 : 0.3316430151462555\n",
      "Training loss for batch 8060 : 0.03416593745350838\n",
      "Training loss for batch 8061 : 0.20707586407661438\n",
      "Training loss for batch 8062 : 0.6690877079963684\n",
      "Training loss for batch 8063 : 0.06012127548456192\n",
      "Training loss for batch 8064 : 0.2564964294433594\n",
      "Training loss for batch 8065 : 0.2472035139799118\n",
      "Training loss for batch 8066 : 0.2544926106929779\n",
      "Training loss for batch 8067 : 0.07129789143800735\n",
      "Training loss for batch 8068 : 0.09069645404815674\n",
      "Training loss for batch 8069 : 0.18140901625156403\n",
      "Training loss for batch 8070 : 0.1333947479724884\n",
      "Training loss for batch 8071 : 0.07014758884906769\n",
      "Training loss for batch 8072 : 0.3554663062095642\n",
      "Training loss for batch 8073 : 0.09177035093307495\n",
      "Training loss for batch 8074 : 0.13198189437389374\n",
      "Training loss for batch 8075 : 0.003336529014632106\n",
      "Training loss for batch 8076 : 0.14455226063728333\n",
      "Training loss for batch 8077 : 0.4139062464237213\n",
      "Training loss for batch 8078 : 0.13264413177967072\n",
      "Training loss for batch 8079 : 0.25727832317352295\n",
      "Training loss for batch 8080 : 0.05373146012425423\n",
      "Training loss for batch 8081 : 0.11680258065462112\n",
      "Training loss for batch 8082 : 0.030432159081101418\n",
      "Training loss for batch 8083 : 0.18370051681995392\n",
      "Training loss for batch 8084 : 0.14247316122055054\n",
      "Training loss for batch 8085 : 0.0\n",
      "Training loss for batch 8086 : 0.027180388569831848\n",
      "Training loss for batch 8087 : 0.08738123625516891\n",
      "Training loss for batch 8088 : 0.09978019446134567\n",
      "Training loss for batch 8089 : 0.1969929188489914\n",
      "Training loss for batch 8090 : 0.0505792535841465\n",
      "Training loss for batch 8091 : 0.023101311177015305\n",
      "Training loss for batch 8092 : 0.05211818590760231\n",
      "Training loss for batch 8093 : 0.19914525747299194\n",
      "Training loss for batch 8094 : 0.15328338742256165\n",
      "Training loss for batch 8095 : 0.3089093863964081\n",
      "Training loss for batch 8096 : 0.05206014961004257\n",
      "Training loss for batch 8097 : 0.029739288613200188\n",
      "Training loss for batch 8098 : 0.0409042052924633\n",
      "Training loss for batch 8099 : 0.05970582365989685\n",
      "Training loss for batch 8100 : 0.048507533967494965\n",
      "Training loss for batch 8101 : 0.06293326616287231\n",
      "Training loss for batch 8102 : 0.23219913244247437\n",
      "Training loss for batch 8103 : 0.1932273507118225\n",
      "Training loss for batch 8104 : 0.08664266020059586\n",
      "Training loss for batch 8105 : 0.13431914150714874\n",
      "Training loss for batch 8106 : 0.0673958957195282\n",
      "Training loss for batch 8107 : 0.05056419223546982\n",
      "Training loss for batch 8108 : 0.27283450961112976\n",
      "Training loss for batch 8109 : 0.20994946360588074\n",
      "Training loss for batch 8110 : 0.10835105180740356\n",
      "Training loss for batch 8111 : 0.02256043814122677\n",
      "Training loss for batch 8112 : 0.04824582487344742\n",
      "Training loss for batch 8113 : 0.0649120882153511\n",
      "Training loss for batch 8114 : 0.09279187768697739\n",
      "Training loss for batch 8115 : 0.06865455955266953\n",
      "Training loss for batch 8116 : 0.250129371881485\n",
      "Training loss for batch 8117 : 0.2371811717748642\n",
      "Training loss for batch 8118 : 0.05482609570026398\n",
      "Training loss for batch 8119 : 0.013450957834720612\n",
      "Training loss for batch 8120 : 0.1069336012005806\n",
      "Training loss for batch 8121 : 0.09593870490789413\n",
      "Training loss for batch 8122 : 0.07878728955984116\n",
      "Training loss for batch 8123 : 0.0\n",
      "Training loss for batch 8124 : 0.15326867997646332\n",
      "Training loss for batch 8125 : 0.10610133409500122\n",
      "Training loss for batch 8126 : 0.16366316378116608\n",
      "Training loss for batch 8127 : 0.000876694917678833\n",
      "Training loss for batch 8128 : 0.0693788006901741\n",
      "Training loss for batch 8129 : 0.0921933501958847\n",
      "Training loss for batch 8130 : 0.29551947116851807\n",
      "Training loss for batch 8131 : 0.07753870636224747\n",
      "Training loss for batch 8132 : 0.2919138967990875\n",
      "Training loss for batch 8133 : 0.18892166018486023\n",
      "Training loss for batch 8134 : 0.16511857509613037\n",
      "Training loss for batch 8135 : 0.0021066467743366957\n",
      "Training loss for batch 8136 : 0.15298163890838623\n",
      "Training loss for batch 8137 : 0.019402790814638138\n",
      "Training loss for batch 8138 : 0.02006388269364834\n",
      "Training loss for batch 8139 : 0.29743966460227966\n",
      "Training loss for batch 8140 : 0.049441516399383545\n",
      "Training loss for batch 8141 : 0.060527361929416656\n",
      "Training loss for batch 8142 : 0.2168729603290558\n",
      "Training loss for batch 8143 : 0.20468974113464355\n",
      "Training loss for batch 8144 : 0.04449966177344322\n",
      "Training loss for batch 8145 : 0.2031642496585846\n",
      "Training loss for batch 8146 : 0.00908185075968504\n",
      "Training loss for batch 8147 : 0.2594415545463562\n",
      "Training loss for batch 8148 : 0.21220839023590088\n",
      "Training loss for batch 8149 : 0.24286006391048431\n",
      "Training loss for batch 8150 : 0.3814027011394501\n",
      "Training loss for batch 8151 : 0.21303829550743103\n",
      "Training loss for batch 8152 : 0.07405166327953339\n",
      "Training loss for batch 8153 : 0.16805118322372437\n",
      "Training loss for batch 8154 : 0.07540412247180939\n",
      "Training loss for batch 8155 : 0.13693147897720337\n",
      "Training loss for batch 8156 : 0.18099847435951233\n",
      "Training loss for batch 8157 : 0.2051040232181549\n",
      "Training loss for batch 8158 : 0.13582542538642883\n",
      "Training loss for batch 8159 : 0.06580477952957153\n",
      "Training loss for batch 8160 : 0.18228600919246674\n",
      "Training loss for batch 8161 : 0.1362873613834381\n",
      "Training loss for batch 8162 : 0.23938123881816864\n",
      "Training loss for batch 8163 : 0.10864384472370148\n",
      "Training loss for batch 8164 : 0.09546609222888947\n",
      "Training loss for batch 8165 : 0.08360973000526428\n",
      "Training loss for batch 8166 : 0.08900914341211319\n",
      "Training loss for batch 8167 : 0.07425447553396225\n",
      "Training loss for batch 8168 : 0.007596453186124563\n",
      "Training loss for batch 8169 : 0.1594526320695877\n",
      "Training loss for batch 8170 : 0.11034677922725677\n",
      "Training loss for batch 8171 : 0.18222495913505554\n",
      "Training loss for batch 8172 : 0.0050485143437981606\n",
      "Training loss for batch 8173 : 0.0570998415350914\n",
      "Training loss for batch 8174 : 0.24943134188652039\n",
      "Training loss for batch 8175 : 0.13636112213134766\n",
      "Training loss for batch 8176 : 0.19506192207336426\n",
      "Training loss for batch 8177 : 0.18116122484207153\n",
      "Training loss for batch 8178 : 0.03905397281050682\n",
      "Training loss for batch 8179 : 0.2789543867111206\n",
      "Training loss for batch 8180 : 0.1901017725467682\n",
      "Training loss for batch 8181 : 0.18406735360622406\n",
      "Training loss for batch 8182 : 0.15459971129894257\n",
      "Training loss for batch 8183 : 0.2533043622970581\n",
      "Training loss for batch 8184 : 0.08757828921079636\n",
      "Training loss for batch 8185 : 0.09600528329610825\n",
      "Training loss for batch 8186 : 0.007916366681456566\n",
      "Training loss for batch 8187 : 0.0574093833565712\n",
      "Training loss for batch 8188 : 0.27188873291015625\n",
      "Training loss for batch 8189 : 0.09065251797437668\n",
      "Training loss for batch 8190 : 0.15505743026733398\n",
      "Training loss for batch 8191 : 0.2446146160364151\n",
      "Training loss for batch 8192 : 0.18038016557693481\n",
      "Training loss for batch 8193 : 0.12580493092536926\n",
      "Training loss for batch 8194 : 0.31455886363983154\n",
      "Training loss for batch 8195 : 0.11494430899620056\n",
      "Training loss for batch 8196 : 0.09408065676689148\n",
      "Training loss for batch 8197 : 0.059461548924446106\n",
      "Training loss for batch 8198 : 0.03542691469192505\n",
      "Training loss for batch 8199 : 0.30029261112213135\n",
      "Training loss for batch 8200 : 0.05958624929189682\n",
      "Training loss for batch 8201 : 0.35587021708488464\n",
      "Training loss for batch 8202 : 0.1538047045469284\n",
      "Training loss for batch 8203 : 0.2831794023513794\n",
      "Training loss for batch 8204 : 0.18092215061187744\n",
      "Training loss for batch 8205 : 0.2548987567424774\n",
      "Training loss for batch 8206 : 0.07590018957853317\n",
      "Training loss for batch 8207 : 0.1529291868209839\n",
      "Training loss for batch 8208 : 0.1981150060892105\n",
      "Training loss for batch 8209 : 0.17123709619045258\n",
      "Training loss for batch 8210 : 0.2834741771221161\n",
      "Training loss for batch 8211 : 0.21622444689273834\n",
      "Training loss for batch 8212 : 0.14119763672351837\n",
      "Training loss for batch 8213 : 0.17250245809555054\n",
      "Training loss for batch 8214 : 0.2891978323459625\n",
      "Training loss for batch 8215 : 0.4023832380771637\n",
      "Training loss for batch 8216 : 0.2613424062728882\n",
      "Training loss for batch 8217 : 0.26482924818992615\n",
      "Training loss for batch 8218 : 0.22241321206092834\n",
      "Training loss for batch 8219 : -0.0008539746049791574\n",
      "Training loss for batch 8220 : 0.015088061802089214\n",
      "Training loss for batch 8221 : 0.16901442408561707\n",
      "Training loss for batch 8222 : 0.17760896682739258\n",
      "Training loss for batch 8223 : 0.1261565089225769\n",
      "Training loss for batch 8224 : 0.02763495407998562\n",
      "Training loss for batch 8225 : 0.06301935762166977\n",
      "Training loss for batch 8226 : 0.12088184803724289\n",
      "Training loss for batch 8227 : 0.12422503530979156\n",
      "Training loss for batch 8228 : 0.1590736210346222\n",
      "Training loss for batch 8229 : 0.09509863704442978\n",
      "Training loss for batch 8230 : 0.16220955550670624\n",
      "Training loss for batch 8231 : 0.12756335735321045\n",
      "Training loss for batch 8232 : 0.2221047282218933\n",
      "Training loss for batch 8233 : 0.1459684818983078\n",
      "Training loss for batch 8234 : 0.1218380555510521\n",
      "Training loss for batch 8235 : 0.03629211708903313\n",
      "Training loss for batch 8236 : 0.06061894819140434\n",
      "Training loss for batch 8237 : 0.007010745815932751\n",
      "Training loss for batch 8238 : 0.06721623241901398\n",
      "Training loss for batch 8239 : 0.3058125972747803\n",
      "Training loss for batch 8240 : 0.20264285802841187\n",
      "Training loss for batch 8241 : 0.16618120670318604\n",
      "Training loss for batch 8242 : 0.03200361132621765\n",
      "Training loss for batch 8243 : 0.2033483237028122\n",
      "Training loss for batch 8244 : 0.07058686763048172\n",
      "Training loss for batch 8245 : 0.01999523490667343\n",
      "Training loss for batch 8246 : 0.14496701955795288\n",
      "Training loss for batch 8247 : 0.10511072725057602\n",
      "Training loss for batch 8248 : 0.2926221489906311\n",
      "Training loss for batch 8249 : 0.4154883921146393\n",
      "Training loss for batch 8250 : 0.23307880759239197\n",
      "Training loss for batch 8251 : 0.19808867573738098\n",
      "Training loss for batch 8252 : 0.3374253511428833\n",
      "Training loss for batch 8253 : 0.058111678808927536\n",
      "Training loss for batch 8254 : 0.12466542422771454\n",
      "Training loss for batch 8255 : 0.14508816599845886\n",
      "Training loss for batch 8256 : 0.06604788452386856\n",
      "Training loss for batch 8257 : 0.14373239874839783\n",
      "Training loss for batch 8258 : 0.10944059491157532\n",
      "Training loss for batch 8259 : 0.10058512538671494\n",
      "Training loss for batch 8260 : 0.0961034744977951\n",
      "Training loss for batch 8261 : 0.06642114371061325\n",
      "Training loss for batch 8262 : 0.020507052540779114\n",
      "Training loss for batch 8263 : 0.11681367456912994\n",
      "Training loss for batch 8264 : 0.02162071317434311\n",
      "Training loss for batch 8265 : 0.186767578125\n",
      "Training loss for batch 8266 : 0.1740724742412567\n",
      "Training loss for batch 8267 : 0.4643297493457794\n",
      "Training loss for batch 8268 : 0.2312050610780716\n",
      "Training loss for batch 8269 : 0.005737177561968565\n",
      "Training loss for batch 8270 : 0.023263495415449142\n",
      "Training loss for batch 8271 : 0.13318271934986115\n",
      "Training loss for batch 8272 : 0.017598770558834076\n",
      "Training loss for batch 8273 : 0.5182554125785828\n",
      "Training loss for batch 8274 : 0.1046423465013504\n",
      "Training loss for batch 8275 : 0.0899987444281578\n",
      "Training loss for batch 8276 : 0.3385286033153534\n",
      "Training loss for batch 8277 : 0.1511339396238327\n",
      "Training loss for batch 8278 : 0.04824887588620186\n",
      "Training loss for batch 8279 : 0.34923577308654785\n",
      "Training loss for batch 8280 : 0.14449332654476166\n",
      "Training loss for batch 8281 : 0.13726018369197845\n",
      "Training loss for batch 8282 : 0.26683521270751953\n",
      "Training loss for batch 8283 : 0.0\n",
      "Training loss for batch 8284 : -0.0015217632753774524\n",
      "Training loss for batch 8285 : 0.3031265139579773\n",
      "Training loss for batch 8286 : 0.14754709601402283\n",
      "Training loss for batch 8287 : 0.052934467792510986\n",
      "Training loss for batch 8288 : 0.5819421410560608\n",
      "Training loss for batch 8289 : 0.09221487492322922\n",
      "Training loss for batch 8290 : 0.09744808077812195\n",
      "Training loss for batch 8291 : 0.08707408607006073\n",
      "Training loss for batch 8292 : 0.3104746341705322\n",
      "Training loss for batch 8293 : 0.22450904548168182\n",
      "Training loss for batch 8294 : 0.11405523121356964\n",
      "Training loss for batch 8295 : 0.3203704357147217\n",
      "Training loss for batch 8296 : 0.0\n",
      "Training loss for batch 8297 : 0.08358713984489441\n",
      "Training loss for batch 8298 : 0.009146511554718018\n",
      "Training loss for batch 8299 : 0.4374753534793854\n",
      "Training loss for batch 8300 : 0.06140250340104103\n",
      "Training loss for batch 8301 : 0.06563930958509445\n",
      "Training loss for batch 8302 : 0.06881973147392273\n",
      "Training loss for batch 8303 : 0.40190616250038147\n",
      "Training loss for batch 8304 : 0.030825087800621986\n",
      "Training loss for batch 8305 : 0.07576333731412888\n",
      "Training loss for batch 8306 : 0.24896705150604248\n",
      "Training loss for batch 8307 : 0.7994557023048401\n",
      "Parameter containing:\n",
      "tensor(-0.2006, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 0.7994418740272522\n",
      "Training loss for batch 1 : 0.799423098564148\n",
      "Training loss for batch 2 : 0.7994001507759094\n",
      "Training loss for batch 3 : 0.7993736863136292\n",
      "Training loss for batch 4 : 0.7993441224098206\n",
      "Training loss for batch 5 : 0.7993119359016418\n",
      "Training loss for batch 6 : 0.7992774248123169\n",
      "Training loss for batch 7 : 0.7992411851882935\n",
      "Training loss for batch 8 : 0.7992032766342163\n",
      "Training loss for batch 9 : 0.7991640567779541\n",
      "Training loss for batch 10 : 0.7991236448287964\n",
      "Training loss for batch 11 : 0.7990823984146118\n",
      "Training loss for batch 12 : 0.0610007606446743\n",
      "Training loss for batch 13 : 0.09098436683416367\n",
      "Training loss for batch 14 : 0.3355341851711273\n",
      "Training loss for batch 15 : 0.03765186667442322\n",
      "Training loss for batch 16 : 0.06617478281259537\n",
      "Training loss for batch 17 : 0.23855364322662354\n",
      "Training loss for batch 18 : 0.052948497235774994\n",
      "Training loss for batch 19 : 0.02264186181128025\n",
      "Training loss for batch 20 : 0.1066940501332283\n",
      "Training loss for batch 21 : 0.13164663314819336\n",
      "Training loss for batch 22 : 0.1281697303056717\n",
      "Training loss for batch 23 : 0.0389217808842659\n",
      "Training loss for batch 24 : 0.060016926378011703\n",
      "Training loss for batch 25 : 0.3253262937068939\n",
      "Training loss for batch 26 : 0.2664496898651123\n",
      "Training loss for batch 27 : 0.11839708685874939\n",
      "Training loss for batch 28 : 0.16983665525913239\n",
      "Training loss for batch 29 : 0.31741663813591003\n",
      "Training loss for batch 30 : 0.26380401849746704\n",
      "Training loss for batch 31 : 0.08837354183197021\n",
      "Training loss for batch 32 : 0.05558670684695244\n",
      "Training loss for batch 33 : 0.10316380858421326\n",
      "Training loss for batch 34 : 0.06658177822828293\n",
      "Training loss for batch 35 : 0.03208049014210701\n",
      "Training loss for batch 36 : 0.1271769404411316\n",
      "Training loss for batch 37 : 0.019113799557089806\n",
      "Training loss for batch 38 : 0.1350630670785904\n",
      "Training loss for batch 39 : 0.025028295814990997\n",
      "Training loss for batch 40 : 0.09120874106884003\n",
      "Training loss for batch 41 : 0.11601225286722183\n",
      "Training loss for batch 42 : 0.1079355776309967\n",
      "Training loss for batch 43 : -0.0017329825786873698\n",
      "Training loss for batch 44 : 0.0\n",
      "Training loss for batch 45 : 0.08020102977752686\n",
      "Training loss for batch 46 : 0.14230528473854065\n",
      "Training loss for batch 47 : 0.2097681313753128\n",
      "Training loss for batch 48 : 0.05841439217329025\n",
      "Training loss for batch 49 : 0.18540839850902557\n",
      "Training loss for batch 50 : 0.2746198773384094\n",
      "Training loss for batch 51 : 0.18973854184150696\n",
      "Training loss for batch 52 : 0.0\n",
      "Training loss for batch 53 : 0.08288244158029556\n",
      "Training loss for batch 54 : 0.05473507195711136\n",
      "Training loss for batch 55 : 0.08532636612653732\n",
      "Training loss for batch 56 : 0.15453755855560303\n",
      "Training loss for batch 57 : 0.11111794412136078\n",
      "Training loss for batch 58 : -0.002087351167574525\n",
      "Training loss for batch 59 : 0.276818186044693\n",
      "Training loss for batch 60 : 0.1406363844871521\n",
      "Training loss for batch 61 : 0.07755716145038605\n",
      "Training loss for batch 62 : 0.05897223576903343\n",
      "Training loss for batch 63 : 0.02069617435336113\n",
      "Training loss for batch 64 : 0.04127024859189987\n",
      "Training loss for batch 65 : 0.0008512735366821289\n",
      "Training loss for batch 66 : 0.041473060846328735\n",
      "Training loss for batch 67 : 0.07599387317895889\n",
      "Training loss for batch 68 : 0.06619010865688324\n",
      "Training loss for batch 69 : 0.03073502518236637\n",
      "Training loss for batch 70 : 0.09396614134311676\n",
      "Training loss for batch 71 : 0.09178421646356583\n",
      "Training loss for batch 72 : -0.004076254088431597\n",
      "Training loss for batch 73 : 0.22353202104568481\n",
      "Training loss for batch 74 : 0.08974926173686981\n",
      "Training loss for batch 75 : 0.12053968012332916\n",
      "Training loss for batch 76 : 0.05271613970398903\n",
      "Training loss for batch 77 : 0.003187398426234722\n",
      "Training loss for batch 78 : 0.0\n",
      "Training loss for batch 79 : 0.0755038931965828\n",
      "Training loss for batch 80 : 0.02875588834285736\n",
      "Training loss for batch 81 : 0.1430261880159378\n",
      "Training loss for batch 82 : 0.06847464293241501\n",
      "Training loss for batch 83 : 0.21718798577785492\n",
      "Training loss for batch 84 : 0.07344668358564377\n",
      "Training loss for batch 85 : 0.30731868743896484\n",
      "Training loss for batch 86 : 0.02638697624206543\n",
      "Training loss for batch 87 : 0.04789028316736221\n",
      "Training loss for batch 88 : 0.11605819314718246\n",
      "Training loss for batch 89 : 0.09105634689331055\n",
      "Training loss for batch 90 : 0.0387226827442646\n",
      "Training loss for batch 91 : 0.06350670009851456\n",
      "Training loss for batch 92 : 0.151095911860466\n",
      "Training loss for batch 93 : 0.016829228028655052\n",
      "Training loss for batch 94 : 0.012950028292834759\n",
      "Training loss for batch 95 : 0.2270575910806656\n",
      "Training loss for batch 96 : 0.24121351540088654\n",
      "Training loss for batch 97 : 0.0\n",
      "Training loss for batch 98 : 0.09456591308116913\n",
      "Training loss for batch 99 : 0.042910050600767136\n",
      "Training loss for batch 100 : 0.18911661207675934\n",
      "Training loss for batch 101 : 0.013381093740463257\n",
      "Training loss for batch 102 : 0.24927052855491638\n",
      "Training loss for batch 103 : 0.16680243611335754\n",
      "Training loss for batch 104 : 0.01875237189233303\n",
      "Training loss for batch 105 : 0.2176370918750763\n",
      "Training loss for batch 106 : 0.15443943440914154\n",
      "Training loss for batch 107 : 0.18265165388584137\n",
      "Training loss for batch 108 : -0.00027135226991958916\n",
      "Training loss for batch 109 : -0.00017144168668892235\n",
      "Training loss for batch 110 : 0.23665300011634827\n",
      "Training loss for batch 111 : 0.09731566160917282\n",
      "Training loss for batch 112 : 0.18587633967399597\n",
      "Training loss for batch 113 : 0.021258629858493805\n",
      "Training loss for batch 114 : 0.16476160287857056\n",
      "Training loss for batch 115 : 0.4336678981781006\n",
      "Training loss for batch 116 : 0.030828451737761497\n",
      "Training loss for batch 117 : 0.1469925493001938\n",
      "Training loss for batch 118 : 0.1058274656534195\n",
      "Training loss for batch 119 : 0.2176639437675476\n",
      "Training loss for batch 120 : 0.02414761111140251\n",
      "Training loss for batch 121 : 0.083835668861866\n",
      "Training loss for batch 122 : 0.2064359337091446\n",
      "Training loss for batch 123 : 0.06123657524585724\n",
      "Training loss for batch 124 : 0.26749294996261597\n",
      "Training loss for batch 125 : 0.11575871706008911\n",
      "Training loss for batch 126 : 0.0067563895136117935\n",
      "Training loss for batch 127 : 0.19040116667747498\n",
      "Training loss for batch 128 : 0.13093732297420502\n",
      "Training loss for batch 129 : 0.31286102533340454\n",
      "Training loss for batch 130 : 0.00782671757042408\n",
      "Training loss for batch 131 : 0.021358981728553772\n",
      "Training loss for batch 132 : 0.21355761587619781\n",
      "Training loss for batch 133 : 0.09946674853563309\n",
      "Training loss for batch 134 : 0.16689561307430267\n",
      "Training loss for batch 135 : 0.32538723945617676\n",
      "Training loss for batch 136 : 0.1843785047531128\n",
      "Training loss for batch 137 : 0.0413859561085701\n",
      "Training loss for batch 138 : 0.04083714634180069\n",
      "Training loss for batch 139 : 0.02101168967783451\n",
      "Training loss for batch 140 : 0.33286774158477783\n",
      "Training loss for batch 141 : 0.13936685025691986\n",
      "Training loss for batch 142 : 0.09836694598197937\n",
      "Training loss for batch 143 : 0.02289208397269249\n",
      "Training loss for batch 144 : 0.3964793086051941\n",
      "Training loss for batch 145 : 0.32030627131462097\n",
      "Training loss for batch 146 : 0.19694124162197113\n",
      "Training loss for batch 147 : 0.245401069521904\n",
      "Training loss for batch 148 : 0.021925535053014755\n",
      "Training loss for batch 149 : 0.1450888067483902\n",
      "Training loss for batch 150 : 0.005274153780192137\n",
      "Training loss for batch 151 : 0.3415157198905945\n",
      "Training loss for batch 152 : 0.2842620015144348\n",
      "Training loss for batch 153 : 0.0039305295795202255\n",
      "Training loss for batch 154 : 0.06746987253427505\n",
      "Training loss for batch 155 : 0.043620288372039795\n",
      "Training loss for batch 156 : 0.022360436618328094\n",
      "Training loss for batch 157 : 0.15186014771461487\n",
      "Training loss for batch 158 : 0.15011300146579742\n",
      "Training loss for batch 159 : 0.20174923539161682\n",
      "Training loss for batch 160 : 0.05772615969181061\n",
      "Training loss for batch 161 : 0.16009414196014404\n",
      "Training loss for batch 162 : 0.09772326052188873\n",
      "Training loss for batch 163 : 0.031181341037154198\n",
      "Training loss for batch 164 : 0.3676178455352783\n",
      "Training loss for batch 165 : 0.0813104659318924\n",
      "Training loss for batch 166 : 0.2449844926595688\n",
      "Training loss for batch 167 : 0.026440035551786423\n",
      "Training loss for batch 168 : 0.12268534302711487\n",
      "Training loss for batch 169 : 0.2551042437553406\n",
      "Training loss for batch 170 : 0.08068395406007767\n",
      "Training loss for batch 171 : 0.06492765247821808\n",
      "Training loss for batch 172 : 0.1870366483926773\n",
      "Training loss for batch 173 : 0.12451429665088654\n",
      "Training loss for batch 174 : 0.08086590468883514\n",
      "Training loss for batch 175 : 0.1021830290555954\n",
      "Training loss for batch 176 : 0.0\n",
      "Training loss for batch 177 : 0.1249040812253952\n",
      "Training loss for batch 178 : 0.04808194562792778\n",
      "Training loss for batch 179 : 0.16781410574913025\n",
      "Training loss for batch 180 : 0.07123085856437683\n",
      "Training loss for batch 181 : 0.12530601024627686\n",
      "Training loss for batch 182 : 0.07050184160470963\n",
      "Training loss for batch 183 : 0.23525096476078033\n",
      "Training loss for batch 184 : 0.09494888782501221\n",
      "Training loss for batch 185 : 0.07525106519460678\n",
      "Training loss for batch 186 : 0.06671366840600967\n",
      "Training loss for batch 187 : 0.011141384020447731\n",
      "Training loss for batch 188 : 0.5141754150390625\n",
      "Training loss for batch 189 : 0.3240674138069153\n",
      "Training loss for batch 190 : 0.045907407999038696\n",
      "Training loss for batch 191 : 0.08355281502008438\n",
      "Training loss for batch 192 : 0.04323223978281021\n",
      "Training loss for batch 193 : 0.19399508833885193\n",
      "Training loss for batch 194 : 0.061201855540275574\n",
      "Training loss for batch 195 : 0.19144338369369507\n",
      "Training loss for batch 196 : 0.10286203026771545\n",
      "Training loss for batch 197 : 0.08429564535617828\n",
      "Training loss for batch 198 : 0.030789794400334358\n",
      "Training loss for batch 199 : 0.223374605178833\n",
      "Training loss for batch 200 : 0.009128129109740257\n",
      "Training loss for batch 201 : 0.11465607583522797\n",
      "Training loss for batch 202 : 0.00045439181849360466\n",
      "Training loss for batch 203 : 0.061440955847501755\n",
      "Training loss for batch 204 : 0.04587151110172272\n",
      "Training loss for batch 205 : 0.13648302853107452\n",
      "Training loss for batch 206 : 0.15016710758209229\n",
      "Training loss for batch 207 : 0.3697667717933655\n",
      "Training loss for batch 208 : 0.00232225120998919\n",
      "Training loss for batch 209 : 0.15511469542980194\n",
      "Training loss for batch 210 : 0.02700064331293106\n",
      "Training loss for batch 211 : 0.01661951281130314\n",
      "Training loss for batch 212 : 0.024662405252456665\n",
      "Training loss for batch 213 : 0.11494231224060059\n",
      "Training loss for batch 214 : 0.07050866633653641\n",
      "Training loss for batch 215 : 0.0319800078868866\n",
      "Training loss for batch 216 : 0.35184258222579956\n",
      "Training loss for batch 217 : 0.36071014404296875\n",
      "Training loss for batch 218 : 0.38408997654914856\n",
      "Training loss for batch 219 : 0.0618460588157177\n",
      "Training loss for batch 220 : 0.11757120490074158\n",
      "Training loss for batch 221 : 0.2863287925720215\n",
      "Training loss for batch 222 : 0.060163822025060654\n",
      "Training loss for batch 223 : 0.025297556072473526\n",
      "Training loss for batch 224 : 0.2704504728317261\n",
      "Training loss for batch 225 : 0.20095711946487427\n",
      "Training loss for batch 226 : 0.18030576407909393\n",
      "Training loss for batch 227 : 0.09567845612764359\n",
      "Training loss for batch 228 : 0.1522410362958908\n",
      "Training loss for batch 229 : 0.3083358108997345\n",
      "Training loss for batch 230 : 0.03594488278031349\n",
      "Training loss for batch 231 : 0.028109878301620483\n",
      "Training loss for batch 232 : 0.06585456430912018\n",
      "Training loss for batch 233 : 0.1212005615234375\n",
      "Training loss for batch 234 : 0.1754050999879837\n",
      "Training loss for batch 235 : 0.08563388884067535\n",
      "Training loss for batch 236 : 0.06115318089723587\n",
      "Training loss for batch 237 : 0.1982431709766388\n",
      "Training loss for batch 238 : 0.17382648587226868\n",
      "Training loss for batch 239 : 0.1608763188123703\n",
      "Training loss for batch 240 : 0.20170535147190094\n",
      "Training loss for batch 241 : 0.12315817177295685\n",
      "Training loss for batch 242 : 0.05277527868747711\n",
      "Training loss for batch 243 : 0.0755312368273735\n",
      "Training loss for batch 244 : 0.2823799252510071\n",
      "Training loss for batch 245 : 6.422738078981638e-05\n",
      "Training loss for batch 246 : 0.10981222987174988\n",
      "Training loss for batch 247 : 0.004225423093885183\n",
      "Training loss for batch 248 : 0.026217859238386154\n",
      "Training loss for batch 249 : 0.17413094639778137\n",
      "Training loss for batch 250 : 0.2107323855161667\n",
      "Training loss for batch 251 : 0.13627688586711884\n",
      "Training loss for batch 252 : 0.09518779814243317\n",
      "Training loss for batch 253 : 0.14675720036029816\n",
      "Training loss for batch 254 : 0.07547694444656372\n",
      "Training loss for batch 255 : 0.121681347489357\n",
      "Training loss for batch 256 : 0.42477506399154663\n",
      "Training loss for batch 257 : 0.2535349726676941\n",
      "Training loss for batch 258 : 0.10017406940460205\n",
      "Training loss for batch 259 : 0.079309843480587\n",
      "Training loss for batch 260 : 0.19007274508476257\n",
      "Training loss for batch 261 : 0.2672547698020935\n",
      "Training loss for batch 262 : 0.28421682119369507\n",
      "Training loss for batch 263 : 0.18862448632717133\n",
      "Training loss for batch 264 : 0.047703180462121964\n",
      "Training loss for batch 265 : 0.19048838317394257\n",
      "Training loss for batch 266 : 0.11147086322307587\n",
      "Training loss for batch 267 : 0.033394016325473785\n",
      "Training loss for batch 268 : 0.08306114375591278\n",
      "Training loss for batch 269 : 0.13347069919109344\n",
      "Training loss for batch 270 : 0.010539660230278969\n",
      "Training loss for batch 271 : 0.23320193588733673\n",
      "Training loss for batch 272 : 0.08778706192970276\n",
      "Training loss for batch 273 : 0.06458218395709991\n",
      "Training loss for batch 274 : 0.22309868037700653\n",
      "Training loss for batch 275 : 0.14430643618106842\n",
      "Training loss for batch 276 : 0.06831824034452438\n",
      "Training loss for batch 277 : 0.15754279494285583\n",
      "Training loss for batch 278 : 0.24291592836380005\n",
      "Training loss for batch 279 : 0.008642240427434444\n",
      "Training loss for batch 280 : 0.20974239706993103\n",
      "Training loss for batch 281 : 0.05447712168097496\n",
      "Training loss for batch 282 : 0.16794534027576447\n",
      "Training loss for batch 283 : 0.0642322525382042\n",
      "Training loss for batch 284 : 0.24423155188560486\n",
      "Training loss for batch 285 : 0.2560464143753052\n",
      "Training loss for batch 286 : 0.043818213045597076\n",
      "Training loss for batch 287 : 0.1766626536846161\n",
      "Training loss for batch 288 : 0.209731787443161\n",
      "Training loss for batch 289 : 0.17883680760860443\n",
      "Training loss for batch 290 : 0.07006926089525223\n",
      "Training loss for batch 291 : 0.03327757492661476\n",
      "Training loss for batch 292 : 0.009714075364172459\n",
      "Training loss for batch 293 : 0.042730316519737244\n",
      "Training loss for batch 294 : 0.13947008550167084\n",
      "Training loss for batch 295 : 0.03821602463722229\n",
      "Training loss for batch 296 : 0.09424728900194168\n",
      "Training loss for batch 297 : 0.1430569887161255\n",
      "Training loss for batch 298 : 0.04079693555831909\n",
      "Training loss for batch 299 : 0.1611553430557251\n",
      "Training loss for batch 300 : 0.24488499760627747\n",
      "Training loss for batch 301 : 0.2989687919616699\n",
      "Training loss for batch 302 : -0.0012768191518262029\n",
      "Training loss for batch 303 : 0.12224840372800827\n",
      "Training loss for batch 304 : 0.1937628835439682\n",
      "Training loss for batch 305 : 0.14606311917304993\n",
      "Training loss for batch 306 : 0.03261358290910721\n",
      "Training loss for batch 307 : 0.008855770342051983\n",
      "Training loss for batch 308 : 0.2142990529537201\n",
      "Training loss for batch 309 : 0.3497155010700226\n",
      "Training loss for batch 310 : 0.023738859221339226\n",
      "Training loss for batch 311 : 0.0740189477801323\n",
      "Training loss for batch 312 : 0.16180750727653503\n",
      "Training loss for batch 313 : 0.39726313948631287\n",
      "Training loss for batch 314 : 0.1814354956150055\n",
      "Training loss for batch 315 : 0.10866829752922058\n",
      "Training loss for batch 316 : 0.04086017981171608\n",
      "Training loss for batch 317 : 0.11571657657623291\n",
      "Training loss for batch 318 : 0.09670564532279968\n",
      "Training loss for batch 319 : -0.0015073521062731743\n",
      "Training loss for batch 320 : 0.20832887291908264\n",
      "Training loss for batch 321 : 0.017165478318929672\n",
      "Training loss for batch 322 : 0.0\n",
      "Training loss for batch 323 : 0.24458661675453186\n",
      "Training loss for batch 324 : 0.2045801728963852\n",
      "Training loss for batch 325 : 0.10977064073085785\n",
      "Training loss for batch 326 : 0.09113789349794388\n",
      "Training loss for batch 327 : 0.1512540876865387\n",
      "Training loss for batch 328 : 0.24124819040298462\n",
      "Training loss for batch 329 : 0.03767826035618782\n",
      "Training loss for batch 330 : 0.1487135887145996\n",
      "Training loss for batch 331 : 0.22824588418006897\n",
      "Training loss for batch 332 : 0.11789777129888535\n",
      "Training loss for batch 333 : 0.04645600914955139\n",
      "Training loss for batch 334 : 0.1212051585316658\n",
      "Training loss for batch 335 : 0.16416749358177185\n",
      "Training loss for batch 336 : 0.17124420404434204\n",
      "Training loss for batch 337 : 0.14251241087913513\n",
      "Training loss for batch 338 : 0.23864001035690308\n",
      "Training loss for batch 339 : 0.059352368116378784\n",
      "Training loss for batch 340 : 0.2309437096118927\n",
      "Training loss for batch 341 : 0.29201918840408325\n",
      "Training loss for batch 342 : 0.12265780568122864\n",
      "Training loss for batch 343 : 0.20145155489444733\n",
      "Training loss for batch 344 : 0.22977252304553986\n",
      "Training loss for batch 345 : 0.10206203162670135\n",
      "Training loss for batch 346 : 0.05474666878581047\n",
      "Training loss for batch 347 : 0.040241312235593796\n",
      "Training loss for batch 348 : 0.0813596174120903\n",
      "Training loss for batch 349 : 0.19302381575107574\n",
      "Training loss for batch 350 : 0.08727873116731644\n",
      "Training loss for batch 351 : 0.008177807554602623\n",
      "Training loss for batch 352 : 0.07419488579034805\n",
      "Training loss for batch 353 : 0.14505642652511597\n",
      "Training loss for batch 354 : 0.001881331205368042\n",
      "Training loss for batch 355 : 0.1302911788225174\n",
      "Training loss for batch 356 : 0.11604553461074829\n",
      "Training loss for batch 357 : 0.12234268337488174\n",
      "Training loss for batch 358 : 0.0871674120426178\n",
      "Training loss for batch 359 : 0.01308626588433981\n",
      "Training loss for batch 360 : 0.1613098382949829\n",
      "Training loss for batch 361 : 0.28712818026542664\n",
      "Training loss for batch 362 : 0.2522338926792145\n",
      "Training loss for batch 363 : 0.03374762833118439\n",
      "Training loss for batch 364 : 0.11914418637752533\n",
      "Training loss for batch 365 : 0.272194504737854\n",
      "Training loss for batch 366 : 0.005255944561213255\n",
      "Training loss for batch 367 : 0.19795182347297668\n",
      "Training loss for batch 368 : 0.029030637815594673\n",
      "Training loss for batch 369 : 0.10266350954771042\n",
      "Training loss for batch 370 : 0.038135915994644165\n",
      "Training loss for batch 371 : 0.26901498436927795\n",
      "Training loss for batch 372 : 0.17696431279182434\n",
      "Training loss for batch 373 : 0.13792897760868073\n",
      "Training loss for batch 374 : 0.08613971620798111\n",
      "Training loss for batch 375 : 0.1781611442565918\n",
      "Training loss for batch 376 : 0.08076294511556625\n",
      "Training loss for batch 377 : 0.03790667653083801\n",
      "Training loss for batch 378 : 0.25406721234321594\n",
      "Training loss for batch 379 : 0.07229496538639069\n",
      "Training loss for batch 380 : 0.10137490183115005\n",
      "Training loss for batch 381 : 0.1744168996810913\n",
      "Training loss for batch 382 : 0.14194446802139282\n",
      "Training loss for batch 383 : 0.1883566826581955\n",
      "Training loss for batch 384 : 0.019291428849101067\n",
      "Training loss for batch 385 : 0.03656202554702759\n",
      "Training loss for batch 386 : 0.0\n",
      "Training loss for batch 387 : 0.010909901931881905\n",
      "Training loss for batch 388 : 0.06942294538021088\n",
      "Training loss for batch 389 : 0.08148996531963348\n",
      "Training loss for batch 390 : 0.2195107489824295\n",
      "Training loss for batch 391 : -0.0013697425602003932\n",
      "Training loss for batch 392 : 0.028007149696350098\n",
      "Training loss for batch 393 : 0.07280109822750092\n",
      "Training loss for batch 394 : 0.08766764402389526\n",
      "Training loss for batch 395 : 0.00872427225112915\n",
      "Training loss for batch 396 : 0.10325152426958084\n",
      "Training loss for batch 397 : 0.07722848653793335\n",
      "Training loss for batch 398 : 0.23922045528888702\n",
      "Training loss for batch 399 : 0.08013709634542465\n",
      "Training loss for batch 400 : 0.033938080072402954\n",
      "Training loss for batch 401 : 0.1274571269750595\n",
      "Training loss for batch 402 : -0.0001448901602998376\n",
      "Training loss for batch 403 : 0.024018293246626854\n",
      "Training loss for batch 404 : 0.20358538627624512\n",
      "Training loss for batch 405 : 0.2164546102285385\n",
      "Training loss for batch 406 : 0.0068280575796961784\n",
      "Training loss for batch 407 : 0.05913100019097328\n",
      "Training loss for batch 408 : 0.01737171970307827\n",
      "Training loss for batch 409 : 0.1373264193534851\n",
      "Training loss for batch 410 : 0.30445221066474915\n",
      "Training loss for batch 411 : 0.11564624309539795\n",
      "Training loss for batch 412 : 0.23151788115501404\n",
      "Training loss for batch 413 : -0.0019203061237931252\n",
      "Training loss for batch 414 : 0.1578601896762848\n",
      "Training loss for batch 415 : 0.030415358021855354\n",
      "Training loss for batch 416 : 0.025401970371603966\n",
      "Training loss for batch 417 : 0.0824366956949234\n",
      "Training loss for batch 418 : 0.06607946753501892\n",
      "Training loss for batch 419 : 0.17000529170036316\n",
      "Training loss for batch 420 : 0.018074190244078636\n",
      "Training loss for batch 421 : 0.18440984189510345\n",
      "Training loss for batch 422 : 0.15487097203731537\n",
      "Training loss for batch 423 : 0.032894380390644073\n",
      "Training loss for batch 424 : 0.07691637426614761\n",
      "Training loss for batch 425 : 0.12289183586835861\n",
      "Training loss for batch 426 : 0.02407379448413849\n",
      "Training loss for batch 427 : 0.03331226110458374\n",
      "Training loss for batch 428 : 0.2559938132762909\n",
      "Training loss for batch 429 : 0.21230703592300415\n",
      "Training loss for batch 430 : 0.30267447233200073\n",
      "Training loss for batch 431 : 0.013276508077979088\n",
      "Training loss for batch 432 : 0.24737095832824707\n",
      "Training loss for batch 433 : 0.17032387852668762\n",
      "Training loss for batch 434 : 0.0\n",
      "Training loss for batch 435 : 0.058128174394369125\n",
      "Training loss for batch 436 : 0.03531671687960625\n",
      "Training loss for batch 437 : 0.21341893076896667\n",
      "Training loss for batch 438 : 0.02674601599574089\n",
      "Training loss for batch 439 : 0.27816683053970337\n",
      "Training loss for batch 440 : 0.19590777158737183\n",
      "Training loss for batch 441 : 0.10799986124038696\n",
      "Training loss for batch 442 : 0.08423882722854614\n",
      "Training loss for batch 443 : 0.1757858395576477\n",
      "Training loss for batch 444 : 0.23609763383865356\n",
      "Training loss for batch 445 : 0.19403526186943054\n",
      "Training loss for batch 446 : 0.007731617894023657\n",
      "Training loss for batch 447 : 0.10285316407680511\n",
      "Training loss for batch 448 : 0.14773471653461456\n",
      "Training loss for batch 449 : 0.2850567698478699\n",
      "Training loss for batch 450 : 0.15025942027568817\n",
      "Training loss for batch 451 : 0.11744765937328339\n",
      "Training loss for batch 452 : 0.0\n",
      "Training loss for batch 453 : 0.004762087948620319\n",
      "Training loss for batch 454 : 0.054085779935121536\n",
      "Training loss for batch 455 : 0.003391232341527939\n",
      "Training loss for batch 456 : 0.015506671741604805\n",
      "Training loss for batch 457 : 0.05281670391559601\n",
      "Training loss for batch 458 : 0.016816552728414536\n",
      "Training loss for batch 459 : 0.10029043257236481\n",
      "Training loss for batch 460 : 0.0561143159866333\n",
      "Training loss for batch 461 : 0.02913859486579895\n",
      "Training loss for batch 462 : 0.010600972920656204\n",
      "Training loss for batch 463 : 0.1593385636806488\n",
      "Training loss for batch 464 : 0.13768959045410156\n",
      "Training loss for batch 465 : 0.02802732028067112\n",
      "Training loss for batch 466 : 0.1396063268184662\n",
      "Training loss for batch 467 : 0.003081897972151637\n",
      "Training loss for batch 468 : 0.2739613652229309\n",
      "Training loss for batch 469 : 0.050459083169698715\n",
      "Training loss for batch 470 : 0.3207383155822754\n",
      "Training loss for batch 471 : 0.026000766083598137\n",
      "Training loss for batch 472 : 0.24217136204242706\n",
      "Training loss for batch 473 : 0.04437929764389992\n",
      "Training loss for batch 474 : 0.12254968285560608\n",
      "Training loss for batch 475 : 0.013688496313989162\n",
      "Training loss for batch 476 : 0.006608839146792889\n",
      "Training loss for batch 477 : 0.13414990901947021\n",
      "Training loss for batch 478 : 0.07332207262516022\n",
      "Training loss for batch 479 : 0.07370590418577194\n",
      "Training loss for batch 480 : 0.35199519991874695\n",
      "Training loss for batch 481 : 0.11461152881383896\n",
      "Training loss for batch 482 : 0.08773145079612732\n",
      "Training loss for batch 483 : 0.034674517810344696\n",
      "Training loss for batch 484 : 0.16560807824134827\n",
      "Training loss for batch 485 : 0.010928217321634293\n",
      "Training loss for batch 486 : 0.10780411213636398\n",
      "Training loss for batch 487 : 0.10404640436172485\n",
      "Training loss for batch 488 : 0.07015936076641083\n",
      "Training loss for batch 489 : 0.05449249967932701\n",
      "Training loss for batch 490 : 0.03612947463989258\n",
      "Training loss for batch 491 : 0.11088503897190094\n",
      "Training loss for batch 492 : 0.026043955236673355\n",
      "Training loss for batch 493 : 0.06452139467000961\n",
      "Training loss for batch 494 : 0.0346798449754715\n",
      "Training loss for batch 495 : 0.26218757033348083\n",
      "Training loss for batch 496 : 0.019184892997145653\n",
      "Training loss for batch 497 : 0.07611672580242157\n",
      "Training loss for batch 498 : 0.22996585071086884\n",
      "Training loss for batch 499 : 0.11529629677534103\n",
      "Training loss for batch 500 : 0.0037324230652302504\n",
      "Training loss for batch 501 : 0.2889364957809448\n",
      "Training loss for batch 502 : 0.054950349032878876\n",
      "Training loss for batch 503 : 0.044855646789073944\n",
      "Training loss for batch 504 : 0.09034749120473862\n",
      "Training loss for batch 505 : 0.01684068888425827\n",
      "Training loss for batch 506 : 0.11916957795619965\n",
      "Training loss for batch 507 : 0.017029903829097748\n",
      "Training loss for batch 508 : 0.22450807690620422\n",
      "Training loss for batch 509 : 0.018743712455034256\n",
      "Training loss for batch 510 : 0.3925541937351227\n",
      "Training loss for batch 511 : 0.08449909090995789\n",
      "Training loss for batch 512 : 0.08414851874113083\n",
      "Training loss for batch 513 : 0.07644909620285034\n",
      "Training loss for batch 514 : 0.10495954751968384\n",
      "Training loss for batch 515 : 0.20685231685638428\n",
      "Training loss for batch 516 : 0.15683795511722565\n",
      "Training loss for batch 517 : 0.06710594892501831\n",
      "Training loss for batch 518 : 0.10364800691604614\n",
      "Training loss for batch 519 : 0.10958477854728699\n",
      "Training loss for batch 520 : 0.24519193172454834\n",
      "Training loss for batch 521 : 0.003680598456412554\n",
      "Training loss for batch 522 : 0.15285725891590118\n",
      "Training loss for batch 523 : 0.2986659109592438\n",
      "Training loss for batch 524 : 0.09038223326206207\n",
      "Training loss for batch 525 : 0.17406000196933746\n",
      "Training loss for batch 526 : 0.026646416634321213\n",
      "Training loss for batch 527 : 0.1574103832244873\n",
      "Training loss for batch 528 : 0.1378456950187683\n",
      "Training loss for batch 529 : 0.017571544274687767\n",
      "Training loss for batch 530 : 0.14393824338912964\n",
      "Training loss for batch 531 : 0.08921516686677933\n",
      "Training loss for batch 532 : 0.03911940008401871\n",
      "Training loss for batch 533 : 0.14080281555652618\n",
      "Training loss for batch 534 : 0.04901367798447609\n",
      "Training loss for batch 535 : 0.03268425166606903\n",
      "Training loss for batch 536 : 0.1808597445487976\n",
      "Training loss for batch 537 : 0.25071829557418823\n",
      "Training loss for batch 538 : 0.0650331974029541\n",
      "Training loss for batch 539 : 0.05954596400260925\n",
      "Training loss for batch 540 : 0.011366525664925575\n",
      "Training loss for batch 541 : 0.075201116502285\n",
      "Training loss for batch 542 : 0.06656645983457565\n",
      "Training loss for batch 543 : 0.3300867974758148\n",
      "Training loss for batch 544 : 0.5221250057220459\n",
      "Training loss for batch 545 : 0.031184818595647812\n",
      "Training loss for batch 546 : 0.22727686166763306\n",
      "Training loss for batch 547 : 0.007164369337260723\n",
      "Training loss for batch 548 : 0.20017403364181519\n",
      "Training loss for batch 549 : 0.12759017944335938\n",
      "Training loss for batch 550 : 0.006264360621571541\n",
      "Training loss for batch 551 : 0.01442873477935791\n",
      "Training loss for batch 552 : 0.09776473790407181\n",
      "Training loss for batch 553 : 0.15198691189289093\n",
      "Training loss for batch 554 : 0.12786130607128143\n",
      "Training loss for batch 555 : 0.13682684302330017\n",
      "Training loss for batch 556 : 0.0009812822099775076\n",
      "Training loss for batch 557 : 0.068367138504982\n",
      "Training loss for batch 558 : 0.2144468128681183\n",
      "Training loss for batch 559 : 0.1324186474084854\n",
      "Training loss for batch 560 : 0.07660161703824997\n",
      "Training loss for batch 561 : 0.19900010526180267\n",
      "Training loss for batch 562 : 0.13650940358638763\n",
      "Training loss for batch 563 : 0.0045585036277771\n",
      "Training loss for batch 564 : 0.07668796181678772\n",
      "Training loss for batch 565 : 0.03056664764881134\n",
      "Training loss for batch 566 : 0.07910703122615814\n",
      "Training loss for batch 567 : 0.19906949996948242\n",
      "Training loss for batch 568 : 0.2051916867494583\n",
      "Training loss for batch 569 : 0.01443618256598711\n",
      "Training loss for batch 570 : 0.04055555537343025\n",
      "Training loss for batch 571 : 0.27803486585617065\n",
      "Training loss for batch 572 : 0.0029749374371021986\n",
      "Training loss for batch 573 : 0.1326613426208496\n",
      "Training loss for batch 574 : 0.006073018070310354\n",
      "Training loss for batch 575 : 0.01811496540904045\n",
      "Training loss for batch 576 : 0.24866585433483124\n",
      "Training loss for batch 577 : 0.08748076856136322\n",
      "Training loss for batch 578 : 0.1619139313697815\n",
      "Training loss for batch 579 : 0.0423581562936306\n",
      "Training loss for batch 580 : 0.06644033640623093\n",
      "Training loss for batch 581 : 0.03969081491231918\n",
      "Training loss for batch 582 : 0.046918705105781555\n",
      "Training loss for batch 583 : 0.09963570535182953\n",
      "Training loss for batch 584 : 0.1064942330121994\n",
      "Training loss for batch 585 : 0.008438041433691978\n",
      "Training loss for batch 586 : 0.09881671518087387\n",
      "Training loss for batch 587 : 0.09634692966938019\n",
      "Training loss for batch 588 : 0.0\n",
      "Training loss for batch 589 : 0.03000878356397152\n",
      "Training loss for batch 590 : 0.03967823088169098\n",
      "Training loss for batch 591 : 0.17013855278491974\n",
      "Training loss for batch 592 : 0.13826562464237213\n",
      "Training loss for batch 593 : 0.055253226310014725\n",
      "Training loss for batch 594 : 0.12411599606275558\n",
      "Training loss for batch 595 : 0.07539654523134232\n",
      "Training loss for batch 596 : 0.015505950897932053\n",
      "Training loss for batch 597 : 0.15859493613243103\n",
      "Training loss for batch 598 : 0.1708907037973404\n",
      "Training loss for batch 599 : 0.13677337765693665\n",
      "Training loss for batch 600 : 0.03393106162548065\n",
      "Training loss for batch 601 : 0.07503937184810638\n",
      "Training loss for batch 602 : 0.09114222228527069\n",
      "Training loss for batch 603 : 0.2784457802772522\n",
      "Training loss for batch 604 : 0.3416435420513153\n",
      "Training loss for batch 605 : 0.2634906470775604\n",
      "Training loss for batch 606 : 0.024274662137031555\n",
      "Training loss for batch 607 : 0.2579836845397949\n",
      "Training loss for batch 608 : 0.02734883688390255\n",
      "Training loss for batch 609 : 0.01767566241323948\n",
      "Training loss for batch 610 : 0.21124906837940216\n",
      "Training loss for batch 611 : 0.2628743648529053\n",
      "Training loss for batch 612 : 0.22265157103538513\n",
      "Training loss for batch 613 : 0.06622494012117386\n",
      "Training loss for batch 614 : 0.05029473826289177\n",
      "Training loss for batch 615 : 0.18005481362342834\n",
      "Training loss for batch 616 : 0.1405816376209259\n",
      "Training loss for batch 617 : 0.05298418551683426\n",
      "Training loss for batch 618 : 0.17271333932876587\n",
      "Training loss for batch 619 : 0.009537721052765846\n",
      "Training loss for batch 620 : 0.0839308351278305\n",
      "Training loss for batch 621 : 0.12075076997280121\n",
      "Training loss for batch 622 : 0.153594508767128\n",
      "Training loss for batch 623 : 0.037989094853401184\n",
      "Training loss for batch 624 : 0.22114922106266022\n",
      "Training loss for batch 625 : 0.14254282414913177\n",
      "Training loss for batch 626 : 0.20714923739433289\n",
      "Training loss for batch 627 : 0.2466166913509369\n",
      "Training loss for batch 628 : 0.24738742411136627\n",
      "Training loss for batch 629 : 0.287350058555603\n",
      "Training loss for batch 630 : 0.08067234605550766\n",
      "Training loss for batch 631 : 0.05804338678717613\n",
      "Training loss for batch 632 : 0.11777440458536148\n",
      "Training loss for batch 633 : 0.03589460998773575\n",
      "Training loss for batch 634 : 0.03580643609166145\n",
      "Training loss for batch 635 : 0.06589771807193756\n",
      "Training loss for batch 636 : 0.059336088597774506\n",
      "Training loss for batch 637 : 0.2158288061618805\n",
      "Training loss for batch 638 : 0.08088893443346024\n",
      "Training loss for batch 639 : 0.15850478410720825\n",
      "Training loss for batch 640 : 0.03358055651187897\n",
      "Training loss for batch 641 : 0.08049621433019638\n",
      "Training loss for batch 642 : 0.0\n",
      "Training loss for batch 643 : 0.004145856015384197\n",
      "Training loss for batch 644 : 0.052724890410900116\n",
      "Training loss for batch 645 : 0.06427668035030365\n",
      "Training loss for batch 646 : 0.07794825732707977\n",
      "Training loss for batch 647 : 0.17053484916687012\n",
      "Training loss for batch 648 : 0.034898243844509125\n",
      "Training loss for batch 649 : 0.2771121859550476\n",
      "Training loss for batch 650 : 0.03478797525167465\n",
      "Training loss for batch 651 : 0.033111948519945145\n",
      "Training loss for batch 652 : 0.06501610577106476\n",
      "Training loss for batch 653 : 0.0568096823990345\n",
      "Training loss for batch 654 : 0.04465030878782272\n",
      "Training loss for batch 655 : 0.09593960642814636\n",
      "Training loss for batch 656 : 0.24651509523391724\n",
      "Training loss for batch 657 : 0.14786286652088165\n",
      "Training loss for batch 658 : 0.07715518772602081\n",
      "Training loss for batch 659 : 0.0700611025094986\n",
      "Training loss for batch 660 : 0.11468184739351273\n",
      "Training loss for batch 661 : 0.06975624710321426\n",
      "Training loss for batch 662 : 0.14967909455299377\n",
      "Training loss for batch 663 : 0.3162569999694824\n",
      "Training loss for batch 664 : -0.0024964867625385523\n",
      "Training loss for batch 665 : 0.1247522160410881\n",
      "Training loss for batch 666 : 0.08351655304431915\n",
      "Training loss for batch 667 : 0.07851414382457733\n",
      "Training loss for batch 668 : 0.11364875733852386\n",
      "Training loss for batch 669 : 0.26126012206077576\n",
      "Training loss for batch 670 : 0.013684025034308434\n",
      "Training loss for batch 671 : 0.08971656858921051\n",
      "Training loss for batch 672 : 0.02353084273636341\n",
      "Training loss for batch 673 : 0.19716589152812958\n",
      "Training loss for batch 674 : 0.03526540845632553\n",
      "Training loss for batch 675 : 0.027095258235931396\n",
      "Training loss for batch 676 : 0.4422399699687958\n",
      "Training loss for batch 677 : 0.03934764489531517\n",
      "Training loss for batch 678 : 0.1817677617073059\n",
      "Training loss for batch 679 : 0.023331880569458008\n",
      "Training loss for batch 680 : 0.15975195169448853\n",
      "Training loss for batch 681 : 0.013317211531102657\n",
      "Training loss for batch 682 : 0.17130739986896515\n",
      "Training loss for batch 683 : 0.03214166685938835\n",
      "Training loss for batch 684 : 0.10782624781131744\n",
      "Training loss for batch 685 : 0.17577862739562988\n",
      "Training loss for batch 686 : 0.3019682466983795\n",
      "Training loss for batch 687 : 0.0857754796743393\n",
      "Training loss for batch 688 : 0.22029924392700195\n",
      "Training loss for batch 689 : 0.15405774116516113\n",
      "Training loss for batch 690 : 0.06803880631923676\n",
      "Training loss for batch 691 : 0.0578048899769783\n",
      "Training loss for batch 692 : 0.09321720898151398\n",
      "Training loss for batch 693 : 0.2360311597585678\n",
      "Training loss for batch 694 : 0.06697580963373184\n",
      "Training loss for batch 695 : 0.2773275077342987\n",
      "Training loss for batch 696 : 0.1694793552160263\n",
      "Training loss for batch 697 : 0.021709226071834564\n",
      "Training loss for batch 698 : 0.2589399218559265\n",
      "Training loss for batch 699 : 0.21845468878746033\n",
      "Training loss for batch 700 : 0.01563238725066185\n",
      "Training loss for batch 701 : 0.07853318005800247\n",
      "Training loss for batch 702 : 0.11366145312786102\n",
      "Training loss for batch 703 : 0.13579976558685303\n",
      "Training loss for batch 704 : 0.04445858299732208\n",
      "Training loss for batch 705 : 0.0650252029299736\n",
      "Training loss for batch 706 : 0.09724830836057663\n",
      "Training loss for batch 707 : 0.17586827278137207\n",
      "Training loss for batch 708 : 0.1263793408870697\n",
      "Training loss for batch 709 : 0.18888704478740692\n",
      "Training loss for batch 710 : 0.05050741136074066\n",
      "Training loss for batch 711 : 0.034943316131830215\n",
      "Training loss for batch 712 : 0.08793391287326813\n",
      "Training loss for batch 713 : 0.04913442209362984\n",
      "Training loss for batch 714 : 0.3083273470401764\n",
      "Training loss for batch 715 : 0.03695288673043251\n",
      "Training loss for batch 716 : 0.185116246342659\n",
      "Training loss for batch 717 : 0.25523167848587036\n",
      "Training loss for batch 718 : 0.4067225456237793\n",
      "Training loss for batch 719 : 0.2023489624261856\n",
      "Training loss for batch 720 : 0.34623807668685913\n",
      "Training loss for batch 721 : 0.378648042678833\n",
      "Training loss for batch 722 : 0.024517783895134926\n",
      "Training loss for batch 723 : 0.08782535791397095\n",
      "Training loss for batch 724 : 0.14063194394111633\n",
      "Training loss for batch 725 : 0.024687450379133224\n",
      "Training loss for batch 726 : 0.06953634321689606\n",
      "Training loss for batch 727 : 0.47148627042770386\n",
      "Training loss for batch 728 : 0.13082925975322723\n",
      "Training loss for batch 729 : 0.15615326166152954\n",
      "Training loss for batch 730 : 0.10307057201862335\n",
      "Training loss for batch 731 : 0.03124179318547249\n",
      "Training loss for batch 732 : 0.07482846826314926\n",
      "Training loss for batch 733 : 0.14659494161605835\n",
      "Training loss for batch 734 : 0.03324633836746216\n",
      "Training loss for batch 735 : 0.07618235796689987\n",
      "Training loss for batch 736 : 0.03831817954778671\n",
      "Training loss for batch 737 : 0.03098185919225216\n",
      "Training loss for batch 738 : 0.06500664353370667\n",
      "Training loss for batch 739 : 0.30951178073883057\n",
      "Training loss for batch 740 : 0.136738121509552\n",
      "Training loss for batch 741 : 0.21537834405899048\n",
      "Training loss for batch 742 : -0.0034884486813098192\n",
      "Training loss for batch 743 : 0.264315128326416\n",
      "Training loss for batch 744 : 0.030025189742445946\n",
      "Training loss for batch 745 : 0.09542962908744812\n",
      "Training loss for batch 746 : 0.04114455357193947\n",
      "Training loss for batch 747 : 0.06420676410198212\n",
      "Training loss for batch 748 : 0.03808522969484329\n",
      "Training loss for batch 749 : 0.04637514799833298\n",
      "Training loss for batch 750 : 0.0743691623210907\n",
      "Training loss for batch 751 : 0.3372371792793274\n",
      "Training loss for batch 752 : 0.120807945728302\n",
      "Training loss for batch 753 : 0.2501320540904999\n",
      "Training loss for batch 754 : 0.11231564730405807\n",
      "Training loss for batch 755 : 0.01742907427251339\n",
      "Training loss for batch 756 : 0.06363167613744736\n",
      "Training loss for batch 757 : 0.0\n",
      "Training loss for batch 758 : 0.13268035650253296\n",
      "Training loss for batch 759 : 0.26847538352012634\n",
      "Training loss for batch 760 : 0.055538877844810486\n",
      "Training loss for batch 761 : 0.16516216099262238\n",
      "Training loss for batch 762 : -0.003949122503399849\n",
      "Training loss for batch 763 : 0.18535099923610687\n",
      "Training loss for batch 764 : 0.022245105355978012\n",
      "Training loss for batch 765 : 0.012557035312056541\n",
      "Training loss for batch 766 : 0.24538537859916687\n",
      "Training loss for batch 767 : 0.12899412214756012\n",
      "Training loss for batch 768 : 0.18499262630939484\n",
      "Training loss for batch 769 : 0.026529455557465553\n",
      "Training loss for batch 770 : 0.12046398222446442\n",
      "Training loss for batch 771 : 0.31012633442878723\n",
      "Training loss for batch 772 : 0.16646850109100342\n",
      "Training loss for batch 773 : 0.16536635160446167\n",
      "Training loss for batch 774 : 0.1124839037656784\n",
      "Training loss for batch 775 : 0.07729329913854599\n",
      "Training loss for batch 776 : 0.19735835492610931\n",
      "Training loss for batch 777 : 0.051791347563266754\n",
      "Training loss for batch 778 : 0.07979343086481094\n",
      "Training loss for batch 779 : 0.11478935182094574\n",
      "Training loss for batch 780 : 0.15054196119308472\n",
      "Training loss for batch 781 : 0.16295233368873596\n",
      "Training loss for batch 782 : 0.1109853982925415\n",
      "Training loss for batch 783 : 0.16418594121932983\n",
      "Training loss for batch 784 : 0.034365005791187286\n",
      "Training loss for batch 785 : 0.15527932345867157\n",
      "Training loss for batch 786 : 0.11629766970872879\n",
      "Training loss for batch 787 : 0.10665691643953323\n",
      "Training loss for batch 788 : 0.06717716157436371\n",
      "Training loss for batch 789 : 0.16230559349060059\n",
      "Training loss for batch 790 : 0.1549823135137558\n",
      "Training loss for batch 791 : 0.1380109041929245\n",
      "Training loss for batch 792 : 0.07170048356056213\n",
      "Training loss for batch 793 : 0.24388718605041504\n",
      "Training loss for batch 794 : 0.10910390317440033\n",
      "Training loss for batch 795 : 0.19981268048286438\n",
      "Training loss for batch 796 : 0.14443618059158325\n",
      "Training loss for batch 797 : 0.13650567829608917\n",
      "Training loss for batch 798 : 0.10172508656978607\n",
      "Training loss for batch 799 : 0.002517525339499116\n",
      "Training loss for batch 800 : 0.26765528321266174\n",
      "Training loss for batch 801 : 0.10133081674575806\n",
      "Training loss for batch 802 : 0.18346408009529114\n",
      "Training loss for batch 803 : 0.05568956211209297\n",
      "Training loss for batch 804 : -1.6241670891758986e-05\n",
      "Training loss for batch 805 : 0.015184209682047367\n",
      "Training loss for batch 806 : 0.3485909402370453\n",
      "Training loss for batch 807 : -0.002755203051492572\n",
      "Training loss for batch 808 : 0.12772119045257568\n",
      "Training loss for batch 809 : 0.1557847410440445\n",
      "Training loss for batch 810 : 0.2152407169342041\n",
      "Training loss for batch 811 : 0.18525069952011108\n",
      "Training loss for batch 812 : 0.12032721936702728\n",
      "Training loss for batch 813 : 0.1440134346485138\n",
      "Training loss for batch 814 : 0.1081458106637001\n",
      "Training loss for batch 815 : 0.16857685148715973\n",
      "Training loss for batch 816 : 0.20506994426250458\n",
      "Training loss for batch 817 : 0.2537500560283661\n",
      "Training loss for batch 818 : 0.1490507572889328\n",
      "Training loss for batch 819 : 0.027634702622890472\n",
      "Training loss for batch 820 : 0.21998873353004456\n",
      "Training loss for batch 821 : 0.3147292137145996\n",
      "Training loss for batch 822 : 0.00807519443333149\n",
      "Training loss for batch 823 : 0.2886878252029419\n",
      "Training loss for batch 824 : 0.014012472704052925\n",
      "Training loss for batch 825 : 0.057196568697690964\n",
      "Training loss for batch 826 : 0.1857164204120636\n",
      "Training loss for batch 827 : 0.16139239072799683\n",
      "Training loss for batch 828 : 0.021095851436257362\n",
      "Training loss for batch 829 : 0.15299175679683685\n",
      "Training loss for batch 830 : 0.0052418638952076435\n",
      "Training loss for batch 831 : 0.19932563602924347\n",
      "Training loss for batch 832 : 0.08333113044500351\n",
      "Training loss for batch 833 : 0.11344398558139801\n",
      "Training loss for batch 834 : 0.11840996891260147\n",
      "Training loss for batch 835 : 0.05963705852627754\n",
      "Training loss for batch 836 : 0.03855481743812561\n",
      "Training loss for batch 837 : 0.21328718960285187\n",
      "Training loss for batch 838 : 0.1010499969124794\n",
      "Training loss for batch 839 : 0.16104795038700104\n",
      "Training loss for batch 840 : 0.05939289927482605\n",
      "Training loss for batch 841 : 0.07784277945756912\n",
      "Training loss for batch 842 : 0.009862769395112991\n",
      "Training loss for batch 843 : 0.03165373206138611\n",
      "Training loss for batch 844 : 0.0038878871127963066\n",
      "Training loss for batch 845 : 0.0038284461479634047\n",
      "Training loss for batch 846 : 0.15627813339233398\n",
      "Training loss for batch 847 : 0.2593398690223694\n",
      "Training loss for batch 848 : 0.07200928777456284\n",
      "Training loss for batch 849 : 0.14923904836177826\n",
      "Training loss for batch 850 : 0.2587735950946808\n",
      "Training loss for batch 851 : 0.14109031856060028\n",
      "Training loss for batch 852 : 0.004243791103363037\n",
      "Training loss for batch 853 : 0.016243943944573402\n",
      "Training loss for batch 854 : 0.19912007451057434\n",
      "Training loss for batch 855 : 0.08062328398227692\n",
      "Training loss for batch 856 : 0.0032023689709603786\n",
      "Training loss for batch 857 : 0.009851107373833656\n",
      "Training loss for batch 858 : 0.1727590262889862\n",
      "Training loss for batch 859 : 0.0005164345493540168\n",
      "Training loss for batch 860 : 0.17959430813789368\n",
      "Training loss for batch 861 : 0.11757910251617432\n",
      "Training loss for batch 862 : 0.13238029181957245\n",
      "Training loss for batch 863 : 0.07951229810714722\n",
      "Training loss for batch 864 : 0.4498238265514374\n",
      "Training loss for batch 865 : 0.17193250358104706\n",
      "Training loss for batch 866 : 0.20689304172992706\n",
      "Training loss for batch 867 : 0.07949811220169067\n",
      "Training loss for batch 868 : 0.23408329486846924\n",
      "Training loss for batch 869 : 0.1511802077293396\n",
      "Training loss for batch 870 : 0.015943504869937897\n",
      "Training loss for batch 871 : 0.10348211228847504\n",
      "Training loss for batch 872 : 0.3416350483894348\n",
      "Training loss for batch 873 : 0.04911813139915466\n",
      "Training loss for batch 874 : 0.07599254697561264\n",
      "Training loss for batch 875 : 0.3043365776538849\n",
      "Training loss for batch 876 : 0.07135047763586044\n",
      "Training loss for batch 877 : 0.12167172878980637\n",
      "Training loss for batch 878 : 0.11831089109182358\n",
      "Training loss for batch 879 : 0.29797276854515076\n",
      "Training loss for batch 880 : 0.001384695409797132\n",
      "Training loss for batch 881 : 0.2963593602180481\n",
      "Training loss for batch 882 : 0.005961817689239979\n",
      "Training loss for batch 883 : 0.11869118362665176\n",
      "Training loss for batch 884 : 0.3042813837528229\n",
      "Training loss for batch 885 : 0.08166589587926865\n",
      "Training loss for batch 886 : 0.1843460351228714\n",
      "Training loss for batch 887 : 0.1312081515789032\n",
      "Training loss for batch 888 : 0.268756628036499\n",
      "Training loss for batch 889 : 0.19558241963386536\n",
      "Training loss for batch 890 : 0.1708943247795105\n",
      "Training loss for batch 891 : 0.06831245869398117\n",
      "Training loss for batch 892 : 0.032948799431324005\n",
      "Training loss for batch 893 : 0.3493782579898834\n",
      "Training loss for batch 894 : 0.08074373751878738\n",
      "Training loss for batch 895 : 0.05936700850725174\n",
      "Training loss for batch 896 : 0.1331680566072464\n",
      "Training loss for batch 897 : 0.10288383811712265\n",
      "Training loss for batch 898 : 0.052861861884593964\n",
      "Training loss for batch 899 : 0.22570540010929108\n",
      "Training loss for batch 900 : 0.10626204311847687\n",
      "Training loss for batch 901 : 0.05613476037979126\n",
      "Training loss for batch 902 : 0.03399520367383957\n",
      "Training loss for batch 903 : 0.0584561824798584\n",
      "Training loss for batch 904 : 0.2957122027873993\n",
      "Training loss for batch 905 : 0.06438025832176208\n",
      "Training loss for batch 906 : 0.14336827397346497\n",
      "Training loss for batch 907 : 0.061570119112730026\n",
      "Training loss for batch 908 : 0.06832832098007202\n",
      "Training loss for batch 909 : 0.1239045113325119\n",
      "Training loss for batch 910 : 0.3161764442920685\n",
      "Training loss for batch 911 : 0.08061777800321579\n",
      "Training loss for batch 912 : 0.07829967141151428\n",
      "Training loss for batch 913 : 0.2076157033443451\n",
      "Training loss for batch 914 : 0.23768140375614166\n",
      "Training loss for batch 915 : 0.1877550482749939\n",
      "Training loss for batch 916 : 0.0631176233291626\n",
      "Training loss for batch 917 : 0.013718447647988796\n",
      "Training loss for batch 918 : 0.13094297051429749\n",
      "Training loss for batch 919 : 0.11627574265003204\n",
      "Training loss for batch 920 : 0.1840539425611496\n",
      "Training loss for batch 921 : 0.00731792114675045\n",
      "Training loss for batch 922 : 0.010802398435771465\n",
      "Training loss for batch 923 : 0.09674794971942902\n",
      "Training loss for batch 924 : 0.402523398399353\n",
      "Training loss for batch 925 : 0.07163161039352417\n",
      "Training loss for batch 926 : 0.12480391561985016\n",
      "Training loss for batch 927 : 0.22609779238700867\n",
      "Training loss for batch 928 : 0.09955195337533951\n",
      "Training loss for batch 929 : 0.0068487427197396755\n",
      "Training loss for batch 930 : 0.3121846914291382\n",
      "Training loss for batch 931 : 0.29537004232406616\n",
      "Training loss for batch 932 : 0.010885185562074184\n",
      "Training loss for batch 933 : 0.19351470470428467\n",
      "Training loss for batch 934 : 0.11919732391834259\n",
      "Training loss for batch 935 : 0.03296927735209465\n",
      "Training loss for batch 936 : 0.020225919783115387\n",
      "Training loss for batch 937 : 0.024197794497013092\n",
      "Training loss for batch 938 : 0.1689554750919342\n",
      "Training loss for batch 939 : 0.06309786438941956\n",
      "Training loss for batch 940 : 0.04028300940990448\n",
      "Training loss for batch 941 : 0.27351924777030945\n",
      "Training loss for batch 942 : 0.04316141456365585\n",
      "Training loss for batch 943 : 0.17580334842205048\n",
      "Training loss for batch 944 : 0.16205883026123047\n",
      "Training loss for batch 945 : 0.030057698488235474\n",
      "Training loss for batch 946 : 0.09402357041835785\n",
      "Training loss for batch 947 : 0.33410874009132385\n",
      "Training loss for batch 948 : 0.04107561334967613\n",
      "Training loss for batch 949 : 0.3389463722705841\n",
      "Training loss for batch 950 : 0.11466705799102783\n",
      "Training loss for batch 951 : 0.07895303517580032\n",
      "Training loss for batch 952 : 0.06996841728687286\n",
      "Training loss for batch 953 : 0.00980310421437025\n",
      "Training loss for batch 954 : 0.08799125254154205\n",
      "Training loss for batch 955 : 0.021015776321291924\n",
      "Training loss for batch 956 : 0.0820363312959671\n",
      "Training loss for batch 957 : 0.07494493573904037\n",
      "Training loss for batch 958 : 0.07966355979442596\n",
      "Training loss for batch 959 : 0.36204418540000916\n",
      "Training loss for batch 960 : 0.029683362692594528\n",
      "Training loss for batch 961 : 0.06812162697315216\n",
      "Training loss for batch 962 : 0.2349255532026291\n",
      "Training loss for batch 963 : 0.3635353147983551\n",
      "Training loss for batch 964 : 0.06349335610866547\n",
      "Training loss for batch 965 : 0.23107761144638062\n",
      "Training loss for batch 966 : 0.21403692662715912\n",
      "Training loss for batch 967 : 0.1744721680879593\n",
      "Training loss for batch 968 : 0.19857797026634216\n",
      "Training loss for batch 969 : 0.026398539543151855\n",
      "Training loss for batch 970 : 0.05289580672979355\n",
      "Training loss for batch 971 : 0.2099403440952301\n",
      "Training loss for batch 972 : 0.03025365248322487\n",
      "Training loss for batch 973 : 0.12123491615056992\n",
      "Training loss for batch 974 : 0.045886069536209106\n",
      "Training loss for batch 975 : 0.16822010278701782\n",
      "Training loss for batch 976 : 0.12930293381214142\n",
      "Training loss for batch 977 : 0.053880155086517334\n",
      "Training loss for batch 978 : 0.13955818116664886\n",
      "Training loss for batch 979 : 0.13514459133148193\n",
      "Training loss for batch 980 : 0.21097132563591003\n",
      "Training loss for batch 981 : 0.006558592431247234\n",
      "Training loss for batch 982 : 0.19864977896213531\n",
      "Training loss for batch 983 : 0.09220969676971436\n",
      "Training loss for batch 984 : 0.11562570184469223\n",
      "Training loss for batch 985 : 0.0996309369802475\n",
      "Training loss for batch 986 : 0.02033245377242565\n",
      "Training loss for batch 987 : 0.15423980355262756\n",
      "Training loss for batch 988 : 0.24319154024124146\n",
      "Training loss for batch 989 : 0.1771189272403717\n",
      "Training loss for batch 990 : 0.04470066726207733\n",
      "Training loss for batch 991 : 0.1438010036945343\n",
      "Training loss for batch 992 : 0.1475977599620819\n",
      "Training loss for batch 993 : 0.11450685560703278\n",
      "Training loss for batch 994 : 0.12818863987922668\n",
      "Training loss for batch 995 : 0.09079571813344955\n",
      "Training loss for batch 996 : 0.16645744442939758\n",
      "Training loss for batch 997 : 0.1522085964679718\n",
      "Training loss for batch 998 : 0.22311343252658844\n",
      "Training loss for batch 999 : 0.11648152023553848\n",
      "Training loss for batch 1000 : 0.06160273030400276\n",
      "Training loss for batch 1001 : 0.2395215928554535\n",
      "Training loss for batch 1002 : 0.05440826714038849\n",
      "Training loss for batch 1003 : 0.0542074590921402\n",
      "Training loss for batch 1004 : 0.1874036192893982\n",
      "Training loss for batch 1005 : 0.09494418650865555\n",
      "Training loss for batch 1006 : 0.07616858184337616\n",
      "Training loss for batch 1007 : 0.14930304884910583\n",
      "Training loss for batch 1008 : 0.048272036015987396\n",
      "Training loss for batch 1009 : 0.12370160222053528\n",
      "Training loss for batch 1010 : 0.07807254046201706\n",
      "Training loss for batch 1011 : 0.03487080708146095\n",
      "Training loss for batch 1012 : 0.11739376187324524\n",
      "Training loss for batch 1013 : 0.17783112823963165\n",
      "Training loss for batch 1014 : 0.04050099849700928\n",
      "Training loss for batch 1015 : 0.2739506959915161\n",
      "Training loss for batch 1016 : 0.09158717840909958\n",
      "Training loss for batch 1017 : 0.0347093790769577\n",
      "Training loss for batch 1018 : 0.09318026155233383\n",
      "Training loss for batch 1019 : 0.25764647126197815\n",
      "Training loss for batch 1020 : 0.00669390382245183\n",
      "Training loss for batch 1021 : 0.2523718476295471\n",
      "Training loss for batch 1022 : 0.25028759241104126\n",
      "Training loss for batch 1023 : 0.26138490438461304\n",
      "Training loss for batch 1024 : 0.15952876210212708\n",
      "Training loss for batch 1025 : 0.153827503323555\n",
      "Training loss for batch 1026 : 0.21926015615463257\n",
      "Training loss for batch 1027 : 0.06452354043722153\n",
      "Training loss for batch 1028 : 0.11560363322496414\n",
      "Training loss for batch 1029 : 0.2567644417285919\n",
      "Training loss for batch 1030 : 0.02842266671359539\n",
      "Training loss for batch 1031 : 0.09315941482782364\n",
      "Training loss for batch 1032 : 0.06049627810716629\n",
      "Training loss for batch 1033 : 0.17410333454608917\n",
      "Training loss for batch 1034 : 0.0034218975342810154\n",
      "Training loss for batch 1035 : 0.0097338343039155\n",
      "Training loss for batch 1036 : 0.01733017899096012\n",
      "Training loss for batch 1037 : 0.10330948233604431\n",
      "Training loss for batch 1038 : 0.026771193370223045\n",
      "Training loss for batch 1039 : 0.11840375512838364\n",
      "Training loss for batch 1040 : 0.40853753685951233\n",
      "Training loss for batch 1041 : 0.1677704006433487\n",
      "Training loss for batch 1042 : 0.05318798869848251\n",
      "Training loss for batch 1043 : 0.09680066257715225\n",
      "Training loss for batch 1044 : 0.13880810141563416\n",
      "Training loss for batch 1045 : 0.0662173181772232\n",
      "Training loss for batch 1046 : 0.03312888368964195\n",
      "Training loss for batch 1047 : 0.20508092641830444\n",
      "Training loss for batch 1048 : 0.05210433527827263\n",
      "Training loss for batch 1049 : 0.3763071298599243\n",
      "Training loss for batch 1050 : 0.17706996202468872\n",
      "Training loss for batch 1051 : 0.10354946553707123\n",
      "Training loss for batch 1052 : -0.0005239078891463578\n",
      "Training loss for batch 1053 : 0.21731793880462646\n",
      "Training loss for batch 1054 : 0.16423796117305756\n",
      "Training loss for batch 1055 : 0.16621029376983643\n",
      "Training loss for batch 1056 : 0.12824895977973938\n",
      "Training loss for batch 1057 : 0.18755556643009186\n",
      "Training loss for batch 1058 : 0.04752487689256668\n",
      "Training loss for batch 1059 : 0.012277967296540737\n",
      "Training loss for batch 1060 : 0.06665175408124924\n",
      "Training loss for batch 1061 : 0.19436737895011902\n",
      "Training loss for batch 1062 : 0.2070494145154953\n",
      "Training loss for batch 1063 : 0.09045480936765671\n",
      "Training loss for batch 1064 : 0.06898435205221176\n",
      "Training loss for batch 1065 : 0.16870301961898804\n",
      "Training loss for batch 1066 : 0.07153560221195221\n",
      "Training loss for batch 1067 : 0.022259533405303955\n",
      "Training loss for batch 1068 : 0.3110169768333435\n",
      "Training loss for batch 1069 : 0.076605886220932\n",
      "Training loss for batch 1070 : 0.18226979672908783\n",
      "Training loss for batch 1071 : 0.08122214674949646\n",
      "Training loss for batch 1072 : 0.07133257389068604\n",
      "Training loss for batch 1073 : 0.005539960693567991\n",
      "Training loss for batch 1074 : 0.07853849977254868\n",
      "Training loss for batch 1075 : 0.26596492528915405\n",
      "Training loss for batch 1076 : 0.12025314569473267\n",
      "Training loss for batch 1077 : 0.05837637931108475\n",
      "Training loss for batch 1078 : 0.020630672574043274\n",
      "Training loss for batch 1079 : 0.00275343656539917\n",
      "Training loss for batch 1080 : 0.05806778743863106\n",
      "Training loss for batch 1081 : 0.044171951711177826\n",
      "Training loss for batch 1082 : 0.22707432508468628\n",
      "Training loss for batch 1083 : 0.08821475505828857\n",
      "Training loss for batch 1084 : 0.00267760269343853\n",
      "Training loss for batch 1085 : 0.11652638763189316\n",
      "Training loss for batch 1086 : 0.03169945999979973\n",
      "Training loss for batch 1087 : 0.14446142315864563\n",
      "Training loss for batch 1088 : 0.1899995058774948\n",
      "Training loss for batch 1089 : 0.08402855694293976\n",
      "Training loss for batch 1090 : 0.045042239129543304\n",
      "Training loss for batch 1091 : 0.023615706712007523\n",
      "Training loss for batch 1092 : 0.12889306247234344\n",
      "Training loss for batch 1093 : 0.054158762097358704\n",
      "Training loss for batch 1094 : 0.07467347383499146\n",
      "Training loss for batch 1095 : 0.325334757566452\n",
      "Training loss for batch 1096 : 0.3845902979373932\n",
      "Training loss for batch 1097 : 0.05480232089757919\n",
      "Training loss for batch 1098 : 0.006105651613324881\n",
      "Training loss for batch 1099 : 0.15379419922828674\n",
      "Training loss for batch 1100 : 0.08017448335886002\n",
      "Training loss for batch 1101 : 0.25786063075065613\n",
      "Training loss for batch 1102 : 0.030788080766797066\n",
      "Training loss for batch 1103 : 0.18908527493476868\n",
      "Training loss for batch 1104 : 0.29554957151412964\n",
      "Training loss for batch 1105 : 0.0\n",
      "Training loss for batch 1106 : 0.06244442239403725\n",
      "Training loss for batch 1107 : 0.2468375712633133\n",
      "Training loss for batch 1108 : 0.1560560166835785\n",
      "Training loss for batch 1109 : 0.10034573078155518\n",
      "Training loss for batch 1110 : 0.09362971037626266\n",
      "Training loss for batch 1111 : 0.07354681193828583\n",
      "Training loss for batch 1112 : 0.2401505410671234\n",
      "Training loss for batch 1113 : 0.2784138321876526\n",
      "Training loss for batch 1114 : 0.10651621967554092\n",
      "Training loss for batch 1115 : 0.03067723661661148\n",
      "Training loss for batch 1116 : 0.035728078335523605\n",
      "Training loss for batch 1117 : 0.006518661044538021\n",
      "Training loss for batch 1118 : -0.00785931944847107\n",
      "Training loss for batch 1119 : 0.06662477552890778\n",
      "Training loss for batch 1120 : 0.04640450328588486\n",
      "Training loss for batch 1121 : 0.07229902595281601\n",
      "Training loss for batch 1122 : 0.22337840497493744\n",
      "Training loss for batch 1123 : 0.15340279042720795\n",
      "Training loss for batch 1124 : 0.08985549956560135\n",
      "Training loss for batch 1125 : 0.11289071291685104\n",
      "Training loss for batch 1126 : 0.1582164466381073\n",
      "Training loss for batch 1127 : 0.0067327264696359634\n",
      "Training loss for batch 1128 : 0.10023972392082214\n",
      "Training loss for batch 1129 : 0.020519066601991653\n",
      "Training loss for batch 1130 : 0.12415384501218796\n",
      "Training loss for batch 1131 : 0.07397166639566422\n",
      "Training loss for batch 1132 : 0.17001444101333618\n",
      "Training loss for batch 1133 : 0.11535471677780151\n",
      "Training loss for batch 1134 : 0.13358384370803833\n",
      "Training loss for batch 1135 : 0.1727517694234848\n",
      "Training loss for batch 1136 : 0.0005139525746926665\n",
      "Training loss for batch 1137 : 0.06958140432834625\n",
      "Training loss for batch 1138 : 0.1831378936767578\n",
      "Training loss for batch 1139 : 0.09346770495176315\n",
      "Training loss for batch 1140 : 0.10815202444791794\n",
      "Training loss for batch 1141 : 0.001987507101148367\n",
      "Training loss for batch 1142 : 0.06478875875473022\n",
      "Training loss for batch 1143 : 0.1892251968383789\n",
      "Training loss for batch 1144 : 0.2397855818271637\n",
      "Training loss for batch 1145 : 0.09348953515291214\n",
      "Training loss for batch 1146 : 0.06325104832649231\n",
      "Training loss for batch 1147 : 0.2252655029296875\n",
      "Training loss for batch 1148 : 0.15545077621936798\n",
      "Training loss for batch 1149 : 0.0\n",
      "Training loss for batch 1150 : 0.011606013402342796\n",
      "Training loss for batch 1151 : 0.17135582864284515\n",
      "Training loss for batch 1152 : 0.26328545808792114\n",
      "Training loss for batch 1153 : 0.02888190746307373\n",
      "Training loss for batch 1154 : 0.026793960481882095\n",
      "Training loss for batch 1155 : 0.11320823431015015\n",
      "Training loss for batch 1156 : 0.11586470156908035\n",
      "Training loss for batch 1157 : 0.1947368085384369\n",
      "Training loss for batch 1158 : 0.5048558712005615\n",
      "Training loss for batch 1159 : 0.030194493010640144\n",
      "Training loss for batch 1160 : 0.02140195481479168\n",
      "Training loss for batch 1161 : 0.201321542263031\n",
      "Training loss for batch 1162 : 0.1399204134941101\n",
      "Training loss for batch 1163 : 0.28374892473220825\n",
      "Training loss for batch 1164 : 0.0\n",
      "Training loss for batch 1165 : 0.5586808919906616\n",
      "Training loss for batch 1166 : 0.26582732796669006\n",
      "Training loss for batch 1167 : 0.01709134504199028\n",
      "Training loss for batch 1168 : 0.06872252374887466\n",
      "Training loss for batch 1169 : 0.053832486271858215\n",
      "Training loss for batch 1170 : 0.11009226739406586\n",
      "Training loss for batch 1171 : 0.07854572683572769\n",
      "Training loss for batch 1172 : 0.013324936851859093\n",
      "Training loss for batch 1173 : 0.037682365626096725\n",
      "Training loss for batch 1174 : 0.08677303045988083\n",
      "Training loss for batch 1175 : 0.3199608623981476\n",
      "Training loss for batch 1176 : 0.027971208095550537\n",
      "Training loss for batch 1177 : 0.0660695880651474\n",
      "Training loss for batch 1178 : 0.08336003869771957\n",
      "Training loss for batch 1179 : 0.2331538200378418\n",
      "Training loss for batch 1180 : 0.24413859844207764\n",
      "Training loss for batch 1181 : 0.04893612489104271\n",
      "Training loss for batch 1182 : 0.3600684404373169\n",
      "Training loss for batch 1183 : 0.1039644107222557\n",
      "Training loss for batch 1184 : 0.18685728311538696\n",
      "Training loss for batch 1185 : 0.3983473777770996\n",
      "Training loss for batch 1186 : 0.08384984731674194\n",
      "Training loss for batch 1187 : 0.12441893666982651\n",
      "Training loss for batch 1188 : 0.36923444271087646\n",
      "Training loss for batch 1189 : 0.29197120666503906\n",
      "Training loss for batch 1190 : 0.11120791733264923\n",
      "Training loss for batch 1191 : 0.046760231256484985\n",
      "Training loss for batch 1192 : 0.20095160603523254\n",
      "Training loss for batch 1193 : 0.017636582255363464\n",
      "Training loss for batch 1194 : 0.014255542308092117\n",
      "Training loss for batch 1195 : 0.07921537011861801\n",
      "Training loss for batch 1196 : 0.03568870201706886\n",
      "Training loss for batch 1197 : 0.04382985830307007\n",
      "Training loss for batch 1198 : 0.018762268126010895\n",
      "Training loss for batch 1199 : 0.054433029145002365\n",
      "Training loss for batch 1200 : 0.10546079277992249\n",
      "Training loss for batch 1201 : 0.06674665212631226\n",
      "Training loss for batch 1202 : 0.08717070519924164\n",
      "Training loss for batch 1203 : 0.17792338132858276\n",
      "Training loss for batch 1204 : 0.0011746883392333984\n",
      "Training loss for batch 1205 : 0.08463853597640991\n",
      "Training loss for batch 1206 : 0.21169109642505646\n",
      "Training loss for batch 1207 : -0.001614764565601945\n",
      "Training loss for batch 1208 : 0.06877805292606354\n",
      "Training loss for batch 1209 : 0.18609368801116943\n",
      "Training loss for batch 1210 : 0.14935961365699768\n",
      "Training loss for batch 1211 : 0.0\n",
      "Training loss for batch 1212 : 0.233407124876976\n",
      "Training loss for batch 1213 : 0.12417209148406982\n",
      "Training loss for batch 1214 : 0.1514245569705963\n",
      "Training loss for batch 1215 : 0.11286911368370056\n",
      "Training loss for batch 1216 : 0.2696788012981415\n",
      "Training loss for batch 1217 : 0.042866747826337814\n",
      "Training loss for batch 1218 : 0.39902323484420776\n",
      "Training loss for batch 1219 : 0.1795482188463211\n",
      "Training loss for batch 1220 : 0.05533682182431221\n",
      "Training loss for batch 1221 : 0.1567682921886444\n",
      "Training loss for batch 1222 : 0.33645203709602356\n",
      "Training loss for batch 1223 : 0.13205471634864807\n",
      "Training loss for batch 1224 : 0.09999807178974152\n",
      "Training loss for batch 1225 : 0.1739625483751297\n",
      "Training loss for batch 1226 : 0.0958365947008133\n",
      "Training loss for batch 1227 : 0.08226657658815384\n",
      "Training loss for batch 1228 : 0.2286742627620697\n",
      "Training loss for batch 1229 : 0.017966439947485924\n",
      "Training loss for batch 1230 : 0.03493528813123703\n",
      "Training loss for batch 1231 : 0.060603730380535126\n",
      "Training loss for batch 1232 : 0.17674140632152557\n",
      "Training loss for batch 1233 : 0.22884581983089447\n",
      "Training loss for batch 1234 : 0.3209223449230194\n",
      "Training loss for batch 1235 : 0.08494943380355835\n",
      "Training loss for batch 1236 : 0.17626029253005981\n",
      "Training loss for batch 1237 : 0.1086166650056839\n",
      "Training loss for batch 1238 : 0.2501118779182434\n",
      "Training loss for batch 1239 : 0.02038181945681572\n",
      "Training loss for batch 1240 : 0.10838153213262558\n",
      "Training loss for batch 1241 : 0.023250125348567963\n",
      "Training loss for batch 1242 : 0.18459579348564148\n",
      "Training loss for batch 1243 : 0.023504357784986496\n",
      "Training loss for batch 1244 : 0.03266189247369766\n",
      "Training loss for batch 1245 : 0.09149011969566345\n",
      "Training loss for batch 1246 : 0.05457601696252823\n",
      "Training loss for batch 1247 : 0.1084943637251854\n",
      "Training loss for batch 1248 : 0.021045338362455368\n",
      "Training loss for batch 1249 : 0.17549626529216766\n",
      "Training loss for batch 1250 : 0.11514836549758911\n",
      "Training loss for batch 1251 : 0.15052974224090576\n",
      "Training loss for batch 1252 : 0.02156967855989933\n",
      "Training loss for batch 1253 : 0.01941453479230404\n",
      "Training loss for batch 1254 : 0.10179252177476883\n",
      "Training loss for batch 1255 : 0.039695512503385544\n",
      "Training loss for batch 1256 : 0.10496275871992111\n",
      "Training loss for batch 1257 : 0.13333186507225037\n",
      "Training loss for batch 1258 : 0.009977648966014385\n",
      "Training loss for batch 1259 : 0.07453783601522446\n",
      "Training loss for batch 1260 : 0.06998461484909058\n",
      "Training loss for batch 1261 : 0.04650825262069702\n",
      "Training loss for batch 1262 : 0.15951547026634216\n",
      "Training loss for batch 1263 : 0.12587687373161316\n",
      "Training loss for batch 1264 : 0.15613782405853271\n",
      "Training loss for batch 1265 : 0.024664729833602905\n",
      "Training loss for batch 1266 : 0.01107611320912838\n",
      "Training loss for batch 1267 : 0.0233166441321373\n",
      "Training loss for batch 1268 : 0.014552438631653786\n",
      "Training loss for batch 1269 : 0.03266645595431328\n",
      "Training loss for batch 1270 : 0.06678551435470581\n",
      "Training loss for batch 1271 : 0.052482184022665024\n",
      "Training loss for batch 1272 : 0.01780061237514019\n",
      "Training loss for batch 1273 : 0.1408446729183197\n",
      "Training loss for batch 1274 : 0.07633230835199356\n",
      "Training loss for batch 1275 : 0.06792698800563812\n",
      "Training loss for batch 1276 : 0.14592385292053223\n",
      "Training loss for batch 1277 : 0.16423356533050537\n",
      "Training loss for batch 1278 : 0.05213168263435364\n",
      "Training loss for batch 1279 : 0.0700531005859375\n",
      "Training loss for batch 1280 : 0.04429116100072861\n",
      "Training loss for batch 1281 : 0.03232566639780998\n",
      "Training loss for batch 1282 : 0.1689760535955429\n",
      "Training loss for batch 1283 : 0.0009449253557249904\n",
      "Training loss for batch 1284 : 0.13847827911376953\n",
      "Training loss for batch 1285 : 0.2252037525177002\n",
      "Training loss for batch 1286 : 0.38571926951408386\n",
      "Training loss for batch 1287 : 0.10091550648212433\n",
      "Training loss for batch 1288 : 0.07327472418546677\n",
      "Training loss for batch 1289 : 0.038232505321502686\n",
      "Training loss for batch 1290 : 0.013525370508432388\n",
      "Training loss for batch 1291 : 0.13174889981746674\n",
      "Training loss for batch 1292 : 0.03004629909992218\n",
      "Training loss for batch 1293 : 0.09690959751605988\n",
      "Training loss for batch 1294 : 0.06689094007015228\n",
      "Training loss for batch 1295 : 0.03877095505595207\n",
      "Training loss for batch 1296 : 0.24284733831882477\n",
      "Training loss for batch 1297 : 0.1894509494304657\n",
      "Training loss for batch 1298 : 0.03356165438890457\n",
      "Training loss for batch 1299 : 0.16269558668136597\n",
      "Training loss for batch 1300 : 0.11347305029630661\n",
      "Training loss for batch 1301 : 0.20076432824134827\n",
      "Training loss for batch 1302 : 0.09775257855653763\n",
      "Training loss for batch 1303 : 0.04684298485517502\n",
      "Training loss for batch 1304 : 0.2668302059173584\n",
      "Training loss for batch 1305 : 0.21311375498771667\n",
      "Training loss for batch 1306 : 0.016722925007343292\n",
      "Training loss for batch 1307 : 0.2090681940317154\n",
      "Training loss for batch 1308 : 0.09152280539274216\n",
      "Training loss for batch 1309 : 0.07785055041313171\n",
      "Training loss for batch 1310 : 0.2680652439594269\n",
      "Training loss for batch 1311 : 0.05849388614296913\n",
      "Training loss for batch 1312 : 0.18892580270767212\n",
      "Training loss for batch 1313 : 0.08593123406171799\n",
      "Training loss for batch 1314 : 0.090484119951725\n",
      "Training loss for batch 1315 : 0.23947632312774658\n",
      "Training loss for batch 1316 : 0.08365859091281891\n",
      "Training loss for batch 1317 : 0.15299613773822784\n",
      "Training loss for batch 1318 : 0.10383270680904388\n",
      "Training loss for batch 1319 : 0.014047788456082344\n",
      "Training loss for batch 1320 : 0.0991368517279625\n",
      "Training loss for batch 1321 : 0.01172483991831541\n",
      "Training loss for batch 1322 : 0.07902152091264725\n",
      "Training loss for batch 1323 : 0.20496736466884613\n",
      "Training loss for batch 1324 : 0.09483717381954193\n",
      "Training loss for batch 1325 : 0.3023394048213959\n",
      "Training loss for batch 1326 : 0.22688044607639313\n",
      "Training loss for batch 1327 : 0.008455559611320496\n",
      "Training loss for batch 1328 : 0.21878695487976074\n",
      "Training loss for batch 1329 : 0.0331583097577095\n",
      "Training loss for batch 1330 : 0.16672825813293457\n",
      "Training loss for batch 1331 : 0.038237135857343674\n",
      "Training loss for batch 1332 : 0.10356498509645462\n",
      "Training loss for batch 1333 : 0.16673772037029266\n",
      "Training loss for batch 1334 : 0.21364344656467438\n",
      "Training loss for batch 1335 : 0.38734421133995056\n",
      "Training loss for batch 1336 : 0.15178531408309937\n",
      "Training loss for batch 1337 : 0.19242587685585022\n",
      "Training loss for batch 1338 : 0.2544077932834625\n",
      "Training loss for batch 1339 : 0.31206220388412476\n",
      "Training loss for batch 1340 : 0.08350668847560883\n",
      "Training loss for batch 1341 : 0.026688016951084137\n",
      "Training loss for batch 1342 : 0.06270776689052582\n",
      "Training loss for batch 1343 : 0.11095404624938965\n",
      "Training loss for batch 1344 : 0.3370245099067688\n",
      "Training loss for batch 1345 : 0.04505833983421326\n",
      "Training loss for batch 1346 : 0.20608486235141754\n",
      "Training loss for batch 1347 : 0.04348886385560036\n",
      "Training loss for batch 1348 : 0.11664882302284241\n",
      "Training loss for batch 1349 : 0.24849659204483032\n",
      "Training loss for batch 1350 : 0.14805813133716583\n",
      "Training loss for batch 1351 : -0.0022762229200452566\n",
      "Training loss for batch 1352 : 0.15684525668621063\n",
      "Training loss for batch 1353 : 0.2667183578014374\n",
      "Training loss for batch 1354 : 0.04348193109035492\n",
      "Training loss for batch 1355 : 0.07546620815992355\n",
      "Training loss for batch 1356 : 0.2228957712650299\n",
      "Training loss for batch 1357 : 0.26686161756515503\n",
      "Training loss for batch 1358 : 0.18673381209373474\n",
      "Training loss for batch 1359 : 0.0010981758823618293\n",
      "Training loss for batch 1360 : 0.15787823498249054\n",
      "Training loss for batch 1361 : 0.12760229408740997\n",
      "Training loss for batch 1362 : 0.14403031766414642\n",
      "Training loss for batch 1363 : 0.046439386904239655\n",
      "Training loss for batch 1364 : 0.6144992709159851\n",
      "Training loss for batch 1365 : 0.003505145665258169\n",
      "Training loss for batch 1366 : 0.06198957562446594\n",
      "Training loss for batch 1367 : 0.0020721284672617912\n",
      "Training loss for batch 1368 : 0.1974635124206543\n",
      "Training loss for batch 1369 : 0.11169499158859253\n",
      "Training loss for batch 1370 : 0.09234017133712769\n",
      "Training loss for batch 1371 : 0.03492553159594536\n",
      "Training loss for batch 1372 : 0.18462467193603516\n",
      "Training loss for batch 1373 : 0.0857899934053421\n",
      "Training loss for batch 1374 : 0.12898801267147064\n",
      "Training loss for batch 1375 : 0.0805392786860466\n",
      "Training loss for batch 1376 : 0.1361081898212433\n",
      "Training loss for batch 1377 : 0.25742360949516296\n",
      "Training loss for batch 1378 : 0.03828376159071922\n",
      "Training loss for batch 1379 : 0.056955404579639435\n",
      "Training loss for batch 1380 : 0.13416852056980133\n",
      "Training loss for batch 1381 : 0.18000811338424683\n",
      "Training loss for batch 1382 : 0.07651877403259277\n",
      "Training loss for batch 1383 : 0.09510142356157303\n",
      "Training loss for batch 1384 : 0.0763883963227272\n",
      "Training loss for batch 1385 : 0.10915538668632507\n",
      "Training loss for batch 1386 : 0.1401384025812149\n",
      "Training loss for batch 1387 : 0.027527689933776855\n",
      "Training loss for batch 1388 : 0.016886118799448013\n",
      "Training loss for batch 1389 : 0.12404819577932358\n",
      "Training loss for batch 1390 : 0.09828788787126541\n",
      "Training loss for batch 1391 : 0.05250609293580055\n",
      "Training loss for batch 1392 : 0.1290755569934845\n",
      "Training loss for batch 1393 : 0.03875751793384552\n",
      "Training loss for batch 1394 : 0.14368878304958344\n",
      "Training loss for batch 1395 : 0.03173785284161568\n",
      "Training loss for batch 1396 : 0.07950242608785629\n",
      "Training loss for batch 1397 : 0.031517643481492996\n",
      "Training loss for batch 1398 : 0.2064495086669922\n",
      "Training loss for batch 1399 : 0.13459819555282593\n",
      "Training loss for batch 1400 : 0.265500009059906\n",
      "Training loss for batch 1401 : 0.14128224551677704\n",
      "Training loss for batch 1402 : 0.1659477800130844\n",
      "Training loss for batch 1403 : 0.32932350039482117\n",
      "Training loss for batch 1404 : 0.22051522135734558\n",
      "Training loss for batch 1405 : 0.16385386884212494\n",
      "Training loss for batch 1406 : 0.2692336440086365\n",
      "Training loss for batch 1407 : 0.09860417991876602\n",
      "Training loss for batch 1408 : 0.14057987928390503\n",
      "Training loss for batch 1409 : 0.18978457152843475\n",
      "Training loss for batch 1410 : 0.18491753935813904\n",
      "Training loss for batch 1411 : 0.034604690968990326\n",
      "Training loss for batch 1412 : 0.05918506532907486\n",
      "Training loss for batch 1413 : 0.013319847173988819\n",
      "Training loss for batch 1414 : 0.08547699451446533\n",
      "Training loss for batch 1415 : 0.11804526299238205\n",
      "Training loss for batch 1416 : 0.18343152105808258\n",
      "Training loss for batch 1417 : 0.08333244919776917\n",
      "Training loss for batch 1418 : 0.13210436701774597\n",
      "Training loss for batch 1419 : 0.17718665301799774\n",
      "Training loss for batch 1420 : 0.1632424294948578\n",
      "Training loss for batch 1421 : 0.1524186134338379\n",
      "Training loss for batch 1422 : 0.09456488490104675\n",
      "Training loss for batch 1423 : 0.041485220193862915\n",
      "Training loss for batch 1424 : 0.06160698086023331\n",
      "Training loss for batch 1425 : 0.216477632522583\n",
      "Training loss for batch 1426 : 0.05326740816235542\n",
      "Training loss for batch 1427 : 0.13901588320732117\n",
      "Training loss for batch 1428 : 0.2671535909175873\n",
      "Training loss for batch 1429 : 0.017502879723906517\n",
      "Training loss for batch 1430 : 0.03437722474336624\n",
      "Training loss for batch 1431 : 0.06556727737188339\n",
      "Training loss for batch 1432 : 0.09056413173675537\n",
      "Training loss for batch 1433 : 0.07564718276262283\n",
      "Training loss for batch 1434 : 0.14882875978946686\n",
      "Training loss for batch 1435 : 0.04845975339412689\n",
      "Training loss for batch 1436 : 0.2348611205816269\n",
      "Training loss for batch 1437 : 0.07385222613811493\n",
      "Training loss for batch 1438 : -0.0011395975016057491\n",
      "Training loss for batch 1439 : 0.17193752527236938\n",
      "Training loss for batch 1440 : 0.012970706447958946\n",
      "Training loss for batch 1441 : 0.29833754897117615\n",
      "Training loss for batch 1442 : 0.03294498100876808\n",
      "Training loss for batch 1443 : 0.14941257238388062\n",
      "Training loss for batch 1444 : 0.08702749758958817\n",
      "Training loss for batch 1445 : 0.2502947747707367\n",
      "Training loss for batch 1446 : 0.14890390634536743\n",
      "Training loss for batch 1447 : 0.01944594644010067\n",
      "Training loss for batch 1448 : 0.004703055135905743\n",
      "Training loss for batch 1449 : 0.1093645766377449\n",
      "Training loss for batch 1450 : -0.0031968315597623587\n",
      "Training loss for batch 1451 : 0.053950272500514984\n",
      "Training loss for batch 1452 : 0.20263205468654633\n",
      "Training loss for batch 1453 : 0.12498082965612411\n",
      "Training loss for batch 1454 : 0.2575511038303375\n",
      "Training loss for batch 1455 : 0.06715540587902069\n",
      "Training loss for batch 1456 : 0.26338228583335876\n",
      "Training loss for batch 1457 : 0.15759438276290894\n",
      "Training loss for batch 1458 : 0.12987487018108368\n",
      "Training loss for batch 1459 : 0.2797495424747467\n",
      "Training loss for batch 1460 : 0.19972968101501465\n",
      "Training loss for batch 1461 : 0.08833099901676178\n",
      "Training loss for batch 1462 : 0.18212106823921204\n",
      "Training loss for batch 1463 : 0.24190983176231384\n",
      "Training loss for batch 1464 : 0.0529419481754303\n",
      "Training loss for batch 1465 : 0.09735022485256195\n",
      "Training loss for batch 1466 : 0.2299020141363144\n",
      "Training loss for batch 1467 : 0.08422965556383133\n",
      "Training loss for batch 1468 : 0.18031975626945496\n",
      "Training loss for batch 1469 : 0.09239561855792999\n",
      "Training loss for batch 1470 : 0.1649893820285797\n",
      "Training loss for batch 1471 : 0.07230331003665924\n",
      "Training loss for batch 1472 : 0.1894112527370453\n",
      "Training loss for batch 1473 : 0.2629416584968567\n",
      "Training loss for batch 1474 : 0.3190527558326721\n",
      "Training loss for batch 1475 : 0.01055475976318121\n",
      "Training loss for batch 1476 : 0.10574357956647873\n",
      "Training loss for batch 1477 : 0.26325690746307373\n",
      "Training loss for batch 1478 : 0.09908841550350189\n",
      "Training loss for batch 1479 : 0.21342867612838745\n",
      "Training loss for batch 1480 : 0.21426881849765778\n",
      "Training loss for batch 1481 : 0.034729018807411194\n",
      "Training loss for batch 1482 : 0.08379843086004257\n",
      "Training loss for batch 1483 : 0.2200651615858078\n",
      "Training loss for batch 1484 : 0.0020307800732553005\n",
      "Training loss for batch 1485 : 0.35977816581726074\n",
      "Training loss for batch 1486 : 0.06750240176916122\n",
      "Training loss for batch 1487 : 0.05308851599693298\n",
      "Training loss for batch 1488 : 0.10783998668193817\n",
      "Training loss for batch 1489 : 0.1609748899936676\n",
      "Training loss for batch 1490 : 0.023783594369888306\n",
      "Training loss for batch 1491 : 0.17795100808143616\n",
      "Training loss for batch 1492 : 0.028821531683206558\n",
      "Training loss for batch 1493 : 0.008742290548980236\n",
      "Training loss for batch 1494 : 0.09549591690301895\n",
      "Training loss for batch 1495 : 0.08159661293029785\n",
      "Training loss for batch 1496 : 0.14126989245414734\n",
      "Training loss for batch 1497 : 0.018453188240528107\n",
      "Training loss for batch 1498 : 0.1709412783384323\n",
      "Training loss for batch 1499 : 0.09361124038696289\n",
      "Training loss for batch 1500 : 0.07975611090660095\n",
      "Training loss for batch 1501 : 0.04909830540418625\n",
      "Training loss for batch 1502 : 0.2437172681093216\n",
      "Training loss for batch 1503 : 0.08779401332139969\n",
      "Training loss for batch 1504 : 0.05282323434948921\n",
      "Training loss for batch 1505 : 0.13301455974578857\n",
      "Training loss for batch 1506 : 0.09890037775039673\n",
      "Training loss for batch 1507 : 0.19243155419826508\n",
      "Training loss for batch 1508 : 0.20822712779045105\n",
      "Training loss for batch 1509 : 0.15662434697151184\n",
      "Training loss for batch 1510 : 0.1288631111383438\n",
      "Training loss for batch 1511 : 0.041235025972127914\n",
      "Training loss for batch 1512 : 0.05887766182422638\n",
      "Training loss for batch 1513 : 0.02997516095638275\n",
      "Training loss for batch 1514 : 0.08556927740573883\n",
      "Training loss for batch 1515 : 0.03197481855750084\n",
      "Training loss for batch 1516 : 0.13873842358589172\n",
      "Training loss for batch 1517 : 0.018273422494530678\n",
      "Training loss for batch 1518 : 0.011802742257714272\n",
      "Training loss for batch 1519 : 0.17949816584587097\n",
      "Training loss for batch 1520 : 0.13528692722320557\n",
      "Training loss for batch 1521 : 0.19249555468559265\n",
      "Training loss for batch 1522 : 0.09791913628578186\n",
      "Training loss for batch 1523 : 0.05245775729417801\n",
      "Training loss for batch 1524 : 0.0397973470389843\n",
      "Training loss for batch 1525 : 0.1423884630203247\n",
      "Training loss for batch 1526 : 0.010603569447994232\n",
      "Training loss for batch 1527 : 0.16547445952892303\n",
      "Training loss for batch 1528 : 0.07180443406105042\n",
      "Training loss for batch 1529 : 0.042521875351667404\n",
      "Training loss for batch 1530 : 0.05270804464817047\n",
      "Training loss for batch 1531 : 0.05810222774744034\n",
      "Training loss for batch 1532 : 0.20692434906959534\n",
      "Training loss for batch 1533 : 0.025237668305635452\n",
      "Training loss for batch 1534 : 0.09550122916698456\n",
      "Training loss for batch 1535 : 0.3198089003562927\n",
      "Training loss for batch 1536 : 0.001221011159941554\n",
      "Training loss for batch 1537 : 0.18856659531593323\n",
      "Training loss for batch 1538 : -0.0015020035207271576\n",
      "Training loss for batch 1539 : 0.007698738947510719\n",
      "Training loss for batch 1540 : 0.18649467825889587\n",
      "Training loss for batch 1541 : 0.03582997992634773\n",
      "Training loss for batch 1542 : 0.11822459846735\n",
      "Training loss for batch 1543 : 0.23321282863616943\n",
      "Training loss for batch 1544 : 0.22357770800590515\n",
      "Training loss for batch 1545 : 0.04653654247522354\n",
      "Training loss for batch 1546 : 0.09072315692901611\n",
      "Training loss for batch 1547 : 0.21448123455047607\n",
      "Training loss for batch 1548 : 0.07519873976707458\n",
      "Training loss for batch 1549 : 0.10305137932300568\n",
      "Training loss for batch 1550 : 0.006660620681941509\n",
      "Training loss for batch 1551 : 0.0625624880194664\n",
      "Training loss for batch 1552 : 0.013289278373122215\n",
      "Training loss for batch 1553 : 0.0\n",
      "Training loss for batch 1554 : 0.10914801806211472\n",
      "Training loss for batch 1555 : 0.11959262937307358\n",
      "Training loss for batch 1556 : 0.09671399742364883\n",
      "Training loss for batch 1557 : 0.08479855954647064\n",
      "Training loss for batch 1558 : 0.13747969269752502\n",
      "Training loss for batch 1559 : 0.03560968488454819\n",
      "Training loss for batch 1560 : 0.01750066503882408\n",
      "Training loss for batch 1561 : 0.07967501133680344\n",
      "Training loss for batch 1562 : 0.09757670760154724\n",
      "Training loss for batch 1563 : 0.08559327572584152\n",
      "Training loss for batch 1564 : 0.06953712552785873\n",
      "Training loss for batch 1565 : 0.33354249596595764\n",
      "Training loss for batch 1566 : 0.06383393704891205\n",
      "Training loss for batch 1567 : 0.031191812828183174\n",
      "Training loss for batch 1568 : 0.09380581229925156\n",
      "Training loss for batch 1569 : 0.1456267237663269\n",
      "Training loss for batch 1570 : 0.015045005828142166\n",
      "Training loss for batch 1571 : 0.1829984486103058\n",
      "Training loss for batch 1572 : 0.022743776440620422\n",
      "Training loss for batch 1573 : 0.07909591495990753\n",
      "Training loss for batch 1574 : 0.021584227681159973\n",
      "Training loss for batch 1575 : 0.09630924463272095\n",
      "Training loss for batch 1576 : 0.025276008993387222\n",
      "Training loss for batch 1577 : 0.1878160536289215\n",
      "Training loss for batch 1578 : -0.0007191627519205213\n",
      "Training loss for batch 1579 : 0.21724525094032288\n",
      "Training loss for batch 1580 : 0.2111126184463501\n",
      "Training loss for batch 1581 : 0.22781260311603546\n",
      "Training loss for batch 1582 : 0.29514116048812866\n",
      "Training loss for batch 1583 : -0.002005370333790779\n",
      "Training loss for batch 1584 : 0.12557458877563477\n",
      "Training loss for batch 1585 : 0.15327763557434082\n",
      "Training loss for batch 1586 : 0.239825040102005\n",
      "Training loss for batch 1587 : 0.03016017936170101\n",
      "Training loss for batch 1588 : 0.18405945599079132\n",
      "Training loss for batch 1589 : 0.006641252897679806\n",
      "Training loss for batch 1590 : 0.003451575990766287\n",
      "Training loss for batch 1591 : 0.07813478261232376\n",
      "Training loss for batch 1592 : 0.2861689329147339\n",
      "Training loss for batch 1593 : 0.15146398544311523\n",
      "Training loss for batch 1594 : 0.0962248295545578\n",
      "Training loss for batch 1595 : 0.0790468379855156\n",
      "Training loss for batch 1596 : 0.11977560818195343\n",
      "Training loss for batch 1597 : 0.040642887353897095\n",
      "Training loss for batch 1598 : 0.01472829282283783\n",
      "Training loss for batch 1599 : 0.07703790068626404\n",
      "Training loss for batch 1600 : 0.0\n",
      "Training loss for batch 1601 : 0.0317598395049572\n",
      "Training loss for batch 1602 : 0.07856065779924393\n",
      "Training loss for batch 1603 : 0.09183180332183838\n",
      "Training loss for batch 1604 : 0.21762827038764954\n",
      "Training loss for batch 1605 : 0.15824860334396362\n",
      "Training loss for batch 1606 : 0.023726077750325203\n",
      "Training loss for batch 1607 : 0.25760892033576965\n",
      "Training loss for batch 1608 : 0.28939440846443176\n",
      "Training loss for batch 1609 : 0.06315413117408752\n",
      "Training loss for batch 1610 : 0.09520231187343597\n",
      "Training loss for batch 1611 : 0.08685281872749329\n",
      "Training loss for batch 1612 : 0.12303830683231354\n",
      "Training loss for batch 1613 : 0.06763798743486404\n",
      "Training loss for batch 1614 : 0.03893432021141052\n",
      "Training loss for batch 1615 : 0.07736016809940338\n",
      "Training loss for batch 1616 : 0.08430219441652298\n",
      "Training loss for batch 1617 : 0.6680510640144348\n",
      "Training loss for batch 1618 : 0.09985226392745972\n",
      "Training loss for batch 1619 : 0.15149638056755066\n",
      "Training loss for batch 1620 : 0.09498685598373413\n",
      "Training loss for batch 1621 : 0.00976849626749754\n",
      "Training loss for batch 1622 : 0.31731218099594116\n",
      "Training loss for batch 1623 : 0.15106770396232605\n",
      "Training loss for batch 1624 : 0.22515782713890076\n",
      "Training loss for batch 1625 : 0.0952451229095459\n",
      "Training loss for batch 1626 : 0.04021506756544113\n",
      "Training loss for batch 1627 : 0.3871970474720001\n",
      "Training loss for batch 1628 : 0.027637165039777756\n",
      "Training loss for batch 1629 : 0.07086940109729767\n",
      "Training loss for batch 1630 : 0.2649868130683899\n",
      "Training loss for batch 1631 : 0.25407207012176514\n",
      "Training loss for batch 1632 : 0.40343186259269714\n",
      "Training loss for batch 1633 : 0.14604082703590393\n",
      "Training loss for batch 1634 : 0.30446168780326843\n",
      "Training loss for batch 1635 : 0.22329317033290863\n",
      "Training loss for batch 1636 : 0.20581543445587158\n",
      "Training loss for batch 1637 : 0.09913265705108643\n",
      "Training loss for batch 1638 : 0.2268921136856079\n",
      "Training loss for batch 1639 : 0.14728054404258728\n",
      "Training loss for batch 1640 : 0.22659939527511597\n",
      "Training loss for batch 1641 : 0.08855480700731277\n",
      "Training loss for batch 1642 : 0.28112539649009705\n",
      "Training loss for batch 1643 : 0.03924688324332237\n",
      "Training loss for batch 1644 : 0.3200220465660095\n",
      "Training loss for batch 1645 : 0.04460558295249939\n",
      "Training loss for batch 1646 : 0.009628554806113243\n",
      "Training loss for batch 1647 : 0.1728076934814453\n",
      "Training loss for batch 1648 : 0.21279603242874146\n",
      "Training loss for batch 1649 : 0.15723498165607452\n",
      "Training loss for batch 1650 : 0.021121356636285782\n",
      "Training loss for batch 1651 : 0.071916863322258\n",
      "Training loss for batch 1652 : 0.15089978277683258\n",
      "Training loss for batch 1653 : 0.11760342121124268\n",
      "Training loss for batch 1654 : 0.13737472891807556\n",
      "Training loss for batch 1655 : 0.0\n",
      "Training loss for batch 1656 : 0.05624314770102501\n",
      "Training loss for batch 1657 : 0.0008349716663360596\n",
      "Training loss for batch 1658 : 0.27372226119041443\n",
      "Training loss for batch 1659 : 0.0764899030327797\n",
      "Training loss for batch 1660 : 0.042615197598934174\n",
      "Training loss for batch 1661 : 0.11223071068525314\n",
      "Training loss for batch 1662 : 0.0383262112736702\n",
      "Training loss for batch 1663 : 0.012629542499780655\n",
      "Training loss for batch 1664 : 0.0728154182434082\n",
      "Training loss for batch 1665 : 0.039907652884721756\n",
      "Training loss for batch 1666 : 0.013094209134578705\n",
      "Training loss for batch 1667 : 0.19057291746139526\n",
      "Training loss for batch 1668 : 0.1187458410859108\n",
      "Training loss for batch 1669 : 0.06882764399051666\n",
      "Training loss for batch 1670 : 0.14112581312656403\n",
      "Training loss for batch 1671 : 0.2174827754497528\n",
      "Training loss for batch 1672 : 0.14211493730545044\n",
      "Training loss for batch 1673 : 0.22102057933807373\n",
      "Training loss for batch 1674 : 0.25245901942253113\n",
      "Training loss for batch 1675 : 0.019358843564987183\n",
      "Training loss for batch 1676 : 0.15311986207962036\n",
      "Training loss for batch 1677 : 0.05346725508570671\n",
      "Training loss for batch 1678 : 0.09996600449085236\n",
      "Training loss for batch 1679 : 0.13855014741420746\n",
      "Training loss for batch 1680 : 0.11735247075557709\n",
      "Training loss for batch 1681 : 0.016790829598903656\n",
      "Training loss for batch 1682 : 0.038701776415109634\n",
      "Training loss for batch 1683 : 0.0860833078622818\n",
      "Training loss for batch 1684 : 0.09284503757953644\n",
      "Training loss for batch 1685 : 0.3081943094730377\n",
      "Training loss for batch 1686 : 0.3278200626373291\n",
      "Training loss for batch 1687 : 0.09142648428678513\n",
      "Training loss for batch 1688 : 0.08401753753423691\n",
      "Training loss for batch 1689 : 0.2074340283870697\n",
      "Training loss for batch 1690 : 0.21342450380325317\n",
      "Training loss for batch 1691 : 0.039094120264053345\n",
      "Training loss for batch 1692 : 0.2275524139404297\n",
      "Training loss for batch 1693 : 0.12002426385879517\n",
      "Training loss for batch 1694 : 0.03195071220397949\n",
      "Training loss for batch 1695 : 0.13255524635314941\n",
      "Training loss for batch 1696 : 0.10740427672863007\n",
      "Training loss for batch 1697 : 0.11859802901744843\n",
      "Training loss for batch 1698 : 0.23042404651641846\n",
      "Training loss for batch 1699 : 0.1907888650894165\n",
      "Training loss for batch 1700 : 0.07290562242269516\n",
      "Training loss for batch 1701 : 0.15777015686035156\n",
      "Training loss for batch 1702 : 0.14037565886974335\n",
      "Training loss for batch 1703 : 0.03486053645610809\n",
      "Training loss for batch 1704 : 0.07586269080638885\n",
      "Training loss for batch 1705 : 0.058737222105264664\n",
      "Training loss for batch 1706 : 0.13051225244998932\n",
      "Training loss for batch 1707 : 0.035148028284311295\n",
      "Training loss for batch 1708 : 0.1562020480632782\n",
      "Training loss for batch 1709 : 0.23600904643535614\n",
      "Training loss for batch 1710 : 0.1628795564174652\n",
      "Training loss for batch 1711 : 0.1546432226896286\n",
      "Training loss for batch 1712 : 0.053035710006952286\n",
      "Training loss for batch 1713 : 0.055562883615493774\n",
      "Training loss for batch 1714 : 0.14822351932525635\n",
      "Training loss for batch 1715 : 0.3231676518917084\n",
      "Training loss for batch 1716 : 0.06943348050117493\n",
      "Training loss for batch 1717 : 0.099798284471035\n",
      "Training loss for batch 1718 : 0.09952586889266968\n",
      "Training loss for batch 1719 : 0.056150440126657486\n",
      "Training loss for batch 1720 : 0.19023089110851288\n",
      "Training loss for batch 1721 : 0.0009067555656656623\n",
      "Training loss for batch 1722 : 0.029636364430189133\n",
      "Training loss for batch 1723 : 0.07962009310722351\n",
      "Training loss for batch 1724 : 0.016156069934368134\n",
      "Training loss for batch 1725 : 0.1597864180803299\n",
      "Training loss for batch 1726 : 0.16392962634563446\n",
      "Training loss for batch 1727 : 0.044901132583618164\n",
      "Training loss for batch 1728 : 0.12918686866760254\n",
      "Training loss for batch 1729 : -0.003352833678945899\n",
      "Training loss for batch 1730 : 0.04939262196421623\n",
      "Training loss for batch 1731 : 0.12185975164175034\n",
      "Training loss for batch 1732 : 0.0728006437420845\n",
      "Training loss for batch 1733 : 0.12076341360807419\n",
      "Training loss for batch 1734 : 0.33703160285949707\n",
      "Training loss for batch 1735 : 0.020982425659894943\n",
      "Training loss for batch 1736 : 0.18619713187217712\n",
      "Training loss for batch 1737 : 0.05645298212766647\n",
      "Training loss for batch 1738 : 0.05938689410686493\n",
      "Training loss for batch 1739 : 0.09723011404275894\n",
      "Training loss for batch 1740 : 0.07834453880786896\n",
      "Training loss for batch 1741 : 0.31683439016342163\n",
      "Training loss for batch 1742 : 0.11798271536827087\n",
      "Training loss for batch 1743 : 0.07642719894647598\n",
      "Training loss for batch 1744 : -0.0037356282118707895\n",
      "Training loss for batch 1745 : 0.0962289348244667\n",
      "Training loss for batch 1746 : 0.20501944422721863\n",
      "Training loss for batch 1747 : 0.11803372204303741\n",
      "Training loss for batch 1748 : 0.23358787596225739\n",
      "Training loss for batch 1749 : 0.005272696726024151\n",
      "Training loss for batch 1750 : 0.008228689432144165\n",
      "Training loss for batch 1751 : 0.16154421865940094\n",
      "Training loss for batch 1752 : 0.24627737700939178\n",
      "Training loss for batch 1753 : 0.057923056185245514\n",
      "Training loss for batch 1754 : 0.07085520774126053\n",
      "Training loss for batch 1755 : 0.04670673981308937\n",
      "Training loss for batch 1756 : 0.06409081816673279\n",
      "Training loss for batch 1757 : 0.1474064439535141\n",
      "Training loss for batch 1758 : 0.18038874864578247\n",
      "Training loss for batch 1759 : 0.2637369632720947\n",
      "Training loss for batch 1760 : 0.18305064737796783\n",
      "Training loss for batch 1761 : 0.05586439371109009\n",
      "Training loss for batch 1762 : 0.14831234514713287\n",
      "Training loss for batch 1763 : 0.13671360909938812\n",
      "Training loss for batch 1764 : 0.11902984231710434\n",
      "Training loss for batch 1765 : 0.1630934327840805\n",
      "Training loss for batch 1766 : 0.04946575686335564\n",
      "Training loss for batch 1767 : 0.09846632182598114\n",
      "Training loss for batch 1768 : 0.3216695785522461\n",
      "Training loss for batch 1769 : 0.06713732331991196\n",
      "Training loss for batch 1770 : 0.007444371934980154\n",
      "Training loss for batch 1771 : 0.10596500337123871\n",
      "Training loss for batch 1772 : 0.005373348947614431\n",
      "Training loss for batch 1773 : 0.0402594693005085\n",
      "Training loss for batch 1774 : 0.014170569367706776\n",
      "Training loss for batch 1775 : 0.2779221534729004\n",
      "Training loss for batch 1776 : 0.21312230825424194\n",
      "Training loss for batch 1777 : 0.07305383682250977\n",
      "Training loss for batch 1778 : 0.2500797212123871\n",
      "Training loss for batch 1779 : 0.06322897970676422\n",
      "Training loss for batch 1780 : 0.03245394676923752\n",
      "Training loss for batch 1781 : 0.09379388391971588\n",
      "Training loss for batch 1782 : 0.17815911769866943\n",
      "Training loss for batch 1783 : 0.16307345032691956\n",
      "Training loss for batch 1784 : 0.1125367134809494\n",
      "Training loss for batch 1785 : 0.2038191705942154\n",
      "Training loss for batch 1786 : 0.0\n",
      "Training loss for batch 1787 : 0.4649031162261963\n",
      "Training loss for batch 1788 : 0.0676908940076828\n",
      "Training loss for batch 1789 : 0.2427995353937149\n",
      "Training loss for batch 1790 : 0.14438268542289734\n",
      "Training loss for batch 1791 : 0.10922902822494507\n",
      "Training loss for batch 1792 : 0.1603962481021881\n",
      "Training loss for batch 1793 : 0.41404470801353455\n",
      "Training loss for batch 1794 : 0.07637637853622437\n",
      "Training loss for batch 1795 : 0.13324441015720367\n",
      "Training loss for batch 1796 : 0.20663222670555115\n",
      "Training loss for batch 1797 : 0.04164738953113556\n",
      "Training loss for batch 1798 : 0.08039718121290207\n",
      "Training loss for batch 1799 : 0.04619396850466728\n",
      "Training loss for batch 1800 : 0.15166467428207397\n",
      "Training loss for batch 1801 : 0.02098792791366577\n",
      "Training loss for batch 1802 : 0.07764962315559387\n",
      "Training loss for batch 1803 : 0.30319392681121826\n",
      "Training loss for batch 1804 : 0.009499279782176018\n",
      "Training loss for batch 1805 : 0.0340290293097496\n",
      "Training loss for batch 1806 : 0.11740957200527191\n",
      "Training loss for batch 1807 : 0.06687958538532257\n",
      "Training loss for batch 1808 : 0.21444129943847656\n",
      "Training loss for batch 1809 : 0.23877942562103271\n",
      "Training loss for batch 1810 : 0.15667438507080078\n",
      "Training loss for batch 1811 : 0.23339705169200897\n",
      "Training loss for batch 1812 : 0.5475077629089355\n",
      "Training loss for batch 1813 : 0.037974268198013306\n",
      "Training loss for batch 1814 : 0.06030074506998062\n",
      "Training loss for batch 1815 : 0.13605450093746185\n",
      "Training loss for batch 1816 : 0.14612016081809998\n",
      "Training loss for batch 1817 : 0.05285204201936722\n",
      "Training loss for batch 1818 : 0.06473353505134583\n",
      "Training loss for batch 1819 : 0.19167062640190125\n",
      "Training loss for batch 1820 : 0.05916580930352211\n",
      "Training loss for batch 1821 : 0.4406887888908386\n",
      "Training loss for batch 1822 : 0.0\n",
      "Training loss for batch 1823 : 0.02574041113257408\n",
      "Training loss for batch 1824 : 0.1438884437084198\n",
      "Training loss for batch 1825 : 0.05005934089422226\n",
      "Training loss for batch 1826 : 0.17914040386676788\n",
      "Training loss for batch 1827 : 0.0849774107336998\n",
      "Training loss for batch 1828 : 0.06437230110168457\n",
      "Training loss for batch 1829 : 0.18517571687698364\n",
      "Training loss for batch 1830 : 0.08159312605857849\n",
      "Training loss for batch 1831 : 0.16173647344112396\n",
      "Training loss for batch 1832 : 0.1408337503671646\n",
      "Training loss for batch 1833 : 0.16229334473609924\n",
      "Training loss for batch 1834 : 0.11268946528434753\n",
      "Training loss for batch 1835 : 0.03295358270406723\n",
      "Training loss for batch 1836 : 0.1834089457988739\n",
      "Training loss for batch 1837 : 0.014351153746247292\n",
      "Training loss for batch 1838 : 0.1848648637533188\n",
      "Training loss for batch 1839 : 0.04808757081627846\n",
      "Training loss for batch 1840 : 0.11013919860124588\n",
      "Training loss for batch 1841 : 0.18591147661209106\n",
      "Training loss for batch 1842 : 0.2063058316707611\n",
      "Training loss for batch 1843 : 0.14191094040870667\n",
      "Training loss for batch 1844 : 0.09741062670946121\n",
      "Training loss for batch 1845 : 0.20609766244888306\n",
      "Training loss for batch 1846 : 0.05874284729361534\n",
      "Training loss for batch 1847 : 0.010897630825638771\n",
      "Training loss for batch 1848 : 0.2135176956653595\n",
      "Training loss for batch 1849 : 0.10526391863822937\n",
      "Training loss for batch 1850 : 0.03297043964266777\n",
      "Training loss for batch 1851 : 0.19218385219573975\n",
      "Training loss for batch 1852 : 0.13686583936214447\n",
      "Training loss for batch 1853 : 0.28500640392303467\n",
      "Training loss for batch 1854 : 0.33841636776924133\n",
      "Training loss for batch 1855 : 0.12505269050598145\n",
      "Training loss for batch 1856 : 0.00619814358651638\n",
      "Training loss for batch 1857 : 0.06070664897561073\n",
      "Training loss for batch 1858 : 9.999226313084364e-05\n",
      "Training loss for batch 1859 : 0.15014085173606873\n",
      "Training loss for batch 1860 : 0.019405510276556015\n",
      "Training loss for batch 1861 : 0.06022578477859497\n",
      "Training loss for batch 1862 : 0.195654958486557\n",
      "Training loss for batch 1863 : 0.06789334118366241\n",
      "Training loss for batch 1864 : 0.10451828688383102\n",
      "Training loss for batch 1865 : 0.114397794008255\n",
      "Training loss for batch 1866 : 0.19621339440345764\n",
      "Training loss for batch 1867 : 0.1749364733695984\n",
      "Training loss for batch 1868 : 0.17759346961975098\n",
      "Training loss for batch 1869 : 0.01802261546254158\n",
      "Training loss for batch 1870 : 0.10121805965900421\n",
      "Training loss for batch 1871 : 0.3364463746547699\n",
      "Training loss for batch 1872 : -0.0005738364416174591\n",
      "Training loss for batch 1873 : 0.16972649097442627\n",
      "Training loss for batch 1874 : 0.009439246729016304\n",
      "Training loss for batch 1875 : 0.023366779088974\n",
      "Training loss for batch 1876 : 0.18850049376487732\n",
      "Training loss for batch 1877 : 0.17705373466014862\n",
      "Training loss for batch 1878 : 0.023154862225055695\n",
      "Training loss for batch 1879 : 0.13626055419445038\n",
      "Training loss for batch 1880 : 0.16624805331230164\n",
      "Training loss for batch 1881 : -0.0037650985177606344\n",
      "Training loss for batch 1882 : 0.22934424877166748\n",
      "Training loss for batch 1883 : 0.03199855983257294\n",
      "Training loss for batch 1884 : 0.07160256803035736\n",
      "Training loss for batch 1885 : 0.16840854287147522\n",
      "Training loss for batch 1886 : 0.014759479090571404\n",
      "Training loss for batch 1887 : 0.3409138023853302\n",
      "Training loss for batch 1888 : 0.08206769078969955\n",
      "Training loss for batch 1889 : 0.006371960509568453\n",
      "Training loss for batch 1890 : 0.29725411534309387\n",
      "Training loss for batch 1891 : 0.21871012449264526\n",
      "Training loss for batch 1892 : 0.07257753610610962\n",
      "Training loss for batch 1893 : 0.010791301727294922\n",
      "Training loss for batch 1894 : 0.08611702919006348\n",
      "Training loss for batch 1895 : 0.24557186663150787\n",
      "Training loss for batch 1896 : 0.08422685414552689\n",
      "Training loss for batch 1897 : 0.41473379731178284\n",
      "Training loss for batch 1898 : 0.2544635236263275\n",
      "Training loss for batch 1899 : 0.0\n",
      "Training loss for batch 1900 : 0.1950559914112091\n",
      "Training loss for batch 1901 : 0.13395951688289642\n",
      "Training loss for batch 1902 : 0.1643393635749817\n",
      "Training loss for batch 1903 : 0.23109163343906403\n",
      "Training loss for batch 1904 : 0.04578258842229843\n",
      "Training loss for batch 1905 : 0.004405529238283634\n",
      "Training loss for batch 1906 : 0.014470466412603855\n",
      "Training loss for batch 1907 : 0.030224837362766266\n",
      "Training loss for batch 1908 : 0.10346941649913788\n",
      "Training loss for batch 1909 : 0.008524736389517784\n",
      "Training loss for batch 1910 : 0.05714452639222145\n",
      "Training loss for batch 1911 : 0.025968478992581367\n",
      "Training loss for batch 1912 : 0.11143888533115387\n",
      "Training loss for batch 1913 : 0.02636999450623989\n",
      "Training loss for batch 1914 : 0.0910269096493721\n",
      "Training loss for batch 1915 : 0.1982516050338745\n",
      "Training loss for batch 1916 : 0.11604178696870804\n",
      "Training loss for batch 1917 : 0.0894370749592781\n",
      "Training loss for batch 1918 : 0.22128984332084656\n",
      "Training loss for batch 1919 : 0.10814520716667175\n",
      "Training loss for batch 1920 : 0.2469724863767624\n",
      "Training loss for batch 1921 : 0.04429924115538597\n",
      "Training loss for batch 1922 : 0.1471700221300125\n",
      "Training loss for batch 1923 : 0.06345215439796448\n",
      "Training loss for batch 1924 : 0.02477806806564331\n",
      "Training loss for batch 1925 : 0.039632149040699005\n",
      "Training loss for batch 1926 : 0.06539709120988846\n",
      "Training loss for batch 1927 : 0.007657001726329327\n",
      "Training loss for batch 1928 : 0.010403256863355637\n",
      "Training loss for batch 1929 : 0.07861948758363724\n",
      "Training loss for batch 1930 : 0.039055533707141876\n",
      "Training loss for batch 1931 : 0.12579578161239624\n",
      "Training loss for batch 1932 : 0.003061340656131506\n",
      "Training loss for batch 1933 : 0.01995747536420822\n",
      "Training loss for batch 1934 : 0.0860903337597847\n",
      "Training loss for batch 1935 : 0.030474524945020676\n",
      "Training loss for batch 1936 : 0.019136004149913788\n",
      "Training loss for batch 1937 : 0.19938799738883972\n",
      "Training loss for batch 1938 : 0.1937851756811142\n",
      "Training loss for batch 1939 : 0.08891215175390244\n",
      "Training loss for batch 1940 : -0.0012850039638578892\n",
      "Training loss for batch 1941 : 0.10875724256038666\n",
      "Training loss for batch 1942 : 0.14158524572849274\n",
      "Training loss for batch 1943 : 0.04886021092534065\n",
      "Training loss for batch 1944 : 0.0142811955884099\n",
      "Training loss for batch 1945 : 0.07893562316894531\n",
      "Training loss for batch 1946 : 0.52995765209198\n",
      "Training loss for batch 1947 : 0.05016469210386276\n",
      "Training loss for batch 1948 : 0.015592114999890327\n",
      "Training loss for batch 1949 : 0.1284228265285492\n",
      "Training loss for batch 1950 : 0.19300691783428192\n",
      "Training loss for batch 1951 : 0.11876709759235382\n",
      "Training loss for batch 1952 : 0.03131719306111336\n",
      "Training loss for batch 1953 : 0.1427587866783142\n",
      "Training loss for batch 1954 : 0.0\n",
      "Training loss for batch 1955 : 0.1376122236251831\n",
      "Training loss for batch 1956 : 0.15070009231567383\n",
      "Training loss for batch 1957 : 0.06565812230110168\n",
      "Training loss for batch 1958 : -0.0018966075731441379\n",
      "Training loss for batch 1959 : 0.2860645055770874\n",
      "Training loss for batch 1960 : 0.15800192952156067\n",
      "Training loss for batch 1961 : 0.11807185411453247\n",
      "Training loss for batch 1962 : 0.06935357302427292\n",
      "Training loss for batch 1963 : 0.02513916790485382\n",
      "Training loss for batch 1964 : 0.03255331143736839\n",
      "Training loss for batch 1965 : 0.060732074081897736\n",
      "Training loss for batch 1966 : 0.25610196590423584\n",
      "Training loss for batch 1967 : 0.1632813960313797\n",
      "Training loss for batch 1968 : 0.009841013699769974\n",
      "Training loss for batch 1969 : 0.2864401638507843\n",
      "Training loss for batch 1970 : 0.11805209517478943\n",
      "Training loss for batch 1971 : 0.14645478129386902\n",
      "Training loss for batch 1972 : 0.01055860798805952\n",
      "Training loss for batch 1973 : 0.024749111384153366\n",
      "Training loss for batch 1974 : 0.05523456633090973\n",
      "Training loss for batch 1975 : 0.08141852915287018\n",
      "Training loss for batch 1976 : 0.04604935273528099\n",
      "Training loss for batch 1977 : 0.051499709486961365\n",
      "Training loss for batch 1978 : 0.022892698645591736\n",
      "Training loss for batch 1979 : 0.14022473990917206\n",
      "Training loss for batch 1980 : 0.25523361563682556\n",
      "Training loss for batch 1981 : 0.1818762868642807\n",
      "Training loss for batch 1982 : 0.23445060849189758\n",
      "Training loss for batch 1983 : 0.2896274924278259\n",
      "Training loss for batch 1984 : 0.001786559820175171\n",
      "Training loss for batch 1985 : 0.07180412113666534\n",
      "Training loss for batch 1986 : 0.3826223313808441\n",
      "Training loss for batch 1987 : 0.3586658537387848\n",
      "Training loss for batch 1988 : 0.1502322107553482\n",
      "Training loss for batch 1989 : 0.13301105797290802\n",
      "Training loss for batch 1990 : 0.06747853755950928\n",
      "Training loss for batch 1991 : 0.16363216936588287\n",
      "Training loss for batch 1992 : 0.16423434019088745\n",
      "Training loss for batch 1993 : 0.11021599173545837\n",
      "Training loss for batch 1994 : 0.18145208060741425\n",
      "Training loss for batch 1995 : 0.046320851892232895\n",
      "Training loss for batch 1996 : 0.1828094720840454\n",
      "Training loss for batch 1997 : 0.25938165187835693\n",
      "Training loss for batch 1998 : 0.21189992129802704\n",
      "Training loss for batch 1999 : 0.2704363465309143\n",
      "Training loss for batch 2000 : 0.1347827911376953\n",
      "Training loss for batch 2001 : 0.3310104012489319\n",
      "Training loss for batch 2002 : 0.07258894294500351\n",
      "Training loss for batch 2003 : 0.19566121697425842\n",
      "Training loss for batch 2004 : 0.15038394927978516\n",
      "Training loss for batch 2005 : 0.25034642219543457\n",
      "Training loss for batch 2006 : 0.005967400968074799\n",
      "Training loss for batch 2007 : 0.1654055118560791\n",
      "Training loss for batch 2008 : 0.08085835725069046\n",
      "Training loss for batch 2009 : 0.2186506688594818\n",
      "Training loss for batch 2010 : 0.03620218113064766\n",
      "Training loss for batch 2011 : 0.6332064867019653\n",
      "Training loss for batch 2012 : 0.2552172839641571\n",
      "Training loss for batch 2013 : 0.1974876970052719\n",
      "Training loss for batch 2014 : 0.17159779369831085\n",
      "Training loss for batch 2015 : 0.11203945428133011\n",
      "Training loss for batch 2016 : 0.15740209817886353\n",
      "Training loss for batch 2017 : 0.0\n",
      "Training loss for batch 2018 : 0.03907594084739685\n",
      "Training loss for batch 2019 : 0.22309674322605133\n",
      "Training loss for batch 2020 : 0.03457331284880638\n",
      "Training loss for batch 2021 : 0.14448319375514984\n",
      "Training loss for batch 2022 : 0.12772130966186523\n",
      "Training loss for batch 2023 : 0.1284591108560562\n",
      "Training loss for batch 2024 : 0.22817158699035645\n",
      "Training loss for batch 2025 : 0.34248799085617065\n",
      "Training loss for batch 2026 : 0.05339646711945534\n",
      "Training loss for batch 2027 : 0.09635979682207108\n",
      "Training loss for batch 2028 : 0.11673741042613983\n",
      "Training loss for batch 2029 : 0.07358536869287491\n",
      "Training loss for batch 2030 : 0.002652677707374096\n",
      "Training loss for batch 2031 : 0.04887792840600014\n",
      "Training loss for batch 2032 : 0.1870017647743225\n",
      "Training loss for batch 2033 : 0.15271639823913574\n",
      "Training loss for batch 2034 : 0.26579439640045166\n",
      "Training loss for batch 2035 : 0.2176389992237091\n",
      "Training loss for batch 2036 : 0.13311618566513062\n",
      "Training loss for batch 2037 : 0.41212326288223267\n",
      "Training loss for batch 2038 : 0.21687652170658112\n",
      "Training loss for batch 2039 : 0.0010110661387443542\n",
      "Training loss for batch 2040 : 0.04787813872098923\n",
      "Training loss for batch 2041 : 0.16275174915790558\n",
      "Training loss for batch 2042 : 0.2316979467868805\n",
      "Training loss for batch 2043 : 0.2536177337169647\n",
      "Training loss for batch 2044 : 0.16431337594985962\n",
      "Training loss for batch 2045 : 0.19585171341896057\n",
      "Training loss for batch 2046 : 0.235069140791893\n",
      "Training loss for batch 2047 : 0.15122893452644348\n",
      "Training loss for batch 2048 : 0.053352124989032745\n",
      "Training loss for batch 2049 : 0.10336571931838989\n",
      "Training loss for batch 2050 : 0.025134433060884476\n",
      "Training loss for batch 2051 : 0.14208614826202393\n",
      "Training loss for batch 2052 : 0.01818038523197174\n",
      "Training loss for batch 2053 : 0.15097416937351227\n",
      "Training loss for batch 2054 : 0.023082148283720016\n",
      "Training loss for batch 2055 : 0.2843097448348999\n",
      "Training loss for batch 2056 : 0.11743100732564926\n",
      "Training loss for batch 2057 : 0.047007665038108826\n",
      "Training loss for batch 2058 : 0.22939328849315643\n",
      "Training loss for batch 2059 : 0.12243971973657608\n",
      "Training loss for batch 2060 : 0.12810172140598297\n",
      "Training loss for batch 2061 : 0.03002404049038887\n",
      "Training loss for batch 2062 : 0.23430660367012024\n",
      "Training loss for batch 2063 : 0.26264193654060364\n",
      "Training loss for batch 2064 : 0.23798269033432007\n",
      "Training loss for batch 2065 : 0.15317898988723755\n",
      "Training loss for batch 2066 : 0.006772307679057121\n",
      "Training loss for batch 2067 : 0.1769237071275711\n",
      "Training loss for batch 2068 : 0.009615005925297737\n",
      "Training loss for batch 2069 : 0.09935508668422699\n",
      "Training loss for batch 2070 : 0.04835508018732071\n",
      "Training loss for batch 2071 : 0.0811077207326889\n",
      "Training loss for batch 2072 : 0.14469540119171143\n",
      "Training loss for batch 2073 : 0.3193695843219757\n",
      "Training loss for batch 2074 : 0.06474423408508301\n",
      "Training loss for batch 2075 : 0.059422947466373444\n",
      "Training loss for batch 2076 : 0.2665117084980011\n",
      "Training loss for batch 2077 : 0.1112292930483818\n",
      "Training loss for batch 2078 : 0.06075487285852432\n",
      "Training loss for batch 2079 : 0.040134795010089874\n",
      "Training loss for batch 2080 : 0.10223472118377686\n",
      "Training loss for batch 2081 : 0.01644202135503292\n",
      "Training loss for batch 2082 : 0.2381039708852768\n",
      "Training loss for batch 2083 : 0.12195505201816559\n",
      "Training loss for batch 2084 : 0.25379517674446106\n",
      "Training loss for batch 2085 : 0.05671113729476929\n",
      "Training loss for batch 2086 : 0.1048448458313942\n",
      "Training loss for batch 2087 : 0.021308166906237602\n",
      "Training loss for batch 2088 : 0.22103722393512726\n",
      "Training loss for batch 2089 : 0.2918950319290161\n",
      "Training loss for batch 2090 : 0.2243669033050537\n",
      "Training loss for batch 2091 : 0.18655243515968323\n",
      "Training loss for batch 2092 : 0.12963669002056122\n",
      "Training loss for batch 2093 : 0.005675471853464842\n",
      "Training loss for batch 2094 : 0.17117083072662354\n",
      "Training loss for batch 2095 : 0.0\n",
      "Training loss for batch 2096 : 0.07978294789791107\n",
      "Training loss for batch 2097 : 0.25873059034347534\n",
      "Training loss for batch 2098 : 0.10836588591337204\n",
      "Training loss for batch 2099 : 0.20907290279865265\n",
      "Training loss for batch 2100 : 0.0854204073548317\n",
      "Training loss for batch 2101 : 0.2100829780101776\n",
      "Training loss for batch 2102 : 0.0504712350666523\n",
      "Training loss for batch 2103 : 0.14307548105716705\n",
      "Training loss for batch 2104 : 0.023231208324432373\n",
      "Training loss for batch 2105 : 0.33592385053634644\n",
      "Training loss for batch 2106 : 0.04626733437180519\n",
      "Training loss for batch 2107 : 0.15422704815864563\n",
      "Training loss for batch 2108 : 0.16823987662792206\n",
      "Training loss for batch 2109 : 0.051062434911727905\n",
      "Training loss for batch 2110 : 0.2491491138935089\n",
      "Training loss for batch 2111 : 0.04064220190048218\n",
      "Training loss for batch 2112 : 0.13416774570941925\n",
      "Training loss for batch 2113 : 0.18910370767116547\n",
      "Training loss for batch 2114 : 0.038384221494197845\n",
      "Training loss for batch 2115 : 0.042757414281368256\n",
      "Training loss for batch 2116 : 0.029082467779517174\n",
      "Training loss for batch 2117 : 0.06755878776311874\n",
      "Training loss for batch 2118 : 0.18480725586414337\n",
      "Training loss for batch 2119 : 0.20199142396450043\n",
      "Training loss for batch 2120 : 0.18542061746120453\n",
      "Training loss for batch 2121 : 0.20001651346683502\n",
      "Training loss for batch 2122 : 0.07653294503688812\n",
      "Training loss for batch 2123 : 0.20467987656593323\n",
      "Training loss for batch 2124 : 0.15768009424209595\n",
      "Training loss for batch 2125 : 0.04363185912370682\n",
      "Training loss for batch 2126 : 0.028636131435632706\n",
      "Training loss for batch 2127 : 0.029060781002044678\n",
      "Training loss for batch 2128 : 0.1320388913154602\n",
      "Training loss for batch 2129 : 0.17507992684841156\n",
      "Training loss for batch 2130 : 0.1018814742565155\n",
      "Training loss for batch 2131 : 0.3113803565502167\n",
      "Training loss for batch 2132 : 0.08508026599884033\n",
      "Training loss for batch 2133 : 0.059570737183094025\n",
      "Training loss for batch 2134 : 0.15384912490844727\n",
      "Training loss for batch 2135 : -0.0017122143181040883\n",
      "Training loss for batch 2136 : 0.18235769867897034\n",
      "Training loss for batch 2137 : 0.14820128679275513\n",
      "Training loss for batch 2138 : 0.12302559614181519\n",
      "Training loss for batch 2139 : 0.14181748032569885\n",
      "Training loss for batch 2140 : 0.1768464893102646\n",
      "Training loss for batch 2141 : 0.13349206745624542\n",
      "Training loss for batch 2142 : 0.057477399706840515\n",
      "Training loss for batch 2143 : 0.10692033916711807\n",
      "Training loss for batch 2144 : 0.022447774186730385\n",
      "Training loss for batch 2145 : 0.00933213159441948\n",
      "Training loss for batch 2146 : 0.16663305461406708\n",
      "Training loss for batch 2147 : 0.035335972905159\n",
      "Training loss for batch 2148 : 0.052619509398937225\n",
      "Training loss for batch 2149 : -0.0007594279013574123\n",
      "Training loss for batch 2150 : 0.13083656132221222\n",
      "Training loss for batch 2151 : 0.19296026229858398\n",
      "Training loss for batch 2152 : 0.03322524577379227\n",
      "Training loss for batch 2153 : 0.12979906797409058\n",
      "Training loss for batch 2154 : 0.040518395602703094\n",
      "Training loss for batch 2155 : 0.06353041529655457\n",
      "Training loss for batch 2156 : 0.15807181596755981\n",
      "Training loss for batch 2157 : 0.2154516875743866\n",
      "Training loss for batch 2158 : 0.26797163486480713\n",
      "Training loss for batch 2159 : 0.04517894983291626\n",
      "Training loss for batch 2160 : 0.08540192246437073\n",
      "Training loss for batch 2161 : 0.15876853466033936\n",
      "Training loss for batch 2162 : 0.08324617892503738\n",
      "Training loss for batch 2163 : 0.016662990674376488\n",
      "Training loss for batch 2164 : 0.2214040756225586\n",
      "Training loss for batch 2165 : 0.01946236751973629\n",
      "Training loss for batch 2166 : 0.18429328501224518\n",
      "Training loss for batch 2167 : 0.0368899330496788\n",
      "Training loss for batch 2168 : 0.12159913778305054\n",
      "Training loss for batch 2169 : 0.08242981135845184\n",
      "Training loss for batch 2170 : 0.13149073719978333\n",
      "Training loss for batch 2171 : 0.16600441932678223\n",
      "Training loss for batch 2172 : 0.1832510530948639\n",
      "Training loss for batch 2173 : 0.04402521997690201\n",
      "Training loss for batch 2174 : 0.30715492367744446\n",
      "Training loss for batch 2175 : 0.04086797684431076\n",
      "Training loss for batch 2176 : 0.09935349971055984\n",
      "Training loss for batch 2177 : 0.2387947142124176\n",
      "Training loss for batch 2178 : 0.0654793530702591\n",
      "Training loss for batch 2179 : 0.18320508301258087\n",
      "Training loss for batch 2180 : 0.06350201368331909\n",
      "Training loss for batch 2181 : 0.15897822380065918\n",
      "Training loss for batch 2182 : 0.06608891487121582\n",
      "Training loss for batch 2183 : 0.18351976573467255\n",
      "Training loss for batch 2184 : 0.1839555948972702\n",
      "Training loss for batch 2185 : 0.15721577405929565\n",
      "Training loss for batch 2186 : 0.09158290922641754\n",
      "Training loss for batch 2187 : 0.21555060148239136\n",
      "Training loss for batch 2188 : 0.08772426843643188\n",
      "Training loss for batch 2189 : 0.04942076653242111\n",
      "Training loss for batch 2190 : 0.10171958804130554\n",
      "Training loss for batch 2191 : 0.1142328679561615\n",
      "Training loss for batch 2192 : 0.14350034296512604\n",
      "Training loss for batch 2193 : 0.2196064591407776\n",
      "Training loss for batch 2194 : 0.17005997896194458\n",
      "Training loss for batch 2195 : 0.12126491218805313\n",
      "Training loss for batch 2196 : 0.12998543679714203\n",
      "Training loss for batch 2197 : 0.11228884756565094\n",
      "Training loss for batch 2198 : 0.022857891395688057\n",
      "Training loss for batch 2199 : 0.18675100803375244\n",
      "Training loss for batch 2200 : 0.3686390519142151\n",
      "Training loss for batch 2201 : 0.0752170979976654\n",
      "Training loss for batch 2202 : 0.0861678496003151\n",
      "Training loss for batch 2203 : 0.08250830322504044\n",
      "Training loss for batch 2204 : 0.02488514594733715\n",
      "Training loss for batch 2205 : 0.04217355698347092\n",
      "Training loss for batch 2206 : 0.08797022700309753\n",
      "Training loss for batch 2207 : 0.19545909762382507\n",
      "Training loss for batch 2208 : 0.18431715667247772\n",
      "Training loss for batch 2209 : 0.030270978808403015\n",
      "Training loss for batch 2210 : 0.15641474723815918\n",
      "Training loss for batch 2211 : 0.08897508680820465\n",
      "Training loss for batch 2212 : 0.2397887408733368\n",
      "Training loss for batch 2213 : 0.09856058657169342\n",
      "Training loss for batch 2214 : 0.13796739280223846\n",
      "Training loss for batch 2215 : 0.0\n",
      "Training loss for batch 2216 : 0.10990698635578156\n",
      "Training loss for batch 2217 : 0.024344857782125473\n",
      "Training loss for batch 2218 : 0.2685072422027588\n",
      "Training loss for batch 2219 : 0.06827452033758163\n",
      "Training loss for batch 2220 : 0.20883531868457794\n",
      "Training loss for batch 2221 : 0.19101011753082275\n",
      "Training loss for batch 2222 : 0.14308398962020874\n",
      "Training loss for batch 2223 : 0.19529663026332855\n",
      "Training loss for batch 2224 : 0.13108128309249878\n",
      "Training loss for batch 2225 : 0.14318403601646423\n",
      "Training loss for batch 2226 : 0.1036214753985405\n",
      "Training loss for batch 2227 : 0.08175784349441528\n",
      "Training loss for batch 2228 : 0.03215840831398964\n",
      "Training loss for batch 2229 : 0.0906519964337349\n",
      "Training loss for batch 2230 : 0.11358486860990524\n",
      "Training loss for batch 2231 : 0.2320556640625\n",
      "Training loss for batch 2232 : 0.16428707540035248\n",
      "Training loss for batch 2233 : 0.11054286360740662\n",
      "Training loss for batch 2234 : 0.10856419056653976\n",
      "Training loss for batch 2235 : 0.10115299373865128\n",
      "Training loss for batch 2236 : 0.02068592607975006\n",
      "Training loss for batch 2237 : 0.04200666397809982\n",
      "Training loss for batch 2238 : 0.275460422039032\n",
      "Training loss for batch 2239 : 0.0\n",
      "Training loss for batch 2240 : 0.39950573444366455\n",
      "Training loss for batch 2241 : 0.268086314201355\n",
      "Training loss for batch 2242 : 0.031617771834135056\n",
      "Training loss for batch 2243 : 0.08765263855457306\n",
      "Training loss for batch 2244 : 0.1557673066854477\n",
      "Training loss for batch 2245 : 0.2298728972673416\n",
      "Training loss for batch 2246 : -0.005992496851831675\n",
      "Training loss for batch 2247 : 0.18914717435836792\n",
      "Training loss for batch 2248 : 0.12671589851379395\n",
      "Training loss for batch 2249 : 0.08572021126747131\n",
      "Training loss for batch 2250 : 0.2131117433309555\n",
      "Training loss for batch 2251 : 0.15551188588142395\n",
      "Training loss for batch 2252 : 0.19890981912612915\n",
      "Training loss for batch 2253 : 0.15124954283237457\n",
      "Training loss for batch 2254 : 0.20468579232692719\n",
      "Training loss for batch 2255 : 0.29587826132774353\n",
      "Training loss for batch 2256 : 0.02281206101179123\n",
      "Training loss for batch 2257 : 0.007737217005342245\n",
      "Training loss for batch 2258 : 0.05282113701105118\n",
      "Training loss for batch 2259 : 0.10682989656925201\n",
      "Training loss for batch 2260 : 0.060935601592063904\n",
      "Training loss for batch 2261 : 0.18980355560779572\n",
      "Training loss for batch 2262 : 0.0989498496055603\n",
      "Training loss for batch 2263 : -0.0004160180687904358\n",
      "Training loss for batch 2264 : 0.044929880648851395\n",
      "Training loss for batch 2265 : 0.039865486323833466\n",
      "Training loss for batch 2266 : 0.015453517436981201\n",
      "Training loss for batch 2267 : 0.2358018010854721\n",
      "Training loss for batch 2268 : 0.274264931678772\n",
      "Training loss for batch 2269 : 0.044435907155275345\n",
      "Training loss for batch 2270 : 0.036862052977085114\n",
      "Training loss for batch 2271 : 0.02949337288737297\n",
      "Training loss for batch 2272 : 0.17084553837776184\n",
      "Training loss for batch 2273 : 0.43337157368659973\n",
      "Training loss for batch 2274 : 0.08363935351371765\n",
      "Training loss for batch 2275 : 0.04443807899951935\n",
      "Training loss for batch 2276 : 0.24549317359924316\n",
      "Training loss for batch 2277 : 0.04505212604999542\n",
      "Training loss for batch 2278 : 0.17969529330730438\n",
      "Training loss for batch 2279 : 0.02302437648177147\n",
      "Training loss for batch 2280 : 0.07400088757276535\n",
      "Training loss for batch 2281 : 0.026867378503084183\n",
      "Training loss for batch 2282 : 0.17983411252498627\n",
      "Training loss for batch 2283 : 0.032940685749053955\n",
      "Training loss for batch 2284 : 0.09707315266132355\n",
      "Training loss for batch 2285 : 0.034917883574962616\n",
      "Training loss for batch 2286 : 0.19441582262516022\n",
      "Training loss for batch 2287 : 0.20350134372711182\n",
      "Training loss for batch 2288 : 0.22369670867919922\n",
      "Training loss for batch 2289 : 0.07312558591365814\n",
      "Training loss for batch 2290 : 0.06705630570650101\n",
      "Training loss for batch 2291 : 0.076697938144207\n",
      "Training loss for batch 2292 : 0.0505911260843277\n",
      "Training loss for batch 2293 : 0.0987720787525177\n",
      "Training loss for batch 2294 : 0.03258959576487541\n",
      "Training loss for batch 2295 : 0.046406861394643784\n",
      "Training loss for batch 2296 : 0.10714881867170334\n",
      "Training loss for batch 2297 : 0.18284086883068085\n",
      "Training loss for batch 2298 : 0.05444187670946121\n",
      "Training loss for batch 2299 : 0.0695732906460762\n",
      "Training loss for batch 2300 : 0.36621513962745667\n",
      "Training loss for batch 2301 : 0.06452314555644989\n",
      "Training loss for batch 2302 : -0.0015553986886516213\n",
      "Training loss for batch 2303 : 0.21933463215827942\n",
      "Training loss for batch 2304 : 0.07271480560302734\n",
      "Training loss for batch 2305 : 0.2966884970664978\n",
      "Training loss for batch 2306 : 0.09221217036247253\n",
      "Training loss for batch 2307 : 0.06890307366847992\n",
      "Training loss for batch 2308 : 0.0697336345911026\n",
      "Training loss for batch 2309 : 0.04310980066657066\n",
      "Training loss for batch 2310 : 0.0\n",
      "Training loss for batch 2311 : 0.017673326656222343\n",
      "Training loss for batch 2312 : 0.07902226597070694\n",
      "Training loss for batch 2313 : 0.1325097233057022\n",
      "Training loss for batch 2314 : 0.14332270622253418\n",
      "Training loss for batch 2315 : 0.08138998597860336\n",
      "Training loss for batch 2316 : 0.148689866065979\n",
      "Training loss for batch 2317 : 0.0\n",
      "Training loss for batch 2318 : 0.1459675133228302\n",
      "Training loss for batch 2319 : 0.06790374219417572\n",
      "Training loss for batch 2320 : 0.16354334354400635\n",
      "Training loss for batch 2321 : 0.027870073914527893\n",
      "Training loss for batch 2322 : 0.17975680530071259\n",
      "Training loss for batch 2323 : 0.15628603100776672\n",
      "Training loss for batch 2324 : 0.08824364840984344\n",
      "Training loss for batch 2325 : 0.05318772792816162\n",
      "Training loss for batch 2326 : 0.0018419455736875534\n",
      "Training loss for batch 2327 : 0.2285594344139099\n",
      "Training loss for batch 2328 : 0.3918113708496094\n",
      "Training loss for batch 2329 : 0.2549554705619812\n",
      "Training loss for batch 2330 : 0.13617783784866333\n",
      "Training loss for batch 2331 : 0.44337254762649536\n",
      "Training loss for batch 2332 : 0.2027650624513626\n",
      "Training loss for batch 2333 : 0.20888416469097137\n",
      "Training loss for batch 2334 : 0.08156978338956833\n",
      "Training loss for batch 2335 : 0.007154405117034912\n",
      "Training loss for batch 2336 : 0.06173393875360489\n",
      "Training loss for batch 2337 : 0.32656627893447876\n",
      "Training loss for batch 2338 : 0.039052218198776245\n",
      "Training loss for batch 2339 : 0.04123479127883911\n",
      "Training loss for batch 2340 : 0.04603511840105057\n",
      "Training loss for batch 2341 : 0.13548055291175842\n",
      "Training loss for batch 2342 : 0.022062869742512703\n",
      "Training loss for batch 2343 : 0.26919078826904297\n",
      "Training loss for batch 2344 : 0.09603118896484375\n",
      "Training loss for batch 2345 : 0.1701524555683136\n",
      "Training loss for batch 2346 : 0.03573688119649887\n",
      "Training loss for batch 2347 : 0.06490222364664078\n",
      "Training loss for batch 2348 : 0.19667375087738037\n",
      "Training loss for batch 2349 : 0.17914795875549316\n",
      "Training loss for batch 2350 : 0.16269510984420776\n",
      "Training loss for batch 2351 : 0.10407525300979614\n",
      "Training loss for batch 2352 : 0.09694702923297882\n",
      "Training loss for batch 2353 : 0.22781994938850403\n",
      "Training loss for batch 2354 : 0.006533724255859852\n",
      "Training loss for batch 2355 : 0.16225238144397736\n",
      "Training loss for batch 2356 : 0.17096909880638123\n",
      "Training loss for batch 2357 : 0.15805143117904663\n",
      "Training loss for batch 2358 : 0.09670614451169968\n",
      "Training loss for batch 2359 : 0.04352053254842758\n",
      "Training loss for batch 2360 : 0.1764523684978485\n",
      "Training loss for batch 2361 : 0.09703122824430466\n",
      "Training loss for batch 2362 : 0.19221650063991547\n",
      "Training loss for batch 2363 : 0.036540284752845764\n",
      "Training loss for batch 2364 : 0.016495877876877785\n",
      "Training loss for batch 2365 : 0.013460670597851276\n",
      "Training loss for batch 2366 : 0.026203036308288574\n",
      "Training loss for batch 2367 : 0.06709089875221252\n",
      "Training loss for batch 2368 : 0.005490690469741821\n",
      "Training loss for batch 2369 : 0.004752200562506914\n",
      "Training loss for batch 2370 : 0.11572349816560745\n",
      "Training loss for batch 2371 : 0.18686680495738983\n",
      "Training loss for batch 2372 : 0.10845696181058884\n",
      "Training loss for batch 2373 : 0.1458313763141632\n",
      "Training loss for batch 2374 : 0.04138590395450592\n",
      "Training loss for batch 2375 : 0.026111029088497162\n",
      "Training loss for batch 2376 : 0.17064937949180603\n",
      "Training loss for batch 2377 : 0.010429606772959232\n",
      "Training loss for batch 2378 : 0.0387800857424736\n",
      "Training loss for batch 2379 : 0.09479113668203354\n",
      "Training loss for batch 2380 : 0.15887442231178284\n",
      "Training loss for batch 2381 : 0.09042128175497055\n",
      "Training loss for batch 2382 : 0.1077359989285469\n",
      "Training loss for batch 2383 : 0.224103182554245\n",
      "Training loss for batch 2384 : 0.18798828125\n",
      "Training loss for batch 2385 : 0.038015756756067276\n",
      "Training loss for batch 2386 : 0.01768517680466175\n",
      "Training loss for batch 2387 : 0.04591934382915497\n",
      "Training loss for batch 2388 : 0.1284334361553192\n",
      "Training loss for batch 2389 : 0.036619849503040314\n",
      "Training loss for batch 2390 : 0.10107257217168808\n",
      "Training loss for batch 2391 : 0.12454157322645187\n",
      "Training loss for batch 2392 : 0.027667008340358734\n",
      "Training loss for batch 2393 : 0.26133573055267334\n",
      "Training loss for batch 2394 : 0.29823175072669983\n",
      "Training loss for batch 2395 : 0.20027241110801697\n",
      "Training loss for batch 2396 : 0.0362347848713398\n",
      "Training loss for batch 2397 : 0.14165596663951874\n",
      "Training loss for batch 2398 : 0.16975069046020508\n",
      "Training loss for batch 2399 : 0.05695272982120514\n",
      "Training loss for batch 2400 : 0.08144581317901611\n",
      "Training loss for batch 2401 : 0.10743793845176697\n",
      "Training loss for batch 2402 : 0.08838236331939697\n",
      "Training loss for batch 2403 : 0.03167818859219551\n",
      "Training loss for batch 2404 : 0.0473843514919281\n",
      "Training loss for batch 2405 : 0.019288387149572372\n",
      "Training loss for batch 2406 : 0.07947594672441483\n",
      "Training loss for batch 2407 : 0.036017291247844696\n",
      "Training loss for batch 2408 : 0.024351174011826515\n",
      "Training loss for batch 2409 : 0.111086905002594\n",
      "Training loss for batch 2410 : 0.015761297196149826\n",
      "Training loss for batch 2411 : 0.07135387510061264\n",
      "Training loss for batch 2412 : 0.07619430869817734\n",
      "Training loss for batch 2413 : 0.04816381633281708\n",
      "Training loss for batch 2414 : 0.06642739474773407\n",
      "Training loss for batch 2415 : 0.1812071055173874\n",
      "Training loss for batch 2416 : 0.16611750423908234\n",
      "Training loss for batch 2417 : 0.06436161696910858\n",
      "Training loss for batch 2418 : 0.21470531821250916\n",
      "Training loss for batch 2419 : 0.14694680273532867\n",
      "Training loss for batch 2420 : 0.053840383887290955\n",
      "Training loss for batch 2421 : 0.09986801445484161\n",
      "Training loss for batch 2422 : 0.3958330452442169\n",
      "Training loss for batch 2423 : 0.25544974207878113\n",
      "Training loss for batch 2424 : 0.14251115918159485\n",
      "Training loss for batch 2425 : 0.30090218782424927\n",
      "Training loss for batch 2426 : 0.165740504860878\n",
      "Training loss for batch 2427 : 0.22271330654621124\n",
      "Training loss for batch 2428 : 0.13820978999137878\n",
      "Training loss for batch 2429 : -0.0024555684067308903\n",
      "Training loss for batch 2430 : 0.05195608735084534\n",
      "Training loss for batch 2431 : 0.09663893282413483\n",
      "Training loss for batch 2432 : 0.05107077583670616\n",
      "Training loss for batch 2433 : 0.07808658480644226\n",
      "Training loss for batch 2434 : 0.22333857417106628\n",
      "Training loss for batch 2435 : 0.08810532838106155\n",
      "Training loss for batch 2436 : 0.4023151397705078\n",
      "Training loss for batch 2437 : 0.05839316174387932\n",
      "Training loss for batch 2438 : 0.04521363973617554\n",
      "Training loss for batch 2439 : 0.03760657459497452\n",
      "Training loss for batch 2440 : 0.19654923677444458\n",
      "Training loss for batch 2441 : 0.22657644748687744\n",
      "Training loss for batch 2442 : 0.3026036024093628\n",
      "Training loss for batch 2443 : 0.16517534852027893\n",
      "Training loss for batch 2444 : 0.14785587787628174\n",
      "Training loss for batch 2445 : 0.0\n",
      "Training loss for batch 2446 : 0.13900557160377502\n",
      "Training loss for batch 2447 : 0.005348700098693371\n",
      "Training loss for batch 2448 : 0.04506010562181473\n",
      "Training loss for batch 2449 : 0.05984855815768242\n",
      "Training loss for batch 2450 : 0.08729130774736404\n",
      "Training loss for batch 2451 : 0.07600820064544678\n",
      "Training loss for batch 2452 : 0.05704115703701973\n",
      "Training loss for batch 2453 : 0.16308946907520294\n",
      "Training loss for batch 2454 : 0.17508772015571594\n",
      "Training loss for batch 2455 : 0.06229877471923828\n",
      "Training loss for batch 2456 : 0.2406945526599884\n",
      "Training loss for batch 2457 : 0.16651469469070435\n",
      "Training loss for batch 2458 : 0.014921218156814575\n",
      "Training loss for batch 2459 : 0.020885877311229706\n",
      "Training loss for batch 2460 : 0.06274586915969849\n",
      "Training loss for batch 2461 : 0.08823119103908539\n",
      "Training loss for batch 2462 : 0.041950177401304245\n",
      "Training loss for batch 2463 : 0.019832685589790344\n",
      "Training loss for batch 2464 : 0.14355941116809845\n",
      "Training loss for batch 2465 : 0.027621690183877945\n",
      "Training loss for batch 2466 : 0.06389863789081573\n",
      "Training loss for batch 2467 : 0.1403399109840393\n",
      "Training loss for batch 2468 : 0.15830077230930328\n",
      "Training loss for batch 2469 : 0.05672995001077652\n",
      "Training loss for batch 2470 : 0.16538089513778687\n",
      "Training loss for batch 2471 : 0.22814644873142242\n",
      "Training loss for batch 2472 : 0.3315807580947876\n",
      "Training loss for batch 2473 : 0.12293118238449097\n",
      "Training loss for batch 2474 : 0.3600405156612396\n",
      "Training loss for batch 2475 : 0.19761775434017181\n",
      "Training loss for batch 2476 : 0.12000179290771484\n",
      "Training loss for batch 2477 : 0.16501620411872864\n",
      "Training loss for batch 2478 : 0.2332998365163803\n",
      "Training loss for batch 2479 : 0.24107789993286133\n",
      "Training loss for batch 2480 : 0.04157131537795067\n",
      "Training loss for batch 2481 : 0.0006021857261657715\n",
      "Training loss for batch 2482 : 0.020084302872419357\n",
      "Training loss for batch 2483 : 0.013813188299536705\n",
      "Training loss for batch 2484 : 0.08601442724466324\n",
      "Training loss for batch 2485 : 0.06573264300823212\n",
      "Training loss for batch 2486 : 0.08420261740684509\n",
      "Training loss for batch 2487 : 0.14851725101470947\n",
      "Training loss for batch 2488 : 0.1781076192855835\n",
      "Training loss for batch 2489 : 0.26194438338279724\n",
      "Training loss for batch 2490 : -0.00036936713149771094\n",
      "Training loss for batch 2491 : 0.08158622682094574\n",
      "Training loss for batch 2492 : 0.15798354148864746\n",
      "Training loss for batch 2493 : 0.16751955449581146\n",
      "Training loss for batch 2494 : 0.0\n",
      "Training loss for batch 2495 : 0.0\n",
      "Training loss for batch 2496 : 0.1592295914888382\n",
      "Training loss for batch 2497 : 0.17300239205360413\n",
      "Training loss for batch 2498 : 0.09088578820228577\n",
      "Training loss for batch 2499 : 0.0\n",
      "Training loss for batch 2500 : 0.1354209929704666\n",
      "Training loss for batch 2501 : 0.025106165558099747\n",
      "Training loss for batch 2502 : 0.013571585528552532\n",
      "Training loss for batch 2503 : 0.0698855072259903\n",
      "Training loss for batch 2504 : 0.20735782384872437\n",
      "Training loss for batch 2505 : 0.026703398674726486\n",
      "Training loss for batch 2506 : 0.13715630769729614\n",
      "Training loss for batch 2507 : 0.06815449148416519\n",
      "Training loss for batch 2508 : 0.1456630975008011\n",
      "Training loss for batch 2509 : 0.21367299556732178\n",
      "Training loss for batch 2510 : 0.016575312241911888\n",
      "Training loss for batch 2511 : 0.08429461717605591\n",
      "Training loss for batch 2512 : 0.03703323006629944\n",
      "Training loss for batch 2513 : 0.2785864472389221\n",
      "Training loss for batch 2514 : 0.13376253843307495\n",
      "Training loss for batch 2515 : 0.26384326815605164\n",
      "Training loss for batch 2516 : 0.20578262209892273\n",
      "Training loss for batch 2517 : 0.038926996290683746\n",
      "Training loss for batch 2518 : 0.035360485315322876\n",
      "Training loss for batch 2519 : 0.3903577923774719\n",
      "Training loss for batch 2520 : -9.967380901798606e-05\n",
      "Training loss for batch 2521 : 0.1348559856414795\n",
      "Training loss for batch 2522 : 0.05451219528913498\n",
      "Training loss for batch 2523 : 0.030963987112045288\n",
      "Training loss for batch 2524 : 0.06354133784770966\n",
      "Training loss for batch 2525 : 0.00788547471165657\n",
      "Training loss for batch 2526 : 0.48264554142951965\n",
      "Training loss for batch 2527 : 0.1654047667980194\n",
      "Training loss for batch 2528 : 0.2744869887828827\n",
      "Training loss for batch 2529 : 0.2691822350025177\n",
      "Training loss for batch 2530 : 0.16850823163986206\n",
      "Training loss for batch 2531 : 0.1827464997768402\n",
      "Training loss for batch 2532 : 0.20263661444187164\n",
      "Training loss for batch 2533 : 0.14993616938591003\n",
      "Training loss for batch 2534 : 0.01870424672961235\n",
      "Training loss for batch 2535 : 0.17429643869400024\n",
      "Training loss for batch 2536 : 0.11856590211391449\n",
      "Training loss for batch 2537 : 0.2471553236246109\n",
      "Training loss for batch 2538 : 0.15226715803146362\n",
      "Training loss for batch 2539 : 0.2666349709033966\n",
      "Training loss for batch 2540 : 0.3430914878845215\n",
      "Training loss for batch 2541 : 0.07030133157968521\n",
      "Training loss for batch 2542 : 0.19673284888267517\n",
      "Training loss for batch 2543 : 0.028433457016944885\n",
      "Training loss for batch 2544 : 0.12481988966464996\n",
      "Training loss for batch 2545 : 0.18328268826007843\n",
      "Training loss for batch 2546 : 0.021222859621047974\n",
      "Training loss for batch 2547 : 0.10028102993965149\n",
      "Training loss for batch 2548 : 0.014152264222502708\n",
      "Training loss for batch 2549 : 0.07376077771186829\n",
      "Training loss for batch 2550 : 0.10079607367515564\n",
      "Training loss for batch 2551 : 0.01412174291908741\n",
      "Training loss for batch 2552 : 0.06959135085344315\n",
      "Training loss for batch 2553 : 0.11029507219791412\n",
      "Training loss for batch 2554 : 0.11038541793823242\n",
      "Training loss for batch 2555 : 0.12700513005256653\n",
      "Training loss for batch 2556 : 0.1544102132320404\n",
      "Training loss for batch 2557 : 0.04082684963941574\n",
      "Training loss for batch 2558 : 0.13495837152004242\n",
      "Training loss for batch 2559 : 0.042010873556137085\n",
      "Training loss for batch 2560 : 0.19947487115859985\n",
      "Training loss for batch 2561 : 0.007727066520601511\n",
      "Training loss for batch 2562 : 0.31503036618232727\n",
      "Training loss for batch 2563 : 0.016032777726650238\n",
      "Training loss for batch 2564 : 0.06690952181816101\n",
      "Training loss for batch 2565 : 0.05684922635555267\n",
      "Training loss for batch 2566 : 0.02741362527012825\n",
      "Training loss for batch 2567 : 0.20996035635471344\n",
      "Training loss for batch 2568 : 0.021788327023386955\n",
      "Training loss for batch 2569 : 0.23214134573936462\n",
      "Training loss for batch 2570 : 0.1011868417263031\n",
      "Training loss for batch 2571 : 0.1883528083562851\n",
      "Training loss for batch 2572 : 0.16138692200183868\n",
      "Training loss for batch 2573 : 0.06129426509141922\n",
      "Training loss for batch 2574 : 0.0453910268843174\n",
      "Training loss for batch 2575 : 0.147074893116951\n",
      "Training loss for batch 2576 : 0.03653017804026604\n",
      "Training loss for batch 2577 : 0.018343694508075714\n",
      "Training loss for batch 2578 : 0.21332073211669922\n",
      "Training loss for batch 2579 : 0.042963527143001556\n",
      "Training loss for batch 2580 : 0.007432781159877777\n",
      "Training loss for batch 2581 : 0.06798408925533295\n",
      "Training loss for batch 2582 : 0.007523498497903347\n",
      "Training loss for batch 2583 : 0.2890298068523407\n",
      "Training loss for batch 2584 : 0.03176946938037872\n",
      "Training loss for batch 2585 : 0.12036369740962982\n",
      "Training loss for batch 2586 : 0.1084061712026596\n",
      "Training loss for batch 2587 : 0.15044915676116943\n",
      "Training loss for batch 2588 : 0.07660719007253647\n",
      "Training loss for batch 2589 : 0.16475895047187805\n",
      "Training loss for batch 2590 : 0.09608836472034454\n",
      "Training loss for batch 2591 : 0.2317529171705246\n",
      "Training loss for batch 2592 : 0.1224876195192337\n",
      "Training loss for batch 2593 : 0.24618583917617798\n",
      "Training loss for batch 2594 : 0.0658646896481514\n",
      "Training loss for batch 2595 : 0.38226747512817383\n",
      "Training loss for batch 2596 : 0.03394516184926033\n",
      "Training loss for batch 2597 : 0.0011866220738738775\n",
      "Training loss for batch 2598 : 0.14034506678581238\n",
      "Training loss for batch 2599 : 0.11363516747951508\n",
      "Training loss for batch 2600 : 0.0517740324139595\n",
      "Training loss for batch 2601 : 0.0\n",
      "Training loss for batch 2602 : 0.018229525536298752\n",
      "Training loss for batch 2603 : 0.07415462285280228\n",
      "Training loss for batch 2604 : 0.020749274641275406\n",
      "Training loss for batch 2605 : 0.10963006317615509\n",
      "Training loss for batch 2606 : 0.05473317205905914\n",
      "Training loss for batch 2607 : 0.10742613673210144\n",
      "Training loss for batch 2608 : 0.11555831879377365\n",
      "Training loss for batch 2609 : 0.11058454215526581\n",
      "Training loss for batch 2610 : 0.038478150963783264\n",
      "Training loss for batch 2611 : 0.09002861380577087\n",
      "Training loss for batch 2612 : 0.07429318875074387\n",
      "Training loss for batch 2613 : 0.007052453234791756\n",
      "Training loss for batch 2614 : 0.2544325888156891\n",
      "Training loss for batch 2615 : 0.09938818961381912\n",
      "Training loss for batch 2616 : 0.010574187152087688\n",
      "Training loss for batch 2617 : 0.11151432991027832\n",
      "Training loss for batch 2618 : 0.2206553816795349\n",
      "Training loss for batch 2619 : 0.07035738974809647\n",
      "Training loss for batch 2620 : 0.06619638949632645\n",
      "Training loss for batch 2621 : 0.06450419127941132\n",
      "Training loss for batch 2622 : 0.00713209668174386\n",
      "Training loss for batch 2623 : 0.16426081955432892\n",
      "Training loss for batch 2624 : 0.26812225580215454\n",
      "Training loss for batch 2625 : 0.08028135448694229\n",
      "Training loss for batch 2626 : 0.01682281866669655\n",
      "Training loss for batch 2627 : 0.0\n",
      "Training loss for batch 2628 : 0.08961738646030426\n",
      "Training loss for batch 2629 : 0.1791183352470398\n",
      "Training loss for batch 2630 : 0.2625488340854645\n",
      "Training loss for batch 2631 : 0.3609790503978729\n",
      "Training loss for batch 2632 : 0.019702408462762833\n",
      "Training loss for batch 2633 : 0.20008838176727295\n",
      "Training loss for batch 2634 : 0.04421459138393402\n",
      "Training loss for batch 2635 : 0.019668204709887505\n",
      "Training loss for batch 2636 : 0.04894396662712097\n",
      "Training loss for batch 2637 : -0.0005185341578908265\n",
      "Training loss for batch 2638 : 0.09555042535066605\n",
      "Training loss for batch 2639 : 0.07601004838943481\n",
      "Training loss for batch 2640 : 0.01975899562239647\n",
      "Training loss for batch 2641 : 0.267346054315567\n",
      "Training loss for batch 2642 : 0.09506016224622726\n",
      "Training loss for batch 2643 : 0.23578129708766937\n",
      "Training loss for batch 2644 : 0.0\n",
      "Training loss for batch 2645 : 0.11938712745904922\n",
      "Training loss for batch 2646 : 0.023938024416565895\n",
      "Training loss for batch 2647 : 0.018519368022680283\n",
      "Training loss for batch 2648 : 0.17732946574687958\n",
      "Training loss for batch 2649 : 0.07881751656532288\n",
      "Training loss for batch 2650 : 0.05931071937084198\n",
      "Training loss for batch 2651 : 0.0445568822324276\n",
      "Training loss for batch 2652 : 0.011443758383393288\n",
      "Training loss for batch 2653 : 0.12704813480377197\n",
      "Training loss for batch 2654 : 0.044420886784791946\n",
      "Training loss for batch 2655 : 0.2568560838699341\n",
      "Training loss for batch 2656 : 0.0\n",
      "Training loss for batch 2657 : 0.22795823216438293\n",
      "Training loss for batch 2658 : 0.049105145037174225\n",
      "Training loss for batch 2659 : 0.11404349654912949\n",
      "Training loss for batch 2660 : 0.005298247095197439\n",
      "Training loss for batch 2661 : 0.00026355189038440585\n",
      "Training loss for batch 2662 : 0.030005168169736862\n",
      "Training loss for batch 2663 : 0.018077228218317032\n",
      "Training loss for batch 2664 : 0.4409281611442566\n",
      "Training loss for batch 2665 : 0.14210988581180573\n",
      "Training loss for batch 2666 : 0.0690913051366806\n",
      "Training loss for batch 2667 : 0.0010597447399049997\n",
      "Training loss for batch 2668 : 0.08046145737171173\n",
      "Training loss for batch 2669 : 0.17546162009239197\n",
      "Training loss for batch 2670 : 0.15559853613376617\n",
      "Training loss for batch 2671 : 0.0637168288230896\n",
      "Training loss for batch 2672 : 0.376487135887146\n",
      "Training loss for batch 2673 : 0.07463307678699493\n",
      "Training loss for batch 2674 : 0.15064148604869843\n",
      "Training loss for batch 2675 : 0.09562292695045471\n",
      "Training loss for batch 2676 : 0.24514088034629822\n",
      "Training loss for batch 2677 : 0.042345136404037476\n",
      "Training loss for batch 2678 : 0.13879334926605225\n",
      "Training loss for batch 2679 : 0.11515678465366364\n",
      "Training loss for batch 2680 : 0.24131886661052704\n",
      "Training loss for batch 2681 : 0.3415565490722656\n",
      "Training loss for batch 2682 : 0.034974560141563416\n",
      "Training loss for batch 2683 : 0.06358429789543152\n",
      "Training loss for batch 2684 : 0.04412277787923813\n",
      "Training loss for batch 2685 : 0.2823314368724823\n",
      "Training loss for batch 2686 : 0.2084411084651947\n",
      "Training loss for batch 2687 : 0.1287565529346466\n",
      "Training loss for batch 2688 : 0.28880926966667175\n",
      "Training loss for batch 2689 : 0.012877881526947021\n",
      "Training loss for batch 2690 : 0.0\n",
      "Training loss for batch 2691 : 0.11850010603666306\n",
      "Training loss for batch 2692 : 0.23521199822425842\n",
      "Training loss for batch 2693 : 0.1289108693599701\n",
      "Training loss for batch 2694 : 0.03912892937660217\n",
      "Training loss for batch 2695 : 0.1905774176120758\n",
      "Training loss for batch 2696 : 0.17491744458675385\n",
      "Training loss for batch 2697 : 0.10065718740224838\n",
      "Training loss for batch 2698 : 0.11067887395620346\n",
      "Training loss for batch 2699 : 0.025443749502301216\n",
      "Training loss for batch 2700 : 0.23310944437980652\n",
      "Training loss for batch 2701 : 0.030916735529899597\n",
      "Training loss for batch 2702 : 0.18728064000606537\n",
      "Training loss for batch 2703 : 0.09369891881942749\n",
      "Training loss for batch 2704 : 0.06898337602615356\n",
      "Training loss for batch 2705 : 0.1095552146434784\n",
      "Training loss for batch 2706 : 0.053179070353507996\n",
      "Training loss for batch 2707 : 0.10196433961391449\n",
      "Training loss for batch 2708 : 0.2792178988456726\n",
      "Training loss for batch 2709 : 0.1574673354625702\n",
      "Training loss for batch 2710 : 0.09630380570888519\n",
      "Training loss for batch 2711 : 0.07176382839679718\n",
      "Training loss for batch 2712 : 0.16349780559539795\n",
      "Training loss for batch 2713 : 0.10763631016016006\n",
      "Training loss for batch 2714 : 0.001348509918898344\n",
      "Training loss for batch 2715 : 0.32883983850479126\n",
      "Training loss for batch 2716 : 0.0481593981385231\n",
      "Training loss for batch 2717 : 0.10280859470367432\n",
      "Training loss for batch 2718 : 0.15871483087539673\n",
      "Training loss for batch 2719 : 0.12390797585248947\n",
      "Training loss for batch 2720 : 0.052673377096652985\n",
      "Training loss for batch 2721 : 0.06659886240959167\n",
      "Training loss for batch 2722 : 0.1749461591243744\n",
      "Training loss for batch 2723 : -0.0009449981153011322\n",
      "Training loss for batch 2724 : 0.26882505416870117\n",
      "Training loss for batch 2725 : 0.2016199827194214\n",
      "Training loss for batch 2726 : 0.10529240220785141\n",
      "Training loss for batch 2727 : 0.0366511344909668\n",
      "Training loss for batch 2728 : 0.0\n",
      "Training loss for batch 2729 : 0.027650674805045128\n",
      "Training loss for batch 2730 : 0.016065001487731934\n",
      "Training loss for batch 2731 : 0.05270906910300255\n",
      "Training loss for batch 2732 : 0.08709602057933807\n",
      "Training loss for batch 2733 : 0.1327945441007614\n",
      "Training loss for batch 2734 : 0.2402980774641037\n",
      "Training loss for batch 2735 : 0.04638377204537392\n",
      "Training loss for batch 2736 : 0.1790556013584137\n",
      "Training loss for batch 2737 : 0.16366304457187653\n",
      "Training loss for batch 2738 : 0.021367337554693222\n",
      "Training loss for batch 2739 : 0.059644877910614014\n",
      "Training loss for batch 2740 : 0.020899515599012375\n",
      "Training loss for batch 2741 : 0.031657710671424866\n",
      "Training loss for batch 2742 : 0.14426745474338531\n",
      "Training loss for batch 2743 : 0.0556357279419899\n",
      "Training loss for batch 2744 : 0.03233221173286438\n",
      "Training loss for batch 2745 : 0.09933000057935715\n",
      "Training loss for batch 2746 : 0.02442234754562378\n",
      "Training loss for batch 2747 : 0.001797688310034573\n",
      "Training loss for batch 2748 : 0.04847053810954094\n",
      "Training loss for batch 2749 : 0.3321320414543152\n",
      "Training loss for batch 2750 : 0.15608716011047363\n",
      "Training loss for batch 2751 : 0.16670818626880646\n",
      "Training loss for batch 2752 : 0.2173275202512741\n",
      "Training loss for batch 2753 : 0.09823979437351227\n",
      "Training loss for batch 2754 : 0.09289567917585373\n",
      "Training loss for batch 2755 : 0.2710432708263397\n",
      "Training loss for batch 2756 : 0.034188780933618546\n",
      "Training loss for batch 2757 : 0.062026966363191605\n",
      "Training loss for batch 2758 : 0.06756921112537384\n",
      "Training loss for batch 2759 : 0.10436267405748367\n",
      "Training loss for batch 2760 : 0.04558059945702553\n",
      "Training loss for batch 2761 : 0.05514543503522873\n",
      "Training loss for batch 2762 : 0.06304976344108582\n",
      "Training loss for batch 2763 : 0.05724416300654411\n",
      "Training loss for batch 2764 : 0.056894540786743164\n",
      "Training loss for batch 2765 : 0.0940835252404213\n",
      "Training loss for batch 2766 : 0.02517368271946907\n",
      "Training loss for batch 2767 : 0.10846132785081863\n",
      "Training loss for batch 2768 : 0.05858661234378815\n",
      "Training loss for batch 2769 : 0.10813212394714355\n",
      "Training loss for batch 2770 : 0.37376993894577026\n",
      "Training loss for batch 2771 : 0.1363307237625122\n",
      "Training loss for batch 2772 : 0.06817790120840073\n",
      "Training loss for batch 2773 : -0.002790651749819517\n",
      "Training loss for batch 2774 : 0.09716342389583588\n",
      "Training loss for batch 2775 : 0.05514037609100342\n",
      "Training loss for batch 2776 : 0.1975211352109909\n",
      "Training loss for batch 2777 : 0.10615646839141846\n",
      "Training loss for batch 2778 : 0.08877075463533401\n",
      "Training loss for batch 2779 : 0.2935671806335449\n",
      "Training loss for batch 2780 : 0.15410800278186798\n",
      "Training loss for batch 2781 : 0.0049638948403298855\n",
      "Training loss for batch 2782 : 0.20950675010681152\n",
      "Training loss for batch 2783 : 0.11208349466323853\n",
      "Training loss for batch 2784 : 0.07580234855413437\n",
      "Training loss for batch 2785 : 0.0745457112789154\n",
      "Training loss for batch 2786 : 0.07948462665081024\n",
      "Training loss for batch 2787 : 0.14814916253089905\n",
      "Training loss for batch 2788 : 0.09926199913024902\n",
      "Training loss for batch 2789 : 0.12128893285989761\n",
      "Training loss for batch 2790 : 0.09068406373262405\n",
      "Training loss for batch 2791 : 0.11468711495399475\n",
      "Training loss for batch 2792 : 0.03147544339299202\n",
      "Training loss for batch 2793 : 0.0921178013086319\n",
      "Training loss for batch 2794 : 0.2455235868692398\n",
      "Training loss for batch 2795 : 0.1434183418750763\n",
      "Training loss for batch 2796 : 0.2040993571281433\n",
      "Training loss for batch 2797 : 0.13038696348667145\n",
      "Training loss for batch 2798 : 0.16098040342330933\n",
      "Training loss for batch 2799 : 0.03526778891682625\n",
      "Training loss for batch 2800 : 0.10069703310728073\n",
      "Training loss for batch 2801 : 0.2012617290019989\n",
      "Training loss for batch 2802 : 0.22714585065841675\n",
      "Training loss for batch 2803 : 0.07376887649297714\n",
      "Training loss for batch 2804 : 0.17182990908622742\n",
      "Training loss for batch 2805 : 0.15353195369243622\n",
      "Training loss for batch 2806 : 0.1564997285604477\n",
      "Training loss for batch 2807 : 0.06528108566999435\n",
      "Training loss for batch 2808 : 0.08851922303438187\n",
      "Training loss for batch 2809 : 0.05003570765256882\n",
      "Training loss for batch 2810 : 0.03870999068021774\n",
      "Training loss for batch 2811 : 0.2712537348270416\n",
      "Training loss for batch 2812 : 0.10064591467380524\n",
      "Training loss for batch 2813 : 0.1880241483449936\n",
      "Training loss for batch 2814 : 0.12261424213647842\n",
      "Training loss for batch 2815 : 0.026265690103173256\n",
      "Training loss for batch 2816 : 0.018340742215514183\n",
      "Training loss for batch 2817 : 0.22824794054031372\n",
      "Training loss for batch 2818 : 0.10074279457330704\n",
      "Training loss for batch 2819 : 0.18494369089603424\n",
      "Training loss for batch 2820 : 0.1329575926065445\n",
      "Training loss for batch 2821 : 0.08267173171043396\n",
      "Training loss for batch 2822 : 0.012173770926892757\n",
      "Training loss for batch 2823 : 0.36201977729797363\n",
      "Training loss for batch 2824 : 0.1036054715514183\n",
      "Training loss for batch 2825 : 0.14923539757728577\n",
      "Training loss for batch 2826 : 0.34221771359443665\n",
      "Training loss for batch 2827 : 0.057169459760189056\n",
      "Training loss for batch 2828 : 0.19009792804718018\n",
      "Training loss for batch 2829 : 0.03422507643699646\n",
      "Training loss for batch 2830 : 0.1353505700826645\n",
      "Training loss for batch 2831 : 0.0030493545345962048\n",
      "Training loss for batch 2832 : 0.03051382675766945\n",
      "Training loss for batch 2833 : -6.222556112334132e-05\n",
      "Training loss for batch 2834 : 0.20815441012382507\n",
      "Training loss for batch 2835 : 0.23014485836029053\n",
      "Training loss for batch 2836 : 0.09834801405668259\n",
      "Training loss for batch 2837 : 0.12914720177650452\n",
      "Training loss for batch 2838 : 0.18972209095954895\n",
      "Training loss for batch 2839 : 0.11221008002758026\n",
      "Training loss for batch 2840 : 0.1154974102973938\n",
      "Training loss for batch 2841 : 0.017568469047546387\n",
      "Training loss for batch 2842 : 0.26408815383911133\n",
      "Training loss for batch 2843 : 0.0005161399021744728\n",
      "Training loss for batch 2844 : 0.1277899444103241\n",
      "Training loss for batch 2845 : 0.04595198482275009\n",
      "Training loss for batch 2846 : 0.003884238423779607\n",
      "Training loss for batch 2847 : 0.07046972960233688\n",
      "Training loss for batch 2848 : 0.2117811143398285\n",
      "Training loss for batch 2849 : 0.2410125583410263\n",
      "Training loss for batch 2850 : 0.09765844792127609\n",
      "Training loss for batch 2851 : 0.019521333277225494\n",
      "Training loss for batch 2852 : 0.16242028772830963\n",
      "Training loss for batch 2853 : 0.03280172124505043\n",
      "Training loss for batch 2854 : 0.1108197495341301\n",
      "Training loss for batch 2855 : 0.1374373435974121\n",
      "Training loss for batch 2856 : 0.19870540499687195\n",
      "Training loss for batch 2857 : 0.05289293825626373\n",
      "Training loss for batch 2858 : 0.07975021004676819\n",
      "Training loss for batch 2859 : 0.09935234487056732\n",
      "Training loss for batch 2860 : 0.21318542957305908\n",
      "Training loss for batch 2861 : 0.07976379990577698\n",
      "Training loss for batch 2862 : 0.15885388851165771\n",
      "Training loss for batch 2863 : 0.08115675300359726\n",
      "Training loss for batch 2864 : 0.08804597705602646\n",
      "Training loss for batch 2865 : 0.034288983792066574\n",
      "Training loss for batch 2866 : 0.1792447417974472\n",
      "Training loss for batch 2867 : 0.009166320785880089\n",
      "Training loss for batch 2868 : 0.01642892137169838\n",
      "Training loss for batch 2869 : 0.15449240803718567\n",
      "Training loss for batch 2870 : 0.020312031731009483\n",
      "Training loss for batch 2871 : 0.19916152954101562\n",
      "Training loss for batch 2872 : 0.04033500328660011\n",
      "Training loss for batch 2873 : 0.13922318816184998\n",
      "Training loss for batch 2874 : 0.026468336582183838\n",
      "Training loss for batch 2875 : 0.08453458547592163\n",
      "Training loss for batch 2876 : 0.053203463554382324\n",
      "Training loss for batch 2877 : 0.03340880572795868\n",
      "Training loss for batch 2878 : 0.15848493576049805\n",
      "Training loss for batch 2879 : 0.0850154459476471\n",
      "Training loss for batch 2880 : 0.06674482673406601\n",
      "Training loss for batch 2881 : 0.11924391984939575\n",
      "Training loss for batch 2882 : 0.10278527438640594\n",
      "Training loss for batch 2883 : 0.20218831300735474\n",
      "Training loss for batch 2884 : 0.21210609376430511\n",
      "Training loss for batch 2885 : 0.3478474020957947\n",
      "Training loss for batch 2886 : 0.09670605510473251\n",
      "Training loss for batch 2887 : 0.10368847846984863\n",
      "Training loss for batch 2888 : 0.004660925827920437\n",
      "Training loss for batch 2889 : 0.20350989699363708\n",
      "Training loss for batch 2890 : 0.06213764473795891\n",
      "Training loss for batch 2891 : 0.12011238932609558\n",
      "Training loss for batch 2892 : 0.16806352138519287\n",
      "Training loss for batch 2893 : 0.508994460105896\n",
      "Training loss for batch 2894 : 0.0844615176320076\n",
      "Training loss for batch 2895 : 0.06993523985147476\n",
      "Training loss for batch 2896 : 0.14956027269363403\n",
      "Training loss for batch 2897 : 0.13687005639076233\n",
      "Training loss for batch 2898 : 0.2298831045627594\n",
      "Training loss for batch 2899 : 0.1192491427063942\n",
      "Training loss for batch 2900 : 0.43284815549850464\n",
      "Training loss for batch 2901 : 0.2166713923215866\n",
      "Training loss for batch 2902 : 0.010300111025571823\n",
      "Training loss for batch 2903 : 0.07075043767690659\n",
      "Training loss for batch 2904 : 0.08015809208154678\n",
      "Training loss for batch 2905 : 0.04498399794101715\n",
      "Training loss for batch 2906 : 0.020660003647208214\n",
      "Training loss for batch 2907 : 0.10011696815490723\n",
      "Training loss for batch 2908 : 0.23791250586509705\n",
      "Training loss for batch 2909 : 0.059317585080862045\n",
      "Training loss for batch 2910 : 0.07194654643535614\n",
      "Training loss for batch 2911 : 0.14693841338157654\n",
      "Training loss for batch 2912 : 0.32636937499046326\n",
      "Training loss for batch 2913 : 0.3751027584075928\n",
      "Training loss for batch 2914 : 0.16561636328697205\n",
      "Training loss for batch 2915 : 0.004291089251637459\n",
      "Training loss for batch 2916 : 0.0268169566988945\n",
      "Training loss for batch 2917 : 0.011903518810868263\n",
      "Training loss for batch 2918 : 0.013487492688000202\n",
      "Training loss for batch 2919 : 0.01969262957572937\n",
      "Training loss for batch 2920 : 0.35383501648902893\n",
      "Training loss for batch 2921 : 0.1075805127620697\n",
      "Training loss for batch 2922 : 0.08872685581445694\n",
      "Training loss for batch 2923 : 0.15555721521377563\n",
      "Training loss for batch 2924 : 0.3490859568119049\n",
      "Training loss for batch 2925 : 0.0016921316273510456\n",
      "Training loss for batch 2926 : 0.342598557472229\n",
      "Training loss for batch 2927 : 0.1409991979598999\n",
      "Training loss for batch 2928 : 0.029041549190878868\n",
      "Training loss for batch 2929 : 0.14451728761196136\n",
      "Training loss for batch 2930 : 0.1568136364221573\n",
      "Training loss for batch 2931 : 0.06671090424060822\n",
      "Training loss for batch 2932 : 0.2382992058992386\n",
      "Training loss for batch 2933 : 0.14640632271766663\n",
      "Training loss for batch 2934 : 0.13840439915657043\n",
      "Training loss for batch 2935 : 0.14256227016448975\n",
      "Training loss for batch 2936 : 0.21922868490219116\n",
      "Training loss for batch 2937 : 0.04271872341632843\n",
      "Training loss for batch 2938 : 0.3124026656150818\n",
      "Training loss for batch 2939 : 0.12215778231620789\n",
      "Training loss for batch 2940 : 0.278860867023468\n",
      "Training loss for batch 2941 : 0.03077862225472927\n",
      "Training loss for batch 2942 : 0.04466913267970085\n",
      "Training loss for batch 2943 : 0.10893562436103821\n",
      "Training loss for batch 2944 : 0.07845094054937363\n",
      "Training loss for batch 2945 : 0.03538041561841965\n",
      "Training loss for batch 2946 : 0.16203422844409943\n",
      "Training loss for batch 2947 : 0.11294101923704147\n",
      "Training loss for batch 2948 : 0.15628892183303833\n",
      "Training loss for batch 2949 : 0.17682310938835144\n",
      "Training loss for batch 2950 : 0.08399294316768646\n",
      "Training loss for batch 2951 : 0.002398330718278885\n",
      "Training loss for batch 2952 : 0.13425302505493164\n",
      "Training loss for batch 2953 : 0.13951514661312103\n",
      "Training loss for batch 2954 : 0.08148357272148132\n",
      "Training loss for batch 2955 : 0.11701636016368866\n",
      "Training loss for batch 2956 : 0.09818018227815628\n",
      "Training loss for batch 2957 : 0.023737376555800438\n",
      "Training loss for batch 2958 : 0.0\n",
      "Training loss for batch 2959 : 0.041957348585128784\n",
      "Training loss for batch 2960 : 0.1711922436952591\n",
      "Training loss for batch 2961 : 0.005899965763092041\n",
      "Training loss for batch 2962 : 0.05365975946187973\n",
      "Training loss for batch 2963 : 0.3331710398197174\n",
      "Training loss for batch 2964 : 0.1461959332227707\n",
      "Training loss for batch 2965 : 0.13417422771453857\n",
      "Training loss for batch 2966 : 0.07856304198503494\n",
      "Training loss for batch 2967 : 0.1259249746799469\n",
      "Training loss for batch 2968 : 0.06787549704313278\n",
      "Training loss for batch 2969 : 0.19812950491905212\n",
      "Training loss for batch 2970 : 0.0644269585609436\n",
      "Training loss for batch 2971 : 0.15983152389526367\n",
      "Training loss for batch 2972 : -0.0026954871136695147\n",
      "Training loss for batch 2973 : 0.004547415301203728\n",
      "Training loss for batch 2974 : 0.0\n",
      "Training loss for batch 2975 : 0.18432575464248657\n",
      "Training loss for batch 2976 : 0.01770218461751938\n",
      "Training loss for batch 2977 : 0.012773329392075539\n",
      "Training loss for batch 2978 : 0.21796706318855286\n",
      "Training loss for batch 2979 : 0.15889577567577362\n",
      "Training loss for batch 2980 : 0.053313493728637695\n",
      "Training loss for batch 2981 : 0.15372473001480103\n",
      "Training loss for batch 2982 : 0.12210850417613983\n",
      "Training loss for batch 2983 : 0.013499146327376366\n",
      "Training loss for batch 2984 : 0.0\n",
      "Training loss for batch 2985 : 0.05444691702723503\n",
      "Training loss for batch 2986 : 0.3840707242488861\n",
      "Training loss for batch 2987 : -0.001710302196443081\n",
      "Training loss for batch 2988 : 0.24568849802017212\n",
      "Training loss for batch 2989 : 0.11422421783208847\n",
      "Training loss for batch 2990 : 0.022454749792814255\n",
      "Training loss for batch 2991 : 0.014613046310842037\n",
      "Training loss for batch 2992 : 0.09965414553880692\n",
      "Training loss for batch 2993 : 0.003839661832898855\n",
      "Training loss for batch 2994 : 0.20376843214035034\n",
      "Training loss for batch 2995 : 0.22301045060157776\n",
      "Training loss for batch 2996 : 0.08415877819061279\n",
      "Training loss for batch 2997 : 0.0007979969377629459\n",
      "Training loss for batch 2998 : 0.05508977174758911\n",
      "Training loss for batch 2999 : 0.1362038403749466\n",
      "Training loss for batch 3000 : 0.13968409597873688\n",
      "Training loss for batch 3001 : 0.019560029730200768\n",
      "Training loss for batch 3002 : 0.040834199637174606\n",
      "Training loss for batch 3003 : 0.0385773628950119\n",
      "Training loss for batch 3004 : 0.016202092170715332\n",
      "Training loss for batch 3005 : 0.2705337405204773\n",
      "Training loss for batch 3006 : 0.10168222337961197\n",
      "Training loss for batch 3007 : 0.12533138692378998\n",
      "Training loss for batch 3008 : 0.11221751570701599\n",
      "Training loss for batch 3009 : 0.06448271125555038\n",
      "Training loss for batch 3010 : 0.07026369124650955\n",
      "Training loss for batch 3011 : 0.047906629741191864\n",
      "Training loss for batch 3012 : 0.15648019313812256\n",
      "Training loss for batch 3013 : 0.09341111034154892\n",
      "Training loss for batch 3014 : 0.0016172131290659308\n",
      "Training loss for batch 3015 : 0.054002195596694946\n",
      "Training loss for batch 3016 : 0.04390004649758339\n",
      "Training loss for batch 3017 : 0.04005202651023865\n",
      "Training loss for batch 3018 : 0.10865183174610138\n",
      "Training loss for batch 3019 : 0.23392799496650696\n",
      "Training loss for batch 3020 : 0.11149170994758606\n",
      "Training loss for batch 3021 : 0.029914509505033493\n",
      "Training loss for batch 3022 : 0.06931734085083008\n",
      "Training loss for batch 3023 : 0.07361575216054916\n",
      "Training loss for batch 3024 : 0.151818186044693\n",
      "Training loss for batch 3025 : 0.0529155433177948\n",
      "Training loss for batch 3026 : 0.07260847836732864\n",
      "Training loss for batch 3027 : 0.04684526473283768\n",
      "Training loss for batch 3028 : 0.23311200737953186\n",
      "Training loss for batch 3029 : 0.1554628163576126\n",
      "Training loss for batch 3030 : 0.073135145008564\n",
      "Training loss for batch 3031 : 0.0\n",
      "Training loss for batch 3032 : 0.1321299970149994\n",
      "Training loss for batch 3033 : 0.1061745285987854\n",
      "Training loss for batch 3034 : 0.2775121033191681\n",
      "Training loss for batch 3035 : 0.1705833077430725\n",
      "Training loss for batch 3036 : 0.1274186074733734\n",
      "Training loss for batch 3037 : 0.22707915306091309\n",
      "Training loss for batch 3038 : 0.11009122431278229\n",
      "Training loss for batch 3039 : 0.2912095785140991\n",
      "Training loss for batch 3040 : 0.19404728710651398\n",
      "Training loss for batch 3041 : 0.041702236980199814\n",
      "Training loss for batch 3042 : 0.03691435605287552\n",
      "Training loss for batch 3043 : 0.05404200032353401\n",
      "Training loss for batch 3044 : 0.059773676097393036\n",
      "Training loss for batch 3045 : 0.12399595975875854\n",
      "Training loss for batch 3046 : 0.002584904432296753\n",
      "Training loss for batch 3047 : 0.3355926275253296\n",
      "Training loss for batch 3048 : 0.24244631826877594\n",
      "Training loss for batch 3049 : 0.1251482218503952\n",
      "Training loss for batch 3050 : 0.03675417974591255\n",
      "Training loss for batch 3051 : 0.08230911940336227\n",
      "Training loss for batch 3052 : 0.19709527492523193\n",
      "Training loss for batch 3053 : 0.1646059900522232\n",
      "Training loss for batch 3054 : 0.00549885630607605\n",
      "Training loss for batch 3055 : 0.19417084753513336\n",
      "Training loss for batch 3056 : 0.018803836777806282\n",
      "Training loss for batch 3057 : 0.0\n",
      "Training loss for batch 3058 : 0.026481900364160538\n",
      "Training loss for batch 3059 : 0.003425092902034521\n",
      "Training loss for batch 3060 : 0.2604442238807678\n",
      "Training loss for batch 3061 : 0.17132693529129028\n",
      "Training loss for batch 3062 : 0.20055684447288513\n",
      "Training loss for batch 3063 : 0.1886356920003891\n",
      "Training loss for batch 3064 : 0.08963143825531006\n",
      "Training loss for batch 3065 : 0.11003752797842026\n",
      "Training loss for batch 3066 : 0.01595468446612358\n",
      "Training loss for batch 3067 : 0.05629163235425949\n",
      "Training loss for batch 3068 : 0.23049470782279968\n",
      "Training loss for batch 3069 : 0.13161513209342957\n",
      "Training loss for batch 3070 : 0.0740804448723793\n",
      "Training loss for batch 3071 : 0.014117486774921417\n",
      "Training loss for batch 3072 : 0.027632426470518112\n",
      "Training loss for batch 3073 : 0.2595463991165161\n",
      "Training loss for batch 3074 : 0.009787033312022686\n",
      "Training loss for batch 3075 : 0.059607796370983124\n",
      "Training loss for batch 3076 : 0.05265967175364494\n",
      "Training loss for batch 3077 : 0.3397277593612671\n",
      "Training loss for batch 3078 : 0.34276843070983887\n",
      "Training loss for batch 3079 : 0.053478579968214035\n",
      "Training loss for batch 3080 : 0.3074544072151184\n",
      "Training loss for batch 3081 : 0.3333019018173218\n",
      "Training loss for batch 3082 : 0.06704595685005188\n",
      "Training loss for batch 3083 : 0.23597551882266998\n",
      "Training loss for batch 3084 : 0.11572029441595078\n",
      "Training loss for batch 3085 : 0.0729999914765358\n",
      "Training loss for batch 3086 : 0.12998560070991516\n",
      "Training loss for batch 3087 : 0.0\n",
      "Training loss for batch 3088 : 0.05523674935102463\n",
      "Training loss for batch 3089 : 0.03373144194483757\n",
      "Training loss for batch 3090 : 0.4350924789905548\n",
      "Training loss for batch 3091 : 0.1603798121213913\n",
      "Training loss for batch 3092 : 0.06585752964019775\n",
      "Training loss for batch 3093 : 0.0398547500371933\n",
      "Training loss for batch 3094 : 0.06317977607250214\n",
      "Training loss for batch 3095 : 0.14449124038219452\n",
      "Training loss for batch 3096 : 0.02474897727370262\n",
      "Training loss for batch 3097 : 0.16376696527004242\n",
      "Training loss for batch 3098 : 0.020778249949216843\n",
      "Training loss for batch 3099 : 0.026548869907855988\n",
      "Training loss for batch 3100 : 0.13944929838180542\n",
      "Training loss for batch 3101 : 0.20368966460227966\n",
      "Training loss for batch 3102 : -0.0017971144989132881\n",
      "Training loss for batch 3103 : 0.11151210963726044\n",
      "Training loss for batch 3104 : 0.2282634973526001\n",
      "Training loss for batch 3105 : 0.05411411449313164\n",
      "Training loss for batch 3106 : 0.30991899967193604\n",
      "Training loss for batch 3107 : 0.1180746853351593\n",
      "Training loss for batch 3108 : 0.18727006018161774\n",
      "Training loss for batch 3109 : 0.1433844417333603\n",
      "Training loss for batch 3110 : 0.007669500075280666\n",
      "Training loss for batch 3111 : 0.05411997437477112\n",
      "Training loss for batch 3112 : 0.2865793704986572\n",
      "Training loss for batch 3113 : 0.14095836877822876\n",
      "Training loss for batch 3114 : 0.10602767020463943\n",
      "Training loss for batch 3115 : 0.03409166634082794\n",
      "Training loss for batch 3116 : 0.0\n",
      "Training loss for batch 3117 : 0.143178790807724\n",
      "Training loss for batch 3118 : 0.002960546873509884\n",
      "Training loss for batch 3119 : 0.09975583106279373\n",
      "Training loss for batch 3120 : 0.1104373037815094\n",
      "Training loss for batch 3121 : 0.1403956264257431\n",
      "Training loss for batch 3122 : 0.053072668612003326\n",
      "Training loss for batch 3123 : 0.05256275087594986\n",
      "Training loss for batch 3124 : 0.14948846399784088\n",
      "Training loss for batch 3125 : 0.1228923574090004\n",
      "Training loss for batch 3126 : 0.13112951815128326\n",
      "Training loss for batch 3127 : 0.024552060291171074\n",
      "Training loss for batch 3128 : 0.1643306463956833\n",
      "Training loss for batch 3129 : 0.0\n",
      "Training loss for batch 3130 : 0.09156892448663712\n",
      "Training loss for batch 3131 : 0.048699427396059036\n",
      "Training loss for batch 3132 : 0.03484352305531502\n",
      "Training loss for batch 3133 : 0.01096928771585226\n",
      "Training loss for batch 3134 : 0.06883721798658371\n",
      "Training loss for batch 3135 : 0.23931151628494263\n",
      "Training loss for batch 3136 : 0.43203386664390564\n",
      "Training loss for batch 3137 : 0.0991283655166626\n",
      "Training loss for batch 3138 : 0.047182414680719376\n",
      "Training loss for batch 3139 : 0.1521783173084259\n",
      "Training loss for batch 3140 : 0.03978991508483887\n",
      "Training loss for batch 3141 : 0.009358890354633331\n",
      "Training loss for batch 3142 : 0.07946738600730896\n",
      "Training loss for batch 3143 : 0.08199230581521988\n",
      "Training loss for batch 3144 : 0.09735333174467087\n",
      "Training loss for batch 3145 : 0.056913409382104874\n",
      "Training loss for batch 3146 : 0.3772199749946594\n",
      "Training loss for batch 3147 : 0.08002710342407227\n",
      "Training loss for batch 3148 : 0.14523635804653168\n",
      "Training loss for batch 3149 : 0.014012039639055729\n",
      "Training loss for batch 3150 : 0.04437647759914398\n",
      "Training loss for batch 3151 : 0.5915127396583557\n",
      "Training loss for batch 3152 : 0.09542090445756912\n",
      "Training loss for batch 3153 : 0.14872778952121735\n",
      "Training loss for batch 3154 : 0.2749165892601013\n",
      "Training loss for batch 3155 : 0.012316839769482613\n",
      "Training loss for batch 3156 : 0.0\n",
      "Training loss for batch 3157 : 0.08637762069702148\n",
      "Training loss for batch 3158 : 0.29813042283058167\n",
      "Training loss for batch 3159 : 0.1828482449054718\n",
      "Training loss for batch 3160 : 0.03514545410871506\n",
      "Training loss for batch 3161 : 0.07124102115631104\n",
      "Training loss for batch 3162 : 0.08835969120264053\n",
      "Training loss for batch 3163 : 0.048756156116724014\n",
      "Training loss for batch 3164 : 0.24261221289634705\n",
      "Training loss for batch 3165 : 0.0762949287891388\n",
      "Training loss for batch 3166 : 0.08136120438575745\n",
      "Training loss for batch 3167 : 0.15243539214134216\n",
      "Training loss for batch 3168 : 0.134104385972023\n",
      "Training loss for batch 3169 : 0.15230126678943634\n",
      "Training loss for batch 3170 : 0.06423188000917435\n",
      "Training loss for batch 3171 : 0.1483592987060547\n",
      "Training loss for batch 3172 : 0.04268676042556763\n",
      "Training loss for batch 3173 : 0.2781486511230469\n",
      "Training loss for batch 3174 : 0.07446816563606262\n",
      "Training loss for batch 3175 : 0.1269836723804474\n",
      "Training loss for batch 3176 : 0.07956147938966751\n",
      "Training loss for batch 3177 : 0.07202846556901932\n",
      "Training loss for batch 3178 : 0.1454492062330246\n",
      "Training loss for batch 3179 : 0.1073244959115982\n",
      "Training loss for batch 3180 : 0.0052412874065339565\n",
      "Training loss for batch 3181 : 0.03715004771947861\n",
      "Training loss for batch 3182 : 0.13211402297019958\n",
      "Training loss for batch 3183 : 0.34908023476600647\n",
      "Training loss for batch 3184 : 0.10479243844747543\n",
      "Training loss for batch 3185 : 0.08215022087097168\n",
      "Training loss for batch 3186 : 0.21771198511123657\n",
      "Training loss for batch 3187 : 0.18621310591697693\n",
      "Training loss for batch 3188 : 0.05879278853535652\n",
      "Training loss for batch 3189 : 0.00030607980443164706\n",
      "Training loss for batch 3190 : 0.09963199496269226\n",
      "Training loss for batch 3191 : 0.2931680977344513\n",
      "Training loss for batch 3192 : 0.5665596127510071\n",
      "Training loss for batch 3193 : 0.11870822310447693\n",
      "Training loss for batch 3194 : 0.09009569138288498\n",
      "Training loss for batch 3195 : 0.039888281375169754\n",
      "Training loss for batch 3196 : 0.024783385917544365\n",
      "Training loss for batch 3197 : 0.012968146242201328\n",
      "Training loss for batch 3198 : 0.06224307417869568\n",
      "Training loss for batch 3199 : 0.1339159607887268\n",
      "Training loss for batch 3200 : 0.16218405961990356\n",
      "Training loss for batch 3201 : 0.17704884707927704\n",
      "Training loss for batch 3202 : 0.03990209475159645\n",
      "Training loss for batch 3203 : 0.06238795071840286\n",
      "Training loss for batch 3204 : 0.2259988635778427\n",
      "Training loss for batch 3205 : 0.04080304503440857\n",
      "Training loss for batch 3206 : 0.18905740976333618\n",
      "Training loss for batch 3207 : 0.25328192114830017\n",
      "Training loss for batch 3208 : 0.07633110880851746\n",
      "Training loss for batch 3209 : 0.017646413296461105\n",
      "Training loss for batch 3210 : 0.023542601615190506\n",
      "Training loss for batch 3211 : 0.04109375178813934\n",
      "Training loss for batch 3212 : 0.17576660215854645\n",
      "Training loss for batch 3213 : 0.11425253748893738\n",
      "Training loss for batch 3214 : 0.1632743924856186\n",
      "Training loss for batch 3215 : 0.3215506672859192\n",
      "Training loss for batch 3216 : 0.09544161707162857\n",
      "Training loss for batch 3217 : 0.1939435750246048\n",
      "Training loss for batch 3218 : 0.2520597577095032\n",
      "Training loss for batch 3219 : 0.08055577427148819\n",
      "Training loss for batch 3220 : 0.12771078944206238\n",
      "Training loss for batch 3221 : 0.021727174520492554\n",
      "Training loss for batch 3222 : 0.05326300114393234\n",
      "Training loss for batch 3223 : 0.3132045269012451\n",
      "Training loss for batch 3224 : 0.2281191498041153\n",
      "Training loss for batch 3225 : 0.15689174830913544\n",
      "Training loss for batch 3226 : 0.01765444315969944\n",
      "Training loss for batch 3227 : 0.241677924990654\n",
      "Training loss for batch 3228 : 0.21312415599822998\n",
      "Training loss for batch 3229 : 0.32722777128219604\n",
      "Training loss for batch 3230 : 0.3459427058696747\n",
      "Training loss for batch 3231 : 0.01322200894355774\n",
      "Training loss for batch 3232 : 0.025633227080106735\n",
      "Training loss for batch 3233 : 0.17117539048194885\n",
      "Training loss for batch 3234 : 0.06541308760643005\n",
      "Training loss for batch 3235 : 0.08234898746013641\n",
      "Training loss for batch 3236 : 0.1587524712085724\n",
      "Training loss for batch 3237 : 0.11677286028862\n",
      "Training loss for batch 3238 : 0.09473058581352234\n",
      "Training loss for batch 3239 : 0.025014616549015045\n",
      "Training loss for batch 3240 : 0.26889675855636597\n",
      "Training loss for batch 3241 : 0.07081159949302673\n",
      "Training loss for batch 3242 : 0.18545740842819214\n",
      "Training loss for batch 3243 : 0.10528764128684998\n",
      "Training loss for batch 3244 : 0.10402462631464005\n",
      "Training loss for batch 3245 : 0.1957845538854599\n",
      "Training loss for batch 3246 : 0.2588490843772888\n",
      "Training loss for batch 3247 : 0.23642732203006744\n",
      "Training loss for batch 3248 : 0.11273150891065598\n",
      "Training loss for batch 3249 : 0.05187263712286949\n",
      "Training loss for batch 3250 : 0.06125423684716225\n",
      "Training loss for batch 3251 : 0.08652519434690475\n",
      "Training loss for batch 3252 : 0.011358950287103653\n",
      "Training loss for batch 3253 : 0.018677346408367157\n",
      "Training loss for batch 3254 : 0.1715245246887207\n",
      "Training loss for batch 3255 : 0.20571774244308472\n",
      "Training loss for batch 3256 : 0.056510526686906815\n",
      "Training loss for batch 3257 : 0.19946464896202087\n",
      "Training loss for batch 3258 : 0.32837948203086853\n",
      "Training loss for batch 3259 : 0.14421163499355316\n",
      "Training loss for batch 3260 : 0.17957788705825806\n",
      "Training loss for batch 3261 : 0.1939142644405365\n",
      "Training loss for batch 3262 : 0.35678139328956604\n",
      "Training loss for batch 3263 : 0.07781538367271423\n",
      "Training loss for batch 3264 : 0.032894033938646317\n",
      "Training loss for batch 3265 : 0.36145490407943726\n",
      "Training loss for batch 3266 : 0.0647602304816246\n",
      "Training loss for batch 3267 : 0.2431465983390808\n",
      "Training loss for batch 3268 : 0.1448974311351776\n",
      "Training loss for batch 3269 : 0.17408747971057892\n",
      "Training loss for batch 3270 : 0.2811073064804077\n",
      "Training loss for batch 3271 : 0.050864238291978836\n",
      "Training loss for batch 3272 : 0.22228015959262848\n",
      "Training loss for batch 3273 : 0.06716606020927429\n",
      "Training loss for batch 3274 : 0.047336645424366\n",
      "Training loss for batch 3275 : 0.07262846827507019\n",
      "Training loss for batch 3276 : 0.09118161350488663\n",
      "Training loss for batch 3277 : 0.020037280395627022\n",
      "Training loss for batch 3278 : 0.12028525024652481\n",
      "Training loss for batch 3279 : 0.05536923184990883\n",
      "Training loss for batch 3280 : 0.16827130317687988\n",
      "Training loss for batch 3281 : 0.13450102508068085\n",
      "Training loss for batch 3282 : 0.4312981069087982\n",
      "Training loss for batch 3283 : 0.0936172753572464\n",
      "Training loss for batch 3284 : 0.0041706375777721405\n",
      "Training loss for batch 3285 : 0.20493745803833008\n",
      "Training loss for batch 3286 : 0.28803056478500366\n",
      "Training loss for batch 3287 : 0.07625626772642136\n",
      "Training loss for batch 3288 : 0.09821219742298126\n",
      "Training loss for batch 3289 : 0.12130390107631683\n",
      "Training loss for batch 3290 : 0.2073938101530075\n",
      "Training loss for batch 3291 : 0.17989930510520935\n",
      "Training loss for batch 3292 : 0.12916095554828644\n",
      "Training loss for batch 3293 : 0.15220879018306732\n",
      "Training loss for batch 3294 : 0.1712818741798401\n",
      "Training loss for batch 3295 : 0.1537298560142517\n",
      "Training loss for batch 3296 : 0.201609805226326\n",
      "Training loss for batch 3297 : 0.19482378661632538\n",
      "Training loss for batch 3298 : 0.020916415378451347\n",
      "Training loss for batch 3299 : 0.07869651913642883\n",
      "Training loss for batch 3300 : 0.07571011036634445\n",
      "Training loss for batch 3301 : 0.09582112729549408\n",
      "Training loss for batch 3302 : 0.2863091826438904\n",
      "Training loss for batch 3303 : 0.20088320970535278\n",
      "Training loss for batch 3304 : 0.08534803241491318\n",
      "Training loss for batch 3305 : 0.20789794623851776\n",
      "Training loss for batch 3306 : 0.08282937854528427\n",
      "Training loss for batch 3307 : 0.16056765615940094\n",
      "Training loss for batch 3308 : 0.021645937114953995\n",
      "Training loss for batch 3309 : 0.14895939826965332\n",
      "Training loss for batch 3310 : 0.23042093217372894\n",
      "Training loss for batch 3311 : 0.14054571092128754\n",
      "Training loss for batch 3312 : 0.010529965162277222\n",
      "Training loss for batch 3313 : 0.0698230117559433\n",
      "Training loss for batch 3314 : 0.016301853582262993\n",
      "Training loss for batch 3315 : 0.08315012603998184\n",
      "Training loss for batch 3316 : 0.1374412477016449\n",
      "Training loss for batch 3317 : 0.0804709792137146\n",
      "Training loss for batch 3318 : 0.14210620522499084\n",
      "Training loss for batch 3319 : 0.04868442192673683\n",
      "Training loss for batch 3320 : 0.08678943663835526\n",
      "Training loss for batch 3321 : 0.11006394773721695\n",
      "Training loss for batch 3322 : 0.003932712133973837\n",
      "Training loss for batch 3323 : 0.13099811971187592\n",
      "Training loss for batch 3324 : 0.23236848413944244\n",
      "Training loss for batch 3325 : 0.11643063277006149\n",
      "Training loss for batch 3326 : 0.22836194932460785\n",
      "Training loss for batch 3327 : 0.13424675166606903\n",
      "Training loss for batch 3328 : -0.002168480074033141\n",
      "Training loss for batch 3329 : 0.19197002053260803\n",
      "Training loss for batch 3330 : 0.1370011866092682\n",
      "Training loss for batch 3331 : 0.178683802485466\n",
      "Training loss for batch 3332 : 0.04288337752223015\n",
      "Training loss for batch 3333 : 0.0\n",
      "Training loss for batch 3334 : 0.06214934214949608\n",
      "Training loss for batch 3335 : 0.07088401913642883\n",
      "Training loss for batch 3336 : 0.042993366718292236\n",
      "Training loss for batch 3337 : 0.1719593107700348\n",
      "Training loss for batch 3338 : 0.36470162868499756\n",
      "Training loss for batch 3339 : 0.06251781433820724\n",
      "Training loss for batch 3340 : 0.09401436895132065\n",
      "Training loss for batch 3341 : 0.02435426041483879\n",
      "Training loss for batch 3342 : 0.22107845544815063\n",
      "Training loss for batch 3343 : 0.1234380453824997\n",
      "Training loss for batch 3344 : 0.041360899806022644\n",
      "Training loss for batch 3345 : 0.04848826676607132\n",
      "Training loss for batch 3346 : 0.06425541639328003\n",
      "Training loss for batch 3347 : 0.1065221056342125\n",
      "Training loss for batch 3348 : 0.0759429857134819\n",
      "Training loss for batch 3349 : 0.15420857071876526\n",
      "Training loss for batch 3350 : 0.22830063104629517\n",
      "Training loss for batch 3351 : 0.10352185368537903\n",
      "Training loss for batch 3352 : 0.10883133858442307\n",
      "Training loss for batch 3353 : 0.010435959324240685\n",
      "Training loss for batch 3354 : 0.004063848406076431\n",
      "Training loss for batch 3355 : 0.03358442336320877\n",
      "Training loss for batch 3356 : 0.18967723846435547\n",
      "Training loss for batch 3357 : 0.10871725529432297\n",
      "Training loss for batch 3358 : 0.12453257292509079\n",
      "Training loss for batch 3359 : 0.3220617473125458\n",
      "Training loss for batch 3360 : 0.3016898036003113\n",
      "Training loss for batch 3361 : 0.1342211365699768\n",
      "Training loss for batch 3362 : 0.16426753997802734\n",
      "Training loss for batch 3363 : 0.0868322104215622\n",
      "Training loss for batch 3364 : 0.08120912313461304\n",
      "Training loss for batch 3365 : 0.18007275462150574\n",
      "Training loss for batch 3366 : 0.07311885803937912\n",
      "Training loss for batch 3367 : 0.18865343928337097\n",
      "Training loss for batch 3368 : 0.02966446429491043\n",
      "Training loss for batch 3369 : 0.13482362031936646\n",
      "Training loss for batch 3370 : 0.12562742829322815\n",
      "Training loss for batch 3371 : 0.0454101637005806\n",
      "Training loss for batch 3372 : 0.1673373579978943\n",
      "Training loss for batch 3373 : 0.17518991231918335\n",
      "Training loss for batch 3374 : 0.060488831251859665\n",
      "Training loss for batch 3375 : 0.04221227020025253\n",
      "Training loss for batch 3376 : 0.0522259846329689\n",
      "Training loss for batch 3377 : 0.0013538396451622248\n",
      "Training loss for batch 3378 : 0.12096744030714035\n",
      "Training loss for batch 3379 : 0.06205597519874573\n",
      "Training loss for batch 3380 : 0.14703503251075745\n",
      "Training loss for batch 3381 : 0.1885940283536911\n",
      "Training loss for batch 3382 : 0.08007568120956421\n",
      "Training loss for batch 3383 : -0.0010757057461887598\n",
      "Training loss for batch 3384 : 0.0915350392460823\n",
      "Training loss for batch 3385 : 0.2112385779619217\n",
      "Training loss for batch 3386 : 0.026753626763820648\n",
      "Training loss for batch 3387 : 0.21037127077579498\n",
      "Training loss for batch 3388 : 0.18865692615509033\n",
      "Training loss for batch 3389 : 0.015955304726958275\n",
      "Training loss for batch 3390 : 0.0929383859038353\n",
      "Training loss for batch 3391 : 0.040761273354291916\n",
      "Training loss for batch 3392 : 0.0566929467022419\n",
      "Training loss for batch 3393 : 0.10410982370376587\n",
      "Training loss for batch 3394 : 0.0\n",
      "Training loss for batch 3395 : 0.1407591998577118\n",
      "Training loss for batch 3396 : 0.09575270116329193\n",
      "Training loss for batch 3397 : -5.348456761566922e-05\n",
      "Training loss for batch 3398 : 0.13782474398612976\n",
      "Training loss for batch 3399 : 0.22470875084400177\n",
      "Training loss for batch 3400 : 0.04323567822575569\n",
      "Training loss for batch 3401 : 0.1121501475572586\n",
      "Training loss for batch 3402 : 0.04271859675645828\n",
      "Training loss for batch 3403 : 0.0\n",
      "Training loss for batch 3404 : 0.00038789346581324935\n",
      "Training loss for batch 3405 : 0.03532177582383156\n",
      "Training loss for batch 3406 : 0.2554094195365906\n",
      "Training loss for batch 3407 : 0.002301543951034546\n",
      "Training loss for batch 3408 : 0.0\n",
      "Training loss for batch 3409 : 0.17184703052043915\n",
      "Training loss for batch 3410 : 0.05439317598938942\n",
      "Training loss for batch 3411 : 0.06180727854371071\n",
      "Training loss for batch 3412 : 0.3364642262458801\n",
      "Training loss for batch 3413 : 0.07031923532485962\n",
      "Training loss for batch 3414 : 0.009342669509351254\n",
      "Training loss for batch 3415 : 0.04067116975784302\n",
      "Training loss for batch 3416 : 0.13479751348495483\n",
      "Training loss for batch 3417 : 0.2005852907896042\n",
      "Training loss for batch 3418 : 0.15913648903369904\n",
      "Training loss for batch 3419 : 0.23528017103672028\n",
      "Training loss for batch 3420 : 0.01719525083899498\n",
      "Training loss for batch 3421 : 0.2794439196586609\n",
      "Training loss for batch 3422 : 0.08528190106153488\n",
      "Training loss for batch 3423 : 0.23787696659564972\n",
      "Training loss for batch 3424 : -6.11133873462677e-05\n",
      "Training loss for batch 3425 : -0.001562405494041741\n",
      "Training loss for batch 3426 : 0.10922540724277496\n",
      "Training loss for batch 3427 : 0.15580116212368011\n",
      "Training loss for batch 3428 : 0.11724480241537094\n",
      "Training loss for batch 3429 : 0.012358203530311584\n",
      "Training loss for batch 3430 : 0.13343945145606995\n",
      "Training loss for batch 3431 : 0.3548565208911896\n",
      "Training loss for batch 3432 : 0.04326562583446503\n",
      "Training loss for batch 3433 : 0.10895219445228577\n",
      "Training loss for batch 3434 : 0.1383075714111328\n",
      "Training loss for batch 3435 : 0.024702604860067368\n",
      "Training loss for batch 3436 : 0.26562175154685974\n",
      "Training loss for batch 3437 : 0.05660184845328331\n",
      "Training loss for batch 3438 : 0.03254849463701248\n",
      "Training loss for batch 3439 : 0.009108996018767357\n",
      "Training loss for batch 3440 : 0.22716665267944336\n",
      "Training loss for batch 3441 : 0.12604936957359314\n",
      "Training loss for batch 3442 : 0.016699034720659256\n",
      "Training loss for batch 3443 : 0.13643409311771393\n",
      "Training loss for batch 3444 : 0.03898629546165466\n",
      "Training loss for batch 3445 : 0.16108624637126923\n",
      "Training loss for batch 3446 : 0.17226152122020721\n",
      "Training loss for batch 3447 : 0.1654452532529831\n",
      "Training loss for batch 3448 : 0.10742375254631042\n",
      "Training loss for batch 3449 : 0.1909276247024536\n",
      "Training loss for batch 3450 : 0.1981331706047058\n",
      "Training loss for batch 3451 : 0.05271779000759125\n",
      "Training loss for batch 3452 : 0.18484264612197876\n",
      "Training loss for batch 3453 : 0.061426833271980286\n",
      "Training loss for batch 3454 : 0.0028065741062164307\n",
      "Training loss for batch 3455 : 0.09902101010084152\n",
      "Training loss for batch 3456 : 0.21189510822296143\n",
      "Training loss for batch 3457 : 0.15928494930267334\n",
      "Training loss for batch 3458 : 0.09688760340213776\n",
      "Training loss for batch 3459 : 0.26873016357421875\n",
      "Training loss for batch 3460 : 0.048169247806072235\n",
      "Training loss for batch 3461 : 0.016815563663840294\n",
      "Training loss for batch 3462 : 0.1453847438097\n",
      "Training loss for batch 3463 : 0.048029057681560516\n",
      "Training loss for batch 3464 : 0.2788076400756836\n",
      "Training loss for batch 3465 : 0.00935833714902401\n",
      "Training loss for batch 3466 : 0.11597482860088348\n",
      "Training loss for batch 3467 : 0.17040066421031952\n",
      "Training loss for batch 3468 : 0.3629486858844757\n",
      "Training loss for batch 3469 : 0.3247666358947754\n",
      "Training loss for batch 3470 : 0.02846076712012291\n",
      "Training loss for batch 3471 : 0.22330611944198608\n",
      "Training loss for batch 3472 : 0.2844591736793518\n",
      "Training loss for batch 3473 : 0.088709257543087\n",
      "Training loss for batch 3474 : 0.13275718688964844\n",
      "Training loss for batch 3475 : 0.1462264508008957\n",
      "Training loss for batch 3476 : 0.0849323645234108\n",
      "Training loss for batch 3477 : 0.20539771020412445\n",
      "Training loss for batch 3478 : 0.0\n",
      "Training loss for batch 3479 : 0.17745651304721832\n",
      "Training loss for batch 3480 : -0.00032646534964442253\n",
      "Training loss for batch 3481 : 0.14613059163093567\n",
      "Training loss for batch 3482 : 0.18643026053905487\n",
      "Training loss for batch 3483 : 0.12432438135147095\n",
      "Training loss for batch 3484 : 0.1826927661895752\n",
      "Training loss for batch 3485 : 0.1180725172162056\n",
      "Training loss for batch 3486 : 0.2222333699464798\n",
      "Training loss for batch 3487 : 0.029486481100320816\n",
      "Training loss for batch 3488 : 0.21833184361457825\n",
      "Training loss for batch 3489 : 0.06780526787042618\n",
      "Training loss for batch 3490 : 0.0940714105963707\n",
      "Training loss for batch 3491 : 0.041574131697416306\n",
      "Training loss for batch 3492 : 0.29578807950019836\n",
      "Training loss for batch 3493 : 0.07486268877983093\n",
      "Training loss for batch 3494 : 0.026700012385845184\n",
      "Training loss for batch 3495 : 0.2848260998725891\n",
      "Training loss for batch 3496 : 0.15756797790527344\n",
      "Training loss for batch 3497 : 0.23371879756450653\n",
      "Training loss for batch 3498 : 0.28278419375419617\n",
      "Training loss for batch 3499 : 0.27802515029907227\n",
      "Training loss for batch 3500 : 0.15652960538864136\n",
      "Training loss for batch 3501 : 0.12673215568065643\n",
      "Training loss for batch 3502 : 0.2148037552833557\n",
      "Training loss for batch 3503 : 0.06264565885066986\n",
      "Training loss for batch 3504 : 0.05315878242254257\n",
      "Training loss for batch 3505 : 0.16607318818569183\n",
      "Training loss for batch 3506 : 0.1929212361574173\n",
      "Training loss for batch 3507 : 0.42573094367980957\n",
      "Training loss for batch 3508 : 0.2875554859638214\n",
      "Training loss for batch 3509 : 0.06146826222538948\n",
      "Training loss for batch 3510 : 0.09620412439107895\n",
      "Training loss for batch 3511 : 0.09079332649707794\n",
      "Training loss for batch 3512 : 0.34298956394195557\n",
      "Training loss for batch 3513 : 0.024420902132987976\n",
      "Training loss for batch 3514 : 0.24022576212882996\n",
      "Training loss for batch 3515 : 0.16607007384300232\n",
      "Training loss for batch 3516 : 0.23509445786476135\n",
      "Training loss for batch 3517 : 0.008905798196792603\n",
      "Training loss for batch 3518 : 0.04620915651321411\n",
      "Training loss for batch 3519 : -0.0009467803756706417\n",
      "Training loss for batch 3520 : 0.002703859703615308\n",
      "Training loss for batch 3521 : 0.08717015385627747\n",
      "Training loss for batch 3522 : 0.45429685711860657\n",
      "Training loss for batch 3523 : 0.02565346471965313\n",
      "Training loss for batch 3524 : 0.10136988759040833\n",
      "Training loss for batch 3525 : 0.3426501154899597\n",
      "Training loss for batch 3526 : 0.1816856563091278\n",
      "Training loss for batch 3527 : 0.000516975880600512\n",
      "Training loss for batch 3528 : 0.2022165060043335\n",
      "Training loss for batch 3529 : 0.054867446422576904\n",
      "Training loss for batch 3530 : 0.11672874540090561\n",
      "Training loss for batch 3531 : 0.1636630892753601\n",
      "Training loss for batch 3532 : 0.03137161210179329\n",
      "Training loss for batch 3533 : 0.1097726821899414\n",
      "Training loss for batch 3534 : 0.24770690500736237\n",
      "Training loss for batch 3535 : 0.06494837999343872\n",
      "Training loss for batch 3536 : 0.17111346125602722\n",
      "Training loss for batch 3537 : 0.16371244192123413\n",
      "Training loss for batch 3538 : 0.15872110426425934\n",
      "Training loss for batch 3539 : 0.181020587682724\n",
      "Training loss for batch 3540 : 0.035876646637916565\n",
      "Training loss for batch 3541 : 0.18314088881015778\n",
      "Training loss for batch 3542 : 0.18425318598747253\n",
      "Training loss for batch 3543 : 0.030779698863625526\n",
      "Training loss for batch 3544 : 0.09169262647628784\n",
      "Training loss for batch 3545 : 0.1378973126411438\n",
      "Training loss for batch 3546 : 0.033827655017375946\n",
      "Training loss for batch 3547 : 0.06203923001885414\n",
      "Training loss for batch 3548 : 0.26436272263526917\n",
      "Training loss for batch 3549 : 0.1691616326570511\n",
      "Training loss for batch 3550 : 0.0\n",
      "Training loss for batch 3551 : 0.2462054342031479\n",
      "Training loss for batch 3552 : 0.0013459227047860622\n",
      "Training loss for batch 3553 : 0.042612940073013306\n",
      "Training loss for batch 3554 : 0.017111221328377724\n",
      "Training loss for batch 3555 : 0.1622270792722702\n",
      "Training loss for batch 3556 : 0.04413458704948425\n",
      "Training loss for batch 3557 : 0.07530343532562256\n",
      "Training loss for batch 3558 : 0.13349278271198273\n",
      "Training loss for batch 3559 : 0.2557542622089386\n",
      "Training loss for batch 3560 : 0.027933070436120033\n",
      "Training loss for batch 3561 : 0.09221597760915756\n",
      "Training loss for batch 3562 : -0.0017408643616363406\n",
      "Training loss for batch 3563 : 0.05939357355237007\n",
      "Training loss for batch 3564 : 0.2639150023460388\n",
      "Training loss for batch 3565 : 0.05155983567237854\n",
      "Training loss for batch 3566 : 0.07565862685441971\n",
      "Training loss for batch 3567 : 0.1243816465139389\n",
      "Training loss for batch 3568 : 0.09162630140781403\n",
      "Training loss for batch 3569 : 0.31989938020706177\n",
      "Training loss for batch 3570 : 0.11764751374721527\n",
      "Training loss for batch 3571 : 0.009210090152919292\n",
      "Training loss for batch 3572 : 0.17196325957775116\n",
      "Training loss for batch 3573 : 0.22720788419246674\n",
      "Training loss for batch 3574 : 0.12364356964826584\n",
      "Training loss for batch 3575 : 0.31588584184646606\n",
      "Training loss for batch 3576 : 0.1963399052619934\n",
      "Training loss for batch 3577 : 0.27032577991485596\n",
      "Training loss for batch 3578 : 0.1202305257320404\n",
      "Training loss for batch 3579 : 0.0498880073428154\n",
      "Training loss for batch 3580 : 0.069887675344944\n",
      "Training loss for batch 3581 : 0.21025028824806213\n",
      "Training loss for batch 3582 : 0.04359247907996178\n",
      "Training loss for batch 3583 : 0.10315081477165222\n",
      "Training loss for batch 3584 : 0.08475229144096375\n",
      "Training loss for batch 3585 : 0.0039825402200222015\n",
      "Training loss for batch 3586 : 0.0011390636209398508\n",
      "Training loss for batch 3587 : 0.09255190193653107\n",
      "Training loss for batch 3588 : 0.0712200179696083\n",
      "Training loss for batch 3589 : 0.2680480480194092\n",
      "Training loss for batch 3590 : 0.1214611604809761\n",
      "Training loss for batch 3591 : 0.09156870096921921\n",
      "Training loss for batch 3592 : 0.014884911477565765\n",
      "Training loss for batch 3593 : 0.2522621750831604\n",
      "Training loss for batch 3594 : -0.001007663900963962\n",
      "Training loss for batch 3595 : 0.20989444851875305\n",
      "Training loss for batch 3596 : 0.15693998336791992\n",
      "Training loss for batch 3597 : 0.12265873700380325\n",
      "Training loss for batch 3598 : 0.16449379920959473\n",
      "Training loss for batch 3599 : 0.0\n",
      "Training loss for batch 3600 : 0.08818113058805466\n",
      "Training loss for batch 3601 : 0.43768665194511414\n",
      "Training loss for batch 3602 : 0.16175279021263123\n",
      "Training loss for batch 3603 : 0.06623820960521698\n",
      "Training loss for batch 3604 : 0.09186454117298126\n",
      "Training loss for batch 3605 : 0.04753384739160538\n",
      "Training loss for batch 3606 : 0.1567758023738861\n",
      "Training loss for batch 3607 : 0.1412472426891327\n",
      "Training loss for batch 3608 : 0.1940910667181015\n",
      "Training loss for batch 3609 : 0.2457796335220337\n",
      "Training loss for batch 3610 : 0.23585133254528046\n",
      "Training loss for batch 3611 : 0.06435176730155945\n",
      "Training loss for batch 3612 : 0.0807933509349823\n",
      "Training loss for batch 3613 : 0.03143873065710068\n",
      "Training loss for batch 3614 : 0.06471884250640869\n",
      "Training loss for batch 3615 : 0.031040549278259277\n",
      "Training loss for batch 3616 : 0.029059723019599915\n",
      "Training loss for batch 3617 : 0.1413128823041916\n",
      "Training loss for batch 3618 : 0.08152113109827042\n",
      "Training loss for batch 3619 : 0.0943698137998581\n",
      "Training loss for batch 3620 : 0.17489150166511536\n",
      "Training loss for batch 3621 : 0.15960590541362762\n",
      "Training loss for batch 3622 : 0.146671861410141\n",
      "Training loss for batch 3623 : 0.330045223236084\n",
      "Training loss for batch 3624 : 0.20501106977462769\n",
      "Training loss for batch 3625 : 0.03632042557001114\n",
      "Training loss for batch 3626 : 0.013423293828964233\n",
      "Training loss for batch 3627 : 0.1633674055337906\n",
      "Training loss for batch 3628 : 0.029834475368261337\n",
      "Training loss for batch 3629 : 0.20130008459091187\n",
      "Training loss for batch 3630 : 0.14498353004455566\n",
      "Training loss for batch 3631 : 0.044500209391117096\n",
      "Training loss for batch 3632 : 0.06479199230670929\n",
      "Training loss for batch 3633 : 0.06818923354148865\n",
      "Training loss for batch 3634 : 0.20548036694526672\n",
      "Training loss for batch 3635 : -0.0007096215849742293\n",
      "Training loss for batch 3636 : 0.10032761842012405\n",
      "Training loss for batch 3637 : 0.14618034660816193\n",
      "Training loss for batch 3638 : 0.0941530242562294\n",
      "Training loss for batch 3639 : 0.1554238200187683\n",
      "Training loss for batch 3640 : 0.0\n",
      "Training loss for batch 3641 : 0.28060460090637207\n",
      "Training loss for batch 3642 : 0.24175290763378143\n",
      "Training loss for batch 3643 : 0.2750930190086365\n",
      "Training loss for batch 3644 : 0.03839094191789627\n",
      "Training loss for batch 3645 : 0.05720077455043793\n",
      "Training loss for batch 3646 : 0.11658960580825806\n",
      "Training loss for batch 3647 : 0.08325308561325073\n",
      "Training loss for batch 3648 : 0.07159719616174698\n",
      "Training loss for batch 3649 : 0.3160548806190491\n",
      "Training loss for batch 3650 : 0.3172360956668854\n",
      "Training loss for batch 3651 : 0.05919753760099411\n",
      "Training loss for batch 3652 : 0.0027096401900053024\n",
      "Training loss for batch 3653 : 0.06915707886219025\n",
      "Training loss for batch 3654 : 0.045108452439308167\n",
      "Training loss for batch 3655 : 0.05223546922206879\n",
      "Training loss for batch 3656 : 0.006342755630612373\n",
      "Training loss for batch 3657 : 0.318946897983551\n",
      "Training loss for batch 3658 : 0.13159936666488647\n",
      "Training loss for batch 3659 : 0.1960204392671585\n",
      "Training loss for batch 3660 : 0.25571727752685547\n",
      "Training loss for batch 3661 : 0.027060601860284805\n",
      "Training loss for batch 3662 : 0.0747378021478653\n",
      "Training loss for batch 3663 : 0.032553113996982574\n",
      "Training loss for batch 3664 : 0.25463807582855225\n",
      "Training loss for batch 3665 : 0.02645699307322502\n",
      "Training loss for batch 3666 : 0.1526779979467392\n",
      "Training loss for batch 3667 : 0.10593767464160919\n",
      "Training loss for batch 3668 : 0.08723873645067215\n",
      "Training loss for batch 3669 : 0.32928991317749023\n",
      "Training loss for batch 3670 : 0.17492501437664032\n",
      "Training loss for batch 3671 : 0.25696149468421936\n",
      "Training loss for batch 3672 : 0.07235025614500046\n",
      "Training loss for batch 3673 : 0.24732062220573425\n",
      "Training loss for batch 3674 : 0.11548787355422974\n",
      "Training loss for batch 3675 : 0.24666264653205872\n",
      "Training loss for batch 3676 : 0.058200400322675705\n",
      "Training loss for batch 3677 : 0.25825589895248413\n",
      "Training loss for batch 3678 : 0.09264956414699554\n",
      "Training loss for batch 3679 : 0.19424696266651154\n",
      "Training loss for batch 3680 : 0.1958513855934143\n",
      "Training loss for batch 3681 : 0.24743306636810303\n",
      "Training loss for batch 3682 : 0.06260010600090027\n",
      "Training loss for batch 3683 : 0.27227476239204407\n",
      "Training loss for batch 3684 : 0.0635475218296051\n",
      "Training loss for batch 3685 : 0.061563245952129364\n",
      "Training loss for batch 3686 : 0.09908084571361542\n",
      "Training loss for batch 3687 : 0.18933914601802826\n",
      "Training loss for batch 3688 : 0.24661824107170105\n",
      "Training loss for batch 3689 : 0.02330796793103218\n",
      "Training loss for batch 3690 : 0.03712038695812225\n",
      "Training loss for batch 3691 : 0.08567443490028381\n",
      "Training loss for batch 3692 : 0.05344473943114281\n",
      "Training loss for batch 3693 : 0.036608438938856125\n",
      "Training loss for batch 3694 : 0.08399270474910736\n",
      "Training loss for batch 3695 : 0.1061178669333458\n",
      "Training loss for batch 3696 : 0.05226074904203415\n",
      "Training loss for batch 3697 : 0.18032261729240417\n",
      "Training loss for batch 3698 : 0.1700081080198288\n",
      "Training loss for batch 3699 : 0.11664694547653198\n",
      "Training loss for batch 3700 : 0.13345366716384888\n",
      "Training loss for batch 3701 : 0.18186157941818237\n",
      "Training loss for batch 3702 : 0.020663846284151077\n",
      "Training loss for batch 3703 : 0.11765959113836288\n",
      "Training loss for batch 3704 : 0.12129385024309158\n",
      "Training loss for batch 3705 : 0.04882332682609558\n",
      "Training loss for batch 3706 : 0.254533588886261\n",
      "Training loss for batch 3707 : 0.2270800620317459\n",
      "Training loss for batch 3708 : 0.13060814142227173\n",
      "Training loss for batch 3709 : 0.17974185943603516\n",
      "Training loss for batch 3710 : 0.23134100437164307\n",
      "Training loss for batch 3711 : 0.15504403412342072\n",
      "Training loss for batch 3712 : 0.25306570529937744\n",
      "Training loss for batch 3713 : 0.05191686749458313\n",
      "Training loss for batch 3714 : 0.07951369136571884\n",
      "Training loss for batch 3715 : 0.1353478729724884\n",
      "Training loss for batch 3716 : 0.06548116356134415\n",
      "Training loss for batch 3717 : 0.19399213790893555\n",
      "Training loss for batch 3718 : 0.017495231702923775\n",
      "Training loss for batch 3719 : 0.013806263916194439\n",
      "Training loss for batch 3720 : 0.04845458269119263\n",
      "Training loss for batch 3721 : 0.10897676646709442\n",
      "Training loss for batch 3722 : 0.3896455466747284\n",
      "Training loss for batch 3723 : 0.10185651481151581\n",
      "Training loss for batch 3724 : 0.010629448108375072\n",
      "Training loss for batch 3725 : 0.10330875962972641\n",
      "Training loss for batch 3726 : 0.08756744861602783\n",
      "Training loss for batch 3727 : 0.1030535101890564\n",
      "Training loss for batch 3728 : 0.17041264474391937\n",
      "Training loss for batch 3729 : 0.009734909050166607\n",
      "Training loss for batch 3730 : 0.08218042552471161\n",
      "Training loss for batch 3731 : 0.12972202897071838\n",
      "Training loss for batch 3732 : 0.21744054555892944\n",
      "Training loss for batch 3733 : 0.18886351585388184\n",
      "Training loss for batch 3734 : 0.04579111561179161\n",
      "Training loss for batch 3735 : 0.03960607945919037\n",
      "Training loss for batch 3736 : 0.03753010928630829\n",
      "Training loss for batch 3737 : 0.11672721058130264\n",
      "Training loss for batch 3738 : 0.11312668770551682\n",
      "Training loss for batch 3739 : 0.033540450036525726\n",
      "Training loss for batch 3740 : 0.061280958354473114\n",
      "Training loss for batch 3741 : 0.059819094836711884\n",
      "Training loss for batch 3742 : 0.0\n",
      "Training loss for batch 3743 : 0.005437627900391817\n",
      "Training loss for batch 3744 : 0.0755978375673294\n",
      "Training loss for batch 3745 : 0.08206920325756073\n",
      "Training loss for batch 3746 : 0.11191621422767639\n",
      "Training loss for batch 3747 : 0.26363956928253174\n",
      "Training loss for batch 3748 : 0.03543233126401901\n",
      "Training loss for batch 3749 : 0.16804538667201996\n",
      "Training loss for batch 3750 : 0.18156921863555908\n",
      "Training loss for batch 3751 : 0.3126038610935211\n",
      "Training loss for batch 3752 : 0.35121050477027893\n",
      "Training loss for batch 3753 : 0.29465287923812866\n",
      "Training loss for batch 3754 : 0.12129469215869904\n",
      "Training loss for batch 3755 : 0.043031882494688034\n",
      "Training loss for batch 3756 : -0.0009329309687018394\n",
      "Training loss for batch 3757 : 0.023499205708503723\n",
      "Training loss for batch 3758 : 0.2616574764251709\n",
      "Training loss for batch 3759 : 0.044438887387514114\n",
      "Training loss for batch 3760 : 0.19692479074001312\n",
      "Training loss for batch 3761 : 0.11433405429124832\n",
      "Training loss for batch 3762 : 0.01021735928952694\n",
      "Training loss for batch 3763 : 0.21203181147575378\n",
      "Training loss for batch 3764 : 0.12705181539058685\n",
      "Training loss for batch 3765 : 0.28953927755355835\n",
      "Training loss for batch 3766 : 0.14310531318187714\n",
      "Training loss for batch 3767 : 0.19333259761333466\n",
      "Training loss for batch 3768 : 0.03780078515410423\n",
      "Training loss for batch 3769 : 0.11511239409446716\n",
      "Training loss for batch 3770 : 0.14814676344394684\n",
      "Training loss for batch 3771 : 0.27749326825141907\n",
      "Training loss for batch 3772 : 0.0527978353202343\n",
      "Training loss for batch 3773 : 0.06839583069086075\n",
      "Training loss for batch 3774 : 0.16230438649654388\n",
      "Training loss for batch 3775 : 0.0648588240146637\n",
      "Training loss for batch 3776 : 0.1645994782447815\n",
      "Training loss for batch 3777 : 0.07399792969226837\n",
      "Training loss for batch 3778 : 0.308761328458786\n",
      "Training loss for batch 3779 : 0.17293041944503784\n",
      "Training loss for batch 3780 : 0.06897872686386108\n",
      "Training loss for batch 3781 : 0.09948407858610153\n",
      "Training loss for batch 3782 : 0.07249253988265991\n",
      "Training loss for batch 3783 : 0.1436212658882141\n",
      "Training loss for batch 3784 : 0.0991828665137291\n",
      "Training loss for batch 3785 : 0.14220601320266724\n",
      "Training loss for batch 3786 : 0.12743446230888367\n",
      "Training loss for batch 3787 : 0.2192029058933258\n",
      "Training loss for batch 3788 : 0.011948047205805779\n",
      "Training loss for batch 3789 : 0.18415841460227966\n",
      "Training loss for batch 3790 : 0.08115807175636292\n",
      "Training loss for batch 3791 : 0.13308918476104736\n",
      "Training loss for batch 3792 : 0.08837100118398666\n",
      "Training loss for batch 3793 : 0.0017101168632507324\n",
      "Training loss for batch 3794 : 0.21244530379772186\n",
      "Training loss for batch 3795 : 0.26020756363868713\n",
      "Training loss for batch 3796 : 0.12098246067762375\n",
      "Training loss for batch 3797 : 0.028108084574341774\n",
      "Training loss for batch 3798 : 0.0\n",
      "Training loss for batch 3799 : 0.11782802641391754\n",
      "Training loss for batch 3800 : 0.1309334933757782\n",
      "Training loss for batch 3801 : 0.05880022048950195\n",
      "Training loss for batch 3802 : 0.15224185585975647\n",
      "Training loss for batch 3803 : 0.17016036808490753\n",
      "Training loss for batch 3804 : 0.07280471175909042\n",
      "Training loss for batch 3805 : 0.294808954000473\n",
      "Training loss for batch 3806 : 0.1317700445652008\n",
      "Training loss for batch 3807 : 0.2675168216228485\n",
      "Training loss for batch 3808 : 0.11974890530109406\n",
      "Training loss for batch 3809 : 0.3766314387321472\n",
      "Training loss for batch 3810 : 0.03203361853957176\n",
      "Training loss for batch 3811 : 0.04458843916654587\n",
      "Training loss for batch 3812 : 0.20598670840263367\n",
      "Training loss for batch 3813 : 0.06367158144712448\n",
      "Training loss for batch 3814 : 0.04196333885192871\n",
      "Training loss for batch 3815 : 0.06876854598522186\n",
      "Training loss for batch 3816 : 0.36288172006607056\n",
      "Training loss for batch 3817 : 0.7699205875396729\n",
      "Training loss for batch 3818 : 0.0848081186413765\n",
      "Training loss for batch 3819 : 0.1733381599187851\n",
      "Training loss for batch 3820 : 0.12202584743499756\n",
      "Training loss for batch 3821 : 0.06495679914951324\n",
      "Training loss for batch 3822 : 0.21829062700271606\n",
      "Training loss for batch 3823 : 0.21245381236076355\n",
      "Training loss for batch 3824 : 0.05617735907435417\n",
      "Training loss for batch 3825 : 0.052333880215883255\n",
      "Training loss for batch 3826 : 0.09392432868480682\n",
      "Training loss for batch 3827 : 0.1728641390800476\n",
      "Training loss for batch 3828 : 0.014186816290020943\n",
      "Training loss for batch 3829 : 0.014247237704694271\n",
      "Training loss for batch 3830 : 0.21916942298412323\n",
      "Training loss for batch 3831 : 0.18464456498622894\n",
      "Training loss for batch 3832 : 0.053989872336387634\n",
      "Training loss for batch 3833 : 0.21683573722839355\n",
      "Training loss for batch 3834 : 0.10418148338794708\n",
      "Training loss for batch 3835 : 0.19257213175296783\n",
      "Training loss for batch 3836 : 0.10111065208911896\n",
      "Training loss for batch 3837 : 0.027374520897865295\n",
      "Training loss for batch 3838 : 0.06199578195810318\n",
      "Training loss for batch 3839 : 0.0629316046833992\n",
      "Training loss for batch 3840 : 0.0660739466547966\n",
      "Training loss for batch 3841 : 0.04081245884299278\n",
      "Training loss for batch 3842 : 0.23597809672355652\n",
      "Training loss for batch 3843 : 0.20935320854187012\n",
      "Training loss for batch 3844 : 0.03900358825922012\n",
      "Training loss for batch 3845 : 0.02911541983485222\n",
      "Training loss for batch 3846 : 0.15368583798408508\n",
      "Training loss for batch 3847 : 0.1494835615158081\n",
      "Training loss for batch 3848 : 0.07493193447589874\n",
      "Training loss for batch 3849 : 0.3095785081386566\n",
      "Training loss for batch 3850 : 0.02726167067885399\n",
      "Training loss for batch 3851 : 0.1228179782629013\n",
      "Training loss for batch 3852 : 0.04957841709256172\n",
      "Training loss for batch 3853 : 0.15467390418052673\n",
      "Training loss for batch 3854 : 0.013858919963240623\n",
      "Training loss for batch 3855 : 0.2628280222415924\n",
      "Training loss for batch 3856 : 0.20449787378311157\n",
      "Training loss for batch 3857 : 0.1307886689901352\n",
      "Training loss for batch 3858 : 0.005981807131320238\n",
      "Training loss for batch 3859 : 0.057672157883644104\n",
      "Training loss for batch 3860 : 0.18478722870349884\n",
      "Training loss for batch 3861 : 0.13818198442459106\n",
      "Training loss for batch 3862 : 0.029366258531808853\n",
      "Training loss for batch 3863 : 0.21824485063552856\n",
      "Training loss for batch 3864 : 0.16103485226631165\n",
      "Training loss for batch 3865 : 0.055249981582164764\n",
      "Training loss for batch 3866 : 0.0\n",
      "Training loss for batch 3867 : 0.33818402886390686\n",
      "Training loss for batch 3868 : 0.21258971095085144\n",
      "Training loss for batch 3869 : 0.2772842049598694\n",
      "Training loss for batch 3870 : 0.016056200489401817\n",
      "Training loss for batch 3871 : 0.0434892363846302\n",
      "Training loss for batch 3872 : 0.11003392189741135\n",
      "Training loss for batch 3873 : 0.14246097207069397\n",
      "Training loss for batch 3874 : 0.23037374019622803\n",
      "Training loss for batch 3875 : 0.1204383373260498\n",
      "Training loss for batch 3876 : 0.0999717116355896\n",
      "Training loss for batch 3877 : 0.2466590702533722\n",
      "Training loss for batch 3878 : 0.08944331109523773\n",
      "Training loss for batch 3879 : 0.29611560702323914\n",
      "Training loss for batch 3880 : 0.02030225470662117\n",
      "Training loss for batch 3881 : 0.14138241112232208\n",
      "Training loss for batch 3882 : 0.157842218875885\n",
      "Training loss for batch 3883 : 0.30064326524734497\n",
      "Training loss for batch 3884 : 0.0007308290805667639\n",
      "Training loss for batch 3885 : 0.028179103508591652\n",
      "Training loss for batch 3886 : 0.07889919728040695\n",
      "Training loss for batch 3887 : 0.24643640220165253\n",
      "Training loss for batch 3888 : 0.1442348062992096\n",
      "Training loss for batch 3889 : 0.053416021168231964\n",
      "Training loss for batch 3890 : 0.21462351083755493\n",
      "Training loss for batch 3891 : 0.04466225206851959\n",
      "Training loss for batch 3892 : 0.2134336531162262\n",
      "Training loss for batch 3893 : 0.09205218404531479\n",
      "Training loss for batch 3894 : 0.0723462924361229\n",
      "Training loss for batch 3895 : 0.0918477326631546\n",
      "Training loss for batch 3896 : 0.16227446496486664\n",
      "Training loss for batch 3897 : 0.13472726941108704\n",
      "Training loss for batch 3898 : 0.10088604688644409\n",
      "Training loss for batch 3899 : 0.02073758840560913\n",
      "Training loss for batch 3900 : 0.21082642674446106\n",
      "Training loss for batch 3901 : 0.004995266906917095\n",
      "Training loss for batch 3902 : 0.03827209770679474\n",
      "Training loss for batch 3903 : 0.023756887763738632\n",
      "Training loss for batch 3904 : 0.41951513290405273\n",
      "Training loss for batch 3905 : 0.10238681733608246\n",
      "Training loss for batch 3906 : 0.02782806009054184\n",
      "Training loss for batch 3907 : 0.2023429572582245\n",
      "Training loss for batch 3908 : 0.052319303154945374\n",
      "Training loss for batch 3909 : 0.2854897081851959\n",
      "Training loss for batch 3910 : 0.12122675776481628\n",
      "Training loss for batch 3911 : 0.1883792281150818\n",
      "Training loss for batch 3912 : 0.14129197597503662\n",
      "Training loss for batch 3913 : 0.16256073117256165\n",
      "Training loss for batch 3914 : 0.0769067257642746\n",
      "Training loss for batch 3915 : 0.06428419053554535\n",
      "Training loss for batch 3916 : 0.1278078705072403\n",
      "Training loss for batch 3917 : 0.15146677196025848\n",
      "Training loss for batch 3918 : 0.01802648790180683\n",
      "Training loss for batch 3919 : 0.1482924520969391\n",
      "Training loss for batch 3920 : -0.006990449968725443\n",
      "Training loss for batch 3921 : 0.16157978773117065\n",
      "Training loss for batch 3922 : 0.0\n",
      "Training loss for batch 3923 : 0.15169444680213928\n",
      "Training loss for batch 3924 : 0.21596631407737732\n",
      "Training loss for batch 3925 : 0.14857234060764313\n",
      "Training loss for batch 3926 : 0.3724399209022522\n",
      "Training loss for batch 3927 : 0.10364754498004913\n",
      "Training loss for batch 3928 : 0.006890147924423218\n",
      "Training loss for batch 3929 : 0.03233157843351364\n",
      "Training loss for batch 3930 : 0.028681688010692596\n",
      "Training loss for batch 3931 : -0.0035374269355088472\n",
      "Training loss for batch 3932 : 0.2412378489971161\n",
      "Training loss for batch 3933 : 0.0046429745852947235\n",
      "Training loss for batch 3934 : 0.04144850745797157\n",
      "Training loss for batch 3935 : 0.1081651896238327\n",
      "Training loss for batch 3936 : 0.005134944804012775\n",
      "Training loss for batch 3937 : 0.20482170581817627\n",
      "Training loss for batch 3938 : 0.2949966788291931\n",
      "Training loss for batch 3939 : 0.11074509471654892\n",
      "Training loss for batch 3940 : 0.012252043932676315\n",
      "Training loss for batch 3941 : 0.03218035399913788\n",
      "Training loss for batch 3942 : 0.011253759264945984\n",
      "Training loss for batch 3943 : 0.14665833115577698\n",
      "Training loss for batch 3944 : 0.24851810932159424\n",
      "Training loss for batch 3945 : 0.18868115544319153\n",
      "Training loss for batch 3946 : 0.17769531905651093\n",
      "Training loss for batch 3947 : 0.1937568336725235\n",
      "Training loss for batch 3948 : 0.38542425632476807\n",
      "Training loss for batch 3949 : 0.06031052768230438\n",
      "Training loss for batch 3950 : 0.01901695691049099\n",
      "Training loss for batch 3951 : 0.3144174814224243\n",
      "Training loss for batch 3952 : 0.0967414379119873\n",
      "Training loss for batch 3953 : 0.08748223632574081\n",
      "Training loss for batch 3954 : 0.11138571053743362\n",
      "Training loss for batch 3955 : 0.038610413670539856\n",
      "Training loss for batch 3956 : 0.18260256946086884\n",
      "Training loss for batch 3957 : 0.04217679798603058\n",
      "Training loss for batch 3958 : 0.05309358239173889\n",
      "Training loss for batch 3959 : 0.07103364914655685\n",
      "Training loss for batch 3960 : 0.16460436582565308\n",
      "Training loss for batch 3961 : 0.019595887511968613\n",
      "Training loss for batch 3962 : 0.1600084751844406\n",
      "Training loss for batch 3963 : 0.35971468687057495\n",
      "Training loss for batch 3964 : 0.04406177997589111\n",
      "Training loss for batch 3965 : 0.17880848050117493\n",
      "Training loss for batch 3966 : 0.11612066626548767\n",
      "Training loss for batch 3967 : 0.0008043536217883229\n",
      "Training loss for batch 3968 : 0.10044590383768082\n",
      "Training loss for batch 3969 : 0.23609524965286255\n",
      "Training loss for batch 3970 : 0.023552507162094116\n",
      "Training loss for batch 3971 : 0.0\n",
      "Training loss for batch 3972 : 0.12516959011554718\n",
      "Training loss for batch 3973 : 0.12664341926574707\n",
      "Training loss for batch 3974 : 0.014196524396538734\n",
      "Training loss for batch 3975 : 0.0\n",
      "Training loss for batch 3976 : 0.0667436420917511\n",
      "Training loss for batch 3977 : 0.051058389246463776\n",
      "Training loss for batch 3978 : 0.038668956607580185\n",
      "Training loss for batch 3979 : 0.11672517657279968\n",
      "Training loss for batch 3980 : 0.0\n",
      "Training loss for batch 3981 : 0.13367316126823425\n",
      "Training loss for batch 3982 : 0.09940426796674728\n",
      "Training loss for batch 3983 : 0.13910126686096191\n",
      "Training loss for batch 3984 : 0.15884168446063995\n",
      "Training loss for batch 3985 : 0.15726825594902039\n",
      "Training loss for batch 3986 : 0.40192168951034546\n",
      "Training loss for batch 3987 : 0.18880240619182587\n",
      "Training loss for batch 3988 : 0.16246646642684937\n",
      "Training loss for batch 3989 : 0.09582030773162842\n",
      "Training loss for batch 3990 : 0.16642330586910248\n",
      "Training loss for batch 3991 : 0.11328820884227753\n",
      "Training loss for batch 3992 : 0.20274999737739563\n",
      "Training loss for batch 3993 : 0.04048818349838257\n",
      "Training loss for batch 3994 : 0.008939504623413086\n",
      "Training loss for batch 3995 : 0.06702334433794022\n",
      "Training loss for batch 3996 : 0.22956380248069763\n",
      "Training loss for batch 3997 : 0.34053316712379456\n",
      "Training loss for batch 3998 : 0.1380745768547058\n",
      "Training loss for batch 3999 : 0.04471689462661743\n",
      "Training loss for batch 4000 : 0.07396131753921509\n",
      "Training loss for batch 4001 : 0.21263164281845093\n",
      "Training loss for batch 4002 : 0.1373589038848877\n",
      "Training loss for batch 4003 : 0.04206627979874611\n",
      "Training loss for batch 4004 : 0.16940903663635254\n",
      "Training loss for batch 4005 : 0.024203859269618988\n",
      "Training loss for batch 4006 : 0.04367818683385849\n",
      "Training loss for batch 4007 : 0.08944307267665863\n",
      "Training loss for batch 4008 : 0.016034159809350967\n",
      "Training loss for batch 4009 : 0.057996563613414764\n",
      "Training loss for batch 4010 : 0.0849592387676239\n",
      "Training loss for batch 4011 : 0.21148711442947388\n",
      "Training loss for batch 4012 : 0.09542613476514816\n",
      "Training loss for batch 4013 : 0.12634563446044922\n",
      "Training loss for batch 4014 : 0.0921030268073082\n",
      "Training loss for batch 4015 : 0.018219370394945145\n",
      "Training loss for batch 4016 : 0.2737662196159363\n",
      "Training loss for batch 4017 : 0.07047414779663086\n",
      "Training loss for batch 4018 : 0.615027666091919\n",
      "Training loss for batch 4019 : 0.001593301771208644\n",
      "Training loss for batch 4020 : 0.07645939290523529\n",
      "Training loss for batch 4021 : 0.22019743919372559\n",
      "Training loss for batch 4022 : 0.15672913193702698\n",
      "Training loss for batch 4023 : 0.0876893624663353\n",
      "Training loss for batch 4024 : 0.20209170877933502\n",
      "Training loss for batch 4025 : 0.1342368870973587\n",
      "Training loss for batch 4026 : 0.1442628800868988\n",
      "Training loss for batch 4027 : 0.224267840385437\n",
      "Training loss for batch 4028 : 0.20644456148147583\n",
      "Training loss for batch 4029 : 0.03312142938375473\n",
      "Training loss for batch 4030 : 0.2639285922050476\n",
      "Training loss for batch 4031 : 0.3160088062286377\n",
      "Training loss for batch 4032 : 0.2504722476005554\n",
      "Training loss for batch 4033 : 0.11503169685602188\n",
      "Training loss for batch 4034 : 0.0\n",
      "Training loss for batch 4035 : 0.11910039186477661\n",
      "Training loss for batch 4036 : 0.3720931112766266\n",
      "Training loss for batch 4037 : 0.1386345773935318\n",
      "Training loss for batch 4038 : 0.11416711658239365\n",
      "Training loss for batch 4039 : 0.2341894507408142\n",
      "Training loss for batch 4040 : 0.16425296664237976\n",
      "Training loss for batch 4041 : 0.007037170697003603\n",
      "Training loss for batch 4042 : 0.20509116351604462\n",
      "Training loss for batch 4043 : 0.19900469481945038\n",
      "Training loss for batch 4044 : 0.0035269223153591156\n",
      "Training loss for batch 4045 : 0.2491925209760666\n",
      "Training loss for batch 4046 : 0.01625490002334118\n",
      "Training loss for batch 4047 : 0.0\n",
      "Training loss for batch 4048 : 0.20803172886371613\n",
      "Training loss for batch 4049 : 0.04239116609096527\n",
      "Training loss for batch 4050 : 0.0405050590634346\n",
      "Training loss for batch 4051 : 0.16345804929733276\n",
      "Training loss for batch 4052 : 0.13047856092453003\n",
      "Training loss for batch 4053 : 0.04076400399208069\n",
      "Training loss for batch 4054 : 0.0002579689025878906\n",
      "Training loss for batch 4055 : 0.11996570229530334\n",
      "Training loss for batch 4056 : 0.37725991010665894\n",
      "Training loss for batch 4057 : 0.25153085589408875\n",
      "Training loss for batch 4058 : 0.1976020187139511\n",
      "Training loss for batch 4059 : 0.07446447014808655\n",
      "Training loss for batch 4060 : 0.017080625519156456\n",
      "Training loss for batch 4061 : 0.26232290267944336\n",
      "Training loss for batch 4062 : 0.12264396250247955\n",
      "Training loss for batch 4063 : 0.024219512939453125\n",
      "Training loss for batch 4064 : 0.09090414643287659\n",
      "Training loss for batch 4065 : 0.011330496519804\n",
      "Training loss for batch 4066 : 0.04069015383720398\n",
      "Training loss for batch 4067 : 0.09563615918159485\n",
      "Training loss for batch 4068 : 0.01791229099035263\n",
      "Training loss for batch 4069 : 0.08461009711027145\n",
      "Training loss for batch 4070 : 0.039412785321474075\n",
      "Training loss for batch 4071 : 0.14045006036758423\n",
      "Training loss for batch 4072 : 0.13415148854255676\n",
      "Training loss for batch 4073 : 0.14659228920936584\n",
      "Training loss for batch 4074 : 0.20681609213352203\n",
      "Training loss for batch 4075 : 0.1917123794555664\n",
      "Training loss for batch 4076 : 0.30932870507240295\n",
      "Training loss for batch 4077 : 0.2210184633731842\n",
      "Training loss for batch 4078 : 0.097648985683918\n",
      "Training loss for batch 4079 : 0.0448085255920887\n",
      "Training loss for batch 4080 : 0.2537005841732025\n",
      "Training loss for batch 4081 : 0.21046032011508942\n",
      "Training loss for batch 4082 : 0.2319488376379013\n",
      "Training loss for batch 4083 : 0.22586007416248322\n",
      "Training loss for batch 4084 : 0.252895325422287\n",
      "Training loss for batch 4085 : 0.38839295506477356\n",
      "Training loss for batch 4086 : 0.11133705079555511\n",
      "Training loss for batch 4087 : 0.18033047020435333\n",
      "Training loss for batch 4088 : 0.11788397282361984\n",
      "Training loss for batch 4089 : 0.19921967387199402\n",
      "Training loss for batch 4090 : 0.021011721342802048\n",
      "Training loss for batch 4091 : 0.10880051553249359\n",
      "Training loss for batch 4092 : 0.09564175456762314\n",
      "Training loss for batch 4093 : 0.03893892839550972\n",
      "Training loss for batch 4094 : 0.08989071100950241\n",
      "Training loss for batch 4095 : 0.006052643992006779\n",
      "Training loss for batch 4096 : -0.0007743385503999889\n",
      "Training loss for batch 4097 : 0.02348499186336994\n",
      "Training loss for batch 4098 : 0.06915437430143356\n",
      "Training loss for batch 4099 : 0.005556877236813307\n",
      "Training loss for batch 4100 : 0.018497146666049957\n",
      "Training loss for batch 4101 : 0.001768867252394557\n",
      "Training loss for batch 4102 : 0.15792721509933472\n",
      "Training loss for batch 4103 : 0.11170054972171783\n",
      "Training loss for batch 4104 : 0.4760783910751343\n",
      "Training loss for batch 4105 : 0.1786288321018219\n",
      "Training loss for batch 4106 : 0.0662560984492302\n",
      "Training loss for batch 4107 : 0.35272833704948425\n",
      "Training loss for batch 4108 : 0.19565331935882568\n",
      "Training loss for batch 4109 : 0.09167176485061646\n",
      "Training loss for batch 4110 : 0.017234336584806442\n",
      "Training loss for batch 4111 : 0.13590507209300995\n",
      "Training loss for batch 4112 : 0.10826359689235687\n",
      "Training loss for batch 4113 : 0.23577316105365753\n",
      "Training loss for batch 4114 : 0.12791980803012848\n",
      "Training loss for batch 4115 : 0.05190639942884445\n",
      "Training loss for batch 4116 : 0.18111135065555573\n",
      "Training loss for batch 4117 : 0.12139944732189178\n",
      "Training loss for batch 4118 : 0.017894914373755455\n",
      "Training loss for batch 4119 : 0.14600151777267456\n",
      "Training loss for batch 4120 : 0.5066803693771362\n",
      "Training loss for batch 4121 : 0.18903489410877228\n",
      "Training loss for batch 4122 : 0.20847845077514648\n",
      "Training loss for batch 4123 : 0.1807858645915985\n",
      "Training loss for batch 4124 : 0.0667668879032135\n",
      "Training loss for batch 4125 : 0.22472301125526428\n",
      "Training loss for batch 4126 : 0.03482044115662575\n",
      "Training loss for batch 4127 : 0.20081311464309692\n",
      "Training loss for batch 4128 : 0.07942230999469757\n",
      "Training loss for batch 4129 : 0.06568238884210587\n",
      "Training loss for batch 4130 : 0.10005347430706024\n",
      "Training loss for batch 4131 : 0.1287035048007965\n",
      "Training loss for batch 4132 : 0.0307694673538208\n",
      "Training loss for batch 4133 : 0.14567500352859497\n",
      "Training loss for batch 4134 : 0.059172630310058594\n",
      "Training loss for batch 4135 : 0.11052276194095612\n",
      "Training loss for batch 4136 : 0.07107052952051163\n",
      "Training loss for batch 4137 : 0.23993425071239471\n",
      "Training loss for batch 4138 : 0.3379392623901367\n",
      "Training loss for batch 4139 : 0.10686187446117401\n",
      "Training loss for batch 4140 : 0.09350142627954483\n",
      "Training loss for batch 4141 : 0.26063793897628784\n",
      "Training loss for batch 4142 : 0.035138633102178574\n",
      "Training loss for batch 4143 : 0.037332773208618164\n",
      "Training loss for batch 4144 : 0.060334257781505585\n",
      "Training loss for batch 4145 : 0.07303082197904587\n",
      "Training loss for batch 4146 : 0.08563598245382309\n",
      "Training loss for batch 4147 : 0.01980835199356079\n",
      "Training loss for batch 4148 : 0.07049574702978134\n",
      "Training loss for batch 4149 : 0.0\n",
      "Training loss for batch 4150 : 0.0778682678937912\n",
      "Training loss for batch 4151 : 0.10379700362682343\n",
      "Training loss for batch 4152 : 0.010242491960525513\n",
      "Training loss for batch 4153 : 0.09587528556585312\n",
      "Training loss for batch 4154 : 0.0559057891368866\n",
      "Training loss for batch 4155 : 0.010450475849211216\n",
      "Training loss for batch 4156 : 0.15380826592445374\n",
      "Training loss for batch 4157 : 0.14905939996242523\n",
      "Training loss for batch 4158 : 0.29081058502197266\n",
      "Training loss for batch 4159 : 0.21339192986488342\n",
      "Training loss for batch 4160 : 0.27394595742225647\n",
      "Training loss for batch 4161 : 0.010958731174468994\n",
      "Training loss for batch 4162 : 0.12917938828468323\n",
      "Training loss for batch 4163 : 0.10463147610425949\n",
      "Training loss for batch 4164 : 0.19129633903503418\n",
      "Training loss for batch 4165 : 0.3140296936035156\n",
      "Training loss for batch 4166 : 0.5804681181907654\n",
      "Training loss for batch 4167 : 0.03388596698641777\n",
      "Training loss for batch 4168 : 0.2685658633708954\n",
      "Training loss for batch 4169 : 0.16774752736091614\n",
      "Training loss for batch 4170 : 0.25057005882263184\n",
      "Training loss for batch 4171 : 0.1572367399930954\n",
      "Training loss for batch 4172 : 0.2340386062860489\n",
      "Training loss for batch 4173 : 0.20823828876018524\n",
      "Training loss for batch 4174 : 0.14567112922668457\n",
      "Training loss for batch 4175 : 0.024245979264378548\n",
      "Training loss for batch 4176 : 0.22474125027656555\n",
      "Training loss for batch 4177 : 0.00609422754496336\n",
      "Training loss for batch 4178 : 0.03268655762076378\n",
      "Training loss for batch 4179 : 0.06468982994556427\n",
      "Training loss for batch 4180 : 0.0\n",
      "Training loss for batch 4181 : 0.08033618330955505\n",
      "Training loss for batch 4182 : 0.19762033224105835\n",
      "Training loss for batch 4183 : 0.10149011760950089\n",
      "Training loss for batch 4184 : 0.04989725723862648\n",
      "Training loss for batch 4185 : 0.053535155951976776\n",
      "Training loss for batch 4186 : 0.11209115386009216\n",
      "Training loss for batch 4187 : 0.1242213100194931\n",
      "Training loss for batch 4188 : 0.3241100311279297\n",
      "Training loss for batch 4189 : 0.2178490161895752\n",
      "Training loss for batch 4190 : 0.01368999108672142\n",
      "Training loss for batch 4191 : 0.13432404398918152\n",
      "Training loss for batch 4192 : 0.19079995155334473\n",
      "Training loss for batch 4193 : 0.18553581833839417\n",
      "Training loss for batch 4194 : 0.09845362603664398\n",
      "Training loss for batch 4195 : 0.11648805439472198\n",
      "Training loss for batch 4196 : 0.24592570960521698\n",
      "Training loss for batch 4197 : 0.09100012481212616\n",
      "Training loss for batch 4198 : 0.023107066750526428\n",
      "Training loss for batch 4199 : 0.07174432277679443\n",
      "Training loss for batch 4200 : 0.15410277247428894\n",
      "Training loss for batch 4201 : 0.3444540798664093\n",
      "Training loss for batch 4202 : 0.05086366832256317\n",
      "Training loss for batch 4203 : 0.1020924523472786\n",
      "Training loss for batch 4204 : 0.18513105809688568\n",
      "Training loss for batch 4205 : 0.031901489943265915\n",
      "Training loss for batch 4206 : 0.1127128079533577\n",
      "Training loss for batch 4207 : 0.025297723710536957\n",
      "Training loss for batch 4208 : 0.012077822349965572\n",
      "Training loss for batch 4209 : 0.1437961459159851\n",
      "Training loss for batch 4210 : 0.1682756543159485\n",
      "Training loss for batch 4211 : 0.16508278250694275\n",
      "Training loss for batch 4212 : 0.005323489662259817\n",
      "Training loss for batch 4213 : 0.08874675631523132\n",
      "Training loss for batch 4214 : 0.2407909482717514\n",
      "Training loss for batch 4215 : 0.06266598403453827\n",
      "Training loss for batch 4216 : 0.16783180832862854\n",
      "Training loss for batch 4217 : 0.0806771069765091\n",
      "Training loss for batch 4218 : 0.08573292195796967\n",
      "Training loss for batch 4219 : 0.12016874551773071\n",
      "Training loss for batch 4220 : 0.20971840620040894\n",
      "Training loss for batch 4221 : 0.0005037784576416016\n",
      "Training loss for batch 4222 : 0.16080865263938904\n",
      "Training loss for batch 4223 : 0.07648992538452148\n",
      "Training loss for batch 4224 : 0.0014310507103800774\n",
      "Training loss for batch 4225 : 0.23771992325782776\n",
      "Training loss for batch 4226 : 0.2033747136592865\n",
      "Training loss for batch 4227 : 0.06344165652990341\n",
      "Training loss for batch 4228 : 0.1785251796245575\n",
      "Training loss for batch 4229 : 0.08005174994468689\n",
      "Training loss for batch 4230 : 0.14625084400177002\n",
      "Training loss for batch 4231 : 0.026436010375618935\n",
      "Training loss for batch 4232 : 0.3955903649330139\n",
      "Training loss for batch 4233 : 0.08645879477262497\n",
      "Training loss for batch 4234 : 0.08017522096633911\n",
      "Training loss for batch 4235 : 0.06353147327899933\n",
      "Training loss for batch 4236 : 0.05567946285009384\n",
      "Training loss for batch 4237 : 0.36660271883010864\n",
      "Training loss for batch 4238 : 0.06293076276779175\n",
      "Training loss for batch 4239 : 0.049796003848314285\n",
      "Training loss for batch 4240 : 0.048146966844797134\n",
      "Training loss for batch 4241 : 0.07230301201343536\n",
      "Training loss for batch 4242 : 0.0063664778135716915\n",
      "Training loss for batch 4243 : 0.11040143668651581\n",
      "Training loss for batch 4244 : 0.16205553710460663\n",
      "Training loss for batch 4245 : 0.028825975954532623\n",
      "Training loss for batch 4246 : 0.0337633341550827\n",
      "Training loss for batch 4247 : 0.1388591229915619\n",
      "Training loss for batch 4248 : 0.2275732457637787\n",
      "Training loss for batch 4249 : 0.03518224135041237\n",
      "Training loss for batch 4250 : 0.23100635409355164\n",
      "Training loss for batch 4251 : 0.01243077777326107\n",
      "Training loss for batch 4252 : 0.035269033163785934\n",
      "Training loss for batch 4253 : 0.0735318511724472\n",
      "Training loss for batch 4254 : 0.16092155873775482\n",
      "Training loss for batch 4255 : 0.23647856712341309\n",
      "Training loss for batch 4256 : 0.19854937493801117\n",
      "Training loss for batch 4257 : 0.031128982082009315\n",
      "Training loss for batch 4258 : 0.10523276776075363\n",
      "Training loss for batch 4259 : 0.0003506803186610341\n",
      "Training loss for batch 4260 : 0.05509451776742935\n",
      "Training loss for batch 4261 : 0.09613627940416336\n",
      "Training loss for batch 4262 : 0.058088261634111404\n",
      "Training loss for batch 4263 : 0.15884096920490265\n",
      "Training loss for batch 4264 : 0.24800150096416473\n",
      "Training loss for batch 4265 : 0.1306990683078766\n",
      "Training loss for batch 4266 : 0.2262355089187622\n",
      "Training loss for batch 4267 : 0.029635997489094734\n",
      "Training loss for batch 4268 : 0.1288626343011856\n",
      "Training loss for batch 4269 : 0.11793079972267151\n",
      "Training loss for batch 4270 : 0.1169116273522377\n",
      "Training loss for batch 4271 : 0.02449868619441986\n",
      "Training loss for batch 4272 : 0.17169146239757538\n",
      "Training loss for batch 4273 : 0.29331904649734497\n",
      "Training loss for batch 4274 : 0.17328877747058868\n",
      "Training loss for batch 4275 : 0.1712089478969574\n",
      "Training loss for batch 4276 : 0.15551479160785675\n",
      "Training loss for batch 4277 : 0.15618856251239777\n",
      "Training loss for batch 4278 : 0.1384764313697815\n",
      "Training loss for batch 4279 : 0.04799988120794296\n",
      "Training loss for batch 4280 : 0.01592209003865719\n",
      "Training loss for batch 4281 : 0.036502860486507416\n",
      "Training loss for batch 4282 : 0.08212064206600189\n",
      "Training loss for batch 4283 : 0.026721563190221786\n",
      "Training loss for batch 4284 : 0.38751208782196045\n",
      "Training loss for batch 4285 : 0.0478140264749527\n",
      "Training loss for batch 4286 : 0.1945413053035736\n",
      "Training loss for batch 4287 : 0.14766569435596466\n",
      "Training loss for batch 4288 : 0.24179264903068542\n",
      "Training loss for batch 4289 : 0.07985471189022064\n",
      "Training loss for batch 4290 : 0.15296784043312073\n",
      "Training loss for batch 4291 : 0.12561635673046112\n",
      "Training loss for batch 4292 : 0.2790583670139313\n",
      "Training loss for batch 4293 : 0.1696866899728775\n",
      "Training loss for batch 4294 : 0.08778741955757141\n",
      "Training loss for batch 4295 : 0.32247868180274963\n",
      "Training loss for batch 4296 : 0.23003101348876953\n",
      "Training loss for batch 4297 : 0.0\n",
      "Training loss for batch 4298 : 0.06832796335220337\n",
      "Training loss for batch 4299 : 0.1438232660293579\n",
      "Training loss for batch 4300 : 0.07673268765211105\n",
      "Training loss for batch 4301 : 0.10095468163490295\n",
      "Training loss for batch 4302 : 0.009331421926617622\n",
      "Training loss for batch 4303 : 0.04437742754817009\n",
      "Training loss for batch 4304 : 0.2594324052333832\n",
      "Training loss for batch 4305 : 0.0\n",
      "Training loss for batch 4306 : 0.2742304801940918\n",
      "Training loss for batch 4307 : 0.000493336992803961\n",
      "Training loss for batch 4308 : 0.13474038243293762\n",
      "Training loss for batch 4309 : 0.18309685587882996\n",
      "Training loss for batch 4310 : 0.26131269335746765\n",
      "Training loss for batch 4311 : 0.008092091418802738\n",
      "Training loss for batch 4312 : 0.16103878617286682\n",
      "Training loss for batch 4313 : 0.49171382188796997\n",
      "Training loss for batch 4314 : 0.18787911534309387\n",
      "Training loss for batch 4315 : 0.0677017942070961\n",
      "Training loss for batch 4316 : 0.14114734530448914\n",
      "Training loss for batch 4317 : 0.040268752723932266\n",
      "Training loss for batch 4318 : 0.09257474541664124\n",
      "Training loss for batch 4319 : 0.12631163001060486\n",
      "Training loss for batch 4320 : 0.15923917293548584\n",
      "Training loss for batch 4321 : 0.07532480359077454\n",
      "Training loss for batch 4322 : 0.02688692882657051\n",
      "Training loss for batch 4323 : 0.19326406717300415\n",
      "Training loss for batch 4324 : 0.047716837376356125\n",
      "Training loss for batch 4325 : 0.08769116550683975\n",
      "Training loss for batch 4326 : 0.22131425142288208\n",
      "Training loss for batch 4327 : 0.21711871027946472\n",
      "Training loss for batch 4328 : 0.21218860149383545\n",
      "Training loss for batch 4329 : 0.11753661185503006\n",
      "Training loss for batch 4330 : 0.2135949581861496\n",
      "Training loss for batch 4331 : 0.027421802282333374\n",
      "Training loss for batch 4332 : 0.1666770875453949\n",
      "Training loss for batch 4333 : 0.09583396464586258\n",
      "Training loss for batch 4334 : 0.1420127898454666\n",
      "Training loss for batch 4335 : 0.04696370288729668\n",
      "Training loss for batch 4336 : 0.15340732038021088\n",
      "Training loss for batch 4337 : 0.0\n",
      "Training loss for batch 4338 : 0.005549279972910881\n",
      "Training loss for batch 4339 : -0.0013152873143553734\n",
      "Training loss for batch 4340 : 0.40051546692848206\n",
      "Training loss for batch 4341 : -0.000739272334612906\n",
      "Training loss for batch 4342 : 0.15283456444740295\n",
      "Training loss for batch 4343 : 0.11382833868265152\n",
      "Training loss for batch 4344 : 0.16982467472553253\n",
      "Training loss for batch 4345 : 0.2729586064815521\n",
      "Training loss for batch 4346 : 0.26646870374679565\n",
      "Training loss for batch 4347 : 0.020268291234970093\n",
      "Training loss for batch 4348 : 0.165205180644989\n",
      "Training loss for batch 4349 : 0.16949479281902313\n",
      "Training loss for batch 4350 : 0.19928479194641113\n",
      "Training loss for batch 4351 : 0.2837899923324585\n",
      "Training loss for batch 4352 : 0.08229011297225952\n",
      "Training loss for batch 4353 : 0.2890836298465729\n",
      "Training loss for batch 4354 : 0.12547877430915833\n",
      "Training loss for batch 4355 : 0.13374589383602142\n",
      "Training loss for batch 4356 : 0.05229185149073601\n",
      "Training loss for batch 4357 : 0.279973566532135\n",
      "Training loss for batch 4358 : 0.197750523686409\n",
      "Training loss for batch 4359 : 0.031247243285179138\n",
      "Training loss for batch 4360 : 0.13086946308612823\n",
      "Training loss for batch 4361 : 0.07676912099123001\n",
      "Training loss for batch 4362 : 0.3183613717556\n",
      "Training loss for batch 4363 : 0.03521529957652092\n",
      "Training loss for batch 4364 : 0.03078024461865425\n",
      "Training loss for batch 4365 : 0.18840694427490234\n",
      "Training loss for batch 4366 : 0.018001176416873932\n",
      "Training loss for batch 4367 : 0.106199249625206\n",
      "Training loss for batch 4368 : 0.1761474758386612\n",
      "Training loss for batch 4369 : 0.05359999090433121\n",
      "Training loss for batch 4370 : 0.07135173678398132\n",
      "Training loss for batch 4371 : 0.014295090921223164\n",
      "Training loss for batch 4372 : 0.4193973243236542\n",
      "Training loss for batch 4373 : 0.08785372972488403\n",
      "Training loss for batch 4374 : 0.2739372253417969\n",
      "Training loss for batch 4375 : 0.11142401397228241\n",
      "Training loss for batch 4376 : 0.05319969356060028\n",
      "Training loss for batch 4377 : 0.1367557793855667\n",
      "Training loss for batch 4378 : 0.015830449759960175\n",
      "Training loss for batch 4379 : 0.0979049801826477\n",
      "Training loss for batch 4380 : 0.053908489644527435\n",
      "Training loss for batch 4381 : 0.07059141993522644\n",
      "Training loss for batch 4382 : 0.1846771538257599\n",
      "Training loss for batch 4383 : 0.14455316960811615\n",
      "Training loss for batch 4384 : 0.0023237960413098335\n",
      "Training loss for batch 4385 : 0.1978294402360916\n",
      "Training loss for batch 4386 : 0.16769291460514069\n",
      "Training loss for batch 4387 : 0.07929161190986633\n",
      "Training loss for batch 4388 : 0.06927170604467392\n",
      "Training loss for batch 4389 : 0.17637977004051208\n",
      "Training loss for batch 4390 : 0.06173490360379219\n",
      "Training loss for batch 4391 : 0.2899351716041565\n",
      "Training loss for batch 4392 : 0.0007567442953586578\n",
      "Training loss for batch 4393 : 0.04101931303739548\n",
      "Training loss for batch 4394 : 0.10876541584730148\n",
      "Training loss for batch 4395 : 0.08206872642040253\n",
      "Training loss for batch 4396 : 0.13341476023197174\n",
      "Training loss for batch 4397 : 0.02190735563635826\n",
      "Training loss for batch 4398 : 0.034107521176338196\n",
      "Training loss for batch 4399 : 0.1944912075996399\n",
      "Training loss for batch 4400 : 0.03262984752655029\n",
      "Training loss for batch 4401 : 0.028878219425678253\n",
      "Training loss for batch 4402 : 0.3373917043209076\n",
      "Training loss for batch 4403 : 0.26793456077575684\n",
      "Training loss for batch 4404 : 0.14141792058944702\n",
      "Training loss for batch 4405 : 0.07238003611564636\n",
      "Training loss for batch 4406 : 0.06077123433351517\n",
      "Training loss for batch 4407 : 0.09547558426856995\n",
      "Training loss for batch 4408 : 0.24654732644557953\n",
      "Training loss for batch 4409 : 0.002129793167114258\n",
      "Training loss for batch 4410 : 0.17715997993946075\n",
      "Training loss for batch 4411 : 0.043759703636169434\n",
      "Training loss for batch 4412 : 0.12465586513280869\n",
      "Training loss for batch 4413 : 0.26370882987976074\n",
      "Training loss for batch 4414 : 0.17615297436714172\n",
      "Training loss for batch 4415 : 0.13231682777404785\n",
      "Training loss for batch 4416 : 0.14302442967891693\n",
      "Training loss for batch 4417 : 0.3221110999584198\n",
      "Training loss for batch 4418 : 0.011573953554034233\n",
      "Training loss for batch 4419 : 0.20730715990066528\n",
      "Training loss for batch 4420 : 0.0078059351071715355\n",
      "Training loss for batch 4421 : 0.22035929560661316\n",
      "Training loss for batch 4422 : 0.2332298457622528\n",
      "Training loss for batch 4423 : 0.10871770977973938\n",
      "Training loss for batch 4424 : 0.004639424383640289\n",
      "Training loss for batch 4425 : 0.09327566623687744\n",
      "Training loss for batch 4426 : 0.04828641191124916\n",
      "Training loss for batch 4427 : 0.21290545165538788\n",
      "Training loss for batch 4428 : 0.13857866823673248\n",
      "Training loss for batch 4429 : 0.14289672672748566\n",
      "Training loss for batch 4430 : 0.15330570936203003\n",
      "Training loss for batch 4431 : 0.1569070667028427\n",
      "Training loss for batch 4432 : 0.19029314815998077\n",
      "Training loss for batch 4433 : 0.16968518495559692\n",
      "Training loss for batch 4434 : 0.15206678211688995\n",
      "Training loss for batch 4435 : 0.09488920867443085\n",
      "Training loss for batch 4436 : 0.19598348438739777\n",
      "Training loss for batch 4437 : 0.05457068234682083\n",
      "Training loss for batch 4438 : 0.016072701662778854\n",
      "Training loss for batch 4439 : 0.15643590688705444\n",
      "Training loss for batch 4440 : 0.0017133455257862806\n",
      "Training loss for batch 4441 : 0.06747844070196152\n",
      "Training loss for batch 4442 : 0.008595381863415241\n",
      "Training loss for batch 4443 : 0.02125539444386959\n",
      "Training loss for batch 4444 : 0.09356652945280075\n",
      "Training loss for batch 4445 : 0.036712151020765305\n",
      "Training loss for batch 4446 : 0.05705723166465759\n",
      "Training loss for batch 4447 : 0.08031854778528214\n",
      "Training loss for batch 4448 : 0.12335383147001266\n",
      "Training loss for batch 4449 : 0.04674212634563446\n",
      "Training loss for batch 4450 : 0.21257472038269043\n",
      "Training loss for batch 4451 : 0.03837313503026962\n",
      "Training loss for batch 4452 : 0.13661019504070282\n",
      "Training loss for batch 4453 : 0.20516817271709442\n",
      "Training loss for batch 4454 : 0.34719204902648926\n",
      "Training loss for batch 4455 : 0.09751604497432709\n",
      "Training loss for batch 4456 : 0.044749096035957336\n",
      "Training loss for batch 4457 : 0.05133865028619766\n",
      "Training loss for batch 4458 : 0.21403729915618896\n",
      "Training loss for batch 4459 : 0.2085568755865097\n",
      "Training loss for batch 4460 : 0.14530374109745026\n",
      "Training loss for batch 4461 : 0.06519557535648346\n",
      "Training loss for batch 4462 : 0.09360797703266144\n",
      "Training loss for batch 4463 : 0.1395639181137085\n",
      "Training loss for batch 4464 : 0.04349213093519211\n",
      "Training loss for batch 4465 : 0.19805675745010376\n",
      "Training loss for batch 4466 : 0.05681140348315239\n",
      "Training loss for batch 4467 : 0.09067941457033157\n",
      "Training loss for batch 4468 : 0.0639311820268631\n",
      "Training loss for batch 4469 : 0.25087007880210876\n",
      "Training loss for batch 4470 : 0.024648597463965416\n",
      "Training loss for batch 4471 : 0.024504130706191063\n",
      "Training loss for batch 4472 : 0.025585714727640152\n",
      "Training loss for batch 4473 : 0.15176981687545776\n",
      "Training loss for batch 4474 : 0.1907145082950592\n",
      "Training loss for batch 4475 : 0.08676580339670181\n",
      "Training loss for batch 4476 : 0.006009012460708618\n",
      "Training loss for batch 4477 : 0.35658228397369385\n",
      "Training loss for batch 4478 : 0.04174135997891426\n",
      "Training loss for batch 4479 : 0.2023981660604477\n",
      "Training loss for batch 4480 : 0.021915804594755173\n",
      "Training loss for batch 4481 : 0.15075074136257172\n",
      "Training loss for batch 4482 : 0.12653382122516632\n",
      "Training loss for batch 4483 : 0.14636574685573578\n",
      "Training loss for batch 4484 : 0.11430789530277252\n",
      "Training loss for batch 4485 : 0.34119048714637756\n",
      "Training loss for batch 4486 : 0.03661154955625534\n",
      "Training loss for batch 4487 : 0.21538032591342926\n",
      "Training loss for batch 4488 : 0.0015557905426248908\n",
      "Training loss for batch 4489 : 0.053292203694581985\n",
      "Training loss for batch 4490 : 0.01617411896586418\n",
      "Training loss for batch 4491 : 0.18988434970378876\n",
      "Training loss for batch 4492 : 0.3165269196033478\n",
      "Training loss for batch 4493 : 0.15919893980026245\n",
      "Training loss for batch 4494 : 0.12951889634132385\n",
      "Training loss for batch 4495 : 0.2540537714958191\n",
      "Training loss for batch 4496 : 0.2779805064201355\n",
      "Training loss for batch 4497 : 0.1127261146903038\n",
      "Training loss for batch 4498 : 0.08330950140953064\n",
      "Training loss for batch 4499 : 0.01558296475559473\n",
      "Training loss for batch 4500 : 0.16981713473796844\n",
      "Training loss for batch 4501 : 0.1473609209060669\n",
      "Training loss for batch 4502 : 0.16068634390830994\n",
      "Training loss for batch 4503 : 0.10714728385210037\n",
      "Training loss for batch 4504 : 0.05693822354078293\n",
      "Training loss for batch 4505 : 0.027098573744297028\n",
      "Training loss for batch 4506 : -0.0029093027114868164\n",
      "Training loss for batch 4507 : 0.15571823716163635\n",
      "Training loss for batch 4508 : 0.19681277871131897\n",
      "Training loss for batch 4509 : 0.22365868091583252\n",
      "Training loss for batch 4510 : 0.12721000611782074\n",
      "Training loss for batch 4511 : 0.13504119217395782\n",
      "Training loss for batch 4512 : 0.05454644560813904\n",
      "Training loss for batch 4513 : 0.18582451343536377\n",
      "Training loss for batch 4514 : 0.22694100439548492\n",
      "Training loss for batch 4515 : 0.24119697511196136\n",
      "Training loss for batch 4516 : 0.16806036233901978\n",
      "Training loss for batch 4517 : 0.20208317041397095\n",
      "Training loss for batch 4518 : 0.4008420407772064\n",
      "Training loss for batch 4519 : 0.2898765802383423\n",
      "Training loss for batch 4520 : 0.11960295587778091\n",
      "Training loss for batch 4521 : 0.03830862045288086\n",
      "Training loss for batch 4522 : 0.00908834207803011\n",
      "Training loss for batch 4523 : 0.16837486624717712\n",
      "Training loss for batch 4524 : 0.05101385712623596\n",
      "Training loss for batch 4525 : 0.11058711260557175\n",
      "Training loss for batch 4526 : 0.16790233552455902\n",
      "Training loss for batch 4527 : 0.0370730459690094\n",
      "Training loss for batch 4528 : 0.06279977411031723\n",
      "Training loss for batch 4529 : 0.24527417123317719\n",
      "Training loss for batch 4530 : 0.3942762017250061\n",
      "Training loss for batch 4531 : 0.07737304270267487\n",
      "Training loss for batch 4532 : 0.0664202868938446\n",
      "Training loss for batch 4533 : 0.11326302587985992\n",
      "Training loss for batch 4534 : 0.00963513646274805\n",
      "Training loss for batch 4535 : 0.3748284578323364\n",
      "Training loss for batch 4536 : 0.06406474113464355\n",
      "Training loss for batch 4537 : 0.14563681185245514\n",
      "Training loss for batch 4538 : 0.35044538974761963\n",
      "Training loss for batch 4539 : 0.07938337326049805\n",
      "Training loss for batch 4540 : 0.18630027770996094\n",
      "Training loss for batch 4541 : 0.11966066807508469\n",
      "Training loss for batch 4542 : 0.06489385664463043\n",
      "Training loss for batch 4543 : 0.08779673278331757\n",
      "Training loss for batch 4544 : 0.13991737365722656\n",
      "Training loss for batch 4545 : 0.21962150931358337\n",
      "Training loss for batch 4546 : -0.002082876395434141\n",
      "Training loss for batch 4547 : 0.06888356804847717\n",
      "Training loss for batch 4548 : 0.0025345152243971825\n",
      "Training loss for batch 4549 : 0.07815167307853699\n",
      "Training loss for batch 4550 : 0.08929868042469025\n",
      "Training loss for batch 4551 : 0.05578627809882164\n",
      "Training loss for batch 4552 : 0.06777560710906982\n",
      "Training loss for batch 4553 : 0.0024263872765004635\n",
      "Training loss for batch 4554 : 0.2557763159275055\n",
      "Training loss for batch 4555 : 0.17446108162403107\n",
      "Training loss for batch 4556 : 0.18026414513587952\n",
      "Training loss for batch 4557 : 0.047072671353816986\n",
      "Training loss for batch 4558 : 0.010885858908295631\n",
      "Training loss for batch 4559 : 0.3337835967540741\n",
      "Training loss for batch 4560 : 0.1443081945180893\n",
      "Training loss for batch 4561 : 0.018818771466612816\n",
      "Training loss for batch 4562 : 0.12247124314308167\n",
      "Training loss for batch 4563 : 0.26009052991867065\n",
      "Training loss for batch 4564 : 0.07129528373479843\n",
      "Training loss for batch 4565 : 0.03371158242225647\n",
      "Training loss for batch 4566 : 0.03854598104953766\n",
      "Training loss for batch 4567 : 0.22210273146629333\n",
      "Training loss for batch 4568 : 0.13600601255893707\n",
      "Training loss for batch 4569 : 0.12634329497814178\n",
      "Training loss for batch 4570 : 0.11310537159442902\n",
      "Training loss for batch 4571 : 0.07228144258260727\n",
      "Training loss for batch 4572 : 0.09757132083177567\n",
      "Training loss for batch 4573 : 0.04577965661883354\n",
      "Training loss for batch 4574 : 0.051777325570583344\n",
      "Training loss for batch 4575 : 0.10273277014493942\n",
      "Training loss for batch 4576 : 0.1139192208647728\n",
      "Training loss for batch 4577 : 0.08429767191410065\n",
      "Training loss for batch 4578 : 0.04387406259775162\n",
      "Training loss for batch 4579 : 0.15555527806282043\n",
      "Training loss for batch 4580 : 0.07692238688468933\n",
      "Training loss for batch 4581 : 0.018342431634664536\n",
      "Training loss for batch 4582 : 0.06997545808553696\n",
      "Training loss for batch 4583 : -0.0011910049943253398\n",
      "Training loss for batch 4584 : 0.019038861617445946\n",
      "Training loss for batch 4585 : 0.08350066840648651\n",
      "Training loss for batch 4586 : 0.23961254954338074\n",
      "Training loss for batch 4587 : 0.17076891660690308\n",
      "Training loss for batch 4588 : 0.19644314050674438\n",
      "Training loss for batch 4589 : 0.2817234992980957\n",
      "Training loss for batch 4590 : 0.09721699357032776\n",
      "Training loss for batch 4591 : 0.008931681513786316\n",
      "Training loss for batch 4592 : 0.055681262165308\n",
      "Training loss for batch 4593 : 0.12169875204563141\n",
      "Training loss for batch 4594 : -0.0008922223933041096\n",
      "Training loss for batch 4595 : 0.07020801305770874\n",
      "Training loss for batch 4596 : 0.054501667618751526\n",
      "Training loss for batch 4597 : 0.17534612119197845\n",
      "Training loss for batch 4598 : 0.08570541441440582\n",
      "Training loss for batch 4599 : 0.07667381316423416\n",
      "Training loss for batch 4600 : 0.15810702741146088\n",
      "Training loss for batch 4601 : 0.12619706988334656\n",
      "Training loss for batch 4602 : 0.16948054730892181\n",
      "Training loss for batch 4603 : 0.0\n",
      "Training loss for batch 4604 : 0.17897260189056396\n",
      "Training loss for batch 4605 : 0.01546934898942709\n",
      "Training loss for batch 4606 : 0.08989610522985458\n",
      "Training loss for batch 4607 : 0.050274282693862915\n",
      "Training loss for batch 4608 : 0.02930651791393757\n",
      "Training loss for batch 4609 : 0.2087799608707428\n",
      "Training loss for batch 4610 : 0.18405386805534363\n",
      "Training loss for batch 4611 : 0.07063700258731842\n",
      "Training loss for batch 4612 : -0.0019205986754968762\n",
      "Training loss for batch 4613 : 0.08987410366535187\n",
      "Training loss for batch 4614 : 0.16604015231132507\n",
      "Training loss for batch 4615 : 0.37364718317985535\n",
      "Training loss for batch 4616 : 0.006241997238248587\n",
      "Training loss for batch 4617 : 0.4709198474884033\n",
      "Training loss for batch 4618 : 0.11227497458457947\n",
      "Training loss for batch 4619 : 0.10240555554628372\n",
      "Training loss for batch 4620 : 0.0516909621655941\n",
      "Training loss for batch 4621 : 0.18349887430667877\n",
      "Training loss for batch 4622 : 0.050303708761930466\n",
      "Training loss for batch 4623 : 0.19874870777130127\n",
      "Training loss for batch 4624 : 0.022952429950237274\n",
      "Training loss for batch 4625 : 0.4217848777770996\n",
      "Training loss for batch 4626 : 0.060210198163986206\n",
      "Training loss for batch 4627 : -0.0011372396256774664\n",
      "Training loss for batch 4628 : 0.06920015066862106\n",
      "Training loss for batch 4629 : 0.04085441678762436\n",
      "Training loss for batch 4630 : 0.257046103477478\n",
      "Training loss for batch 4631 : 0.1870059221982956\n",
      "Training loss for batch 4632 : 0.1519157886505127\n",
      "Training loss for batch 4633 : 0.16965433955192566\n",
      "Training loss for batch 4634 : 0.24211373925209045\n",
      "Training loss for batch 4635 : 0.14627738296985626\n",
      "Training loss for batch 4636 : 0.0366419292986393\n",
      "Training loss for batch 4637 : 0.09893346577882767\n",
      "Training loss for batch 4638 : 0.03827390819787979\n",
      "Training loss for batch 4639 : 0.06651324033737183\n",
      "Training loss for batch 4640 : 0.1054266095161438\n",
      "Training loss for batch 4641 : 0.18407730758190155\n",
      "Training loss for batch 4642 : 0.030117927119135857\n",
      "Training loss for batch 4643 : 0.06008881330490112\n",
      "Training loss for batch 4644 : -0.00496447691693902\n",
      "Training loss for batch 4645 : 0.08627724647521973\n",
      "Training loss for batch 4646 : 0.0942889004945755\n",
      "Training loss for batch 4647 : 0.1787358671426773\n",
      "Training loss for batch 4648 : 0.306827187538147\n",
      "Training loss for batch 4649 : 0.10614944994449615\n",
      "Training loss for batch 4650 : 0.08460663259029388\n",
      "Training loss for batch 4651 : 0.1432020515203476\n",
      "Training loss for batch 4652 : 0.21046534180641174\n",
      "Training loss for batch 4653 : 0.14084002375602722\n",
      "Training loss for batch 4654 : 0.0\n",
      "Training loss for batch 4655 : 0.0823911800980568\n",
      "Training loss for batch 4656 : 0.003153732046484947\n",
      "Training loss for batch 4657 : 0.152682363986969\n",
      "Training loss for batch 4658 : 0.1821749359369278\n",
      "Training loss for batch 4659 : 0.1762676239013672\n",
      "Training loss for batch 4660 : 0.056188106536865234\n",
      "Training loss for batch 4661 : 0.2545810043811798\n",
      "Training loss for batch 4662 : 0.03250811621546745\n",
      "Training loss for batch 4663 : 0.1816103458404541\n",
      "Training loss for batch 4664 : 0.2351210117340088\n",
      "Training loss for batch 4665 : 0.2903343439102173\n",
      "Training loss for batch 4666 : 0.033318206667900085\n",
      "Training loss for batch 4667 : 0.18326708674430847\n",
      "Training loss for batch 4668 : 0.07426341623067856\n",
      "Training loss for batch 4669 : 0.12242741882801056\n",
      "Training loss for batch 4670 : 0.0886005386710167\n",
      "Training loss for batch 4671 : 0.0\n",
      "Training loss for batch 4672 : 0.05500062555074692\n",
      "Training loss for batch 4673 : 0.14922454953193665\n",
      "Training loss for batch 4674 : 0.028991110622882843\n",
      "Training loss for batch 4675 : 0.11434099078178406\n",
      "Training loss for batch 4676 : 0.2898907959461212\n",
      "Training loss for batch 4677 : 0.00625546183437109\n",
      "Training loss for batch 4678 : 0.26998239755630493\n",
      "Training loss for batch 4679 : -0.00010792571993079036\n",
      "Training loss for batch 4680 : 0.12137618660926819\n",
      "Training loss for batch 4681 : 0.07022509723901749\n",
      "Training loss for batch 4682 : 0.24182862043380737\n",
      "Training loss for batch 4683 : 0.008620808832347393\n",
      "Training loss for batch 4684 : 0.027503713965415955\n",
      "Training loss for batch 4685 : 0.023475803434848785\n",
      "Training loss for batch 4686 : 0.0017857750644907355\n",
      "Training loss for batch 4687 : 0.05915669724345207\n",
      "Training loss for batch 4688 : 0.02014167048037052\n",
      "Training loss for batch 4689 : 0.10219843685626984\n",
      "Training loss for batch 4690 : 0.2995723485946655\n",
      "Training loss for batch 4691 : 0.06245968118309975\n",
      "Training loss for batch 4692 : 0.2462138682603836\n",
      "Training loss for batch 4693 : 0.3128957748413086\n",
      "Training loss for batch 4694 : 0.2990761697292328\n",
      "Training loss for batch 4695 : 0.14348651468753815\n",
      "Training loss for batch 4696 : 0.4174061119556427\n",
      "Training loss for batch 4697 : 0.14261040091514587\n",
      "Training loss for batch 4698 : 0.09950296580791473\n",
      "Training loss for batch 4699 : 0.16848385334014893\n",
      "Training loss for batch 4700 : 0.0823473185300827\n",
      "Training loss for batch 4701 : 0.05083788186311722\n",
      "Training loss for batch 4702 : 0.04102981090545654\n",
      "Training loss for batch 4703 : 0.12872982025146484\n",
      "Training loss for batch 4704 : 0.1982651948928833\n",
      "Training loss for batch 4705 : 0.02541973814368248\n",
      "Training loss for batch 4706 : 0.2923208177089691\n",
      "Training loss for batch 4707 : 0.008100799284875393\n",
      "Training loss for batch 4708 : 0.04468676447868347\n",
      "Training loss for batch 4709 : 0.26805517077445984\n",
      "Training loss for batch 4710 : 0.14260253310203552\n",
      "Training loss for batch 4711 : 0.16070222854614258\n",
      "Training loss for batch 4712 : 0.1482127606868744\n",
      "Training loss for batch 4713 : 0.28214532136917114\n",
      "Training loss for batch 4714 : 0.06358350068330765\n",
      "Training loss for batch 4715 : 0.011980310082435608\n",
      "Training loss for batch 4716 : 0.12414225935935974\n",
      "Training loss for batch 4717 : 0.044233690947294235\n",
      "Training loss for batch 4718 : 0.06382514536380768\n",
      "Training loss for batch 4719 : 0.053932465612888336\n",
      "Training loss for batch 4720 : 0.2518521845340729\n",
      "Training loss for batch 4721 : 0.07641282677650452\n",
      "Training loss for batch 4722 : 0.03541097790002823\n",
      "Training loss for batch 4723 : 0.06116852909326553\n",
      "Training loss for batch 4724 : 0.20780441164970398\n",
      "Training loss for batch 4725 : 0.3464360535144806\n",
      "Training loss for batch 4726 : 0.09620648622512817\n",
      "Training loss for batch 4727 : 0.17948731780052185\n",
      "Training loss for batch 4728 : 0.04858298972249031\n",
      "Training loss for batch 4729 : 0.11419457942247391\n",
      "Training loss for batch 4730 : 0.06813889741897583\n",
      "Training loss for batch 4731 : 0.19958236813545227\n",
      "Training loss for batch 4732 : 0.07632269710302353\n",
      "Training loss for batch 4733 : 0.3203897178173065\n",
      "Training loss for batch 4734 : 0.16641318798065186\n",
      "Training loss for batch 4735 : 0.21615605056285858\n",
      "Training loss for batch 4736 : 0.07505825161933899\n",
      "Training loss for batch 4737 : 0.055850643664598465\n",
      "Training loss for batch 4738 : 0.06740140914916992\n",
      "Training loss for batch 4739 : 0.4130874276161194\n",
      "Training loss for batch 4740 : 0.10207467526197433\n",
      "Training loss for batch 4741 : 0.13025513291358948\n",
      "Training loss for batch 4742 : 0.05619727075099945\n",
      "Training loss for batch 4743 : 0.1276695877313614\n",
      "Training loss for batch 4744 : 0.19158540666103363\n",
      "Training loss for batch 4745 : 0.06851035356521606\n",
      "Training loss for batch 4746 : 0.0784035176038742\n",
      "Training loss for batch 4747 : 0.07322493940591812\n",
      "Training loss for batch 4748 : 0.0989912748336792\n",
      "Training loss for batch 4749 : 0.08495314419269562\n",
      "Training loss for batch 4750 : 0.11284245550632477\n",
      "Training loss for batch 4751 : 0.0684615895152092\n",
      "Training loss for batch 4752 : 0.035301052033901215\n",
      "Training loss for batch 4753 : 0.0019354920368641615\n",
      "Training loss for batch 4754 : 0.05020793154835701\n",
      "Training loss for batch 4755 : 0.15036575496196747\n",
      "Training loss for batch 4756 : 0.015773266553878784\n",
      "Training loss for batch 4757 : 0.29303425550460815\n",
      "Training loss for batch 4758 : 0.034952811896800995\n",
      "Training loss for batch 4759 : 0.05174769088625908\n",
      "Training loss for batch 4760 : 0.041636351495981216\n",
      "Training loss for batch 4761 : 0.03085256740450859\n",
      "Training loss for batch 4762 : 0.14742590487003326\n",
      "Training loss for batch 4763 : 0.12007297575473785\n",
      "Training loss for batch 4764 : 0.0380176305770874\n",
      "Training loss for batch 4765 : 0.03440999984741211\n",
      "Training loss for batch 4766 : 0.17741043865680695\n",
      "Training loss for batch 4767 : 0.08800573647022247\n",
      "Training loss for batch 4768 : 0.28562912344932556\n",
      "Training loss for batch 4769 : 0.21026162803173065\n",
      "Training loss for batch 4770 : 0.31049099564552307\n",
      "Training loss for batch 4771 : -0.008976000361144543\n",
      "Training loss for batch 4772 : 0.09577644616365433\n",
      "Training loss for batch 4773 : 0.024498799815773964\n",
      "Training loss for batch 4774 : 0.09608238935470581\n",
      "Training loss for batch 4775 : 0.004636804573237896\n",
      "Training loss for batch 4776 : 0.24358856678009033\n",
      "Training loss for batch 4777 : 0.10297109931707382\n",
      "Training loss for batch 4778 : 0.06568856537342072\n",
      "Training loss for batch 4779 : 0.1779474914073944\n",
      "Training loss for batch 4780 : 0.17336803674697876\n",
      "Training loss for batch 4781 : 0.2117185890674591\n",
      "Training loss for batch 4782 : 0.007191634736955166\n",
      "Training loss for batch 4783 : 0.2041848599910736\n",
      "Training loss for batch 4784 : 0.12117540091276169\n",
      "Training loss for batch 4785 : 0.07082445174455643\n",
      "Training loss for batch 4786 : 0.19337162375450134\n",
      "Training loss for batch 4787 : 0.13182169198989868\n",
      "Training loss for batch 4788 : 0.018029579892754555\n",
      "Training loss for batch 4789 : 0.006150607019662857\n",
      "Training loss for batch 4790 : 0.044398702681064606\n",
      "Training loss for batch 4791 : 0.12954598665237427\n",
      "Training loss for batch 4792 : 0.17150375247001648\n",
      "Training loss for batch 4793 : 0.030653048306703568\n",
      "Training loss for batch 4794 : 0.001974050421267748\n",
      "Training loss for batch 4795 : 0.12961554527282715\n",
      "Training loss for batch 4796 : 0.020680295303463936\n",
      "Training loss for batch 4797 : 0.0772913470864296\n",
      "Training loss for batch 4798 : 0.0713481530547142\n",
      "Training loss for batch 4799 : 0.14543366432189941\n",
      "Training loss for batch 4800 : 0.18474608659744263\n",
      "Training loss for batch 4801 : 0.0024326080456376076\n",
      "Training loss for batch 4802 : 0.08111276477575302\n",
      "Training loss for batch 4803 : 0.1449628472328186\n",
      "Training loss for batch 4804 : 0.047627292573451996\n",
      "Training loss for batch 4805 : 0.08240354061126709\n",
      "Training loss for batch 4806 : 0.019569506868720055\n",
      "Training loss for batch 4807 : 0.013560759834945202\n",
      "Training loss for batch 4808 : 0.05284951254725456\n",
      "Training loss for batch 4809 : 0.026882614940404892\n",
      "Training loss for batch 4810 : 0.11367326974868774\n",
      "Training loss for batch 4811 : 0.04666997119784355\n",
      "Training loss for batch 4812 : 0.30302420258522034\n",
      "Training loss for batch 4813 : 0.04016071557998657\n",
      "Training loss for batch 4814 : 0.00044817477464675903\n",
      "Training loss for batch 4815 : 0.03691216930747032\n",
      "Training loss for batch 4816 : 0.40455162525177\n",
      "Training loss for batch 4817 : 0.03394543379545212\n",
      "Training loss for batch 4818 : 0.10859224945306778\n",
      "Training loss for batch 4819 : 0.005198539234697819\n",
      "Training loss for batch 4820 : 0.043867822736501694\n",
      "Training loss for batch 4821 : 0.29567304253578186\n",
      "Training loss for batch 4822 : 0.09290768206119537\n",
      "Training loss for batch 4823 : 0.013558641076087952\n",
      "Training loss for batch 4824 : 0.2034294605255127\n",
      "Training loss for batch 4825 : 0.11794202029705048\n",
      "Training loss for batch 4826 : 0.13579124212265015\n",
      "Training loss for batch 4827 : 0.05111830309033394\n",
      "Training loss for batch 4828 : 0.12202073633670807\n",
      "Training loss for batch 4829 : 0.13191373646259308\n",
      "Training loss for batch 4830 : 0.2103562355041504\n",
      "Training loss for batch 4831 : 0.025797581300139427\n",
      "Training loss for batch 4832 : 0.15473459661006927\n",
      "Training loss for batch 4833 : 0.028350718319416046\n",
      "Training loss for batch 4834 : 0.18838150799274445\n",
      "Training loss for batch 4835 : 0.0\n",
      "Training loss for batch 4836 : 0.20741096138954163\n",
      "Training loss for batch 4837 : 0.04085874930024147\n",
      "Training loss for batch 4838 : 0.39076030254364014\n",
      "Training loss for batch 4839 : 0.43369901180267334\n",
      "Training loss for batch 4840 : 0.22214530408382416\n",
      "Training loss for batch 4841 : 0.10704467445611954\n",
      "Training loss for batch 4842 : 0.09752598404884338\n",
      "Training loss for batch 4843 : 0.044239651411771774\n",
      "Training loss for batch 4844 : 0.10536452382802963\n",
      "Training loss for batch 4845 : 0.048976801335811615\n",
      "Training loss for batch 4846 : 0.0\n",
      "Training loss for batch 4847 : 0.09609664976596832\n",
      "Training loss for batch 4848 : 0.07729652523994446\n",
      "Training loss for batch 4849 : 0.2001277506351471\n",
      "Training loss for batch 4850 : 0.040410082787275314\n",
      "Training loss for batch 4851 : 0.2247198075056076\n",
      "Training loss for batch 4852 : 0.3047693967819214\n",
      "Training loss for batch 4853 : 0.053838446736335754\n",
      "Training loss for batch 4854 : 0.08561089634895325\n",
      "Training loss for batch 4855 : 0.11826163530349731\n",
      "Training loss for batch 4856 : 0.034950148314237595\n",
      "Training loss for batch 4857 : 0.23617880046367645\n",
      "Training loss for batch 4858 : 0.181015282869339\n",
      "Training loss for batch 4859 : 0.13662855327129364\n",
      "Training loss for batch 4860 : 0.04617413505911827\n",
      "Training loss for batch 4861 : 0.17159676551818848\n",
      "Training loss for batch 4862 : 0.1555706262588501\n",
      "Training loss for batch 4863 : 0.025367282330989838\n",
      "Training loss for batch 4864 : 0.07718335837125778\n",
      "Training loss for batch 4865 : 0.008647802285850048\n",
      "Training loss for batch 4866 : 0.10839918255805969\n",
      "Training loss for batch 4867 : 0.09611066430807114\n",
      "Training loss for batch 4868 : 0.1992267370223999\n",
      "Training loss for batch 4869 : 0.07109208405017853\n",
      "Training loss for batch 4870 : 0.1497500240802765\n",
      "Training loss for batch 4871 : 0.06193346157670021\n",
      "Training loss for batch 4872 : 0.09090249985456467\n",
      "Training loss for batch 4873 : 0.10162118822336197\n",
      "Training loss for batch 4874 : 0.11957035958766937\n",
      "Training loss for batch 4875 : 0.0\n",
      "Training loss for batch 4876 : 0.15678770840168\n",
      "Training loss for batch 4877 : 0.15811994671821594\n",
      "Training loss for batch 4878 : 0.2786310613155365\n",
      "Training loss for batch 4879 : 0.10958784818649292\n",
      "Training loss for batch 4880 : 0.09233739972114563\n",
      "Training loss for batch 4881 : 0.016515208408236504\n",
      "Training loss for batch 4882 : 0.06710167974233627\n",
      "Training loss for batch 4883 : 0.3878173232078552\n",
      "Training loss for batch 4884 : 0.14211541414260864\n",
      "Training loss for batch 4885 : 0.31150519847869873\n",
      "Training loss for batch 4886 : 0.027303844690322876\n",
      "Training loss for batch 4887 : 0.35904017090797424\n",
      "Training loss for batch 4888 : 0.289137601852417\n",
      "Training loss for batch 4889 : 0.06318513303995132\n",
      "Training loss for batch 4890 : 0.12010924518108368\n",
      "Training loss for batch 4891 : 0.12706038355827332\n",
      "Training loss for batch 4892 : 0.06772643327713013\n",
      "Training loss for batch 4893 : 0.16756334900856018\n",
      "Training loss for batch 4894 : -0.0010657449020072818\n",
      "Training loss for batch 4895 : 0.6349288821220398\n",
      "Training loss for batch 4896 : 0.1522858887910843\n",
      "Training loss for batch 4897 : 0.16373175382614136\n",
      "Training loss for batch 4898 : 0.16905049979686737\n",
      "Training loss for batch 4899 : 0.16129809617996216\n",
      "Training loss for batch 4900 : 0.03356603905558586\n",
      "Training loss for batch 4901 : 0.20630991458892822\n",
      "Training loss for batch 4902 : 0.22036212682724\n",
      "Training loss for batch 4903 : 0.029933404177427292\n",
      "Training loss for batch 4904 : 0.00573448371142149\n",
      "Training loss for batch 4905 : 0.20782285928726196\n",
      "Training loss for batch 4906 : 0.13209190964698792\n",
      "Training loss for batch 4907 : 0.04122506082057953\n",
      "Training loss for batch 4908 : 0.30106300115585327\n",
      "Training loss for batch 4909 : 0.03653082996606827\n",
      "Training loss for batch 4910 : 0.25808629393577576\n",
      "Training loss for batch 4911 : 0.1636740118265152\n",
      "Training loss for batch 4912 : 0.16461995244026184\n",
      "Training loss for batch 4913 : 0.07440508902072906\n",
      "Training loss for batch 4914 : 0.032367344945669174\n",
      "Training loss for batch 4915 : 0.11803699284791946\n",
      "Training loss for batch 4916 : 0.5972377061843872\n",
      "Training loss for batch 4917 : 0.019632257521152496\n",
      "Training loss for batch 4918 : 0.07766925543546677\n",
      "Training loss for batch 4919 : 0.3476612865924835\n",
      "Training loss for batch 4920 : 0.07652091979980469\n",
      "Training loss for batch 4921 : 0.011590047739446163\n",
      "Training loss for batch 4922 : 0.18721625208854675\n",
      "Training loss for batch 4923 : 0.0804273933172226\n",
      "Training loss for batch 4924 : 0.041148003190755844\n",
      "Training loss for batch 4925 : 0.0005744695663452148\n",
      "Training loss for batch 4926 : 0.11006010323762894\n",
      "Training loss for batch 4927 : 0.1469317376613617\n",
      "Training loss for batch 4928 : 0.12467245757579803\n",
      "Training loss for batch 4929 : 0.02641453966498375\n",
      "Training loss for batch 4930 : 0.03056982345879078\n",
      "Training loss for batch 4931 : 0.014518216252326965\n",
      "Training loss for batch 4932 : 0.18964333832263947\n",
      "Training loss for batch 4933 : 0.30791404843330383\n",
      "Training loss for batch 4934 : 0.03464358299970627\n",
      "Training loss for batch 4935 : 0.15365144610404968\n",
      "Training loss for batch 4936 : 0.19063206017017365\n",
      "Training loss for batch 4937 : 0.07860471308231354\n",
      "Training loss for batch 4938 : 0.05312029644846916\n",
      "Training loss for batch 4939 : 0.06603723019361496\n",
      "Training loss for batch 4940 : 0.14606383442878723\n",
      "Training loss for batch 4941 : 0.02893293835222721\n",
      "Training loss for batch 4942 : 0.14151045680046082\n",
      "Training loss for batch 4943 : 0.11407040059566498\n",
      "Training loss for batch 4944 : 0.025092680007219315\n",
      "Training loss for batch 4945 : 0.14930228888988495\n",
      "Training loss for batch 4946 : 0.11436443030834198\n",
      "Training loss for batch 4947 : 0.08142335712909698\n",
      "Training loss for batch 4948 : 0.4746459722518921\n",
      "Training loss for batch 4949 : 0.28624120354652405\n",
      "Training loss for batch 4950 : 0.040990594774484634\n",
      "Training loss for batch 4951 : 0.09732946753501892\n",
      "Training loss for batch 4952 : 0.372206449508667\n",
      "Training loss for batch 4953 : 0.03808826953172684\n",
      "Training loss for batch 4954 : 0.19025155901908875\n",
      "Training loss for batch 4955 : 0.10968625545501709\n",
      "Training loss for batch 4956 : 0.025626488029956818\n",
      "Training loss for batch 4957 : 0.09350989013910294\n",
      "Training loss for batch 4958 : 0.07489405572414398\n",
      "Training loss for batch 4959 : 0.16568484902381897\n",
      "Training loss for batch 4960 : 0.004059948958456516\n",
      "Training loss for batch 4961 : 0.18621988594532013\n",
      "Training loss for batch 4962 : 0.20206868648529053\n",
      "Training loss for batch 4963 : 0.23065532743930817\n",
      "Training loss for batch 4964 : 0.14604175090789795\n",
      "Training loss for batch 4965 : 0.008216961286962032\n",
      "Training loss for batch 4966 : 0.3549606502056122\n",
      "Training loss for batch 4967 : 0.04951494187116623\n",
      "Training loss for batch 4968 : 0.17202657461166382\n",
      "Training loss for batch 4969 : 0.15617641806602478\n",
      "Training loss for batch 4970 : 0.2806222438812256\n",
      "Training loss for batch 4971 : 0.14591000974178314\n",
      "Training loss for batch 4972 : 0.12764257192611694\n",
      "Training loss for batch 4973 : 0.19326190650463104\n",
      "Training loss for batch 4974 : 0.07120952755212784\n",
      "Training loss for batch 4975 : 0.0\n",
      "Training loss for batch 4976 : 0.1834871917963028\n",
      "Training loss for batch 4977 : 0.007041638251394033\n",
      "Training loss for batch 4978 : 0.032894317060709\n",
      "Training loss for batch 4979 : 0.08806557208299637\n",
      "Training loss for batch 4980 : 0.2371084839105606\n",
      "Training loss for batch 4981 : 0.362210750579834\n",
      "Training loss for batch 4982 : 0.11540652811527252\n",
      "Training loss for batch 4983 : 0.2655274569988251\n",
      "Training loss for batch 4984 : 0.14484228193759918\n",
      "Training loss for batch 4985 : 0.11256781965494156\n",
      "Training loss for batch 4986 : 0.07833646982908249\n",
      "Training loss for batch 4987 : 0.22021162509918213\n",
      "Training loss for batch 4988 : 0.23754556477069855\n",
      "Training loss for batch 4989 : 0.0643758624792099\n",
      "Training loss for batch 4990 : 0.014197361655533314\n",
      "Training loss for batch 4991 : 0.12150587886571884\n",
      "Training loss for batch 4992 : 0.1283445656299591\n",
      "Training loss for batch 4993 : 0.02510426566004753\n",
      "Training loss for batch 4994 : 0.06466033309698105\n",
      "Training loss for batch 4995 : 0.0009180009365081787\n",
      "Training loss for batch 4996 : 0.08663536608219147\n",
      "Training loss for batch 4997 : 0.4757990539073944\n",
      "Training loss for batch 4998 : 0.13498227298259735\n",
      "Training loss for batch 4999 : -0.0020292236004024744\n",
      "Training loss for batch 5000 : 0.07906368374824524\n",
      "Training loss for batch 5001 : 0.09088899195194244\n",
      "Training loss for batch 5002 : 0.1870073825120926\n",
      "Training loss for batch 5003 : 0.02320925146341324\n",
      "Training loss for batch 5004 : 0.06218482181429863\n",
      "Training loss for batch 5005 : 0.16181616485118866\n",
      "Training loss for batch 5006 : 0.08758649230003357\n",
      "Training loss for batch 5007 : 0.10333046317100525\n",
      "Training loss for batch 5008 : 0.0730167105793953\n",
      "Training loss for batch 5009 : 0.11139257252216339\n",
      "Training loss for batch 5010 : 0.11560274660587311\n",
      "Training loss for batch 5011 : 0.057118091732263565\n",
      "Training loss for batch 5012 : 0.11950469762086868\n",
      "Training loss for batch 5013 : 0.30338969826698303\n",
      "Training loss for batch 5014 : 0.04721440374851227\n",
      "Training loss for batch 5015 : 0.1593795269727707\n",
      "Training loss for batch 5016 : 0.11356667429208755\n",
      "Training loss for batch 5017 : 0.1287003457546234\n",
      "Training loss for batch 5018 : 0.012447640299797058\n",
      "Training loss for batch 5019 : 0.08737611770629883\n",
      "Training loss for batch 5020 : 0.18204203248023987\n",
      "Training loss for batch 5021 : 0.03022466041147709\n",
      "Training loss for batch 5022 : 0.1444861888885498\n",
      "Training loss for batch 5023 : 0.137771874666214\n",
      "Training loss for batch 5024 : 0.1948401927947998\n",
      "Training loss for batch 5025 : 0.3924517035484314\n",
      "Training loss for batch 5026 : 0.0\n",
      "Training loss for batch 5027 : 0.2794366478919983\n",
      "Training loss for batch 5028 : 0.2708744406700134\n",
      "Training loss for batch 5029 : 0.04925679415464401\n",
      "Training loss for batch 5030 : 0.06632044911384583\n",
      "Training loss for batch 5031 : 0.07551652193069458\n",
      "Training loss for batch 5032 : 0.17292538285255432\n",
      "Training loss for batch 5033 : 0.16535820066928864\n",
      "Training loss for batch 5034 : 0.08938207477331161\n",
      "Training loss for batch 5035 : 0.06069346144795418\n",
      "Training loss for batch 5036 : 0.07759609818458557\n",
      "Training loss for batch 5037 : 0.07530974596738815\n",
      "Training loss for batch 5038 : 0.08318030834197998\n",
      "Training loss for batch 5039 : 0.003991739824414253\n",
      "Training loss for batch 5040 : 0.016330085694789886\n",
      "Training loss for batch 5041 : 0.07260246574878693\n",
      "Training loss for batch 5042 : 0.0606762059032917\n",
      "Training loss for batch 5043 : 0.0007565319538116455\n",
      "Training loss for batch 5044 : 0.0841626301407814\n",
      "Training loss for batch 5045 : 0.04792095720767975\n",
      "Training loss for batch 5046 : 0.004483362194150686\n",
      "Training loss for batch 5047 : 0.11712898313999176\n",
      "Training loss for batch 5048 : 0.08829586207866669\n",
      "Training loss for batch 5049 : 0.06604284048080444\n",
      "Training loss for batch 5050 : 0.1943923383951187\n",
      "Training loss for batch 5051 : 0.14647063612937927\n",
      "Training loss for batch 5052 : 0.05908547341823578\n",
      "Training loss for batch 5053 : 0.026644332334399223\n",
      "Training loss for batch 5054 : 0.21775886416435242\n",
      "Training loss for batch 5055 : 0.23926979303359985\n",
      "Training loss for batch 5056 : 0.1774083822965622\n",
      "Training loss for batch 5057 : 0.08195871859788895\n",
      "Training loss for batch 5058 : 0.06482555717229843\n",
      "Training loss for batch 5059 : 0.11835626512765884\n",
      "Training loss for batch 5060 : 0.12181122601032257\n",
      "Training loss for batch 5061 : 0.042425114661455154\n",
      "Training loss for batch 5062 : 0.0430738590657711\n",
      "Training loss for batch 5063 : -0.0014153793454170227\n",
      "Training loss for batch 5064 : 0.020551111549139023\n",
      "Training loss for batch 5065 : 0.288247287273407\n",
      "Training loss for batch 5066 : 0.05484085530042648\n",
      "Training loss for batch 5067 : 0.02158493921160698\n",
      "Training loss for batch 5068 : 0.05446547269821167\n",
      "Training loss for batch 5069 : 0.13514983654022217\n",
      "Training loss for batch 5070 : 0.07273143529891968\n",
      "Training loss for batch 5071 : 0.013225121423602104\n",
      "Training loss for batch 5072 : 0.16659942269325256\n",
      "Training loss for batch 5073 : 0.09864594042301178\n",
      "Training loss for batch 5074 : 0.002287447452545166\n",
      "Training loss for batch 5075 : 0.012460911646485329\n",
      "Training loss for batch 5076 : 0.2744424641132355\n",
      "Training loss for batch 5077 : 0.16457334160804749\n",
      "Training loss for batch 5078 : 0.07318278402090073\n",
      "Training loss for batch 5079 : 0.02466057799756527\n",
      "Training loss for batch 5080 : 0.16264325380325317\n",
      "Training loss for batch 5081 : -0.0018003307050094008\n",
      "Training loss for batch 5082 : 0.02357402816414833\n",
      "Training loss for batch 5083 : 0.055923786014318466\n",
      "Training loss for batch 5084 : 0.04237685352563858\n",
      "Training loss for batch 5085 : 0.21205675601959229\n",
      "Training loss for batch 5086 : 0.08516217768192291\n",
      "Training loss for batch 5087 : 0.17536064982414246\n",
      "Training loss for batch 5088 : 0.27932536602020264\n",
      "Training loss for batch 5089 : 0.050901275128126144\n",
      "Training loss for batch 5090 : 0.0633057951927185\n",
      "Training loss for batch 5091 : 0.0564744770526886\n",
      "Training loss for batch 5092 : 0.018113860860466957\n",
      "Training loss for batch 5093 : 0.2729516923427582\n",
      "Training loss for batch 5094 : 0.10802989453077316\n",
      "Training loss for batch 5095 : 0.1612393707036972\n",
      "Training loss for batch 5096 : 0.004032552242279053\n",
      "Training loss for batch 5097 : 0.1289237141609192\n",
      "Training loss for batch 5098 : 0.043766483664512634\n",
      "Training loss for batch 5099 : 0.12165361642837524\n",
      "Training loss for batch 5100 : 0.0\n",
      "Training loss for batch 5101 : 0.10290832817554474\n",
      "Training loss for batch 5102 : 0.012376724742352962\n",
      "Training loss for batch 5103 : 0.09510955214500427\n",
      "Training loss for batch 5104 : 0.35649633407592773\n",
      "Training loss for batch 5105 : 0.0036972167436033487\n",
      "Training loss for batch 5106 : 0.011259717866778374\n",
      "Training loss for batch 5107 : 0.06998253613710403\n",
      "Training loss for batch 5108 : 0.010638661682605743\n",
      "Training loss for batch 5109 : 0.0412733219563961\n",
      "Training loss for batch 5110 : 0.03485994413495064\n",
      "Training loss for batch 5111 : 0.003343448042869568\n",
      "Training loss for batch 5112 : 0.12329960614442825\n",
      "Training loss for batch 5113 : 0.20223459601402283\n",
      "Training loss for batch 5114 : 0.1395527869462967\n",
      "Training loss for batch 5115 : 0.09966098517179489\n",
      "Training loss for batch 5116 : 0.4528670608997345\n",
      "Training loss for batch 5117 : 0.18405595421791077\n",
      "Training loss for batch 5118 : 0.6613341569900513\n",
      "Training loss for batch 5119 : 0.02351309359073639\n",
      "Training loss for batch 5120 : 0.14669428765773773\n",
      "Training loss for batch 5121 : 0.015720903873443604\n",
      "Training loss for batch 5122 : 0.16744780540466309\n",
      "Training loss for batch 5123 : 0.18720297515392303\n",
      "Training loss for batch 5124 : 0.09912946820259094\n",
      "Training loss for batch 5125 : 0.009245552122592926\n",
      "Training loss for batch 5126 : 0.006332585588097572\n",
      "Training loss for batch 5127 : 0.20224788784980774\n",
      "Training loss for batch 5128 : 0.08717601746320724\n",
      "Training loss for batch 5129 : 0.05242403596639633\n",
      "Training loss for batch 5130 : 0.011579911224544048\n",
      "Training loss for batch 5131 : 0.26986250281333923\n",
      "Training loss for batch 5132 : 0.16863879561424255\n",
      "Training loss for batch 5133 : 0.02988128736615181\n",
      "Training loss for batch 5134 : 0.312910795211792\n",
      "Training loss for batch 5135 : 0.1194138377904892\n",
      "Training loss for batch 5136 : 0.16880185902118683\n",
      "Training loss for batch 5137 : 0.28223586082458496\n",
      "Training loss for batch 5138 : 0.1001586765050888\n",
      "Training loss for batch 5139 : 0.13251575827598572\n",
      "Training loss for batch 5140 : 0.12628181278705597\n",
      "Training loss for batch 5141 : 0.04768695682287216\n",
      "Training loss for batch 5142 : 0.10224532335996628\n",
      "Training loss for batch 5143 : 0.03239208832383156\n",
      "Training loss for batch 5144 : 0.1872292011976242\n",
      "Training loss for batch 5145 : 0.023944886401295662\n",
      "Training loss for batch 5146 : 0.1385161578655243\n",
      "Training loss for batch 5147 : 0.1356637179851532\n",
      "Training loss for batch 5148 : 0.1548827737569809\n",
      "Training loss for batch 5149 : 0.012372852303087711\n",
      "Training loss for batch 5150 : 0.20665337145328522\n",
      "Training loss for batch 5151 : 0.23078638315200806\n",
      "Training loss for batch 5152 : 0.08477865904569626\n",
      "Training loss for batch 5153 : 0.021859243512153625\n",
      "Training loss for batch 5154 : 0.15400822460651398\n",
      "Training loss for batch 5155 : 0.11407352238893509\n",
      "Training loss for batch 5156 : 0.20484279096126556\n",
      "Training loss for batch 5157 : 0.06131105124950409\n",
      "Training loss for batch 5158 : 0.20383159816265106\n",
      "Training loss for batch 5159 : 0.039918795228004456\n",
      "Training loss for batch 5160 : 0.09409557282924652\n",
      "Training loss for batch 5161 : 0.22874493896961212\n",
      "Training loss for batch 5162 : 0.18896061182022095\n",
      "Training loss for batch 5163 : 0.03596360608935356\n",
      "Training loss for batch 5164 : 0.12381940335035324\n",
      "Training loss for batch 5165 : 0.060910798609256744\n",
      "Training loss for batch 5166 : 0.1694045215845108\n",
      "Training loss for batch 5167 : 0.13658055663108826\n",
      "Training loss for batch 5168 : 0.1473754644393921\n",
      "Training loss for batch 5169 : 0.11981065571308136\n",
      "Training loss for batch 5170 : -0.0014105620793998241\n",
      "Training loss for batch 5171 : 0.12940432131290436\n",
      "Training loss for batch 5172 : 0.055705469101667404\n",
      "Training loss for batch 5173 : 0.02206984907388687\n",
      "Training loss for batch 5174 : 0.08195803314447403\n",
      "Training loss for batch 5175 : 0.2107982337474823\n",
      "Training loss for batch 5176 : 0.043163567781448364\n",
      "Training loss for batch 5177 : 0.0255451500415802\n",
      "Training loss for batch 5178 : 0.09427036345005035\n",
      "Training loss for batch 5179 : 0.04237748309969902\n",
      "Training loss for batch 5180 : 0.613903820514679\n",
      "Training loss for batch 5181 : 0.15224964916706085\n",
      "Training loss for batch 5182 : 0.07244451344013214\n",
      "Training loss for batch 5183 : 0.160521000623703\n",
      "Training loss for batch 5184 : 0.05251425877213478\n",
      "Training loss for batch 5185 : 0.022543853148818016\n",
      "Training loss for batch 5186 : 0.04013554006814957\n",
      "Training loss for batch 5187 : 0.27479881048202515\n",
      "Training loss for batch 5188 : 0.21353663504123688\n",
      "Training loss for batch 5189 : 0.08537295460700989\n",
      "Training loss for batch 5190 : 0.28008562326431274\n",
      "Training loss for batch 5191 : 0.270972341299057\n",
      "Training loss for batch 5192 : 0.2341347634792328\n",
      "Training loss for batch 5193 : 0.27590128779411316\n",
      "Training loss for batch 5194 : 0.016428261995315552\n",
      "Training loss for batch 5195 : 0.00751166045665741\n",
      "Training loss for batch 5196 : 0.0\n",
      "Training loss for batch 5197 : 0.08485426008701324\n",
      "Training loss for batch 5198 : 0.1162300854921341\n",
      "Training loss for batch 5199 : 0.06650448590517044\n",
      "Training loss for batch 5200 : 0.17911966145038605\n",
      "Training loss for batch 5201 : 0.17403939366340637\n",
      "Training loss for batch 5202 : 0.5338662266731262\n",
      "Training loss for batch 5203 : 0.030579086393117905\n",
      "Training loss for batch 5204 : 0.09939070791006088\n",
      "Training loss for batch 5205 : 0.09588539600372314\n",
      "Training loss for batch 5206 : 0.05880144238471985\n",
      "Training loss for batch 5207 : 0.1254843771457672\n",
      "Training loss for batch 5208 : 0.08881746977567673\n",
      "Training loss for batch 5209 : 0.3815528154373169\n",
      "Training loss for batch 5210 : 0.2858503460884094\n",
      "Training loss for batch 5211 : 0.18308812379837036\n",
      "Training loss for batch 5212 : 0.04840145632624626\n",
      "Training loss for batch 5213 : 0.2364816963672638\n",
      "Training loss for batch 5214 : 0.018970055505633354\n",
      "Training loss for batch 5215 : 0.1826918125152588\n",
      "Training loss for batch 5216 : 0.03450494259595871\n",
      "Training loss for batch 5217 : 0.0060158465057611465\n",
      "Training loss for batch 5218 : 0.12450890243053436\n",
      "Training loss for batch 5219 : 0.033738840371370316\n",
      "Training loss for batch 5220 : 0.005481190513819456\n",
      "Training loss for batch 5221 : 0.13740885257720947\n",
      "Training loss for batch 5222 : 0.0011761784553527832\n",
      "Training loss for batch 5223 : 0.08583150804042816\n",
      "Training loss for batch 5224 : 0.1066986545920372\n",
      "Training loss for batch 5225 : 0.06630139797925949\n",
      "Training loss for batch 5226 : 0.009842404164373875\n",
      "Training loss for batch 5227 : 0.06120884791016579\n",
      "Training loss for batch 5228 : 0.1646299958229065\n",
      "Training loss for batch 5229 : 0.13145366311073303\n",
      "Training loss for batch 5230 : 0.007578025572001934\n",
      "Training loss for batch 5231 : 0.2828589379787445\n",
      "Training loss for batch 5232 : 0.15114513039588928\n",
      "Training loss for batch 5233 : 0.15182772278785706\n",
      "Training loss for batch 5234 : 0.3733002245426178\n",
      "Training loss for batch 5235 : 0.14035047590732574\n",
      "Training loss for batch 5236 : 0.13134367763996124\n",
      "Training loss for batch 5237 : 0.0701795220375061\n",
      "Training loss for batch 5238 : 0.14162476360797882\n",
      "Training loss for batch 5239 : 0.10289165377616882\n",
      "Training loss for batch 5240 : 0.15969693660736084\n",
      "Training loss for batch 5241 : 0.07132588326931\n",
      "Training loss for batch 5242 : 0.23870849609375\n",
      "Training loss for batch 5243 : 0.2318352609872818\n",
      "Training loss for batch 5244 : 0.21314331889152527\n",
      "Training loss for batch 5245 : 0.0974443256855011\n",
      "Training loss for batch 5246 : 0.09934642165899277\n",
      "Training loss for batch 5247 : 0.162306547164917\n",
      "Training loss for batch 5248 : 0.2715035080909729\n",
      "Training loss for batch 5249 : 0.32948920130729675\n",
      "Training loss for batch 5250 : 0.0051115164533257484\n",
      "Training loss for batch 5251 : 0.027604617178440094\n",
      "Training loss for batch 5252 : 0.24012187123298645\n",
      "Training loss for batch 5253 : 0.01613563299179077\n",
      "Training loss for batch 5254 : 0.17136867344379425\n",
      "Training loss for batch 5255 : 0.0\n",
      "Training loss for batch 5256 : 0.0002012451586779207\n",
      "Training loss for batch 5257 : 0.13496994972229004\n",
      "Training loss for batch 5258 : 0.20010988414287567\n",
      "Training loss for batch 5259 : 0.03369772434234619\n",
      "Training loss for batch 5260 : 0.25750163197517395\n",
      "Training loss for batch 5261 : 0.05908136069774628\n",
      "Training loss for batch 5262 : 0.21161997318267822\n",
      "Training loss for batch 5263 : 0.06751038879156113\n",
      "Training loss for batch 5264 : 0.14676246047019958\n",
      "Training loss for batch 5265 : 0.08694617450237274\n",
      "Training loss for batch 5266 : 0.0958760678768158\n",
      "Training loss for batch 5267 : 0.0807594284415245\n",
      "Training loss for batch 5268 : 0.15287929773330688\n",
      "Training loss for batch 5269 : 0.31284263730049133\n",
      "Training loss for batch 5270 : 0.11450894176959991\n",
      "Training loss for batch 5271 : 0.4640387296676636\n",
      "Training loss for batch 5272 : 0.1079225167632103\n",
      "Training loss for batch 5273 : -0.009092289954423904\n",
      "Training loss for batch 5274 : 0.1478986144065857\n",
      "Training loss for batch 5275 : 0.11884622275829315\n",
      "Training loss for batch 5276 : 0.011225959286093712\n",
      "Training loss for batch 5277 : 0.1497969925403595\n",
      "Training loss for batch 5278 : 0.005755196325480938\n",
      "Training loss for batch 5279 : 0.23884272575378418\n",
      "Training loss for batch 5280 : 0.04921916127204895\n",
      "Training loss for batch 5281 : 0.2779580056667328\n",
      "Training loss for batch 5282 : 0.15128053724765778\n",
      "Training loss for batch 5283 : 0.18069003522396088\n",
      "Training loss for batch 5284 : 0.052009373903274536\n",
      "Training loss for batch 5285 : 0.2687629163265228\n",
      "Training loss for batch 5286 : 0.21864259243011475\n",
      "Training loss for batch 5287 : 0.05719920992851257\n",
      "Training loss for batch 5288 : 0.25778740644454956\n",
      "Training loss for batch 5289 : 0.19590593874454498\n",
      "Training loss for batch 5290 : 0.3995603024959564\n",
      "Training loss for batch 5291 : 0.0967395231127739\n",
      "Training loss for batch 5292 : 0.011696575209498405\n",
      "Training loss for batch 5293 : 0.0767730250954628\n",
      "Training loss for batch 5294 : 0.15509085357189178\n",
      "Training loss for batch 5295 : 0.08963420242071152\n",
      "Training loss for batch 5296 : 0.13882198929786682\n",
      "Training loss for batch 5297 : 0.2426196038722992\n",
      "Training loss for batch 5298 : 0.11506204307079315\n",
      "Training loss for batch 5299 : 0.1356753408908844\n",
      "Training loss for batch 5300 : 0.1720542162656784\n",
      "Training loss for batch 5301 : 0.06941647827625275\n",
      "Training loss for batch 5302 : 0.12528082728385925\n",
      "Training loss for batch 5303 : 0.1351412832736969\n",
      "Training loss for batch 5304 : 0.14514479041099548\n",
      "Training loss for batch 5305 : 0.48487892746925354\n",
      "Training loss for batch 5306 : 0.20126986503601074\n",
      "Training loss for batch 5307 : 0.1069420576095581\n",
      "Training loss for batch 5308 : 0.11191292107105255\n",
      "Training loss for batch 5309 : 0.3590084910392761\n",
      "Training loss for batch 5310 : 0.05069372057914734\n",
      "Training loss for batch 5311 : 0.09648001194000244\n",
      "Training loss for batch 5312 : 0.18449154496192932\n",
      "Training loss for batch 5313 : 0.03255762904882431\n",
      "Training loss for batch 5314 : 0.059526510536670685\n",
      "Training loss for batch 5315 : 0.15944398939609528\n",
      "Training loss for batch 5316 : 0.09120183438062668\n",
      "Training loss for batch 5317 : -0.0036040949635207653\n",
      "Training loss for batch 5318 : 0.5703557729721069\n",
      "Training loss for batch 5319 : 0.042684562504291534\n",
      "Training loss for batch 5320 : 0.011003192514181137\n",
      "Training loss for batch 5321 : 0.12595880031585693\n",
      "Training loss for batch 5322 : 0.1374857872724533\n",
      "Training loss for batch 5323 : 0.058578189462423325\n",
      "Training loss for batch 5324 : 0.10549560934305191\n",
      "Training loss for batch 5325 : 0.048339229077100754\n",
      "Training loss for batch 5326 : 0.13983483612537384\n",
      "Training loss for batch 5327 : 0.1784006506204605\n",
      "Training loss for batch 5328 : 0.18454816937446594\n",
      "Training loss for batch 5329 : 0.012298653833568096\n",
      "Training loss for batch 5330 : 0.09408131241798401\n",
      "Training loss for batch 5331 : 0.025828281417489052\n",
      "Training loss for batch 5332 : 0.08078701049089432\n",
      "Training loss for batch 5333 : 0.024782272055745125\n",
      "Training loss for batch 5334 : 0.16444537043571472\n",
      "Training loss for batch 5335 : 0.2759116291999817\n",
      "Training loss for batch 5336 : 0.17084632813930511\n",
      "Training loss for batch 5337 : 0.1586359143257141\n",
      "Training loss for batch 5338 : 0.07498040795326233\n",
      "Training loss for batch 5339 : 0.04348360747098923\n",
      "Training loss for batch 5340 : 0.1740206778049469\n",
      "Training loss for batch 5341 : 0.022170722484588623\n",
      "Training loss for batch 5342 : 0.16088822484016418\n",
      "Training loss for batch 5343 : 0.09485364705324173\n",
      "Training loss for batch 5344 : 0.38440269231796265\n",
      "Training loss for batch 5345 : 0.04401193931698799\n",
      "Training loss for batch 5346 : 0.010344305075705051\n",
      "Training loss for batch 5347 : 0.01089966669678688\n",
      "Training loss for batch 5348 : 0.11412329971790314\n",
      "Training loss for batch 5349 : 0.1281394511461258\n",
      "Training loss for batch 5350 : 0.33570557832717896\n",
      "Training loss for batch 5351 : 0.05605107545852661\n",
      "Training loss for batch 5352 : 0.07710790634155273\n",
      "Training loss for batch 5353 : 0.17576457560062408\n",
      "Training loss for batch 5354 : 0.009959904477000237\n",
      "Training loss for batch 5355 : 0.3264477550983429\n",
      "Training loss for batch 5356 : 0.17072176933288574\n",
      "Training loss for batch 5357 : 0.039468757808208466\n",
      "Training loss for batch 5358 : 0.03796882927417755\n",
      "Training loss for batch 5359 : 0.25930002331733704\n",
      "Training loss for batch 5360 : 0.10342987626791\n",
      "Training loss for batch 5361 : 0.013348450884222984\n",
      "Training loss for batch 5362 : 0.07488784193992615\n",
      "Training loss for batch 5363 : 0.04424330219626427\n",
      "Training loss for batch 5364 : 0.06345468759536743\n",
      "Training loss for batch 5365 : 0.11871547251939774\n",
      "Training loss for batch 5366 : 0.0034218979999423027\n",
      "Training loss for batch 5367 : 0.041953280568122864\n",
      "Training loss for batch 5368 : 0.040649738162755966\n",
      "Training loss for batch 5369 : 0.057496339082717896\n",
      "Training loss for batch 5370 : 0.19507785141468048\n",
      "Training loss for batch 5371 : 0.22015416622161865\n",
      "Training loss for batch 5372 : 0.25571170449256897\n",
      "Training loss for batch 5373 : 0.14144229888916016\n",
      "Training loss for batch 5374 : 0.2131284475326538\n",
      "Training loss for batch 5375 : 0.18298226594924927\n",
      "Training loss for batch 5376 : 0.03364182636141777\n",
      "Training loss for batch 5377 : 0.3356175720691681\n",
      "Training loss for batch 5378 : 0.05462242662906647\n",
      "Training loss for batch 5379 : 0.10891086608171463\n",
      "Training loss for batch 5380 : 0.01925632916390896\n",
      "Training loss for batch 5381 : 0.03511999174952507\n",
      "Training loss for batch 5382 : 0.23951488733291626\n",
      "Training loss for batch 5383 : 0.18008673191070557\n",
      "Training loss for batch 5384 : 0.23279115557670593\n",
      "Training loss for batch 5385 : 0.036625541746616364\n",
      "Training loss for batch 5386 : 0.04564613476395607\n",
      "Training loss for batch 5387 : 0.15227071940898895\n",
      "Training loss for batch 5388 : 0.10252875834703445\n",
      "Training loss for batch 5389 : -0.0026756057050079107\n",
      "Training loss for batch 5390 : 0.15731748938560486\n",
      "Training loss for batch 5391 : 0.015079818665981293\n",
      "Training loss for batch 5392 : 0.050379928201436996\n",
      "Training loss for batch 5393 : 0.16639071702957153\n",
      "Training loss for batch 5394 : 0.009911910630762577\n",
      "Training loss for batch 5395 : 0.19017267227172852\n",
      "Training loss for batch 5396 : 0.05676189064979553\n",
      "Training loss for batch 5397 : 0.10328613221645355\n",
      "Training loss for batch 5398 : 0.1160016655921936\n",
      "Training loss for batch 5399 : 0.18895475566387177\n",
      "Training loss for batch 5400 : 0.05665549635887146\n",
      "Training loss for batch 5401 : 0.22626879811286926\n",
      "Training loss for batch 5402 : 0.11457313597202301\n",
      "Training loss for batch 5403 : 0.057680413126945496\n",
      "Training loss for batch 5404 : 0.21575307846069336\n",
      "Training loss for batch 5405 : 0.09536807239055634\n",
      "Training loss for batch 5406 : 0.08000444620847702\n",
      "Training loss for batch 5407 : 0.258303701877594\n",
      "Training loss for batch 5408 : 0.07032383233308792\n",
      "Training loss for batch 5409 : 0.10137543082237244\n",
      "Training loss for batch 5410 : 0.03599591553211212\n",
      "Training loss for batch 5411 : 0.24721334874629974\n",
      "Training loss for batch 5412 : 0.25065770745277405\n",
      "Training loss for batch 5413 : 0.07525792717933655\n",
      "Training loss for batch 5414 : 0.036255743354558945\n",
      "Training loss for batch 5415 : 0.06569409370422363\n",
      "Training loss for batch 5416 : 0.11807699501514435\n",
      "Training loss for batch 5417 : 0.27557921409606934\n",
      "Training loss for batch 5418 : 0.018676847219467163\n",
      "Training loss for batch 5419 : 0.08952638506889343\n",
      "Training loss for batch 5420 : 0.0015728273428976536\n",
      "Training loss for batch 5421 : 0.2408299595117569\n",
      "Training loss for batch 5422 : 0.055914003401994705\n",
      "Training loss for batch 5423 : 0.11490482836961746\n",
      "Training loss for batch 5424 : 0.1359802782535553\n",
      "Training loss for batch 5425 : 0.0860385000705719\n",
      "Training loss for batch 5426 : 0.02312406897544861\n",
      "Training loss for batch 5427 : 0.0906473845243454\n",
      "Training loss for batch 5428 : 0.07508474588394165\n",
      "Training loss for batch 5429 : 0.17924028635025024\n",
      "Training loss for batch 5430 : 0.2085154503583908\n",
      "Training loss for batch 5431 : 0.03072008490562439\n",
      "Training loss for batch 5432 : 0.1592196673154831\n",
      "Training loss for batch 5433 : 0.09390914440155029\n",
      "Training loss for batch 5434 : 0.06889348477125168\n",
      "Training loss for batch 5435 : 0.1964949667453766\n",
      "Training loss for batch 5436 : 0.07473405450582504\n",
      "Training loss for batch 5437 : 0.2828481197357178\n",
      "Training loss for batch 5438 : 0.23575091361999512\n",
      "Training loss for batch 5439 : 0.0\n",
      "Training loss for batch 5440 : 0.022714868187904358\n",
      "Training loss for batch 5441 : 0.20194321870803833\n",
      "Training loss for batch 5442 : 0.2755487859249115\n",
      "Training loss for batch 5443 : 0.13805757462978363\n",
      "Training loss for batch 5444 : 0.3433326780796051\n",
      "Training loss for batch 5445 : 0.02090795524418354\n",
      "Training loss for batch 5446 : 0.09925629198551178\n",
      "Training loss for batch 5447 : 0.12324192374944687\n",
      "Training loss for batch 5448 : 0.08346915990114212\n",
      "Training loss for batch 5449 : 0.15381881594657898\n",
      "Training loss for batch 5450 : 0.11678759008646011\n",
      "Training loss for batch 5451 : 0.058541737496852875\n",
      "Training loss for batch 5452 : 0.02823254093527794\n",
      "Training loss for batch 5453 : 0.05912093445658684\n",
      "Training loss for batch 5454 : 0.04894685745239258\n",
      "Training loss for batch 5455 : 0.31205427646636963\n",
      "Training loss for batch 5456 : 0.08289536833763123\n",
      "Training loss for batch 5457 : 0.08885326236486435\n",
      "Training loss for batch 5458 : 0.1823297142982483\n",
      "Training loss for batch 5459 : 0.12224163860082626\n",
      "Training loss for batch 5460 : 0.043413203209638596\n",
      "Training loss for batch 5461 : -0.0003897889982908964\n",
      "Training loss for batch 5462 : 0.007888277992606163\n",
      "Training loss for batch 5463 : 0.026059865951538086\n",
      "Training loss for batch 5464 : 0.2283112108707428\n",
      "Training loss for batch 5465 : 0.11481691151857376\n",
      "Training loss for batch 5466 : 0.22382214665412903\n",
      "Training loss for batch 5467 : 0.07286685705184937\n",
      "Training loss for batch 5468 : 0.17843729257583618\n",
      "Training loss for batch 5469 : 0.0020429445430636406\n",
      "Training loss for batch 5470 : 0.1176401823759079\n",
      "Training loss for batch 5471 : 0.04287148267030716\n",
      "Training loss for batch 5472 : 0.15401838719844818\n",
      "Training loss for batch 5473 : 0.036807186901569366\n",
      "Training loss for batch 5474 : 0.1314118504524231\n",
      "Training loss for batch 5475 : 0.06362433731555939\n",
      "Training loss for batch 5476 : 0.15948018431663513\n",
      "Training loss for batch 5477 : 0.11326593905687332\n",
      "Training loss for batch 5478 : 0.23626936972141266\n",
      "Training loss for batch 5479 : 0.11338840425014496\n",
      "Training loss for batch 5480 : 0.14782220125198364\n",
      "Training loss for batch 5481 : 0.10609584301710129\n",
      "Training loss for batch 5482 : 0.11389636993408203\n",
      "Training loss for batch 5483 : 0.041800178587436676\n",
      "Training loss for batch 5484 : 0.07944745570421219\n",
      "Training loss for batch 5485 : 0.01085304468870163\n",
      "Training loss for batch 5486 : 0.011577703058719635\n",
      "Training loss for batch 5487 : 0.009959395974874496\n",
      "Training loss for batch 5488 : 0.037404078990221024\n",
      "Training loss for batch 5489 : 0.33428916335105896\n",
      "Training loss for batch 5490 : 0.028043502941727638\n",
      "Training loss for batch 5491 : 0.27132511138916016\n",
      "Training loss for batch 5492 : 0.16650079190731049\n",
      "Training loss for batch 5493 : 0.18826918303966522\n",
      "Training loss for batch 5494 : 0.01629636622965336\n",
      "Training loss for batch 5495 : 0.2000715732574463\n",
      "Training loss for batch 5496 : 0.027867168188095093\n",
      "Training loss for batch 5497 : 0.0057273549027740955\n",
      "Training loss for batch 5498 : 0.30293768644332886\n",
      "Training loss for batch 5499 : 0.2408219277858734\n",
      "Training loss for batch 5500 : 0.1536335051059723\n",
      "Training loss for batch 5501 : 0.0008451541652902961\n",
      "Training loss for batch 5502 : 0.10159913450479507\n",
      "Training loss for batch 5503 : 0.16945549845695496\n",
      "Training loss for batch 5504 : 0.07140371203422546\n",
      "Training loss for batch 5505 : 0.13225895166397095\n",
      "Training loss for batch 5506 : 0.12079628556966782\n",
      "Training loss for batch 5507 : 0.03770698979496956\n",
      "Training loss for batch 5508 : 0.32551324367523193\n",
      "Training loss for batch 5509 : 0.012251457199454308\n",
      "Training loss for batch 5510 : 0.11177763342857361\n",
      "Training loss for batch 5511 : 0.19980645179748535\n",
      "Training loss for batch 5512 : 0.0965566486120224\n",
      "Training loss for batch 5513 : 0.0564449205994606\n",
      "Training loss for batch 5514 : 0.15579748153686523\n",
      "Training loss for batch 5515 : 0.327659547328949\n",
      "Training loss for batch 5516 : 0.06405822932720184\n",
      "Training loss for batch 5517 : 0.17710348963737488\n",
      "Training loss for batch 5518 : 0.027723779901862144\n",
      "Training loss for batch 5519 : 0.042960815131664276\n",
      "Training loss for batch 5520 : 0.09317772090435028\n",
      "Training loss for batch 5521 : 0.014580708928406239\n",
      "Training loss for batch 5522 : 0.1000126302242279\n",
      "Training loss for batch 5523 : 0.16141481697559357\n",
      "Training loss for batch 5524 : 0.09443091601133347\n",
      "Training loss for batch 5525 : 0.1680719405412674\n",
      "Training loss for batch 5526 : 0.0774008184671402\n",
      "Training loss for batch 5527 : 0.013472153805196285\n",
      "Training loss for batch 5528 : 0.06680571287870407\n",
      "Training loss for batch 5529 : 0.09177178889513016\n",
      "Training loss for batch 5530 : 0.03777168318629265\n",
      "Training loss for batch 5531 : 0.0019425363279879093\n",
      "Training loss for batch 5532 : 0.02864651195704937\n",
      "Training loss for batch 5533 : 0.14247792959213257\n",
      "Training loss for batch 5534 : 0.044320374727249146\n",
      "Training loss for batch 5535 : 0.05255788564682007\n",
      "Training loss for batch 5536 : 0.11834302544593811\n",
      "Training loss for batch 5537 : 0.19100427627563477\n",
      "Training loss for batch 5538 : 0.14687705039978027\n",
      "Training loss for batch 5539 : 0.13051320612430573\n",
      "Training loss for batch 5540 : 0.0027902377769351006\n",
      "Training loss for batch 5541 : 0.18872147798538208\n",
      "Training loss for batch 5542 : 0.08463647961616516\n",
      "Training loss for batch 5543 : 0.19638921320438385\n",
      "Training loss for batch 5544 : 0.13954579830169678\n",
      "Training loss for batch 5545 : 0.0\n",
      "Training loss for batch 5546 : 0.0011433304753154516\n",
      "Training loss for batch 5547 : 0.24089401960372925\n",
      "Training loss for batch 5548 : 0.032695472240448\n",
      "Training loss for batch 5549 : 0.4091585874557495\n",
      "Training loss for batch 5550 : 0.022649694234132767\n",
      "Training loss for batch 5551 : 0.11429488658905029\n",
      "Training loss for batch 5552 : 0.22810135781764984\n",
      "Training loss for batch 5553 : 0.1103188544511795\n",
      "Training loss for batch 5554 : 0.04363035038113594\n",
      "Training loss for batch 5555 : 0.15919342637062073\n",
      "Training loss for batch 5556 : 0.13973332941532135\n",
      "Training loss for batch 5557 : 0.06330938637256622\n",
      "Training loss for batch 5558 : 0.05693276971578598\n",
      "Training loss for batch 5559 : 0.024796823039650917\n",
      "Training loss for batch 5560 : 0.13143277168273926\n",
      "Training loss for batch 5561 : 0.014033094048500061\n",
      "Training loss for batch 5562 : 0.08938652276992798\n",
      "Training loss for batch 5563 : 0.007308382540941238\n",
      "Training loss for batch 5564 : 0.17236925661563873\n",
      "Training loss for batch 5565 : 0.09123721718788147\n",
      "Training loss for batch 5566 : 0.11764942854642868\n",
      "Training loss for batch 5567 : 0.12984387576580048\n",
      "Training loss for batch 5568 : 0.04762108251452446\n",
      "Training loss for batch 5569 : 0.09495743364095688\n",
      "Training loss for batch 5570 : 0.2551579475402832\n",
      "Training loss for batch 5571 : 0.2553964853286743\n",
      "Training loss for batch 5572 : 0.07782997190952301\n",
      "Training loss for batch 5573 : 0.23287609219551086\n",
      "Training loss for batch 5574 : 0.11125707626342773\n",
      "Training loss for batch 5575 : 0.0\n",
      "Training loss for batch 5576 : 0.09305588901042938\n",
      "Training loss for batch 5577 : 0.04210667684674263\n",
      "Training loss for batch 5578 : 0.10076363384723663\n",
      "Training loss for batch 5579 : 0.02743077278137207\n",
      "Training loss for batch 5580 : 0.284684419631958\n",
      "Training loss for batch 5581 : 0.10122907161712646\n",
      "Training loss for batch 5582 : 0.26215365529060364\n",
      "Training loss for batch 5583 : -0.003197583369910717\n",
      "Training loss for batch 5584 : 0.06323091685771942\n",
      "Training loss for batch 5585 : 0.15780189633369446\n",
      "Training loss for batch 5586 : 0.16150470077991486\n",
      "Training loss for batch 5587 : 0.07721028476953506\n",
      "Training loss for batch 5588 : 0.09418150782585144\n",
      "Training loss for batch 5589 : 0.20352819561958313\n",
      "Training loss for batch 5590 : 0.14906735718250275\n",
      "Training loss for batch 5591 : 0.2510003447532654\n",
      "Training loss for batch 5592 : 0.06278044730424881\n",
      "Training loss for batch 5593 : 0.18151669204235077\n",
      "Training loss for batch 5594 : 0.04499509930610657\n",
      "Training loss for batch 5595 : 0.01921682618558407\n",
      "Training loss for batch 5596 : 0.18490400910377502\n",
      "Training loss for batch 5597 : 0.037595994770526886\n",
      "Training loss for batch 5598 : 0.003682399168610573\n",
      "Training loss for batch 5599 : 0.2845361828804016\n",
      "Training loss for batch 5600 : 0.24739283323287964\n",
      "Training loss for batch 5601 : 0.0635632574558258\n",
      "Training loss for batch 5602 : 0.16742576658725739\n",
      "Training loss for batch 5603 : 0.11735999584197998\n",
      "Training loss for batch 5604 : 0.11587001383304596\n",
      "Training loss for batch 5605 : 0.06823711842298508\n",
      "Training loss for batch 5606 : 0.020217102020978928\n",
      "Training loss for batch 5607 : 0.21767543256282806\n",
      "Training loss for batch 5608 : 0.20870094001293182\n",
      "Training loss for batch 5609 : 0.06412356346845627\n",
      "Training loss for batch 5610 : 0.01848049834370613\n",
      "Training loss for batch 5611 : 0.18899153172969818\n",
      "Training loss for batch 5612 : 0.07746383547782898\n",
      "Training loss for batch 5613 : 0.02368547022342682\n",
      "Training loss for batch 5614 : 0.11846659332513809\n",
      "Training loss for batch 5615 : 0.24006468057632446\n",
      "Training loss for batch 5616 : 0.1933831125497818\n",
      "Training loss for batch 5617 : 0.071329265832901\n",
      "Training loss for batch 5618 : 0.05536578595638275\n",
      "Training loss for batch 5619 : 0.12578821182250977\n",
      "Training loss for batch 5620 : 0.06869062036275864\n",
      "Training loss for batch 5621 : 0.187123641371727\n",
      "Training loss for batch 5622 : 0.1085672378540039\n",
      "Training loss for batch 5623 : 0.06269070506095886\n",
      "Training loss for batch 5624 : 0.3119669258594513\n",
      "Training loss for batch 5625 : 0.13772189617156982\n",
      "Training loss for batch 5626 : 0.3434317708015442\n",
      "Training loss for batch 5627 : 0.24689677357673645\n",
      "Training loss for batch 5628 : 0.0012775007635354996\n",
      "Training loss for batch 5629 : 0.28120797872543335\n",
      "Training loss for batch 5630 : 0.2525690197944641\n",
      "Training loss for batch 5631 : 0.2284669280052185\n",
      "Training loss for batch 5632 : 0.08653628826141357\n",
      "Training loss for batch 5633 : 0.18510553240776062\n",
      "Training loss for batch 5634 : 0.10051196813583374\n",
      "Training loss for batch 5635 : 0.1674574464559555\n",
      "Training loss for batch 5636 : 0.03895482048392296\n",
      "Training loss for batch 5637 : 0.006882087327539921\n",
      "Training loss for batch 5638 : 0.22491851449012756\n",
      "Training loss for batch 5639 : 0.14532670378684998\n",
      "Training loss for batch 5640 : 0.4353763461112976\n",
      "Training loss for batch 5641 : 0.07136764377355576\n",
      "Training loss for batch 5642 : 0.11994867026805878\n",
      "Training loss for batch 5643 : 0.06326068937778473\n",
      "Training loss for batch 5644 : 0.13261224329471588\n",
      "Training loss for batch 5645 : 0.1949600726366043\n",
      "Training loss for batch 5646 : 0.16273969411849976\n",
      "Training loss for batch 5647 : 0.261996865272522\n",
      "Training loss for batch 5648 : 0.02800324559211731\n",
      "Training loss for batch 5649 : 0.2758590579032898\n",
      "Training loss for batch 5650 : 0.12670238316059113\n",
      "Training loss for batch 5651 : 0.29196619987487793\n",
      "Training loss for batch 5652 : 0.13712789118289948\n",
      "Training loss for batch 5653 : 0.12582902610301971\n",
      "Training loss for batch 5654 : 0.15600478649139404\n",
      "Training loss for batch 5655 : 0.046678029000759125\n",
      "Training loss for batch 5656 : 0.08025762438774109\n",
      "Training loss for batch 5657 : 0.11865589022636414\n",
      "Training loss for batch 5658 : 0.3135811686515808\n",
      "Training loss for batch 5659 : 0.26437902450561523\n",
      "Training loss for batch 5660 : 0.15508748590946198\n",
      "Training loss for batch 5661 : 0.09199908375740051\n",
      "Training loss for batch 5662 : 0.14962857961654663\n",
      "Training loss for batch 5663 : 0.11068856716156006\n",
      "Training loss for batch 5664 : 0.10829148441553116\n",
      "Training loss for batch 5665 : 0.03956587612628937\n",
      "Training loss for batch 5666 : 0.04489488527178764\n",
      "Training loss for batch 5667 : 0.06001148372888565\n",
      "Training loss for batch 5668 : 0.030731838196516037\n",
      "Training loss for batch 5669 : 0.2392849624156952\n",
      "Training loss for batch 5670 : 0.05517921596765518\n",
      "Training loss for batch 5671 : 0.5049601793289185\n",
      "Training loss for batch 5672 : 0.06012902408838272\n",
      "Training loss for batch 5673 : 0.1278861165046692\n",
      "Training loss for batch 5674 : 0.18584772944450378\n",
      "Training loss for batch 5675 : 0.04961368814110756\n",
      "Training loss for batch 5676 : 0.34096741676330566\n",
      "Training loss for batch 5677 : 0.05346772074699402\n",
      "Training loss for batch 5678 : 0.14466264843940735\n",
      "Training loss for batch 5679 : 0.03926343098282814\n",
      "Training loss for batch 5680 : 0.11765392869710922\n",
      "Training loss for batch 5681 : 0.007245426531881094\n",
      "Training loss for batch 5682 : -0.0007200178224593401\n",
      "Training loss for batch 5683 : 0.3335655927658081\n",
      "Training loss for batch 5684 : 0.05039189010858536\n",
      "Training loss for batch 5685 : 0.16356459259986877\n",
      "Training loss for batch 5686 : 0.025647884234786034\n",
      "Training loss for batch 5687 : 0.006188427563756704\n",
      "Training loss for batch 5688 : 0.150856152176857\n",
      "Training loss for batch 5689 : 0.04283600300550461\n",
      "Training loss for batch 5690 : 0.26976341009140015\n",
      "Training loss for batch 5691 : 0.11424995213747025\n",
      "Training loss for batch 5692 : 0.04157330095767975\n",
      "Training loss for batch 5693 : 0.106717050075531\n",
      "Training loss for batch 5694 : 0.02662680670619011\n",
      "Training loss for batch 5695 : 0.0491766557097435\n",
      "Training loss for batch 5696 : 0.0016326340846717358\n",
      "Training loss for batch 5697 : 0.07735724747180939\n",
      "Training loss for batch 5698 : 0.35307538509368896\n",
      "Training loss for batch 5699 : 0.0006632943404838443\n",
      "Training loss for batch 5700 : 0.2091175764799118\n",
      "Training loss for batch 5701 : 0.2716854214668274\n",
      "Training loss for batch 5702 : 0.03457004204392433\n",
      "Training loss for batch 5703 : 0.09992879629135132\n",
      "Training loss for batch 5704 : 0.09309834986925125\n",
      "Training loss for batch 5705 : 0.0823964923620224\n",
      "Training loss for batch 5706 : 0.06858579814434052\n",
      "Training loss for batch 5707 : 0.22630394995212555\n",
      "Training loss for batch 5708 : 0.27405810356140137\n",
      "Training loss for batch 5709 : 0.11469204723834991\n",
      "Training loss for batch 5710 : 0.12628796696662903\n",
      "Training loss for batch 5711 : 0.024280287325382233\n",
      "Training loss for batch 5712 : 0.3487054705619812\n",
      "Training loss for batch 5713 : 0.027284706011414528\n",
      "Training loss for batch 5714 : 0.34576523303985596\n",
      "Training loss for batch 5715 : 0.12886755168437958\n",
      "Training loss for batch 5716 : 0.06330876797437668\n",
      "Training loss for batch 5717 : 0.06116538494825363\n",
      "Training loss for batch 5718 : 0.008910264819860458\n",
      "Training loss for batch 5719 : 0.11480332911014557\n",
      "Training loss for batch 5720 : 0.11285805702209473\n",
      "Training loss for batch 5721 : 0.04985258728265762\n",
      "Training loss for batch 5722 : 0.07913683354854584\n",
      "Training loss for batch 5723 : 0.0735190287232399\n",
      "Training loss for batch 5724 : 0.19057807326316833\n",
      "Training loss for batch 5725 : 0.28483736515045166\n",
      "Training loss for batch 5726 : 0.24680708348751068\n",
      "Training loss for batch 5727 : 0.2725249230861664\n",
      "Training loss for batch 5728 : 0.1289301961660385\n",
      "Training loss for batch 5729 : 0.029034646227955818\n",
      "Training loss for batch 5730 : 0.1534660905599594\n",
      "Training loss for batch 5731 : 0.25394943356513977\n",
      "Training loss for batch 5732 : 0.05452129244804382\n",
      "Training loss for batch 5733 : 0.19248414039611816\n",
      "Training loss for batch 5734 : 0.23599836230278015\n",
      "Training loss for batch 5735 : 0.14363060891628265\n",
      "Training loss for batch 5736 : 0.08907808363437653\n",
      "Training loss for batch 5737 : 0.26887238025665283\n",
      "Training loss for batch 5738 : 0.2083132565021515\n",
      "Training loss for batch 5739 : 0.01053557824343443\n",
      "Training loss for batch 5740 : 0.5461884140968323\n",
      "Training loss for batch 5741 : 0.48927661776542664\n",
      "Training loss for batch 5742 : 0.020635098218917847\n",
      "Training loss for batch 5743 : 0.039384108036756516\n",
      "Training loss for batch 5744 : 0.0062675755470991135\n",
      "Training loss for batch 5745 : 0.05915233492851257\n",
      "Training loss for batch 5746 : 0.41339439153671265\n",
      "Training loss for batch 5747 : 0.08075195550918579\n",
      "Training loss for batch 5748 : 0.28800031542778015\n",
      "Training loss for batch 5749 : 0.0018165608635172248\n",
      "Training loss for batch 5750 : 0.01194828748703003\n",
      "Training loss for batch 5751 : 0.12764659523963928\n",
      "Training loss for batch 5752 : 0.16498440504074097\n",
      "Training loss for batch 5753 : 0.2337118536233902\n",
      "Training loss for batch 5754 : 0.0369064025580883\n",
      "Training loss for batch 5755 : 0.028095882385969162\n",
      "Training loss for batch 5756 : 0.019612055271863937\n",
      "Training loss for batch 5757 : 0.20257459580898285\n",
      "Training loss for batch 5758 : 0.2324019968509674\n",
      "Training loss for batch 5759 : 0.0\n",
      "Training loss for batch 5760 : 0.12519364058971405\n",
      "Training loss for batch 5761 : 0.21724753081798553\n",
      "Training loss for batch 5762 : 0.0807059109210968\n",
      "Training loss for batch 5763 : 0.09763214737176895\n",
      "Training loss for batch 5764 : 0.245141863822937\n",
      "Training loss for batch 5765 : 0.32749831676483154\n",
      "Training loss for batch 5766 : 0.04956405982375145\n",
      "Training loss for batch 5767 : 0.387090265750885\n",
      "Training loss for batch 5768 : 0.06910264492034912\n",
      "Training loss for batch 5769 : 0.21128059923648834\n",
      "Training loss for batch 5770 : 0.17049695551395416\n",
      "Training loss for batch 5771 : 0.020487282425165176\n",
      "Training loss for batch 5772 : 0.14992187917232513\n",
      "Training loss for batch 5773 : 0.14667785167694092\n",
      "Training loss for batch 5774 : 0.0808202400803566\n",
      "Training loss for batch 5775 : 0.012664789333939552\n",
      "Training loss for batch 5776 : 0.1472853422164917\n",
      "Training loss for batch 5777 : 0.04807955399155617\n",
      "Training loss for batch 5778 : 0.0650719702243805\n",
      "Training loss for batch 5779 : 0.011025354266166687\n",
      "Training loss for batch 5780 : 0.25986531376838684\n",
      "Training loss for batch 5781 : 0.162407785654068\n",
      "Training loss for batch 5782 : 0.254257470369339\n",
      "Training loss for batch 5783 : 0.0\n",
      "Training loss for batch 5784 : 0.14663538336753845\n",
      "Training loss for batch 5785 : 0.1425146758556366\n",
      "Training loss for batch 5786 : 0.210811585187912\n",
      "Training loss for batch 5787 : 0.10384930670261383\n",
      "Training loss for batch 5788 : 0.1264648139476776\n",
      "Training loss for batch 5789 : 0.36667582392692566\n",
      "Training loss for batch 5790 : 0.10298243165016174\n",
      "Training loss for batch 5791 : 0.18809577822685242\n",
      "Training loss for batch 5792 : 0.22921587526798248\n",
      "Training loss for batch 5793 : 0.07779764384031296\n",
      "Training loss for batch 5794 : 0.11063285917043686\n",
      "Training loss for batch 5795 : 0.18880340456962585\n",
      "Training loss for batch 5796 : 0.0160922110080719\n",
      "Training loss for batch 5797 : 0.3420165777206421\n",
      "Training loss for batch 5798 : 0.0425475537776947\n",
      "Training loss for batch 5799 : 0.18770137429237366\n",
      "Training loss for batch 5800 : 0.23863503336906433\n",
      "Training loss for batch 5801 : 0.21433918178081512\n",
      "Training loss for batch 5802 : 0.11076688021421432\n",
      "Training loss for batch 5803 : 0.22215089201927185\n",
      "Training loss for batch 5804 : 0.045684266835451126\n",
      "Training loss for batch 5805 : 0.0012479121796786785\n",
      "Training loss for batch 5806 : 0.12635424733161926\n",
      "Training loss for batch 5807 : 0.23929426074028015\n",
      "Training loss for batch 5808 : 0.07952357828617096\n",
      "Training loss for batch 5809 : 0.01942255161702633\n",
      "Training loss for batch 5810 : 0.09236553311347961\n",
      "Training loss for batch 5811 : 0.11778593808412552\n",
      "Training loss for batch 5812 : 0.08070102334022522\n",
      "Training loss for batch 5813 : 0.06545441597700119\n",
      "Training loss for batch 5814 : 0.15886445343494415\n",
      "Training loss for batch 5815 : 0.2215185910463333\n",
      "Training loss for batch 5816 : 0.49229249358177185\n",
      "Training loss for batch 5817 : 0.12139296531677246\n",
      "Training loss for batch 5818 : 0.13261809945106506\n",
      "Training loss for batch 5819 : 0.21919569373130798\n",
      "Training loss for batch 5820 : 0.07550966739654541\n",
      "Training loss for batch 5821 : 0.08383706957101822\n",
      "Training loss for batch 5822 : 0.12759408354759216\n",
      "Training loss for batch 5823 : 0.16729454696178436\n",
      "Training loss for batch 5824 : 0.04041028395295143\n",
      "Training loss for batch 5825 : 0.10307493060827255\n",
      "Training loss for batch 5826 : 0.1559353470802307\n",
      "Training loss for batch 5827 : 0.002127010840922594\n",
      "Training loss for batch 5828 : 0.12567941844463348\n",
      "Training loss for batch 5829 : 0.15357491374015808\n",
      "Training loss for batch 5830 : 0.18037402629852295\n",
      "Training loss for batch 5831 : 0.13837528228759766\n",
      "Training loss for batch 5832 : 0.10992521047592163\n",
      "Training loss for batch 5833 : 0.17647479474544525\n",
      "Training loss for batch 5834 : 0.42252588272094727\n",
      "Training loss for batch 5835 : 0.04784941300749779\n",
      "Training loss for batch 5836 : 0.07752332091331482\n",
      "Training loss for batch 5837 : 0.06840646266937256\n",
      "Training loss for batch 5838 : 0.05610549822449684\n",
      "Training loss for batch 5839 : 0.09925541281700134\n",
      "Training loss for batch 5840 : 0.07085855305194855\n",
      "Training loss for batch 5841 : 0.01591726578772068\n",
      "Training loss for batch 5842 : 0.1692741960287094\n",
      "Training loss for batch 5843 : 0.03442259877920151\n",
      "Training loss for batch 5844 : 0.014375212602317333\n",
      "Training loss for batch 5845 : 0.08679241687059402\n",
      "Training loss for batch 5846 : 0.056151099503040314\n",
      "Training loss for batch 5847 : 0.10716903209686279\n",
      "Training loss for batch 5848 : 0.14710134267807007\n",
      "Training loss for batch 5849 : 0.1238732859492302\n",
      "Training loss for batch 5850 : 0.02997293509542942\n",
      "Training loss for batch 5851 : 0.30081817507743835\n",
      "Training loss for batch 5852 : 0.32773739099502563\n",
      "Training loss for batch 5853 : 0.10857048630714417\n",
      "Training loss for batch 5854 : 0.1665448546409607\n",
      "Training loss for batch 5855 : 0.1262035369873047\n",
      "Training loss for batch 5856 : 0.003808260429650545\n",
      "Training loss for batch 5857 : 0.0190268624573946\n",
      "Training loss for batch 5858 : 0.2604866325855255\n",
      "Training loss for batch 5859 : 0.32621529698371887\n",
      "Training loss for batch 5860 : 0.10999733209609985\n",
      "Training loss for batch 5861 : 0.0516059547662735\n",
      "Training loss for batch 5862 : 0.009486055001616478\n",
      "Training loss for batch 5863 : 0.06908721476793289\n",
      "Training loss for batch 5864 : 0.04735798388719559\n",
      "Training loss for batch 5865 : 0.12600523233413696\n",
      "Training loss for batch 5866 : 0.03357474505901337\n",
      "Training loss for batch 5867 : 0.06143609806895256\n",
      "Training loss for batch 5868 : 0.07171989232301712\n",
      "Training loss for batch 5869 : 0.12258614599704742\n",
      "Training loss for batch 5870 : 0.257818341255188\n",
      "Training loss for batch 5871 : 0.11869769543409348\n",
      "Training loss for batch 5872 : 0.0637848898768425\n",
      "Training loss for batch 5873 : 0.39212626218795776\n",
      "Training loss for batch 5874 : 0.40854355692863464\n",
      "Training loss for batch 5875 : 0.16661033034324646\n",
      "Training loss for batch 5876 : 0.10178642719984055\n",
      "Training loss for batch 5877 : 0.08694997429847717\n",
      "Training loss for batch 5878 : 0.24519027769565582\n",
      "Training loss for batch 5879 : 0.08462780714035034\n",
      "Training loss for batch 5880 : 0.03719489648938179\n",
      "Training loss for batch 5881 : 0.2733827233314514\n",
      "Training loss for batch 5882 : 0.09082873910665512\n",
      "Training loss for batch 5883 : 0.029603421688079834\n",
      "Training loss for batch 5884 : 0.19404764473438263\n",
      "Training loss for batch 5885 : 0.2463759481906891\n",
      "Training loss for batch 5886 : 0.06794053316116333\n",
      "Training loss for batch 5887 : 0.11823320388793945\n",
      "Training loss for batch 5888 : 0.0881662666797638\n",
      "Training loss for batch 5889 : 0.3139253556728363\n",
      "Training loss for batch 5890 : 0.19868691265583038\n",
      "Training loss for batch 5891 : 0.2764640152454376\n",
      "Training loss for batch 5892 : 0.07748795300722122\n",
      "Training loss for batch 5893 : 0.08945521712303162\n",
      "Training loss for batch 5894 : 0.07327316701412201\n",
      "Training loss for batch 5895 : 0.3295583724975586\n",
      "Training loss for batch 5896 : 0.22952014207839966\n",
      "Training loss for batch 5897 : 0.13747477531433105\n",
      "Training loss for batch 5898 : 0.10354329645633698\n",
      "Training loss for batch 5899 : 0.35916557908058167\n",
      "Training loss for batch 5900 : 0.23663774132728577\n",
      "Training loss for batch 5901 : 0.14859753847122192\n",
      "Training loss for batch 5902 : 0.10208427906036377\n",
      "Training loss for batch 5903 : 0.05770328640937805\n",
      "Training loss for batch 5904 : 0.07335713505744934\n",
      "Training loss for batch 5905 : 0.0921010747551918\n",
      "Training loss for batch 5906 : 0.23388037085533142\n",
      "Training loss for batch 5907 : 0.0333363376557827\n",
      "Training loss for batch 5908 : 0.0672796294093132\n",
      "Training loss for batch 5909 : 0.04617823660373688\n",
      "Training loss for batch 5910 : 0.03223853558301926\n",
      "Training loss for batch 5911 : 0.026470284909009933\n",
      "Training loss for batch 5912 : 0.09646302461624146\n",
      "Training loss for batch 5913 : 0.2856667637825012\n",
      "Training loss for batch 5914 : 0.15610726177692413\n",
      "Training loss for batch 5915 : 0.1159759983420372\n",
      "Training loss for batch 5916 : 0.06317003071308136\n",
      "Training loss for batch 5917 : 0.033276982605457306\n",
      "Training loss for batch 5918 : 0.008205914869904518\n",
      "Training loss for batch 5919 : 0.08227711915969849\n",
      "Training loss for batch 5920 : 0.13680848479270935\n",
      "Training loss for batch 5921 : 0.2856971323490143\n",
      "Training loss for batch 5922 : 0.04434118792414665\n",
      "Training loss for batch 5923 : 0.03359805420041084\n",
      "Training loss for batch 5924 : 0.17076508700847626\n",
      "Training loss for batch 5925 : 0.0241861455142498\n",
      "Training loss for batch 5926 : 0.14706721901893616\n",
      "Training loss for batch 5927 : 0.08173464983701706\n",
      "Training loss for batch 5928 : 0.12302953004837036\n",
      "Training loss for batch 5929 : 0.34683215618133545\n",
      "Training loss for batch 5930 : 0.16877637803554535\n",
      "Training loss for batch 5931 : 0.5715727806091309\n",
      "Training loss for batch 5932 : 0.22133088111877441\n",
      "Training loss for batch 5933 : 0.26424673199653625\n",
      "Training loss for batch 5934 : -0.0037569578271359205\n",
      "Training loss for batch 5935 : 0.04193848744034767\n",
      "Training loss for batch 5936 : 0.02321384847164154\n",
      "Training loss for batch 5937 : 0.04034529626369476\n",
      "Training loss for batch 5938 : 0.19404509663581848\n",
      "Training loss for batch 5939 : 0.0323980376124382\n",
      "Training loss for batch 5940 : 0.346650630235672\n",
      "Training loss for batch 5941 : 0.02356923557817936\n",
      "Training loss for batch 5942 : 0.05422712489962578\n",
      "Training loss for batch 5943 : 0.05746595561504364\n",
      "Training loss for batch 5944 : 0.0783090889453888\n",
      "Training loss for batch 5945 : 0.2435605227947235\n",
      "Training loss for batch 5946 : 0.08519817143678665\n",
      "Training loss for batch 5947 : 0.025757113471627235\n",
      "Training loss for batch 5948 : 0.1692197471857071\n",
      "Training loss for batch 5949 : 0.1802552044391632\n",
      "Training loss for batch 5950 : 0.09618492424488068\n",
      "Training loss for batch 5951 : 0.1038791835308075\n",
      "Training loss for batch 5952 : 0.13563686609268188\n",
      "Training loss for batch 5953 : 0.11774104088544846\n",
      "Training loss for batch 5954 : 0.008637093007564545\n",
      "Training loss for batch 5955 : 0.24414151906967163\n",
      "Training loss for batch 5956 : 0.046781137585639954\n",
      "Training loss for batch 5957 : 0.03122403845191002\n",
      "Training loss for batch 5958 : 0.21825169026851654\n",
      "Training loss for batch 5959 : 0.10996301472187042\n",
      "Training loss for batch 5960 : 0.07042736560106277\n",
      "Training loss for batch 5961 : 0.044812142848968506\n",
      "Training loss for batch 5962 : 0.1725172996520996\n",
      "Training loss for batch 5963 : 0.029650026932358742\n",
      "Training loss for batch 5964 : 0.3051062524318695\n",
      "Training loss for batch 5965 : 0.06995309889316559\n",
      "Training loss for batch 5966 : 0.12640617787837982\n",
      "Training loss for batch 5967 : 0.20110544562339783\n",
      "Training loss for batch 5968 : 0.19632896780967712\n",
      "Training loss for batch 5969 : 0.20994830131530762\n",
      "Training loss for batch 5970 : 0.13447362184524536\n",
      "Training loss for batch 5971 : 0.11593519151210785\n",
      "Training loss for batch 5972 : 0.09700360894203186\n",
      "Training loss for batch 5973 : 0.270386278629303\n",
      "Training loss for batch 5974 : 0.017200632020831108\n",
      "Training loss for batch 5975 : 0.13896101713180542\n",
      "Training loss for batch 5976 : 0.16520600020885468\n",
      "Training loss for batch 5977 : 0.1632191240787506\n",
      "Training loss for batch 5978 : 0.24441814422607422\n",
      "Training loss for batch 5979 : 0.03138133883476257\n",
      "Training loss for batch 5980 : 0.07452170550823212\n",
      "Training loss for batch 5981 : 0.13207712769508362\n",
      "Training loss for batch 5982 : 0.025724390521645546\n",
      "Training loss for batch 5983 : 0.1460793912410736\n",
      "Training loss for batch 5984 : 0.24787506461143494\n",
      "Training loss for batch 5985 : 0.40431419014930725\n",
      "Training loss for batch 5986 : 0.18018826842308044\n",
      "Training loss for batch 5987 : 0.2371908724308014\n",
      "Training loss for batch 5988 : 0.08536684513092041\n",
      "Training loss for batch 5989 : 0.3073718249797821\n",
      "Training loss for batch 5990 : 0.25742465257644653\n",
      "Training loss for batch 5991 : 0.26308900117874146\n",
      "Training loss for batch 5992 : 0.08425700664520264\n",
      "Training loss for batch 5993 : 0.07093121111392975\n",
      "Training loss for batch 5994 : 0.005664626136422157\n",
      "Training loss for batch 5995 : 0.06163210794329643\n",
      "Training loss for batch 5996 : 0.12771618366241455\n",
      "Training loss for batch 5997 : 0.06222677603363991\n",
      "Training loss for batch 5998 : 0.13860172033309937\n",
      "Training loss for batch 5999 : 0.04264310002326965\n",
      "Training loss for batch 6000 : 0.10056997090578079\n",
      "Training loss for batch 6001 : 0.38346391916275024\n",
      "Training loss for batch 6002 : 0.2192276269197464\n",
      "Training loss for batch 6003 : 0.023452289402484894\n",
      "Training loss for batch 6004 : 0.0394778698682785\n",
      "Training loss for batch 6005 : 0.13769924640655518\n",
      "Training loss for batch 6006 : 0.15786278247833252\n",
      "Training loss for batch 6007 : 0.07877860963344574\n",
      "Training loss for batch 6008 : 0.10355182737112045\n",
      "Training loss for batch 6009 : 0.08992717415094376\n",
      "Training loss for batch 6010 : 0.1581244021654129\n",
      "Training loss for batch 6011 : 0.10974041372537613\n",
      "Training loss for batch 6012 : 0.07540468871593475\n",
      "Training loss for batch 6013 : 0.23423883318901062\n",
      "Training loss for batch 6014 : 0.06269896030426025\n",
      "Training loss for batch 6015 : 0.058942634612321854\n",
      "Training loss for batch 6016 : 0.009008240886032581\n",
      "Training loss for batch 6017 : 0.12991991639137268\n",
      "Training loss for batch 6018 : 0.1415036916732788\n",
      "Training loss for batch 6019 : 0.25638288259506226\n",
      "Training loss for batch 6020 : 0.024773217737674713\n",
      "Training loss for batch 6021 : 0.03612229973077774\n",
      "Training loss for batch 6022 : 0.05894411355257034\n",
      "Training loss for batch 6023 : 0.11793511360883713\n",
      "Training loss for batch 6024 : 0.30752241611480713\n",
      "Training loss for batch 6025 : 0.028263550251722336\n",
      "Training loss for batch 6026 : 0.0\n",
      "Training loss for batch 6027 : 0.013859815895557404\n",
      "Training loss for batch 6028 : 0.05782647058367729\n",
      "Training loss for batch 6029 : 0.1506442278623581\n",
      "Training loss for batch 6030 : 0.014894853346049786\n",
      "Training loss for batch 6031 : 0.12423120439052582\n",
      "Training loss for batch 6032 : 0.358729749917984\n",
      "Training loss for batch 6033 : 0.0007100403308868408\n",
      "Training loss for batch 6034 : 0.1411582976579666\n",
      "Training loss for batch 6035 : 0.22496859729290009\n",
      "Training loss for batch 6036 : 0.11513473838567734\n",
      "Training loss for batch 6037 : 0.15454454720020294\n",
      "Training loss for batch 6038 : 0.02425956353545189\n",
      "Training loss for batch 6039 : 0.0939057469367981\n",
      "Training loss for batch 6040 : 0.05839572846889496\n",
      "Training loss for batch 6041 : 0.009171288460493088\n",
      "Training loss for batch 6042 : 0.14289864897727966\n",
      "Training loss for batch 6043 : 0.1644059717655182\n",
      "Training loss for batch 6044 : 0.003925066441297531\n",
      "Training loss for batch 6045 : 0.26578670740127563\n",
      "Training loss for batch 6046 : 0.09667468070983887\n",
      "Training loss for batch 6047 : 0.1845678985118866\n",
      "Training loss for batch 6048 : 0.13322246074676514\n",
      "Training loss for batch 6049 : 0.20924751460552216\n",
      "Training loss for batch 6050 : 0.2847743332386017\n",
      "Training loss for batch 6051 : 0.1470796763896942\n",
      "Training loss for batch 6052 : 0.08160004019737244\n",
      "Training loss for batch 6053 : 0.007233083248138428\n",
      "Training loss for batch 6054 : 0.1265876144170761\n",
      "Training loss for batch 6055 : 0.0023493270855396986\n",
      "Training loss for batch 6056 : 0.4406591057777405\n",
      "Training loss for batch 6057 : 0.1198115423321724\n",
      "Training loss for batch 6058 : 0.13744625449180603\n",
      "Training loss for batch 6059 : 0.015384793281555176\n",
      "Training loss for batch 6060 : 0.24609078466892242\n",
      "Training loss for batch 6061 : 0.023666825145483017\n",
      "Training loss for batch 6062 : 0.25852006673812866\n",
      "Training loss for batch 6063 : 0.23771128058433533\n",
      "Training loss for batch 6064 : 0.19555890560150146\n",
      "Training loss for batch 6065 : 0.1335238814353943\n",
      "Training loss for batch 6066 : 0.15279892086982727\n",
      "Training loss for batch 6067 : 0.3107609152793884\n",
      "Training loss for batch 6068 : 0.13915301859378815\n",
      "Training loss for batch 6069 : 0.13290196657180786\n",
      "Training loss for batch 6070 : 0.22192704677581787\n",
      "Training loss for batch 6071 : 0.02287144958972931\n",
      "Training loss for batch 6072 : 0.15923523902893066\n",
      "Training loss for batch 6073 : 0.05771883204579353\n",
      "Training loss for batch 6074 : 0.18938179314136505\n",
      "Training loss for batch 6075 : 0.1289340853691101\n",
      "Training loss for batch 6076 : 0.01667775772511959\n",
      "Training loss for batch 6077 : 0.04550720006227493\n",
      "Training loss for batch 6078 : 0.1468752920627594\n",
      "Training loss for batch 6079 : 0.05975259467959404\n",
      "Training loss for batch 6080 : 0.06851159781217575\n",
      "Training loss for batch 6081 : 0.13244470953941345\n",
      "Training loss for batch 6082 : 0.08882949501276016\n",
      "Training loss for batch 6083 : 0.17268337309360504\n",
      "Training loss for batch 6084 : 0.10265558958053589\n",
      "Training loss for batch 6085 : 0.04963843896985054\n",
      "Training loss for batch 6086 : 0.05272085964679718\n",
      "Training loss for batch 6087 : 0.10992655903100967\n",
      "Training loss for batch 6088 : 0.13334064185619354\n",
      "Training loss for batch 6089 : 0.1345374435186386\n",
      "Training loss for batch 6090 : 0.15303492546081543\n",
      "Training loss for batch 6091 : 0.09641988575458527\n",
      "Training loss for batch 6092 : 0.09656281769275665\n",
      "Training loss for batch 6093 : 0.02443087287247181\n",
      "Training loss for batch 6094 : 0.05874031037092209\n",
      "Training loss for batch 6095 : 0.23633375763893127\n",
      "Training loss for batch 6096 : 0.35725897550582886\n",
      "Training loss for batch 6097 : 0.3199969232082367\n",
      "Training loss for batch 6098 : 0.3460429012775421\n",
      "Training loss for batch 6099 : 0.021527530625462532\n",
      "Training loss for batch 6100 : 0.05421997234225273\n",
      "Training loss for batch 6101 : 0.058904632925987244\n",
      "Training loss for batch 6102 : 0.12671127915382385\n",
      "Training loss for batch 6103 : 0.06592397391796112\n",
      "Training loss for batch 6104 : 0.1108783707022667\n",
      "Training loss for batch 6105 : 0.1478726863861084\n",
      "Training loss for batch 6106 : 0.053849752992391586\n",
      "Training loss for batch 6107 : 0.01935814693570137\n",
      "Training loss for batch 6108 : 0.08066330850124359\n",
      "Training loss for batch 6109 : 0.24948179721832275\n",
      "Training loss for batch 6110 : 0.20693914592266083\n",
      "Training loss for batch 6111 : 0.3514401912689209\n",
      "Training loss for batch 6112 : 0.3089565634727478\n",
      "Training loss for batch 6113 : 0.025527533143758774\n",
      "Training loss for batch 6114 : 0.00819426029920578\n",
      "Training loss for batch 6115 : 0.15519961714744568\n",
      "Training loss for batch 6116 : 0.21504054963588715\n",
      "Training loss for batch 6117 : 0.27539440989494324\n",
      "Training loss for batch 6118 : 0.07536551356315613\n",
      "Training loss for batch 6119 : 0.0\n",
      "Training loss for batch 6120 : 0.16812820732593536\n",
      "Training loss for batch 6121 : 0.03204064071178436\n",
      "Training loss for batch 6122 : 0.03393784165382385\n",
      "Training loss for batch 6123 : 0.029556695371866226\n",
      "Training loss for batch 6124 : 0.06042111665010452\n",
      "Training loss for batch 6125 : 0.07200565934181213\n",
      "Training loss for batch 6126 : 0.2609465718269348\n",
      "Training loss for batch 6127 : 0.061313457787036896\n",
      "Training loss for batch 6128 : 0.1295781135559082\n",
      "Training loss for batch 6129 : 0.12513968348503113\n",
      "Training loss for batch 6130 : 0.05751696601510048\n",
      "Training loss for batch 6131 : 0.1508837342262268\n",
      "Training loss for batch 6132 : 0.1616140902042389\n",
      "Training loss for batch 6133 : 0.17435377836227417\n",
      "Training loss for batch 6134 : 0.4937821328639984\n",
      "Training loss for batch 6135 : 0.022056736052036285\n",
      "Training loss for batch 6136 : 0.1665862798690796\n",
      "Training loss for batch 6137 : 0.01302835252135992\n",
      "Training loss for batch 6138 : 0.12706969678401947\n",
      "Training loss for batch 6139 : 0.03019505925476551\n",
      "Training loss for batch 6140 : 0.02787889912724495\n",
      "Training loss for batch 6141 : 0.11144306510686874\n",
      "Training loss for batch 6142 : 0.18841895461082458\n",
      "Training loss for batch 6143 : 0.05301087722182274\n",
      "Training loss for batch 6144 : -0.0024164768401533365\n",
      "Training loss for batch 6145 : 0.2975146174430847\n",
      "Training loss for batch 6146 : 0.2532724142074585\n",
      "Training loss for batch 6147 : 0.07762034982442856\n",
      "Training loss for batch 6148 : 0.18802787363529205\n",
      "Training loss for batch 6149 : 0.0819096788764\n",
      "Training loss for batch 6150 : 0.08952596783638\n",
      "Training loss for batch 6151 : 0.12649217247962952\n",
      "Training loss for batch 6152 : 0.4752265214920044\n",
      "Training loss for batch 6153 : 0.05101216211915016\n",
      "Training loss for batch 6154 : 0.1047506257891655\n",
      "Training loss for batch 6155 : 0.008804037235677242\n",
      "Training loss for batch 6156 : 0.17562799155712128\n",
      "Training loss for batch 6157 : 0.10073128342628479\n",
      "Training loss for batch 6158 : 0.010977638885378838\n",
      "Training loss for batch 6159 : 0.15640488266944885\n",
      "Training loss for batch 6160 : 0.2761276364326477\n",
      "Training loss for batch 6161 : 0.1101628914475441\n",
      "Training loss for batch 6162 : 0.08251048624515533\n",
      "Training loss for batch 6163 : 0.16597360372543335\n",
      "Training loss for batch 6164 : 0.0\n",
      "Training loss for batch 6165 : 0.20563051104545593\n",
      "Training loss for batch 6166 : 0.11225904524326324\n",
      "Training loss for batch 6167 : 0.10086666792631149\n",
      "Training loss for batch 6168 : 0.0648655816912651\n",
      "Training loss for batch 6169 : 0.13046985864639282\n",
      "Training loss for batch 6170 : 0.00556395947933197\n",
      "Training loss for batch 6171 : 0.1809922456741333\n",
      "Training loss for batch 6172 : 0.1810065656900406\n",
      "Training loss for batch 6173 : 0.1910601705312729\n",
      "Training loss for batch 6174 : 0.1790010780096054\n",
      "Training loss for batch 6175 : 0.016411688178777695\n",
      "Training loss for batch 6176 : 0.003559849224984646\n",
      "Training loss for batch 6177 : 0.0\n",
      "Training loss for batch 6178 : 0.06977648288011551\n",
      "Training loss for batch 6179 : 0.25011008977890015\n",
      "Training loss for batch 6180 : 0.08688928931951523\n",
      "Training loss for batch 6181 : 0.12263317406177521\n",
      "Training loss for batch 6182 : 0.03590193763375282\n",
      "Training loss for batch 6183 : 0.0675373300909996\n",
      "Training loss for batch 6184 : 0.13105818629264832\n",
      "Training loss for batch 6185 : 0.10162769258022308\n",
      "Training loss for batch 6186 : 0.21436160802841187\n",
      "Training loss for batch 6187 : 0.4352877736091614\n",
      "Training loss for batch 6188 : 0.17370450496673584\n",
      "Training loss for batch 6189 : 0.21000215411186218\n",
      "Training loss for batch 6190 : 0.16916528344154358\n",
      "Training loss for batch 6191 : 0.009404639713466167\n",
      "Training loss for batch 6192 : 0.08922815322875977\n",
      "Training loss for batch 6193 : 0.1707928627729416\n",
      "Training loss for batch 6194 : 0.10120107233524323\n",
      "Training loss for batch 6195 : 0.053892046213150024\n",
      "Training loss for batch 6196 : 0.03882264345884323\n",
      "Training loss for batch 6197 : 0.19470106065273285\n",
      "Training loss for batch 6198 : 0.1128208339214325\n",
      "Training loss for batch 6199 : 0.0353381410241127\n",
      "Training loss for batch 6200 : 0.25422412157058716\n",
      "Training loss for batch 6201 : 0.1778571754693985\n",
      "Training loss for batch 6202 : 0.10513798147439957\n",
      "Training loss for batch 6203 : 0.16583094000816345\n",
      "Training loss for batch 6204 : 0.13057631254196167\n",
      "Training loss for batch 6205 : 0.17093273997306824\n",
      "Training loss for batch 6206 : 0.22686822712421417\n",
      "Training loss for batch 6207 : 0.2590600550174713\n",
      "Training loss for batch 6208 : 0.2755870521068573\n",
      "Training loss for batch 6209 : 0.001574099063873291\n",
      "Training loss for batch 6210 : 0.0643276795744896\n",
      "Training loss for batch 6211 : 0.0929175466299057\n",
      "Training loss for batch 6212 : 0.1711309254169464\n",
      "Training loss for batch 6213 : 0.339366614818573\n",
      "Training loss for batch 6214 : 0.06111941114068031\n",
      "Training loss for batch 6215 : 0.13704735040664673\n",
      "Training loss for batch 6216 : 0.09712138026952744\n",
      "Training loss for batch 6217 : 0.2102871984243393\n",
      "Training loss for batch 6218 : 0.2195993810892105\n",
      "Training loss for batch 6219 : 0.07371464371681213\n",
      "Training loss for batch 6220 : 0.07075417041778564\n",
      "Training loss for batch 6221 : 0.04466366022825241\n",
      "Training loss for batch 6222 : 0.027981020510196686\n",
      "Training loss for batch 6223 : 0.0494781956076622\n",
      "Training loss for batch 6224 : -0.0019412422552704811\n",
      "Training loss for batch 6225 : 0.05390726774930954\n",
      "Training loss for batch 6226 : 0.1439971774816513\n",
      "Training loss for batch 6227 : 0.2081201672554016\n",
      "Training loss for batch 6228 : 0.10877849906682968\n",
      "Training loss for batch 6229 : 0.06376820802688599\n",
      "Training loss for batch 6230 : 0.030705301091074944\n",
      "Training loss for batch 6231 : 0.03828992694616318\n",
      "Training loss for batch 6232 : 0.22428666055202484\n",
      "Training loss for batch 6233 : 0.03960011526942253\n",
      "Training loss for batch 6234 : 0.09649232774972916\n",
      "Training loss for batch 6235 : 0.069828100502491\n",
      "Training loss for batch 6236 : 0.18237927556037903\n",
      "Training loss for batch 6237 : 0.11093126982450485\n",
      "Training loss for batch 6238 : 0.08712553232908249\n",
      "Training loss for batch 6239 : 0.01797325536608696\n",
      "Training loss for batch 6240 : 0.23117557168006897\n",
      "Training loss for batch 6241 : 0.21482592821121216\n",
      "Training loss for batch 6242 : 0.257136732339859\n",
      "Training loss for batch 6243 : 0.2797756791114807\n",
      "Training loss for batch 6244 : 0.22371649742126465\n",
      "Training loss for batch 6245 : 0.01840061880648136\n",
      "Training loss for batch 6246 : 0.002906688954681158\n",
      "Training loss for batch 6247 : 0.011853613890707493\n",
      "Training loss for batch 6248 : 0.42926961183547974\n",
      "Training loss for batch 6249 : 0.2494765818119049\n",
      "Training loss for batch 6250 : 0.0215276051312685\n",
      "Training loss for batch 6251 : 0.0\n",
      "Training loss for batch 6252 : 0.03355451300740242\n",
      "Training loss for batch 6253 : 0.013351037167012691\n",
      "Training loss for batch 6254 : 0.07726381719112396\n",
      "Training loss for batch 6255 : 0.1301603764295578\n",
      "Training loss for batch 6256 : 0.08995555341243744\n",
      "Training loss for batch 6257 : 0.040152452886104584\n",
      "Training loss for batch 6258 : 0.0\n",
      "Training loss for batch 6259 : 0.31684762239456177\n",
      "Training loss for batch 6260 : 0.0350332111120224\n",
      "Training loss for batch 6261 : 0.1712653934955597\n",
      "Training loss for batch 6262 : 0.3202066421508789\n",
      "Training loss for batch 6263 : 0.19486786425113678\n",
      "Training loss for batch 6264 : 0.3500750660896301\n",
      "Training loss for batch 6265 : 0.1480700820684433\n",
      "Training loss for batch 6266 : 0.2794417142868042\n",
      "Training loss for batch 6267 : 0.011223386973142624\n",
      "Training loss for batch 6268 : 0.17226266860961914\n",
      "Training loss for batch 6269 : 0.13787448406219482\n",
      "Training loss for batch 6270 : 0.28908199071884155\n",
      "Training loss for batch 6271 : 0.062180839478969574\n",
      "Training loss for batch 6272 : 0.01653987169265747\n",
      "Training loss for batch 6273 : 0.1485796421766281\n",
      "Training loss for batch 6274 : 0.02403591200709343\n",
      "Training loss for batch 6275 : 0.21111427247524261\n",
      "Training loss for batch 6276 : 0.05951879173517227\n",
      "Training loss for batch 6277 : 0.1756153106689453\n",
      "Training loss for batch 6278 : 0.18396076560020447\n",
      "Training loss for batch 6279 : 0.25247615575790405\n",
      "Training loss for batch 6280 : 0.19291481375694275\n",
      "Training loss for batch 6281 : 0.10814092308282852\n",
      "Training loss for batch 6282 : 0.032318174839019775\n",
      "Training loss for batch 6283 : 0.31493520736694336\n",
      "Training loss for batch 6284 : 0.0\n",
      "Training loss for batch 6285 : 0.0876680314540863\n",
      "Training loss for batch 6286 : 0.10454751551151276\n",
      "Training loss for batch 6287 : 0.004753629676997662\n",
      "Training loss for batch 6288 : 0.08260171115398407\n",
      "Training loss for batch 6289 : 0.4674229621887207\n",
      "Training loss for batch 6290 : 0.3425658643245697\n",
      "Training loss for batch 6291 : 0.1269233524799347\n",
      "Training loss for batch 6292 : 0.04019712656736374\n",
      "Training loss for batch 6293 : 0.19930678606033325\n",
      "Training loss for batch 6294 : 0.1478295773267746\n",
      "Training loss for batch 6295 : 0.1238146647810936\n",
      "Training loss for batch 6296 : 0.02118014544248581\n",
      "Training loss for batch 6297 : 0.14740313589572906\n",
      "Training loss for batch 6298 : 0.08797109127044678\n",
      "Training loss for batch 6299 : 0.04620882123708725\n",
      "Training loss for batch 6300 : 0.050221435725688934\n",
      "Training loss for batch 6301 : 0.15299847722053528\n",
      "Training loss for batch 6302 : 0.16332121193408966\n",
      "Training loss for batch 6303 : 0.22169137001037598\n",
      "Training loss for batch 6304 : 0.048555441200733185\n",
      "Training loss for batch 6305 : 0.024968981742858887\n",
      "Training loss for batch 6306 : 0.07105093449354172\n",
      "Training loss for batch 6307 : 0.05436566472053528\n",
      "Training loss for batch 6308 : 0.1275174617767334\n",
      "Training loss for batch 6309 : 0.2838210165500641\n",
      "Training loss for batch 6310 : 0.19925710558891296\n",
      "Training loss for batch 6311 : 0.03548974171280861\n",
      "Training loss for batch 6312 : 0.017208203673362732\n",
      "Training loss for batch 6313 : 0.12503544986248016\n",
      "Training loss for batch 6314 : 0.026721227914094925\n",
      "Training loss for batch 6315 : 0.037588708102703094\n",
      "Training loss for batch 6316 : 0.11137133091688156\n",
      "Training loss for batch 6317 : 0.06144881248474121\n",
      "Training loss for batch 6318 : 0.05631082132458687\n",
      "Training loss for batch 6319 : 0.1769115924835205\n",
      "Training loss for batch 6320 : 0.0457768440246582\n",
      "Training loss for batch 6321 : 0.0991067886352539\n",
      "Training loss for batch 6322 : 0.07644392549991608\n",
      "Training loss for batch 6323 : 0.24037235975265503\n",
      "Training loss for batch 6324 : 0.2554771900177002\n",
      "Training loss for batch 6325 : 0.12328813970088959\n",
      "Training loss for batch 6326 : 0.028103023767471313\n",
      "Training loss for batch 6327 : 0.03606794774532318\n",
      "Training loss for batch 6328 : 0.30050912499427795\n",
      "Training loss for batch 6329 : 0.14218062162399292\n",
      "Training loss for batch 6330 : 0.21349471807479858\n",
      "Training loss for batch 6331 : 0.02305508591234684\n",
      "Training loss for batch 6332 : 0.22327372431755066\n",
      "Training loss for batch 6333 : 0.19246265292167664\n",
      "Training loss for batch 6334 : 0.09331837296485901\n",
      "Training loss for batch 6335 : 0.3443346619606018\n",
      "Training loss for batch 6336 : 0.2290479987859726\n",
      "Training loss for batch 6337 : 0.1734524816274643\n",
      "Training loss for batch 6338 : 0.0027716595213860273\n",
      "Training loss for batch 6339 : 0.09591332077980042\n",
      "Training loss for batch 6340 : 0.19366006553173065\n",
      "Training loss for batch 6341 : 0.017741195857524872\n",
      "Training loss for batch 6342 : 0.11414163559675217\n",
      "Training loss for batch 6343 : 0.3346576392650604\n",
      "Training loss for batch 6344 : 0.3136384189128876\n",
      "Training loss for batch 6345 : 0.013941952958703041\n",
      "Training loss for batch 6346 : 0.10453201085329056\n",
      "Training loss for batch 6347 : 0.08213568478822708\n",
      "Training loss for batch 6348 : 0.2889508605003357\n",
      "Training loss for batch 6349 : 0.1289457231760025\n",
      "Training loss for batch 6350 : 0.1532721370458603\n",
      "Training loss for batch 6351 : 0.18994592130184174\n",
      "Training loss for batch 6352 : 0.04258749634027481\n",
      "Training loss for batch 6353 : 0.028263812884688377\n",
      "Training loss for batch 6354 : 0.05490924417972565\n",
      "Training loss for batch 6355 : 0.03361665830016136\n",
      "Training loss for batch 6356 : 0.03303892910480499\n",
      "Training loss for batch 6357 : 0.09179376810789108\n",
      "Training loss for batch 6358 : 0.15078821778297424\n",
      "Training loss for batch 6359 : 0.15958662331104279\n",
      "Training loss for batch 6360 : 0.013583446852862835\n",
      "Training loss for batch 6361 : 0.06684650480747223\n",
      "Training loss for batch 6362 : 0.08088047802448273\n",
      "Training loss for batch 6363 : 0.05028027668595314\n",
      "Training loss for batch 6364 : 0.12180499732494354\n",
      "Training loss for batch 6365 : 0.17689117789268494\n",
      "Training loss for batch 6366 : 0.0860263854265213\n",
      "Training loss for batch 6367 : 0.07788585126399994\n",
      "Training loss for batch 6368 : 0.07770578563213348\n",
      "Training loss for batch 6369 : 0.057239726185798645\n",
      "Training loss for batch 6370 : 0.38532546162605286\n",
      "Training loss for batch 6371 : 0.22645699977874756\n",
      "Training loss for batch 6372 : 0.20910847187042236\n",
      "Training loss for batch 6373 : -1.299683117395034e-05\n",
      "Training loss for batch 6374 : 0.0807017982006073\n",
      "Training loss for batch 6375 : 0.05162884294986725\n",
      "Training loss for batch 6376 : 0.30430129170417786\n",
      "Training loss for batch 6377 : 0.12467976659536362\n",
      "Training loss for batch 6378 : 0.17029669880867004\n",
      "Training loss for batch 6379 : 0.015769345685839653\n",
      "Training loss for batch 6380 : 0.2346263974905014\n",
      "Training loss for batch 6381 : 0.21844197809696198\n",
      "Training loss for batch 6382 : 0.15329299867153168\n",
      "Training loss for batch 6383 : 0.09107047319412231\n",
      "Training loss for batch 6384 : 0.1288943886756897\n",
      "Training loss for batch 6385 : 0.2131556272506714\n",
      "Training loss for batch 6386 : 0.09160837531089783\n",
      "Training loss for batch 6387 : 0.16597649455070496\n",
      "Training loss for batch 6388 : 0.0712566152215004\n",
      "Training loss for batch 6389 : 0.056719355285167694\n",
      "Training loss for batch 6390 : 0.0020626287441700697\n",
      "Training loss for batch 6391 : 0.14390790462493896\n",
      "Training loss for batch 6392 : 0.16386036574840546\n",
      "Training loss for batch 6393 : 0.2887745201587677\n",
      "Training loss for batch 6394 : 0.13002988696098328\n",
      "Training loss for batch 6395 : 0.04769322648644447\n",
      "Training loss for batch 6396 : 0.313254177570343\n",
      "Training loss for batch 6397 : 0.23846149444580078\n",
      "Training loss for batch 6398 : 0.12578371167182922\n",
      "Training loss for batch 6399 : 0.09074543416500092\n",
      "Training loss for batch 6400 : 0.0010862152557820082\n",
      "Training loss for batch 6401 : 0.07365134358406067\n",
      "Training loss for batch 6402 : 0.5526736378669739\n",
      "Training loss for batch 6403 : 0.23340103030204773\n",
      "Training loss for batch 6404 : 0.0756411999464035\n",
      "Training loss for batch 6405 : 0.13921773433685303\n",
      "Training loss for batch 6406 : 0.22515495121479034\n",
      "Training loss for batch 6407 : 0.16637848317623138\n",
      "Training loss for batch 6408 : -0.0031552384607493877\n",
      "Training loss for batch 6409 : 0.03114178776741028\n",
      "Training loss for batch 6410 : 0.2211081087589264\n",
      "Training loss for batch 6411 : 0.038688503205776215\n",
      "Training loss for batch 6412 : 0.31155914068222046\n",
      "Training loss for batch 6413 : 0.1562364399433136\n",
      "Training loss for batch 6414 : 0.23935723304748535\n",
      "Training loss for batch 6415 : 0.0355379581451416\n",
      "Training loss for batch 6416 : 0.1776602566242218\n",
      "Training loss for batch 6417 : 0.17392891645431519\n",
      "Training loss for batch 6418 : 0.1706697791814804\n",
      "Training loss for batch 6419 : 0.06810365617275238\n",
      "Training loss for batch 6420 : 0.0905831903219223\n",
      "Training loss for batch 6421 : 0.18693429231643677\n",
      "Training loss for batch 6422 : 0.1713024377822876\n",
      "Training loss for batch 6423 : 0.11418788880109787\n",
      "Training loss for batch 6424 : 0.034837253391742706\n",
      "Training loss for batch 6425 : 0.08083242177963257\n",
      "Training loss for batch 6426 : 0.045995745807886124\n",
      "Training loss for batch 6427 : 0.10116904973983765\n",
      "Training loss for batch 6428 : 0.04567810148000717\n",
      "Training loss for batch 6429 : 0.006386111490428448\n",
      "Training loss for batch 6430 : 0.09483657032251358\n",
      "Training loss for batch 6431 : 0.15531577169895172\n",
      "Training loss for batch 6432 : 0.1565905511379242\n",
      "Training loss for batch 6433 : 0.04557903856039047\n",
      "Training loss for batch 6434 : 0.09663228690624237\n",
      "Training loss for batch 6435 : 0.1380307972431183\n",
      "Training loss for batch 6436 : 0.010453041642904282\n",
      "Training loss for batch 6437 : 0.14943857491016388\n",
      "Training loss for batch 6438 : 0.10620833933353424\n",
      "Training loss for batch 6439 : 0.14925359189510345\n",
      "Training loss for batch 6440 : 0.1576576679944992\n",
      "Training loss for batch 6441 : 0.1244480162858963\n",
      "Training loss for batch 6442 : 0.034374043345451355\n",
      "Training loss for batch 6443 : 0.04145277291536331\n",
      "Training loss for batch 6444 : 0.0025631606113165617\n",
      "Training loss for batch 6445 : 0.12601792812347412\n",
      "Training loss for batch 6446 : 0.0950031653046608\n",
      "Training loss for batch 6447 : 0.005114595405757427\n",
      "Training loss for batch 6448 : 0.08448775857686996\n",
      "Training loss for batch 6449 : 0.00033315527252852917\n",
      "Training loss for batch 6450 : 0.24773475527763367\n",
      "Training loss for batch 6451 : 0.11340548098087311\n",
      "Training loss for batch 6452 : 0.20121616125106812\n",
      "Training loss for batch 6453 : 0.04397878795862198\n",
      "Training loss for batch 6454 : 0.25570249557495117\n",
      "Training loss for batch 6455 : 0.10566601902246475\n",
      "Training loss for batch 6456 : 0.0424441434442997\n",
      "Training loss for batch 6457 : 0.0012479182332754135\n",
      "Training loss for batch 6458 : 0.043796148151159286\n",
      "Training loss for batch 6459 : 0.33055761456489563\n",
      "Training loss for batch 6460 : 0.14433544874191284\n",
      "Training loss for batch 6461 : 0.17430706322193146\n",
      "Training loss for batch 6462 : 0.09685613960027695\n",
      "Training loss for batch 6463 : 0.07845001667737961\n",
      "Training loss for batch 6464 : 0.3422462046146393\n",
      "Training loss for batch 6465 : 0.1161121353507042\n",
      "Training loss for batch 6466 : 0.6822109222412109\n",
      "Training loss for batch 6467 : 0.07777725160121918\n",
      "Training loss for batch 6468 : 0.0\n",
      "Training loss for batch 6469 : 0.15870913863182068\n",
      "Training loss for batch 6470 : 0.288669228553772\n",
      "Training loss for batch 6471 : 0.07808706909418106\n",
      "Training loss for batch 6472 : 0.05704720690846443\n",
      "Training loss for batch 6473 : 0.05098994821310043\n",
      "Training loss for batch 6474 : 0.05143081024289131\n",
      "Training loss for batch 6475 : 0.21356993913650513\n",
      "Training loss for batch 6476 : 0.2457704395055771\n",
      "Training loss for batch 6477 : 0.0411338210105896\n",
      "Training loss for batch 6478 : 0.18852102756500244\n",
      "Training loss for batch 6479 : 0.5482818484306335\n",
      "Training loss for batch 6480 : 0.06096433103084564\n",
      "Training loss for batch 6481 : 0.0793231725692749\n",
      "Training loss for batch 6482 : 0.07403668761253357\n",
      "Training loss for batch 6483 : 0.17139670252799988\n",
      "Training loss for batch 6484 : 0.05039461702108383\n",
      "Training loss for batch 6485 : 0.19526967406272888\n",
      "Training loss for batch 6486 : 0.08094117045402527\n",
      "Training loss for batch 6487 : 0.0\n",
      "Training loss for batch 6488 : 0.0\n",
      "Training loss for batch 6489 : 0.1050463616847992\n",
      "Training loss for batch 6490 : 0.2878458797931671\n",
      "Training loss for batch 6491 : 0.15691962838172913\n",
      "Training loss for batch 6492 : 0.21598592400550842\n",
      "Training loss for batch 6493 : 0.15028132498264313\n",
      "Training loss for batch 6494 : 0.09630973637104034\n",
      "Training loss for batch 6495 : 0.32696616649627686\n",
      "Training loss for batch 6496 : 0.14273729920387268\n",
      "Training loss for batch 6497 : 0.26345163583755493\n",
      "Training loss for batch 6498 : 0.12923407554626465\n",
      "Training loss for batch 6499 : 0.13133838772773743\n",
      "Training loss for batch 6500 : 0.023754794150590897\n",
      "Training loss for batch 6501 : 0.12186281383037567\n",
      "Training loss for batch 6502 : 0.0479087233543396\n",
      "Training loss for batch 6503 : 0.02312873676419258\n",
      "Training loss for batch 6504 : 0.0021348397713154554\n",
      "Training loss for batch 6505 : 0.27157118916511536\n",
      "Training loss for batch 6506 : 0.09058017283678055\n",
      "Training loss for batch 6507 : 0.035639580339193344\n",
      "Training loss for batch 6508 : 0.03610853850841522\n",
      "Training loss for batch 6509 : 0.08680421113967896\n",
      "Training loss for batch 6510 : 0.011480278335511684\n",
      "Training loss for batch 6511 : 0.06231008470058441\n",
      "Training loss for batch 6512 : 0.17643576860427856\n",
      "Training loss for batch 6513 : 0.04121081158518791\n",
      "Training loss for batch 6514 : 0.11801959574222565\n",
      "Training loss for batch 6515 : 0.02658240869641304\n",
      "Training loss for batch 6516 : 0.36374422907829285\n",
      "Training loss for batch 6517 : 0.11210229247808456\n",
      "Training loss for batch 6518 : 0.03578956052660942\n",
      "Training loss for batch 6519 : 0.15087459981441498\n",
      "Training loss for batch 6520 : 0.10976997762918472\n",
      "Training loss for batch 6521 : 0.2092932015657425\n",
      "Training loss for batch 6522 : 0.05833473056554794\n",
      "Training loss for batch 6523 : 0.084477499127388\n",
      "Training loss for batch 6524 : 0.23321326076984406\n",
      "Training loss for batch 6525 : 0.02592604234814644\n",
      "Training loss for batch 6526 : 0.11210335791110992\n",
      "Training loss for batch 6527 : 0.06000897288322449\n",
      "Training loss for batch 6528 : 0.031973905861377716\n",
      "Training loss for batch 6529 : 0.17748479545116425\n",
      "Training loss for batch 6530 : 0.15412038564682007\n",
      "Training loss for batch 6531 : 0.0596882626414299\n",
      "Training loss for batch 6532 : 0.21445287764072418\n",
      "Training loss for batch 6533 : 0.0955830067396164\n",
      "Training loss for batch 6534 : 0.003441592212766409\n",
      "Training loss for batch 6535 : 0.11881110817193985\n",
      "Training loss for batch 6536 : 0.23606476187705994\n",
      "Training loss for batch 6537 : 0.1486147940158844\n",
      "Training loss for batch 6538 : 0.1493755429983139\n",
      "Training loss for batch 6539 : 0.019372619688510895\n",
      "Training loss for batch 6540 : 0.1077389121055603\n",
      "Training loss for batch 6541 : 0.2817208170890808\n",
      "Training loss for batch 6542 : 0.28368398547172546\n",
      "Training loss for batch 6543 : 0.3743242919445038\n",
      "Training loss for batch 6544 : 0.03274722024798393\n",
      "Training loss for batch 6545 : 0.07719270884990692\n",
      "Training loss for batch 6546 : 0.05625611171126366\n",
      "Training loss for batch 6547 : 0.0037092436105012894\n",
      "Training loss for batch 6548 : 0.00034630298614501953\n",
      "Training loss for batch 6549 : 0.17817358672618866\n",
      "Training loss for batch 6550 : 0.12139314413070679\n",
      "Training loss for batch 6551 : 0.13379904627799988\n",
      "Training loss for batch 6552 : 0.05105024203658104\n",
      "Training loss for batch 6553 : 0.14045178890228271\n",
      "Training loss for batch 6554 : 0.06782522052526474\n",
      "Training loss for batch 6555 : 0.33365532755851746\n",
      "Training loss for batch 6556 : 0.19043660163879395\n",
      "Training loss for batch 6557 : 0.08505679666996002\n",
      "Training loss for batch 6558 : 0.15594008564949036\n",
      "Training loss for batch 6559 : 0.09862084686756134\n",
      "Training loss for batch 6560 : 0.12433303892612457\n",
      "Training loss for batch 6561 : 0.13476651906967163\n",
      "Training loss for batch 6562 : 0.19735494256019592\n",
      "Training loss for batch 6563 : 0.1899929642677307\n",
      "Training loss for batch 6564 : 0.18154436349868774\n",
      "Training loss for batch 6565 : 0.18823252618312836\n",
      "Training loss for batch 6566 : 0.12176279723644257\n",
      "Training loss for batch 6567 : 0.06720316410064697\n",
      "Training loss for batch 6568 : 0.24194982647895813\n",
      "Training loss for batch 6569 : 0.13703668117523193\n",
      "Training loss for batch 6570 : 0.044726770371198654\n",
      "Training loss for batch 6571 : 0.01949460431933403\n",
      "Training loss for batch 6572 : 0.20761802792549133\n",
      "Training loss for batch 6573 : 0.08574416488409042\n",
      "Training loss for batch 6574 : 0.1604112684726715\n",
      "Training loss for batch 6575 : 0.1277848780155182\n",
      "Training loss for batch 6576 : 0.07191471755504608\n",
      "Training loss for batch 6577 : 0.07655631750822067\n",
      "Training loss for batch 6578 : 0.0\n",
      "Training loss for batch 6579 : 0.0\n",
      "Training loss for batch 6580 : 0.204475536942482\n",
      "Training loss for batch 6581 : 0.1469641625881195\n",
      "Training loss for batch 6582 : 0.16274666786193848\n",
      "Training loss for batch 6583 : 0.2831084430217743\n",
      "Training loss for batch 6584 : 0.30866745114326477\n",
      "Training loss for batch 6585 : 0.1609382927417755\n",
      "Training loss for batch 6586 : 0.2542399764060974\n",
      "Training loss for batch 6587 : 0.08637689799070358\n",
      "Training loss for batch 6588 : 0.09648291766643524\n",
      "Training loss for batch 6589 : 0.4069714844226837\n",
      "Training loss for batch 6590 : 0.03370796889066696\n",
      "Training loss for batch 6591 : 0.1479896903038025\n",
      "Training loss for batch 6592 : 0.2691936194896698\n",
      "Training loss for batch 6593 : 0.13590717315673828\n",
      "Training loss for batch 6594 : 0.0511038713157177\n",
      "Training loss for batch 6595 : 0.035921599715948105\n",
      "Training loss for batch 6596 : 0.11174210160970688\n",
      "Training loss for batch 6597 : 0.09622490406036377\n",
      "Training loss for batch 6598 : 0.24256154894828796\n",
      "Training loss for batch 6599 : 0.15248167514801025\n",
      "Training loss for batch 6600 : 0.012899278663098812\n",
      "Training loss for batch 6601 : 0.29648488759994507\n",
      "Training loss for batch 6602 : 0.24931743741035461\n",
      "Training loss for batch 6603 : 0.041272006928920746\n",
      "Training loss for batch 6604 : 0.05752383917570114\n",
      "Training loss for batch 6605 : 0.10696499049663544\n",
      "Training loss for batch 6606 : 0.15603139996528625\n",
      "Training loss for batch 6607 : 0.10092345625162125\n",
      "Training loss for batch 6608 : 0.2835038900375366\n",
      "Training loss for batch 6609 : 0.048466600477695465\n",
      "Training loss for batch 6610 : 0.28587305545806885\n",
      "Training loss for batch 6611 : 0.24357041716575623\n",
      "Training loss for batch 6612 : 0.1296468824148178\n",
      "Training loss for batch 6613 : 0.18505370616912842\n",
      "Training loss for batch 6614 : 0.06098935753107071\n",
      "Training loss for batch 6615 : 0.11754509806632996\n",
      "Training loss for batch 6616 : 0.1488676369190216\n",
      "Training loss for batch 6617 : 0.1386842131614685\n",
      "Training loss for batch 6618 : 0.23848025500774384\n",
      "Training loss for batch 6619 : 0.05658433586359024\n",
      "Training loss for batch 6620 : 0.0\n",
      "Training loss for batch 6621 : 0.35959452390670776\n",
      "Training loss for batch 6622 : 0.05114062875509262\n",
      "Training loss for batch 6623 : 0.022159136831760406\n",
      "Training loss for batch 6624 : 0.1604430079460144\n",
      "Training loss for batch 6625 : 0.16688647866249084\n",
      "Training loss for batch 6626 : 0.1337142288684845\n",
      "Training loss for batch 6627 : 0.05975912883877754\n",
      "Training loss for batch 6628 : 0.12023686617612839\n",
      "Training loss for batch 6629 : 0.1598854809999466\n",
      "Training loss for batch 6630 : 0.04223678633570671\n",
      "Training loss for batch 6631 : 0.15624894201755524\n",
      "Training loss for batch 6632 : 0.13023057579994202\n",
      "Training loss for batch 6633 : 0.16932892799377441\n",
      "Training loss for batch 6634 : 0.14464284479618073\n",
      "Training loss for batch 6635 : 0.2031867951154709\n",
      "Training loss for batch 6636 : 0.12307682633399963\n",
      "Training loss for batch 6637 : 0.17510956525802612\n",
      "Training loss for batch 6638 : 0.13943496346473694\n",
      "Training loss for batch 6639 : 0.2532983720302582\n",
      "Training loss for batch 6640 : 0.07617123425006866\n",
      "Training loss for batch 6641 : 0.22439345717430115\n",
      "Training loss for batch 6642 : 0.24611437320709229\n",
      "Training loss for batch 6643 : 0.035039644688367844\n",
      "Training loss for batch 6644 : 0.10897601395845413\n",
      "Training loss for batch 6645 : 0.019904019311070442\n",
      "Training loss for batch 6646 : 0.28499624133110046\n",
      "Training loss for batch 6647 : 0.20487602055072784\n",
      "Training loss for batch 6648 : 0.052243974059820175\n",
      "Training loss for batch 6649 : 0.0637323334813118\n",
      "Training loss for batch 6650 : 0.04114049673080444\n",
      "Training loss for batch 6651 : 0.06706643104553223\n",
      "Training loss for batch 6652 : 0.27661508321762085\n",
      "Training loss for batch 6653 : 0.3849397897720337\n",
      "Training loss for batch 6654 : 0.14503099024295807\n",
      "Training loss for batch 6655 : 0.22110632061958313\n",
      "Training loss for batch 6656 : 0.4048748016357422\n",
      "Training loss for batch 6657 : 0.0505010187625885\n",
      "Training loss for batch 6658 : 0.22483858466148376\n",
      "Training loss for batch 6659 : 0.06299006193876266\n",
      "Training loss for batch 6660 : 0.17598941922187805\n",
      "Training loss for batch 6661 : 0.37657326459884644\n",
      "Training loss for batch 6662 : 0.33646756410598755\n",
      "Training loss for batch 6663 : 0.16167788207530975\n",
      "Training loss for batch 6664 : 0.03166073188185692\n",
      "Training loss for batch 6665 : 0.2162436842918396\n",
      "Training loss for batch 6666 : 0.17543159425258636\n",
      "Training loss for batch 6667 : 0.07063967734575272\n",
      "Training loss for batch 6668 : 0.1401253044605255\n",
      "Training loss for batch 6669 : 0.08221600949764252\n",
      "Training loss for batch 6670 : 0.12806738913059235\n",
      "Training loss for batch 6671 : 0.2088470160961151\n",
      "Training loss for batch 6672 : 0.2486078292131424\n",
      "Training loss for batch 6673 : 0.05122768133878708\n",
      "Training loss for batch 6674 : 0.2192503660917282\n",
      "Training loss for batch 6675 : 0.10041472315788269\n",
      "Training loss for batch 6676 : -0.0015575872967019677\n",
      "Training loss for batch 6677 : 0.3174731731414795\n",
      "Training loss for batch 6678 : 0.12682096660137177\n",
      "Training loss for batch 6679 : 0.23400938510894775\n",
      "Training loss for batch 6680 : 0.049367308616638184\n",
      "Training loss for batch 6681 : 0.19088687002658844\n",
      "Training loss for batch 6682 : 0.1966753751039505\n",
      "Training loss for batch 6683 : 0.2798093259334564\n",
      "Training loss for batch 6684 : 0.19961999356746674\n",
      "Training loss for batch 6685 : 0.1186249852180481\n",
      "Training loss for batch 6686 : 0.14988426864147186\n",
      "Training loss for batch 6687 : 0.29608675837516785\n",
      "Training loss for batch 6688 : 0.25607630610466003\n",
      "Training loss for batch 6689 : 0.037960078567266464\n",
      "Training loss for batch 6690 : 0.14144015312194824\n",
      "Training loss for batch 6691 : 0.08081924170255661\n",
      "Training loss for batch 6692 : 0.19469702243804932\n",
      "Training loss for batch 6693 : 0.09205887466669083\n",
      "Training loss for batch 6694 : 0.03993167728185654\n",
      "Training loss for batch 6695 : 0.09395122528076172\n",
      "Training loss for batch 6696 : 0.11566080152988434\n",
      "Training loss for batch 6697 : 0.14713799953460693\n",
      "Training loss for batch 6698 : 0.08469812572002411\n",
      "Training loss for batch 6699 : 0.0669407844543457\n",
      "Training loss for batch 6700 : 0.05489111319184303\n",
      "Training loss for batch 6701 : 0.012751393020153046\n",
      "Training loss for batch 6702 : 0.02501964382827282\n",
      "Training loss for batch 6703 : 0.11796557903289795\n",
      "Training loss for batch 6704 : 0.043248601257801056\n",
      "Training loss for batch 6705 : 0.06568684428930283\n",
      "Training loss for batch 6706 : 0.21709734201431274\n",
      "Training loss for batch 6707 : 0.009616739116609097\n",
      "Training loss for batch 6708 : 0.17347729206085205\n",
      "Training loss for batch 6709 : 0.25277650356292725\n",
      "Training loss for batch 6710 : 0.0787588581442833\n",
      "Training loss for batch 6711 : 0.09022776782512665\n",
      "Training loss for batch 6712 : 0.10738812386989594\n",
      "Training loss for batch 6713 : 0.019825540482997894\n",
      "Training loss for batch 6714 : 0.0430125892162323\n",
      "Training loss for batch 6715 : 0.17015889286994934\n",
      "Training loss for batch 6716 : 0.5507321953773499\n",
      "Training loss for batch 6717 : 0.05450824275612831\n",
      "Training loss for batch 6718 : 0.1638936996459961\n",
      "Training loss for batch 6719 : 0.4485229253768921\n",
      "Training loss for batch 6720 : 0.08911853283643723\n",
      "Training loss for batch 6721 : 0.29205504059791565\n",
      "Training loss for batch 6722 : 0.11373043060302734\n",
      "Training loss for batch 6723 : 0.3167162239551544\n",
      "Training loss for batch 6724 : 0.01703791879117489\n",
      "Training loss for batch 6725 : 0.22234252095222473\n",
      "Training loss for batch 6726 : 0.14508217573165894\n",
      "Training loss for batch 6727 : 0.11609362065792084\n",
      "Training loss for batch 6728 : 0.07859198749065399\n",
      "Training loss for batch 6729 : 0.01447613537311554\n",
      "Training loss for batch 6730 : 0.12803050875663757\n",
      "Training loss for batch 6731 : 0.12200009077787399\n",
      "Training loss for batch 6732 : 0.12495468556880951\n",
      "Training loss for batch 6733 : 0.18190087378025055\n",
      "Training loss for batch 6734 : 0.044833939522504807\n",
      "Training loss for batch 6735 : 0.1201850101351738\n",
      "Training loss for batch 6736 : 0.24877122044563293\n",
      "Training loss for batch 6737 : 0.2079523801803589\n",
      "Training loss for batch 6738 : 0.07987042516469955\n",
      "Training loss for batch 6739 : 0.14208954572677612\n",
      "Training loss for batch 6740 : 0.0335531160235405\n",
      "Training loss for batch 6741 : 0.2290862500667572\n",
      "Training loss for batch 6742 : -0.0011523963185027242\n",
      "Training loss for batch 6743 : 0.10603129863739014\n",
      "Training loss for batch 6744 : 0.1294202208518982\n",
      "Training loss for batch 6745 : 0.030922647565603256\n",
      "Training loss for batch 6746 : 0.09373994171619415\n",
      "Training loss for batch 6747 : 0.09093037247657776\n",
      "Training loss for batch 6748 : 0.05331411212682724\n",
      "Training loss for batch 6749 : 0.23276640474796295\n",
      "Training loss for batch 6750 : 0.26836249232292175\n",
      "Training loss for batch 6751 : 0.025045771151781082\n",
      "Training loss for batch 6752 : 0.1357620358467102\n",
      "Training loss for batch 6753 : 0.218291237950325\n",
      "Training loss for batch 6754 : 0.10119335353374481\n",
      "Training loss for batch 6755 : 0.23981298506259918\n",
      "Training loss for batch 6756 : 0.15445773303508759\n",
      "Training loss for batch 6757 : 0.1302083134651184\n",
      "Training loss for batch 6758 : 0.008565614931285381\n",
      "Training loss for batch 6759 : 0.04156074300408363\n",
      "Training loss for batch 6760 : 0.1007450520992279\n",
      "Training loss for batch 6761 : 0.061795126646757126\n",
      "Training loss for batch 6762 : 0.0899554193019867\n",
      "Training loss for batch 6763 : 0.17624329030513763\n",
      "Training loss for batch 6764 : 0.23253165185451508\n",
      "Training loss for batch 6765 : 0.30407989025115967\n",
      "Training loss for batch 6766 : 0.0758143961429596\n",
      "Training loss for batch 6767 : 0.05217701196670532\n",
      "Training loss for batch 6768 : 0.1183396577835083\n",
      "Training loss for batch 6769 : 0.04406702145934105\n",
      "Training loss for batch 6770 : 0.03462516516447067\n",
      "Training loss for batch 6771 : 0.037718355655670166\n",
      "Training loss for batch 6772 : 0.12706011533737183\n",
      "Training loss for batch 6773 : 0.019990529865026474\n",
      "Training loss for batch 6774 : 0.19504177570343018\n",
      "Training loss for batch 6775 : 0.25761955976486206\n",
      "Training loss for batch 6776 : 0.10954052209854126\n",
      "Training loss for batch 6777 : 0.0732264518737793\n",
      "Training loss for batch 6778 : 0.022521913051605225\n",
      "Training loss for batch 6779 : 0.11333017796278\n",
      "Training loss for batch 6780 : 0.18003436923027039\n",
      "Training loss for batch 6781 : 0.18088653683662415\n",
      "Training loss for batch 6782 : 0.15138550102710724\n",
      "Training loss for batch 6783 : 0.21385282278060913\n",
      "Training loss for batch 6784 : 0.04329899698495865\n",
      "Training loss for batch 6785 : 0.032053954899311066\n",
      "Training loss for batch 6786 : 0.015383989550173283\n",
      "Training loss for batch 6787 : 0.007776772137731314\n",
      "Training loss for batch 6788 : 0.13916562497615814\n",
      "Training loss for batch 6789 : 0.015391470864415169\n",
      "Training loss for batch 6790 : 0.009896814823150635\n",
      "Training loss for batch 6791 : 0.012746470049023628\n",
      "Training loss for batch 6792 : 0.19929583370685577\n",
      "Training loss for batch 6793 : 0.09306864440441132\n",
      "Training loss for batch 6794 : 0.05471976846456528\n",
      "Training loss for batch 6795 : 0.06557353585958481\n",
      "Training loss for batch 6796 : 0.033037714660167694\n",
      "Training loss for batch 6797 : 0.03744180127978325\n",
      "Training loss for batch 6798 : 0.19056613743305206\n",
      "Training loss for batch 6799 : 0.1671196073293686\n",
      "Training loss for batch 6800 : 0.05435998737812042\n",
      "Training loss for batch 6801 : 0.1395832598209381\n",
      "Training loss for batch 6802 : 0.08125676214694977\n",
      "Training loss for batch 6803 : 0.04136287793517113\n",
      "Training loss for batch 6804 : 0.11879068613052368\n",
      "Training loss for batch 6805 : 0.06566682457923889\n",
      "Training loss for batch 6806 : 0.17967915534973145\n",
      "Training loss for batch 6807 : 0.230886310338974\n",
      "Training loss for batch 6808 : 0.023986760526895523\n",
      "Training loss for batch 6809 : 0.011483079753816128\n",
      "Training loss for batch 6810 : 0.06834392994642258\n",
      "Training loss for batch 6811 : 0.0\n",
      "Training loss for batch 6812 : 0.2776136100292206\n",
      "Training loss for batch 6813 : 0.1941726803779602\n",
      "Training loss for batch 6814 : 0.5190138816833496\n",
      "Training loss for batch 6815 : 0.11770343780517578\n",
      "Training loss for batch 6816 : 0.062338750809431076\n",
      "Training loss for batch 6817 : 0.3548150658607483\n",
      "Training loss for batch 6818 : 0.2549397349357605\n",
      "Training loss for batch 6819 : 0.0\n",
      "Training loss for batch 6820 : 0.0428326353430748\n",
      "Training loss for batch 6821 : 0.22388523817062378\n",
      "Training loss for batch 6822 : 0.03265295550227165\n",
      "Training loss for batch 6823 : 0.07300598174333572\n",
      "Training loss for batch 6824 : 0.024810120463371277\n",
      "Training loss for batch 6825 : 0.06423009932041168\n",
      "Training loss for batch 6826 : 0.07444349676370621\n",
      "Training loss for batch 6827 : 0.016736511141061783\n",
      "Training loss for batch 6828 : 0.013570038601756096\n",
      "Training loss for batch 6829 : 0.04535491392016411\n",
      "Training loss for batch 6830 : 0.05185472220182419\n",
      "Training loss for batch 6831 : 0.17147637903690338\n",
      "Training loss for batch 6832 : 0.10770251601934433\n",
      "Training loss for batch 6833 : 0.11291129887104034\n",
      "Training loss for batch 6834 : 0.1996229588985443\n",
      "Training loss for batch 6835 : 0.14822235703468323\n",
      "Training loss for batch 6836 : 0.0325562059879303\n",
      "Training loss for batch 6837 : 0.04932093992829323\n",
      "Training loss for batch 6838 : 0.1617874652147293\n",
      "Training loss for batch 6839 : 0.13391081988811493\n",
      "Training loss for batch 6840 : 0.09056302160024643\n",
      "Training loss for batch 6841 : 0.12550410628318787\n",
      "Training loss for batch 6842 : 0.11568419635295868\n",
      "Training loss for batch 6843 : 0.2213127613067627\n",
      "Training loss for batch 6844 : 0.030790921300649643\n",
      "Training loss for batch 6845 : 0.07010311633348465\n",
      "Training loss for batch 6846 : 0.11286365240812302\n",
      "Training loss for batch 6847 : 0.009828448295593262\n",
      "Training loss for batch 6848 : 0.04299114644527435\n",
      "Training loss for batch 6849 : 0.014074793085455894\n",
      "Training loss for batch 6850 : 0.1144576221704483\n",
      "Training loss for batch 6851 : 0.028033984825015068\n",
      "Training loss for batch 6852 : 0.194639652967453\n",
      "Training loss for batch 6853 : 0.10452289879322052\n",
      "Training loss for batch 6854 : 0.003929068800061941\n",
      "Training loss for batch 6855 : 0.1303928941488266\n",
      "Training loss for batch 6856 : 0.05492335185408592\n",
      "Training loss for batch 6857 : 0.1401422768831253\n",
      "Training loss for batch 6858 : 0.01520185824483633\n",
      "Training loss for batch 6859 : 0.10597776621580124\n",
      "Training loss for batch 6860 : 0.09968222677707672\n",
      "Training loss for batch 6861 : 0.3618502914905548\n",
      "Training loss for batch 6862 : 0.019055616110563278\n",
      "Training loss for batch 6863 : 0.1581066995859146\n",
      "Training loss for batch 6864 : 0.17050224542617798\n",
      "Training loss for batch 6865 : 0.0\n",
      "Training loss for batch 6866 : 0.2138824164867401\n",
      "Training loss for batch 6867 : 0.05376506224274635\n",
      "Training loss for batch 6868 : 0.4579271376132965\n",
      "Training loss for batch 6869 : 0.1370256394147873\n",
      "Training loss for batch 6870 : 0.37195971608161926\n",
      "Training loss for batch 6871 : 0.21711686253547668\n",
      "Training loss for batch 6872 : 0.009594273753464222\n",
      "Training loss for batch 6873 : 0.07677366584539413\n",
      "Training loss for batch 6874 : 0.16842146217823029\n",
      "Training loss for batch 6875 : 0.13343545794487\n",
      "Training loss for batch 6876 : 0.21954506635665894\n",
      "Training loss for batch 6877 : 0.23921063542366028\n",
      "Training loss for batch 6878 : 0.1273099035024643\n",
      "Training loss for batch 6879 : 0.1724475771188736\n",
      "Training loss for batch 6880 : 0.3386685252189636\n",
      "Training loss for batch 6881 : 0.07015016674995422\n",
      "Training loss for batch 6882 : 0.15790525078773499\n",
      "Training loss for batch 6883 : 0.1149262934923172\n",
      "Training loss for batch 6884 : 0.17327210307121277\n",
      "Training loss for batch 6885 : 0.23116764426231384\n",
      "Training loss for batch 6886 : 0.14737564325332642\n",
      "Training loss for batch 6887 : 0.33871132135391235\n",
      "Training loss for batch 6888 : 0.02556176297366619\n",
      "Training loss for batch 6889 : 0.010918106883764267\n",
      "Training loss for batch 6890 : 0.06902557611465454\n",
      "Training loss for batch 6891 : 0.09485535323619843\n",
      "Training loss for batch 6892 : 0.03527582064270973\n",
      "Training loss for batch 6893 : 0.18209904432296753\n",
      "Training loss for batch 6894 : 0.031003104522824287\n",
      "Training loss for batch 6895 : 0.10959132015705109\n",
      "Training loss for batch 6896 : 0.005314371082931757\n",
      "Training loss for batch 6897 : 0.29302388429641724\n",
      "Training loss for batch 6898 : 0.1502620428800583\n",
      "Training loss for batch 6899 : 0.2980046272277832\n",
      "Training loss for batch 6900 : 0.2831116020679474\n",
      "Training loss for batch 6901 : 0.30307990312576294\n",
      "Training loss for batch 6902 : 0.12368914484977722\n",
      "Training loss for batch 6903 : 0.09350689500570297\n",
      "Training loss for batch 6904 : 0.13698256015777588\n",
      "Training loss for batch 6905 : 0.04386311024427414\n",
      "Training loss for batch 6906 : 0.13106781244277954\n",
      "Training loss for batch 6907 : -0.0006454149261116982\n",
      "Training loss for batch 6908 : 0.14588862657546997\n",
      "Training loss for batch 6909 : 0.28623881936073303\n",
      "Training loss for batch 6910 : 0.2763277292251587\n",
      "Training loss for batch 6911 : -0.001948355115018785\n",
      "Training loss for batch 6912 : 0.04844069108366966\n",
      "Training loss for batch 6913 : 0.05609911307692528\n",
      "Training loss for batch 6914 : 0.08222432434558868\n",
      "Training loss for batch 6915 : 0.06634663045406342\n",
      "Training loss for batch 6916 : 0.23137830197811127\n",
      "Training loss for batch 6917 : 0.0934428796172142\n",
      "Training loss for batch 6918 : 0.02699311263859272\n",
      "Training loss for batch 6919 : 0.029092304408550262\n",
      "Training loss for batch 6920 : 0.15237702429294586\n",
      "Training loss for batch 6921 : 0.0670454353094101\n",
      "Training loss for batch 6922 : 0.0949738621711731\n",
      "Training loss for batch 6923 : 0.08503615111112595\n",
      "Training loss for batch 6924 : 0.10855509340763092\n",
      "Training loss for batch 6925 : 0.2036009430885315\n",
      "Training loss for batch 6926 : 0.18935343623161316\n",
      "Training loss for batch 6927 : 0.3786735236644745\n",
      "Training loss for batch 6928 : 0.15340694785118103\n",
      "Training loss for batch 6929 : 0.17773085832595825\n",
      "Training loss for batch 6930 : 0.1088389903306961\n",
      "Training loss for batch 6931 : 0.1968720406293869\n",
      "Training loss for batch 6932 : 0.21597322821617126\n",
      "Training loss for batch 6933 : 0.3303568363189697\n",
      "Training loss for batch 6934 : 0.21366742253303528\n",
      "Training loss for batch 6935 : 0.13347558677196503\n",
      "Training loss for batch 6936 : 0.08242667466402054\n",
      "Training loss for batch 6937 : 0.19014675915241241\n",
      "Training loss for batch 6938 : 0.1877967268228531\n",
      "Training loss for batch 6939 : 0.08924954384565353\n",
      "Training loss for batch 6940 : 0.006650654599070549\n",
      "Training loss for batch 6941 : 0.010475954040884972\n",
      "Training loss for batch 6942 : 0.05531538650393486\n",
      "Training loss for batch 6943 : 0.0860130712389946\n",
      "Training loss for batch 6944 : 0.10145141184329987\n",
      "Training loss for batch 6945 : 0.45516034960746765\n",
      "Training loss for batch 6946 : 0.06532993912696838\n",
      "Training loss for batch 6947 : 0.20353630185127258\n",
      "Training loss for batch 6948 : 0.06409987807273865\n",
      "Training loss for batch 6949 : 0.0575973317027092\n",
      "Training loss for batch 6950 : 0.08039510250091553\n",
      "Training loss for batch 6951 : 0.19703730940818787\n",
      "Training loss for batch 6952 : 0.1863408386707306\n",
      "Training loss for batch 6953 : 0.24923957884311676\n",
      "Training loss for batch 6954 : 0.03874902054667473\n",
      "Training loss for batch 6955 : 0.08966049551963806\n",
      "Training loss for batch 6956 : 0.07455647736787796\n",
      "Training loss for batch 6957 : 0.3137279748916626\n",
      "Training loss for batch 6958 : 0.12604865431785583\n",
      "Training loss for batch 6959 : 0.12806953489780426\n",
      "Training loss for batch 6960 : 0.07888610661029816\n",
      "Training loss for batch 6961 : 0.18086251616477966\n",
      "Training loss for batch 6962 : 0.07020452618598938\n",
      "Training loss for batch 6963 : 0.029733512550592422\n",
      "Training loss for batch 6964 : 0.05253111198544502\n",
      "Training loss for batch 6965 : 0.2721731960773468\n",
      "Training loss for batch 6966 : 0.04236127436161041\n",
      "Training loss for batch 6967 : 0.10956965386867523\n",
      "Training loss for batch 6968 : 0.05008067935705185\n",
      "Training loss for batch 6969 : 0.12651467323303223\n",
      "Training loss for batch 6970 : 0.06397845596075058\n",
      "Training loss for batch 6971 : 0.3323493003845215\n",
      "Training loss for batch 6972 : 0.0644599199295044\n",
      "Training loss for batch 6973 : 0.13114692270755768\n",
      "Training loss for batch 6974 : 0.03094916045665741\n",
      "Training loss for batch 6975 : 0.1485375463962555\n",
      "Training loss for batch 6976 : 0.33074358105659485\n",
      "Training loss for batch 6977 : 0.07653214037418365\n",
      "Training loss for batch 6978 : 0.3400835394859314\n",
      "Training loss for batch 6979 : 0.05568985268473625\n",
      "Training loss for batch 6980 : 0.1889047920703888\n",
      "Training loss for batch 6981 : 0.09707795828580856\n",
      "Training loss for batch 6982 : 0.3252098262310028\n",
      "Training loss for batch 6983 : 0.33729302883148193\n",
      "Training loss for batch 6984 : 0.2608282268047333\n",
      "Training loss for batch 6985 : 0.04239761829376221\n",
      "Training loss for batch 6986 : 0.07569503039121628\n",
      "Training loss for batch 6987 : 0.0684407651424408\n",
      "Training loss for batch 6988 : 0.02505415305495262\n",
      "Training loss for batch 6989 : 0.09903710335493088\n",
      "Training loss for batch 6990 : 0.06081546097993851\n",
      "Training loss for batch 6991 : 0.21688024699687958\n",
      "Training loss for batch 6992 : 0.22928473353385925\n",
      "Training loss for batch 6993 : 0.13959138095378876\n",
      "Training loss for batch 6994 : 0.07017993181943893\n",
      "Training loss for batch 6995 : 0.1597275584936142\n",
      "Training loss for batch 6996 : 0.22352778911590576\n",
      "Training loss for batch 6997 : 0.1349431872367859\n",
      "Training loss for batch 6998 : 0.1543363630771637\n",
      "Training loss for batch 6999 : 0.020514672622084618\n",
      "Training loss for batch 7000 : -0.0014629403594881296\n",
      "Training loss for batch 7001 : 0.18351291120052338\n",
      "Training loss for batch 7002 : 0.09770575165748596\n",
      "Training loss for batch 7003 : 0.10032495856285095\n",
      "Training loss for batch 7004 : 0.7427799105644226\n",
      "Training loss for batch 7005 : 0.026867995038628578\n",
      "Training loss for batch 7006 : 0.05073337256908417\n",
      "Training loss for batch 7007 : 0.0\n",
      "Training loss for batch 7008 : 0.2607060670852661\n",
      "Training loss for batch 7009 : 0.27204713225364685\n",
      "Training loss for batch 7010 : 0.1819382607936859\n",
      "Training loss for batch 7011 : 0.30197784304618835\n",
      "Training loss for batch 7012 : 0.018008043989539146\n",
      "Training loss for batch 7013 : 0.2046203911304474\n",
      "Training loss for batch 7014 : 0.18917274475097656\n",
      "Training loss for batch 7015 : 0.0\n",
      "Training loss for batch 7016 : 0.018050892278552055\n",
      "Training loss for batch 7017 : 0.009141807444393635\n",
      "Training loss for batch 7018 : -0.002900501247495413\n",
      "Training loss for batch 7019 : 0.10896678268909454\n",
      "Training loss for batch 7020 : 0.03893997147679329\n",
      "Training loss for batch 7021 : 0.04046166315674782\n",
      "Training loss for batch 7022 : 0.0423874706029892\n",
      "Training loss for batch 7023 : 0.259021520614624\n",
      "Training loss for batch 7024 : 0.20827405154705048\n",
      "Training loss for batch 7025 : 0.11872436851263046\n",
      "Training loss for batch 7026 : 0.2668463885784149\n",
      "Training loss for batch 7027 : 0.04864053428173065\n",
      "Training loss for batch 7028 : 0.04043334722518921\n",
      "Training loss for batch 7029 : 0.07013269513845444\n",
      "Training loss for batch 7030 : 0.11702802777290344\n",
      "Training loss for batch 7031 : 0.07092025876045227\n",
      "Training loss for batch 7032 : 0.16326873004436493\n",
      "Training loss for batch 7033 : 0.05863768234848976\n",
      "Training loss for batch 7034 : 0.027333375066518784\n",
      "Training loss for batch 7035 : 0.3019185960292816\n",
      "Training loss for batch 7036 : 0.2422497719526291\n",
      "Training loss for batch 7037 : 0.07982224225997925\n",
      "Training loss for batch 7038 : 0.10138709843158722\n",
      "Training loss for batch 7039 : 0.2395746111869812\n",
      "Training loss for batch 7040 : 0.07008214294910431\n",
      "Training loss for batch 7041 : 0.08663323521614075\n",
      "Training loss for batch 7042 : 0.1714877188205719\n",
      "Training loss for batch 7043 : 0.18590722978115082\n",
      "Training loss for batch 7044 : 0.2811516523361206\n",
      "Training loss for batch 7045 : 0.053366176784038544\n",
      "Training loss for batch 7046 : 0.21537137031555176\n",
      "Training loss for batch 7047 : 0.02024766243994236\n",
      "Training loss for batch 7048 : 0.06461330503225327\n",
      "Training loss for batch 7049 : 0.031817611306905746\n",
      "Training loss for batch 7050 : 0.01620260439813137\n",
      "Training loss for batch 7051 : 0.10212092846632004\n",
      "Training loss for batch 7052 : 0.13087843358516693\n",
      "Training loss for batch 7053 : 0.006304196082055569\n",
      "Training loss for batch 7054 : 0.25717079639434814\n",
      "Training loss for batch 7055 : 0.11303263902664185\n",
      "Training loss for batch 7056 : 0.23399408161640167\n",
      "Training loss for batch 7057 : 0.45343196392059326\n",
      "Training loss for batch 7058 : 0.1198296919465065\n",
      "Training loss for batch 7059 : 0.044779106974601746\n",
      "Training loss for batch 7060 : -8.749868720769882e-05\n",
      "Training loss for batch 7061 : 0.06822866201400757\n",
      "Training loss for batch 7062 : 0.14013710618019104\n",
      "Training loss for batch 7063 : 0.039614491164684296\n",
      "Training loss for batch 7064 : 0.23030710220336914\n",
      "Training loss for batch 7065 : 0.16590073704719543\n",
      "Training loss for batch 7066 : 0.05507305637001991\n",
      "Training loss for batch 7067 : 0.12315601110458374\n",
      "Training loss for batch 7068 : 0.09870456159114838\n",
      "Training loss for batch 7069 : 0.16287465393543243\n",
      "Training loss for batch 7070 : 0.2233189195394516\n",
      "Training loss for batch 7071 : 0.05204877257347107\n",
      "Training loss for batch 7072 : 0.2697546184062958\n",
      "Training loss for batch 7073 : 0.18368585407733917\n",
      "Training loss for batch 7074 : 0.02047947235405445\n",
      "Training loss for batch 7075 : 0.0816565603017807\n",
      "Training loss for batch 7076 : 0.013293285854160786\n",
      "Training loss for batch 7077 : 0.20138181746006012\n",
      "Training loss for batch 7078 : 0.06461642682552338\n",
      "Training loss for batch 7079 : 0.24638625979423523\n",
      "Training loss for batch 7080 : 0.09696758538484573\n",
      "Training loss for batch 7081 : 0.20436851680278778\n",
      "Training loss for batch 7082 : 0.1451060026884079\n",
      "Training loss for batch 7083 : 0.10512809455394745\n",
      "Training loss for batch 7084 : 0.28970593214035034\n",
      "Training loss for batch 7085 : 0.06429043412208557\n",
      "Training loss for batch 7086 : 0.24615392088890076\n",
      "Training loss for batch 7087 : 0.0958472266793251\n",
      "Training loss for batch 7088 : 0.5259485244750977\n",
      "Training loss for batch 7089 : 0.166290283203125\n",
      "Training loss for batch 7090 : 0.12069256603717804\n",
      "Training loss for batch 7091 : 0.08865024149417877\n",
      "Training loss for batch 7092 : 0.10374107956886292\n",
      "Training loss for batch 7093 : 0.36135363578796387\n",
      "Training loss for batch 7094 : 0.010007737204432487\n",
      "Training loss for batch 7095 : 0.0312843956053257\n",
      "Training loss for batch 7096 : 0.232285737991333\n",
      "Training loss for batch 7097 : 0.27394533157348633\n",
      "Training loss for batch 7098 : 0.4499351680278778\n",
      "Training loss for batch 7099 : 0.01967610977590084\n",
      "Training loss for batch 7100 : 0.24896270036697388\n",
      "Training loss for batch 7101 : 0.13037514686584473\n",
      "Training loss for batch 7102 : 0.03790194168686867\n",
      "Training loss for batch 7103 : 0.11098192632198334\n",
      "Training loss for batch 7104 : 0.22814175486564636\n",
      "Training loss for batch 7105 : 0.14542311429977417\n",
      "Training loss for batch 7106 : 0.05995335057377815\n",
      "Training loss for batch 7107 : 0.06572141498327255\n",
      "Training loss for batch 7108 : 0.13930292427539825\n",
      "Training loss for batch 7109 : 0.050513334572315216\n",
      "Training loss for batch 7110 : 0.06082684546709061\n",
      "Training loss for batch 7111 : 0.061908286064863205\n",
      "Training loss for batch 7112 : 0.04068372771143913\n",
      "Training loss for batch 7113 : 0.06440761685371399\n",
      "Training loss for batch 7114 : 0.19321498274803162\n",
      "Training loss for batch 7115 : 0.0745929554104805\n",
      "Training loss for batch 7116 : 0.2255457639694214\n",
      "Training loss for batch 7117 : 0.21940040588378906\n",
      "Training loss for batch 7118 : 0.044179726392030716\n",
      "Training loss for batch 7119 : 0.12426580488681793\n",
      "Training loss for batch 7120 : 0.18211692571640015\n",
      "Training loss for batch 7121 : 0.21417300403118134\n",
      "Training loss for batch 7122 : 0.3525431156158447\n",
      "Training loss for batch 7123 : 0.1492481827735901\n",
      "Training loss for batch 7124 : 0.10137484222650528\n",
      "Training loss for batch 7125 : 0.2057127058506012\n",
      "Training loss for batch 7126 : 0.11127212643623352\n",
      "Training loss for batch 7127 : 0.31366485357284546\n",
      "Training loss for batch 7128 : 0.15909720957279205\n",
      "Training loss for batch 7129 : 0.06395088136196136\n",
      "Training loss for batch 7130 : 0.06831352412700653\n",
      "Training loss for batch 7131 : 0.018192479386925697\n",
      "Training loss for batch 7132 : 0.193390890955925\n",
      "Training loss for batch 7133 : 0.23031771183013916\n",
      "Training loss for batch 7134 : 0.05512497201561928\n",
      "Training loss for batch 7135 : 0.12271498888731003\n",
      "Training loss for batch 7136 : 0.12340989708900452\n",
      "Training loss for batch 7137 : 0.045020267367362976\n",
      "Training loss for batch 7138 : 0.308499276638031\n",
      "Training loss for batch 7139 : 0.08013573288917542\n",
      "Training loss for batch 7140 : 0.04650267958641052\n",
      "Training loss for batch 7141 : 0.2600761651992798\n",
      "Training loss for batch 7142 : 0.34613749384880066\n",
      "Training loss for batch 7143 : 0.11935699731111526\n",
      "Training loss for batch 7144 : 0.06552132964134216\n",
      "Training loss for batch 7145 : 0.35340723395347595\n",
      "Training loss for batch 7146 : 0.01791011169552803\n",
      "Training loss for batch 7147 : 0.10789540410041809\n",
      "Training loss for batch 7148 : 0.18926669657230377\n",
      "Training loss for batch 7149 : 0.17780518531799316\n",
      "Training loss for batch 7150 : 0.1530412882566452\n",
      "Training loss for batch 7151 : 0.022628093138337135\n",
      "Training loss for batch 7152 : 0.2095355987548828\n",
      "Training loss for batch 7153 : 0.021252792328596115\n",
      "Training loss for batch 7154 : 0.09258332848548889\n",
      "Training loss for batch 7155 : 0.04093912988901138\n",
      "Training loss for batch 7156 : 0.07093895226716995\n",
      "Training loss for batch 7157 : 0.029002804309129715\n",
      "Training loss for batch 7158 : 0.052141908556222916\n",
      "Training loss for batch 7159 : 0.1441039890050888\n",
      "Training loss for batch 7160 : 0.040299709886312485\n",
      "Training loss for batch 7161 : 0.04002215713262558\n",
      "Training loss for batch 7162 : 0.341148316860199\n",
      "Training loss for batch 7163 : 0.004897852893918753\n",
      "Training loss for batch 7164 : 0.1663157194852829\n",
      "Training loss for batch 7165 : 0.02826918289065361\n",
      "Training loss for batch 7166 : 0.13862475752830505\n",
      "Training loss for batch 7167 : 0.12152284383773804\n",
      "Training loss for batch 7168 : 0.024026503786444664\n",
      "Training loss for batch 7169 : 0.0760093405842781\n",
      "Training loss for batch 7170 : 0.23660551011562347\n",
      "Training loss for batch 7171 : 0.1036585122346878\n",
      "Training loss for batch 7172 : 0.15854567289352417\n",
      "Training loss for batch 7173 : 0.26355934143066406\n",
      "Training loss for batch 7174 : 0.0753173679113388\n",
      "Training loss for batch 7175 : 0.25764259696006775\n",
      "Training loss for batch 7176 : 0.15711085498332977\n",
      "Training loss for batch 7177 : 0.0498301200568676\n",
      "Training loss for batch 7178 : 0.08547284454107285\n",
      "Training loss for batch 7179 : 0.048945795744657516\n",
      "Training loss for batch 7180 : 0.027564633637666702\n",
      "Training loss for batch 7181 : 0.23949459195137024\n",
      "Training loss for batch 7182 : 0.1312144547700882\n",
      "Training loss for batch 7183 : 0.22362709045410156\n",
      "Training loss for batch 7184 : 0.31306788325309753\n",
      "Training loss for batch 7185 : 0.1658792644739151\n",
      "Training loss for batch 7186 : 0.08973217755556107\n",
      "Training loss for batch 7187 : 0.21688535809516907\n",
      "Training loss for batch 7188 : 0.181491419672966\n",
      "Training loss for batch 7189 : 0.09270286560058594\n",
      "Training loss for batch 7190 : 0.10400551557540894\n",
      "Training loss for batch 7191 : 0.13666923344135284\n",
      "Training loss for batch 7192 : 0.06684453040361404\n",
      "Training loss for batch 7193 : 0.2775270342826843\n",
      "Training loss for batch 7194 : 0.26994073390960693\n",
      "Training loss for batch 7195 : 0.16565419733524323\n",
      "Training loss for batch 7196 : 0.177982360124588\n",
      "Training loss for batch 7197 : 0.18701766431331635\n",
      "Training loss for batch 7198 : 0.2553648352622986\n",
      "Training loss for batch 7199 : 0.17579972743988037\n",
      "Training loss for batch 7200 : 0.11859533190727234\n",
      "Training loss for batch 7201 : 0.04202320799231529\n",
      "Training loss for batch 7202 : 0.15643665194511414\n",
      "Training loss for batch 7203 : 0.000722393102478236\n",
      "Training loss for batch 7204 : 0.012347429990768433\n",
      "Training loss for batch 7205 : 0.21069404482841492\n",
      "Training loss for batch 7206 : 0.14688868820667267\n",
      "Training loss for batch 7207 : 0.013833446428179741\n",
      "Training loss for batch 7208 : 0.2965276539325714\n",
      "Training loss for batch 7209 : 0.18647608160972595\n",
      "Training loss for batch 7210 : 0.25385934114456177\n",
      "Training loss for batch 7211 : 0.2675594687461853\n",
      "Training loss for batch 7212 : 0.14691822230815887\n",
      "Training loss for batch 7213 : 0.10426197946071625\n",
      "Training loss for batch 7214 : 0.04841572046279907\n",
      "Training loss for batch 7215 : -0.000651367474347353\n",
      "Training loss for batch 7216 : 0.021248560398817062\n",
      "Training loss for batch 7217 : 0.15050536394119263\n",
      "Training loss for batch 7218 : 0.043448083102703094\n",
      "Training loss for batch 7219 : 0.05138794332742691\n",
      "Training loss for batch 7220 : 0.16321232914924622\n",
      "Training loss for batch 7221 : 0.23066607117652893\n",
      "Training loss for batch 7222 : 0.08784207701683044\n",
      "Training loss for batch 7223 : 0.042874209582805634\n",
      "Training loss for batch 7224 : 0.3953174352645874\n",
      "Training loss for batch 7225 : 0.21312059462070465\n",
      "Training loss for batch 7226 : 0.03404834866523743\n",
      "Training loss for batch 7227 : 0.03556257113814354\n",
      "Training loss for batch 7228 : 0.17182478308677673\n",
      "Training loss for batch 7229 : 0.10817467421293259\n",
      "Training loss for batch 7230 : 0.11356516927480698\n",
      "Training loss for batch 7231 : 0.16016916930675507\n",
      "Training loss for batch 7232 : 0.04361528530716896\n",
      "Training loss for batch 7233 : 0.10658064484596252\n",
      "Training loss for batch 7234 : 0.2216847985982895\n",
      "Training loss for batch 7235 : 0.03054123744368553\n",
      "Training loss for batch 7236 : 0.14257293939590454\n",
      "Training loss for batch 7237 : 0.07216637581586838\n",
      "Training loss for batch 7238 : 0.10904599726200104\n",
      "Training loss for batch 7239 : 0.03305958956480026\n",
      "Training loss for batch 7240 : 0.2705969512462616\n",
      "Training loss for batch 7241 : 0.027878837659955025\n",
      "Training loss for batch 7242 : 0.028675662353634834\n",
      "Training loss for batch 7243 : 0.15684255957603455\n",
      "Training loss for batch 7244 : 0.16447271406650543\n",
      "Training loss for batch 7245 : 0.12038129568099976\n",
      "Training loss for batch 7246 : 0.1869676262140274\n",
      "Training loss for batch 7247 : 0.22237271070480347\n",
      "Training loss for batch 7248 : 0.060393109917640686\n",
      "Training loss for batch 7249 : 0.08156286180019379\n",
      "Training loss for batch 7250 : 0.0752396285533905\n",
      "Training loss for batch 7251 : 0.06905759871006012\n",
      "Training loss for batch 7252 : 0.06889775395393372\n",
      "Training loss for batch 7253 : 0.02406545728445053\n",
      "Training loss for batch 7254 : 0.004383141174912453\n",
      "Training loss for batch 7255 : 0.04467112943530083\n",
      "Training loss for batch 7256 : 0.14462178945541382\n",
      "Training loss for batch 7257 : 0.052687376737594604\n",
      "Training loss for batch 7258 : 0.12012988328933716\n",
      "Training loss for batch 7259 : 0.11023977398872375\n",
      "Training loss for batch 7260 : 0.12491044402122498\n",
      "Training loss for batch 7261 : 0.43116652965545654\n",
      "Training loss for batch 7262 : 0.4859793782234192\n",
      "Training loss for batch 7263 : 0.10074461251497269\n",
      "Training loss for batch 7264 : 0.07981258630752563\n",
      "Training loss for batch 7265 : 0.11780553311109543\n",
      "Training loss for batch 7266 : 0.002136531751602888\n",
      "Training loss for batch 7267 : 0.00767466239631176\n",
      "Training loss for batch 7268 : 0.1115444004535675\n",
      "Training loss for batch 7269 : 0.21165063977241516\n",
      "Training loss for batch 7270 : 0.01782947964966297\n",
      "Training loss for batch 7271 : 0.13141438364982605\n",
      "Training loss for batch 7272 : 0.0\n",
      "Training loss for batch 7273 : 0.06162841618061066\n",
      "Training loss for batch 7274 : 0.05309303477406502\n",
      "Training loss for batch 7275 : 0.25579383969306946\n",
      "Training loss for batch 7276 : 0.0891275554895401\n",
      "Training loss for batch 7277 : 0.03872232884168625\n",
      "Training loss for batch 7278 : 0.2073589563369751\n",
      "Training loss for batch 7279 : 0.4518035352230072\n",
      "Training loss for batch 7280 : 0.1087520569562912\n",
      "Training loss for batch 7281 : 0.24353638291358948\n",
      "Training loss for batch 7282 : 0.035939134657382965\n",
      "Training loss for batch 7283 : 0.11820965260267258\n",
      "Training loss for batch 7284 : 0.11059748381376266\n",
      "Training loss for batch 7285 : 0.3427554666996002\n",
      "Training loss for batch 7286 : 0.13826417922973633\n",
      "Training loss for batch 7287 : 0.08003132045269012\n",
      "Training loss for batch 7288 : 0.02232566475868225\n",
      "Training loss for batch 7289 : 0.2117772102355957\n",
      "Training loss for batch 7290 : 0.0\n",
      "Training loss for batch 7291 : 0.06626171618700027\n",
      "Training loss for batch 7292 : 0.17409580945968628\n",
      "Training loss for batch 7293 : 0.22600704431533813\n",
      "Training loss for batch 7294 : 0.0012841041898354888\n",
      "Training loss for batch 7295 : 0.009714614599943161\n",
      "Training loss for batch 7296 : 0.16111387312412262\n",
      "Training loss for batch 7297 : 0.2509678304195404\n",
      "Training loss for batch 7298 : 0.03553161025047302\n",
      "Training loss for batch 7299 : 0.0029394314624369144\n",
      "Training loss for batch 7300 : 0.037107959389686584\n",
      "Training loss for batch 7301 : 0.3602355420589447\n",
      "Training loss for batch 7302 : 0.056527022272348404\n",
      "Training loss for batch 7303 : 0.11600387096405029\n",
      "Training loss for batch 7304 : 0.0008205870981328189\n",
      "Training loss for batch 7305 : 0.07240565121173859\n",
      "Training loss for batch 7306 : 0.01715080253779888\n",
      "Training loss for batch 7307 : 0.0997854620218277\n",
      "Training loss for batch 7308 : 0.0503634512424469\n",
      "Training loss for batch 7309 : 0.12223424017429352\n",
      "Training loss for batch 7310 : 0.09834210574626923\n",
      "Training loss for batch 7311 : 0.0461319237947464\n",
      "Training loss for batch 7312 : 0.08701607584953308\n",
      "Training loss for batch 7313 : 0.23300737142562866\n",
      "Training loss for batch 7314 : 0.10312607139348984\n",
      "Training loss for batch 7315 : 0.04386526346206665\n",
      "Training loss for batch 7316 : 0.20293334126472473\n",
      "Training loss for batch 7317 : 0.07824362069368362\n",
      "Training loss for batch 7318 : 0.15281297266483307\n",
      "Training loss for batch 7319 : 0.3835129737854004\n",
      "Training loss for batch 7320 : 0.04289686307311058\n",
      "Training loss for batch 7321 : 0.0012300172820687294\n",
      "Training loss for batch 7322 : 0.027359073981642723\n",
      "Training loss for batch 7323 : 0.03159578889608383\n",
      "Training loss for batch 7324 : 0.1593419760465622\n",
      "Training loss for batch 7325 : 0.06180255115032196\n",
      "Training loss for batch 7326 : 0.11259161680936813\n",
      "Training loss for batch 7327 : 0.37342560291290283\n",
      "Training loss for batch 7328 : 0.34947362542152405\n",
      "Training loss for batch 7329 : 0.12335293740034103\n",
      "Training loss for batch 7330 : 0.14224378764629364\n",
      "Training loss for batch 7331 : 0.12774549424648285\n",
      "Training loss for batch 7332 : 0.00747313816100359\n",
      "Training loss for batch 7333 : 0.21967531740665436\n",
      "Training loss for batch 7334 : -0.0005920203402638435\n",
      "Training loss for batch 7335 : 0.011537572368979454\n",
      "Training loss for batch 7336 : 0.06468423455953598\n",
      "Training loss for batch 7337 : 0.07237374037504196\n",
      "Training loss for batch 7338 : 0.25095856189727783\n",
      "Training loss for batch 7339 : 0.23169299960136414\n",
      "Training loss for batch 7340 : 0.14199493825435638\n",
      "Training loss for batch 7341 : 0.07250693440437317\n",
      "Training loss for batch 7342 : 0.3617819845676422\n",
      "Training loss for batch 7343 : 0.0907384529709816\n",
      "Training loss for batch 7344 : 0.018568288534879684\n",
      "Training loss for batch 7345 : 0.12647117674350739\n",
      "Training loss for batch 7346 : 0.28218311071395874\n",
      "Training loss for batch 7347 : 0.019868141040205956\n",
      "Training loss for batch 7348 : 0.23740947246551514\n",
      "Training loss for batch 7349 : 0.33925527334213257\n",
      "Training loss for batch 7350 : 0.13991932570934296\n",
      "Training loss for batch 7351 : 0.006793734151870012\n",
      "Training loss for batch 7352 : 0.25202256441116333\n",
      "Training loss for batch 7353 : 0.3279244005680084\n",
      "Training loss for batch 7354 : 0.2708776891231537\n",
      "Training loss for batch 7355 : 0.33157607913017273\n",
      "Training loss for batch 7356 : 0.39493829011917114\n",
      "Training loss for batch 7357 : 0.21803966164588928\n",
      "Training loss for batch 7358 : 0.0904000848531723\n",
      "Training loss for batch 7359 : 0.2890879511833191\n",
      "Training loss for batch 7360 : 0.049801282584667206\n",
      "Training loss for batch 7361 : 0.17910005152225494\n",
      "Training loss for batch 7362 : 0.06324870884418488\n",
      "Training loss for batch 7363 : 0.03737970441579819\n",
      "Training loss for batch 7364 : 0.04525821655988693\n",
      "Training loss for batch 7365 : 0.1476929932832718\n",
      "Training loss for batch 7366 : 0.1523767113685608\n",
      "Training loss for batch 7367 : 0.23550726473331451\n",
      "Training loss for batch 7368 : 0.06965970247983932\n",
      "Training loss for batch 7369 : 0.0\n",
      "Training loss for batch 7370 : 0.025203753262758255\n",
      "Training loss for batch 7371 : 0.23740555346012115\n",
      "Training loss for batch 7372 : 0.19937145709991455\n",
      "Training loss for batch 7373 : 0.1967993974685669\n",
      "Training loss for batch 7374 : 0.4656852185726166\n",
      "Training loss for batch 7375 : 0.24473051726818085\n",
      "Training loss for batch 7376 : 0.1236228346824646\n",
      "Training loss for batch 7377 : 0.17518287897109985\n",
      "Training loss for batch 7378 : 0.0\n",
      "Training loss for batch 7379 : 0.38900136947631836\n",
      "Training loss for batch 7380 : 0.19835610687732697\n",
      "Training loss for batch 7381 : 0.0921621173620224\n",
      "Training loss for batch 7382 : 0.28308072686195374\n",
      "Training loss for batch 7383 : 0.028905414044857025\n",
      "Training loss for batch 7384 : 0.0738414078950882\n",
      "Training loss for batch 7385 : 0.2066187560558319\n",
      "Training loss for batch 7386 : 0.015452923253178596\n",
      "Training loss for batch 7387 : 0.16365323960781097\n",
      "Training loss for batch 7388 : 0.07513420283794403\n",
      "Training loss for batch 7389 : 0.0020589332561939955\n",
      "Training loss for batch 7390 : 0.034999411553144455\n",
      "Training loss for batch 7391 : 0.009878795593976974\n",
      "Training loss for batch 7392 : 0.027348201721906662\n",
      "Training loss for batch 7393 : 0.010716985911130905\n",
      "Training loss for batch 7394 : 0.015810824930667877\n",
      "Training loss for batch 7395 : 0.1358298361301422\n",
      "Training loss for batch 7396 : 0.40423640608787537\n",
      "Training loss for batch 7397 : 0.1395791471004486\n",
      "Training loss for batch 7398 : 0.044879645109176636\n",
      "Training loss for batch 7399 : 0.11911657452583313\n",
      "Training loss for batch 7400 : 0.10458236932754517\n",
      "Training loss for batch 7401 : 0.005430999211966991\n",
      "Training loss for batch 7402 : 0.1604098677635193\n",
      "Training loss for batch 7403 : 0.2950475811958313\n",
      "Training loss for batch 7404 : 0.17495064437389374\n",
      "Training loss for batch 7405 : 0.06968112289905548\n",
      "Training loss for batch 7406 : 0.2455986887216568\n",
      "Training loss for batch 7407 : 0.061900146305561066\n",
      "Training loss for batch 7408 : 0.09642888605594635\n",
      "Training loss for batch 7409 : 0.04654964804649353\n",
      "Training loss for batch 7410 : 0.16167894005775452\n",
      "Training loss for batch 7411 : 0.04772887006402016\n",
      "Training loss for batch 7412 : 0.010440178215503693\n",
      "Training loss for batch 7413 : 0.26961350440979004\n",
      "Training loss for batch 7414 : 0.31534987688064575\n",
      "Training loss for batch 7415 : 0.02358335070312023\n",
      "Training loss for batch 7416 : 0.13471189141273499\n",
      "Training loss for batch 7417 : 0.35913705825805664\n",
      "Training loss for batch 7418 : 0.026745542883872986\n",
      "Training loss for batch 7419 : 0.012394394725561142\n",
      "Training loss for batch 7420 : 0.12893690168857574\n",
      "Training loss for batch 7421 : 0.19689154624938965\n",
      "Training loss for batch 7422 : 0.09268404543399811\n",
      "Training loss for batch 7423 : 0.09596588462591171\n",
      "Training loss for batch 7424 : 0.01407296396791935\n",
      "Training loss for batch 7425 : 0.03202054649591446\n",
      "Training loss for batch 7426 : 0.16677221655845642\n",
      "Training loss for batch 7427 : 0.1184193342924118\n",
      "Training loss for batch 7428 : 0.04612073674798012\n",
      "Training loss for batch 7429 : 0.22328555583953857\n",
      "Training loss for batch 7430 : 0.16878238320350647\n",
      "Training loss for batch 7431 : 0.2735399603843689\n",
      "Training loss for batch 7432 : 0.029969528317451477\n",
      "Training loss for batch 7433 : 0.0325973778963089\n",
      "Training loss for batch 7434 : 0.0909840315580368\n",
      "Training loss for batch 7435 : 0.1019207239151001\n",
      "Training loss for batch 7436 : 0.024536466225981712\n",
      "Training loss for batch 7437 : 0.009648512117564678\n",
      "Training loss for batch 7438 : 0.10794267803430557\n",
      "Training loss for batch 7439 : 0.09640029072761536\n",
      "Training loss for batch 7440 : 0.21862569451332092\n",
      "Training loss for batch 7441 : 0.04592033848166466\n",
      "Training loss for batch 7442 : 0.03279278427362442\n",
      "Training loss for batch 7443 : 0.11626356095075607\n",
      "Training loss for batch 7444 : 0.035237330943346024\n",
      "Training loss for batch 7445 : 0.4453158974647522\n",
      "Training loss for batch 7446 : 0.07347127050161362\n",
      "Training loss for batch 7447 : 0.2972591817378998\n",
      "Training loss for batch 7448 : 0.06379148364067078\n",
      "Training loss for batch 7449 : 0.1831115484237671\n",
      "Training loss for batch 7450 : 0.019895661622285843\n",
      "Training loss for batch 7451 : 0.11987899988889694\n",
      "Training loss for batch 7452 : 0.12056581676006317\n",
      "Training loss for batch 7453 : 0.08819937705993652\n",
      "Training loss for batch 7454 : 0.14324718713760376\n",
      "Training loss for batch 7455 : 0.2347366213798523\n",
      "Training loss for batch 7456 : 0.037004001438617706\n",
      "Training loss for batch 7457 : 0.016053462401032448\n",
      "Training loss for batch 7458 : 0.14740121364593506\n",
      "Training loss for batch 7459 : 0.27166077494621277\n",
      "Training loss for batch 7460 : 0.16122141480445862\n",
      "Training loss for batch 7461 : 0.010267406702041626\n",
      "Training loss for batch 7462 : 0.06022822856903076\n",
      "Training loss for batch 7463 : 0.09173202514648438\n",
      "Training loss for batch 7464 : 0.0742083191871643\n",
      "Training loss for batch 7465 : 0.08791648596525192\n",
      "Training loss for batch 7466 : 0.02359938621520996\n",
      "Training loss for batch 7467 : 0.1046137809753418\n",
      "Training loss for batch 7468 : 0.0971200168132782\n",
      "Training loss for batch 7469 : 0.002346588531509042\n",
      "Training loss for batch 7470 : 0.3539728820323944\n",
      "Training loss for batch 7471 : 0.0794772356748581\n",
      "Training loss for batch 7472 : 0.05078493058681488\n",
      "Training loss for batch 7473 : 0.2334676831960678\n",
      "Training loss for batch 7474 : 0.15449932217597961\n",
      "Training loss for batch 7475 : 0.023311631754040718\n",
      "Training loss for batch 7476 : 0.042184848338365555\n",
      "Training loss for batch 7477 : 0.12426039576530457\n",
      "Training loss for batch 7478 : 0.13314630091190338\n",
      "Training loss for batch 7479 : 0.2605009973049164\n",
      "Training loss for batch 7480 : 0.09042047709226608\n",
      "Training loss for batch 7481 : 0.07980336248874664\n",
      "Training loss for batch 7482 : 0.057669054716825485\n",
      "Training loss for batch 7483 : 0.08251475542783737\n",
      "Training loss for batch 7484 : 0.18221893906593323\n",
      "Training loss for batch 7485 : 0.21138618886470795\n",
      "Training loss for batch 7486 : 0.06033859774470329\n",
      "Training loss for batch 7487 : 0.07693535834550858\n",
      "Training loss for batch 7488 : 0.02614307589828968\n",
      "Training loss for batch 7489 : 0.05750994011759758\n",
      "Training loss for batch 7490 : 0.16107556223869324\n",
      "Training loss for batch 7491 : 0.17136169970035553\n",
      "Training loss for batch 7492 : 0.15099668502807617\n",
      "Training loss for batch 7493 : 0.04403022676706314\n",
      "Training loss for batch 7494 : 0.0844714567065239\n",
      "Training loss for batch 7495 : 0.1906682848930359\n",
      "Training loss for batch 7496 : 0.20164886116981506\n",
      "Training loss for batch 7497 : 0.38921380043029785\n",
      "Training loss for batch 7498 : 0.0009312040638178587\n",
      "Training loss for batch 7499 : 0.05431871861219406\n",
      "Training loss for batch 7500 : 0.2079092115163803\n",
      "Training loss for batch 7501 : 0.0\n",
      "Training loss for batch 7502 : 0.3293561041355133\n",
      "Training loss for batch 7503 : 0.09212810546159744\n",
      "Training loss for batch 7504 : 0.04009401425719261\n",
      "Training loss for batch 7505 : 0.02237383835017681\n",
      "Training loss for batch 7506 : 0.19749432802200317\n",
      "Training loss for batch 7507 : 0.06379850953817368\n",
      "Training loss for batch 7508 : 0.007123410701751709\n",
      "Training loss for batch 7509 : 0.019789502024650574\n",
      "Training loss for batch 7510 : 0.14258445799350739\n",
      "Training loss for batch 7511 : 0.2374715507030487\n",
      "Training loss for batch 7512 : 0.03411633148789406\n",
      "Training loss for batch 7513 : 0.01963583379983902\n",
      "Training loss for batch 7514 : 0.023316605016589165\n",
      "Training loss for batch 7515 : 0.09031346440315247\n",
      "Training loss for batch 7516 : 0.08523167669773102\n",
      "Training loss for batch 7517 : 0.46355482935905457\n",
      "Training loss for batch 7518 : 0.031401127576828\n",
      "Training loss for batch 7519 : 0.031093470752239227\n",
      "Training loss for batch 7520 : 0.3073073625564575\n",
      "Training loss for batch 7521 : 0.16854798793792725\n",
      "Training loss for batch 7522 : 0.06558456271886826\n",
      "Training loss for batch 7523 : 0.19707414507865906\n",
      "Training loss for batch 7524 : 0.1689298301935196\n",
      "Training loss for batch 7525 : 0.03374779224395752\n",
      "Training loss for batch 7526 : 0.07563816010951996\n",
      "Training loss for batch 7527 : 0.061513546854257584\n",
      "Training loss for batch 7528 : 0.11002025753259659\n",
      "Training loss for batch 7529 : 0.4591028392314911\n",
      "Training loss for batch 7530 : 0.3072299659252167\n",
      "Training loss for batch 7531 : 0.1301034688949585\n",
      "Training loss for batch 7532 : 0.07086050510406494\n",
      "Training loss for batch 7533 : 0.1908431351184845\n",
      "Training loss for batch 7534 : 0.15251246094703674\n",
      "Training loss for batch 7535 : 0.2418886125087738\n",
      "Training loss for batch 7536 : 0.0\n",
      "Training loss for batch 7537 : 0.20918549597263336\n",
      "Training loss for batch 7538 : 0.20276756584644318\n",
      "Training loss for batch 7539 : 0.005859876051545143\n",
      "Training loss for batch 7540 : -0.0006325207650661469\n",
      "Training loss for batch 7541 : 0.05929257348179817\n",
      "Training loss for batch 7542 : 0.08490972965955734\n",
      "Training loss for batch 7543 : 0.3281343877315521\n",
      "Training loss for batch 7544 : 0.12315219640731812\n",
      "Training loss for batch 7545 : 0.2782793939113617\n",
      "Training loss for batch 7546 : 0.06484152376651764\n",
      "Training loss for batch 7547 : 0.0043167672120034695\n",
      "Training loss for batch 7548 : 0.08321887254714966\n",
      "Training loss for batch 7549 : 0.16324259340763092\n",
      "Training loss for batch 7550 : 0.16986998915672302\n",
      "Training loss for batch 7551 : 0.07716071605682373\n",
      "Training loss for batch 7552 : 0.030689654871821404\n",
      "Training loss for batch 7553 : 0.09034095704555511\n",
      "Training loss for batch 7554 : 0.0263529010117054\n",
      "Training loss for batch 7555 : 0.021302256733179092\n",
      "Training loss for batch 7556 : 0.05919405817985535\n",
      "Training loss for batch 7557 : 0.06126999855041504\n",
      "Training loss for batch 7558 : 0.03230154141783714\n",
      "Training loss for batch 7559 : 0.0289775338023901\n",
      "Training loss for batch 7560 : 0.22824938595294952\n",
      "Training loss for batch 7561 : 0.12690569460391998\n",
      "Training loss for batch 7562 : 0.02775864489376545\n",
      "Training loss for batch 7563 : 0.21157266199588776\n",
      "Training loss for batch 7564 : 0.14589622616767883\n",
      "Training loss for batch 7565 : 0.10478827357292175\n",
      "Training loss for batch 7566 : 0.3488999307155609\n",
      "Training loss for batch 7567 : 0.008577806875109673\n",
      "Training loss for batch 7568 : 0.03038737177848816\n",
      "Training loss for batch 7569 : 0.12871766090393066\n",
      "Training loss for batch 7570 : 0.2587682604789734\n",
      "Training loss for batch 7571 : 0.0014487938024103642\n",
      "Training loss for batch 7572 : 0.16913074254989624\n",
      "Training loss for batch 7573 : 0.08303694427013397\n",
      "Training loss for batch 7574 : 0.271924763917923\n",
      "Training loss for batch 7575 : 0.04491136968135834\n",
      "Training loss for batch 7576 : 0.277895987033844\n",
      "Training loss for batch 7577 : 0.07315404713153839\n",
      "Training loss for batch 7578 : 0.16638579964637756\n",
      "Training loss for batch 7579 : 0.044549182057380676\n",
      "Training loss for batch 7580 : 0.04928060993552208\n",
      "Training loss for batch 7581 : 0.17541378736495972\n",
      "Training loss for batch 7582 : 0.2329622060060501\n",
      "Training loss for batch 7583 : -0.0018525331979617476\n",
      "Training loss for batch 7584 : 0.2809145748615265\n",
      "Training loss for batch 7585 : 0.21996711194515228\n",
      "Training loss for batch 7586 : 0.05630633980035782\n",
      "Training loss for batch 7587 : 0.246440589427948\n",
      "Training loss for batch 7588 : 0.02746519260108471\n",
      "Training loss for batch 7589 : 0.3111180067062378\n",
      "Training loss for batch 7590 : 0.06309174746274948\n",
      "Training loss for batch 7591 : 0.122972771525383\n",
      "Training loss for batch 7592 : 0.047131799161434174\n",
      "Training loss for batch 7593 : 0.1122075617313385\n",
      "Training loss for batch 7594 : 0.16391973197460175\n",
      "Training loss for batch 7595 : 0.017686769366264343\n",
      "Training loss for batch 7596 : 0.0395222008228302\n",
      "Training loss for batch 7597 : 0.13925330340862274\n",
      "Training loss for batch 7598 : 0.22611483931541443\n",
      "Training loss for batch 7599 : 0.16807596385478973\n",
      "Training loss for batch 7600 : 0.0074754683300852776\n",
      "Training loss for batch 7601 : 0.11851932108402252\n",
      "Training loss for batch 7602 : 0.3161850571632385\n",
      "Training loss for batch 7603 : 0.08947497606277466\n",
      "Training loss for batch 7604 : 0.023644447326660156\n",
      "Training loss for batch 7605 : 0.03376373276114464\n",
      "Training loss for batch 7606 : 0.05805452540516853\n",
      "Training loss for batch 7607 : 0.15698441863059998\n",
      "Training loss for batch 7608 : 0.17404207587242126\n",
      "Training loss for batch 7609 : 0.22751745581626892\n",
      "Training loss for batch 7610 : 0.06764113903045654\n",
      "Training loss for batch 7611 : 0.11485525965690613\n",
      "Training loss for batch 7612 : 0.049340128898620605\n",
      "Training loss for batch 7613 : 0.06397313624620438\n",
      "Training loss for batch 7614 : 0.20076821744441986\n",
      "Training loss for batch 7615 : 0.23885589838027954\n",
      "Training loss for batch 7616 : 0.1914648711681366\n",
      "Training loss for batch 7617 : 0.1508529782295227\n",
      "Training loss for batch 7618 : 0.08994575589895248\n",
      "Training loss for batch 7619 : 0.13703390955924988\n",
      "Training loss for batch 7620 : 0.42789196968078613\n",
      "Training loss for batch 7621 : 0.32870471477508545\n",
      "Training loss for batch 7622 : 0.14029806852340698\n",
      "Training loss for batch 7623 : 0.036702144891023636\n",
      "Training loss for batch 7624 : 0.388384610414505\n",
      "Training loss for batch 7625 : 0.30079519748687744\n",
      "Training loss for batch 7626 : 0.2479763925075531\n",
      "Training loss for batch 7627 : 0.14605723321437836\n",
      "Training loss for batch 7628 : 0.25060364603996277\n",
      "Training loss for batch 7629 : 0.12991644442081451\n",
      "Training loss for batch 7630 : 0.1804981529712677\n",
      "Training loss for batch 7631 : 0.0024660539347678423\n",
      "Training loss for batch 7632 : 0.06943344324827194\n",
      "Training loss for batch 7633 : 0.05552712455391884\n",
      "Training loss for batch 7634 : 0.1741672307252884\n",
      "Training loss for batch 7635 : 0.30772173404693604\n",
      "Training loss for batch 7636 : 0.016232213005423546\n",
      "Training loss for batch 7637 : 0.12632453441619873\n",
      "Training loss for batch 7638 : 0.02221098728477955\n",
      "Training loss for batch 7639 : 0.09560263156890869\n",
      "Training loss for batch 7640 : 0.04849439486861229\n",
      "Training loss for batch 7641 : 0.20143739879131317\n",
      "Training loss for batch 7642 : 0.3748898208141327\n",
      "Training loss for batch 7643 : 0.10386142879724503\n",
      "Training loss for batch 7644 : 0.259263813495636\n",
      "Training loss for batch 7645 : 0.3974493145942688\n",
      "Training loss for batch 7646 : 0.003503756131976843\n",
      "Training loss for batch 7647 : 0.0038913986645638943\n",
      "Training loss for batch 7648 : 0.3448724150657654\n",
      "Training loss for batch 7649 : 0.2599353790283203\n",
      "Training loss for batch 7650 : 0.009198155254125595\n",
      "Training loss for batch 7651 : 0.07775558531284332\n",
      "Training loss for batch 7652 : 0.6802888512611389\n",
      "Training loss for batch 7653 : 0.10674837231636047\n",
      "Training loss for batch 7654 : 0.14068534970283508\n",
      "Training loss for batch 7655 : 0.19560420513153076\n",
      "Training loss for batch 7656 : 0.029767876490950584\n",
      "Training loss for batch 7657 : 0.031709302216768265\n",
      "Training loss for batch 7658 : 0.04731855168938637\n",
      "Training loss for batch 7659 : 0.11135145276784897\n",
      "Training loss for batch 7660 : 0.1102570965886116\n",
      "Training loss for batch 7661 : 0.029119983315467834\n",
      "Training loss for batch 7662 : 0.07526910305023193\n",
      "Training loss for batch 7663 : 0.17470361292362213\n",
      "Training loss for batch 7664 : 0.28668010234832764\n",
      "Training loss for batch 7665 : 0.21193480491638184\n",
      "Training loss for batch 7666 : 0.10695143789052963\n",
      "Training loss for batch 7667 : 0.09449592232704163\n",
      "Training loss for batch 7668 : 0.25423645973205566\n",
      "Training loss for batch 7669 : 0.1648830622434616\n",
      "Training loss for batch 7670 : 0.07279619574546814\n",
      "Training loss for batch 7671 : 0.17795512080192566\n",
      "Training loss for batch 7672 : 0.047135286033153534\n",
      "Training loss for batch 7673 : 0.4031065106391907\n",
      "Training loss for batch 7674 : 0.005521039478480816\n",
      "Training loss for batch 7675 : 0.22825615108013153\n",
      "Training loss for batch 7676 : 0.29139822721481323\n",
      "Training loss for batch 7677 : 0.1049928069114685\n",
      "Training loss for batch 7678 : 0.0001265713945031166\n",
      "Training loss for batch 7679 : 0.40059518814086914\n",
      "Training loss for batch 7680 : 0.02454150840640068\n",
      "Training loss for batch 7681 : 0.042971134185791016\n",
      "Training loss for batch 7682 : 0.12350067496299744\n",
      "Training loss for batch 7683 : 0.27554306387901306\n",
      "Training loss for batch 7684 : 0.10278778523206711\n",
      "Training loss for batch 7685 : 0.29374000430107117\n",
      "Training loss for batch 7686 : 0.08021155744791031\n",
      "Training loss for batch 7687 : 0.05800182372331619\n",
      "Training loss for batch 7688 : 0.15622231364250183\n",
      "Training loss for batch 7689 : 0.0882703959941864\n",
      "Training loss for batch 7690 : 0.06503815203905106\n",
      "Training loss for batch 7691 : 0.02333948202431202\n",
      "Training loss for batch 7692 : 0.14479544758796692\n",
      "Training loss for batch 7693 : 0.2046508938074112\n",
      "Training loss for batch 7694 : 0.029372289776802063\n",
      "Training loss for batch 7695 : 0.21326707303524017\n",
      "Training loss for batch 7696 : 0.3583868741989136\n",
      "Training loss for batch 7697 : 0.09610851109027863\n",
      "Training loss for batch 7698 : 0.06653573364019394\n",
      "Training loss for batch 7699 : 0.13242223858833313\n",
      "Training loss for batch 7700 : 0.09668594598770142\n",
      "Training loss for batch 7701 : 0.1253376007080078\n",
      "Training loss for batch 7702 : 0.142632856965065\n",
      "Training loss for batch 7703 : 0.09196939319372177\n",
      "Training loss for batch 7704 : 0.1726057380437851\n",
      "Training loss for batch 7705 : 0.1044047474861145\n",
      "Training loss for batch 7706 : 0.26891887187957764\n",
      "Training loss for batch 7707 : 0.12316891551017761\n",
      "Training loss for batch 7708 : 0.16470538079738617\n",
      "Training loss for batch 7709 : 0.1457337886095047\n",
      "Training loss for batch 7710 : 0.06647094339132309\n",
      "Training loss for batch 7711 : 0.06592177599668503\n",
      "Training loss for batch 7712 : 0.2851017713546753\n",
      "Training loss for batch 7713 : 0.1160627156496048\n",
      "Training loss for batch 7714 : 0.005902796983718872\n",
      "Training loss for batch 7715 : 0.13494016230106354\n",
      "Training loss for batch 7716 : 0.10219839960336685\n",
      "Training loss for batch 7717 : 0.10939791798591614\n",
      "Training loss for batch 7718 : 0.17118696868419647\n",
      "Training loss for batch 7719 : 0.08276454359292984\n",
      "Training loss for batch 7720 : 0.0681341290473938\n",
      "Training loss for batch 7721 : 0.12069061398506165\n",
      "Training loss for batch 7722 : 0.047328878194093704\n",
      "Training loss for batch 7723 : 0.15787538886070251\n",
      "Training loss for batch 7724 : 0.240609809756279\n",
      "Training loss for batch 7725 : -0.005117816384881735\n",
      "Training loss for batch 7726 : 0.13704320788383484\n",
      "Training loss for batch 7727 : 0.07379834353923798\n",
      "Training loss for batch 7728 : 0.024630825966596603\n",
      "Training loss for batch 7729 : 0.09745001792907715\n",
      "Training loss for batch 7730 : 0.08545102179050446\n",
      "Training loss for batch 7731 : 0.056480489671230316\n",
      "Training loss for batch 7732 : 0.029526878148317337\n",
      "Training loss for batch 7733 : 0.09585773199796677\n",
      "Training loss for batch 7734 : 0.2614787817001343\n",
      "Training loss for batch 7735 : 0.0889844000339508\n",
      "Training loss for batch 7736 : 0.19600453972816467\n",
      "Training loss for batch 7737 : 0.1911669373512268\n",
      "Training loss for batch 7738 : 0.07152125239372253\n",
      "Training loss for batch 7739 : 0.20659586787223816\n",
      "Training loss for batch 7740 : 0.07851506769657135\n",
      "Training loss for batch 7741 : 0.1138056218624115\n",
      "Training loss for batch 7742 : 0.0\n",
      "Training loss for batch 7743 : 0.02156795561313629\n",
      "Training loss for batch 7744 : 0.0\n",
      "Training loss for batch 7745 : 0.12093265354633331\n",
      "Training loss for batch 7746 : 0.08328906446695328\n",
      "Training loss for batch 7747 : 0.042033594101667404\n",
      "Training loss for batch 7748 : 0.054914653301239014\n",
      "Training loss for batch 7749 : 0.20480529963970184\n",
      "Training loss for batch 7750 : 0.07588908076286316\n",
      "Training loss for batch 7751 : 0.14307644963264465\n",
      "Training loss for batch 7752 : 0.1915728747844696\n",
      "Training loss for batch 7753 : 0.3116719722747803\n",
      "Training loss for batch 7754 : 0.1818365454673767\n",
      "Training loss for batch 7755 : 0.25734415650367737\n",
      "Training loss for batch 7756 : 0.2660222351551056\n",
      "Training loss for batch 7757 : 0.09130586683750153\n",
      "Training loss for batch 7758 : 0.07438243180513382\n",
      "Training loss for batch 7759 : 0.2197764813899994\n",
      "Training loss for batch 7760 : 0.1456001102924347\n",
      "Training loss for batch 7761 : 0.14246585965156555\n",
      "Training loss for batch 7762 : 0.0\n",
      "Training loss for batch 7763 : 0.1496063470840454\n",
      "Training loss for batch 7764 : 0.20997938513755798\n",
      "Training loss for batch 7765 : 0.1217268854379654\n",
      "Training loss for batch 7766 : 0.05487854406237602\n",
      "Training loss for batch 7767 : 0.018021954223513603\n",
      "Training loss for batch 7768 : 0.021344533190131187\n",
      "Training loss for batch 7769 : 0.02795856073498726\n",
      "Training loss for batch 7770 : -0.0027039353735744953\n",
      "Training loss for batch 7771 : 0.06796685606241226\n",
      "Training loss for batch 7772 : 0.021579524502158165\n",
      "Training loss for batch 7773 : 0.013991845771670341\n",
      "Training loss for batch 7774 : 0.09991326183080673\n",
      "Training loss for batch 7775 : 0.013570557348430157\n",
      "Training loss for batch 7776 : 0.06107982620596886\n",
      "Training loss for batch 7777 : 0.0822824090719223\n",
      "Training loss for batch 7778 : 0.08119985461235046\n",
      "Training loss for batch 7779 : 0.3047707676887512\n",
      "Training loss for batch 7780 : 0.12126864492893219\n",
      "Training loss for batch 7781 : -0.0009153546998277307\n",
      "Training loss for batch 7782 : 0.21869981288909912\n",
      "Training loss for batch 7783 : -0.0007501477375626564\n",
      "Training loss for batch 7784 : 0.00019797589629888535\n",
      "Training loss for batch 7785 : 0.17004792392253876\n",
      "Training loss for batch 7786 : 0.11021767556667328\n",
      "Training loss for batch 7787 : 0.07482513785362244\n",
      "Training loss for batch 7788 : 0.1339070051908493\n",
      "Training loss for batch 7789 : 0.060757897794246674\n",
      "Training loss for batch 7790 : 0.10186296701431274\n",
      "Training loss for batch 7791 : 0.012990130111575127\n",
      "Training loss for batch 7792 : -0.002035762183368206\n",
      "Training loss for batch 7793 : 0.15897104144096375\n",
      "Training loss for batch 7794 : 0.4028628170490265\n",
      "Training loss for batch 7795 : 0.09489860385656357\n",
      "Training loss for batch 7796 : 0.07961501181125641\n",
      "Training loss for batch 7797 : 0.08246627449989319\n",
      "Training loss for batch 7798 : 0.15874290466308594\n",
      "Training loss for batch 7799 : 0.11706031858921051\n",
      "Training loss for batch 7800 : 0.26954224705696106\n",
      "Training loss for batch 7801 : 0.5129503607749939\n",
      "Training loss for batch 7802 : 0.030849572271108627\n",
      "Training loss for batch 7803 : 0.16658315062522888\n",
      "Training loss for batch 7804 : 0.016639575362205505\n",
      "Training loss for batch 7805 : 0.2504616975784302\n",
      "Training loss for batch 7806 : 0.03175888583064079\n",
      "Training loss for batch 7807 : 0.05218696966767311\n",
      "Training loss for batch 7808 : 0.17681491374969482\n",
      "Training loss for batch 7809 : 0.09363338351249695\n",
      "Training loss for batch 7810 : 0.15956823527812958\n",
      "Training loss for batch 7811 : 0.18552637100219727\n",
      "Training loss for batch 7812 : 0.11050914227962494\n",
      "Training loss for batch 7813 : 0.055687811225652695\n",
      "Training loss for batch 7814 : 0.08820933103561401\n",
      "Training loss for batch 7815 : 0.1503164917230606\n",
      "Training loss for batch 7816 : 0.04544299095869064\n",
      "Training loss for batch 7817 : 0.2791406512260437\n",
      "Training loss for batch 7818 : 0.22311580181121826\n",
      "Training loss for batch 7819 : 0.03308369591832161\n",
      "Training loss for batch 7820 : 0.023228706791996956\n",
      "Training loss for batch 7821 : 0.21223902702331543\n",
      "Training loss for batch 7822 : 0.12557704746723175\n",
      "Training loss for batch 7823 : 0.18571394681930542\n",
      "Training loss for batch 7824 : 0.19974713027477264\n",
      "Training loss for batch 7825 : 0.1537591516971588\n",
      "Training loss for batch 7826 : -0.007352930493652821\n",
      "Training loss for batch 7827 : 0.2139831781387329\n",
      "Training loss for batch 7828 : 0.31778013706207275\n",
      "Training loss for batch 7829 : 0.20777133107185364\n",
      "Training loss for batch 7830 : 0.034374531358480453\n",
      "Training loss for batch 7831 : 0.06150534376502037\n",
      "Training loss for batch 7832 : 0.10033337771892548\n",
      "Training loss for batch 7833 : 0.07743872702121735\n",
      "Training loss for batch 7834 : 0.3313799500465393\n",
      "Training loss for batch 7835 : 0.19066458940505981\n",
      "Training loss for batch 7836 : 0.27615997195243835\n",
      "Training loss for batch 7837 : 0.17831289768218994\n",
      "Training loss for batch 7838 : 0.1511194258928299\n",
      "Training loss for batch 7839 : 0.19869911670684814\n",
      "Training loss for batch 7840 : 0.2536020278930664\n",
      "Training loss for batch 7841 : 0.09807109832763672\n",
      "Training loss for batch 7842 : 0.08621132373809814\n",
      "Training loss for batch 7843 : -0.010257646441459656\n",
      "Training loss for batch 7844 : 0.13280710577964783\n",
      "Training loss for batch 7845 : 0.12633997201919556\n",
      "Training loss for batch 7846 : 0.02076740562915802\n",
      "Training loss for batch 7847 : 0.1195012554526329\n",
      "Training loss for batch 7848 : 0.14074531197547913\n",
      "Training loss for batch 7849 : 0.2532515525817871\n",
      "Training loss for batch 7850 : 0.03635047748684883\n",
      "Training loss for batch 7851 : 0.07118918001651764\n",
      "Training loss for batch 7852 : 0.2044871300458908\n",
      "Training loss for batch 7853 : 0.07817690074443817\n",
      "Training loss for batch 7854 : 0.28022778034210205\n",
      "Training loss for batch 7855 : 0.0\n",
      "Training loss for batch 7856 : 0.0913894772529602\n",
      "Training loss for batch 7857 : 0.10912895202636719\n",
      "Training loss for batch 7858 : 0.18639133870601654\n",
      "Training loss for batch 7859 : 0.0044225952588021755\n",
      "Training loss for batch 7860 : 0.033888064324855804\n",
      "Training loss for batch 7861 : 0.04816999286413193\n",
      "Training loss for batch 7862 : 0.04203243553638458\n",
      "Training loss for batch 7863 : 0.13513147830963135\n",
      "Training loss for batch 7864 : 0.09609963744878769\n",
      "Training loss for batch 7865 : 0.08550088852643967\n",
      "Training loss for batch 7866 : 0.19415591657161713\n",
      "Training loss for batch 7867 : 0.08184781670570374\n",
      "Training loss for batch 7868 : 0.13796643912792206\n",
      "Training loss for batch 7869 : 0.15384258329868317\n",
      "Training loss for batch 7870 : 0.007547810673713684\n",
      "Training loss for batch 7871 : 0.1763177216053009\n",
      "Training loss for batch 7872 : 0.10533218085765839\n",
      "Training loss for batch 7873 : 0.08206433802843094\n",
      "Training loss for batch 7874 : 0.05446555092930794\n",
      "Training loss for batch 7875 : 0.0025594434700906277\n",
      "Training loss for batch 7876 : 0.04301225766539574\n",
      "Training loss for batch 7877 : 0.0677778497338295\n",
      "Training loss for batch 7878 : 0.06128092482686043\n",
      "Training loss for batch 7879 : 0.09265585243701935\n",
      "Training loss for batch 7880 : 0.28877946734428406\n",
      "Training loss for batch 7881 : 0.17858053743839264\n",
      "Training loss for batch 7882 : 0.058692120015621185\n",
      "Training loss for batch 7883 : 0.29293617606163025\n",
      "Training loss for batch 7884 : 0.23572847247123718\n",
      "Training loss for batch 7885 : 0.030102599412202835\n",
      "Training loss for batch 7886 : 0.09587159007787704\n",
      "Training loss for batch 7887 : 0.0\n",
      "Training loss for batch 7888 : 0.059892818331718445\n",
      "Training loss for batch 7889 : 0.30196884274482727\n",
      "Training loss for batch 7890 : 0.2013070434331894\n",
      "Training loss for batch 7891 : 0.17568811774253845\n",
      "Training loss for batch 7892 : 0.049955327063798904\n",
      "Training loss for batch 7893 : 0.03369322791695595\n",
      "Training loss for batch 7894 : 0.06655734777450562\n",
      "Training loss for batch 7895 : 0.09885172545909882\n",
      "Training loss for batch 7896 : 0.03823673725128174\n",
      "Training loss for batch 7897 : 0.5548186302185059\n",
      "Training loss for batch 7898 : 0.24034950137138367\n",
      "Training loss for batch 7899 : 0.08453765511512756\n",
      "Training loss for batch 7900 : 0.46768683195114136\n",
      "Training loss for batch 7901 : 0.00033086538314819336\n",
      "Training loss for batch 7902 : 0.03499910235404968\n",
      "Training loss for batch 7903 : 0.35627323389053345\n",
      "Training loss for batch 7904 : 0.11033284664154053\n",
      "Training loss for batch 7905 : 0.11849536001682281\n",
      "Training loss for batch 7906 : 0.13091740012168884\n",
      "Training loss for batch 7907 : 0.1509571373462677\n",
      "Training loss for batch 7908 : 0.006317023187875748\n",
      "Training loss for batch 7909 : 0.07688149064779282\n",
      "Training loss for batch 7910 : 0.2470717430114746\n",
      "Training loss for batch 7911 : 0.0861670970916748\n",
      "Training loss for batch 7912 : 0.4008414149284363\n",
      "Training loss for batch 7913 : 0.04205614700913429\n",
      "Training loss for batch 7914 : 0.15835827589035034\n",
      "Training loss for batch 7915 : 0.10759862512350082\n",
      "Training loss for batch 7916 : 0.1652751863002777\n",
      "Training loss for batch 7917 : 0.22083939611911774\n",
      "Training loss for batch 7918 : 0.16737215220928192\n",
      "Training loss for batch 7919 : 0.30118435621261597\n",
      "Training loss for batch 7920 : 0.09021849930286407\n",
      "Training loss for batch 7921 : 0.43964219093322754\n",
      "Training loss for batch 7922 : 0.057909876108169556\n",
      "Training loss for batch 7923 : 0.22182312607765198\n",
      "Training loss for batch 7924 : 0.19945217669010162\n",
      "Training loss for batch 7925 : 0.2478683739900589\n",
      "Training loss for batch 7926 : 0.10642427206039429\n",
      "Training loss for batch 7927 : 0.0761854499578476\n",
      "Training loss for batch 7928 : 0.0007211078191176057\n",
      "Training loss for batch 7929 : 0.1378711611032486\n",
      "Training loss for batch 7930 : 0.08375119417905807\n",
      "Training loss for batch 7931 : 0.10634364187717438\n",
      "Training loss for batch 7932 : 0.17226046323776245\n",
      "Training loss for batch 7933 : 0.13086339831352234\n",
      "Training loss for batch 7934 : 0.05016257241368294\n",
      "Training loss for batch 7935 : 0.11068260669708252\n",
      "Training loss for batch 7936 : 0.011692386120557785\n",
      "Training loss for batch 7937 : 0.20163333415985107\n",
      "Training loss for batch 7938 : 0.12258133292198181\n",
      "Training loss for batch 7939 : 0.09711236506700516\n",
      "Training loss for batch 7940 : 0.17101754248142242\n",
      "Training loss for batch 7941 : 0.052999477833509445\n",
      "Training loss for batch 7942 : 0.12592512369155884\n",
      "Training loss for batch 7943 : 0.06738461554050446\n",
      "Training loss for batch 7944 : 0.25856640934944153\n",
      "Training loss for batch 7945 : 0.038176607340574265\n",
      "Training loss for batch 7946 : 0.29213929176330566\n",
      "Training loss for batch 7947 : 0.0\n",
      "Training loss for batch 7948 : 0.3205373287200928\n",
      "Training loss for batch 7949 : 0.0010212995111942291\n",
      "Training loss for batch 7950 : 0.18345360457897186\n",
      "Training loss for batch 7951 : 0.18156975507736206\n",
      "Training loss for batch 7952 : 0.1931561827659607\n",
      "Training loss for batch 7953 : 0.12436620891094208\n",
      "Training loss for batch 7954 : 0.002047240734100342\n",
      "Training loss for batch 7955 : 0.6823083758354187\n",
      "Training loss for batch 7956 : 0.11612430214881897\n",
      "Training loss for batch 7957 : 0.22739078104496002\n",
      "Training loss for batch 7958 : 0.2593410909175873\n",
      "Training loss for batch 7959 : 0.015914253890514374\n",
      "Training loss for batch 7960 : 0.08939179033041\n",
      "Training loss for batch 7961 : 0.1648389846086502\n",
      "Training loss for batch 7962 : 0.054479628801345825\n",
      "Training loss for batch 7963 : 0.11747080087661743\n",
      "Training loss for batch 7964 : 0.04251125454902649\n",
      "Training loss for batch 7965 : 0.005954789463430643\n",
      "Training loss for batch 7966 : 0.2399027794599533\n",
      "Training loss for batch 7967 : 0.17697757482528687\n",
      "Training loss for batch 7968 : 0.19783541560173035\n",
      "Training loss for batch 7969 : -0.0007697223336435854\n",
      "Training loss for batch 7970 : 0.14143122732639313\n",
      "Training loss for batch 7971 : 0.169338196516037\n",
      "Training loss for batch 7972 : 0.0750182643532753\n",
      "Training loss for batch 7973 : 0.14210186898708344\n",
      "Training loss for batch 7974 : 0.018412288278341293\n",
      "Training loss for batch 7975 : 0.06592979282140732\n",
      "Training loss for batch 7976 : 0.06438066065311432\n",
      "Training loss for batch 7977 : 0.053760625422000885\n",
      "Training loss for batch 7978 : 0.018086792901158333\n",
      "Training loss for batch 7979 : 0.0813772976398468\n",
      "Training loss for batch 7980 : 0.11144963651895523\n",
      "Training loss for batch 7981 : 0.19826000928878784\n",
      "Training loss for batch 7982 : 0.09379768371582031\n",
      "Training loss for batch 7983 : 0.2225162833929062\n",
      "Training loss for batch 7984 : 0.28067511320114136\n",
      "Training loss for batch 7985 : 0.34608563780784607\n",
      "Training loss for batch 7986 : 0.017743214964866638\n",
      "Training loss for batch 7987 : 0.18704649806022644\n",
      "Training loss for batch 7988 : 0.07712379097938538\n",
      "Training loss for batch 7989 : 0.01908745989203453\n",
      "Training loss for batch 7990 : 0.2619306743144989\n",
      "Training loss for batch 7991 : 0.01153920590877533\n",
      "Training loss for batch 7992 : 0.0480586476624012\n",
      "Training loss for batch 7993 : 0.03016924299299717\n",
      "Training loss for batch 7994 : 0.057603009045124054\n",
      "Training loss for batch 7995 : 0.23780277371406555\n",
      "Training loss for batch 7996 : 0.09706011414527893\n",
      "Training loss for batch 7997 : 0.04827938228845596\n",
      "Training loss for batch 7998 : 0.2789487838745117\n",
      "Training loss for batch 7999 : 0.20800504088401794\n",
      "Training loss for batch 8000 : 0.008044179528951645\n",
      "Training loss for batch 8001 : 0.23298251628875732\n",
      "Training loss for batch 8002 : 0.01805853098630905\n",
      "Training loss for batch 8003 : 0.09164047986268997\n",
      "Training loss for batch 8004 : 0.024116545915603638\n",
      "Training loss for batch 8005 : 0.03712019696831703\n",
      "Training loss for batch 8006 : 0.08101215958595276\n",
      "Training loss for batch 8007 : 0.13554976880550385\n",
      "Training loss for batch 8008 : 0.0839170441031456\n",
      "Training loss for batch 8009 : 0.21067744493484497\n",
      "Training loss for batch 8010 : 0.009460054337978363\n",
      "Training loss for batch 8011 : 0.04748925939202309\n",
      "Training loss for batch 8012 : 0.4235266149044037\n",
      "Training loss for batch 8013 : 0.10387454926967621\n",
      "Training loss for batch 8014 : -0.002490424085408449\n",
      "Training loss for batch 8015 : 0.0681624487042427\n",
      "Training loss for batch 8016 : 0.061249591410160065\n",
      "Training loss for batch 8017 : 0.07272601127624512\n",
      "Training loss for batch 8018 : 0.24156513810157776\n",
      "Training loss for batch 8019 : 0.24771225452423096\n",
      "Training loss for batch 8020 : 0.17041760683059692\n",
      "Training loss for batch 8021 : 0.04279499500989914\n",
      "Training loss for batch 8022 : 0.16024208068847656\n",
      "Training loss for batch 8023 : 0.2219162881374359\n",
      "Training loss for batch 8024 : 0.11720889806747437\n",
      "Training loss for batch 8025 : 0.061783429235219955\n",
      "Training loss for batch 8026 : 0.14679555594921112\n",
      "Training loss for batch 8027 : 0.04743615910410881\n",
      "Training loss for batch 8028 : 0.37256211042404175\n",
      "Training loss for batch 8029 : 0.04239671677350998\n",
      "Training loss for batch 8030 : 0.20506799221038818\n",
      "Training loss for batch 8031 : 0.0796908438205719\n",
      "Training loss for batch 8032 : 0.3873569369316101\n",
      "Training loss for batch 8033 : 0.29636770486831665\n",
      "Training loss for batch 8034 : 0.2915652394294739\n",
      "Training loss for batch 8035 : 0.2381729781627655\n",
      "Training loss for batch 8036 : 0.06181376799941063\n",
      "Training loss for batch 8037 : 0.10547566413879395\n",
      "Training loss for batch 8038 : 0.08096540719270706\n",
      "Training loss for batch 8039 : 0.038493555039167404\n",
      "Training loss for batch 8040 : 0.04203604534268379\n",
      "Training loss for batch 8041 : 0.11425021290779114\n",
      "Training loss for batch 8042 : 0.2690805196762085\n",
      "Training loss for batch 8043 : 0.07794541120529175\n",
      "Training loss for batch 8044 : 0.007377949543297291\n",
      "Training loss for batch 8045 : 0.22127887606620789\n",
      "Training loss for batch 8046 : 0.008376795798540115\n",
      "Training loss for batch 8047 : 0.3746200203895569\n",
      "Training loss for batch 8048 : 0.06588733196258545\n",
      "Training loss for batch 8049 : 0.24219784140586853\n",
      "Training loss for batch 8050 : 0.1759594827890396\n",
      "Training loss for batch 8051 : 0.09902554750442505\n",
      "Training loss for batch 8052 : 0.03814778849482536\n",
      "Training loss for batch 8053 : 0.040439359843730927\n",
      "Training loss for batch 8054 : 0.19432926177978516\n",
      "Training loss for batch 8055 : 0.014825046062469482\n",
      "Training loss for batch 8056 : 0.2292548418045044\n",
      "Training loss for batch 8057 : 0.09502477198839188\n",
      "Training loss for batch 8058 : 0.0733419880270958\n",
      "Training loss for batch 8059 : 0.25320127606391907\n",
      "Training loss for batch 8060 : -0.00032661223667673767\n",
      "Training loss for batch 8061 : 0.15025322139263153\n",
      "Training loss for batch 8062 : 0.02768327295780182\n",
      "Training loss for batch 8063 : 0.08178113400936127\n",
      "Training loss for batch 8064 : 0.031116154044866562\n",
      "Training loss for batch 8065 : 0.11140001565217972\n",
      "Training loss for batch 8066 : 0.08582532405853271\n",
      "Training loss for batch 8067 : 0.23071202635765076\n",
      "Training loss for batch 8068 : 0.04955894872546196\n",
      "Training loss for batch 8069 : 0.3762887120246887\n",
      "Training loss for batch 8070 : 0.32647261023521423\n",
      "Training loss for batch 8071 : 0.07220993936061859\n",
      "Training loss for batch 8072 : 0.19120997190475464\n",
      "Training loss for batch 8073 : 0.3411870002746582\n",
      "Training loss for batch 8074 : 0.07889985293149948\n",
      "Training loss for batch 8075 : 0.14058464765548706\n",
      "Training loss for batch 8076 : 0.04610113799571991\n",
      "Training loss for batch 8077 : 0.02375219389796257\n",
      "Training loss for batch 8078 : 0.04239353537559509\n",
      "Training loss for batch 8079 : 0.3381614089012146\n",
      "Training loss for batch 8080 : 0.08747769147157669\n",
      "Training loss for batch 8081 : 0.014235853217542171\n",
      "Training loss for batch 8082 : 0.07977978140115738\n",
      "Training loss for batch 8083 : 0.14656366407871246\n",
      "Training loss for batch 8084 : 0.2163650393486023\n",
      "Training loss for batch 8085 : 0.07772381603717804\n",
      "Training loss for batch 8086 : 0.022984083741903305\n",
      "Training loss for batch 8087 : 0.11601141095161438\n",
      "Training loss for batch 8088 : 0.05597495287656784\n",
      "Training loss for batch 8089 : 0.015285544097423553\n",
      "Training loss for batch 8090 : 0.004953592084348202\n",
      "Training loss for batch 8091 : 0.004534320440143347\n",
      "Training loss for batch 8092 : 0.22069098055362701\n",
      "Training loss for batch 8093 : 0.13983608782291412\n",
      "Training loss for batch 8094 : 0.14131149649620056\n",
      "Training loss for batch 8095 : 0.21829566359519958\n",
      "Training loss for batch 8096 : 0.15885567665100098\n",
      "Training loss for batch 8097 : 0.14259928464889526\n",
      "Training loss for batch 8098 : 0.03675563633441925\n",
      "Training loss for batch 8099 : 0.023034773766994476\n",
      "Training loss for batch 8100 : 0.045068807899951935\n",
      "Training loss for batch 8101 : 0.07407894730567932\n",
      "Training loss for batch 8102 : 0.04895075783133507\n",
      "Training loss for batch 8103 : 0.26113808155059814\n",
      "Training loss for batch 8104 : 0.22857549786567688\n",
      "Training loss for batch 8105 : 0.10527440160512924\n",
      "Training loss for batch 8106 : 0.27313950657844543\n",
      "Training loss for batch 8107 : 0.07340364158153534\n",
      "Training loss for batch 8108 : 0.3394347131252289\n",
      "Training loss for batch 8109 : 0.27385520935058594\n",
      "Training loss for batch 8110 : 0.06481979787349701\n",
      "Training loss for batch 8111 : 0.05172155797481537\n",
      "Training loss for batch 8112 : 0.008528292179107666\n",
      "Training loss for batch 8113 : 0.023348897695541382\n",
      "Training loss for batch 8114 : 0.1830255091190338\n",
      "Training loss for batch 8115 : 0.01020585186779499\n",
      "Training loss for batch 8116 : 0.19652843475341797\n",
      "Training loss for batch 8117 : 0.06264599412679672\n",
      "Training loss for batch 8118 : 0.009889457374811172\n",
      "Training loss for batch 8119 : 0.05946720764040947\n",
      "Training loss for batch 8120 : 0.22698619961738586\n",
      "Training loss for batch 8121 : 0.17864015698432922\n",
      "Training loss for batch 8122 : 0.026767196133732796\n",
      "Training loss for batch 8123 : 0.14889483153820038\n",
      "Training loss for batch 8124 : 0.22106307744979858\n",
      "Training loss for batch 8125 : 0.16144031286239624\n",
      "Training loss for batch 8126 : 0.16056080162525177\n",
      "Training loss for batch 8127 : 0.1591750681400299\n",
      "Training loss for batch 8128 : 0.10635804384946823\n",
      "Training loss for batch 8129 : 0.08030346035957336\n",
      "Training loss for batch 8130 : 0.06545210629701614\n",
      "Training loss for batch 8131 : 0.06341787427663803\n",
      "Training loss for batch 8132 : 0.20711955428123474\n",
      "Training loss for batch 8133 : 0.2576693296432495\n",
      "Training loss for batch 8134 : 0.14257840812206268\n",
      "Training loss for batch 8135 : 0.2707793414592743\n",
      "Training loss for batch 8136 : 0.057615019381046295\n",
      "Training loss for batch 8137 : 0.243296280503273\n",
      "Training loss for batch 8138 : 0.24959029257297516\n",
      "Training loss for batch 8139 : 0.01634342037141323\n",
      "Training loss for batch 8140 : 0.3281039595603943\n",
      "Training loss for batch 8141 : 0.01627431996166706\n",
      "Training loss for batch 8142 : 0.07366083562374115\n",
      "Training loss for batch 8143 : 0.09581708908081055\n",
      "Training loss for batch 8144 : 0.010298838838934898\n",
      "Training loss for batch 8145 : 0.27187222242355347\n",
      "Training loss for batch 8146 : 0.014793754555284977\n",
      "Training loss for batch 8147 : 0.015732288360595703\n",
      "Training loss for batch 8148 : 0.2554238736629486\n",
      "Training loss for batch 8149 : 0.02109965682029724\n",
      "Training loss for batch 8150 : 0.06209271401166916\n",
      "Training loss for batch 8151 : 0.06890927255153656\n",
      "Training loss for batch 8152 : 0.07026464492082596\n",
      "Training loss for batch 8153 : 0.23356260359287262\n",
      "Training loss for batch 8154 : 0.04131470248103142\n",
      "Training loss for batch 8155 : 0.18671293556690216\n",
      "Training loss for batch 8156 : 0.1900852918624878\n",
      "Training loss for batch 8157 : 0.3219969868659973\n",
      "Training loss for batch 8158 : 0.27059367299079895\n",
      "Training loss for batch 8159 : 0.2144327312707901\n",
      "Training loss for batch 8160 : 0.10082485526800156\n",
      "Training loss for batch 8161 : 0.19088619947433472\n",
      "Training loss for batch 8162 : 0.16846327483654022\n",
      "Training loss for batch 8163 : 0.04959606006741524\n",
      "Training loss for batch 8164 : 0.10566446185112\n",
      "Training loss for batch 8165 : 0.08225603401660919\n",
      "Training loss for batch 8166 : 0.2201170176267624\n",
      "Training loss for batch 8167 : 0.026689380407333374\n",
      "Training loss for batch 8168 : 0.012441175058484077\n",
      "Training loss for batch 8169 : 0.034812457859516144\n",
      "Training loss for batch 8170 : 0.08463543653488159\n",
      "Training loss for batch 8171 : 0.09905023872852325\n",
      "Training loss for batch 8172 : 0.05454317107796669\n",
      "Training loss for batch 8173 : 0.1265413463115692\n",
      "Training loss for batch 8174 : 0.1944544017314911\n",
      "Training loss for batch 8175 : 0.26821979880332947\n",
      "Training loss for batch 8176 : 0.028692934662103653\n",
      "Training loss for batch 8177 : 0.02689170092344284\n",
      "Training loss for batch 8178 : 0.13525661826133728\n",
      "Training loss for batch 8179 : 0.07140102982521057\n",
      "Training loss for batch 8180 : 0.10610470920801163\n",
      "Training loss for batch 8181 : 0.0306662879884243\n",
      "Training loss for batch 8182 : 0.06859508901834488\n",
      "Training loss for batch 8183 : 0.09850458055734634\n",
      "Training loss for batch 8184 : 0.15350249409675598\n",
      "Training loss for batch 8185 : 0.2679184079170227\n",
      "Training loss for batch 8186 : 0.15115395188331604\n",
      "Training loss for batch 8187 : 0.024242784827947617\n",
      "Training loss for batch 8188 : 0.1378222405910492\n",
      "Training loss for batch 8189 : 0.026860639452934265\n",
      "Training loss for batch 8190 : 0.07865829765796661\n",
      "Training loss for batch 8191 : 0.3675231337547302\n",
      "Training loss for batch 8192 : 0.0959571897983551\n",
      "Training loss for batch 8193 : 0.3357800543308258\n",
      "Training loss for batch 8194 : 0.09953013062477112\n",
      "Training loss for batch 8195 : 0.02898237109184265\n",
      "Training loss for batch 8196 : 0.2176354080438614\n",
      "Training loss for batch 8197 : 0.05015268921852112\n",
      "Training loss for batch 8198 : 0.038914985954761505\n",
      "Training loss for batch 8199 : 0.11678507924079895\n",
      "Training loss for batch 8200 : 0.04663350433111191\n",
      "Training loss for batch 8201 : 0.26933759450912476\n",
      "Training loss for batch 8202 : 0.2902846038341522\n",
      "Training loss for batch 8203 : 0.0350925475358963\n",
      "Training loss for batch 8204 : 0.04831947386264801\n",
      "Training loss for batch 8205 : 0.10302039980888367\n",
      "Training loss for batch 8206 : 0.03608887642621994\n",
      "Training loss for batch 8207 : 0.2978086471557617\n",
      "Training loss for batch 8208 : 0.2809467613697052\n",
      "Training loss for batch 8209 : 0.2032584249973297\n",
      "Training loss for batch 8210 : 0.05564829334616661\n",
      "Training loss for batch 8211 : 0.07047922909259796\n",
      "Training loss for batch 8212 : 0.013404304161667824\n",
      "Training loss for batch 8213 : 0.03622598946094513\n",
      "Training loss for batch 8214 : 0.07551683485507965\n",
      "Training loss for batch 8215 : 0.20277643203735352\n",
      "Training loss for batch 8216 : 0.08231441676616669\n",
      "Training loss for batch 8217 : 0.2140916883945465\n",
      "Training loss for batch 8218 : 0.16149801015853882\n",
      "Training loss for batch 8219 : 0.009361684322357178\n",
      "Training loss for batch 8220 : 0.014722916297614574\n",
      "Training loss for batch 8221 : 0.02194809541106224\n",
      "Training loss for batch 8222 : 0.12402477860450745\n",
      "Training loss for batch 8223 : 0.0\n",
      "Training loss for batch 8224 : 0.08818110078573227\n",
      "Training loss for batch 8225 : 0.21575927734375\n",
      "Training loss for batch 8226 : 0.2222486138343811\n",
      "Training loss for batch 8227 : 0.12374909222126007\n",
      "Training loss for batch 8228 : 0.0918508917093277\n",
      "Training loss for batch 8229 : 0.10564576089382172\n",
      "Training loss for batch 8230 : 0.5711926221847534\n",
      "Training loss for batch 8231 : 0.013936410658061504\n",
      "Training loss for batch 8232 : 0.12867942452430725\n",
      "Training loss for batch 8233 : 0.07105837762355804\n",
      "Training loss for batch 8234 : 0.2826565206050873\n",
      "Training loss for batch 8235 : 0.3486386835575104\n",
      "Training loss for batch 8236 : 0.33226853609085083\n",
      "Training loss for batch 8237 : 0.18907329440116882\n",
      "Training loss for batch 8238 : 0.20483224093914032\n",
      "Training loss for batch 8239 : 0.0808255523443222\n",
      "Training loss for batch 8240 : 0.1919054388999939\n",
      "Training loss for batch 8241 : 0.11910922080278397\n",
      "Training loss for batch 8242 : 0.036776699125766754\n",
      "Training loss for batch 8243 : 0.054882973432540894\n",
      "Training loss for batch 8244 : 0.2433745414018631\n",
      "Training loss for batch 8245 : 0.32816335558891296\n",
      "Training loss for batch 8246 : 0.11513716727495193\n",
      "Training loss for batch 8247 : 0.17110292613506317\n",
      "Training loss for batch 8248 : 0.48998382687568665\n",
      "Training loss for batch 8249 : 0.08016492426395416\n",
      "Training loss for batch 8250 : 0.007726060226559639\n",
      "Training loss for batch 8251 : 0.1791660189628601\n",
      "Training loss for batch 8252 : 0.040898047387599945\n",
      "Training loss for batch 8253 : 0.1390458196401596\n",
      "Training loss for batch 8254 : 0.13737478852272034\n",
      "Training loss for batch 8255 : 0.010238021612167358\n",
      "Training loss for batch 8256 : 0.18365374207496643\n",
      "Training loss for batch 8257 : 0.5024014711380005\n",
      "Training loss for batch 8258 : 0.04160222411155701\n",
      "Training loss for batch 8259 : 0.44225355982780457\n",
      "Training loss for batch 8260 : 0.30094072222709656\n",
      "Training loss for batch 8261 : 0.06690292060375214\n",
      "Training loss for batch 8262 : 0.3010903596878052\n",
      "Training loss for batch 8263 : 0.22511547803878784\n",
      "Training loss for batch 8264 : 0.0687454342842102\n",
      "Training loss for batch 8265 : 0.08821012079715729\n",
      "Training loss for batch 8266 : 0.11122378706932068\n",
      "Training loss for batch 8267 : 0.021686166524887085\n",
      "Training loss for batch 8268 : 0.03606300801038742\n",
      "Training loss for batch 8269 : 0.44686776399612427\n",
      "Training loss for batch 8270 : 0.2829754948616028\n",
      "Training loss for batch 8271 : 0.027319781482219696\n",
      "Training loss for batch 8272 : 0.1585777848958969\n",
      "Training loss for batch 8273 : 0.003848652122542262\n",
      "Training loss for batch 8274 : 0.21488219499588013\n",
      "Training loss for batch 8275 : 0.1763352006673813\n",
      "Training loss for batch 8276 : 0.051235876977443695\n",
      "Training loss for batch 8277 : 0.10403497517108917\n",
      "Training loss for batch 8278 : 0.046529240906238556\n",
      "Training loss for batch 8279 : 0.3776007890701294\n",
      "Training loss for batch 8280 : 0.027372052893042564\n",
      "Training loss for batch 8281 : 0.1701662838459015\n",
      "Training loss for batch 8282 : 0.15842971205711365\n",
      "Training loss for batch 8283 : 0.2914562225341797\n",
      "Training loss for batch 8284 : 0.22428962588310242\n",
      "Training loss for batch 8285 : 0.011014049872756004\n",
      "Training loss for batch 8286 : 0.00483841635286808\n",
      "Training loss for batch 8287 : 0.11443135142326355\n",
      "Training loss for batch 8288 : 0.03322545066475868\n",
      "Training loss for batch 8289 : 0.04510461911559105\n",
      "Training loss for batch 8290 : 0.07030405104160309\n",
      "Training loss for batch 8291 : 0.5241861343383789\n",
      "Training loss for batch 8292 : 0.2232157588005066\n",
      "Training loss for batch 8293 : 0.027792055159807205\n",
      "Training loss for batch 8294 : 0.13729609549045563\n",
      "Training loss for batch 8295 : 0.20243984460830688\n",
      "Training loss for batch 8296 : 0.10044422000646591\n",
      "Training loss for batch 8297 : 0.17385917901992798\n",
      "Training loss for batch 8298 : 0.19892986118793488\n",
      "Training loss for batch 8299 : 0.012289037927985191\n",
      "Training loss for batch 8300 : 0.17823877930641174\n",
      "Training loss for batch 8301 : 0.13292236626148224\n",
      "Training loss for batch 8302 : 0.24841362237930298\n",
      "Training loss for batch 8303 : 0.13573189079761505\n",
      "Training loss for batch 8304 : 0.1272227019071579\n",
      "Training loss for batch 8305 : 0.4079529643058777\n",
      "Training loss for batch 8306 : 0.04327856004238129\n",
      "Training loss for batch 8307 : 0.7385703921318054\n",
      "Parameter containing:\n",
      "tensor(-0.2614, device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3407)\n",
    "record = []\n",
    "model.train()\n",
    "for epoch in range(EPOCHES):\n",
    "    for fold in range(dataset.kfolds):\n",
    "        # training_dataset, validation_dataset = dataset.kfold_cross_validation(fold)\n",
    "        training_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "        # validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "        index = 0\n",
    "        for batch in training_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # Batch shape: (N, Anchor-Positive-Negative, C, H, W)\n",
    "            predictions = model.forward_features(batch)\n",
    "            training_loss = loss_function(predictions)\n",
    "            training_loss.backward()    \n",
    "            print(f\"Training loss for batch {index} : {training_loss}\")\n",
    "            record.append(training_loss)\n",
    "            index += 1\n",
    "            optimizer.step()\n",
    "        print(loss_function.regularizing_strength)\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'loss_state_dict': loss_function.state_dict(),\n",
    "        'loss': training_loss,\n",
    "        }, f'./Checkpoint/model_{epoch}.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m record \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m record]\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mplot(record)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'record' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "record = [x.cpu().detach() for x in record]\n",
    "plt.plot(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/49828 [00:02<58:03, 14.30it/s] \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.99 GiB total capacity; 10.68 GiB already allocated; 0 bytes free; 11.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(testloader):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Batch shape: (N, Anchor-Positive-Negative, C, H, W)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward_features(batch)\n\u001b[0;32m     12\u001b[0m     test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_function(predictions)\n\u001b[0;32m     13\u001b[0m test_loss \u001b[39m=\u001b[39m test_loss \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(validation_dataloader) \n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Desktop\\Projects\\AgeRecognition\\models.py:36\u001b[0m, in \u001b[0;36mViTAgeRecognizer.forward_features\u001b[1;34m(self, pairings)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pairings\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[0;32m     35\u001b[0m     anchor_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(pairings[:, \u001b[39m0\u001b[39m, :, :, :])\n\u001b[1;32m---> 36\u001b[0m     positive_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(pairings[:, \u001b[39m1\u001b[39;49m, :, :, :])\n\u001b[0;32m     37\u001b[0m     negative_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(pairings[:, \u001b[39m2\u001b[39m, :, :, :])\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([anchor_feat, positive_feat, negative_feat], \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Desktop\\Projects\\AgeRecognition\\models.py:28\u001b[0m, in \u001b[0;36mViTAgeRecognizer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[0;32m     29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(x)\n\u001b[0;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torchvision\\models\\vision_transformer.py:298\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    295\u001b[0m batch_class_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_token\u001b[39m.\u001b[39mexpand(n, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    296\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([batch_class_token, x], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    300\u001b[0m \u001b[39m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[0;32m    301\u001b[0m x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torchvision\\models\\vision_transformer.py:157\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m torch\u001b[39m.\u001b[39m_assert(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding\n\u001b[1;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m)))\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torchvision\\models\\vision_transformer.py:118\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    117\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)\n\u001b[1;32m--> 118\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(y)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m y\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\activation.py:685\u001b[0m, in \u001b[0;36mGELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgelu(\u001b[39minput\u001b[39;49m, approximate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapproximate)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.99 GiB total capacity; 10.68 GiB already allocated; 0 bytes free; 11.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "TEST_PAIRINGS = './test_data.csv'\n",
    "state_dict = torch.load('./Checkpoint/model_1.pt')\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "testset = AgeRecognitionDataset(triplet_csv_path=TEST_PAIRINGS, image_dir=IMAGE_DIR, preprocessor=preprocessor, kfolds=5, device=DEVICE)\n",
    "testloader  = DataLoader(testset, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for batch in tqdm(testloader):\n",
    "    # Batch shape: (N, Anchor-Positive-Negative, C, H, W)\n",
    "    predictions = model.forward_features(batch)\n",
    "    test_loss += loss_function(predictions)\n",
    "test_loss = test_loss * 8 / len(validation_dataloader) \n",
    "print(f\"Average test loss : {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agerec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
