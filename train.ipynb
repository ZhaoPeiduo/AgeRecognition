{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from preprocessor import AgeRecognitionPreprocessor\n",
    "from dataset import AgeRecognitionDataset\n",
    "from models import vit_l_16_age_recognizer, vit_b_16_age_recognizer, resent101_age_recogniser\n",
    "from loss import AgeRecognitionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "\n",
    "IMAGE_DIR = './Cleaned/'\n",
    "VARIANT = 'resnet101'\n",
    "TRAINING_PAIRINGS = './training_data.csv'\n",
    "BATCH_SIZE = 24\n",
    "EPOCHES = 1000\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resent101_age_recogniser().to(DEVICE)\n",
    "loss_function = AgeRecognitionLoss().to(DEVICE)\n",
    "# loss_function.importance.requires_grad = False\n",
    "preprocessor = AgeRecognitionPreprocessor()\n",
    "dataset = AgeRecognitionDataset(triplet_csv_path=TRAINING_PAIRINGS, image_dir=IMAGE_DIR, preprocessor=preprocessor, kfolds=1, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(list(model.parameters()) + list(loss_function.parameters()), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=EPOCHES, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'./Checkpoint/{VARIANT}/best.pt'):\n",
    "    best_state = torch.load(f'./Checkpoint/{VARIANT}/best.pt')\n",
    "    model.load_state_dict(best_state['model_state_dict'])\n",
    "    loss_function.load_state_dict(best_state['loss_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for batch 0 : 1.0000100135803223\n",
      "Training loss for batch 1 : 1.0000100135803223\n",
      "Training loss for batch 2 : 1.0000100135803223\n",
      "Training loss for batch 3 : 1.0000098943710327\n",
      "Training loss for batch 4 : 1.0000100135803223\n",
      "Training loss for batch 5 : 1.0000100135803223\n",
      "Training loss for batch 6 : 1.0000100135803223\n",
      "Training loss for batch 7 : 1.0000100135803223\n",
      "Training loss for batch 8 : 1.0000100135803223\n",
      "Training loss for batch 9 : 1.0000100135803223\n",
      "Training loss for batch 10 : 1.0000098943710327\n",
      "Training loss for batch 11 : 1.0000100135803223\n",
      "Training loss for batch 12 : 0.10966742783784866\n",
      "Training loss for batch 13 : 0.04650833457708359\n",
      "Training loss for batch 14 : 0.19805079698562622\n",
      "Training loss for batch 15 : 0.20085152983665466\n",
      "Training loss for batch 16 : 0.21650613844394684\n",
      "Training loss for batch 17 : 0.03596194460988045\n",
      "Training loss for batch 18 : 0.1458388715982437\n",
      "Training loss for batch 19 : 0.13189668953418732\n",
      "Training loss for batch 20 : 0.05603741481900215\n",
      "Training loss for batch 21 : 0.06487978249788284\n",
      "Training loss for batch 22 : 0.13999512791633606\n",
      "Training loss for batch 23 : 0.16985249519348145\n",
      "Training loss for batch 24 : 0.17294232547283173\n",
      "Training loss for batch 25 : 0.030311010777950287\n",
      "Training loss for batch 26 : 0.03518209233880043\n",
      "Training loss for batch 27 : 0.029641158878803253\n",
      "Training loss for batch 28 : 0.03157447651028633\n",
      "Training loss for batch 29 : 0.2691943049430847\n",
      "Training loss for batch 30 : 0.2546752393245697\n",
      "Training loss for batch 31 : 0.16122440993785858\n",
      "Training loss for batch 32 : 0.1121300458908081\n",
      "Training loss for batch 33 : 0.09140237420797348\n",
      "Training loss for batch 34 : 1.1176643965882249e-07\n",
      "Training loss for batch 35 : 0.12905734777450562\n",
      "Training loss for batch 36 : 0.15758459270000458\n",
      "Training loss for batch 37 : 0.26637086272239685\n",
      "Training loss for batch 38 : 0.04397854208946228\n",
      "Training loss for batch 39 : 0.08567307144403458\n",
      "Training loss for batch 40 : 0.02772943116724491\n",
      "Training loss for batch 41 : 0.11295827478170395\n",
      "Training loss for batch 42 : 0.06646043062210083\n",
      "Training loss for batch 43 : 0.1020539179444313\n",
      "Training loss for batch 44 : 0.2151908576488495\n",
      "Training loss for batch 45 : 0.06037255749106407\n",
      "Training loss for batch 46 : 0.15868854522705078\n",
      "Training loss for batch 47 : 0.1313936859369278\n",
      "Training loss for batch 48 : 0.1417820006608963\n",
      "Training loss for batch 49 : 0.111282117664814\n",
      "Training loss for batch 50 : 0.08922115713357925\n",
      "Training loss for batch 51 : 0.05243714526295662\n",
      "Training loss for batch 52 : 0.04850325360894203\n",
      "Training loss for batch 53 : 0.010696652345359325\n",
      "Training loss for batch 54 : 0.050674282014369965\n",
      "Training loss for batch 55 : 0.13378868997097015\n",
      "Training loss for batch 56 : 0.22749917209148407\n",
      "Training loss for batch 57 : 0.07047217339277267\n",
      "Training loss for batch 58 : 0.0708102285861969\n",
      "Training loss for batch 59 : 0.2218388170003891\n",
      "Training loss for batch 60 : 0.16563205420970917\n",
      "Training loss for batch 61 : 0.1579096019268036\n",
      "Training loss for batch 62 : 0.0874953418970108\n",
      "Training loss for batch 63 : 0.15059536695480347\n",
      "Training loss for batch 64 : 0.009144000709056854\n",
      "Training loss for batch 65 : 0.17775475978851318\n",
      "Training loss for batch 66 : 0.02396666444838047\n",
      "Training loss for batch 67 : 0.15604078769683838\n",
      "Training loss for batch 68 : 0.05747070908546448\n",
      "Training loss for batch 69 : 0.0769408643245697\n",
      "Training loss for batch 70 : 0.06699652969837189\n",
      "Training loss for batch 71 : 0.2654435634613037\n",
      "Training loss for batch 72 : 0.09370901435613632\n",
      "Training loss for batch 73 : 0.23220430314540863\n",
      "Training loss for batch 74 : 0.12296858429908752\n",
      "Training loss for batch 75 : 0.10733044892549515\n",
      "Training loss for batch 76 : 0.06456387042999268\n",
      "Training loss for batch 77 : 0.06926212459802628\n",
      "Training loss for batch 78 : 0.01647958531975746\n",
      "Training loss for batch 79 : 0.0835849791765213\n",
      "Training loss for batch 80 : 0.0860043540596962\n",
      "Training loss for batch 81 : 0.10959762334823608\n",
      "Training loss for batch 82 : 0.18611636757850647\n",
      "Training loss for batch 83 : 0.06667637079954147\n",
      "Training loss for batch 84 : 0.23704345524311066\n",
      "Training loss for batch 85 : 0.37682580947875977\n",
      "Training loss for batch 86 : 0.1119028851389885\n",
      "Training loss for batch 87 : 0.2413102239370346\n",
      "Training loss for batch 88 : 0.11281834542751312\n",
      "Training loss for batch 89 : 0.006953240837901831\n",
      "Training loss for batch 90 : 0.03534175455570221\n",
      "Training loss for batch 91 : 0.30490535497665405\n",
      "Training loss for batch 92 : 0.12370286136865616\n",
      "Training loss for batch 93 : 0.11784128844738007\n",
      "Training loss for batch 94 : 0.1401270627975464\n",
      "Training loss for batch 95 : 0.16276252269744873\n",
      "Training loss for batch 96 : 0.19432085752487183\n",
      "Training loss for batch 97 : 0.22795701026916504\n",
      "Training loss for batch 98 : 0.08012466132640839\n",
      "Training loss for batch 99 : 0.16669915616512299\n",
      "Training loss for batch 100 : 0.04202937334775925\n",
      "Training loss for batch 101 : 0.028670193627476692\n",
      "Training loss for batch 102 : 0.05415857955813408\n",
      "Training loss for batch 103 : 0.2063119113445282\n",
      "Training loss for batch 104 : 0.10377711057662964\n",
      "Training loss for batch 105 : 0.052248675376176834\n",
      "Training loss for batch 106 : 0.24460959434509277\n",
      "Training loss for batch 107 : 0.00662097055464983\n",
      "Training loss for batch 108 : 0.039103880524635315\n",
      "Training loss for batch 109 : 0.012336183339357376\n",
      "Training loss for batch 110 : 0.12142593413591385\n",
      "Training loss for batch 111 : 0.11540722101926804\n",
      "Training loss for batch 112 : 0.03196151554584503\n",
      "Training loss for batch 113 : 0.17541687190532684\n",
      "Training loss for batch 114 : 0.1236446350812912\n",
      "Training loss for batch 115 : 0.2405652403831482\n",
      "Training loss for batch 116 : 0.09264575690031052\n",
      "Training loss for batch 117 : 0.1017283946275711\n",
      "Training loss for batch 118 : 0.10384256392717361\n",
      "Training loss for batch 119 : 0.04631834849715233\n",
      "Training loss for batch 120 : 0.18208250403404236\n",
      "Training loss for batch 121 : 0.06988121569156647\n",
      "Training loss for batch 122 : 0.30575722455978394\n",
      "Training loss for batch 123 : 0.29679447412490845\n",
      "Training loss for batch 124 : 0.07869716733694077\n",
      "Training loss for batch 125 : 0.12166737765073776\n",
      "Training loss for batch 126 : 0.481134295463562\n",
      "Training loss for batch 127 : 0.12248553335666656\n",
      "Training loss for batch 128 : 0.047462914139032364\n",
      "Training loss for batch 129 : 0.1473313868045807\n",
      "Training loss for batch 130 : 0.03961169719696045\n",
      "Training loss for batch 131 : 0.17125806212425232\n",
      "Training loss for batch 132 : 0.02231472358107567\n",
      "Training loss for batch 133 : 0.02959848754107952\n",
      "Training loss for batch 134 : 0.11022111028432846\n",
      "Training loss for batch 135 : 0.1381155103445053\n",
      "Training loss for batch 136 : 0.2567882835865021\n",
      "Training loss for batch 137 : 0.11794190108776093\n",
      "Training loss for batch 138 : 0.20594552159309387\n",
      "Training loss for batch 139 : 0.07800957560539246\n",
      "Training loss for batch 140 : 0.09845572710037231\n",
      "Training loss for batch 141 : 0.19144172966480255\n",
      "Training loss for batch 142 : 0.15041978657245636\n",
      "Training loss for batch 143 : 0.18023622035980225\n",
      "Training loss for batch 144 : 0.06950327008962631\n",
      "Training loss for batch 145 : 0.08916323632001877\n",
      "Training loss for batch 146 : 0.12736202776432037\n",
      "Training loss for batch 147 : 0.07292511314153671\n",
      "Training loss for batch 148 : 0.062360864132642746\n",
      "Training loss for batch 149 : 0.06363137811422348\n",
      "Training loss for batch 150 : 0.05474003031849861\n",
      "Training loss for batch 151 : 0.19315271079540253\n",
      "Training loss for batch 152 : 0.05692202225327492\n",
      "Training loss for batch 153 : 0.04324408993124962\n",
      "Training loss for batch 154 : 0.13708969950675964\n",
      "Training loss for batch 155 : 0.05407775565981865\n",
      "Training loss for batch 156 : 0.03560595214366913\n",
      "Training loss for batch 157 : 0.07418845593929291\n",
      "Training loss for batch 158 : 0.08857249468564987\n",
      "Training loss for batch 159 : 0.1545691043138504\n",
      "Training loss for batch 160 : 0.1573685258626938\n",
      "Training loss for batch 161 : 0.21587452292442322\n",
      "Training loss for batch 162 : 0.3370206654071808\n",
      "Training loss for batch 163 : 0.002504955278709531\n",
      "Training loss for batch 164 : 0.09338827431201935\n",
      "Training loss for batch 165 : 0.10191845148801804\n",
      "Training loss for batch 166 : 0.2604413628578186\n",
      "Training loss for batch 167 : 0.06317456811666489\n",
      "Training loss for batch 168 : 0.15363164246082306\n",
      "Training loss for batch 169 : 0.2270997166633606\n",
      "Training loss for batch 170 : 0.052877362817525864\n",
      "Training loss for batch 171 : 0.3393310010433197\n",
      "Training loss for batch 172 : 0.033212434500455856\n",
      "Training loss for batch 173 : 0.16626133024692535\n",
      "Training loss for batch 174 : 0.1247251033782959\n",
      "Training loss for batch 175 : 0.11899438500404358\n",
      "Training loss for batch 176 : 0.0232665054500103\n",
      "Training loss for batch 177 : 0.10821502655744553\n",
      "Training loss for batch 178 : 0.09537098556756973\n",
      "Training loss for batch 179 : 0.06910624355077744\n",
      "Training loss for batch 180 : 0.14069736003875732\n",
      "Training loss for batch 181 : 0.16218788921833038\n",
      "Training loss for batch 182 : 0.09650366008281708\n",
      "Training loss for batch 183 : 0.011765224859118462\n",
      "Training loss for batch 184 : 0.06897781044244766\n",
      "Training loss for batch 185 : 0.13659581542015076\n",
      "Training loss for batch 186 : 0.08255163580179214\n",
      "Training loss for batch 187 : 0.027093062177300453\n",
      "Training loss for batch 188 : 0.08069809526205063\n",
      "Training loss for batch 189 : 0.15394814312458038\n",
      "Training loss for batch 190 : 0.06902431696653366\n",
      "Training loss for batch 191 : 0.03125562146306038\n",
      "Training loss for batch 192 : 0.15550434589385986\n",
      "Training loss for batch 193 : 0.14255256950855255\n",
      "Training loss for batch 194 : 0.05776127427816391\n",
      "Training loss for batch 195 : 0.02046123892068863\n",
      "Training loss for batch 196 : 0.13953295350074768\n",
      "Training loss for batch 197 : 0.20442987978458405\n",
      "Training loss for batch 198 : 0.10278237611055374\n",
      "Training loss for batch 199 : 0.12408195436000824\n",
      "Training loss for batch 200 : 0.05137854069471359\n",
      "Training loss for batch 201 : 0.15327192842960358\n",
      "Training loss for batch 202 : 0.15866747498512268\n",
      "Training loss for batch 203 : 0.0582270585000515\n",
      "Training loss for batch 204 : 0.24448858201503754\n",
      "Training loss for batch 205 : 0.07115889340639114\n",
      "Training loss for batch 206 : 0.09841222316026688\n",
      "Training loss for batch 207 : 0.08940988779067993\n",
      "Training loss for batch 208 : 0.06132049113512039\n",
      "Training loss for batch 209 : 0.014663612470030785\n",
      "Training loss for batch 210 : 0.22545292973518372\n",
      "Training loss for batch 211 : 0.09743237495422363\n",
      "Training loss for batch 212 : 0.07570192217826843\n",
      "Training loss for batch 213 : 0.1170734241604805\n",
      "Training loss for batch 214 : 0.160841703414917\n",
      "Training loss for batch 215 : 0.06098277121782303\n",
      "Training loss for batch 216 : 0.1300143152475357\n",
      "Training loss for batch 217 : 0.010592212900519371\n",
      "Training loss for batch 218 : 0.04457720369100571\n",
      "Training loss for batch 219 : 0.31737157702445984\n",
      "Training loss for batch 220 : 0.1755312979221344\n",
      "Training loss for batch 221 : 0.17359210550785065\n",
      "Training loss for batch 222 : 0.09495822340250015\n",
      "Training loss for batch 223 : 0.19208486378192902\n",
      "Training loss for batch 224 : 0.020331496372818947\n",
      "Training loss for batch 225 : 0.03414422273635864\n",
      "Training loss for batch 226 : 0.09026265889406204\n",
      "Training loss for batch 227 : 0.0644180104136467\n",
      "Training loss for batch 228 : 0.15895231068134308\n",
      "Training loss for batch 229 : 0.07530844211578369\n",
      "Training loss for batch 230 : 0.08203119039535522\n",
      "Training loss for batch 231 : 0.15566210448741913\n",
      "Training loss for batch 232 : 0.1172529086470604\n",
      "Training loss for batch 233 : 0.21318432688713074\n",
      "Training loss for batch 234 : 0.020726554095745087\n",
      "Training loss for batch 235 : 0.1150185689330101\n",
      "Training loss for batch 236 : 0.10079646855592728\n",
      "Training loss for batch 237 : 0.10409224778413773\n",
      "Training loss for batch 238 : 0.17655998468399048\n",
      "Training loss for batch 239 : 0.2228834182024002\n",
      "Training loss for batch 240 : 0.06204557046294212\n",
      "Training loss for batch 241 : 0.25665563344955444\n",
      "Training loss for batch 242 : 0.12594076991081238\n",
      "Training loss for batch 243 : 0.04496758058667183\n",
      "Training loss for batch 244 : 0.018314339220523834\n",
      "Training loss for batch 245 : 0.09873772412538528\n",
      "Training loss for batch 246 : 0.047167785465717316\n",
      "Training loss for batch 247 : 0.06942431628704071\n",
      "Training loss for batch 248 : 0.17648716270923615\n",
      "Training loss for batch 249 : 0.03966585919260979\n",
      "Training loss for batch 250 : 0.048306144773960114\n",
      "Training loss for batch 251 : 0.15896490216255188\n",
      "Training loss for batch 252 : 0.08268049359321594\n",
      "Training loss for batch 253 : 0.01357482559978962\n",
      "Training loss for batch 254 : 0.08861111104488373\n",
      "Training loss for batch 255 : 0.06512895971536636\n",
      "Training loss for batch 256 : 0.1787450909614563\n",
      "Training loss for batch 257 : 0.1645628660917282\n",
      "Training loss for batch 258 : 0.23656857013702393\n",
      "Training loss for batch 259 : 0.22932890057563782\n",
      "Training loss for batch 260 : 0.06867656111717224\n",
      "Training loss for batch 261 : 0.0981302335858345\n",
      "Training loss for batch 262 : 0.07315186411142349\n",
      "Training loss for batch 263 : 0.019733481109142303\n",
      "Training loss for batch 264 : 0.2236165553331375\n",
      "Training loss for batch 265 : 0.1825346201658249\n",
      "Training loss for batch 266 : 0.04539477825164795\n",
      "Training loss for batch 267 : 0.07155413925647736\n",
      "Training loss for batch 268 : 0.021044135093688965\n",
      "Training loss for batch 269 : 0.27867579460144043\n",
      "Training loss for batch 270 : 0.010211925022304058\n",
      "Training loss for batch 271 : 0.17467549443244934\n",
      "Training loss for batch 272 : 0.1154390498995781\n",
      "Training loss for batch 273 : 0.0500546395778656\n",
      "Training loss for batch 274 : 0.1574048101902008\n",
      "Training loss for batch 275 : 0.14795199036598206\n",
      "Training loss for batch 276 : 0.029534300789237022\n",
      "Training loss for batch 277 : 0.22094811499118805\n",
      "Training loss for batch 278 : 0.14528922736644745\n",
      "Training loss for batch 279 : 0.015772001817822456\n",
      "Training loss for batch 280 : 0.07185576856136322\n",
      "Training loss for batch 281 : 0.02531580813229084\n",
      "Training loss for batch 282 : 0.20291508734226227\n",
      "Training loss for batch 283 : 0.11914239078760147\n",
      "Training loss for batch 284 : 0.21709345281124115\n",
      "Training loss for batch 285 : 0.11838960647583008\n",
      "Training loss for batch 286 : 0.10616252571344376\n",
      "Training loss for batch 287 : 0.028727799654006958\n",
      "Training loss for batch 288 : 0.04512178152799606\n",
      "Training loss for batch 289 : 0.12113432586193085\n",
      "Training loss for batch 290 : 0.0840907022356987\n",
      "Training loss for batch 291 : 0.1606409251689911\n",
      "Training loss for batch 292 : 0.09823700040578842\n",
      "Training loss for batch 293 : 0.06979429721832275\n",
      "Training loss for batch 294 : 0.06967425346374512\n",
      "Training loss for batch 295 : 0.02735733985900879\n",
      "Training loss for batch 296 : 0.050982967019081116\n",
      "Training loss for batch 297 : 0.1687871664762497\n",
      "Training loss for batch 298 : 0.21354524791240692\n",
      "Training loss for batch 299 : 0.11180602014064789\n",
      "Training loss for batch 300 : 0.049067992717027664\n",
      "Training loss for batch 301 : 0.19195656478405\n",
      "Training loss for batch 302 : 0.025922169908881187\n",
      "Training loss for batch 303 : 0.04962393641471863\n",
      "Training loss for batch 304 : 0.047352731227874756\n",
      "Training loss for batch 305 : 0.17098405957221985\n",
      "Training loss for batch 306 : 0.0702691599726677\n",
      "Training loss for batch 307 : 0.08374784141778946\n",
      "Training loss for batch 308 : 0.06774578988552094\n",
      "Training loss for batch 309 : 0.15432266891002655\n",
      "Training loss for batch 310 : 0.0414169579744339\n",
      "Training loss for batch 311 : 0.24752649664878845\n",
      "Training loss for batch 312 : 0.07323413342237473\n",
      "Training loss for batch 313 : 0.04241868853569031\n",
      "Training loss for batch 314 : 0.05044403299689293\n",
      "Training loss for batch 315 : 0.2266806811094284\n",
      "Training loss for batch 316 : 0.16903406381607056\n",
      "Training loss for batch 317 : 0.07464978098869324\n",
      "Training loss for batch 318 : 0.03412272408604622\n",
      "Training loss for batch 319 : 0.10131052881479263\n",
      "Training loss for batch 320 : 0.10127198696136475\n",
      "Training loss for batch 321 : 0.09591805189847946\n",
      "Training loss for batch 322 : 0.09124622493982315\n",
      "Training loss for batch 323 : 0.10406368970870972\n",
      "Training loss for batch 324 : 0.19583284854888916\n",
      "Training loss for batch 325 : 0.15486283600330353\n",
      "Training loss for batch 326 : 0.044616974890232086\n",
      "Training loss for batch 327 : 0.12853460013866425\n",
      "Training loss for batch 328 : 0.015142394229769707\n",
      "Training loss for batch 329 : 0.13187043368816376\n",
      "Training loss for batch 330 : 0.12540456652641296\n",
      "Training loss for batch 331 : 0.00653352914378047\n",
      "Training loss for batch 332 : 0.030221903696656227\n",
      "Training loss for batch 333 : 0.0902499333024025\n",
      "Training loss for batch 334 : 0.01433867309242487\n",
      "Training loss for batch 335 : 0.24263320863246918\n",
      "Training loss for batch 336 : 0.11398212611675262\n",
      "Training loss for batch 337 : 0.11448543518781662\n",
      "Training loss for batch 338 : 0.016447611153125763\n",
      "Training loss for batch 339 : 0.19177886843681335\n",
      "Training loss for batch 340 : 0.24635368585586548\n",
      "Training loss for batch 341 : 0.18376053869724274\n",
      "Training loss for batch 342 : 0.15097859501838684\n",
      "Training loss for batch 343 : 0.12867876887321472\n",
      "Training loss for batch 344 : 0.06221303343772888\n",
      "Training loss for batch 345 : 0.03276246786117554\n",
      "Training loss for batch 346 : 0.10760869085788727\n",
      "Training loss for batch 347 : 0.14967162907123566\n",
      "Training loss for batch 348 : 0.011274794116616249\n",
      "Training loss for batch 349 : 0.012241031043231487\n",
      "Training loss for batch 350 : 0.2144922912120819\n",
      "Training loss for batch 351 : 0.20267096161842346\n",
      "Training loss for batch 352 : 0.015825992450118065\n",
      "Training loss for batch 353 : 0.01171809621155262\n",
      "Training loss for batch 354 : 0.13308556377887726\n",
      "Training loss for batch 355 : 0.02751167118549347\n",
      "Training loss for batch 356 : 0.196223646402359\n",
      "Training loss for batch 357 : 0.08312574774026871\n",
      "Training loss for batch 358 : 0.15317869186401367\n",
      "Training loss for batch 359 : 0.00522862933576107\n",
      "Training loss for batch 360 : 0.21557709574699402\n",
      "Training loss for batch 361 : 0.07041646540164948\n",
      "Training loss for batch 362 : 0.031295545399188995\n",
      "Training loss for batch 363 : 0.23758675158023834\n",
      "Training loss for batch 364 : 0.03905131295323372\n",
      "Training loss for batch 365 : 0.0859273225069046\n",
      "Training loss for batch 366 : 0.28806591033935547\n",
      "Training loss for batch 367 : 0.14270269870758057\n",
      "Training loss for batch 368 : 0.14832469820976257\n",
      "Training loss for batch 369 : 0.06701704859733582\n",
      "Training loss for batch 370 : 0.03429020568728447\n",
      "Training loss for batch 371 : 0.14060257375240326\n",
      "Training loss for batch 372 : 0.10677975416183472\n",
      "Training loss for batch 373 : 0.15892714262008667\n",
      "Training loss for batch 374 : 0.10735908895730972\n",
      "Training loss for batch 375 : 0.15292540192604065\n",
      "Training loss for batch 376 : 0.43677210807800293\n",
      "Training loss for batch 377 : 0.07570201903581619\n",
      "Training loss for batch 378 : 0.007754362653940916\n",
      "Training loss for batch 379 : 0.14377877116203308\n",
      "Training loss for batch 380 : 0.19777143001556396\n",
      "Training loss for batch 381 : 0.1901329904794693\n",
      "Training loss for batch 382 : 0.1651075780391693\n",
      "Training loss for batch 383 : 0.11238836497068405\n",
      "Training loss for batch 384 : 0.17272089421749115\n",
      "Training loss for batch 385 : 0.11545773595571518\n",
      "Training loss for batch 386 : 0.024853408336639404\n",
      "Training loss for batch 387 : 0.09024997800588608\n",
      "Training loss for batch 388 : 0.1919298619031906\n",
      "Training loss for batch 389 : 0.04394334554672241\n",
      "Training loss for batch 390 : 0.1734796017408371\n",
      "Training loss for batch 391 : 0.20465098321437836\n",
      "Training loss for batch 392 : 0.07693637907505035\n",
      "Training loss for batch 393 : 0.1732887476682663\n",
      "Training loss for batch 394 : 0.08633727580308914\n",
      "Training loss for batch 395 : 0.16408076882362366\n",
      "Training loss for batch 396 : 0.06254693865776062\n",
      "Training loss for batch 397 : 0.05127069354057312\n",
      "Training loss for batch 398 : 0.07047167420387268\n",
      "Training loss for batch 399 : 0.00800531730055809\n",
      "Training loss for batch 400 : 0.061560533940792084\n",
      "Training loss for batch 401 : 0.12090007960796356\n",
      "Training loss for batch 402 : 0.07399507611989975\n",
      "Training loss for batch 403 : 0.07477429509162903\n",
      "Training loss for batch 404 : 0.0076518431305885315\n",
      "Training loss for batch 405 : 0.35824069380760193\n",
      "Training loss for batch 406 : 0.053453803062438965\n",
      "Training loss for batch 407 : 0.10745818167924881\n",
      "Training loss for batch 408 : 0.16318047046661377\n",
      "Training loss for batch 409 : 0.12213194370269775\n",
      "Training loss for batch 410 : 0.09793262928724289\n",
      "Training loss for batch 411 : 0.21713145077228546\n",
      "Training loss for batch 412 : 0.04194020479917526\n",
      "Training loss for batch 413 : 0.03003103844821453\n",
      "Training loss for batch 414 : 0.04064524173736572\n",
      "Training loss for batch 415 : 0.15055789053440094\n",
      "Training loss for batch 416 : 0.11841100454330444\n",
      "Training loss for batch 417 : 0.21480503678321838\n",
      "Training loss for batch 418 : 0.1106443777680397\n",
      "Training loss for batch 419 : 0.08167872577905655\n",
      "Training loss for batch 420 : 0.1114930808544159\n",
      "Training loss for batch 421 : 0.06141534447669983\n",
      "Training loss for batch 422 : 0.05304653197526932\n",
      "Training loss for batch 423 : 0.08755634725093842\n",
      "Training loss for batch 424 : 0.011729158461093903\n",
      "Training loss for batch 425 : 0.027928145602345467\n",
      "Training loss for batch 426 : 0.0008769683772698045\n",
      "Training loss for batch 427 : 0.1222483217716217\n",
      "Training loss for batch 428 : 0.08333361148834229\n",
      "Training loss for batch 429 : 0.06987731158733368\n",
      "Training loss for batch 430 : 0.20341293513774872\n",
      "Training loss for batch 431 : 0.06498624384403229\n",
      "Training loss for batch 432 : 0.07314936071634293\n",
      "Training loss for batch 433 : 0.10467486083507538\n",
      "Training loss for batch 434 : 0.16103127598762512\n",
      "Training loss for batch 435 : 0.3238726258277893\n",
      "Training loss for batch 436 : 0.07176093757152557\n",
      "Training loss for batch 437 : 0.2692769169807434\n",
      "Training loss for batch 438 : 0.15796174108982086\n",
      "Training loss for batch 439 : 0.1343657523393631\n",
      "Training loss for batch 440 : 0.02111387439072132\n",
      "Training loss for batch 441 : 0.17861130833625793\n",
      "Training loss for batch 442 : 0.024504372850060463\n",
      "Training loss for batch 443 : 0.033495575189590454\n",
      "Training loss for batch 444 : 0.0007048406987451017\n",
      "Training loss for batch 445 : 0.010664796456694603\n",
      "Training loss for batch 446 : 0.1215423196554184\n",
      "Training loss for batch 447 : 0.09364030510187149\n",
      "Training loss for batch 448 : 0.29733702540397644\n",
      "Training loss for batch 449 : 0.026140933856368065\n",
      "Training loss for batch 450 : 0.21189182996749878\n",
      "Training loss for batch 451 : 0.055845074355602264\n",
      "Training loss for batch 452 : 0.1343126893043518\n",
      "Training loss for batch 453 : 0.0633244439959526\n",
      "Training loss for batch 454 : 0.14316794276237488\n",
      "Training loss for batch 455 : 0.2467309981584549\n",
      "Training loss for batch 456 : 0.08663608133792877\n",
      "Training loss for batch 457 : 0.019999705255031586\n",
      "Training loss for batch 458 : 0.1203644722700119\n",
      "Training loss for batch 459 : 0.07291360944509506\n",
      "Training loss for batch 460 : 0.07470058649778366\n",
      "Training loss for batch 461 : 0.20213694870471954\n",
      "Training loss for batch 462 : 0.03543592244386673\n",
      "Training loss for batch 463 : 0.15022528171539307\n",
      "Training loss for batch 464 : 0.19284893572330475\n",
      "Training loss for batch 465 : 0.30932605266571045\n",
      "Training loss for batch 466 : 0.13881652057170868\n",
      "Training loss for batch 467 : 0.049894366413354874\n",
      "Training loss for batch 468 : 0.2850050926208496\n",
      "Training loss for batch 469 : 0.08130738884210587\n",
      "Training loss for batch 470 : 0.2560836970806122\n",
      "Training loss for batch 471 : 0.06884122639894485\n",
      "Training loss for batch 472 : 0.1323520392179489\n",
      "Training loss for batch 473 : 0.036393921822309494\n",
      "Training loss for batch 474 : 0.08254865556955338\n",
      "Training loss for batch 475 : 0.06388676911592484\n",
      "Training loss for batch 476 : 0.11790499091148376\n",
      "Training loss for batch 477 : 0.04789009317755699\n",
      "Training loss for batch 478 : 0.2023560106754303\n",
      "Training loss for batch 479 : 0.022976821288466454\n",
      "Training loss for batch 480 : 0.07797956466674805\n",
      "Training loss for batch 481 : 0.038041070103645325\n",
      "Training loss for batch 482 : 0.1372179090976715\n",
      "Training loss for batch 483 : 0.14030003547668457\n",
      "Training loss for batch 484 : 0.025013241916894913\n",
      "Training loss for batch 485 : 0.07066308706998825\n",
      "Training loss for batch 486 : 0.08546456694602966\n",
      "Training loss for batch 487 : 0.050423573702573776\n",
      "Training loss for batch 488 : 0.1289805918931961\n",
      "Training loss for batch 489 : 0.0710396021604538\n",
      "Training loss for batch 490 : 0.06515923142433167\n",
      "Training loss for batch 491 : 0.2798137068748474\n",
      "Training loss for batch 492 : 0.07345450669527054\n",
      "Training loss for batch 493 : 0.17020756006240845\n",
      "Training loss for batch 494 : 0.25107964873313904\n",
      "Training loss for batch 495 : 0.06423559784889221\n",
      "Training loss for batch 496 : 0.14015483856201172\n",
      "Training loss for batch 497 : 0.02439248375594616\n",
      "Training loss for batch 498 : 0.045454878360033035\n",
      "Training loss for batch 499 : 0.1518522948026657\n",
      "Training loss for batch 500 : 0.14145946502685547\n",
      "Training loss for batch 501 : 0.048607200384140015\n",
      "Training loss for batch 502 : 0.04748959094285965\n",
      "Training loss for batch 503 : 0.13541445136070251\n",
      "Training loss for batch 504 : 0.11281842738389969\n",
      "Training loss for batch 505 : 0.10116980969905853\n",
      "Training loss for batch 506 : 0.044402509927749634\n",
      "Training loss for batch 507 : 0.19846071302890778\n",
      "Training loss for batch 508 : 0.1070113331079483\n",
      "Training loss for batch 509 : 0.0942082554101944\n",
      "Training loss for batch 510 : 0.3407873511314392\n",
      "Training loss for batch 511 : 0.020807184278964996\n",
      "Training loss for batch 512 : 0.004463347140699625\n",
      "Training loss for batch 513 : 0.17367292940616608\n",
      "Training loss for batch 514 : 0.15874560177326202\n",
      "Training loss for batch 515 : 0.2813502252101898\n",
      "Training loss for batch 516 : 0.33555132150650024\n",
      "Training loss for batch 517 : 0.28168100118637085\n",
      "Training loss for batch 518 : 0.14507345855236053\n",
      "Training loss for batch 519 : 0.041499897837638855\n",
      "Training loss for batch 520 : 0.09998831897974014\n",
      "Training loss for batch 521 : 0.0016248398460447788\n",
      "Training loss for batch 522 : 0.1712215691804886\n",
      "Training loss for batch 523 : 0.09933669865131378\n",
      "Training loss for batch 524 : 0.13172785937786102\n",
      "Training loss for batch 525 : 0.07669532299041748\n",
      "Training loss for batch 526 : 0.12755104899406433\n",
      "Training loss for batch 527 : 0.023273184895515442\n",
      "Training loss for batch 528 : 0.1077127680182457\n",
      "Training loss for batch 529 : 0.01005633920431137\n",
      "Training loss for batch 530 : 0.08434566855430603\n",
      "Training loss for batch 531 : 0.22987774014472961\n",
      "Training loss for batch 532 : 0.047683604061603546\n",
      "Training loss for batch 533 : 0.11492238193750381\n",
      "Training loss for batch 534 : 0.11147110909223557\n",
      "Training loss for batch 535 : 0.09021949023008347\n",
      "Training loss for batch 536 : 0.18826234340667725\n",
      "Training loss for batch 537 : 0.047304894775152206\n",
      "Training loss for batch 538 : 0.1730833500623703\n",
      "Training loss for batch 539 : 0.20569752156734467\n",
      "Training loss for batch 540 : 0.2218134105205536\n",
      "Training loss for batch 541 : 0.022927166894078255\n",
      "Training loss for batch 542 : 0.03188098222017288\n",
      "Training loss for batch 543 : 0.15513090789318085\n",
      "Training loss for batch 544 : 0.17909513413906097\n",
      "Training loss for batch 545 : 0.21114440262317657\n",
      "Training loss for batch 546 : 0.04034098982810974\n",
      "Training loss for batch 547 : 0.07957577705383301\n",
      "Training loss for batch 548 : 0.07772597670555115\n",
      "Training loss for batch 549 : 0.017700333148241043\n",
      "Training loss for batch 550 : 0.06187516450881958\n",
      "Training loss for batch 551 : 0.0280180461704731\n",
      "Training loss for batch 552 : 0.08092481642961502\n",
      "Training loss for batch 553 : 0.27161845564842224\n",
      "Training loss for batch 554 : 0.2738628387451172\n",
      "Training loss for batch 555 : 0.011789621785283089\n",
      "Training loss for batch 556 : 0.09790695458650589\n",
      "Training loss for batch 557 : 0.10259027034044266\n",
      "Training loss for batch 558 : 0.0681341364979744\n",
      "Training loss for batch 559 : 0.19907346367835999\n",
      "Training loss for batch 560 : 0.13676871359348297\n",
      "Training loss for batch 561 : 0.19887256622314453\n",
      "Training loss for batch 562 : 0.03284217789769173\n",
      "Training loss for batch 563 : 0.1364472210407257\n",
      "Training loss for batch 564 : 0.14623337984085083\n",
      "Training loss for batch 565 : 0.15102627873420715\n",
      "Training loss for batch 566 : 0.09629999846220016\n",
      "Training loss for batch 567 : 0.08290570229291916\n",
      "Training loss for batch 568 : 0.10395541787147522\n",
      "Training loss for batch 569 : 0.042310092598199844\n",
      "Training loss for batch 570 : 0.09139268100261688\n",
      "Training loss for batch 571 : 0.02064044214785099\n",
      "Training loss for batch 572 : 0.10506623238325119\n",
      "Training loss for batch 573 : 0.07576533406972885\n",
      "Training loss for batch 574 : 0.10748652368783951\n",
      "Training loss for batch 575 : 0.1893729716539383\n",
      "Training loss for batch 576 : 0.07682029902935028\n",
      "Training loss for batch 577 : 0.20898595452308655\n",
      "Training loss for batch 578 : 0.11472871154546738\n",
      "Training loss for batch 579 : 0.22150138020515442\n",
      "Training loss for batch 580 : 0.05925160273909569\n",
      "Training loss for batch 581 : 0.008504930883646011\n",
      "Training loss for batch 582 : 0.06236259266734123\n",
      "Training loss for batch 583 : 0.12648510932922363\n",
      "Training loss for batch 584 : 7.385212796862106e-08\n",
      "Training loss for batch 585 : 0.06532398611307144\n",
      "Training loss for batch 586 : 0.05636805295944214\n",
      "Training loss for batch 587 : 0.05347451567649841\n",
      "Training loss for batch 588 : 0.020823262631893158\n",
      "Training loss for batch 589 : 0.048381686210632324\n",
      "Training loss for batch 590 : 0.12645326554775238\n",
      "Training loss for batch 591 : 0.13112251460552216\n",
      "Training loss for batch 592 : 0.04425131529569626\n",
      "Training loss for batch 593 : 0.03706815838813782\n",
      "Training loss for batch 594 : 0.20498697459697723\n",
      "Training loss for batch 595 : 0.15333674848079681\n",
      "Training loss for batch 596 : 0.2917632758617401\n",
      "Training loss for batch 597 : 0.03359634429216385\n",
      "Training loss for batch 598 : 0.09961699694395065\n",
      "Training loss for batch 599 : 0.1081656888127327\n",
      "Training loss for batch 600 : 0.08437836170196533\n",
      "Training loss for batch 601 : 0.03658176586031914\n",
      "Training loss for batch 602 : 0.03945880010724068\n",
      "Training loss for batch 603 : 0.25684866309165955\n",
      "Training loss for batch 604 : 0.21874010562896729\n",
      "Training loss for batch 605 : 0.016375990584492683\n",
      "Training loss for batch 606 : 0.07663246244192123\n",
      "Training loss for batch 607 : 0.10869023948907852\n",
      "Training loss for batch 608 : 0.011742916889488697\n",
      "Training loss for batch 609 : 0.05715596303343773\n",
      "Training loss for batch 610 : 0.25320470333099365\n",
      "Training loss for batch 611 : 0.026565926149487495\n",
      "Training loss for batch 612 : 0.1652437150478363\n",
      "Training loss for batch 613 : 0.02342827431857586\n",
      "Training loss for batch 614 : 0.11428537219762802\n",
      "Training loss for batch 615 : 0.06639188528060913\n",
      "Training loss for batch 616 : 0.09526609629392624\n",
      "Training loss for batch 617 : 0.20046548545360565\n",
      "Training loss for batch 618 : 0.14288747310638428\n",
      "Training loss for batch 619 : 0.12193311750888824\n",
      "Training loss for batch 620 : 0.11275891214609146\n",
      "Training loss for batch 621 : 0.08559703081846237\n",
      "Training loss for batch 622 : 0.057482436299324036\n",
      "Training loss for batch 623 : 0.24405518174171448\n",
      "Training loss for batch 624 : 0.0903034657239914\n",
      "Training loss for batch 625 : 0.23406334221363068\n",
      "Training loss for batch 626 : 0.051399167627096176\n",
      "Training loss for batch 627 : 0.18508684635162354\n",
      "Training loss for batch 628 : 0.062040917575359344\n",
      "Training loss for batch 629 : 0.10283131897449493\n",
      "Training loss for batch 630 : 0.16048912703990936\n",
      "Training loss for batch 631 : 0.15471170842647552\n",
      "Training loss for batch 632 : 0.10073588043451309\n",
      "Training loss for batch 633 : 0.08774665743112564\n",
      "Training loss for batch 634 : 0.08017195016145706\n",
      "Training loss for batch 635 : 0.06329800933599472\n",
      "Training loss for batch 636 : 0.08642677217721939\n",
      "Training loss for batch 637 : 0.2564869225025177\n",
      "Training loss for batch 638 : 0.10487713664770126\n",
      "Training loss for batch 639 : 0.044724177569150925\n",
      "Training loss for batch 640 : 0.02721184678375721\n",
      "Training loss for batch 641 : 0.18083959817886353\n",
      "Training loss for batch 642 : 0.13017505407333374\n",
      "Training loss for batch 643 : 0.165960431098938\n",
      "Training loss for batch 644 : 0.06127209961414337\n",
      "Training loss for batch 645 : 0.0629655048251152\n",
      "Training loss for batch 646 : 0.23735080659389496\n",
      "Training loss for batch 647 : 0.09394463896751404\n",
      "Training loss for batch 648 : 0.14849258959293365\n",
      "Training loss for batch 649 : 0.1428052932024002\n",
      "Training loss for batch 650 : 0.05664384365081787\n",
      "Training loss for batch 651 : 0.08335266262292862\n",
      "Training loss for batch 652 : 0.03909426927566528\n",
      "Training loss for batch 653 : 0.050597116351127625\n",
      "Training loss for batch 654 : 0.0496724508702755\n",
      "Training loss for batch 655 : 0.0717644914984703\n",
      "Training loss for batch 656 : 0.045887455344200134\n",
      "Training loss for batch 657 : 0.10100400447845459\n",
      "Training loss for batch 658 : 0.06173872947692871\n",
      "Training loss for batch 659 : 0.10469110310077667\n",
      "Training loss for batch 660 : 0.009685402736067772\n",
      "Training loss for batch 661 : 0.08012580126523972\n",
      "Training loss for batch 662 : 0.09463746100664139\n",
      "Training loss for batch 663 : 0.05658109113574028\n",
      "Training loss for batch 664 : 0.1342436671257019\n",
      "Training loss for batch 665 : 0.00316489115357399\n",
      "Training loss for batch 666 : 0.3346167206764221\n",
      "Training loss for batch 667 : 0.07331440597772598\n",
      "Training loss for batch 668 : 0.029660137370228767\n",
      "Training loss for batch 669 : 0.17859473824501038\n",
      "Training loss for batch 670 : 0.0869738906621933\n",
      "Training loss for batch 671 : 0.03595007210969925\n",
      "Training loss for batch 672 : 0.19146421551704407\n",
      "Training loss for batch 673 : 0.0710853710770607\n",
      "Training loss for batch 674 : 0.19439458847045898\n",
      "Training loss for batch 675 : 0.10823892802000046\n",
      "Training loss for batch 676 : 0.1448262631893158\n",
      "Training loss for batch 677 : 0.03240440413355827\n",
      "Training loss for batch 678 : 0.19849999248981476\n",
      "Training loss for batch 679 : 0.21400392055511475\n",
      "Training loss for batch 680 : 0.11403679102659225\n",
      "Training loss for batch 681 : 0.05391661077737808\n",
      "Training loss for batch 682 : 0.08090648055076599\n",
      "Training loss for batch 683 : 0.01245287898927927\n",
      "Training loss for batch 684 : 0.03997912257909775\n",
      "Training loss for batch 685 : 0.15363045036792755\n",
      "Training loss for batch 686 : 0.07050216197967529\n",
      "Training loss for batch 687 : 0.10072991997003555\n",
      "Training loss for batch 688 : 0.16067318618297577\n",
      "Training loss for batch 689 : 0.2674259543418884\n",
      "Training loss for batch 690 : 0.00519850617274642\n",
      "Training loss for batch 691 : 0.03635681793093681\n",
      "Training loss for batch 692 : 0.29651808738708496\n",
      "Training loss for batch 693 : 0.15653592348098755\n",
      "Training loss for batch 694 : 0.16137024760246277\n",
      "Training loss for batch 695 : 0.17983658611774445\n",
      "Training loss for batch 696 : 0.02068367600440979\n",
      "Training loss for batch 697 : 0.13816791772842407\n",
      "Training loss for batch 698 : 0.01158391498029232\n",
      "Training loss for batch 699 : 0.033526401966810226\n",
      "Training loss for batch 700 : 0.09744009375572205\n",
      "Training loss for batch 701 : 0.17836114764213562\n",
      "Training loss for batch 702 : 0.03319932520389557\n",
      "Training loss for batch 703 : 0.016905834898352623\n",
      "Training loss for batch 704 : 0.10026414692401886\n",
      "Training loss for batch 705 : 0.057812824845314026\n",
      "Training loss for batch 706 : 0.06242768093943596\n",
      "Training loss for batch 707 : 0.30278024077415466\n",
      "Training loss for batch 708 : 0.04303443059325218\n",
      "Training loss for batch 709 : 0.006972260773181915\n",
      "Training loss for batch 710 : 0.0586668960750103\n",
      "Training loss for batch 711 : 0.05068761110305786\n",
      "Training loss for batch 712 : 0.12260501086711884\n",
      "Training loss for batch 713 : 0.052170876413583755\n",
      "Training loss for batch 714 : 0.17477191984653473\n",
      "Training loss for batch 715 : 0.09265501797199249\n",
      "Training loss for batch 716 : 0.18382498621940613\n",
      "Training loss for batch 717 : 0.18808013200759888\n",
      "Training loss for batch 718 : 0.24884173274040222\n",
      "Training loss for batch 719 : 0.2278904914855957\n",
      "Training loss for batch 720 : 0.10268617421388626\n",
      "Training loss for batch 721 : 0.18560104072093964\n",
      "Training loss for batch 722 : 0.04003481939435005\n",
      "Training loss for batch 723 : 0.19286194443702698\n",
      "Training loss for batch 724 : 0.1683410406112671\n",
      "Training loss for batch 725 : 0.20223642885684967\n",
      "Training loss for batch 726 : 0.004208075813949108\n",
      "Training loss for batch 727 : 0.04196843504905701\n",
      "Training loss for batch 728 : 0.09631823003292084\n",
      "Training loss for batch 729 : 0.2710619270801544\n",
      "Training loss for batch 730 : 0.1328052431344986\n",
      "Training loss for batch 731 : 0.1097395196557045\n",
      "Training loss for batch 732 : 0.08242107927799225\n",
      "Training loss for batch 733 : 0.020343460142612457\n",
      "Training loss for batch 734 : 0.058210838586091995\n",
      "Training loss for batch 735 : 0.03756583109498024\n",
      "Training loss for batch 736 : 0.05297552049160004\n",
      "Training loss for batch 737 : 0.1320822834968567\n",
      "Training loss for batch 738 : 0.16546589136123657\n",
      "Training loss for batch 739 : 0.15445725619792938\n",
      "Training loss for batch 740 : 0.08826743066310883\n",
      "Training loss for batch 741 : 0.23596122860908508\n",
      "Training loss for batch 742 : 0.09179292619228363\n",
      "Training loss for batch 743 : 0.09561993926763535\n",
      "Training loss for batch 744 : 0.13150940835475922\n",
      "Training loss for batch 745 : 0.24203117191791534\n",
      "Training loss for batch 746 : 0.07776597142219543\n",
      "Training loss for batch 747 : 0.04958853870630264\n",
      "Training loss for batch 748 : 0.022269314154982567\n",
      "Training loss for batch 749 : 0.031992360949516296\n",
      "Training loss for batch 750 : 0.003981617745012045\n",
      "Training loss for batch 751 : 0.08202652633190155\n",
      "Training loss for batch 752 : 0.03956763818860054\n",
      "Training loss for batch 753 : 0.12263183295726776\n",
      "Training loss for batch 754 : 0.10830041021108627\n",
      "Training loss for batch 755 : 0.09261955320835114\n",
      "Training loss for batch 756 : 0.25743505358695984\n",
      "Training loss for batch 757 : 0.05197758227586746\n",
      "Training loss for batch 758 : 0.012262221425771713\n",
      "Training loss for batch 759 : 0.45536008477211\n",
      "Training loss for batch 760 : 0.012295805849134922\n",
      "Training loss for batch 761 : 0.03285665065050125\n",
      "Training loss for batch 762 : 0.031705230474472046\n",
      "Training loss for batch 763 : 0.10424390435218811\n",
      "Training loss for batch 764 : 0.0467163510620594\n",
      "Training loss for batch 765 : 0.02005949430167675\n",
      "Training loss for batch 766 : 0.08556346595287323\n",
      "Training loss for batch 767 : 0.22831249237060547\n",
      "Training loss for batch 768 : 0.033579062670469284\n",
      "Training loss for batch 769 : 0.09447880834341049\n",
      "Training loss for batch 770 : 0.16578784584999084\n",
      "Training loss for batch 771 : 0.0966871976852417\n",
      "Training loss for batch 772 : 0.021757634356617928\n",
      "Training loss for batch 773 : 0.014893357641994953\n",
      "Training loss for batch 774 : 0.16945978999137878\n",
      "Training loss for batch 775 : 0.11493593454360962\n",
      "Training loss for batch 776 : 0.08313876390457153\n",
      "Training loss for batch 777 : 0.09753058105707169\n",
      "Training loss for batch 778 : 0.4145548641681671\n",
      "Training loss for batch 779 : 0.10446368157863617\n",
      "Training loss for batch 780 : 0.06757380813360214\n",
      "Training loss for batch 781 : 0.07691376656293869\n",
      "Training loss for batch 782 : 0.18961916863918304\n",
      "Training loss for batch 783 : 0.10843249410390854\n",
      "Training loss for batch 784 : 0.1534590870141983\n",
      "Training loss for batch 785 : 0.1446736454963684\n",
      "Training loss for batch 786 : 0.18616983294487\n",
      "Training loss for batch 787 : 0.189094677567482\n",
      "Training loss for batch 788 : 0.04129388555884361\n",
      "Training loss for batch 789 : 0.22035953402519226\n",
      "Training loss for batch 790 : 0.02987075410783291\n",
      "Training loss for batch 791 : 0.0803234651684761\n",
      "Training loss for batch 792 : 0.07712919265031815\n",
      "Training loss for batch 793 : 0.13230302929878235\n",
      "Training loss for batch 794 : 0.13366255164146423\n",
      "Training loss for batch 795 : 0.15372024476528168\n",
      "Training loss for batch 796 : 0.024343080818653107\n",
      "Training loss for batch 797 : 0.02273618057370186\n",
      "Training loss for batch 798 : 0.03335126116871834\n",
      "Training loss for batch 799 : 0.07822921127080917\n",
      "Training loss for batch 800 : 0.0586019791662693\n",
      "Training loss for batch 801 : 0.041156966239213943\n",
      "Training loss for batch 802 : 0.0834113210439682\n",
      "Training loss for batch 803 : 0.04432358965277672\n",
      "Training loss for batch 804 : 0.19914843142032623\n",
      "Training loss for batch 805 : 0.17139610648155212\n",
      "Training loss for batch 806 : 0.0821538120508194\n",
      "Training loss for batch 807 : 0.2396376132965088\n",
      "Training loss for batch 808 : 0.00527077866718173\n",
      "Training loss for batch 809 : 0.00840484257787466\n",
      "Training loss for batch 810 : 0.19694873690605164\n",
      "Training loss for batch 811 : 0.12078753113746643\n",
      "Training loss for batch 812 : 0.03453462943434715\n",
      "Training loss for batch 813 : 0.03492892161011696\n",
      "Training loss for batch 814 : 0.01974182017147541\n",
      "Training loss for batch 815 : 0.06403426826000214\n",
      "Training loss for batch 816 : 0.2539973556995392\n",
      "Training loss for batch 817 : 0.05906859040260315\n",
      "Training loss for batch 818 : 0.05831079185009003\n",
      "Training loss for batch 819 : 0.04951750859618187\n",
      "Training loss for batch 820 : 0.1533186435699463\n",
      "Training loss for batch 821 : 0.07932870090007782\n",
      "Training loss for batch 822 : 0.0067574926652014256\n",
      "Training loss for batch 823 : 0.09650878608226776\n",
      "Training loss for batch 824 : 0.46987223625183105\n",
      "Training loss for batch 825 : 0.030298583209514618\n",
      "Training loss for batch 826 : 0.03990667685866356\n",
      "Training loss for batch 827 : 0.6090512275695801\n",
      "Training loss for batch 828 : 0.04822973534464836\n",
      "Training loss for batch 829 : 0.018301216885447502\n",
      "Training loss for batch 830 : 0.2083594650030136\n",
      "Training loss for batch 831 : 0.021448960527777672\n",
      "Training loss for batch 832 : 0.16661348938941956\n",
      "Training loss for batch 833 : 0.1824897825717926\n",
      "Training loss for batch 834 : 0.017923789098858833\n",
      "Training loss for batch 835 : 0.13769525289535522\n",
      "Training loss for batch 836 : 0.1813783347606659\n",
      "Training loss for batch 837 : 0.28923144936561584\n",
      "Training loss for batch 838 : 0.08399651944637299\n",
      "Training loss for batch 839 : 0.14489951729774475\n",
      "Training loss for batch 840 : 0.08440173417329788\n",
      "Training loss for batch 841 : 0.13331301510334015\n",
      "Training loss for batch 842 : 0.09659592062234879\n",
      "Training loss for batch 843 : 0.382014662027359\n",
      "Training loss for batch 844 : 0.12329315394163132\n",
      "Training loss for batch 845 : 0.09914740175008774\n",
      "Training loss for batch 846 : 0.14562174677848816\n",
      "Training loss for batch 847 : 0.06991807371377945\n",
      "Training loss for batch 848 : 0.046574585139751434\n",
      "Training loss for batch 849 : 0.06934404373168945\n",
      "Training loss for batch 850 : 0.13147535920143127\n",
      "Training loss for batch 851 : 0.18290168046951294\n",
      "Training loss for batch 852 : 0.12619073688983917\n",
      "Training loss for batch 853 : 0.10382687300443649\n",
      "Training loss for batch 854 : 0.14192607998847961\n",
      "Training loss for batch 855 : 0.03074462339282036\n",
      "Training loss for batch 856 : 0.2327294647693634\n",
      "Training loss for batch 857 : 5.508941569587478e-08\n",
      "Training loss for batch 858 : 0.030848626047372818\n",
      "Training loss for batch 859 : 0.24702033400535583\n",
      "Training loss for batch 860 : 0.12498107552528381\n",
      "Training loss for batch 861 : 0.23011454939842224\n",
      "Training loss for batch 862 : 0.11331626772880554\n",
      "Training loss for batch 863 : 0.1808902621269226\n",
      "Training loss for batch 864 : 0.2079763412475586\n",
      "Training loss for batch 865 : 0.035070158541202545\n",
      "Training loss for batch 866 : 0.09353038668632507\n",
      "Training loss for batch 867 : 0.3030858337879181\n",
      "Training loss for batch 868 : 0.32930728793144226\n",
      "Training loss for batch 869 : 0.06555003672838211\n",
      "Training loss for batch 870 : 0.04395917430520058\n",
      "Training loss for batch 871 : 0.13347452878952026\n",
      "Training loss for batch 872 : 0.008870315738022327\n",
      "Training loss for batch 873 : 0.2884490191936493\n",
      "Training loss for batch 874 : 0.198772132396698\n",
      "Training loss for batch 875 : 0.038145262748003006\n",
      "Training loss for batch 876 : 0.16054777801036835\n",
      "Training loss for batch 877 : 0.10103514045476913\n",
      "Training loss for batch 878 : 0.21199563145637512\n",
      "Training loss for batch 879 : 0.05956330522894859\n",
      "Training loss for batch 880 : 0.015105847269296646\n",
      "Training loss for batch 881 : 0.05543465539813042\n",
      "Training loss for batch 882 : 0.19903947412967682\n",
      "Training loss for batch 883 : 0.3431108593940735\n",
      "Training loss for batch 884 : 0.07489186525344849\n",
      "Training loss for batch 885 : 0.058889299631118774\n",
      "Training loss for batch 886 : 0.07000535726547241\n",
      "Training loss for batch 887 : 0.04126374423503876\n",
      "Training loss for batch 888 : 0.031159838661551476\n",
      "Training loss for batch 889 : 0.0774681344628334\n",
      "Training loss for batch 890 : 0.11813758313655853\n",
      "Training loss for batch 891 : 0.2006475329399109\n",
      "Training loss for batch 892 : 0.08869729191064835\n",
      "Training loss for batch 893 : 0.3479028642177582\n",
      "Training loss for batch 894 : 0.1070462018251419\n",
      "Training loss for batch 895 : 0.07346034049987793\n",
      "Training loss for batch 896 : 0.16130121052265167\n",
      "Training loss for batch 897 : 0.296129435300827\n",
      "Training loss for batch 898 : 0.018185297027230263\n",
      "Training loss for batch 899 : 0.12531960010528564\n",
      "Training loss for batch 900 : 0.022526107728481293\n",
      "Training loss for batch 901 : 0.06885526329278946\n",
      "Training loss for batch 902 : 0.16417686641216278\n",
      "Training loss for batch 903 : 0.062145352363586426\n",
      "Training loss for batch 904 : 0.15852929651737213\n",
      "Training loss for batch 905 : 0.024202795699238777\n",
      "Training loss for batch 906 : 0.24802495539188385\n",
      "Training loss for batch 907 : 0.06551803648471832\n",
      "Training loss for batch 908 : 0.05132431909441948\n",
      "Training loss for batch 909 : 0.11587727814912796\n",
      "Training loss for batch 910 : 0.04062959551811218\n",
      "Training loss for batch 911 : 0.16420547664165497\n",
      "Training loss for batch 912 : 0.13953529298305511\n",
      "Training loss for batch 913 : 0.04205396771430969\n",
      "Training loss for batch 914 : 0.025430917739868164\n",
      "Training loss for batch 915 : 0.16877752542495728\n",
      "Training loss for batch 916 : 0.22164317965507507\n",
      "Training loss for batch 917 : 0.18625350296497345\n",
      "Training loss for batch 918 : 0.01820361614227295\n",
      "Training loss for batch 919 : 0.13561369478702545\n",
      "Training loss for batch 920 : 0.20118610560894012\n",
      "Training loss for batch 921 : 0.09359347075223923\n",
      "Training loss for batch 922 : 0.16270311176776886\n",
      "Training loss for batch 923 : 0.07010239362716675\n",
      "Training loss for batch 924 : 0.1874440312385559\n",
      "Training loss for batch 925 : 0.061774514615535736\n",
      "Training loss for batch 926 : 0.044087380170822144\n",
      "Training loss for batch 927 : 0.15134161710739136\n",
      "Training loss for batch 928 : 0.021555813029408455\n",
      "Training loss for batch 929 : 0.07657456398010254\n",
      "Training loss for batch 930 : 0.11542704701423645\n",
      "Training loss for batch 931 : 0.18153934180736542\n",
      "Training loss for batch 932 : 0.05336159095168114\n",
      "Training loss for batch 933 : 0.08112607896327972\n",
      "Training loss for batch 934 : 0.1673271507024765\n",
      "Training loss for batch 935 : 0.19821397960186005\n",
      "Training loss for batch 936 : 0.1477980613708496\n",
      "Training loss for batch 937 : 0.04789341241121292\n",
      "Training loss for batch 938 : 0.0463884063065052\n",
      "Training loss for batch 939 : 0.08324240148067474\n",
      "Training loss for batch 940 : 0.15990518033504486\n",
      "Training loss for batch 941 : 0.0813380777835846\n",
      "Training loss for batch 942 : 0.09337493777275085\n",
      "Training loss for batch 943 : 0.19900082051753998\n",
      "Training loss for batch 944 : 0.08498800545930862\n",
      "Training loss for batch 945 : 0.13836313784122467\n",
      "Training loss for batch 946 : 0.07400643825531006\n",
      "Training loss for batch 947 : 0.3167840242385864\n",
      "Training loss for batch 948 : 0.057857416570186615\n",
      "Training loss for batch 949 : 0.12396853417158127\n",
      "Training loss for batch 950 : 0.18073523044586182\n",
      "Training loss for batch 951 : 0.2048598825931549\n",
      "Training loss for batch 952 : 0.12042515724897385\n",
      "Training loss for batch 953 : 0.039017289876937866\n",
      "Training loss for batch 954 : 0.13777531683444977\n",
      "Training loss for batch 955 : 0.12758617103099823\n",
      "Training loss for batch 956 : 0.00300429854542017\n",
      "Training loss for batch 957 : 0.06888628005981445\n",
      "Training loss for batch 958 : 0.11890743672847748\n",
      "Training loss for batch 959 : 0.14364370703697205\n",
      "Training loss for batch 960 : 0.12803585827350616\n",
      "Training loss for batch 961 : 0.08663589507341385\n",
      "Training loss for batch 962 : 0.019978417083621025\n",
      "Training loss for batch 963 : 0.10411149263381958\n",
      "Training loss for batch 964 : 0.06966429948806763\n",
      "Training loss for batch 965 : 0.19759386777877808\n",
      "Training loss for batch 966 : 0.03736727312207222\n",
      "Training loss for batch 967 : 0.19944456219673157\n",
      "Training loss for batch 968 : 0.2596297264099121\n",
      "Training loss for batch 969 : 0.12930385768413544\n",
      "Training loss for batch 970 : 0.3526957631111145\n",
      "Training loss for batch 971 : 0.029591944068670273\n",
      "Training loss for batch 972 : 0.013078421354293823\n",
      "Training loss for batch 973 : 0.07677103579044342\n",
      "Training loss for batch 974 : 0.25277140736579895\n",
      "Training loss for batch 975 : 0.1071457490324974\n",
      "Training loss for batch 976 : 0.007206717971712351\n",
      "Training loss for batch 977 : 0.17129699885845184\n",
      "Training loss for batch 978 : 0.04578055068850517\n",
      "Training loss for batch 979 : 0.061274442821741104\n",
      "Training loss for batch 980 : 0.09889189898967743\n",
      "Training loss for batch 981 : 0.18281550705432892\n",
      "Training loss for batch 982 : 0.14418861269950867\n",
      "Training loss for batch 983 : 0.02632497251033783\n",
      "Training loss for batch 984 : 0.026558082550764084\n",
      "Training loss for batch 985 : 0.1879264861345291\n",
      "Training loss for batch 986 : 0.06880193948745728\n",
      "Training loss for batch 987 : 0.13235050439834595\n",
      "Training loss for batch 988 : 0.1981625258922577\n",
      "Training loss for batch 989 : 0.06160777062177658\n",
      "Training loss for batch 990 : 0.0482148677110672\n",
      "Training loss for batch 991 : 0.18593786656856537\n",
      "Training loss for batch 992 : 0.13079456984996796\n",
      "Training loss for batch 993 : 0.18294069170951843\n",
      "Training loss for batch 994 : 0.10129747539758682\n",
      "Training loss for batch 995 : 0.23298966884613037\n",
      "Training loss for batch 996 : 0.1184842586517334\n",
      "Training loss for batch 997 : 0.14478203654289246\n",
      "Training loss for batch 998 : 0.16056519746780396\n",
      "Training loss for batch 999 : 0.03271124139428139\n",
      "Training loss for batch 1000 : 0.11396712809801102\n",
      "Training loss for batch 1001 : 0.15468616783618927\n",
      "Training loss for batch 1002 : 0.23399975895881653\n",
      "Training loss for batch 1003 : 0.16649973392486572\n",
      "Training loss for batch 1004 : 0.08837929368019104\n",
      "Training loss for batch 1005 : 0.05368950963020325\n",
      "Training loss for batch 1006 : 0.037261031568050385\n",
      "Training loss for batch 1007 : 0.08452235162258148\n",
      "Training loss for batch 1008 : 0.007986512966454029\n",
      "Training loss for batch 1009 : 0.15400269627571106\n",
      "Training loss for batch 1010 : 0.12510432302951813\n",
      "Training loss for batch 1011 : 0.06935150921344757\n",
      "Training loss for batch 1012 : 0.057229869067668915\n",
      "Training loss for batch 1013 : 0.1112118810415268\n",
      "Training loss for batch 1014 : 0.22158698737621307\n",
      "Training loss for batch 1015 : 0.02036862261593342\n",
      "Training loss for batch 1016 : 0.04134640097618103\n",
      "Training loss for batch 1017 : 0.04116852581501007\n",
      "Training loss for batch 1018 : 0.11451458185911179\n",
      "Training loss for batch 1019 : 0.1388072967529297\n",
      "Training loss for batch 1020 : 0.16648855805397034\n",
      "Training loss for batch 1021 : 0.16460451483726501\n",
      "Training loss for batch 1022 : 0.1659647822380066\n",
      "Training loss for batch 1023 : 0.06691204011440277\n",
      "Training loss for batch 1024 : 0.11630666255950928\n",
      "Training loss for batch 1025 : 0.027085404843091965\n",
      "Training loss for batch 1026 : 0.2686418294906616\n",
      "Training loss for batch 1027 : 0.05995268002152443\n",
      "Training loss for batch 1028 : 0.07174500077962875\n",
      "Training loss for batch 1029 : 0.16200971603393555\n",
      "Training loss for batch 1030 : 0.005019822623580694\n",
      "Training loss for batch 1031 : 0.28656598925590515\n",
      "Training loss for batch 1032 : 0.3329037129878998\n",
      "Training loss for batch 1033 : 0.04866824299097061\n",
      "Training loss for batch 1034 : 0.21554243564605713\n",
      "Training loss for batch 1035 : 0.046915628015995026\n",
      "Training loss for batch 1036 : 0.07053642719984055\n",
      "Training loss for batch 1037 : 0.17955894768238068\n",
      "Training loss for batch 1038 : 0.02144056186079979\n",
      "Training loss for batch 1039 : 0.20775169134140015\n",
      "Training loss for batch 1040 : 0.1926618367433548\n",
      "Training loss for batch 1041 : 0.12469346076250076\n",
      "Training loss for batch 1042 : 0.30844002962112427\n",
      "Training loss for batch 1043 : 0.1251506358385086\n",
      "Training loss for batch 1044 : 0.07111009955406189\n",
      "Training loss for batch 1045 : 0.047750163823366165\n",
      "Training loss for batch 1046 : 0.15016689896583557\n",
      "Training loss for batch 1047 : 0.08570317924022675\n",
      "Training loss for batch 1048 : 0.02492818981409073\n",
      "Training loss for batch 1049 : 0.19720788300037384\n",
      "Training loss for batch 1050 : 0.03196156024932861\n",
      "Training loss for batch 1051 : 0.06742333620786667\n",
      "Training loss for batch 1052 : 0.09109606593847275\n",
      "Training loss for batch 1053 : 0.07005511224269867\n",
      "Training loss for batch 1054 : 0.09737883508205414\n",
      "Training loss for batch 1055 : 0.09033798426389694\n",
      "Training loss for batch 1056 : 0.13900552690029144\n",
      "Training loss for batch 1057 : 0.03866693004965782\n",
      "Training loss for batch 1058 : 0.11419416218996048\n",
      "Training loss for batch 1059 : 0.23659595847129822\n",
      "Training loss for batch 1060 : 0.2798369526863098\n",
      "Training loss for batch 1061 : 0.03950287774205208\n",
      "Training loss for batch 1062 : 0.13409273326396942\n",
      "Training loss for batch 1063 : 0.07995044440031052\n",
      "Training loss for batch 1064 : 0.08116433769464493\n",
      "Training loss for batch 1065 : 0.11489611119031906\n",
      "Training loss for batch 1066 : 0.16067856550216675\n",
      "Training loss for batch 1067 : 0.12733474373817444\n",
      "Training loss for batch 1068 : 0.09602268785238266\n",
      "Training loss for batch 1069 : 0.15301001071929932\n",
      "Training loss for batch 1070 : 0.19357864558696747\n",
      "Training loss for batch 1071 : 0.10789480060338974\n",
      "Training loss for batch 1072 : 0.019415002316236496\n",
      "Training loss for batch 1073 : 0.18205109238624573\n",
      "Training loss for batch 1074 : 0.004350725561380386\n",
      "Training loss for batch 1075 : 0.2036004364490509\n",
      "Training loss for batch 1076 : 0.11191928386688232\n",
      "Training loss for batch 1077 : 0.04000890254974365\n",
      "Training loss for batch 1078 : 0.36576369404792786\n",
      "Training loss for batch 1079 : 0.12479167431592941\n",
      "Training loss for batch 1080 : 0.3309483826160431\n",
      "Training loss for batch 1081 : 0.02956969104707241\n",
      "Training loss for batch 1082 : 0.07573326677083969\n",
      "Training loss for batch 1083 : 0.0\n",
      "Training loss for batch 1084 : 0.2067147046327591\n",
      "Training loss for batch 1085 : 0.11618154495954514\n",
      "Training loss for batch 1086 : 0.35505250096321106\n",
      "Training loss for batch 1087 : 0.01739736646413803\n",
      "Training loss for batch 1088 : 0.10929824411869049\n",
      "Training loss for batch 1089 : 0.06398650258779526\n",
      "Training loss for batch 1090 : 0.08357568830251694\n",
      "Training loss for batch 1091 : 0.1611170917749405\n",
      "Training loss for batch 1092 : 0.10447543114423752\n",
      "Training loss for batch 1093 : 0.021803181618452072\n",
      "Training loss for batch 1094 : 0.09009945392608643\n",
      "Training loss for batch 1095 : 0.03945521265268326\n",
      "Training loss for batch 1096 : 0.13116270303726196\n",
      "Training loss for batch 1097 : 0.016732627525925636\n",
      "Training loss for batch 1098 : 0.1790342628955841\n",
      "Training loss for batch 1099 : 0.12359288334846497\n",
      "Training loss for batch 1100 : 0.38296666741371155\n",
      "Training loss for batch 1101 : 0.09580893814563751\n",
      "Training loss for batch 1102 : 0.16732431948184967\n",
      "Training loss for batch 1103 : 0.06638967245817184\n",
      "Training loss for batch 1104 : 0.0774683952331543\n",
      "Training loss for batch 1105 : 0.09940995275974274\n",
      "Training loss for batch 1106 : 0.2103269249200821\n",
      "Training loss for batch 1107 : 0.06525269150733948\n",
      "Training loss for batch 1108 : 0.05944910645484924\n",
      "Training loss for batch 1109 : 0.04834703728556633\n",
      "Training loss for batch 1110 : 0.10882311314344406\n",
      "Training loss for batch 1111 : 0.20701122283935547\n",
      "Training loss for batch 1112 : 0.027112962678074837\n",
      "Training loss for batch 1113 : 0.1276308298110962\n",
      "Training loss for batch 1114 : 0.14081978797912598\n",
      "Training loss for batch 1115 : 0.12014615535736084\n",
      "Training loss for batch 1116 : 0.07947798818349838\n",
      "Training loss for batch 1117 : 0.06833548098802567\n",
      "Training loss for batch 1118 : 0.03675820678472519\n",
      "Training loss for batch 1119 : 0.08745384216308594\n",
      "Training loss for batch 1120 : 0.018937207758426666\n",
      "Training loss for batch 1121 : 0.14965076744556427\n",
      "Training loss for batch 1122 : 0.09905561804771423\n",
      "Training loss for batch 1123 : 0.06709322333335876\n",
      "Training loss for batch 1124 : 0.07686787843704224\n",
      "Training loss for batch 1125 : 0.07814711332321167\n",
      "Training loss for batch 1126 : 0.2128097414970398\n",
      "Training loss for batch 1127 : 0.19660381972789764\n",
      "Training loss for batch 1128 : 0.07734053581953049\n",
      "Training loss for batch 1129 : 0.26198920607566833\n",
      "Training loss for batch 1130 : 0.05039326846599579\n",
      "Training loss for batch 1131 : 0.11400467902421951\n",
      "Training loss for batch 1132 : 0.13794897496700287\n",
      "Training loss for batch 1133 : 0.04920312017202377\n",
      "Training loss for batch 1134 : 0.04831438139081001\n",
      "Training loss for batch 1135 : 0.03576621413230896\n",
      "Training loss for batch 1136 : 0.13744358718395233\n",
      "Training loss for batch 1137 : 0.2758863568305969\n",
      "Training loss for batch 1138 : 0.21805955469608307\n",
      "Training loss for batch 1139 : 0.1808909922838211\n",
      "Training loss for batch 1140 : 0.041562050580978394\n",
      "Training loss for batch 1141 : 0.06179993227124214\n",
      "Training loss for batch 1142 : 0.1878649890422821\n",
      "Training loss for batch 1143 : 0.30372896790504456\n",
      "Training loss for batch 1144 : 0.05064384639263153\n",
      "Training loss for batch 1145 : 0.15305127203464508\n",
      "Training loss for batch 1146 : 0.11419195681810379\n",
      "Training loss for batch 1147 : 0.2543807625770569\n",
      "Training loss for batch 1148 : 0.14201363921165466\n",
      "Training loss for batch 1149 : 0.09013243764638901\n",
      "Training loss for batch 1150 : 0.06202535331249237\n",
      "Training loss for batch 1151 : 0.024714063853025436\n",
      "Training loss for batch 1152 : 0.08973664790391922\n",
      "Training loss for batch 1153 : 0.033482324331998825\n",
      "Training loss for batch 1154 : 0.24043890833854675\n",
      "Training loss for batch 1155 : 0.07075996696949005\n",
      "Training loss for batch 1156 : 0.06488935649394989\n",
      "Training loss for batch 1157 : 0.11703332513570786\n",
      "Training loss for batch 1158 : 0.010991171933710575\n",
      "Training loss for batch 1159 : 0.1690508872270584\n",
      "Training loss for batch 1160 : 0.3920106291770935\n",
      "Training loss for batch 1161 : 0.13573089241981506\n",
      "Training loss for batch 1162 : 0.056996945291757584\n",
      "Training loss for batch 1163 : 0.24395018815994263\n",
      "Training loss for batch 1164 : 0.08442933857440948\n",
      "Training loss for batch 1165 : 0.17193466424942017\n",
      "Training loss for batch 1166 : 0.08051743358373642\n",
      "Training loss for batch 1167 : 0.11436303704977036\n",
      "Training loss for batch 1168 : 0.06710542738437653\n",
      "Training loss for batch 1169 : 0.05386187136173248\n",
      "Training loss for batch 1170 : 0.056858647614717484\n",
      "Training loss for batch 1171 : 0.02505832351744175\n",
      "Training loss for batch 1172 : 0.21145151555538177\n",
      "Training loss for batch 1173 : 0.22204191982746124\n",
      "Training loss for batch 1174 : 0.049179669469594955\n",
      "Training loss for batch 1175 : 0.116140216588974\n",
      "Training loss for batch 1176 : 0.039708588272333145\n",
      "Training loss for batch 1177 : 0.30250948667526245\n",
      "Training loss for batch 1178 : 0.1525159478187561\n",
      "Training loss for batch 1179 : 0.1408497840166092\n",
      "Training loss for batch 1180 : 0.3287706971168518\n",
      "Training loss for batch 1181 : 0.034864481538534164\n",
      "Training loss for batch 1182 : 0.14749154448509216\n",
      "Training loss for batch 1183 : 0.047935329377651215\n",
      "Training loss for batch 1184 : 0.19994764029979706\n",
      "Training loss for batch 1185 : 0.17788252234458923\n",
      "Training loss for batch 1186 : 0.08730968087911606\n",
      "Training loss for batch 1187 : 0.11005701869726181\n",
      "Training loss for batch 1188 : 0.018304144963622093\n",
      "Training loss for batch 1189 : 0.16587841510772705\n",
      "Training loss for batch 1190 : 0.11202497780323029\n",
      "Training loss for batch 1191 : 0.09232029318809509\n",
      "Training loss for batch 1192 : 0.2684333026409149\n",
      "Training loss for batch 1193 : 0.0894426479935646\n",
      "Training loss for batch 1194 : 0.2059822976589203\n",
      "Training loss for batch 1195 : 0.01795840635895729\n",
      "Training loss for batch 1196 : 0.17387136816978455\n",
      "Training loss for batch 1197 : 0.3170471787452698\n",
      "Training loss for batch 1198 : 0.06255347281694412\n",
      "Training loss for batch 1199 : 0.20950248837471008\n",
      "Training loss for batch 1200 : 0.09958638995885849\n",
      "Training loss for batch 1201 : 0.10734396427869797\n",
      "Training loss for batch 1202 : 0.05614260211586952\n",
      "Training loss for batch 1203 : 0.1492612659931183\n",
      "Training loss for batch 1204 : 0.11259963363409042\n",
      "Training loss for batch 1205 : 0.14164601266384125\n",
      "Training loss for batch 1206 : 0.23389758169651031\n",
      "Training loss for batch 1207 : 0.2143910974264145\n",
      "Training loss for batch 1208 : 0.046737659722566605\n",
      "Training loss for batch 1209 : 0.36679807305336\n",
      "Training loss for batch 1210 : 0.23213239014148712\n",
      "Training loss for batch 1211 : 0.23449717462062836\n",
      "Training loss for batch 1212 : 0.04135727509856224\n",
      "Training loss for batch 1213 : 0.044948287308216095\n",
      "Training loss for batch 1214 : 0.06953739374876022\n",
      "Training loss for batch 1215 : 0.12102080881595612\n",
      "Training loss for batch 1216 : 0.0401935800909996\n",
      "Training loss for batch 1217 : 0.015186325646936893\n",
      "Training loss for batch 1218 : 0.16822896897792816\n",
      "Training loss for batch 1219 : 0.0736922174692154\n",
      "Training loss for batch 1220 : 0.20316877961158752\n",
      "Training loss for batch 1221 : 0.02229725942015648\n",
      "Training loss for batch 1222 : 0.02511921338737011\n",
      "Training loss for batch 1223 : 0.212896466255188\n",
      "Training loss for batch 1224 : 0.31069010496139526\n",
      "Training loss for batch 1225 : 0.09019024670124054\n",
      "Training loss for batch 1226 : 0.1439964324235916\n",
      "Training loss for batch 1227 : 0.2450188249349594\n",
      "Training loss for batch 1228 : 0.12916870415210724\n",
      "Training loss for batch 1229 : 0.07097111642360687\n",
      "Training loss for batch 1230 : 0.120180144906044\n",
      "Training loss for batch 1231 : 0.0\n",
      "Training loss for batch 1232 : 0.035614389926195145\n",
      "Training loss for batch 1233 : 0.09971611201763153\n",
      "Training loss for batch 1234 : 0.08855780959129333\n",
      "Training loss for batch 1235 : 0.08262082934379578\n",
      "Training loss for batch 1236 : 0.11149375140666962\n",
      "Training loss for batch 1237 : 0.07835223525762558\n",
      "Training loss for batch 1238 : 0.08580226451158524\n",
      "Training loss for batch 1239 : 0.02973305620253086\n",
      "Training loss for batch 1240 : 0.06559041887521744\n",
      "Training loss for batch 1241 : 0.2557500898838043\n",
      "Training loss for batch 1242 : 0.17807698249816895\n",
      "Training loss for batch 1243 : 0.10358324646949768\n",
      "Training loss for batch 1244 : 0.1257806420326233\n",
      "Training loss for batch 1245 : 0.1855728179216385\n",
      "Training loss for batch 1246 : 0.008642932400107384\n",
      "Training loss for batch 1247 : 0.03680445998907089\n",
      "Training loss for batch 1248 : 0.12793618440628052\n",
      "Training loss for batch 1249 : 0.2844664454460144\n",
      "Training loss for batch 1250 : 0.1300385743379593\n",
      "Training loss for batch 1251 : 0.041984450072050095\n",
      "Training loss for batch 1252 : 0.05393913760781288\n",
      "Training loss for batch 1253 : 0.15212465822696686\n",
      "Training loss for batch 1254 : 0.15522612631320953\n",
      "Training loss for batch 1255 : 0.1289457529783249\n",
      "Training loss for batch 1256 : 0.32271626591682434\n",
      "Training loss for batch 1257 : 0.02818574197590351\n",
      "Training loss for batch 1258 : 0.12032023072242737\n",
      "Training loss for batch 1259 : 0.030770644545555115\n",
      "Training loss for batch 1260 : 0.07495409995317459\n",
      "Training loss for batch 1261 : 0.03882594406604767\n",
      "Training loss for batch 1262 : 0.04181600734591484\n",
      "Training loss for batch 1263 : 0.045780833810567856\n",
      "Training loss for batch 1264 : 0.1987031251192093\n",
      "Training loss for batch 1265 : 0.10817865282297134\n",
      "Training loss for batch 1266 : 0.09657425433397293\n",
      "Training loss for batch 1267 : 0.04534986615180969\n",
      "Training loss for batch 1268 : 0.06710801273584366\n",
      "Training loss for batch 1269 : 0.19248005747795105\n",
      "Training loss for batch 1270 : 0.13734138011932373\n",
      "Training loss for batch 1271 : 0.1524542272090912\n",
      "Training loss for batch 1272 : 0.03697630763053894\n",
      "Training loss for batch 1273 : 0.026766644790768623\n",
      "Training loss for batch 1274 : 0.04963892698287964\n",
      "Training loss for batch 1275 : 0.08483373373746872\n",
      "Training loss for batch 1276 : 0.11384326964616776\n",
      "Training loss for batch 1277 : 0.04112034663558006\n",
      "Training loss for batch 1278 : 0.021523676812648773\n",
      "Training loss for batch 1279 : 0.2077610194683075\n",
      "Training loss for batch 1280 : 0.25120067596435547\n",
      "Training loss for batch 1281 : 0.07255490869283676\n",
      "Training loss for batch 1282 : 0.05164599046111107\n",
      "Training loss for batch 1283 : 0.19387471675872803\n",
      "Training loss for batch 1284 : 0.0883830264210701\n",
      "Training loss for batch 1285 : 0.05627286434173584\n",
      "Training loss for batch 1286 : 0.06491362303495407\n",
      "Training loss for batch 1287 : 0.06584396213293076\n",
      "Training loss for batch 1288 : 0.14296218752861023\n",
      "Training loss for batch 1289 : 0.05374681577086449\n",
      "Training loss for batch 1290 : 0.0984409973025322\n",
      "Training loss for batch 1291 : 0.00710479449480772\n",
      "Training loss for batch 1292 : 0.03566797450184822\n",
      "Training loss for batch 1293 : 0.22068992257118225\n",
      "Training loss for batch 1294 : 0.08153228461742401\n",
      "Training loss for batch 1295 : 0.1463852822780609\n",
      "Training loss for batch 1296 : 0.13380251824855804\n",
      "Training loss for batch 1297 : 0.018539175391197205\n",
      "Training loss for batch 1298 : 0.05831694230437279\n",
      "Training loss for batch 1299 : 0.12272986769676208\n",
      "Training loss for batch 1300 : 0.203575000166893\n",
      "Training loss for batch 1301 : 0.22783373296260834\n",
      "Training loss for batch 1302 : 0.1590033620595932\n",
      "Training loss for batch 1303 : 0.1333804875612259\n",
      "Training loss for batch 1304 : 0.09029973298311234\n",
      "Training loss for batch 1305 : 0.11656498163938522\n",
      "Training loss for batch 1306 : 0.0021982064936310053\n",
      "Training loss for batch 1307 : 0.023206986486911774\n",
      "Training loss for batch 1308 : 0.01556811761111021\n",
      "Training loss for batch 1309 : 0.14282138645648956\n",
      "Training loss for batch 1310 : 0.07113518565893173\n",
      "Training loss for batch 1311 : 0.1552807092666626\n",
      "Training loss for batch 1312 : 0.320929616689682\n",
      "Training loss for batch 1313 : 0.09839839488267899\n",
      "Training loss for batch 1314 : 0.19087913632392883\n",
      "Training loss for batch 1315 : 0.12662073969841003\n",
      "Training loss for batch 1316 : 0.019271770492196083\n",
      "Training loss for batch 1317 : 0.21157661080360413\n",
      "Training loss for batch 1318 : 0.09123066067695618\n",
      "Training loss for batch 1319 : 0.04018796607851982\n",
      "Training loss for batch 1320 : 0.28664690256118774\n",
      "Training loss for batch 1321 : 0.22828364372253418\n",
      "Training loss for batch 1322 : 0.11413001269102097\n",
      "Training loss for batch 1323 : 0.200341135263443\n",
      "Training loss for batch 1324 : 0.08462465554475784\n",
      "Training loss for batch 1325 : 0.1510169953107834\n",
      "Training loss for batch 1326 : 0.1909823715686798\n",
      "Training loss for batch 1327 : 0.030119087547063828\n",
      "Training loss for batch 1328 : 0.14241038262844086\n",
      "Training loss for batch 1329 : 0.011025041341781616\n",
      "Training loss for batch 1330 : 0.19011829793453217\n",
      "Training loss for batch 1331 : 0.1104600727558136\n",
      "Training loss for batch 1332 : 0.16563700139522552\n",
      "Training loss for batch 1333 : 0.030595384538173676\n",
      "Training loss for batch 1334 : 0.1312970072031021\n",
      "Training loss for batch 1335 : 0.0820537582039833\n",
      "Training loss for batch 1336 : 0.08590766787528992\n",
      "Training loss for batch 1337 : 0.0\n",
      "Training loss for batch 1338 : 0.1273449808359146\n",
      "Training loss for batch 1339 : 0.0950947254896164\n",
      "Training loss for batch 1340 : 0.2517457902431488\n",
      "Training loss for batch 1341 : 0.22063516080379486\n",
      "Training loss for batch 1342 : 0.16731016337871552\n",
      "Training loss for batch 1343 : 0.09462396800518036\n",
      "Training loss for batch 1344 : 0.1431896835565567\n",
      "Training loss for batch 1345 : 0.11335135251283646\n",
      "Training loss for batch 1346 : 0.0359356589615345\n",
      "Training loss for batch 1347 : 0.08942743390798569\n",
      "Training loss for batch 1348 : 0.1280534565448761\n",
      "Training loss for batch 1349 : 0.12511420249938965\n",
      "Training loss for batch 1350 : 0.03512732312083244\n",
      "Training loss for batch 1351 : 0.03971559554338455\n",
      "Training loss for batch 1352 : 0.21412767469882965\n",
      "Training loss for batch 1353 : 0.009061222895979881\n",
      "Training loss for batch 1354 : 0.031135160475969315\n",
      "Training loss for batch 1355 : 0.1552838385105133\n",
      "Training loss for batch 1356 : 0.08028052002191544\n",
      "Training loss for batch 1357 : 0.1995696872472763\n",
      "Training loss for batch 1358 : 0.06108948960900307\n",
      "Training loss for batch 1359 : 0.08216410875320435\n",
      "Training loss for batch 1360 : 0.1728246808052063\n",
      "Training loss for batch 1361 : 0.11979608982801437\n",
      "Training loss for batch 1362 : 0.05938213691115379\n",
      "Training loss for batch 1363 : 0.11417689174413681\n",
      "Training loss for batch 1364 : 0.07805325835943222\n",
      "Training loss for batch 1365 : 0.09305404871702194\n",
      "Training loss for batch 1366 : 0.045149125158786774\n",
      "Training loss for batch 1367 : 0.11415552347898483\n",
      "Training loss for batch 1368 : 0.05433368310332298\n",
      "Training loss for batch 1369 : 0.1153794378042221\n",
      "Training loss for batch 1370 : 0.20286549627780914\n",
      "Training loss for batch 1371 : 0.030568160116672516\n",
      "Training loss for batch 1372 : 0.16450531780719757\n",
      "Training loss for batch 1373 : 0.098333939909935\n",
      "Training loss for batch 1374 : 0.08109021186828613\n",
      "Training loss for batch 1375 : 0.18946948647499084\n",
      "Training loss for batch 1376 : 0.02928755059838295\n",
      "Training loss for batch 1377 : 0.12552738189697266\n",
      "Training loss for batch 1378 : 0.11540032923221588\n",
      "Training loss for batch 1379 : 0.05470442771911621\n",
      "Training loss for batch 1380 : 0.03440612182021141\n",
      "Training loss for batch 1381 : 0.22321006655693054\n",
      "Training loss for batch 1382 : 0.105568066239357\n",
      "Training loss for batch 1383 : 0.09588834643363953\n",
      "Training loss for batch 1384 : 0.0755385011434555\n",
      "Training loss for batch 1385 : 0.07270156592130661\n",
      "Training loss for batch 1386 : 0.16995114088058472\n",
      "Training loss for batch 1387 : 0.0877455472946167\n",
      "Training loss for batch 1388 : 0.11064161360263824\n",
      "Training loss for batch 1389 : 0.10995005071163177\n",
      "Training loss for batch 1390 : 0.2965751886367798\n",
      "Training loss for batch 1391 : 0.07481672614812851\n",
      "Training loss for batch 1392 : 0.03758825734257698\n",
      "Training loss for batch 1393 : 0.12427482008934021\n",
      "Training loss for batch 1394 : 0.05097527429461479\n",
      "Training loss for batch 1395 : 0.13377419114112854\n",
      "Training loss for batch 1396 : 0.17036744952201843\n",
      "Training loss for batch 1397 : 0.21193461120128632\n",
      "Training loss for batch 1398 : 0.15966394543647766\n",
      "Training loss for batch 1399 : 0.15823671221733093\n",
      "Training loss for batch 1400 : 0.028159720823168755\n",
      "Training loss for batch 1401 : 0.09066247940063477\n",
      "Training loss for batch 1402 : 0.011315018869936466\n",
      "Training loss for batch 1403 : 0.06609230488538742\n",
      "Training loss for batch 1404 : 0.12462916225194931\n",
      "Training loss for batch 1405 : 0.03428015485405922\n",
      "Training loss for batch 1406 : 0.1494932621717453\n",
      "Training loss for batch 1407 : 0.04251507669687271\n",
      "Training loss for batch 1408 : 0.04855995252728462\n",
      "Training loss for batch 1409 : 0.13739752769470215\n",
      "Training loss for batch 1410 : 0.13777701556682587\n",
      "Training loss for batch 1411 : 0.05075562000274658\n",
      "Training loss for batch 1412 : 0.1601950228214264\n",
      "Training loss for batch 1413 : 0.03739229962229729\n",
      "Training loss for batch 1414 : 0.14536967873573303\n",
      "Training loss for batch 1415 : 0.3332236409187317\n",
      "Training loss for batch 1416 : 0.16375379264354706\n",
      "Training loss for batch 1417 : 0.021001148968935013\n",
      "Training loss for batch 1418 : 0.03277168422937393\n",
      "Training loss for batch 1419 : 0.1136176586151123\n",
      "Training loss for batch 1420 : 0.022949717938899994\n",
      "Training loss for batch 1421 : 0.0443878248333931\n",
      "Training loss for batch 1422 : 0.04726269096136093\n",
      "Training loss for batch 1423 : 0.050701893866062164\n",
      "Training loss for batch 1424 : 0.1468265950679779\n",
      "Training loss for batch 1425 : 0.00824153982102871\n",
      "Training loss for batch 1426 : 0.05803278088569641\n",
      "Training loss for batch 1427 : 0.025559114292263985\n",
      "Training loss for batch 1428 : 0.0314379520714283\n",
      "Training loss for batch 1429 : 0.04074085131287575\n",
      "Training loss for batch 1430 : 0.11423540115356445\n",
      "Training loss for batch 1431 : 0.07495346665382385\n",
      "Training loss for batch 1432 : 0.02736084908246994\n",
      "Training loss for batch 1433 : 0.09645915776491165\n",
      "Training loss for batch 1434 : 0.28864672780036926\n",
      "Training loss for batch 1435 : 0.027869001030921936\n",
      "Training loss for batch 1436 : 0.21495233476161957\n",
      "Training loss for batch 1437 : 0.0550457164645195\n",
      "Training loss for batch 1438 : 0.06344881653785706\n",
      "Training loss for batch 1439 : 0.012341002002358437\n",
      "Training loss for batch 1440 : 0.08820643275976181\n",
      "Training loss for batch 1441 : 0.065923310816288\n",
      "Training loss for batch 1442 : 0.07190854102373123\n",
      "Training loss for batch 1443 : 0.24764718115329742\n",
      "Training loss for batch 1444 : 0.01937609538435936\n",
      "Training loss for batch 1445 : 0.16701887547969818\n",
      "Training loss for batch 1446 : 0.12479662895202637\n",
      "Training loss for batch 1447 : 0.192450150847435\n",
      "Training loss for batch 1448 : 0.10690156370401382\n",
      "Training loss for batch 1449 : 0.04585324600338936\n",
      "Training loss for batch 1450 : 0.06547600775957108\n",
      "Training loss for batch 1451 : 0.02346865087747574\n",
      "Training loss for batch 1452 : 0.012830989435315132\n",
      "Training loss for batch 1453 : 0.031722985208034515\n",
      "Training loss for batch 1454 : 0.13755229115486145\n",
      "Training loss for batch 1455 : 0.03016936406493187\n",
      "Training loss for batch 1456 : 0.007265202701091766\n",
      "Training loss for batch 1457 : 0.21071100234985352\n",
      "Training loss for batch 1458 : 0.20811089873313904\n",
      "Training loss for batch 1459 : 0.0261903814971447\n",
      "Training loss for batch 1460 : 0.05221983417868614\n",
      "Training loss for batch 1461 : 0.07871843129396439\n",
      "Training loss for batch 1462 : 0.040639571845531464\n",
      "Training loss for batch 1463 : 0.020585963502526283\n",
      "Training loss for batch 1464 : 0.003617292270064354\n",
      "Training loss for batch 1465 : 0.025043509900569916\n",
      "Training loss for batch 1466 : 0.15366409718990326\n",
      "Training loss for batch 1467 : 0.21759778261184692\n",
      "Training loss for batch 1468 : 0.03681204095482826\n",
      "Training loss for batch 1469 : 0.12905605137348175\n",
      "Training loss for batch 1470 : 0.025586597621440887\n",
      "Training loss for batch 1471 : 0.24356219172477722\n",
      "Training loss for batch 1472 : 0.09713571518659592\n",
      "Training loss for batch 1473 : 0.0553518645465374\n",
      "Training loss for batch 1474 : 0.046150654554367065\n",
      "Training loss for batch 1475 : 0.13191914558410645\n",
      "Training loss for batch 1476 : 0.030110986903309822\n",
      "Training loss for batch 1477 : 0.023179862648248672\n",
      "Training loss for batch 1478 : 0.00146679172758013\n",
      "Training loss for batch 1479 : 0.08215757459402084\n",
      "Training loss for batch 1480 : 0.23797407746315002\n",
      "Training loss for batch 1481 : 0.15718314051628113\n",
      "Training loss for batch 1482 : 0.40091609954833984\n",
      "Training loss for batch 1483 : 0.03325531631708145\n",
      "Training loss for batch 1484 : 0.09699054807424545\n",
      "Training loss for batch 1485 : 0.14497824013233185\n",
      "Training loss for batch 1486 : 0.1536751240491867\n",
      "Training loss for batch 1487 : 0.14406663179397583\n",
      "Training loss for batch 1488 : 0.1400480568408966\n",
      "Training loss for batch 1489 : 0.08701907843351364\n",
      "Training loss for batch 1490 : 0.09173993766307831\n",
      "Training loss for batch 1491 : 0.17043177783489227\n",
      "Training loss for batch 1492 : 0.2329997420310974\n",
      "Training loss for batch 1493 : 0.08782549947500229\n",
      "Training loss for batch 1494 : 0.20850013196468353\n",
      "Training loss for batch 1495 : 0.026070499792695045\n",
      "Training loss for batch 1496 : 0.03476079925894737\n",
      "Training loss for batch 1497 : 0.11147242784500122\n",
      "Training loss for batch 1498 : 0.006265840958803892\n",
      "Training loss for batch 1499 : 0.05528589338064194\n",
      "Training loss for batch 1500 : 0.19186601042747498\n",
      "Training loss for batch 1501 : 0.08632007986307144\n",
      "Training loss for batch 1502 : 0.19070328772068024\n",
      "Training loss for batch 1503 : 0.11256009340286255\n",
      "Training loss for batch 1504 : 0.05178995057940483\n",
      "Training loss for batch 1505 : 0.11008910089731216\n",
      "Training loss for batch 1506 : 0.011140371672809124\n",
      "Training loss for batch 1507 : 0.03889969363808632\n",
      "Training loss for batch 1508 : 0.04573088139295578\n",
      "Training loss for batch 1509 : 0.283390611410141\n",
      "Training loss for batch 1510 : 0.1542792022228241\n",
      "Training loss for batch 1511 : 0.025363590568304062\n",
      "Training loss for batch 1512 : 0.054838791489601135\n",
      "Training loss for batch 1513 : 0.22801686823368073\n",
      "Training loss for batch 1514 : 0.048687275499105453\n",
      "Training loss for batch 1515 : 0.306453138589859\n",
      "Training loss for batch 1516 : 0.2866814434528351\n",
      "Training loss for batch 1517 : 0.12757396697998047\n",
      "Training loss for batch 1518 : 0.07073456794023514\n",
      "Training loss for batch 1519 : 0.07219552993774414\n",
      "Training loss for batch 1520 : 0.025654522702097893\n",
      "Training loss for batch 1521 : 0.07772704213857651\n",
      "Training loss for batch 1522 : 0.07635679841041565\n",
      "Training loss for batch 1523 : 0.1209309995174408\n",
      "Training loss for batch 1524 : 0.33002227544784546\n",
      "Training loss for batch 1525 : 0.02740568295121193\n",
      "Training loss for batch 1526 : 0.10166513174772263\n",
      "Training loss for batch 1527 : 0.11192506551742554\n",
      "Training loss for batch 1528 : 0.09264892339706421\n",
      "Training loss for batch 1529 : 0.012718383222818375\n",
      "Training loss for batch 1530 : 0.023407213389873505\n",
      "Training loss for batch 1531 : 0.029712827876210213\n",
      "Training loss for batch 1532 : 0.11655696481466293\n",
      "Training loss for batch 1533 : 0.15855200588703156\n",
      "Training loss for batch 1534 : 0.08452486246824265\n",
      "Training loss for batch 1535 : 0.10028360038995743\n",
      "Training loss for batch 1536 : 0.04997371509671211\n",
      "Training loss for batch 1537 : 0.08667150139808655\n",
      "Training loss for batch 1538 : 0.1353560984134674\n",
      "Training loss for batch 1539 : 0.17367962002754211\n",
      "Training loss for batch 1540 : 0.02729729749262333\n",
      "Training loss for batch 1541 : 0.1276799440383911\n",
      "Training loss for batch 1542 : 0.05936623737215996\n",
      "Training loss for batch 1543 : 0.15475942194461823\n",
      "Training loss for batch 1544 : 0.1072416678071022\n",
      "Training loss for batch 1545 : 0.07013808935880661\n",
      "Training loss for batch 1546 : 0.2544406056404114\n",
      "Training loss for batch 1547 : 0.11870288103818893\n",
      "Training loss for batch 1548 : 0.010531959123909473\n",
      "Training loss for batch 1549 : 0.01785343885421753\n",
      "Training loss for batch 1550 : 0.11031743884086609\n",
      "Training loss for batch 1551 : 0.03579321876168251\n",
      "Training loss for batch 1552 : 0.1109129786491394\n",
      "Training loss for batch 1553 : 0.13787133991718292\n",
      "Training loss for batch 1554 : 0.16850438714027405\n",
      "Training loss for batch 1555 : 0.3234536349773407\n",
      "Training loss for batch 1556 : 0.17744848132133484\n",
      "Training loss for batch 1557 : 0.20601125061511993\n",
      "Training loss for batch 1558 : 0.10200469195842743\n",
      "Training loss for batch 1559 : 0.26945164799690247\n",
      "Training loss for batch 1560 : 0.2359461635351181\n",
      "Training loss for batch 1561 : 0.14075598120689392\n",
      "Training loss for batch 1562 : 0.13796518743038177\n",
      "Training loss for batch 1563 : 0.0031546931713819504\n",
      "Training loss for batch 1564 : 0.18139588832855225\n",
      "Training loss for batch 1565 : 0.0907398909330368\n",
      "Training loss for batch 1566 : 0.18017235398292542\n",
      "Training loss for batch 1567 : 0.1397537887096405\n",
      "Training loss for batch 1568 : 0.04927756264805794\n",
      "Training loss for batch 1569 : 0.024454908445477486\n",
      "Training loss for batch 1570 : 0.11064475774765015\n",
      "Training loss for batch 1571 : 0.09477131068706512\n",
      "Training loss for batch 1572 : 0.08043424040079117\n",
      "Training loss for batch 1573 : 0.12365581840276718\n",
      "Training loss for batch 1574 : 0.03842606768012047\n",
      "Training loss for batch 1575 : 0.013951782137155533\n",
      "Training loss for batch 1576 : 0.025999121367931366\n",
      "Training loss for batch 1577 : 0.14487798511981964\n",
      "Training loss for batch 1578 : 0.03369274362921715\n",
      "Training loss for batch 1579 : 0.09108319133520126\n",
      "Training loss for batch 1580 : 0.20974445343017578\n",
      "Training loss for batch 1581 : 0.11120970547199249\n",
      "Training loss for batch 1582 : 0.030498582869768143\n",
      "Training loss for batch 1583 : 0.021194761618971825\n",
      "Training loss for batch 1584 : 0.2431115359067917\n",
      "Training loss for batch 1585 : 0.13137918710708618\n",
      "Training loss for batch 1586 : 0.18474352359771729\n",
      "Training loss for batch 1587 : 0.036802083253860474\n",
      "Training loss for batch 1588 : 0.08517055958509445\n",
      "Training loss for batch 1589 : 0.09704436361789703\n",
      "Training loss for batch 1590 : 0.18279635906219482\n",
      "Training loss for batch 1591 : 0.09713726490736008\n",
      "Training loss for batch 1592 : 0.031648360192775726\n",
      "Training loss for batch 1593 : 0.04456741735339165\n",
      "Training loss for batch 1594 : 0.06701141595840454\n",
      "Training loss for batch 1595 : 0.04684298858046532\n",
      "Training loss for batch 1596 : 0.04797535762190819\n",
      "Training loss for batch 1597 : 0.02429080754518509\n",
      "Training loss for batch 1598 : 0.12098022550344467\n",
      "Training loss for batch 1599 : 0.030856328085064888\n",
      "Training loss for batch 1600 : 0.21981224417686462\n",
      "Training loss for batch 1601 : 0.044941991567611694\n",
      "Training loss for batch 1602 : 0.16636182367801666\n",
      "Training loss for batch 1603 : 0.045923132449388504\n",
      "Training loss for batch 1604 : 0.09384341537952423\n",
      "Training loss for batch 1605 : 0.012891788966953754\n",
      "Training loss for batch 1606 : 0.005941156763583422\n",
      "Training loss for batch 1607 : 0.23396289348602295\n",
      "Training loss for batch 1608 : 0.14434196054935455\n",
      "Training loss for batch 1609 : 0.20502851903438568\n",
      "Training loss for batch 1610 : 0.09724660962820053\n",
      "Training loss for batch 1611 : 0.07293162494897842\n",
      "Training loss for batch 1612 : 0.086652472615242\n",
      "Training loss for batch 1613 : 0.002178839175030589\n",
      "Training loss for batch 1614 : 0.08892045170068741\n",
      "Training loss for batch 1615 : 0.17024315893650055\n",
      "Training loss for batch 1616 : 0.13576138019561768\n",
      "Training loss for batch 1617 : 0.11005490273237228\n",
      "Training loss for batch 1618 : 0.09418731927871704\n",
      "Training loss for batch 1619 : 0.053461216390132904\n",
      "Training loss for batch 1620 : 0.17008450627326965\n",
      "Training loss for batch 1621 : 0.02574179694056511\n",
      "Training loss for batch 1622 : 0.04604472219944\n",
      "Training loss for batch 1623 : 0.10409387946128845\n",
      "Training loss for batch 1624 : 0.11046022921800613\n",
      "Training loss for batch 1625 : 0.08574455976486206\n",
      "Training loss for batch 1626 : 0.05380455404520035\n",
      "Training loss for batch 1627 : 0.001271115499548614\n",
      "Training loss for batch 1628 : 0.08262546360492706\n",
      "Training loss for batch 1629 : 0.014884527772665024\n",
      "Training loss for batch 1630 : 0.004764861892908812\n",
      "Training loss for batch 1631 : 0.04358015954494476\n",
      "Training loss for batch 1632 : 0.09125438332557678\n",
      "Training loss for batch 1633 : 0.10451333969831467\n",
      "Training loss for batch 1634 : 0.005108281970024109\n",
      "Training loss for batch 1635 : 0.1419057697057724\n",
      "Training loss for batch 1636 : 0.014257097616791725\n",
      "Training loss for batch 1637 : 0.006698961369693279\n",
      "Training loss for batch 1638 : 0.02801906317472458\n",
      "Training loss for batch 1639 : 0.13321466743946075\n",
      "Training loss for batch 1640 : 0.1277991682291031\n",
      "Training loss for batch 1641 : 0.04287979379296303\n",
      "Training loss for batch 1642 : 0.06895837932825089\n",
      "Training loss for batch 1643 : 0.07980556041002274\n",
      "Training loss for batch 1644 : 0.08893482387065887\n",
      "Training loss for batch 1645 : 0.023349704220891\n",
      "Training loss for batch 1646 : 0.07477641105651855\n",
      "Training loss for batch 1647 : 0.12706653773784637\n",
      "Training loss for batch 1648 : 0.016493290662765503\n",
      "Training loss for batch 1649 : 0.021093741059303284\n",
      "Training loss for batch 1650 : 0.1554127186536789\n",
      "Training loss for batch 1651 : 0.0034863611217588186\n",
      "Training loss for batch 1652 : 0.07282418012619019\n",
      "Training loss for batch 1653 : 0.22358864545822144\n",
      "Training loss for batch 1654 : 0.1400580108165741\n",
      "Training loss for batch 1655 : 0.13839225471019745\n",
      "Training loss for batch 1656 : 0.17057378590106964\n",
      "Training loss for batch 1657 : 0.1035459041595459\n",
      "Training loss for batch 1658 : 0.12961401045322418\n",
      "Training loss for batch 1659 : 0.11507690697908401\n",
      "Training loss for batch 1660 : 0.015248401090502739\n",
      "Training loss for batch 1661 : 0.07324658334255219\n",
      "Training loss for batch 1662 : 0.13803023099899292\n",
      "Training loss for batch 1663 : 0.14383015036582947\n",
      "Training loss for batch 1664 : 0.06353960186243057\n",
      "Training loss for batch 1665 : 0.31659653782844543\n",
      "Training loss for batch 1666 : 0.0981905609369278\n",
      "Training loss for batch 1667 : 0.32625246047973633\n",
      "Training loss for batch 1668 : 0.04686478152871132\n",
      "Training loss for batch 1669 : 0.13400450348854065\n",
      "Training loss for batch 1670 : 0.19007816910743713\n",
      "Training loss for batch 1671 : 0.036437131464481354\n",
      "Training loss for batch 1672 : 0.12701338529586792\n",
      "Training loss for batch 1673 : 0.029422055929899216\n",
      "Training loss for batch 1674 : 0.15456704795360565\n",
      "Training loss for batch 1675 : 0.04585010185837746\n",
      "Training loss for batch 1676 : 0.2512475550174713\n",
      "Training loss for batch 1677 : 0.021929604932665825\n",
      "Training loss for batch 1678 : 0.11926725506782532\n",
      "Training loss for batch 1679 : 0.06951530277729034\n",
      "Training loss for batch 1680 : 0.23966744542121887\n",
      "Training loss for batch 1681 : 0.10963213443756104\n",
      "Training loss for batch 1682 : 0.05853566899895668\n",
      "Training loss for batch 1683 : 0.07895994186401367\n",
      "Training loss for batch 1684 : 0.23080934584140778\n",
      "Training loss for batch 1685 : 0.03158227354288101\n",
      "Training loss for batch 1686 : 0.04817570745944977\n",
      "Training loss for batch 1687 : 0.2567411959171295\n",
      "Training loss for batch 1688 : 0.07697798311710358\n",
      "Training loss for batch 1689 : 0.09437340497970581\n",
      "Training loss for batch 1690 : 0.2915060520172119\n",
      "Training loss for batch 1691 : 0.06922508776187897\n",
      "Training loss for batch 1692 : 0.11436935514211655\n",
      "Training loss for batch 1693 : 0.31769782304763794\n",
      "Training loss for batch 1694 : 0.024728618562221527\n",
      "Training loss for batch 1695 : 0.08542267978191376\n",
      "Training loss for batch 1696 : 0.1649155467748642\n",
      "Training loss for batch 1697 : 0.16546978056430817\n",
      "Training loss for batch 1698 : 0.14521431922912598\n",
      "Training loss for batch 1699 : 0.12440269440412521\n",
      "Training loss for batch 1700 : 0.0972193032503128\n",
      "Training loss for batch 1701 : 0.1942373514175415\n",
      "Training loss for batch 1702 : 0.27810734510421753\n",
      "Training loss for batch 1703 : 0.1277390718460083\n",
      "Training loss for batch 1704 : 0.19522090256214142\n",
      "Training loss for batch 1705 : 0.10065365582704544\n",
      "Training loss for batch 1706 : 0.25088414549827576\n",
      "Training loss for batch 1707 : 0.17150455713272095\n",
      "Training loss for batch 1708 : 0.09423983842134476\n",
      "Training loss for batch 1709 : 0.14243602752685547\n",
      "Training loss for batch 1710 : 0.17721955478191376\n",
      "Training loss for batch 1711 : 0.06379269808530807\n",
      "Training loss for batch 1712 : 0.08015944808721542\n",
      "Training loss for batch 1713 : 0.0357455313205719\n",
      "Training loss for batch 1714 : 0.12325264513492584\n",
      "Training loss for batch 1715 : 0.06816872954368591\n",
      "Training loss for batch 1716 : 0.0958981066942215\n",
      "Training loss for batch 1717 : 0.08374976366758347\n",
      "Training loss for batch 1718 : 0.12598232924938202\n",
      "Training loss for batch 1719 : 0.18346841633319855\n",
      "Training loss for batch 1720 : 0.1656588613986969\n",
      "Training loss for batch 1721 : 0.12375884503126144\n",
      "Training loss for batch 1722 : 0.16515140235424042\n",
      "Training loss for batch 1723 : 0.020531317219138145\n",
      "Training loss for batch 1724 : 0.16677139699459076\n",
      "Training loss for batch 1725 : 0.06181035563349724\n",
      "Training loss for batch 1726 : 0.19298425316810608\n",
      "Training loss for batch 1727 : 0.18417976796627045\n",
      "Training loss for batch 1728 : 0.1091790497303009\n",
      "Training loss for batch 1729 : 0.174143984913826\n",
      "Training loss for batch 1730 : 0.10131224989891052\n",
      "Training loss for batch 1731 : 0.02830555848777294\n",
      "Training loss for batch 1732 : 0.05907908082008362\n",
      "Training loss for batch 1733 : 0.14906303584575653\n",
      "Training loss for batch 1734 : 0.0794154703617096\n",
      "Training loss for batch 1735 : 0.06080492585897446\n",
      "Training loss for batch 1736 : 0.095281682908535\n",
      "Training loss for batch 1737 : 0.008296126499772072\n",
      "Training loss for batch 1738 : 0.07786034047603607\n",
      "Training loss for batch 1739 : 0.04986265301704407\n",
      "Training loss for batch 1740 : 0.14960429072380066\n",
      "Training loss for batch 1741 : 0.17519794404506683\n",
      "Training loss for batch 1742 : 0.1554546356201172\n",
      "Training loss for batch 1743 : 0.11846251785755157\n",
      "Training loss for batch 1744 : 0.06806749105453491\n",
      "Training loss for batch 1745 : 0.025236040353775024\n",
      "Training loss for batch 1746 : 0.08101049065589905\n",
      "Training loss for batch 1747 : 0.12637744843959808\n",
      "Training loss for batch 1748 : 0.07559140026569366\n",
      "Training loss for batch 1749 : 0.14140468835830688\n",
      "Training loss for batch 1750 : 0.1747300922870636\n",
      "Training loss for batch 1751 : 0.1194879561662674\n",
      "Training loss for batch 1752 : 0.14945101737976074\n",
      "Training loss for batch 1753 : 0.14235162734985352\n",
      "Training loss for batch 1754 : 0.07956337928771973\n",
      "Training loss for batch 1755 : 0.14906978607177734\n",
      "Training loss for batch 1756 : 0.19523735344409943\n",
      "Training loss for batch 1757 : 0.020665857940912247\n",
      "Training loss for batch 1758 : 0.125080406665802\n",
      "Training loss for batch 1759 : 0.1742379069328308\n",
      "Training loss for batch 1760 : 0.07484877854585648\n",
      "Training loss for batch 1761 : 0.12311835587024689\n",
      "Training loss for batch 1762 : 0.05286354944109917\n",
      "Training loss for batch 1763 : 0.0007179224630817771\n",
      "Training loss for batch 1764 : 0.20224213600158691\n",
      "Training loss for batch 1765 : 0.1523161381483078\n",
      "Training loss for batch 1766 : 0.11403963714838028\n",
      "Training loss for batch 1767 : 0.11441618949174881\n",
      "Training loss for batch 1768 : 0.03366825357079506\n",
      "Training loss for batch 1769 : 0.04551893472671509\n",
      "Training loss for batch 1770 : 0.23787717521190643\n",
      "Training loss for batch 1771 : 0.2603011131286621\n",
      "Training loss for batch 1772 : 0.07311727851629257\n",
      "Training loss for batch 1773 : 0.022867681458592415\n",
      "Training loss for batch 1774 : 0.004035860300064087\n",
      "Training loss for batch 1775 : 0.14697463810443878\n",
      "Training loss for batch 1776 : 0.07264276593923569\n",
      "Training loss for batch 1777 : 0.06026090309023857\n",
      "Training loss for batch 1778 : 0.16275008022785187\n",
      "Training loss for batch 1779 : 0.25878065824508667\n",
      "Training loss for batch 1780 : 0.1252986192703247\n",
      "Training loss for batch 1781 : 0.1727474182844162\n",
      "Training loss for batch 1782 : 0.06839976459741592\n",
      "Training loss for batch 1783 : 0.18067142367362976\n",
      "Training loss for batch 1784 : 0.020096851512789726\n",
      "Training loss for batch 1785 : 0.10837769508361816\n",
      "Training loss for batch 1786 : 0.0817095935344696\n",
      "Training loss for batch 1787 : 0.05861341953277588\n",
      "Training loss for batch 1788 : 0.25006794929504395\n",
      "Training loss for batch 1789 : 0.05826396495103836\n",
      "Training loss for batch 1790 : 0.08404853194952011\n",
      "Training loss for batch 1791 : 0.12900090217590332\n",
      "Training loss for batch 1792 : 0.15790888667106628\n",
      "Training loss for batch 1793 : 0.07607541233301163\n",
      "Training loss for batch 1794 : 0.10780305415391922\n",
      "Training loss for batch 1795 : 0.051656000316143036\n",
      "Training loss for batch 1796 : 0.04833246022462845\n",
      "Training loss for batch 1797 : 0.011247682385146618\n",
      "Training loss for batch 1798 : 0.09924981743097305\n",
      "Training loss for batch 1799 : 0.04513636231422424\n",
      "Training loss for batch 1800 : 0.13540515303611755\n",
      "Training loss for batch 1801 : 0.02915446273982525\n",
      "Training loss for batch 1802 : 0.19160600006580353\n",
      "Training loss for batch 1803 : 0.11140350252389908\n",
      "Training loss for batch 1804 : 0.104373499751091\n",
      "Training loss for batch 1805 : 0.14586375653743744\n",
      "Training loss for batch 1806 : 0.03610466048121452\n",
      "Training loss for batch 1807 : 0.19296112656593323\n",
      "Training loss for batch 1808 : 0.13782860338687897\n",
      "Training loss for batch 1809 : 0.13472329080104828\n",
      "Training loss for batch 1810 : 0.11041078716516495\n",
      "Training loss for batch 1811 : 0.1559123396873474\n",
      "Training loss for batch 1812 : 0.09337085485458374\n",
      "Training loss for batch 1813 : 0.0645623654127121\n",
      "Training loss for batch 1814 : 0.23447826504707336\n",
      "Training loss for batch 1815 : 0.16981135308742523\n",
      "Training loss for batch 1816 : 0.24070285260677338\n",
      "Training loss for batch 1817 : 0.048500798642635345\n",
      "Training loss for batch 1818 : 0.1524285525083542\n",
      "Training loss for batch 1819 : 0.03150321915745735\n",
      "Training loss for batch 1820 : 0.15027889609336853\n",
      "Training loss for batch 1821 : 0.1510767936706543\n",
      "Training loss for batch 1822 : 0.06854125112295151\n",
      "Training loss for batch 1823 : 0.10890804976224899\n",
      "Training loss for batch 1824 : 0.029103733599185944\n",
      "Training loss for batch 1825 : 0.05698244646191597\n",
      "Training loss for batch 1826 : 0.12283378839492798\n",
      "Training loss for batch 1827 : 0.08529464155435562\n",
      "Training loss for batch 1828 : 0.2491200566291809\n",
      "Training loss for batch 1829 : 0.1388053297996521\n",
      "Training loss for batch 1830 : 0.2846824526786804\n",
      "Training loss for batch 1831 : 0.12736330926418304\n",
      "Training loss for batch 1832 : 0.09628363698720932\n",
      "Training loss for batch 1833 : 0.16456501185894012\n",
      "Training loss for batch 1834 : 0.3027949035167694\n",
      "Training loss for batch 1835 : 0.060133520513772964\n",
      "Training loss for batch 1836 : 0.21469733119010925\n",
      "Training loss for batch 1837 : 0.048975735902786255\n",
      "Training loss for batch 1838 : 0.10197363048791885\n",
      "Training loss for batch 1839 : 0.20789627730846405\n",
      "Training loss for batch 1840 : 0.009440481662750244\n",
      "Training loss for batch 1841 : 0.24313977360725403\n",
      "Training loss for batch 1842 : 0.05236535146832466\n",
      "Training loss for batch 1843 : 0.23278269171714783\n",
      "Training loss for batch 1844 : 0.17423565685749054\n",
      "Training loss for batch 1845 : 0.15128934383392334\n",
      "Training loss for batch 1846 : 0.267214834690094\n",
      "Training loss for batch 1847 : 0.12061328440904617\n",
      "Training loss for batch 1848 : 0.21844011545181274\n",
      "Training loss for batch 1849 : 0.12226203083992004\n",
      "Training loss for batch 1850 : 0.011616582050919533\n",
      "Training loss for batch 1851 : 0.07585160434246063\n",
      "Training loss for batch 1852 : 0.11371605843305588\n",
      "Training loss for batch 1853 : 0.057273980230093\n",
      "Training loss for batch 1854 : 0.17781464755535126\n",
      "Training loss for batch 1855 : 0.1534915715456009\n",
      "Training loss for batch 1856 : 0.21153952181339264\n",
      "Training loss for batch 1857 : 0.2242240458726883\n",
      "Training loss for batch 1858 : 0.2633947432041168\n",
      "Training loss for batch 1859 : 0.1372239887714386\n",
      "Training loss for batch 1860 : 0.08705122768878937\n",
      "Training loss for batch 1861 : 0.07559194415807724\n",
      "Training loss for batch 1862 : 0.030777407810091972\n",
      "Training loss for batch 1863 : 0.11552006006240845\n",
      "Training loss for batch 1864 : 0.045077648013830185\n",
      "Training loss for batch 1865 : 0.27271217107772827\n",
      "Training loss for batch 1866 : 0.2614230811595917\n",
      "Training loss for batch 1867 : 0.08034279942512512\n",
      "Training loss for batch 1868 : 0.01248145941644907\n",
      "Training loss for batch 1869 : 0.17243720591068268\n",
      "Training loss for batch 1870 : 0.1883486658334732\n",
      "Training loss for batch 1871 : 0.06504867970943451\n",
      "Training loss for batch 1872 : 0.07299460470676422\n",
      "Training loss for batch 1873 : 0.06927140802145004\n",
      "Training loss for batch 1874 : 0.08280052244663239\n",
      "Training loss for batch 1875 : 0.11457270383834839\n",
      "Training loss for batch 1876 : 0.12390443682670593\n",
      "Training loss for batch 1877 : 0.10953504592180252\n",
      "Training loss for batch 1878 : 0.17930398881435394\n",
      "Training loss for batch 1879 : 0.032715559005737305\n",
      "Training loss for batch 1880 : 0.20927251875400543\n",
      "Training loss for batch 1881 : 0.14600065350532532\n",
      "Training loss for batch 1882 : 0.07583829015493393\n",
      "Training loss for batch 1883 : 0.13954296708106995\n",
      "Training loss for batch 1884 : 0.19982756674289703\n",
      "Training loss for batch 1885 : 0.1340230107307434\n",
      "Training loss for batch 1886 : 0.004941398743540049\n",
      "Training loss for batch 1887 : 0.21158531308174133\n",
      "Training loss for batch 1888 : 0.20408491790294647\n",
      "Training loss for batch 1889 : 0.10003791749477386\n",
      "Training loss for batch 1890 : 0.26747649908065796\n",
      "Training loss for batch 1891 : 0.03642469644546509\n",
      "Training loss for batch 1892 : 0.007532529998570681\n",
      "Training loss for batch 1893 : 0.060888927429914474\n",
      "Training loss for batch 1894 : 0.1611032634973526\n",
      "Training loss for batch 1895 : 0.09755049645900726\n",
      "Training loss for batch 1896 : 0.06133590266108513\n",
      "Training loss for batch 1897 : 0.05219615250825882\n",
      "Training loss for batch 1898 : 0.0741661787033081\n",
      "Training loss for batch 1899 : 0.08363393694162369\n",
      "Training loss for batch 1900 : 0.09264028072357178\n",
      "Training loss for batch 1901 : 0.26329368352890015\n",
      "Training loss for batch 1902 : 0.1813252866268158\n",
      "Training loss for batch 1903 : 0.06151117384433746\n",
      "Training loss for batch 1904 : 0.007118564564734697\n",
      "Training loss for batch 1905 : 0.05939408764243126\n",
      "Training loss for batch 1906 : 0.08176109939813614\n",
      "Training loss for batch 1907 : 0.09685239940881729\n",
      "Training loss for batch 1908 : 0.059112805873155594\n",
      "Training loss for batch 1909 : 0.04485635459423065\n",
      "Training loss for batch 1910 : 0.2664100229740143\n",
      "Training loss for batch 1911 : 0.26698747277259827\n",
      "Training loss for batch 1912 : 0.10280683636665344\n",
      "Training loss for batch 1913 : 0.04250195622444153\n",
      "Training loss for batch 1914 : 0.24574244022369385\n",
      "Training loss for batch 1915 : 0.18526765704154968\n",
      "Training loss for batch 1916 : 0.06008252501487732\n",
      "Training loss for batch 1917 : 0.0802290067076683\n",
      "Training loss for batch 1918 : 0.04914243519306183\n",
      "Training loss for batch 1919 : 0.026770094409585\n",
      "Training loss for batch 1920 : 0.025911876931786537\n",
      "Training loss for batch 1921 : 0.06205658242106438\n",
      "Training loss for batch 1922 : 0.05407495051622391\n",
      "Training loss for batch 1923 : 0.033316634595394135\n",
      "Training loss for batch 1924 : 0.049840204417705536\n",
      "Training loss for batch 1925 : 0.08961135894060135\n",
      "Training loss for batch 1926 : 0.07279454171657562\n",
      "Training loss for batch 1927 : 0.1398801952600479\n",
      "Training loss for batch 1928 : 0.047571927309036255\n",
      "Training loss for batch 1929 : 0.09358784556388855\n",
      "Training loss for batch 1930 : 0.048815932124853134\n",
      "Training loss for batch 1931 : 0.006267496384680271\n",
      "Training loss for batch 1932 : 0.07500623166561127\n",
      "Training loss for batch 1933 : 0.15055765211582184\n",
      "Training loss for batch 1934 : 0.09506413340568542\n",
      "Training loss for batch 1935 : 0.12387453019618988\n",
      "Training loss for batch 1936 : 0.03557358682155609\n",
      "Training loss for batch 1937 : 0.07932087033987045\n",
      "Training loss for batch 1938 : 0.03836323320865631\n",
      "Training loss for batch 1939 : 0.06710586696863174\n",
      "Training loss for batch 1940 : 0.06058530509471893\n",
      "Training loss for batch 1941 : 0.01956164464354515\n",
      "Training loss for batch 1942 : 0.20882856845855713\n",
      "Training loss for batch 1943 : 0.04610246792435646\n",
      "Training loss for batch 1944 : 0.05897081270813942\n",
      "Training loss for batch 1945 : 0.1840571165084839\n",
      "Training loss for batch 1946 : 0.017190292477607727\n",
      "Training loss for batch 1947 : 0.05182512477040291\n",
      "Training loss for batch 1948 : 0.1713881939649582\n",
      "Training loss for batch 1949 : 0.032383136451244354\n",
      "Training loss for batch 1950 : 0.1539991796016693\n",
      "Training loss for batch 1951 : 0.24346329271793365\n",
      "Training loss for batch 1952 : 0.02688995562493801\n",
      "Training loss for batch 1953 : 0.009605448693037033\n",
      "Training loss for batch 1954 : 0.16260969638824463\n",
      "Training loss for batch 1955 : 0.03120831400156021\n",
      "Training loss for batch 1956 : 0.10896984487771988\n",
      "Training loss for batch 1957 : 0.02404480054974556\n",
      "Training loss for batch 1958 : 0.017212599515914917\n",
      "Training loss for batch 1959 : 0.020986245945096016\n",
      "Training loss for batch 1960 : 0.036279067397117615\n",
      "Training loss for batch 1961 : 0.06521283090114594\n",
      "Training loss for batch 1962 : 0.1342308074235916\n",
      "Training loss for batch 1963 : 0.49666181206703186\n",
      "Training loss for batch 1964 : 0.20457467436790466\n",
      "Training loss for batch 1965 : 0.03697993606328964\n",
      "Training loss for batch 1966 : 0.036438144743442535\n",
      "Training loss for batch 1967 : 0.03336150199174881\n",
      "Training loss for batch 1968 : 0.1280064880847931\n",
      "Training loss for batch 1969 : 0.04726124182343483\n",
      "Training loss for batch 1970 : 0.0762484148144722\n",
      "Training loss for batch 1971 : 0.007195018697530031\n",
      "Training loss for batch 1972 : 0.18184883892536163\n",
      "Training loss for batch 1973 : 0.21079999208450317\n",
      "Training loss for batch 1974 : 0.13097551465034485\n",
      "Training loss for batch 1975 : 0.06347206234931946\n",
      "Training loss for batch 1976 : 0.23112799227237701\n",
      "Training loss for batch 1977 : 0.009257742203772068\n",
      "Training loss for batch 1978 : 0.18872545659542084\n",
      "Training loss for batch 1979 : 0.27114957571029663\n",
      "Training loss for batch 1980 : 0.12868329882621765\n",
      "Training loss for batch 1981 : 0.29147830605506897\n",
      "Training loss for batch 1982 : 0.050677523016929626\n",
      "Training loss for batch 1983 : 0.08419399708509445\n",
      "Training loss for batch 1984 : 0.08118972182273865\n",
      "Training loss for batch 1985 : 0.11756842583417892\n",
      "Training loss for batch 1986 : 0.17316655814647675\n",
      "Training loss for batch 1987 : 0.22083714604377747\n",
      "Training loss for batch 1988 : 0.09945859760046005\n",
      "Training loss for batch 1989 : 0.0726243257522583\n",
      "Training loss for batch 1990 : 0.10843245685100555\n",
      "Training loss for batch 1991 : 0.2330286204814911\n",
      "Training loss for batch 1992 : 0.10618919879198074\n",
      "Training loss for batch 1993 : 0.09630011022090912\n",
      "Training loss for batch 1994 : 0.03934306278824806\n",
      "Training loss for batch 1995 : 0.06001120060682297\n",
      "Training loss for batch 1996 : 0.101835697889328\n",
      "Training loss for batch 1997 : 0.02207222953438759\n",
      "Training loss for batch 1998 : 0.14860939979553223\n",
      "Training loss for batch 1999 : 0.08136515319347382\n",
      "Training loss for batch 2000 : 0.04167035222053528\n",
      "Training loss for batch 2001 : 0.17762427031993866\n",
      "Training loss for batch 2002 : 0.06065105274319649\n",
      "Training loss for batch 2003 : 0.0415230356156826\n",
      "Training loss for batch 2004 : 0.08740968257188797\n",
      "Training loss for batch 2005 : 0.08067842572927475\n",
      "Training loss for batch 2006 : 0.15259574353694916\n",
      "Training loss for batch 2007 : 0.11978233605623245\n",
      "Training loss for batch 2008 : 0.14972978830337524\n",
      "Training loss for batch 2009 : 0.009948799386620522\n",
      "Training loss for batch 2010 : 0.14395979046821594\n",
      "Training loss for batch 2011 : 0.1549157351255417\n",
      "Training loss for batch 2012 : 0.09109357744455338\n",
      "Training loss for batch 2013 : 0.11294470727443695\n",
      "Training loss for batch 2014 : 0.04200832173228264\n",
      "Training loss for batch 2015 : 0.09677322208881378\n",
      "Training loss for batch 2016 : 0.12903514504432678\n",
      "Training loss for batch 2017 : 0.26443541049957275\n",
      "Training loss for batch 2018 : 0.07802187651395798\n",
      "Training loss for batch 2019 : 0.00021013089281041175\n",
      "Training loss for batch 2020 : 0.07500015199184418\n",
      "Training loss for batch 2021 : 0.2395058423280716\n",
      "Training loss for batch 2022 : 0.053626932203769684\n",
      "Training loss for batch 2023 : 0.0423792265355587\n",
      "Training loss for batch 2024 : 0.04642011597752571\n",
      "Training loss for batch 2025 : 0.00943866278976202\n",
      "Training loss for batch 2026 : 0.09387782216072083\n",
      "Training loss for batch 2027 : 0.19690759479999542\n",
      "Training loss for batch 2028 : 0.09763110429048538\n",
      "Training loss for batch 2029 : 0.051076725125312805\n",
      "Training loss for batch 2030 : 0.0028395080007612705\n",
      "Training loss for batch 2031 : 0.06049158051609993\n",
      "Training loss for batch 2032 : 0.019726615399122238\n",
      "Training loss for batch 2033 : 0.10225702822208405\n",
      "Training loss for batch 2034 : 0.02755655162036419\n",
      "Training loss for batch 2035 : 0.06396891921758652\n",
      "Training loss for batch 2036 : 0.05723971500992775\n",
      "Training loss for batch 2037 : 0.012530817650258541\n",
      "Training loss for batch 2038 : 0.1909445971250534\n",
      "Training loss for batch 2039 : 0.028927555307745934\n",
      "Training loss for batch 2040 : 0.14876076579093933\n",
      "Training loss for batch 2041 : 0.26919442415237427\n",
      "Training loss for batch 2042 : 0.012712853960692883\n",
      "Training loss for batch 2043 : 0.044038254767656326\n",
      "Training loss for batch 2044 : 0.14667434990406036\n",
      "Training loss for batch 2045 : 0.07025521993637085\n",
      "Training loss for batch 2046 : 0.045800015330314636\n",
      "Training loss for batch 2047 : 0.04419814422726631\n",
      "Training loss for batch 2048 : 0.020195772871375084\n",
      "Training loss for batch 2049 : 0.0027368715964257717\n",
      "Training loss for batch 2050 : 0.10800671577453613\n",
      "Training loss for batch 2051 : 0.04839318245649338\n",
      "Training loss for batch 2052 : 0.2723354399204254\n",
      "Training loss for batch 2053 : 0.10484116524457932\n",
      "Training loss for batch 2054 : 0.12947383522987366\n",
      "Training loss for batch 2055 : 0.07359044998884201\n",
      "Training loss for batch 2056 : 0.05056255683302879\n",
      "Training loss for batch 2057 : 0.08941314369440079\n",
      "Training loss for batch 2058 : 0.2753792703151703\n",
      "Training loss for batch 2059 : 0.021511662751436234\n",
      "Training loss for batch 2060 : 0.33959484100341797\n",
      "Training loss for batch 2061 : 0.1287137269973755\n",
      "Training loss for batch 2062 : 0.04465439170598984\n",
      "Training loss for batch 2063 : 0.22960887849330902\n",
      "Training loss for batch 2064 : 0.2361512929201126\n",
      "Training loss for batch 2065 : 0.010576781816780567\n",
      "Training loss for batch 2066 : 0.18637171387672424\n",
      "Training loss for batch 2067 : 0.12175874412059784\n",
      "Training loss for batch 2068 : 0.1010456532239914\n",
      "Training loss for batch 2069 : 0.09494063258171082\n",
      "Training loss for batch 2070 : 0.2966673970222473\n",
      "Training loss for batch 2071 : 0.05020033195614815\n",
      "Training loss for batch 2072 : 0.06009221076965332\n",
      "Training loss for batch 2073 : 0.0034380480647087097\n",
      "Training loss for batch 2074 : 0.07279281318187714\n",
      "Training loss for batch 2075 : 0.08388227969408035\n",
      "Training loss for batch 2076 : 0.15242549777030945\n",
      "Training loss for batch 2077 : 0.04151579365134239\n",
      "Training loss for batch 2078 : 0.026732923462986946\n",
      "Training loss for batch 2079 : 0.3847190737724304\n",
      "Training loss for batch 2080 : 0.2289818376302719\n",
      "Training loss for batch 2081 : 0.18208922445774078\n",
      "Training loss for batch 2082 : 0.16479112207889557\n",
      "Training loss for batch 2083 : 0.1314655989408493\n",
      "Training loss for batch 2084 : 0.07996165752410889\n",
      "Training loss for batch 2085 : 0.09330819547176361\n",
      "Training loss for batch 2086 : 0.1841774731874466\n",
      "Training loss for batch 2087 : 0.11188916116952896\n",
      "Training loss for batch 2088 : 0.12212628126144409\n",
      "Training loss for batch 2089 : 0.15713974833488464\n",
      "Training loss for batch 2090 : 0.08310006558895111\n",
      "Training loss for batch 2091 : 0.08583180606365204\n",
      "Training loss for batch 2092 : 0.22360298037528992\n",
      "Training loss for batch 2093 : 0.1895146667957306\n",
      "Training loss for batch 2094 : 0.03594670817255974\n",
      "Training loss for batch 2095 : 0.04435339570045471\n",
      "Training loss for batch 2096 : 0.1541931927204132\n",
      "Training loss for batch 2097 : 0.157087042927742\n",
      "Training loss for batch 2098 : 0.1091260239481926\n",
      "Training loss for batch 2099 : 0.16844025254249573\n",
      "Training loss for batch 2100 : 0.08319748938083649\n",
      "Training loss for batch 2101 : 0.12193328142166138\n",
      "Training loss for batch 2102 : 0.03622252121567726\n",
      "Training loss for batch 2103 : 0.09641443192958832\n",
      "Training loss for batch 2104 : 0.11653148382902145\n",
      "Training loss for batch 2105 : 0.12287770956754684\n",
      "Training loss for batch 2106 : 0.20311108231544495\n",
      "Training loss for batch 2107 : 0.18845751881599426\n",
      "Training loss for batch 2108 : 0.07566814869642258\n",
      "Training loss for batch 2109 : 0.2768469750881195\n",
      "Training loss for batch 2110 : 0.08150237798690796\n",
      "Training loss for batch 2111 : 0.06422729790210724\n",
      "Training loss for batch 2112 : 0.03451455757021904\n",
      "Training loss for batch 2113 : 0.10078540444374084\n",
      "Training loss for batch 2114 : 0.14858371019363403\n",
      "Training loss for batch 2115 : 0.19691559672355652\n",
      "Training loss for batch 2116 : 0.19356471300125122\n",
      "Training loss for batch 2117 : 0.22951366007328033\n",
      "Training loss for batch 2118 : 0.19025056064128876\n",
      "Training loss for batch 2119 : 0.17301127314567566\n",
      "Training loss for batch 2120 : 0.12839174270629883\n",
      "Training loss for batch 2121 : 0.11053422093391418\n",
      "Training loss for batch 2122 : 0.15036654472351074\n",
      "Training loss for batch 2123 : 0.04190370813012123\n",
      "Training loss for batch 2124 : 0.19729728996753693\n",
      "Training loss for batch 2125 : 0.07769250869750977\n",
      "Training loss for batch 2126 : 0.06092130020260811\n",
      "Training loss for batch 2127 : 0.22869189083576202\n",
      "Training loss for batch 2128 : 0.061262790113687515\n",
      "Training loss for batch 2129 : 0.1245313212275505\n",
      "Training loss for batch 2130 : 0.07630892843008041\n",
      "Training loss for batch 2131 : 0.06718657910823822\n",
      "Training loss for batch 2132 : 0.07935313880443573\n",
      "Training loss for batch 2133 : 0.026034900918602943\n",
      "Training loss for batch 2134 : 0.10938728600740433\n",
      "Training loss for batch 2135 : 0.10681552439928055\n",
      "Training loss for batch 2136 : 0.09631801396608353\n",
      "Training loss for batch 2137 : 0.12880684435367584\n",
      "Training loss for batch 2138 : 0.2671121060848236\n",
      "Training loss for batch 2139 : 0.2969169318675995\n",
      "Training loss for batch 2140 : 0.005255799274891615\n",
      "Training loss for batch 2141 : 0.08044032007455826\n",
      "Training loss for batch 2142 : 0.03141387552022934\n",
      "Training loss for batch 2143 : 0.06967548280954361\n",
      "Training loss for batch 2144 : 0.018419746309518814\n",
      "Training loss for batch 2145 : 0.08309561014175415\n",
      "Training loss for batch 2146 : 0.06032029539346695\n",
      "Training loss for batch 2147 : 0.10878041386604309\n",
      "Training loss for batch 2148 : 0.032712820917367935\n",
      "Training loss for batch 2149 : 0.25863373279571533\n",
      "Training loss for batch 2150 : 0.13137009739875793\n",
      "Training loss for batch 2151 : 0.11059466004371643\n",
      "Training loss for batch 2152 : 0.09457401186227798\n",
      "Training loss for batch 2153 : 0.08191011101007462\n",
      "Training loss for batch 2154 : 0.1603572517633438\n",
      "Training loss for batch 2155 : 0.021783707663416862\n",
      "Training loss for batch 2156 : 0.11642143875360489\n",
      "Training loss for batch 2157 : 0.0680912435054779\n",
      "Training loss for batch 2158 : 0.13021422922611237\n",
      "Training loss for batch 2159 : 0.030267607420682907\n",
      "Training loss for batch 2160 : 0.17464067041873932\n",
      "Training loss for batch 2161 : 0.03698322921991348\n",
      "Training loss for batch 2162 : 0.01233574841171503\n",
      "Training loss for batch 2163 : 0.04029006510972977\n",
      "Training loss for batch 2164 : 0.12494845688343048\n",
      "Training loss for batch 2165 : 0.03143470361828804\n",
      "Training loss for batch 2166 : 0.21667498350143433\n",
      "Training loss for batch 2167 : 0.2126387059688568\n",
      "Training loss for batch 2168 : 0.09067707508802414\n",
      "Training loss for batch 2169 : 0.20758065581321716\n",
      "Training loss for batch 2170 : 0.053264226764440536\n",
      "Training loss for batch 2171 : 0.21814382076263428\n",
      "Training loss for batch 2172 : 0.12422272562980652\n",
      "Training loss for batch 2173 : 0.1004871055483818\n",
      "Training loss for batch 2174 : 0.12396226823329926\n",
      "Training loss for batch 2175 : 0.18539750576019287\n",
      "Training loss for batch 2176 : 0.013320510275661945\n",
      "Training loss for batch 2177 : 0.00958959013223648\n",
      "Training loss for batch 2178 : 0.08760595321655273\n",
      "Training loss for batch 2179 : 0.07923993468284607\n",
      "Training loss for batch 2180 : 0.009096513502299786\n",
      "Training loss for batch 2181 : 0.23252655565738678\n",
      "Training loss for batch 2182 : 0.08018254488706589\n",
      "Training loss for batch 2183 : 0.0929844081401825\n",
      "Training loss for batch 2184 : 0.10636793822050095\n",
      "Training loss for batch 2185 : 0.058889199048280716\n",
      "Training loss for batch 2186 : 0.0995120033621788\n",
      "Training loss for batch 2187 : 0.029322432354092598\n",
      "Training loss for batch 2188 : 0.22979600727558136\n",
      "Training loss for batch 2189 : 0.21100018918514252\n",
      "Training loss for batch 2190 : 0.15283246338367462\n",
      "Training loss for batch 2191 : 0.07627470791339874\n",
      "Training loss for batch 2192 : 0.17177917063236237\n",
      "Training loss for batch 2193 : 0.17831450700759888\n",
      "Training loss for batch 2194 : 0.09114549309015274\n",
      "Training loss for batch 2195 : 0.02394339255988598\n",
      "Training loss for batch 2196 : 0.34051379561424255\n",
      "Training loss for batch 2197 : 0.17621061205863953\n",
      "Training loss for batch 2198 : 0.29489925503730774\n",
      "Training loss for batch 2199 : 0.07851415127515793\n",
      "Training loss for batch 2200 : 0.20535887777805328\n",
      "Training loss for batch 2201 : 0.2371310293674469\n",
      "Training loss for batch 2202 : 0.14312739670276642\n",
      "Training loss for batch 2203 : 0.1034148633480072\n",
      "Training loss for batch 2204 : 0.1292266547679901\n",
      "Training loss for batch 2205 : 0.0776151493191719\n",
      "Training loss for batch 2206 : 0.17927533388137817\n",
      "Training loss for batch 2207 : 0.05632717162370682\n",
      "Training loss for batch 2208 : 0.06763774156570435\n",
      "Training loss for batch 2209 : 0.009742885828018188\n",
      "Training loss for batch 2210 : 0.13645660877227783\n",
      "Training loss for batch 2211 : 0.07292607426643372\n",
      "Training loss for batch 2212 : 0.135080486536026\n",
      "Training loss for batch 2213 : 0.09206646680831909\n",
      "Training loss for batch 2214 : 0.10240752249956131\n",
      "Training loss for batch 2215 : 0.033910006284713745\n",
      "Training loss for batch 2216 : 0.016168294474482536\n",
      "Training loss for batch 2217 : 0.039067208766937256\n",
      "Training loss for batch 2218 : 0.10684164613485336\n",
      "Training loss for batch 2219 : 0.10470486432313919\n",
      "Training loss for batch 2220 : 0.0507236011326313\n",
      "Training loss for batch 2221 : 0.1284656971693039\n",
      "Training loss for batch 2222 : 0.16577528417110443\n",
      "Training loss for batch 2223 : 0.16058984398841858\n",
      "Training loss for batch 2224 : 0.17307457327842712\n",
      "Training loss for batch 2225 : 0.05920550599694252\n",
      "Training loss for batch 2226 : 0.09644514322280884\n",
      "Training loss for batch 2227 : 0.06109948083758354\n",
      "Training loss for batch 2228 : 0.05208045244216919\n",
      "Training loss for batch 2229 : 0.08329926431179047\n",
      "Training loss for batch 2230 : 0.14792568981647491\n",
      "Training loss for batch 2231 : 0.11009088158607483\n",
      "Training loss for batch 2232 : 0.07417704910039902\n",
      "Training loss for batch 2233 : 0.017423078417778015\n",
      "Training loss for batch 2234 : 0.033422116190195084\n",
      "Training loss for batch 2235 : 0.2102569341659546\n",
      "Training loss for batch 2236 : 0.06640927493572235\n",
      "Training loss for batch 2237 : 0.05923100933432579\n",
      "Training loss for batch 2238 : 0.1834057867527008\n",
      "Training loss for batch 2239 : 0.0838126614689827\n",
      "Training loss for batch 2240 : 0.08008828014135361\n",
      "Training loss for batch 2241 : 0.18199387192726135\n",
      "Training loss for batch 2242 : 0.27677005529403687\n",
      "Training loss for batch 2243 : 0.1136036366224289\n",
      "Training loss for batch 2244 : 0.10385600477457047\n",
      "Training loss for batch 2245 : 0.08191375434398651\n",
      "Training loss for batch 2246 : 0.264254093170166\n",
      "Training loss for batch 2247 : 0.16805538535118103\n",
      "Training loss for batch 2248 : 0.07646311074495316\n",
      "Training loss for batch 2249 : 0.06341344118118286\n",
      "Training loss for batch 2250 : 0.061613913625478745\n",
      "Training loss for batch 2251 : 0.02434871532022953\n",
      "Training loss for batch 2252 : 0.13689129054546356\n",
      "Training loss for batch 2253 : 0.1599189043045044\n",
      "Training loss for batch 2254 : 0.048899102956056595\n",
      "Training loss for batch 2255 : 0.4246106743812561\n",
      "Training loss for batch 2256 : 0.07448654621839523\n",
      "Training loss for batch 2257 : 0.4196409583091736\n",
      "Training loss for batch 2258 : 0.02568749524652958\n",
      "Training loss for batch 2259 : 0.09140851348638535\n",
      "Training loss for batch 2260 : 0.06995974481105804\n",
      "Training loss for batch 2261 : 0.3904366195201874\n",
      "Training loss for batch 2262 : 0.013822026550769806\n",
      "Training loss for batch 2263 : 0.1837771236896515\n",
      "Training loss for batch 2264 : 0.00844641774892807\n",
      "Training loss for batch 2265 : 0.10673632472753525\n",
      "Training loss for batch 2266 : 0.0922543853521347\n",
      "Training loss for batch 2267 : 0.08418742567300797\n",
      "Training loss for batch 2268 : 0.085685595870018\n",
      "Training loss for batch 2269 : 0.31102263927459717\n",
      "Training loss for batch 2270 : 0.015023356303572655\n",
      "Training loss for batch 2271 : 0.049313392490148544\n",
      "Training loss for batch 2272 : 0.022027134895324707\n",
      "Training loss for batch 2273 : 0.051660653203725815\n",
      "Training loss for batch 2274 : 0.05248396098613739\n",
      "Training loss for batch 2275 : 0.06742066890001297\n",
      "Training loss for batch 2276 : 0.23539447784423828\n",
      "Training loss for batch 2277 : 0.09898066520690918\n",
      "Training loss for batch 2278 : 0.17015673220157623\n",
      "Training loss for batch 2279 : 0.16552512347698212\n",
      "Training loss for batch 2280 : 0.01293710246682167\n",
      "Training loss for batch 2281 : 0.07812906801700592\n",
      "Training loss for batch 2282 : 0.19935593008995056\n",
      "Training loss for batch 2283 : 0.2191740721464157\n",
      "Training loss for batch 2284 : 0.02412748523056507\n",
      "Training loss for batch 2285 : 0.030440209433436394\n",
      "Training loss for batch 2286 : 0.14892368018627167\n",
      "Training loss for batch 2287 : 0.20820355415344238\n",
      "Training loss for batch 2288 : 0.05358410254120827\n",
      "Training loss for batch 2289 : 0.05180307850241661\n",
      "Training loss for batch 2290 : 0.033820975571870804\n",
      "Training loss for batch 2291 : 0.05205467715859413\n",
      "Training loss for batch 2292 : 0.18605168163776398\n",
      "Training loss for batch 2293 : 0.13180971145629883\n",
      "Training loss for batch 2294 : 0.08043816685676575\n",
      "Training loss for batch 2295 : 0.1326417773962021\n",
      "Training loss for batch 2296 : 0.024083763360977173\n",
      "Training loss for batch 2297 : 0.15131647884845734\n",
      "Training loss for batch 2298 : 0.022172732278704643\n",
      "Training loss for batch 2299 : 0.024823546409606934\n",
      "Training loss for batch 2300 : 0.13590967655181885\n",
      "Training loss for batch 2301 : 0.33883899450302124\n",
      "Training loss for batch 2302 : 0.29719457030296326\n",
      "Training loss for batch 2303 : 0.07535712420940399\n",
      "Training loss for batch 2304 : 0.13995829224586487\n",
      "Training loss for batch 2305 : 0.0807509794831276\n",
      "Training loss for batch 2306 : 0.11316674202680588\n",
      "Training loss for batch 2307 : 0.11866506189107895\n",
      "Training loss for batch 2308 : 0.16887804865837097\n",
      "Training loss for batch 2309 : 0.04005349427461624\n",
      "Training loss for batch 2310 : 0.007418700493872166\n",
      "Training loss for batch 2311 : 0.12023956328630447\n",
      "Training loss for batch 2312 : 0.23384059965610504\n",
      "Training loss for batch 2313 : 0.2167380452156067\n",
      "Training loss for batch 2314 : 0.06516770273447037\n",
      "Training loss for batch 2315 : 0.026042453944683075\n",
      "Training loss for batch 2316 : 0.41046759486198425\n",
      "Training loss for batch 2317 : 0.10103680193424225\n",
      "Training loss for batch 2318 : 0.18074892461299896\n",
      "Training loss for batch 2319 : 0.09029501676559448\n",
      "Training loss for batch 2320 : 0.05184738710522652\n",
      "Training loss for batch 2321 : 0.07450192421674728\n",
      "Training loss for batch 2322 : 0.11300459504127502\n",
      "Training loss for batch 2323 : 0.17741987109184265\n",
      "Training loss for batch 2324 : 0.21798627078533173\n",
      "Training loss for batch 2325 : 0.18273070454597473\n",
      "Training loss for batch 2326 : 0.18608909845352173\n",
      "Training loss for batch 2327 : 0.0438629649579525\n",
      "Training loss for batch 2328 : 0.1434861421585083\n",
      "Training loss for batch 2329 : 0.0\n",
      "Training loss for batch 2330 : 0.013972588814795017\n",
      "Training loss for batch 2331 : 0.2266787886619568\n",
      "Training loss for batch 2332 : 0.1187596544623375\n",
      "Training loss for batch 2333 : 0.038232266902923584\n",
      "Training loss for batch 2334 : 0.13853749632835388\n",
      "Training loss for batch 2335 : 0.09076742082834244\n",
      "Training loss for batch 2336 : 0.006024546455591917\n",
      "Training loss for batch 2337 : 0.14515405893325806\n",
      "Training loss for batch 2338 : 0.16104087233543396\n",
      "Training loss for batch 2339 : 0.09152418375015259\n",
      "Training loss for batch 2340 : 0.0932801365852356\n",
      "Training loss for batch 2341 : 0.1368015706539154\n",
      "Training loss for batch 2342 : 0.04101777449250221\n",
      "Training loss for batch 2343 : 0.10636717081069946\n",
      "Training loss for batch 2344 : 0.24846875667572021\n",
      "Training loss for batch 2345 : 0.068661630153656\n",
      "Training loss for batch 2346 : 0.05090101435780525\n",
      "Training loss for batch 2347 : 0.23536595702171326\n",
      "Training loss for batch 2348 : 0.06352020055055618\n",
      "Training loss for batch 2349 : 0.17061148583889008\n",
      "Training loss for batch 2350 : 0.12241411954164505\n",
      "Training loss for batch 2351 : 0.11598917096853256\n",
      "Training loss for batch 2352 : 0.14090923964977264\n",
      "Training loss for batch 2353 : 0.0834817960858345\n",
      "Training loss for batch 2354 : 0.14497479796409607\n",
      "Training loss for batch 2355 : 0.16456297039985657\n",
      "Training loss for batch 2356 : 0.19750040769577026\n",
      "Training loss for batch 2357 : 0.06954021006822586\n",
      "Training loss for batch 2358 : 0.09682664275169373\n",
      "Training loss for batch 2359 : 0.07840810716152191\n",
      "Training loss for batch 2360 : 0.18131329119205475\n",
      "Training loss for batch 2361 : 0.10451264679431915\n",
      "Training loss for batch 2362 : 0.07049775123596191\n",
      "Training loss for batch 2363 : 0.0696992501616478\n",
      "Training loss for batch 2364 : 0.18896999955177307\n",
      "Training loss for batch 2365 : 0.15301299095153809\n",
      "Training loss for batch 2366 : 0.0438135527074337\n",
      "Training loss for batch 2367 : 0.017178306356072426\n",
      "Training loss for batch 2368 : 0.20382891595363617\n",
      "Training loss for batch 2369 : 0.11392063647508621\n",
      "Training loss for batch 2370 : 0.17627881467342377\n",
      "Training loss for batch 2371 : 0.31521227955818176\n",
      "Training loss for batch 2372 : 0.046235423535108566\n",
      "Training loss for batch 2373 : 0.1315716952085495\n",
      "Training loss for batch 2374 : 0.0689471960067749\n",
      "Training loss for batch 2375 : 0.06885730475187302\n",
      "Training loss for batch 2376 : 0.07364804297685623\n",
      "Training loss for batch 2377 : 0.13336804509162903\n",
      "Training loss for batch 2378 : 0.25456178188323975\n",
      "Training loss for batch 2379 : 0.20186276733875275\n",
      "Training loss for batch 2380 : 0.2061273604631424\n",
      "Training loss for batch 2381 : 0.0793221965432167\n",
      "Training loss for batch 2382 : 0.10705767571926117\n",
      "Training loss for batch 2383 : 0.16554045677185059\n",
      "Training loss for batch 2384 : 0.0610835999250412\n",
      "Training loss for batch 2385 : 0.025028137490153313\n",
      "Training loss for batch 2386 : 0.19074572622776031\n",
      "Training loss for batch 2387 : 0.05333748832345009\n",
      "Training loss for batch 2388 : 0.15888704359531403\n",
      "Training loss for batch 2389 : 0.20102420449256897\n",
      "Training loss for batch 2390 : 0.014036635868251324\n",
      "Training loss for batch 2391 : 0.04718147963285446\n",
      "Training loss for batch 2392 : 0.04321232810616493\n",
      "Training loss for batch 2393 : 0.03540864586830139\n",
      "Training loss for batch 2394 : 0.1489844024181366\n",
      "Training loss for batch 2395 : 0.21072660386562347\n",
      "Training loss for batch 2396 : 0.12471499294042587\n",
      "Training loss for batch 2397 : 0.04904564842581749\n",
      "Training loss for batch 2398 : 0.19503353536128998\n",
      "Training loss for batch 2399 : 0.020655302330851555\n",
      "Training loss for batch 2400 : 0.055660177022218704\n",
      "Training loss for batch 2401 : 0.019598284736275673\n",
      "Training loss for batch 2402 : 0.20912520587444305\n",
      "Training loss for batch 2403 : 0.12822763621807098\n",
      "Training loss for batch 2404 : 0.14679883420467377\n",
      "Training loss for batch 2405 : 0.04299289360642433\n",
      "Training loss for batch 2406 : 0.10937295109033585\n",
      "Training loss for batch 2407 : 0.16523543000221252\n",
      "Training loss for batch 2408 : 0.03440869227051735\n",
      "Training loss for batch 2409 : 0.016262514516711235\n",
      "Training loss for batch 2410 : 0.04071943089365959\n",
      "Training loss for batch 2411 : 0.07515959441661835\n",
      "Training loss for batch 2412 : 0.08663641661405563\n",
      "Training loss for batch 2413 : 0.06854257732629776\n",
      "Training loss for batch 2414 : 0.08581963181495667\n",
      "Training loss for batch 2415 : 0.07090837508440018\n",
      "Training loss for batch 2416 : 0.10368557274341583\n",
      "Training loss for batch 2417 : 0.09131353348493576\n",
      "Training loss for batch 2418 : 0.15938694775104523\n",
      "Training loss for batch 2419 : 0.11498428881168365\n",
      "Training loss for batch 2420 : 0.028996123000979424\n",
      "Training loss for batch 2421 : 0.10276094079017639\n",
      "Training loss for batch 2422 : 0.16160237789154053\n",
      "Training loss for batch 2423 : 0.1081957221031189\n",
      "Training loss for batch 2424 : 0.16198736429214478\n",
      "Training loss for batch 2425 : 0.055518798530101776\n",
      "Training loss for batch 2426 : 0.17710767686367035\n",
      "Training loss for batch 2427 : 0.30707257986068726\n",
      "Training loss for batch 2428 : 0.08529656380414963\n",
      "Training loss for batch 2429 : 0.06747011840343475\n",
      "Training loss for batch 2430 : 0.18772290647029877\n",
      "Training loss for batch 2431 : 0.1072213426232338\n",
      "Training loss for batch 2432 : 0.08087345212697983\n",
      "Training loss for batch 2433 : 0.19221045076847076\n",
      "Training loss for batch 2434 : 0.07389795780181885\n",
      "Training loss for batch 2435 : 0.03991573303937912\n",
      "Training loss for batch 2436 : 0.1819753795862198\n",
      "Training loss for batch 2437 : 0.26404958963394165\n",
      "Training loss for batch 2438 : 0.04995119944214821\n",
      "Training loss for batch 2439 : 0.12569047510623932\n",
      "Training loss for batch 2440 : 0.1936848908662796\n",
      "Training loss for batch 2441 : 0.026794590055942535\n",
      "Training loss for batch 2442 : 0.1391749083995819\n",
      "Training loss for batch 2443 : 0.09838315099477768\n",
      "Training loss for batch 2444 : 0.027192529290914536\n",
      "Training loss for batch 2445 : 0.28230687975883484\n",
      "Training loss for batch 2446 : 0.08027514815330505\n",
      "Training loss for batch 2447 : 0.04253658652305603\n",
      "Training loss for batch 2448 : 0.019430095329880714\n",
      "Training loss for batch 2449 : 0.24047882854938507\n",
      "Training loss for batch 2450 : 0.11837653815746307\n",
      "Training loss for batch 2451 : 0.04322431981563568\n",
      "Training loss for batch 2452 : 0.22498682141304016\n",
      "Training loss for batch 2453 : 0.14397823810577393\n",
      "Training loss for batch 2454 : 0.1528276652097702\n",
      "Training loss for batch 2455 : 0.11253712326288223\n",
      "Training loss for batch 2456 : 0.17862768471240997\n",
      "Training loss for batch 2457 : 0.20177717506885529\n",
      "Training loss for batch 2458 : 0.23936346173286438\n",
      "Training loss for batch 2459 : 0.25458818674087524\n",
      "Training loss for batch 2460 : 0.0028815190307796\n",
      "Training loss for batch 2461 : 0.11037732660770416\n",
      "Training loss for batch 2462 : 0.048281583935022354\n",
      "Training loss for batch 2463 : 0.21579256653785706\n",
      "Training loss for batch 2464 : 0.20972861349582672\n",
      "Training loss for batch 2465 : 0.09706293046474457\n",
      "Training loss for batch 2466 : 0.10147377103567123\n",
      "Training loss for batch 2467 : 0.073875792324543\n",
      "Training loss for batch 2468 : 0.08673135936260223\n",
      "Training loss for batch 2469 : 0.10549654066562653\n",
      "Training loss for batch 2470 : 0.09038116037845612\n",
      "Training loss for batch 2471 : 0.019779356196522713\n",
      "Training loss for batch 2472 : 0.1651783138513565\n",
      "Training loss for batch 2473 : 0.14192897081375122\n",
      "Training loss for batch 2474 : 0.0374622568488121\n",
      "Training loss for batch 2475 : 0.2493002563714981\n",
      "Training loss for batch 2476 : 0.12524960935115814\n",
      "Training loss for batch 2477 : 0.03759924694895744\n",
      "Training loss for batch 2478 : 0.04471912980079651\n",
      "Training loss for batch 2479 : 0.045719631016254425\n",
      "Training loss for batch 2480 : 0.10661341995000839\n",
      "Training loss for batch 2481 : 0.11171435564756393\n",
      "Training loss for batch 2482 : 0.015742463991045952\n",
      "Training loss for batch 2483 : 0.08602174371480942\n",
      "Training loss for batch 2484 : 0.272817999124527\n",
      "Training loss for batch 2485 : 0.052181143313646317\n",
      "Training loss for batch 2486 : 0.018640931695699692\n",
      "Training loss for batch 2487 : 0.23992633819580078\n",
      "Training loss for batch 2488 : 0.18869885802268982\n",
      "Training loss for batch 2489 : 0.2683330774307251\n",
      "Training loss for batch 2490 : 0.07882293313741684\n",
      "Training loss for batch 2491 : 0.08083830773830414\n",
      "Training loss for batch 2492 : 0.12884700298309326\n",
      "Training loss for batch 2493 : 0.11372489482164383\n",
      "Training loss for batch 2494 : 0.07966993004083633\n",
      "Training loss for batch 2495 : 0.1387445330619812\n",
      "Training loss for batch 2496 : 0.04224882647395134\n",
      "Training loss for batch 2497 : 0.2438121885061264\n",
      "Training loss for batch 2498 : 0.21276907622814178\n",
      "Training loss for batch 2499 : 0.15167246758937836\n",
      "Training loss for batch 2500 : 0.11288858205080032\n",
      "Training loss for batch 2501 : 0.09212770313024521\n",
      "Training loss for batch 2502 : 0.1986616849899292\n",
      "Training loss for batch 2503 : 0.14114446938037872\n",
      "Training loss for batch 2504 : 0.17642253637313843\n",
      "Training loss for batch 2505 : 0.10050154477357864\n",
      "Training loss for batch 2506 : 0.023186514154076576\n",
      "Training loss for batch 2507 : 0.2818610966205597\n",
      "Training loss for batch 2508 : 0.24660462141036987\n",
      "Training loss for batch 2509 : 0.09330733865499496\n",
      "Training loss for batch 2510 : 0.15888762474060059\n",
      "Training loss for batch 2511 : 0.02593677118420601\n",
      "Training loss for batch 2512 : 0.1918163001537323\n",
      "Training loss for batch 2513 : 0.058773431926965714\n",
      "Training loss for batch 2514 : 0.24356301128864288\n",
      "Training loss for batch 2515 : 0.003036817302927375\n",
      "Training loss for batch 2516 : 0.018728824332356453\n",
      "Training loss for batch 2517 : 0.07893550395965576\n",
      "Training loss for batch 2518 : 0.1852574348449707\n",
      "Training loss for batch 2519 : 0.20787043869495392\n",
      "Training loss for batch 2520 : 0.1333954781293869\n",
      "Training loss for batch 2521 : 0.021633604541420937\n",
      "Training loss for batch 2522 : 0.10807675868272781\n",
      "Training loss for batch 2523 : 0.07249735295772552\n",
      "Training loss for batch 2524 : 0.03880921006202698\n",
      "Training loss for batch 2525 : 0.046146318316459656\n",
      "Training loss for batch 2526 : 0.1364176720380783\n",
      "Training loss for batch 2527 : 0.15568721294403076\n",
      "Training loss for batch 2528 : 0.06461512297391891\n",
      "Training loss for batch 2529 : 0.11715567857027054\n",
      "Training loss for batch 2530 : 0.026240546256303787\n",
      "Training loss for batch 2531 : 0.11855557560920715\n",
      "Training loss for batch 2532 : 0.0709700658917427\n",
      "Training loss for batch 2533 : 0.09306453168392181\n",
      "Training loss for batch 2534 : 0.11801685392856598\n",
      "Training loss for batch 2535 : 0.061577487736940384\n",
      "Training loss for batch 2536 : 0.11352501064538956\n",
      "Training loss for batch 2537 : 0.06035865470767021\n",
      "Training loss for batch 2538 : 0.08781547099351883\n",
      "Training loss for batch 2539 : 0.10662522912025452\n",
      "Training loss for batch 2540 : 0.06327491998672485\n",
      "Training loss for batch 2541 : 0.09802380949258804\n",
      "Training loss for batch 2542 : 0.08082935959100723\n",
      "Training loss for batch 2543 : 0.10824906080961227\n",
      "Training loss for batch 2544 : 0.16313087940216064\n",
      "Training loss for batch 2545 : 0.07406876236200333\n",
      "Training loss for batch 2546 : 0.3707403838634491\n",
      "Training loss for batch 2547 : 0.110572449862957\n",
      "Training loss for batch 2548 : 0.12721753120422363\n",
      "Training loss for batch 2549 : 0.14233741164207458\n",
      "Training loss for batch 2550 : 0.022484129294753075\n",
      "Training loss for batch 2551 : 0.03708617389202118\n",
      "Training loss for batch 2552 : 0.31598010659217834\n",
      "Training loss for batch 2553 : 0.08628376573324203\n",
      "Training loss for batch 2554 : 0.08280816674232483\n",
      "Training loss for batch 2555 : 0.08797965943813324\n",
      "Training loss for batch 2556 : 0.1709042489528656\n",
      "Training loss for batch 2557 : 0.096554696559906\n",
      "Training loss for batch 2558 : 0.07819332927465439\n",
      "Training loss for batch 2559 : 0.17129895091056824\n",
      "Training loss for batch 2560 : 0.06067982316017151\n",
      "Training loss for batch 2561 : 0.23097831010818481\n",
      "Training loss for batch 2562 : 0.1598813384771347\n",
      "Training loss for batch 2563 : 0.08739496767520905\n",
      "Training loss for batch 2564 : 0.13663239777088165\n",
      "Training loss for batch 2565 : 0.0787760466337204\n",
      "Training loss for batch 2566 : 0.03655718266963959\n",
      "Training loss for batch 2567 : 0.14443469047546387\n",
      "Training loss for batch 2568 : 0.013225870206952095\n",
      "Training loss for batch 2569 : 0.04519341513514519\n",
      "Training loss for batch 2570 : 0.11083861440420151\n",
      "Training loss for batch 2571 : 0.15198342502117157\n",
      "Training loss for batch 2572 : 0.33932679891586304\n",
      "Training loss for batch 2573 : 0.05593341588973999\n",
      "Training loss for batch 2574 : 0.04768520966172218\n",
      "Training loss for batch 2575 : 0.2480616718530655\n",
      "Training loss for batch 2576 : 0.0424138642847538\n",
      "Training loss for batch 2577 : 0.2202402800321579\n",
      "Training loss for batch 2578 : 0.12425724416971207\n",
      "Training loss for batch 2579 : 0.22306080162525177\n",
      "Training loss for batch 2580 : 0.0018653779989108443\n",
      "Training loss for batch 2581 : 0.08804912865161896\n",
      "Training loss for batch 2582 : 0.04668032005429268\n",
      "Training loss for batch 2583 : 0.22643962502479553\n",
      "Training loss for batch 2584 : 0.01883736439049244\n",
      "Training loss for batch 2585 : 0.07414025068283081\n",
      "Training loss for batch 2586 : 0.005156602244824171\n",
      "Training loss for batch 2587 : 0.1108303889632225\n",
      "Training loss for batch 2588 : 0.10854386538267136\n",
      "Training loss for batch 2589 : 0.1155702993273735\n",
      "Training loss for batch 2590 : 0.0\n",
      "Training loss for batch 2591 : 0.18561454117298126\n",
      "Training loss for batch 2592 : 0.10241273045539856\n",
      "Training loss for batch 2593 : 0.049715809524059296\n",
      "Training loss for batch 2594 : 0.05058040842413902\n",
      "Training loss for batch 2595 : 0.011652118526399136\n",
      "Training loss for batch 2596 : 0.10049783438444138\n",
      "Training loss for batch 2597 : 0.018726754933595657\n",
      "Training loss for batch 2598 : 0.37130752205848694\n",
      "Training loss for batch 2599 : 0.14805558323860168\n",
      "Training loss for batch 2600 : 0.09823042154312134\n",
      "Training loss for batch 2601 : 0.20552533864974976\n",
      "Training loss for batch 2602 : 0.544982373714447\n",
      "Training loss for batch 2603 : 0.058656878769397736\n",
      "Training loss for batch 2604 : 0.013169129379093647\n",
      "Training loss for batch 2605 : 0.09312751889228821\n",
      "Training loss for batch 2606 : 0.09445652365684509\n",
      "Training loss for batch 2607 : 0.011836083605885506\n",
      "Training loss for batch 2608 : 0.20459161698818207\n",
      "Training loss for batch 2609 : 0.18442460894584656\n",
      "Training loss for batch 2610 : 0.1164320856332779\n",
      "Training loss for batch 2611 : 0.1095280796289444\n",
      "Training loss for batch 2612 : 0.09563643485307693\n",
      "Training loss for batch 2613 : 0.146294504404068\n",
      "Training loss for batch 2614 : 0.1090167760848999\n",
      "Training loss for batch 2615 : 0.20544880628585815\n",
      "Training loss for batch 2616 : 0.15744946897029877\n",
      "Training loss for batch 2617 : 0.017729418352246284\n",
      "Training loss for batch 2618 : 0.10256505012512207\n",
      "Training loss for batch 2619 : 0.09734285622835159\n",
      "Training loss for batch 2620 : 0.03419586643576622\n",
      "Training loss for batch 2621 : 0.03027304634451866\n",
      "Training loss for batch 2622 : 0.11063212901353836\n",
      "Training loss for batch 2623 : 0.215177521109581\n",
      "Training loss for batch 2624 : 0.021863805130124092\n",
      "Training loss for batch 2625 : 0.1058829054236412\n",
      "Training loss for batch 2626 : 0.014881056733429432\n",
      "Training loss for batch 2627 : 0.09839215129613876\n",
      "Training loss for batch 2628 : 0.23584316670894623\n",
      "Training loss for batch 2629 : 0.1184718981385231\n",
      "Training loss for batch 2630 : 0.059769731014966965\n",
      "Training loss for batch 2631 : 0.2201720029115677\n",
      "Training loss for batch 2632 : 0.19642391800880432\n",
      "Training loss for batch 2633 : 0.013080654665827751\n",
      "Training loss for batch 2634 : 0.2100532352924347\n",
      "Training loss for batch 2635 : 0.2759239375591278\n",
      "Training loss for batch 2636 : 0.14484591782093048\n",
      "Training loss for batch 2637 : 0.026059895753860474\n",
      "Training loss for batch 2638 : 0.2536814212799072\n",
      "Training loss for batch 2639 : 0.25151243805885315\n",
      "Training loss for batch 2640 : 0.06391516327857971\n",
      "Training loss for batch 2641 : 0.26040515303611755\n",
      "Training loss for batch 2642 : 0.020324068143963814\n",
      "Training loss for batch 2643 : 0.22575470805168152\n",
      "Training loss for batch 2644 : 0.084917351603508\n",
      "Training loss for batch 2645 : 0.08270960301160812\n",
      "Training loss for batch 2646 : 0.1511433720588684\n",
      "Training loss for batch 2647 : 0.05043397098779678\n",
      "Training loss for batch 2648 : 0.18193504214286804\n",
      "Training loss for batch 2649 : 0.22170406579971313\n",
      "Training loss for batch 2650 : 0.005497523583471775\n",
      "Training loss for batch 2651 : 0.08643309772014618\n",
      "Training loss for batch 2652 : 0.23747488856315613\n",
      "Training loss for batch 2653 : 0.09961593896150589\n",
      "Training loss for batch 2654 : 0.18641510605812073\n",
      "Training loss for batch 2655 : 0.21540749073028564\n",
      "Training loss for batch 2656 : 0.0060267397202551365\n",
      "Training loss for batch 2657 : 0.12549759447574615\n",
      "Training loss for batch 2658 : 0.06932630389928818\n",
      "Training loss for batch 2659 : 0.13839490711688995\n",
      "Training loss for batch 2660 : 0.012464095838367939\n",
      "Training loss for batch 2661 : 0.1361149549484253\n",
      "Training loss for batch 2662 : 0.10766465216875076\n",
      "Training loss for batch 2663 : 0.11289411038160324\n",
      "Training loss for batch 2664 : 0.04416028410196304\n",
      "Training loss for batch 2665 : 0.2254578173160553\n",
      "Training loss for batch 2666 : 0.18064656853675842\n",
      "Training loss for batch 2667 : 0.056051064282655716\n",
      "Training loss for batch 2668 : 0.14192791283130646\n",
      "Training loss for batch 2669 : 0.06719809770584106\n",
      "Training loss for batch 2670 : 0.12088239192962646\n",
      "Training loss for batch 2671 : 0.0055947438813745975\n",
      "Training loss for batch 2672 : 0.08796253800392151\n",
      "Training loss for batch 2673 : 0.01931421086192131\n",
      "Training loss for batch 2674 : 0.07999275624752045\n",
      "Training loss for batch 2675 : 0.2238660603761673\n",
      "Training loss for batch 2676 : 0.019402410835027695\n",
      "Training loss for batch 2677 : 0.1261788159608841\n",
      "Training loss for batch 2678 : 0.19324998557567596\n",
      "Training loss for batch 2679 : 0.10415973514318466\n",
      "Training loss for batch 2680 : 0.1403353065252304\n",
      "Training loss for batch 2681 : 0.06392191350460052\n",
      "Training loss for batch 2682 : 0.06250184029340744\n",
      "Training loss for batch 2683 : 0.24858926236629486\n",
      "Training loss for batch 2684 : 0.028065405786037445\n",
      "Training loss for batch 2685 : 0.09861831367015839\n",
      "Training loss for batch 2686 : 0.044836096465587616\n",
      "Training loss for batch 2687 : 0.2160544991493225\n",
      "Training loss for batch 2688 : 0.025047557428479195\n",
      "Training loss for batch 2689 : 0.06182101368904114\n",
      "Training loss for batch 2690 : 0.09164617955684662\n",
      "Training loss for batch 2691 : 0.01853993348777294\n",
      "Training loss for batch 2692 : 0.048212628811597824\n",
      "Training loss for batch 2693 : 0.19270239770412445\n",
      "Training loss for batch 2694 : 0.05682011693716049\n",
      "Training loss for batch 2695 : 0.037394627928733826\n",
      "Training loss for batch 2696 : 0.133316308259964\n",
      "Training loss for batch 2697 : 0.23726730048656464\n",
      "Training loss for batch 2698 : 0.06806820631027222\n",
      "Training loss for batch 2699 : 0.023099295794963837\n",
      "Training loss for batch 2700 : 0.03164706006646156\n",
      "Training loss for batch 2701 : 0.059209682047367096\n",
      "Training loss for batch 2702 : 0.05243702232837677\n",
      "Training loss for batch 2703 : 0.18487891554832458\n",
      "Training loss for batch 2704 : 0.05560855194926262\n",
      "Training loss for batch 2705 : 0.08751066774129868\n",
      "Training loss for batch 2706 : 0.14715562760829926\n",
      "Training loss for batch 2707 : 0.05206878110766411\n",
      "Training loss for batch 2708 : 0.23325037956237793\n",
      "Training loss for batch 2709 : 0.12582923471927643\n",
      "Training loss for batch 2710 : 0.17073942720890045\n",
      "Training loss for batch 2711 : 0.15154191851615906\n",
      "Training loss for batch 2712 : 0.08543995767831802\n",
      "Training loss for batch 2713 : 0.0592072531580925\n",
      "Training loss for batch 2714 : 0.038393884897232056\n",
      "Training loss for batch 2715 : 0.14905154705047607\n",
      "Training loss for batch 2716 : 0.009906823746860027\n",
      "Training loss for batch 2717 : 0.2948257029056549\n",
      "Training loss for batch 2718 : 0.07781452685594559\n",
      "Training loss for batch 2719 : 0.12259279191493988\n",
      "Training loss for batch 2720 : 0.1617175191640854\n",
      "Training loss for batch 2721 : 0.08530600368976593\n",
      "Training loss for batch 2722 : 0.23250903189182281\n",
      "Training loss for batch 2723 : 0.03588049113750458\n",
      "Training loss for batch 2724 : 0.13941453397274017\n",
      "Training loss for batch 2725 : 0.06297072768211365\n",
      "Training loss for batch 2726 : 0.1766379475593567\n",
      "Training loss for batch 2727 : 0.19112327694892883\n",
      "Training loss for batch 2728 : 0.06766999512910843\n",
      "Training loss for batch 2729 : 0.005294242408126593\n",
      "Training loss for batch 2730 : 0.17134533822536469\n",
      "Training loss for batch 2731 : 0.2292390763759613\n",
      "Training loss for batch 2732 : 0.1521836370229721\n",
      "Training loss for batch 2733 : 0.04662011191248894\n",
      "Training loss for batch 2734 : 0.2976934313774109\n",
      "Training loss for batch 2735 : 0.13628734648227692\n",
      "Training loss for batch 2736 : 0.08979254961013794\n",
      "Training loss for batch 2737 : 0.19154022634029388\n",
      "Training loss for batch 2738 : 0.01321384683251381\n",
      "Training loss for batch 2739 : 0.10730773955583572\n",
      "Training loss for batch 2740 : 0.11894028633832932\n",
      "Training loss for batch 2741 : 0.023170500993728638\n",
      "Training loss for batch 2742 : 0.015732593834400177\n",
      "Training loss for batch 2743 : 0.12025883793830872\n",
      "Training loss for batch 2744 : 0.05249699577689171\n",
      "Training loss for batch 2745 : 0.07809681445360184\n",
      "Training loss for batch 2746 : 0.05907934531569481\n",
      "Training loss for batch 2747 : 0.1535310447216034\n",
      "Training loss for batch 2748 : 0.15073372423648834\n",
      "Training loss for batch 2749 : 0.10585564374923706\n",
      "Training loss for batch 2750 : 0.2008265256881714\n",
      "Training loss for batch 2751 : 0.05257924273610115\n",
      "Training loss for batch 2752 : 0.05736599862575531\n",
      "Training loss for batch 2753 : 0.040045931935310364\n",
      "Training loss for batch 2754 : 0.07166688144207001\n",
      "Training loss for batch 2755 : 0.052855826914310455\n",
      "Training loss for batch 2756 : 0.17603811621665955\n",
      "Training loss for batch 2757 : 0.3112245798110962\n",
      "Training loss for batch 2758 : 0.26770761609077454\n",
      "Training loss for batch 2759 : 0.010528607293963432\n",
      "Training loss for batch 2760 : 0.041690532118082047\n",
      "Training loss for batch 2761 : 0.13863477110862732\n",
      "Training loss for batch 2762 : 0.16135141253471375\n",
      "Training loss for batch 2763 : 0.07902073860168457\n",
      "Training loss for batch 2764 : 0.09647319465875626\n",
      "Training loss for batch 2765 : 0.12973247468471527\n",
      "Training loss for batch 2766 : 0.04750880226492882\n",
      "Training loss for batch 2767 : 0.15533916652202606\n",
      "Training loss for batch 2768 : 0.08424841612577438\n",
      "Training loss for batch 2769 : 0.06974565982818604\n",
      "Training loss for batch 2770 : 0.044047724455595016\n",
      "Training loss for batch 2771 : 0.06202424317598343\n",
      "Training loss for batch 2772 : 0.06383399665355682\n",
      "Training loss for batch 2773 : 0.19319839775562286\n",
      "Training loss for batch 2774 : 0.04655018448829651\n",
      "Training loss for batch 2775 : 0.055388662964105606\n",
      "Training loss for batch 2776 : 0.07241037487983704\n",
      "Training loss for batch 2777 : 0.21102073788642883\n",
      "Training loss for batch 2778 : 0.07487636804580688\n",
      "Training loss for batch 2779 : 0.01723802648484707\n",
      "Training loss for batch 2780 : 0.31727051734924316\n",
      "Training loss for batch 2781 : 0.018569989129900932\n",
      "Training loss for batch 2782 : 0.11066494882106781\n",
      "Training loss for batch 2783 : 0.17613352835178375\n",
      "Training loss for batch 2784 : 0.11979062855243683\n",
      "Training loss for batch 2785 : 0.23969529569149017\n",
      "Training loss for batch 2786 : 0.11569833755493164\n",
      "Training loss for batch 2787 : 0.14847050607204437\n",
      "Training loss for batch 2788 : 0.015760473906993866\n",
      "Training loss for batch 2789 : 0.05332726240158081\n",
      "Training loss for batch 2790 : 0.05971558392047882\n",
      "Training loss for batch 2791 : 0.05132460966706276\n",
      "Training loss for batch 2792 : 0.08327513188123703\n",
      "Training loss for batch 2793 : 0.06762602925300598\n",
      "Training loss for batch 2794 : 0.10929334163665771\n",
      "Training loss for batch 2795 : 0.05315165966749191\n",
      "Training loss for batch 2796 : 0.10644453763961792\n",
      "Training loss for batch 2797 : 0.06712078303098679\n",
      "Training loss for batch 2798 : 0.014373662881553173\n",
      "Training loss for batch 2799 : 0.19366396963596344\n",
      "Training loss for batch 2800 : 0.05094003677368164\n",
      "Training loss for batch 2801 : 0.3564048707485199\n",
      "Training loss for batch 2802 : 0.06456416845321655\n",
      "Training loss for batch 2803 : 0.04212729632854462\n",
      "Training loss for batch 2804 : 0.21753738820552826\n",
      "Training loss for batch 2805 : 0.22572284936904907\n",
      "Training loss for batch 2806 : 0.2549459934234619\n",
      "Training loss for batch 2807 : 0.06638625264167786\n",
      "Training loss for batch 2808 : 0.12103822827339172\n",
      "Training loss for batch 2809 : 0.056075699627399445\n",
      "Training loss for batch 2810 : 0.10296901315450668\n",
      "Training loss for batch 2811 : 0.04230588674545288\n",
      "Training loss for batch 2812 : 0.23577438294887543\n",
      "Training loss for batch 2813 : 0.18835245072841644\n",
      "Training loss for batch 2814 : 0.07568962126970291\n",
      "Training loss for batch 2815 : 0.08808178454637527\n",
      "Training loss for batch 2816 : 0.07637769728899002\n",
      "Training loss for batch 2817 : 0.3447611629962921\n",
      "Training loss for batch 2818 : 0.12655207514762878\n",
      "Training loss for batch 2819 : 0.17245985567569733\n",
      "Training loss for batch 2820 : 0.04135196655988693\n",
      "Training loss for batch 2821 : 0.013406223617494106\n",
      "Training loss for batch 2822 : 0.09245440363883972\n",
      "Training loss for batch 2823 : 0.15931780636310577\n",
      "Training loss for batch 2824 : 0.10140644758939743\n",
      "Training loss for batch 2825 : 0.03905081748962402\n",
      "Training loss for batch 2826 : 0.259165495634079\n",
      "Training loss for batch 2827 : 0.05354022979736328\n",
      "Training loss for batch 2828 : 0.18145447969436646\n",
      "Training loss for batch 2829 : 0.3156113624572754\n",
      "Training loss for batch 2830 : 0.18625527620315552\n",
      "Training loss for batch 2831 : 0.14581656455993652\n",
      "Training loss for batch 2832 : 0.0803622230887413\n",
      "Training loss for batch 2833 : 0.027696100994944572\n",
      "Training loss for batch 2834 : 0.08307905495166779\n",
      "Training loss for batch 2835 : 0.1145654171705246\n",
      "Training loss for batch 2836 : 0.1202152669429779\n",
      "Training loss for batch 2837 : 0.07134778052568436\n",
      "Training loss for batch 2838 : 0.08757100254297256\n",
      "Training loss for batch 2839 : 0.13536356389522552\n",
      "Training loss for batch 2840 : 0.09990344941616058\n",
      "Training loss for batch 2841 : 0.15317736566066742\n",
      "Training loss for batch 2842 : 0.17335866391658783\n",
      "Training loss for batch 2843 : 0.09055238217115402\n",
      "Training loss for batch 2844 : 0.08060625940561295\n",
      "Training loss for batch 2845 : 0.07496070116758347\n",
      "Training loss for batch 2846 : 0.21166034042835236\n",
      "Training loss for batch 2847 : 0.08667906373739243\n",
      "Training loss for batch 2848 : 0.11572852730751038\n",
      "Training loss for batch 2849 : 0.06413052976131439\n",
      "Training loss for batch 2850 : 0.07740632444620132\n",
      "Training loss for batch 2851 : 0.1543150693178177\n",
      "Training loss for batch 2852 : 0.087872214615345\n",
      "Training loss for batch 2853 : 0.13077424466609955\n",
      "Training loss for batch 2854 : 0.17255094647407532\n",
      "Training loss for batch 2855 : 0.1741533726453781\n",
      "Training loss for batch 2856 : 0.13715499639511108\n",
      "Training loss for batch 2857 : 0.22006943821907043\n",
      "Training loss for batch 2858 : 0.07243205606937408\n",
      "Training loss for batch 2859 : 0.08094199001789093\n",
      "Training loss for batch 2860 : 0.24821141362190247\n",
      "Training loss for batch 2861 : 0.12622685730457306\n",
      "Training loss for batch 2862 : 0.052651021629571915\n",
      "Training loss for batch 2863 : 0.0647549107670784\n",
      "Training loss for batch 2864 : 0.053722478449344635\n",
      "Training loss for batch 2865 : 0.10051383823156357\n",
      "Training loss for batch 2866 : 0.18518507480621338\n",
      "Training loss for batch 2867 : 0.07297814637422562\n",
      "Training loss for batch 2868 : 0.11317761987447739\n",
      "Training loss for batch 2869 : 0.06850650161504745\n",
      "Training loss for batch 2870 : 0.0785524845123291\n",
      "Training loss for batch 2871 : 0.1842654049396515\n",
      "Training loss for batch 2872 : 0.08156824856996536\n",
      "Training loss for batch 2873 : 0.06502380967140198\n",
      "Training loss for batch 2874 : 0.20814628899097443\n",
      "Training loss for batch 2875 : 0.02924974076449871\n",
      "Training loss for batch 2876 : 0.03113820217549801\n",
      "Training loss for batch 2877 : 0.06233266368508339\n",
      "Training loss for batch 2878 : 0.08763575553894043\n",
      "Training loss for batch 2879 : 0.11927454173564911\n",
      "Training loss for batch 2880 : 0.05289652198553085\n",
      "Training loss for batch 2881 : 0.35891544818878174\n",
      "Training loss for batch 2882 : 0.23307175934314728\n",
      "Training loss for batch 2883 : 0.19641897082328796\n",
      "Training loss for batch 2884 : 0.0\n",
      "Training loss for batch 2885 : 0.1725284457206726\n",
      "Training loss for batch 2886 : 0.09874798357486725\n",
      "Training loss for batch 2887 : 0.07980381697416306\n",
      "Training loss for batch 2888 : 0.1389790028333664\n",
      "Training loss for batch 2889 : 0.08589959144592285\n",
      "Training loss for batch 2890 : 0.07977588474750519\n",
      "Training loss for batch 2891 : 0.10181187093257904\n",
      "Training loss for batch 2892 : 0.12025964260101318\n",
      "Training loss for batch 2893 : 0.02676834911108017\n",
      "Training loss for batch 2894 : 0.1803368330001831\n",
      "Training loss for batch 2895 : 0.030209455639123917\n",
      "Training loss for batch 2896 : 0.06430010497570038\n",
      "Training loss for batch 2897 : 0.030149705708026886\n",
      "Training loss for batch 2898 : 0.13298062980175018\n",
      "Training loss for batch 2899 : 0.2796768248081207\n",
      "Training loss for batch 2900 : 0.16208429634571075\n",
      "Training loss for batch 2901 : 0.09814774990081787\n",
      "Training loss for batch 2902 : 0.06569098681211472\n",
      "Training loss for batch 2903 : 0.044977277517318726\n",
      "Training loss for batch 2904 : 0.0682394951581955\n",
      "Training loss for batch 2905 : 0.046774350106716156\n",
      "Training loss for batch 2906 : 0.07201166450977325\n",
      "Training loss for batch 2907 : 0.16177967190742493\n",
      "Training loss for batch 2908 : 0.10401970893144608\n",
      "Training loss for batch 2909 : 0.012092383578419685\n",
      "Training loss for batch 2910 : 0.12946657836437225\n",
      "Training loss for batch 2911 : 0.13178732991218567\n",
      "Training loss for batch 2912 : 0.17979387938976288\n",
      "Training loss for batch 2913 : 0.017650285735726357\n",
      "Training loss for batch 2914 : 0.14927008748054504\n",
      "Training loss for batch 2915 : 0.026331117376685143\n",
      "Training loss for batch 2916 : 0.023194357752799988\n",
      "Training loss for batch 2917 : 0.2541221082210541\n",
      "Training loss for batch 2918 : 0.029332123696804047\n",
      "Training loss for batch 2919 : 1.3636246798398588e-08\n",
      "Training loss for batch 2920 : 0.20571331679821014\n",
      "Training loss for batch 2921 : 0.04193980619311333\n",
      "Training loss for batch 2922 : 0.02983107790350914\n",
      "Training loss for batch 2923 : 0.1513298749923706\n",
      "Training loss for batch 2924 : 0.07810770720243454\n",
      "Training loss for batch 2925 : 0.06107146665453911\n",
      "Training loss for batch 2926 : 0.13195237517356873\n",
      "Training loss for batch 2927 : 0.01961231790482998\n",
      "Training loss for batch 2928 : 0.030131762847304344\n",
      "Training loss for batch 2929 : 0.03164080157876015\n",
      "Training loss for batch 2930 : 0.04426262155175209\n",
      "Training loss for batch 2931 : 0.08329596370458603\n",
      "Training loss for batch 2932 : 0.09556442499160767\n",
      "Training loss for batch 2933 : 0.014272293075919151\n",
      "Training loss for batch 2934 : 0.05949389189481735\n",
      "Training loss for batch 2935 : 0.11084231734275818\n",
      "Training loss for batch 2936 : 0.13469889760017395\n",
      "Training loss for batch 2937 : 0.08427159488201141\n",
      "Training loss for batch 2938 : 0.02388463169336319\n",
      "Training loss for batch 2939 : 0.11090224981307983\n",
      "Training loss for batch 2940 : 0.14326417446136475\n",
      "Training loss for batch 2941 : 0.14042021334171295\n",
      "Training loss for batch 2942 : 0.03171183168888092\n",
      "Training loss for batch 2943 : 0.04371514916419983\n",
      "Training loss for batch 2944 : 0.21515171229839325\n",
      "Training loss for batch 2945 : 0.017075426876544952\n",
      "Training loss for batch 2946 : 0.007160093169659376\n",
      "Training loss for batch 2947 : 0.12480514496564865\n",
      "Training loss for batch 2948 : 0.31292060017585754\n",
      "Training loss for batch 2949 : 0.30780133605003357\n",
      "Training loss for batch 2950 : 0.07699020951986313\n",
      "Training loss for batch 2951 : 0.23898203670978546\n",
      "Training loss for batch 2952 : 0.22221893072128296\n",
      "Training loss for batch 2953 : 0.06686030328273773\n",
      "Training loss for batch 2954 : 0.04296357184648514\n",
      "Training loss for batch 2955 : 0.007691068109124899\n",
      "Training loss for batch 2956 : 0.028223734349012375\n",
      "Training loss for batch 2957 : 0.12376995384693146\n",
      "Training loss for batch 2958 : 0.15696904063224792\n",
      "Training loss for batch 2959 : 0.20053499937057495\n",
      "Training loss for batch 2960 : 0.07204858213663101\n",
      "Training loss for batch 2961 : 0.06457943469285965\n",
      "Training loss for batch 2962 : 0.33294954895973206\n",
      "Training loss for batch 2963 : 0.03859095275402069\n",
      "Training loss for batch 2964 : 0.26648229360580444\n",
      "Training loss for batch 2965 : 0.144972026348114\n",
      "Training loss for batch 2966 : 0.18815448880195618\n",
      "Training loss for batch 2967 : 0.09330767393112183\n",
      "Training loss for batch 2968 : 0.007576905190944672\n",
      "Training loss for batch 2969 : 0.1466677337884903\n",
      "Training loss for batch 2970 : 0.06417836993932724\n",
      "Training loss for batch 2971 : 0.06253180652856827\n",
      "Training loss for batch 2972 : 0.011168157681822777\n",
      "Training loss for batch 2973 : 0.2018805742263794\n",
      "Training loss for batch 2974 : 0.057249922305345535\n",
      "Training loss for batch 2975 : 0.11581975966691971\n",
      "Training loss for batch 2976 : 0.04179638251662254\n",
      "Training loss for batch 2977 : 0.058050140738487244\n",
      "Training loss for batch 2978 : 0.014733084477484226\n",
      "Training loss for batch 2979 : 0.017661403864622116\n",
      "Training loss for batch 2980 : 0.04264407977461815\n",
      "Training loss for batch 2981 : 0.09579594433307648\n",
      "Training loss for batch 2982 : 0.12799452245235443\n",
      "Training loss for batch 2983 : 0.07933153957128525\n",
      "Training loss for batch 2984 : 0.02642405964434147\n",
      "Training loss for batch 2985 : 0.17486128211021423\n",
      "Training loss for batch 2986 : 0.12152016907930374\n",
      "Training loss for batch 2987 : 0.06921105086803436\n",
      "Training loss for batch 2988 : 0.019396468997001648\n",
      "Training loss for batch 2989 : 0.07088770717382431\n",
      "Training loss for batch 2990 : 0.11399991810321808\n",
      "Training loss for batch 2991 : 0.08375650644302368\n",
      "Training loss for batch 2992 : 0.08773153275251389\n",
      "Training loss for batch 2993 : 0.2594577968120575\n",
      "Training loss for batch 2994 : 0.16016645729541779\n",
      "Training loss for batch 2995 : 0.038701582700014114\n",
      "Training loss for batch 2996 : 0.20755083858966827\n",
      "Training loss for batch 2997 : 0.017264191061258316\n",
      "Training loss for batch 2998 : 0.03574176877737045\n",
      "Training loss for batch 2999 : 0.10433422774076462\n",
      "Training loss for batch 3000 : 0.06243970990180969\n",
      "Training loss for batch 3001 : 0.10958777368068695\n",
      "Training loss for batch 3002 : 0.08405137062072754\n",
      "Training loss for batch 3003 : 0.03001461736857891\n",
      "Training loss for batch 3004 : 0.31470987200737\n",
      "Training loss for batch 3005 : 0.06347334384918213\n",
      "Training loss for batch 3006 : 0.015356752090156078\n",
      "Training loss for batch 3007 : 0.42851850390434265\n",
      "Training loss for batch 3008 : 0.16638219356536865\n",
      "Training loss for batch 3009 : 0.18397513031959534\n",
      "Training loss for batch 3010 : 0.046054907143116\n",
      "Training loss for batch 3011 : 0.060797739773988724\n",
      "Training loss for batch 3012 : 0.0008748075342737138\n",
      "Training loss for batch 3013 : 0.08775707334280014\n",
      "Training loss for batch 3014 : 0.055751733481884\n",
      "Training loss for batch 3015 : 0.025928858667612076\n",
      "Training loss for batch 3016 : 0.06862939149141312\n",
      "Training loss for batch 3017 : 0.2382115125656128\n",
      "Training loss for batch 3018 : 0.1605767458677292\n",
      "Training loss for batch 3019 : 0.06959483027458191\n",
      "Training loss for batch 3020 : 0.11769820749759674\n",
      "Training loss for batch 3021 : 0.03803335502743721\n",
      "Training loss for batch 3022 : 0.21261316537857056\n",
      "Training loss for batch 3023 : 0.049307286739349365\n",
      "Training loss for batch 3024 : 0.07863860577344894\n",
      "Training loss for batch 3025 : 0.17948243021965027\n",
      "Training loss for batch 3026 : 0.16464145481586456\n",
      "Training loss for batch 3027 : 0.09242777526378632\n",
      "Training loss for batch 3028 : 0.20379328727722168\n",
      "Training loss for batch 3029 : 0.19038288295269012\n",
      "Training loss for batch 3030 : 0.019625281915068626\n",
      "Training loss for batch 3031 : 0.11964831501245499\n",
      "Training loss for batch 3032 : 0.00017178335110656917\n",
      "Training loss for batch 3033 : 0.09829461574554443\n",
      "Training loss for batch 3034 : 0.13197815418243408\n",
      "Training loss for batch 3035 : 0.010005856864154339\n",
      "Training loss for batch 3036 : 0.12706655263900757\n",
      "Training loss for batch 3037 : 0.1592722237110138\n",
      "Training loss for batch 3038 : 0.12409018725156784\n",
      "Training loss for batch 3039 : 0.1859123855829239\n",
      "Training loss for batch 3040 : 0.12234359234571457\n",
      "Training loss for batch 3041 : 0.07907216995954514\n",
      "Training loss for batch 3042 : 0.1575869470834732\n",
      "Training loss for batch 3043 : 0.05732066556811333\n",
      "Training loss for batch 3044 : 0.08764054626226425\n",
      "Training loss for batch 3045 : 0.2429666817188263\n",
      "Training loss for batch 3046 : 0.06783644109964371\n",
      "Training loss for batch 3047 : 0.1247139424085617\n",
      "Training loss for batch 3048 : 0.2097572386264801\n",
      "Training loss for batch 3049 : 0.014737377874553204\n",
      "Training loss for batch 3050 : 0.12939390540122986\n",
      "Training loss for batch 3051 : 0.12729021906852722\n",
      "Training loss for batch 3052 : 0.1552557796239853\n",
      "Training loss for batch 3053 : 0.14928917586803436\n",
      "Training loss for batch 3054 : 0.07471819221973419\n",
      "Training loss for batch 3055 : 0.1294306516647339\n",
      "Training loss for batch 3056 : 0.008736470714211464\n",
      "Training loss for batch 3057 : 0.04409372806549072\n",
      "Training loss for batch 3058 : 0.07373468577861786\n",
      "Training loss for batch 3059 : 0.08343660086393356\n",
      "Training loss for batch 3060 : 0.02538352832198143\n",
      "Training loss for batch 3061 : 0.07146216928958893\n",
      "Training loss for batch 3062 : 0.031201504170894623\n",
      "Training loss for batch 3063 : 0.04181121289730072\n",
      "Training loss for batch 3064 : 0.19408676028251648\n",
      "Training loss for batch 3065 : 0.047681406140327454\n",
      "Training loss for batch 3066 : 0.03830508142709732\n",
      "Training loss for batch 3067 : 0.0420958548784256\n",
      "Training loss for batch 3068 : 0.013832774944603443\n",
      "Training loss for batch 3069 : 0.18592572212219238\n",
      "Training loss for batch 3070 : 0.13038061559200287\n",
      "Training loss for batch 3071 : 0.11862596124410629\n",
      "Training loss for batch 3072 : 0.08497319370508194\n",
      "Training loss for batch 3073 : 0.034399356693029404\n",
      "Training loss for batch 3074 : 0.06653595715761185\n",
      "Training loss for batch 3075 : 0.03263391926884651\n",
      "Training loss for batch 3076 : 0.040675174444913864\n",
      "Training loss for batch 3077 : 0.16321037709712982\n",
      "Training loss for batch 3078 : 0.10749667882919312\n",
      "Training loss for batch 3079 : 0.11316604167222977\n",
      "Training loss for batch 3080 : 0.17070220410823822\n",
      "Training loss for batch 3081 : 0.10098857432603836\n",
      "Training loss for batch 3082 : 0.1277477741241455\n",
      "Training loss for batch 3083 : 0.10855962336063385\n",
      "Training loss for batch 3084 : 0.12272656708955765\n",
      "Training loss for batch 3085 : 0.12936687469482422\n",
      "Training loss for batch 3086 : 0.17605392634868622\n",
      "Training loss for batch 3087 : 0.3051294982433319\n",
      "Training loss for batch 3088 : 0.11006815731525421\n",
      "Training loss for batch 3089 : 0.08641241490840912\n",
      "Training loss for batch 3090 : 0.23058465123176575\n",
      "Training loss for batch 3091 : 0.03502914682030678\n",
      "Training loss for batch 3092 : 0.10075145959854126\n",
      "Training loss for batch 3093 : 0.20598188042640686\n",
      "Training loss for batch 3094 : 0.18173643946647644\n",
      "Training loss for batch 3095 : 0.3235549330711365\n",
      "Training loss for batch 3096 : 0.07008595764636993\n",
      "Training loss for batch 3097 : 0.1843525618314743\n",
      "Training loss for batch 3098 : 0.1621904969215393\n",
      "Training loss for batch 3099 : 0.2568022310733795\n",
      "Training loss for batch 3100 : 0.17486755549907684\n",
      "Training loss for batch 3101 : 0.1057492196559906\n",
      "Training loss for batch 3102 : 0.1593034863471985\n",
      "Training loss for batch 3103 : 0.07626768946647644\n",
      "Training loss for batch 3104 : 0.11549248546361923\n",
      "Training loss for batch 3105 : 0.10032225400209427\n",
      "Training loss for batch 3106 : 0.2413613498210907\n",
      "Training loss for batch 3107 : 0.11969678103923798\n",
      "Training loss for batch 3108 : 0.03126462921500206\n",
      "Training loss for batch 3109 : 0.14591975510120392\n",
      "Training loss for batch 3110 : 0.11458837985992432\n",
      "Training loss for batch 3111 : 0.12335574626922607\n",
      "Training loss for batch 3112 : 0.19141285121440887\n",
      "Training loss for batch 3113 : 0.23647886514663696\n",
      "Training loss for batch 3114 : 0.13325141370296478\n",
      "Training loss for batch 3115 : 0.131569504737854\n",
      "Training loss for batch 3116 : 0.18219436705112457\n",
      "Training loss for batch 3117 : 0.011949056759476662\n",
      "Training loss for batch 3118 : 0.12917360663414001\n",
      "Training loss for batch 3119 : 0.18111377954483032\n",
      "Training loss for batch 3120 : 0.06393306702375412\n",
      "Training loss for batch 3121 : 0.06919731199741364\n",
      "Training loss for batch 3122 : 0.14594252407550812\n",
      "Training loss for batch 3123 : 0.11072181165218353\n",
      "Training loss for batch 3124 : 0.057176895439624786\n",
      "Training loss for batch 3125 : 0.05584532395005226\n",
      "Training loss for batch 3126 : 0.08678705245256424\n",
      "Training loss for batch 3127 : 0.14787337183952332\n",
      "Training loss for batch 3128 : 0.0837920680642128\n",
      "Training loss for batch 3129 : 0.10233339667320251\n",
      "Training loss for batch 3130 : 0.033874113112688065\n",
      "Training loss for batch 3131 : 0.03637554869055748\n",
      "Training loss for batch 3132 : 0.1855447143316269\n",
      "Training loss for batch 3133 : 0.1201893612742424\n",
      "Training loss for batch 3134 : 0.20284858345985413\n",
      "Training loss for batch 3135 : 0.07388477772474289\n",
      "Training loss for batch 3136 : 0.33576348423957825\n",
      "Training loss for batch 3137 : 0.024048008024692535\n",
      "Training loss for batch 3138 : 0.11312398314476013\n",
      "Training loss for batch 3139 : 0.10067103803157806\n",
      "Training loss for batch 3140 : 0.026194293051958084\n",
      "Training loss for batch 3141 : 0.09515340626239777\n",
      "Training loss for batch 3142 : 0.21072936058044434\n",
      "Training loss for batch 3143 : 0.14146554470062256\n",
      "Training loss for batch 3144 : 0.0965680181980133\n",
      "Training loss for batch 3145 : 0.0903981626033783\n",
      "Training loss for batch 3146 : 0.08508959412574768\n",
      "Training loss for batch 3147 : 0.13882441818714142\n",
      "Training loss for batch 3148 : 0.05393544211983681\n",
      "Training loss for batch 3149 : 0.09376931935548782\n",
      "Training loss for batch 3150 : 0.14499732851982117\n",
      "Training loss for batch 3151 : 0.13781708478927612\n",
      "Training loss for batch 3152 : 0.2758667469024658\n",
      "Training loss for batch 3153 : 0.14925767481327057\n",
      "Training loss for batch 3154 : 0.26394960284233093\n",
      "Training loss for batch 3155 : 0.09146422892808914\n",
      "Training loss for batch 3156 : 0.04059934616088867\n",
      "Training loss for batch 3157 : 0.09283355623483658\n",
      "Training loss for batch 3158 : 0.09154318273067474\n",
      "Training loss for batch 3159 : 0.19345839321613312\n",
      "Training loss for batch 3160 : 0.17345142364501953\n",
      "Training loss for batch 3161 : 0.05519577115774155\n",
      "Training loss for batch 3162 : 0.08183411508798599\n",
      "Training loss for batch 3163 : 0.14597761631011963\n",
      "Training loss for batch 3164 : 0.13929712772369385\n",
      "Training loss for batch 3165 : 0.11626403033733368\n",
      "Training loss for batch 3166 : 0.34086185693740845\n",
      "Training loss for batch 3167 : 0.11231223493814468\n",
      "Training loss for batch 3168 : 0.06332386285066605\n",
      "Training loss for batch 3169 : 0.14901262521743774\n",
      "Training loss for batch 3170 : 0.12839162349700928\n",
      "Training loss for batch 3171 : 0.1045483946800232\n",
      "Training loss for batch 3172 : 0.09706809371709824\n",
      "Training loss for batch 3173 : 0.16522938013076782\n",
      "Training loss for batch 3174 : 0.14271168410778046\n",
      "Training loss for batch 3175 : 0.23649443686008453\n",
      "Training loss for batch 3176 : 0.08127991110086441\n",
      "Training loss for batch 3177 : 0.0469672791659832\n",
      "Training loss for batch 3178 : 0.21289756894111633\n",
      "Training loss for batch 3179 : 0.13570848107337952\n",
      "Training loss for batch 3180 : 0.10425125807523727\n",
      "Training loss for batch 3181 : 0.19733747839927673\n",
      "Training loss for batch 3182 : 0.09989983588457108\n",
      "Training loss for batch 3183 : 0.4709828197956085\n",
      "Training loss for batch 3184 : 0.20794206857681274\n",
      "Training loss for batch 3185 : 0.07995974272489548\n",
      "Training loss for batch 3186 : 0.08482881635427475\n",
      "Training loss for batch 3187 : 0.04615817591547966\n",
      "Training loss for batch 3188 : 0.12670472264289856\n",
      "Training loss for batch 3189 : 0.13377848267555237\n",
      "Training loss for batch 3190 : 0.19254536926746368\n",
      "Training loss for batch 3191 : 0.036145180463790894\n",
      "Training loss for batch 3192 : 0.2248205840587616\n",
      "Training loss for batch 3193 : 0.050254348665475845\n",
      "Training loss for batch 3194 : 0.025988895446062088\n",
      "Training loss for batch 3195 : 0.23979108035564423\n",
      "Training loss for batch 3196 : 0.21864162385463715\n",
      "Training loss for batch 3197 : 0.12754561007022858\n",
      "Training loss for batch 3198 : 0.04347802326083183\n",
      "Training loss for batch 3199 : 0.27802661061286926\n",
      "Training loss for batch 3200 : 0.036934804171323776\n",
      "Training loss for batch 3201 : 0.16478584706783295\n",
      "Training loss for batch 3202 : 0.10063979029655457\n",
      "Training loss for batch 3203 : 0.26797449588775635\n",
      "Training loss for batch 3204 : 0.11787207424640656\n",
      "Training loss for batch 3205 : 0.09609576314687729\n",
      "Training loss for batch 3206 : 0.08928770571947098\n",
      "Training loss for batch 3207 : 0.10759308189153671\n",
      "Training loss for batch 3208 : 0.24494341015815735\n",
      "Training loss for batch 3209 : 0.09825458377599716\n",
      "Training loss for batch 3210 : 0.0838858038187027\n",
      "Training loss for batch 3211 : 0.0017762399511411786\n",
      "Training loss for batch 3212 : 0.14609752595424652\n",
      "Training loss for batch 3213 : 0.03321318328380585\n",
      "Training loss for batch 3214 : 0.166170135140419\n",
      "Training loss for batch 3215 : 0.19106368720531464\n",
      "Training loss for batch 3216 : 0.0339629091322422\n",
      "Training loss for batch 3217 : 0.3525599241256714\n",
      "Training loss for batch 3218 : 0.15094402432441711\n",
      "Training loss for batch 3219 : 0.15336303412914276\n",
      "Training loss for batch 3220 : 0.05495237931609154\n",
      "Training loss for batch 3221 : 0.21628794074058533\n",
      "Training loss for batch 3222 : 0.11871462315320969\n",
      "Training loss for batch 3223 : 0.06568530946969986\n",
      "Training loss for batch 3224 : 0.04042062908411026\n",
      "Training loss for batch 3225 : 0.23790137469768524\n",
      "Training loss for batch 3226 : 0.21183471381664276\n",
      "Training loss for batch 3227 : 0.03398861736059189\n",
      "Training loss for batch 3228 : 0.13308383524417877\n",
      "Training loss for batch 3229 : 0.1591024100780487\n",
      "Training loss for batch 3230 : 0.12354577332735062\n",
      "Training loss for batch 3231 : 0.023676348850131035\n",
      "Training loss for batch 3232 : 0.0950947031378746\n",
      "Training loss for batch 3233 : 0.013907245360314846\n",
      "Training loss for batch 3234 : 0.12615299224853516\n",
      "Training loss for batch 3235 : 0.2795250415802002\n",
      "Training loss for batch 3236 : 0.17255498468875885\n",
      "Training loss for batch 3237 : 0.08754244446754456\n",
      "Training loss for batch 3238 : 0.032719533890485764\n",
      "Training loss for batch 3239 : 0.2080143690109253\n",
      "Training loss for batch 3240 : 0.032601043581962585\n",
      "Training loss for batch 3241 : 0.169222891330719\n",
      "Training loss for batch 3242 : 0.03884140029549599\n",
      "Training loss for batch 3243 : 0.0849049985408783\n",
      "Training loss for batch 3244 : 0.14707957208156586\n",
      "Training loss for batch 3245 : 0.10743758827447891\n",
      "Training loss for batch 3246 : 0.09030412882566452\n",
      "Training loss for batch 3247 : 0.25023311376571655\n",
      "Training loss for batch 3248 : 0.06359857320785522\n",
      "Training loss for batch 3249 : 0.1811663955450058\n",
      "Training loss for batch 3250 : 0.2089332938194275\n",
      "Training loss for batch 3251 : 0.1671646386384964\n",
      "Training loss for batch 3252 : 0.05338766798377037\n",
      "Training loss for batch 3253 : 0.15137022733688354\n",
      "Training loss for batch 3254 : 0.018996339291334152\n",
      "Training loss for batch 3255 : 0.10612021386623383\n",
      "Training loss for batch 3256 : 0.2228616327047348\n",
      "Training loss for batch 3257 : 0.21066628396511078\n",
      "Training loss for batch 3258 : 0.09146945178508759\n",
      "Training loss for batch 3259 : 0.23629288375377655\n",
      "Training loss for batch 3260 : 0.08089429140090942\n",
      "Training loss for batch 3261 : 0.17552721500396729\n",
      "Training loss for batch 3262 : 0.06245384365320206\n",
      "Training loss for batch 3263 : 0.07444637268781662\n",
      "Training loss for batch 3264 : 0.061682187020778656\n",
      "Training loss for batch 3265 : 0.07393237203359604\n",
      "Training loss for batch 3266 : 0.3457105755805969\n",
      "Training loss for batch 3267 : 0.1456127166748047\n",
      "Training loss for batch 3268 : 0.011551299132406712\n",
      "Training loss for batch 3269 : 0.0186002179980278\n",
      "Training loss for batch 3270 : 0.16211634874343872\n",
      "Training loss for batch 3271 : 0.12803329527378082\n",
      "Training loss for batch 3272 : 0.1115923598408699\n",
      "Training loss for batch 3273 : 0.07577922940254211\n",
      "Training loss for batch 3274 : 0.29018640518188477\n",
      "Training loss for batch 3275 : 0.11929108202457428\n",
      "Training loss for batch 3276 : 0.21312101185321808\n",
      "Training loss for batch 3277 : 0.07596757262945175\n",
      "Training loss for batch 3278 : 0.2767030596733093\n",
      "Training loss for batch 3279 : 0.34588631987571716\n",
      "Training loss for batch 3280 : 0.10134254395961761\n",
      "Training loss for batch 3281 : 0.13822373747825623\n",
      "Training loss for batch 3282 : 0.20921041071414948\n",
      "Training loss for batch 3283 : 0.1447516828775406\n",
      "Training loss for batch 3284 : 0.11869823187589645\n",
      "Training loss for batch 3285 : 0.23697276413440704\n",
      "Training loss for batch 3286 : 0.1715066134929657\n",
      "Training loss for batch 3287 : 0.06193365529179573\n",
      "Training loss for batch 3288 : 0.12125799804925919\n",
      "Training loss for batch 3289 : 0.11769134551286697\n",
      "Training loss for batch 3290 : 0.20372863113880157\n",
      "Training loss for batch 3291 : 0.333619624376297\n",
      "Training loss for batch 3292 : 0.08160772174596786\n",
      "Training loss for batch 3293 : 0.11208115518093109\n",
      "Training loss for batch 3294 : 0.14793817698955536\n",
      "Training loss for batch 3295 : 0.11824595928192139\n",
      "Training loss for batch 3296 : 0.12064316868782043\n",
      "Training loss for batch 3297 : 0.04110487550497055\n",
      "Training loss for batch 3298 : 0.03562866523861885\n",
      "Training loss for batch 3299 : 0.14083729684352875\n",
      "Training loss for batch 3300 : 0.12405332177877426\n",
      "Training loss for batch 3301 : 0.1167919710278511\n",
      "Training loss for batch 3302 : 0.09967737644910812\n",
      "Training loss for batch 3303 : 0.08079070597887039\n",
      "Training loss for batch 3304 : 0.06734327226877213\n",
      "Training loss for batch 3305 : 0.1336587369441986\n",
      "Training loss for batch 3306 : 0.160630464553833\n",
      "Training loss for batch 3307 : 0.16820253431797028\n",
      "Training loss for batch 3308 : 0.07974669337272644\n",
      "Training loss for batch 3309 : 0.08415940403938293\n",
      "Training loss for batch 3310 : 0.031095091253519058\n",
      "Training loss for batch 3311 : 0.07526682317256927\n",
      "Training loss for batch 3312 : 0.09585406631231308\n",
      "Training loss for batch 3313 : 0.08000297099351883\n",
      "Training loss for batch 3314 : 0.010285750031471252\n",
      "Training loss for batch 3315 : 0.056725505739450455\n",
      "Training loss for batch 3316 : 0.21007734537124634\n",
      "Training loss for batch 3317 : 0.11165965348482132\n",
      "Training loss for batch 3318 : 0.11381375044584274\n",
      "Training loss for batch 3319 : 0.053319498896598816\n",
      "Training loss for batch 3320 : 0.04218744859099388\n",
      "Training loss for batch 3321 : 0.07160016894340515\n",
      "Training loss for batch 3322 : 0.0600028820335865\n",
      "Training loss for batch 3323 : 0.20196564495563507\n",
      "Training loss for batch 3324 : 0.08105862885713577\n",
      "Training loss for batch 3325 : 0.18601439893245697\n",
      "Training loss for batch 3326 : 0.19181400537490845\n",
      "Training loss for batch 3327 : 0.1738491803407669\n",
      "Training loss for batch 3328 : 0.11453071981668472\n",
      "Training loss for batch 3329 : 0.11119626462459564\n",
      "Training loss for batch 3330 : 0.14995177090168\n",
      "Training loss for batch 3331 : 0.05202503129839897\n",
      "Training loss for batch 3332 : 0.22795487940311432\n",
      "Training loss for batch 3333 : 0.10614913702011108\n",
      "Training loss for batch 3334 : 0.13797979056835175\n",
      "Training loss for batch 3335 : 0.13479653000831604\n",
      "Training loss for batch 3336 : 0.31400853395462036\n",
      "Training loss for batch 3337 : 0.08399998396635056\n",
      "Training loss for batch 3338 : 0.1380222737789154\n",
      "Training loss for batch 3339 : 0.0513407438993454\n",
      "Training loss for batch 3340 : 0.06860648095607758\n",
      "Training loss for batch 3341 : 0.08747751265764236\n",
      "Training loss for batch 3342 : 0.15641072392463684\n",
      "Training loss for batch 3343 : 0.06361181288957596\n",
      "Training loss for batch 3344 : 0.1493840217590332\n",
      "Training loss for batch 3345 : 0.04911694675683975\n",
      "Training loss for batch 3346 : 0.0464482381939888\n",
      "Training loss for batch 3347 : 0.2215280383825302\n",
      "Training loss for batch 3348 : 0.20985546708106995\n",
      "Training loss for batch 3349 : 0.1448032557964325\n",
      "Training loss for batch 3350 : 0.0721103698015213\n",
      "Training loss for batch 3351 : 0.040242601186037064\n",
      "Training loss for batch 3352 : 0.08263188600540161\n",
      "Training loss for batch 3353 : 0.015207255259156227\n",
      "Training loss for batch 3354 : 0.058828942477703094\n",
      "Training loss for batch 3355 : 0.10209418833255768\n",
      "Training loss for batch 3356 : 0.16050882637500763\n",
      "Training loss for batch 3357 : 0.12927010655403137\n",
      "Training loss for batch 3358 : 0.03545839712023735\n",
      "Training loss for batch 3359 : 0.15615667402744293\n",
      "Training loss for batch 3360 : 0.0777696967124939\n",
      "Training loss for batch 3361 : 0.10906657576560974\n",
      "Training loss for batch 3362 : 0.029690351337194443\n",
      "Training loss for batch 3363 : 0.105070099234581\n",
      "Training loss for batch 3364 : 0.19984900951385498\n",
      "Training loss for batch 3365 : 0.11476803570985794\n",
      "Training loss for batch 3366 : 0.0950930193066597\n",
      "Training loss for batch 3367 : 0.20774182677268982\n",
      "Training loss for batch 3368 : 0.06086110323667526\n",
      "Training loss for batch 3369 : 0.048834044486284256\n",
      "Training loss for batch 3370 : 0.11910179257392883\n",
      "Training loss for batch 3371 : 0.16619989275932312\n",
      "Training loss for batch 3372 : 0.14620724320411682\n",
      "Training loss for batch 3373 : 0.03992287442088127\n",
      "Training loss for batch 3374 : 0.08509865403175354\n",
      "Training loss for batch 3375 : 0.04642174392938614\n",
      "Training loss for batch 3376 : 0.30978208780288696\n",
      "Training loss for batch 3377 : 0.12617181241512299\n",
      "Training loss for batch 3378 : 0.20916612446308136\n",
      "Training loss for batch 3379 : 0.19754484295845032\n",
      "Training loss for batch 3380 : 0.123927041888237\n",
      "Training loss for batch 3381 : 0.06985078752040863\n",
      "Training loss for batch 3382 : 0.12078513950109482\n",
      "Training loss for batch 3383 : 0.08552538603544235\n",
      "Training loss for batch 3384 : 0.2610747814178467\n",
      "Training loss for batch 3385 : 0.09447912871837616\n",
      "Training loss for batch 3386 : 0.0681319460272789\n",
      "Training loss for batch 3387 : 0.12061996757984161\n",
      "Training loss for batch 3388 : 0.06163192540407181\n",
      "Training loss for batch 3389 : 0.09494347870349884\n",
      "Training loss for batch 3390 : 0.14698255062103271\n",
      "Training loss for batch 3391 : 0.10353454947471619\n",
      "Training loss for batch 3392 : 0.18602138757705688\n",
      "Training loss for batch 3393 : 0.09067454189062119\n",
      "Training loss for batch 3394 : 0.030480315908789635\n",
      "Training loss for batch 3395 : 0.1390516310930252\n",
      "Training loss for batch 3396 : 0.2128060758113861\n",
      "Training loss for batch 3397 : 0.0412098653614521\n",
      "Training loss for batch 3398 : 0.04235024377703667\n",
      "Training loss for batch 3399 : 0.0668003112077713\n",
      "Training loss for batch 3400 : 0.10962377488613129\n",
      "Training loss for batch 3401 : 0.14814957976341248\n",
      "Training loss for batch 3402 : 0.0545746386051178\n",
      "Training loss for batch 3403 : 0.17652706801891327\n",
      "Training loss for batch 3404 : 0.29512378573417664\n",
      "Training loss for batch 3405 : 0.007460033055394888\n",
      "Training loss for batch 3406 : 0.0502016544342041\n",
      "Training loss for batch 3407 : 0.19315676391124725\n",
      "Training loss for batch 3408 : 0.11390439420938492\n",
      "Training loss for batch 3409 : 0.009695940650999546\n",
      "Training loss for batch 3410 : 0.10308972746133804\n",
      "Training loss for batch 3411 : 0.2496389001607895\n",
      "Training loss for batch 3412 : 0.2051430493593216\n",
      "Training loss for batch 3413 : 0.044013611972332\n",
      "Training loss for batch 3414 : 0.062444910407066345\n",
      "Training loss for batch 3415 : 0.0665159672498703\n",
      "Training loss for batch 3416 : 0.06862939894199371\n",
      "Training loss for batch 3417 : 0.2301807850599289\n",
      "Training loss for batch 3418 : 0.16395719349384308\n",
      "Training loss for batch 3419 : 0.044588495045900345\n",
      "Training loss for batch 3420 : 0.04085051268339157\n",
      "Training loss for batch 3421 : 0.09561873972415924\n",
      "Training loss for batch 3422 : 0.05623495951294899\n",
      "Training loss for batch 3423 : 0.13383400440216064\n",
      "Training loss for batch 3424 : 0.20784235000610352\n",
      "Training loss for batch 3425 : 0.036669373512268066\n",
      "Training loss for batch 3426 : 0.012585504911839962\n",
      "Training loss for batch 3427 : 0.033922161906957626\n",
      "Training loss for batch 3428 : 0.14671452343463898\n",
      "Training loss for batch 3429 : 0.026362622156739235\n",
      "Training loss for batch 3430 : 0.06676027178764343\n",
      "Training loss for batch 3431 : 0.15363667905330658\n",
      "Training loss for batch 3432 : 0.21241530776023865\n",
      "Training loss for batch 3433 : 0.09474465996026993\n",
      "Training loss for batch 3434 : 0.23793543875217438\n",
      "Training loss for batch 3435 : 0.09059697389602661\n",
      "Training loss for batch 3436 : 0.06869475543498993\n",
      "Training loss for batch 3437 : 0.216366708278656\n",
      "Training loss for batch 3438 : 0.08440906554460526\n",
      "Training loss for batch 3439 : 0.19383478164672852\n",
      "Training loss for batch 3440 : 0.23040667176246643\n",
      "Training loss for batch 3441 : 0.15480130910873413\n",
      "Training loss for batch 3442 : 0.2997235059738159\n",
      "Training loss for batch 3443 : 0.13612504303455353\n",
      "Training loss for batch 3444 : 0.10263461619615555\n",
      "Training loss for batch 3445 : 0.1482531577348709\n",
      "Training loss for batch 3446 : 0.1726032793521881\n",
      "Training loss for batch 3447 : 0.21219544112682343\n",
      "Training loss for batch 3448 : 0.056275930255651474\n",
      "Training loss for batch 3449 : 0.1225375309586525\n",
      "Training loss for batch 3450 : 0.11944063007831573\n",
      "Training loss for batch 3451 : 0.04495806246995926\n",
      "Training loss for batch 3452 : 0.13774888217449188\n",
      "Training loss for batch 3453 : 0.26149263978004456\n",
      "Training loss for batch 3454 : 0.06372293829917908\n",
      "Training loss for batch 3455 : 0.1514192521572113\n",
      "Training loss for batch 3456 : 0.09405410289764404\n",
      "Training loss for batch 3457 : 0.06275005638599396\n",
      "Training loss for batch 3458 : 0.1743956059217453\n",
      "Training loss for batch 3459 : 0.018925534561276436\n",
      "Training loss for batch 3460 : 0.11882174015045166\n",
      "Training loss for batch 3461 : 0.13803398609161377\n",
      "Training loss for batch 3462 : 0.17673158645629883\n",
      "Training loss for batch 3463 : 0.07453164458274841\n",
      "Training loss for batch 3464 : 0.18194782733917236\n",
      "Training loss for batch 3465 : 0.053907912224531174\n",
      "Training loss for batch 3466 : 0.09451117366552353\n",
      "Training loss for batch 3467 : 0.015486062504351139\n",
      "Training loss for batch 3468 : 0.07908830791711807\n",
      "Training loss for batch 3469 : 0.09688863903284073\n",
      "Training loss for batch 3470 : 0.15175911784172058\n",
      "Training loss for batch 3471 : 0.18056315183639526\n",
      "Training loss for batch 3472 : 0.173240065574646\n",
      "Training loss for batch 3473 : 0.01734275370836258\n",
      "Training loss for batch 3474 : 0.19258825480937958\n",
      "Training loss for batch 3475 : 0.23273177444934845\n",
      "Training loss for batch 3476 : 0.09191158413887024\n",
      "Training loss for batch 3477 : 0.12402500957250595\n",
      "Training loss for batch 3478 : 0.2711169719696045\n",
      "Training loss for batch 3479 : 0.16422778367996216\n",
      "Training loss for batch 3480 : 0.06539199501276016\n",
      "Training loss for batch 3481 : 0.11143137514591217\n",
      "Training loss for batch 3482 : 0.0515223927795887\n",
      "Training loss for batch 3483 : 0.08118559420108795\n",
      "Training loss for batch 3484 : 0.14977844059467316\n",
      "Training loss for batch 3485 : 0.12168951332569122\n",
      "Training loss for batch 3486 : 0.024503158405423164\n",
      "Training loss for batch 3487 : 0.02397198975086212\n",
      "Training loss for batch 3488 : 0.05523101985454559\n",
      "Training loss for batch 3489 : 0.19034777581691742\n",
      "Training loss for batch 3490 : 0.12653924524784088\n",
      "Training loss for batch 3491 : 0.223153755068779\n",
      "Training loss for batch 3492 : 0.22526471316814423\n",
      "Training loss for batch 3493 : 0.06654797494411469\n",
      "Training loss for batch 3494 : 0.08731523901224136\n",
      "Training loss for batch 3495 : 0.02747279405593872\n",
      "Training loss for batch 3496 : 0.0671028196811676\n",
      "Training loss for batch 3497 : 0.059977754950523376\n",
      "Training loss for batch 3498 : 0.17634309828281403\n",
      "Training loss for batch 3499 : 0.03430088236927986\n",
      "Training loss for batch 3500 : 0.033209849148988724\n",
      "Training loss for batch 3501 : 0.0004975985502824187\n",
      "Training loss for batch 3502 : 0.1365741640329361\n",
      "Training loss for batch 3503 : 0.05789513140916824\n",
      "Training loss for batch 3504 : 0.09403427690267563\n",
      "Training loss for batch 3505 : 0.04589191451668739\n",
      "Training loss for batch 3506 : 0.0793556272983551\n",
      "Training loss for batch 3507 : 0.09356853365898132\n",
      "Training loss for batch 3508 : 0.06846684962511063\n",
      "Training loss for batch 3509 : 0.11212806403636932\n",
      "Training loss for batch 3510 : 0.036156393587589264\n",
      "Training loss for batch 3511 : 0.14059966802597046\n",
      "Training loss for batch 3512 : 0.1359289139509201\n",
      "Training loss for batch 3513 : 0.10766247659921646\n",
      "Training loss for batch 3514 : 0.020112529397010803\n",
      "Training loss for batch 3515 : 0.01202353835105896\n",
      "Training loss for batch 3516 : 0.0660594031214714\n",
      "Training loss for batch 3517 : 0.0030543713364750147\n",
      "Training loss for batch 3518 : 0.07478974014520645\n",
      "Training loss for batch 3519 : 0.12352416664361954\n",
      "Training loss for batch 3520 : 0.07238757610321045\n",
      "Training loss for batch 3521 : 0.3110901713371277\n",
      "Training loss for batch 3522 : 0.158015176653862\n",
      "Training loss for batch 3523 : 0.023240376263856888\n",
      "Training loss for batch 3524 : 0.034017860889434814\n",
      "Training loss for batch 3525 : 0.029465870931744576\n",
      "Training loss for batch 3526 : 0.15955181419849396\n",
      "Training loss for batch 3527 : 0.05513044446706772\n",
      "Training loss for batch 3528 : 0.07308211177587509\n",
      "Training loss for batch 3529 : 0.20112420618534088\n",
      "Training loss for batch 3530 : 0.2838636636734009\n",
      "Training loss for batch 3531 : 0.010593998245894909\n",
      "Training loss for batch 3532 : 0.10113013535737991\n",
      "Training loss for batch 3533 : 0.041894182562828064\n",
      "Training loss for batch 3534 : 0.1820114403963089\n",
      "Training loss for batch 3535 : 0.15010176599025726\n",
      "Training loss for batch 3536 : 0.02710556983947754\n",
      "Training loss for batch 3537 : 0.160604327917099\n",
      "Training loss for batch 3538 : 0.11688408255577087\n",
      "Training loss for batch 3539 : 0.14868119359016418\n",
      "Training loss for batch 3540 : 0.0707155093550682\n",
      "Training loss for batch 3541 : 0.08614029735326767\n",
      "Training loss for batch 3542 : 0.019587447866797447\n",
      "Training loss for batch 3543 : 0.06274077296257019\n",
      "Training loss for batch 3544 : 0.14262671768665314\n",
      "Training loss for batch 3545 : 0.02746705897152424\n",
      "Training loss for batch 3546 : 0.04323580861091614\n",
      "Training loss for batch 3547 : 0.11193505674600601\n",
      "Training loss for batch 3548 : 0.173394575715065\n",
      "Training loss for batch 3549 : 0.05502738058567047\n",
      "Training loss for batch 3550 : 0.2398391216993332\n",
      "Training loss for batch 3551 : 0.12254726141691208\n",
      "Training loss for batch 3552 : 0.12976036965847015\n",
      "Training loss for batch 3553 : 0.08750636130571365\n",
      "Training loss for batch 3554 : 0.08184684067964554\n",
      "Training loss for batch 3555 : 0.14903588593006134\n",
      "Training loss for batch 3556 : 0.0007517089834436774\n",
      "Training loss for batch 3557 : 0.1479319930076599\n",
      "Training loss for batch 3558 : 0.1262747049331665\n",
      "Training loss for batch 3559 : 0.06655696779489517\n",
      "Training loss for batch 3560 : 0.03416619449853897\n",
      "Training loss for batch 3561 : 0.14685016870498657\n",
      "Training loss for batch 3562 : 0.1552765965461731\n",
      "Training loss for batch 3563 : 0.12323028594255447\n",
      "Training loss for batch 3564 : 0.16214926540851593\n",
      "Training loss for batch 3565 : 0.05905255302786827\n",
      "Training loss for batch 3566 : 0.12807559967041016\n",
      "Training loss for batch 3567 : 0.021826818585395813\n",
      "Training loss for batch 3568 : 0.2281893640756607\n",
      "Training loss for batch 3569 : 0.0872630700469017\n",
      "Training loss for batch 3570 : 0.24432750046253204\n",
      "Training loss for batch 3571 : 0.14154396951198578\n",
      "Training loss for batch 3572 : 0.07908051460981369\n",
      "Training loss for batch 3573 : 0.16476787626743317\n",
      "Training loss for batch 3574 : 0.15622183680534363\n",
      "Training loss for batch 3575 : 0.09099867939949036\n",
      "Training loss for batch 3576 : 0.07534918189048767\n",
      "Training loss for batch 3577 : 0.23210784792900085\n",
      "Training loss for batch 3578 : 0.10547439008951187\n",
      "Training loss for batch 3579 : 0.2905810475349426\n",
      "Training loss for batch 3580 : 0.0646057277917862\n",
      "Training loss for batch 3581 : 0.152079775929451\n",
      "Training loss for batch 3582 : 0.20229949057102203\n",
      "Training loss for batch 3583 : 0.11441471427679062\n",
      "Training loss for batch 3584 : 0.0805162712931633\n",
      "Training loss for batch 3585 : 0.13182498514652252\n",
      "Training loss for batch 3586 : 0.06403323262929916\n",
      "Training loss for batch 3587 : 0.21255286037921906\n",
      "Training loss for batch 3588 : 0.18565168976783752\n",
      "Training loss for batch 3589 : 0.07406802475452423\n",
      "Training loss for batch 3590 : 0.15719722211360931\n",
      "Training loss for batch 3591 : 0.2240256518125534\n",
      "Training loss for batch 3592 : 0.05706410109996796\n",
      "Training loss for batch 3593 : 0.13244618475437164\n",
      "Training loss for batch 3594 : 0.09446131438016891\n",
      "Training loss for batch 3595 : 0.1658315658569336\n",
      "Training loss for batch 3596 : 0.21244578063488007\n",
      "Training loss for batch 3597 : 0.09289810061454773\n",
      "Training loss for batch 3598 : 0.062233224511146545\n",
      "Training loss for batch 3599 : 0.22762367129325867\n",
      "Training loss for batch 3600 : 0.05349578708410263\n",
      "Training loss for batch 3601 : 0.07595957815647125\n",
      "Training loss for batch 3602 : 0.11974328756332397\n",
      "Training loss for batch 3603 : 0.14502955973148346\n",
      "Training loss for batch 3604 : 0.15588264167308807\n",
      "Training loss for batch 3605 : 0.07117535918951035\n",
      "Training loss for batch 3606 : 0.13434569537639618\n",
      "Training loss for batch 3607 : 0.05556681007146835\n",
      "Training loss for batch 3608 : 0.08134938031435013\n",
      "Training loss for batch 3609 : 0.17007410526275635\n",
      "Training loss for batch 3610 : 0.24895714223384857\n",
      "Training loss for batch 3611 : 0.16066138446331024\n",
      "Training loss for batch 3612 : 0.1596149206161499\n",
      "Training loss for batch 3613 : 0.2101866453886032\n",
      "Training loss for batch 3614 : 0.06261865049600601\n",
      "Training loss for batch 3615 : 0.11050968617200851\n",
      "Training loss for batch 3616 : 0.041163548827171326\n",
      "Training loss for batch 3617 : 0.22270873188972473\n",
      "Training loss for batch 3618 : 0.04479401931166649\n",
      "Training loss for batch 3619 : 0.11375165730714798\n",
      "Training loss for batch 3620 : 0.11090568453073502\n",
      "Training loss for batch 3621 : 0.1333480179309845\n",
      "Training loss for batch 3622 : 0.08265240490436554\n",
      "Training loss for batch 3623 : 0.043206870555877686\n",
      "Training loss for batch 3624 : 0.1343207061290741\n",
      "Training loss for batch 3625 : 0.3409292995929718\n",
      "Training loss for batch 3626 : 0.023225372657179832\n",
      "Training loss for batch 3627 : 0.25120943784713745\n",
      "Training loss for batch 3628 : 0.027392245829105377\n",
      "Training loss for batch 3629 : 0.12898071110248566\n",
      "Training loss for batch 3630 : 0.012719397433102131\n",
      "Training loss for batch 3631 : 0.20760372281074524\n",
      "Training loss for batch 3632 : 0.06826349347829819\n",
      "Training loss for batch 3633 : 0.21575981378555298\n",
      "Training loss for batch 3634 : 0.026083866134285927\n",
      "Training loss for batch 3635 : 0.05820285528898239\n",
      "Training loss for batch 3636 : 0.1443476527929306\n",
      "Training loss for batch 3637 : 0.05361363664269447\n",
      "Training loss for batch 3638 : 0.04378001391887665\n",
      "Training loss for batch 3639 : 0.06384090334177017\n",
      "Training loss for batch 3640 : 0.030546395108103752\n",
      "Training loss for batch 3641 : 0.1339501589536667\n",
      "Training loss for batch 3642 : 0.1535848081111908\n",
      "Training loss for batch 3643 : 0.04230854660272598\n",
      "Training loss for batch 3644 : 0.10941097140312195\n",
      "Training loss for batch 3645 : 0.09318779408931732\n",
      "Training loss for batch 3646 : 0.28565090894699097\n",
      "Training loss for batch 3647 : 0.1498267650604248\n",
      "Training loss for batch 3648 : 0.049395233392715454\n",
      "Training loss for batch 3649 : 0.030269265174865723\n",
      "Training loss for batch 3650 : 0.09882256388664246\n",
      "Training loss for batch 3651 : 0.12927742302417755\n",
      "Training loss for batch 3652 : 0.09830403327941895\n",
      "Training loss for batch 3653 : 0.1409442126750946\n",
      "Training loss for batch 3654 : 0.26569250226020813\n",
      "Training loss for batch 3655 : 0.08735465258359909\n",
      "Training loss for batch 3656 : 0.09303353726863861\n",
      "Training loss for batch 3657 : 0.17692135274410248\n",
      "Training loss for batch 3658 : 0.2601511776447296\n",
      "Training loss for batch 3659 : 0.06553766131401062\n",
      "Training loss for batch 3660 : 0.1431037038564682\n",
      "Training loss for batch 3661 : 0.08852943778038025\n",
      "Training loss for batch 3662 : 0.058413103222846985\n",
      "Training loss for batch 3663 : 0.12095913290977478\n",
      "Training loss for batch 3664 : 0.1428883820772171\n",
      "Training loss for batch 3665 : 0.20799627900123596\n",
      "Training loss for batch 3666 : 0.1718265265226364\n",
      "Training loss for batch 3667 : 0.03524993732571602\n",
      "Training loss for batch 3668 : 0.2466544806957245\n",
      "Training loss for batch 3669 : 0.026468496769666672\n",
      "Training loss for batch 3670 : 0.16032089293003082\n",
      "Training loss for batch 3671 : 0.055008187890052795\n",
      "Training loss for batch 3672 : 0.09126437455415726\n",
      "Training loss for batch 3673 : 0.05896769091486931\n",
      "Training loss for batch 3674 : 0.24732254445552826\n",
      "Training loss for batch 3675 : 0.06485097110271454\n",
      "Training loss for batch 3676 : 0.1543876677751541\n",
      "Training loss for batch 3677 : 0.16938252747058868\n",
      "Training loss for batch 3678 : 0.044063497334718704\n",
      "Training loss for batch 3679 : 0.09485489130020142\n",
      "Training loss for batch 3680 : 0.041078489273786545\n",
      "Training loss for batch 3681 : 0.06324229389429092\n",
      "Training loss for batch 3682 : 0.09877090901136398\n",
      "Training loss for batch 3683 : 0.15741579234600067\n",
      "Training loss for batch 3684 : 0.21288512647151947\n",
      "Training loss for batch 3685 : 0.1943000853061676\n",
      "Training loss for batch 3686 : 0.0373104102909565\n",
      "Training loss for batch 3687 : 0.03589462861418724\n",
      "Training loss for batch 3688 : 0.012898516841232777\n",
      "Training loss for batch 3689 : 0.02330845408141613\n",
      "Training loss for batch 3690 : 0.11178477108478546\n",
      "Training loss for batch 3691 : 0.07785253971815109\n",
      "Training loss for batch 3692 : 0.18899132311344147\n",
      "Training loss for batch 3693 : 0.14848226308822632\n",
      "Training loss for batch 3694 : 0.19842292368412018\n",
      "Training loss for batch 3695 : 0.10820063203573227\n",
      "Training loss for batch 3696 : 0.18162496387958527\n",
      "Training loss for batch 3697 : 0.0014357541222125292\n",
      "Training loss for batch 3698 : 0.2349785566329956\n",
      "Training loss for batch 3699 : 0.12052728235721588\n",
      "Training loss for batch 3700 : 0.06374552100896835\n",
      "Training loss for batch 3701 : 1.3759233752352884e-08\n",
      "Training loss for batch 3702 : 0.060334715992212296\n",
      "Training loss for batch 3703 : 0.06640396267175674\n",
      "Training loss for batch 3704 : 0.01724660024046898\n",
      "Training loss for batch 3705 : 0.07299952954053879\n",
      "Training loss for batch 3706 : 0.14999070763587952\n",
      "Training loss for batch 3707 : 0.14589254558086395\n",
      "Training loss for batch 3708 : 0.19218190014362335\n",
      "Training loss for batch 3709 : 0.0416763499379158\n",
      "Training loss for batch 3710 : 0.12769334018230438\n",
      "Training loss for batch 3711 : 0.06291235983371735\n",
      "Training loss for batch 3712 : 0.07244394719600677\n",
      "Training loss for batch 3713 : 0.008097331039607525\n",
      "Training loss for batch 3714 : 0.05462842434644699\n",
      "Training loss for batch 3715 : 0.21807749569416046\n",
      "Training loss for batch 3716 : 0.08418344706296921\n",
      "Training loss for batch 3717 : 0.15127147734165192\n",
      "Training loss for batch 3718 : 0.21895791590213776\n",
      "Training loss for batch 3719 : 0.12492291629314423\n",
      "Training loss for batch 3720 : 0.03041725605726242\n",
      "Training loss for batch 3721 : 0.028650304302573204\n",
      "Training loss for batch 3722 : 0.15991012752056122\n",
      "Training loss for batch 3723 : 0.23467206954956055\n",
      "Training loss for batch 3724 : 0.1447727233171463\n",
      "Training loss for batch 3725 : 0.1350705474615097\n",
      "Training loss for batch 3726 : 0.14390771090984344\n",
      "Training loss for batch 3727 : 0.09372534602880478\n",
      "Training loss for batch 3728 : 0.12349140644073486\n",
      "Training loss for batch 3729 : 0.03075791522860527\n",
      "Training loss for batch 3730 : 0.3792414665222168\n",
      "Training loss for batch 3731 : 0.12973538041114807\n",
      "Training loss for batch 3732 : 0.039035238325595856\n",
      "Training loss for batch 3733 : 0.05631883442401886\n",
      "Training loss for batch 3734 : 0.14508403837680817\n",
      "Training loss for batch 3735 : 0.07106928527355194\n",
      "Training loss for batch 3736 : 0.007405757904052734\n",
      "Training loss for batch 3737 : 0.12428562343120575\n",
      "Training loss for batch 3738 : 0.2641107738018036\n",
      "Training loss for batch 3739 : 0.03962910547852516\n",
      "Training loss for batch 3740 : 0.05797440558671951\n",
      "Training loss for batch 3741 : 0.130707249045372\n",
      "Training loss for batch 3742 : 0.04203328490257263\n",
      "Training loss for batch 3743 : 0.06517158448696136\n",
      "Training loss for batch 3744 : 0.08444874733686447\n",
      "Training loss for batch 3745 : 0.04639109969139099\n",
      "Training loss for batch 3746 : 0.2944691479206085\n",
      "Training loss for batch 3747 : 0.03608693554997444\n",
      "Training loss for batch 3748 : 0.1671293079853058\n",
      "Training loss for batch 3749 : 0.14418856799602509\n",
      "Training loss for batch 3750 : 0.18376344442367554\n",
      "Training loss for batch 3751 : 0.2769370377063751\n",
      "Training loss for batch 3752 : 0.1425618827342987\n",
      "Training loss for batch 3753 : 0.018590224906802177\n",
      "Training loss for batch 3754 : 0.03945617005228996\n",
      "Training loss for batch 3755 : 0.04463380575180054\n",
      "Training loss for batch 3756 : 0.07719790190458298\n",
      "Training loss for batch 3757 : 0.09192235767841339\n",
      "Training loss for batch 3758 : 0.2695904076099396\n",
      "Training loss for batch 3759 : 0.10699568688869476\n",
      "Training loss for batch 3760 : 0.033605124801397324\n",
      "Training loss for batch 3761 : 0.22627200186252594\n",
      "Training loss for batch 3762 : 0.26667356491088867\n",
      "Training loss for batch 3763 : 0.19584624469280243\n",
      "Training loss for batch 3764 : 0.09052114933729172\n",
      "Training loss for batch 3765 : 0.2840212881565094\n",
      "Training loss for batch 3766 : 0.015499977394938469\n",
      "Training loss for batch 3767 : 0.08011814206838608\n",
      "Training loss for batch 3768 : 0.06535319983959198\n",
      "Training loss for batch 3769 : 0.21039044857025146\n",
      "Training loss for batch 3770 : 0.26522478461265564\n",
      "Training loss for batch 3771 : 0.13401876389980316\n",
      "Training loss for batch 3772 : 0.02415962517261505\n",
      "Training loss for batch 3773 : 0.21096882224082947\n",
      "Training loss for batch 3774 : 0.0672830194234848\n",
      "Training loss for batch 3775 : 0.07703142613172531\n",
      "Training loss for batch 3776 : 0.052411917597055435\n",
      "Training loss for batch 3777 : 0.15935289859771729\n",
      "Training loss for batch 3778 : 0.048281434923410416\n",
      "Training loss for batch 3779 : 0.24852842092514038\n",
      "Training loss for batch 3780 : 0.007686922326683998\n",
      "Training loss for batch 3781 : 0.12035227566957474\n",
      "Training loss for batch 3782 : 0.04434959962964058\n",
      "Training loss for batch 3783 : 0.11779405921697617\n",
      "Training loss for batch 3784 : 0.10866187512874603\n",
      "Training loss for batch 3785 : 0.0031863884069025517\n",
      "Training loss for batch 3786 : 0.07852251827716827\n",
      "Training loss for batch 3787 : 0.05827323719859123\n",
      "Training loss for batch 3788 : 0.2549249231815338\n",
      "Training loss for batch 3789 : 0.14790406823158264\n",
      "Training loss for batch 3790 : 0.15767322480678558\n",
      "Training loss for batch 3791 : 0.19461990892887115\n",
      "Training loss for batch 3792 : 0.119588702917099\n",
      "Training loss for batch 3793 : 0.07635458558797836\n",
      "Training loss for batch 3794 : 0.10581847280263901\n",
      "Training loss for batch 3795 : 0.09147676080465317\n",
      "Training loss for batch 3796 : 0.0666227713227272\n",
      "Training loss for batch 3797 : 0.08428753912448883\n",
      "Training loss for batch 3798 : 0.22289001941680908\n",
      "Training loss for batch 3799 : 0.04676052927970886\n",
      "Training loss for batch 3800 : 0.1908210813999176\n",
      "Training loss for batch 3801 : 0.07620987296104431\n",
      "Training loss for batch 3802 : 0.08018343895673752\n",
      "Training loss for batch 3803 : 0.043764833360910416\n",
      "Training loss for batch 3804 : 0.15613074600696564\n",
      "Training loss for batch 3805 : 0.09333381801843643\n",
      "Training loss for batch 3806 : 0.14847035706043243\n",
      "Training loss for batch 3807 : 0.18540769815444946\n",
      "Training loss for batch 3808 : 0.04890097305178642\n",
      "Training loss for batch 3809 : 0.26558756828308105\n",
      "Training loss for batch 3810 : 0.23427966237068176\n",
      "Training loss for batch 3811 : 0.2388284057378769\n",
      "Training loss for batch 3812 : 0.12322704493999481\n",
      "Training loss for batch 3813 : 0.07816135138273239\n",
      "Training loss for batch 3814 : 0.05427553877234459\n",
      "Training loss for batch 3815 : 0.24946585297584534\n",
      "Training loss for batch 3816 : 0.10945381224155426\n",
      "Training loss for batch 3817 : 0.07406695932149887\n",
      "Training loss for batch 3818 : 0.21204890310764313\n",
      "Training loss for batch 3819 : 0.007144338916987181\n",
      "Training loss for batch 3820 : 0.22449727356433868\n",
      "Training loss for batch 3821 : 0.09698701649904251\n",
      "Training loss for batch 3822 : 0.2219470590353012\n",
      "Training loss for batch 3823 : 0.0181707926094532\n",
      "Training loss for batch 3824 : 0.22746726870536804\n",
      "Training loss for batch 3825 : 0.10963539779186249\n",
      "Training loss for batch 3826 : 0.13070352375507355\n",
      "Training loss for batch 3827 : 0.03422711044549942\n",
      "Training loss for batch 3828 : 0.11887245625257492\n",
      "Training loss for batch 3829 : 0.08771011978387833\n",
      "Training loss for batch 3830 : 0.24268022179603577\n",
      "Training loss for batch 3831 : 0.07502510398626328\n",
      "Training loss for batch 3832 : 0.2522399425506592\n",
      "Training loss for batch 3833 : 0.13149747252464294\n",
      "Training loss for batch 3834 : 0.0691983625292778\n",
      "Training loss for batch 3835 : 0.049457013607025146\n",
      "Training loss for batch 3836 : 0.1380150467157364\n",
      "Training loss for batch 3837 : 0.05268120765686035\n",
      "Training loss for batch 3838 : 0.19787763059139252\n",
      "Training loss for batch 3839 : 0.18961967527866364\n",
      "Training loss for batch 3840 : 0.013174961321055889\n",
      "Training loss for batch 3841 : 0.2408430129289627\n",
      "Training loss for batch 3842 : 0.14552552998065948\n",
      "Training loss for batch 3843 : 0.23716770112514496\n",
      "Training loss for batch 3844 : 0.23723380267620087\n",
      "Training loss for batch 3845 : 0.0098457345739007\n",
      "Training loss for batch 3846 : 0.17120686173439026\n",
      "Training loss for batch 3847 : 0.09067032486200333\n",
      "Training loss for batch 3848 : 0.3930041193962097\n",
      "Training loss for batch 3849 : 0.10842510312795639\n",
      "Training loss for batch 3850 : 0.060860078781843185\n",
      "Training loss for batch 3851 : 0.19625972211360931\n",
      "Training loss for batch 3852 : 0.11494770646095276\n",
      "Training loss for batch 3853 : 0.06388223171234131\n",
      "Training loss for batch 3854 : 0.04263301566243172\n",
      "Training loss for batch 3855 : 0.134273961186409\n",
      "Training loss for batch 3856 : 0.22789418697357178\n",
      "Training loss for batch 3857 : 0.0077139767818152905\n",
      "Training loss for batch 3858 : 0.07822305709123611\n",
      "Training loss for batch 3859 : 0.036698345094919205\n",
      "Training loss for batch 3860 : 0.09451397508382797\n",
      "Training loss for batch 3861 : 0.14694727957248688\n",
      "Training loss for batch 3862 : 0.11217549443244934\n",
      "Training loss for batch 3863 : 0.06960409879684448\n",
      "Training loss for batch 3864 : 0.015413906425237656\n",
      "Training loss for batch 3865 : 0.1277022659778595\n",
      "Training loss for batch 3866 : 0.12217650562524796\n",
      "Training loss for batch 3867 : 0.22938556969165802\n",
      "Training loss for batch 3868 : 0.14812316000461578\n",
      "Training loss for batch 3869 : 0.24426597356796265\n",
      "Training loss for batch 3870 : 0.06057446077466011\n",
      "Training loss for batch 3871 : 0.0810670554637909\n",
      "Training loss for batch 3872 : 0.07000483572483063\n",
      "Training loss for batch 3873 : 0.17071533203125\n",
      "Training loss for batch 3874 : 0.07007674127817154\n",
      "Training loss for batch 3875 : 0.1448080986738205\n",
      "Training loss for batch 3876 : 0.2858988046646118\n",
      "Training loss for batch 3877 : 0.16134333610534668\n",
      "Training loss for batch 3878 : 0.10305777937173843\n",
      "Training loss for batch 3879 : 0.1362294703722\n",
      "Training loss for batch 3880 : 0.11775828897953033\n",
      "Training loss for batch 3881 : 0.07589075714349747\n",
      "Training loss for batch 3882 : 0.04722016304731369\n",
      "Training loss for batch 3883 : 0.20313714444637299\n",
      "Training loss for batch 3884 : 0.04729175567626953\n",
      "Training loss for batch 3885 : 0.025610780343413353\n",
      "Training loss for batch 3886 : 0.10365240275859833\n",
      "Training loss for batch 3887 : 0.06080009043216705\n",
      "Training loss for batch 3888 : 0.1298377960920334\n",
      "Training loss for batch 3889 : 0.07627604156732559\n",
      "Training loss for batch 3890 : 0.003993764519691467\n",
      "Training loss for batch 3891 : 0.07583711296319962\n",
      "Training loss for batch 3892 : 0.20914249122142792\n",
      "Training loss for batch 3893 : 0.14302444458007812\n",
      "Training loss for batch 3894 : 0.2401551753282547\n",
      "Training loss for batch 3895 : 0.25394344329833984\n",
      "Training loss for batch 3896 : 0.11803290992975235\n",
      "Training loss for batch 3897 : 0.10410921275615692\n",
      "Training loss for batch 3898 : 0.09612914174795151\n",
      "Training loss for batch 3899 : 0.1222878098487854\n",
      "Training loss for batch 3900 : 0.11125227808952332\n",
      "Training loss for batch 3901 : 0.09130560606718063\n",
      "Training loss for batch 3902 : 0.12956862151622772\n",
      "Training loss for batch 3903 : 0.12886272370815277\n",
      "Training loss for batch 3904 : 0.22142212092876434\n",
      "Training loss for batch 3905 : 0.15268594026565552\n",
      "Training loss for batch 3906 : 0.07497205585241318\n",
      "Training loss for batch 3907 : 0.15143144130706787\n",
      "Training loss for batch 3908 : 0.17529265582561493\n",
      "Training loss for batch 3909 : 0.28910329937934875\n",
      "Training loss for batch 3910 : 0.08309793472290039\n",
      "Training loss for batch 3911 : 0.05148259922862053\n",
      "Training loss for batch 3912 : 0.07721725106239319\n",
      "Training loss for batch 3913 : 0.14969700574874878\n",
      "Training loss for batch 3914 : 0.11763627827167511\n",
      "Training loss for batch 3915 : 0.21555195748806\n",
      "Training loss for batch 3916 : 0.22126404941082\n",
      "Training loss for batch 3917 : 0.10130051523447037\n",
      "Training loss for batch 3918 : 0.17724889516830444\n",
      "Training loss for batch 3919 : 0.2600477337837219\n",
      "Training loss for batch 3920 : 0.1466793715953827\n",
      "Training loss for batch 3921 : 0.11423902958631516\n",
      "Training loss for batch 3922 : 0.06463532894849777\n",
      "Training loss for batch 3923 : 0.24882447719573975\n",
      "Training loss for batch 3924 : 0.1393045336008072\n",
      "Training loss for batch 3925 : 0.0628073513507843\n",
      "Training loss for batch 3926 : 0.21312035620212555\n",
      "Training loss for batch 3927 : 0.11743086576461792\n",
      "Training loss for batch 3928 : 0.021765856072306633\n",
      "Training loss for batch 3929 : 0.12137844413518906\n",
      "Training loss for batch 3930 : 0.16567060351371765\n",
      "Training loss for batch 3931 : 0.023481173440814018\n",
      "Training loss for batch 3932 : 0.06372057646512985\n",
      "Training loss for batch 3933 : 0.12291060388088226\n",
      "Training loss for batch 3934 : 0.1582832634449005\n",
      "Training loss for batch 3935 : 0.48089560866355896\n",
      "Training loss for batch 3936 : 0.1378576159477234\n",
      "Training loss for batch 3937 : 0.015189754776656628\n",
      "Training loss for batch 3938 : 0.25029876828193665\n",
      "Training loss for batch 3939 : 0.10154786705970764\n",
      "Training loss for batch 3940 : 0.13681548833847046\n",
      "Training loss for batch 3941 : 0.10808572173118591\n",
      "Training loss for batch 3942 : 0.15405750274658203\n",
      "Training loss for batch 3943 : 0.07686759531497955\n",
      "Training loss for batch 3944 : 0.10862632840871811\n",
      "Training loss for batch 3945 : 0.052980080246925354\n",
      "Training loss for batch 3946 : 0.1416654884815216\n",
      "Training loss for batch 3947 : 0.12367662787437439\n",
      "Training loss for batch 3948 : 0.12043802440166473\n",
      "Training loss for batch 3949 : 0.22615712881088257\n",
      "Training loss for batch 3950 : 0.14812619984149933\n",
      "Training loss for batch 3951 : 0.1283378154039383\n",
      "Training loss for batch 3952 : 0.12505684792995453\n",
      "Training loss for batch 3953 : 0.12642373144626617\n",
      "Training loss for batch 3954 : 0.041675712913274765\n",
      "Training loss for batch 3955 : 0.07614067196846008\n",
      "Training loss for batch 3956 : 0.2910637855529785\n",
      "Training loss for batch 3957 : 0.17311738431453705\n",
      "Training loss for batch 3958 : 0.040198393166065216\n",
      "Training loss for batch 3959 : 0.06324878334999084\n",
      "Training loss for batch 3960 : 0.14376536011695862\n",
      "Training loss for batch 3961 : 0.1500057727098465\n",
      "Training loss for batch 3962 : 0.10858312249183655\n",
      "Training loss for batch 3963 : 0.03773932158946991\n",
      "Training loss for batch 3964 : 0.15641190111637115\n",
      "Training loss for batch 3965 : 0.0796012431383133\n",
      "Training loss for batch 3966 : 0.003412931924685836\n",
      "Training loss for batch 3967 : 0.02591842971742153\n",
      "Training loss for batch 3968 : 0.27494749426841736\n",
      "Training loss for batch 3969 : 0.07327736914157867\n",
      "Training loss for batch 3970 : 0.11111589521169662\n",
      "Training loss for batch 3971 : 0.0317232571542263\n",
      "Training loss for batch 3972 : 0.030031908303499222\n",
      "Training loss for batch 3973 : 0.09444529563188553\n",
      "Training loss for batch 3974 : 0.19026604294776917\n",
      "Training loss for batch 3975 : 0.05191165953874588\n",
      "Training loss for batch 3976 : 0.052358102053403854\n",
      "Training loss for batch 3977 : 0.018768440932035446\n",
      "Training loss for batch 3978 : 0.14641965925693512\n",
      "Training loss for batch 3979 : 0.1472170352935791\n",
      "Training loss for batch 3980 : 0.04621931537985802\n",
      "Training loss for batch 3981 : 0.1924978494644165\n",
      "Training loss for batch 3982 : 0.07652153819799423\n",
      "Training loss for batch 3983 : 0.05958603695034981\n",
      "Training loss for batch 3984 : 0.11026742309331894\n",
      "Training loss for batch 3985 : 0.09154000878334045\n",
      "Training loss for batch 3986 : 0.22184710204601288\n",
      "Training loss for batch 3987 : 0.03425266593694687\n",
      "Training loss for batch 3988 : 0.07529594004154205\n",
      "Training loss for batch 3989 : 0.18489079177379608\n",
      "Training loss for batch 3990 : 0.09777448326349258\n",
      "Training loss for batch 3991 : 0.15561358630657196\n",
      "Training loss for batch 3992 : 0.15742290019989014\n",
      "Training loss for batch 3993 : 0.24867376685142517\n",
      "Training loss for batch 3994 : 0.06582637876272202\n",
      "Training loss for batch 3995 : 0.0025923948269337416\n",
      "Training loss for batch 3996 : 0.17537233233451843\n",
      "Training loss for batch 3997 : 0.030591771006584167\n",
      "Training loss for batch 3998 : 0.42680707573890686\n",
      "Training loss for batch 3999 : 0.21558897197246552\n",
      "Training loss for batch 4000 : 0.05523640289902687\n",
      "Training loss for batch 4001 : 0.02465949021279812\n",
      "Training loss for batch 4002 : 0.12742824852466583\n",
      "Training loss for batch 4003 : 0.3005354106426239\n",
      "Training loss for batch 4004 : 0.12428129464387894\n",
      "Training loss for batch 4005 : 0.07381720840930939\n",
      "Training loss for batch 4006 : 0.13395895063877106\n",
      "Training loss for batch 4007 : 0.029642965644598007\n",
      "Training loss for batch 4008 : 0.13832351565361023\n",
      "Training loss for batch 4009 : 0.04338604584336281\n",
      "Training loss for batch 4010 : 0.05674389377236366\n",
      "Training loss for batch 4011 : 0.10112208873033524\n",
      "Training loss for batch 4012 : 0.13772471249103546\n",
      "Training loss for batch 4013 : 0.1011093258857727\n",
      "Training loss for batch 4014 : 0.1513126790523529\n",
      "Training loss for batch 4015 : 0.03650190308690071\n",
      "Training loss for batch 4016 : 0.03260599821805954\n",
      "Training loss for batch 4017 : 0.19878819584846497\n",
      "Training loss for batch 4018 : 0.16815894842147827\n",
      "Training loss for batch 4019 : 0.13016201555728912\n",
      "Training loss for batch 4020 : 0.0717405453324318\n",
      "Training loss for batch 4021 : 0.13492903113365173\n",
      "Training loss for batch 4022 : 0.21928438544273376\n",
      "Training loss for batch 4023 : 0.03855961933732033\n",
      "Training loss for batch 4024 : 0.04896276071667671\n",
      "Training loss for batch 4025 : 0.14325083792209625\n",
      "Training loss for batch 4026 : 0.17062129080295563\n",
      "Training loss for batch 4027 : 0.19032028317451477\n",
      "Training loss for batch 4028 : 0.10141142457723618\n",
      "Training loss for batch 4029 : 0.08480492979288101\n",
      "Training loss for batch 4030 : 0.25729262828826904\n",
      "Training loss for batch 4031 : 0.1968143880367279\n",
      "Training loss for batch 4032 : 0.06988514214754105\n",
      "Training loss for batch 4033 : 0.01661905087530613\n",
      "Training loss for batch 4034 : 0.31996864080429077\n",
      "Training loss for batch 4035 : 0.009623972699046135\n",
      "Training loss for batch 4036 : 0.15281829237937927\n",
      "Training loss for batch 4037 : 0.5356937646865845\n",
      "Training loss for batch 4038 : 0.12812282145023346\n",
      "Training loss for batch 4039 : 0.02859371341764927\n",
      "Training loss for batch 4040 : 0.052838511765003204\n",
      "Training loss for batch 4041 : 0.10655591636896133\n",
      "Training loss for batch 4042 : 0.04542413353919983\n",
      "Training loss for batch 4043 : 0.18561576306819916\n",
      "Training loss for batch 4044 : 0.10014467686414719\n",
      "Training loss for batch 4045 : 0.06379203498363495\n",
      "Training loss for batch 4046 : 0.03339130058884621\n",
      "Training loss for batch 4047 : 0.13078726828098297\n",
      "Training loss for batch 4048 : 0.26856300234794617\n",
      "Training loss for batch 4049 : 0.20937031507492065\n",
      "Training loss for batch 4050 : 0.09501144289970398\n",
      "Training loss for batch 4051 : 0.07469978928565979\n",
      "Training loss for batch 4052 : 0.17666837573051453\n",
      "Training loss for batch 4053 : 0.04685244336724281\n",
      "Training loss for batch 4054 : 0.03880559653043747\n",
      "Training loss for batch 4055 : 0.16915066540241241\n",
      "Training loss for batch 4056 : 0.2169574350118637\n",
      "Training loss for batch 4057 : 0.07150441408157349\n",
      "Training loss for batch 4058 : 0.031226085498929024\n",
      "Training loss for batch 4059 : 0.21262593567371368\n",
      "Training loss for batch 4060 : 0.17964255809783936\n",
      "Training loss for batch 4061 : 0.08875150978565216\n",
      "Training loss for batch 4062 : 0.16017036139965057\n",
      "Training loss for batch 4063 : 0.2144957035779953\n",
      "Training loss for batch 4064 : 0.23086149990558624\n",
      "Training loss for batch 4065 : 0.13165737688541412\n",
      "Training loss for batch 4066 : 0.19616827368736267\n",
      "Training loss for batch 4067 : 0.014539284631609917\n",
      "Training loss for batch 4068 : 0.3919437825679779\n",
      "Training loss for batch 4069 : 0.16984856128692627\n",
      "Training loss for batch 4070 : 0.04550805315375328\n",
      "Training loss for batch 4071 : 0.09860850870609283\n",
      "Training loss for batch 4072 : 0.07000568509101868\n",
      "Training loss for batch 4073 : 0.1438012719154358\n",
      "Training loss for batch 4074 : 0.12959034740924835\n",
      "Training loss for batch 4075 : 0.19142800569534302\n",
      "Training loss for batch 4076 : 0.11822634190320969\n",
      "Training loss for batch 4077 : 0.016569726169109344\n",
      "Training loss for batch 4078 : 0.31883370876312256\n",
      "Training loss for batch 4079 : 0.09503579139709473\n",
      "Training loss for batch 4080 : 0.007808982394635677\n",
      "Training loss for batch 4081 : 0.11204615980386734\n",
      "Training loss for batch 4082 : 0.1062338650226593\n",
      "Training loss for batch 4083 : 0.15451908111572266\n",
      "Training loss for batch 4084 : 0.05128531903028488\n",
      "Training loss for batch 4085 : 0.15093006193637848\n",
      "Training loss for batch 4086 : 0.022730018943548203\n",
      "Training loss for batch 4087 : 0.23411662876605988\n",
      "Training loss for batch 4088 : 0.11269023269414902\n",
      "Training loss for batch 4089 : 0.11031569540500641\n",
      "Training loss for batch 4090 : 0.08636162430047989\n",
      "Training loss for batch 4091 : 0.2622411847114563\n",
      "Training loss for batch 4092 : 0.10186191648244858\n",
      "Training loss for batch 4093 : 0.1629398912191391\n",
      "Training loss for batch 4094 : 0.2773136496543884\n",
      "Training loss for batch 4095 : 0.14529187977313995\n",
      "Training loss for batch 4096 : 0.14218859374523163\n",
      "Training loss for batch 4097 : 0.07694253325462341\n",
      "Training loss for batch 4098 : 0.14901039004325867\n",
      "Training loss for batch 4099 : 0.07506924122571945\n",
      "Training loss for batch 4100 : 0.07707895338535309\n",
      "Training loss for batch 4101 : 0.1330343335866928\n",
      "Training loss for batch 4102 : 0.03846907988190651\n",
      "Training loss for batch 4103 : 0.19822797179222107\n",
      "Training loss for batch 4104 : 0.05812191218137741\n",
      "Training loss for batch 4105 : 0.13239344954490662\n",
      "Training loss for batch 4106 : 0.11773507297039032\n",
      "Training loss for batch 4107 : 0.141636461019516\n",
      "Training loss for batch 4108 : 0.10481122881174088\n",
      "Training loss for batch 4109 : 0.05295084789395332\n",
      "Training loss for batch 4110 : 0.24022524058818817\n",
      "Training loss for batch 4111 : 0.18667635321617126\n",
      "Training loss for batch 4112 : 0.10487234592437744\n",
      "Training loss for batch 4113 : 0.17894873023033142\n",
      "Training loss for batch 4114 : 0.09846220910549164\n",
      "Training loss for batch 4115 : 0.036457788199186325\n",
      "Training loss for batch 4116 : 0.07387211918830872\n",
      "Training loss for batch 4117 : 0.044520244002342224\n",
      "Training loss for batch 4118 : 0.07995128631591797\n",
      "Training loss for batch 4119 : 0.20340633392333984\n",
      "Training loss for batch 4120 : 0.07468488067388535\n",
      "Training loss for batch 4121 : 0.11088082194328308\n",
      "Training loss for batch 4122 : 0.07463273406028748\n",
      "Training loss for batch 4123 : 0.0454997718334198\n",
      "Training loss for batch 4124 : 0.13583998382091522\n",
      "Training loss for batch 4125 : 0.12211661785840988\n",
      "Training loss for batch 4126 : 0.1623099446296692\n",
      "Training loss for batch 4127 : 0.22954082489013672\n",
      "Training loss for batch 4128 : 0.032053008675575256\n",
      "Training loss for batch 4129 : 1.2518385972271062e-07\n",
      "Training loss for batch 4130 : 0.13903562724590302\n",
      "Training loss for batch 4131 : 0.07744979113340378\n",
      "Training loss for batch 4132 : 0.003366912482306361\n",
      "Training loss for batch 4133 : 0.23249325156211853\n",
      "Training loss for batch 4134 : 0.07291963696479797\n",
      "Training loss for batch 4135 : 0.13412155210971832\n",
      "Training loss for batch 4136 : 0.036665819585323334\n",
      "Training loss for batch 4137 : 0.09388743340969086\n",
      "Training loss for batch 4138 : 0.1436682641506195\n",
      "Training loss for batch 4139 : 0.1681181937456131\n",
      "Training loss for batch 4140 : 0.16102169454097748\n",
      "Training loss for batch 4141 : 0.026638131588697433\n",
      "Training loss for batch 4142 : 0.06653149425983429\n",
      "Training loss for batch 4143 : 0.27084147930145264\n",
      "Training loss for batch 4144 : 0.030177054926753044\n",
      "Training loss for batch 4145 : 0.06060424819588661\n",
      "Training loss for batch 4146 : 0.014572554267942905\n",
      "Training loss for batch 4147 : 0.20028796792030334\n",
      "Training loss for batch 4148 : 0.06330797076225281\n",
      "Training loss for batch 4149 : 0.07636838406324387\n",
      "Training loss for batch 4150 : 0.14565174281597137\n",
      "Training loss for batch 4151 : 0.16398489475250244\n",
      "Training loss for batch 4152 : 0.42109420895576477\n",
      "Training loss for batch 4153 : 0.1718865931034088\n",
      "Parameter containing:\n",
      "tensor(-0.2614, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.0000100135803223\n",
      "Training loss for batch 1 : 1.0000098943710327\n",
      "Training loss for batch 2 : 1.0000100135803223\n",
      "Training loss for batch 3 : 1.0000100135803223\n",
      "Training loss for batch 4 : 1.0000100135803223\n",
      "Training loss for batch 5 : 1.0000098943710327\n",
      "Training loss for batch 6 : 1.0000100135803223\n",
      "Training loss for batch 7 : 1.0000100135803223\n",
      "Training loss for batch 8 : 1.0000100135803223\n",
      "Training loss for batch 9 : 1.0000100135803223\n",
      "Training loss for batch 10 : 1.0000098943710327\n",
      "Training loss for batch 11 : 1.0000100135803223\n",
      "Training loss for batch 12 : 0.01509171724319458\n",
      "Training loss for batch 13 : 0.048764217644929886\n",
      "Training loss for batch 14 : 0.05695648118853569\n",
      "Training loss for batch 15 : 0.0816054493188858\n",
      "Training loss for batch 16 : 0.06543174386024475\n",
      "Training loss for batch 17 : 0.056765783578157425\n",
      "Training loss for batch 18 : 0.003602578304708004\n",
      "Training loss for batch 19 : 0.05411246046423912\n",
      "Training loss for batch 20 : 0.08438556641340256\n",
      "Training loss for batch 21 : 0.003132769837975502\n",
      "Training loss for batch 22 : 0.035582076758146286\n",
      "Training loss for batch 23 : 0.15200071036815643\n",
      "Training loss for batch 24 : 0.05213006213307381\n",
      "Training loss for batch 25 : 0.02286411076784134\n",
      "Training loss for batch 26 : 0.11487729847431183\n",
      "Training loss for batch 27 : 0.1795956790447235\n",
      "Training loss for batch 28 : 0.15530574321746826\n",
      "Training loss for batch 29 : 0.19381876289844513\n",
      "Training loss for batch 30 : 0.0893351286649704\n",
      "Training loss for batch 31 : 0.04183846712112427\n",
      "Training loss for batch 32 : 0.15723010897636414\n",
      "Training loss for batch 33 : 0.36746546626091003\n",
      "Training loss for batch 34 : 0.091716468334198\n",
      "Training loss for batch 35 : 0.08754775673151016\n",
      "Training loss for batch 36 : 0.20700715482234955\n",
      "Training loss for batch 37 : 0.10695479065179825\n",
      "Training loss for batch 38 : 0.07682706415653229\n",
      "Training loss for batch 39 : 0.07816608995199203\n",
      "Training loss for batch 40 : 0.013407750986516476\n",
      "Training loss for batch 41 : 0.1913764327764511\n",
      "Training loss for batch 42 : 0.08125423640012741\n",
      "Training loss for batch 43 : 0.04633978009223938\n",
      "Training loss for batch 44 : 0.09018642455339432\n",
      "Training loss for batch 45 : 0.01666146144270897\n",
      "Training loss for batch 46 : 0.07442976534366608\n",
      "Training loss for batch 47 : 0.12071096897125244\n",
      "Training loss for batch 48 : 0.16934822499752045\n",
      "Training loss for batch 49 : 0.021214352920651436\n",
      "Training loss for batch 50 : 0.04961973428726196\n",
      "Training loss for batch 51 : 0.03588002920150757\n",
      "Training loss for batch 52 : 0.04309873282909393\n",
      "Training loss for batch 53 : 0.08245094120502472\n",
      "Training loss for batch 54 : 0.039057645946741104\n",
      "Training loss for batch 55 : 0.029886163771152496\n",
      "Training loss for batch 56 : 0.05667465925216675\n",
      "Training loss for batch 57 : 0.07475931197404861\n",
      "Training loss for batch 58 : 0.10218990594148636\n",
      "Training loss for batch 59 : 0.059178274124860764\n",
      "Training loss for batch 60 : 0.008799755945801735\n",
      "Training loss for batch 61 : 0.10717183351516724\n",
      "Training loss for batch 62 : 0.01687064953148365\n",
      "Training loss for batch 63 : 0.1663396805524826\n",
      "Training loss for batch 64 : 0.11339583247900009\n",
      "Training loss for batch 65 : 0.03459376469254494\n",
      "Training loss for batch 66 : 0.17314030230045319\n",
      "Training loss for batch 67 : 0.013281077146530151\n",
      "Training loss for batch 68 : 0.12585464119911194\n",
      "Training loss for batch 69 : 0.2236880511045456\n",
      "Training loss for batch 70 : 0.11333112418651581\n",
      "Training loss for batch 71 : 0.07182265818119049\n",
      "Training loss for batch 72 : 0.018220921978354454\n",
      "Training loss for batch 73 : 0.07036126405000687\n",
      "Training loss for batch 74 : 0.022818563506007195\n",
      "Training loss for batch 75 : 0.055634647607803345\n",
      "Training loss for batch 76 : 0.11259350180625916\n",
      "Training loss for batch 77 : 0.38275155425071716\n",
      "Training loss for batch 78 : 0.014800049364566803\n",
      "Training loss for batch 79 : 0.03173061087727547\n",
      "Training loss for batch 80 : 0.055892277508974075\n",
      "Training loss for batch 81 : 0.2357170432806015\n",
      "Training loss for batch 82 : 0.07333026826381683\n",
      "Training loss for batch 83 : 0.06710938364267349\n",
      "Training loss for batch 84 : 0.15468436479568481\n",
      "Training loss for batch 85 : 0.1494579166173935\n",
      "Training loss for batch 86 : 0.010910953395068645\n",
      "Training loss for batch 87 : 0.14990919828414917\n",
      "Training loss for batch 88 : 0.06124253571033478\n",
      "Training loss for batch 89 : 0.10238334536552429\n",
      "Training loss for batch 90 : 0.1084098219871521\n",
      "Training loss for batch 91 : 0.0649195984005928\n",
      "Training loss for batch 92 : 0.030410917475819588\n",
      "Training loss for batch 93 : 0.14455078542232513\n",
      "Training loss for batch 94 : 0.1904064416885376\n",
      "Training loss for batch 95 : 0.059597842395305634\n",
      "Training loss for batch 96 : 0.15415824949741364\n",
      "Training loss for batch 97 : 0.10565948486328125\n",
      "Training loss for batch 98 : 0.17915378510951996\n",
      "Training loss for batch 99 : 0.04101895913481712\n",
      "Training loss for batch 100 : 0.2730780839920044\n",
      "Training loss for batch 101 : 0.005484997760504484\n",
      "Training loss for batch 102 : 0.07897648215293884\n",
      "Training loss for batch 103 : 0.03531814366579056\n",
      "Training loss for batch 104 : 0.05419924482703209\n",
      "Training loss for batch 105 : 0.00011097000242443755\n",
      "Training loss for batch 106 : 0.045113347470760345\n",
      "Training loss for batch 107 : 0.10647937655448914\n",
      "Training loss for batch 108 : 0.12528301775455475\n",
      "Training loss for batch 109 : 0.13419857621192932\n",
      "Training loss for batch 110 : 0.06563892960548401\n",
      "Training loss for batch 111 : 0.13950951397418976\n",
      "Training loss for batch 112 : 0.05788535252213478\n",
      "Training loss for batch 113 : 0.09348070621490479\n",
      "Training loss for batch 114 : 0.10919663310050964\n",
      "Training loss for batch 115 : 0.28255924582481384\n",
      "Training loss for batch 116 : 0.2926265299320221\n",
      "Training loss for batch 117 : 0.07920312881469727\n",
      "Training loss for batch 118 : 0.15298675000667572\n",
      "Training loss for batch 119 : 0.1601046472787857\n",
      "Training loss for batch 120 : 0.17524424195289612\n",
      "Training loss for batch 121 : 0.18439000844955444\n",
      "Training loss for batch 122 : 0.04446869716048241\n",
      "Training loss for batch 123 : 0.13808223605155945\n",
      "Training loss for batch 124 : 0.13216261565685272\n",
      "Training loss for batch 125 : 0.06787646561861038\n",
      "Training loss for batch 126 : 0.12328208982944489\n",
      "Training loss for batch 127 : 0.06753479689359665\n",
      "Training loss for batch 128 : 0.10305064171552658\n",
      "Training loss for batch 129 : 0.10181282460689545\n",
      "Training loss for batch 130 : 0.08067625761032104\n",
      "Training loss for batch 131 : 0.11131060123443604\n",
      "Training loss for batch 132 : 0.07355982810258865\n",
      "Training loss for batch 133 : 0.07914547622203827\n",
      "Training loss for batch 134 : 0.033286891877651215\n",
      "Training loss for batch 135 : 0.06991589814424515\n",
      "Training loss for batch 136 : 0.0314004011452198\n",
      "Training loss for batch 137 : 0.022573569789528847\n",
      "Training loss for batch 138 : 0.02311769500374794\n",
      "Training loss for batch 139 : 0.05157472938299179\n",
      "Training loss for batch 140 : 0.050104327499866486\n",
      "Training loss for batch 141 : 0.11565842479467392\n",
      "Training loss for batch 142 : 0.08957371115684509\n",
      "Training loss for batch 143 : 0.0031844526529312134\n",
      "Training loss for batch 144 : 0.00970374047756195\n",
      "Training loss for batch 145 : 0.11556163430213928\n",
      "Training loss for batch 146 : 0.11153516173362732\n",
      "Training loss for batch 147 : 0.04410543292760849\n",
      "Training loss for batch 148 : 0.3141791820526123\n",
      "Training loss for batch 149 : 0.044932398945093155\n",
      "Training loss for batch 150 : 0.004879315849393606\n",
      "Training loss for batch 151 : 0.08025356382131577\n",
      "Training loss for batch 152 : 0.02458277717232704\n",
      "Training loss for batch 153 : 0.16470618546009064\n",
      "Training loss for batch 154 : 0.07195339351892471\n",
      "Training loss for batch 155 : 0.07819179445505142\n",
      "Training loss for batch 156 : 0.13760794699192047\n",
      "Training loss for batch 157 : 0.08216363936662674\n",
      "Training loss for batch 158 : 0.050594769418239594\n",
      "Training loss for batch 159 : 0.21570491790771484\n",
      "Training loss for batch 160 : 0.2032206803560257\n",
      "Training loss for batch 161 : 0.013379449024796486\n",
      "Training loss for batch 162 : 0.04320218414068222\n",
      "Training loss for batch 163 : 0.05096033960580826\n",
      "Training loss for batch 164 : 0.06810218840837479\n",
      "Training loss for batch 165 : 0.07501847296953201\n",
      "Training loss for batch 166 : 0.12264841049909592\n",
      "Training loss for batch 167 : 0.12297461181879044\n",
      "Training loss for batch 168 : 0.061699774116277695\n",
      "Training loss for batch 169 : 0.19264890253543854\n",
      "Training loss for batch 170 : 0.2003665566444397\n",
      "Training loss for batch 171 : 0.047190528362989426\n",
      "Training loss for batch 172 : 0.28940996527671814\n",
      "Training loss for batch 173 : 0.05222451686859131\n",
      "Training loss for batch 174 : 0.25194406509399414\n",
      "Training loss for batch 175 : 0.08854807913303375\n",
      "Training loss for batch 176 : 0.16896340250968933\n",
      "Training loss for batch 177 : 0.09251823276281357\n",
      "Training loss for batch 178 : 0.16019126772880554\n",
      "Training loss for batch 179 : 0.17176276445388794\n",
      "Training loss for batch 180 : 0.10891330987215042\n",
      "Training loss for batch 181 : 0.21276576817035675\n",
      "Training loss for batch 182 : 0.05032459646463394\n",
      "Training loss for batch 183 : 0.01507559698075056\n",
      "Training loss for batch 184 : 0.1493813544511795\n",
      "Training loss for batch 185 : 0.11904387176036835\n",
      "Training loss for batch 186 : 0.1598556488752365\n",
      "Training loss for batch 187 : 0.15237489342689514\n",
      "Training loss for batch 188 : 0.03518754988908768\n",
      "Training loss for batch 189 : 0.24685899913311005\n",
      "Training loss for batch 190 : 0.11841660737991333\n",
      "Training loss for batch 191 : 0.3289700448513031\n",
      "Training loss for batch 192 : 0.16800165176391602\n",
      "Training loss for batch 193 : 0.09053628146648407\n",
      "Training loss for batch 194 : 0.01604524999856949\n",
      "Training loss for batch 195 : 0.07187245786190033\n",
      "Training loss for batch 196 : 0.041056256741285324\n",
      "Training loss for batch 197 : 0.1405755579471588\n",
      "Training loss for batch 198 : 0.012061957269906998\n",
      "Training loss for batch 199 : 0.034825921058654785\n",
      "Training loss for batch 200 : 0.11621387302875519\n",
      "Training loss for batch 201 : 0.07897214591503143\n",
      "Training loss for batch 202 : 0.09906485676765442\n",
      "Training loss for batch 203 : 0.027742579579353333\n",
      "Training loss for batch 204 : 0.05109042301774025\n",
      "Training loss for batch 205 : 0.03411155194044113\n",
      "Training loss for batch 206 : 0.1132894903421402\n",
      "Training loss for batch 207 : 0.18503037095069885\n",
      "Training loss for batch 208 : 0.036007653921842575\n",
      "Training loss for batch 209 : 0.10593899339437485\n",
      "Training loss for batch 210 : 0.03257011994719505\n",
      "Training loss for batch 211 : 0.034358978271484375\n",
      "Training loss for batch 212 : 0.0465552993118763\n",
      "Training loss for batch 213 : 0.0506911501288414\n",
      "Training loss for batch 214 : 0.08495137840509415\n",
      "Training loss for batch 215 : 0.15715375542640686\n",
      "Training loss for batch 216 : 0.15437866747379303\n",
      "Training loss for batch 217 : 0.05455394461750984\n",
      "Training loss for batch 218 : 0.11080503463745117\n",
      "Training loss for batch 219 : 0.022738611325621605\n",
      "Training loss for batch 220 : 0.00925230048596859\n",
      "Training loss for batch 221 : 0.12322039902210236\n",
      "Training loss for batch 222 : 0.006786605343222618\n",
      "Training loss for batch 223 : 0.05438964441418648\n",
      "Training loss for batch 224 : 0.08258286118507385\n",
      "Training loss for batch 225 : 0.2043515145778656\n",
      "Training loss for batch 226 : 0.07174500077962875\n",
      "Training loss for batch 227 : 0.06591486930847168\n",
      "Training loss for batch 228 : 0.08651009202003479\n",
      "Training loss for batch 229 : 0.11757553368806839\n",
      "Training loss for batch 230 : 0.17689073085784912\n",
      "Training loss for batch 231 : 0.18244506418704987\n",
      "Training loss for batch 232 : 0.11969727277755737\n",
      "Training loss for batch 233 : 0.0912943035364151\n",
      "Training loss for batch 234 : 0.03570372611284256\n",
      "Training loss for batch 235 : 0.12891247868537903\n",
      "Training loss for batch 236 : 0.08657482266426086\n",
      "Training loss for batch 237 : 0.07106233388185501\n",
      "Training loss for batch 238 : 0.08936731517314911\n",
      "Training loss for batch 239 : 0.031879451125860214\n",
      "Training loss for batch 240 : 0.10183218866586685\n",
      "Training loss for batch 241 : 0.06669910997152328\n",
      "Training loss for batch 242 : 0.1081845834851265\n",
      "Training loss for batch 243 : 0.09769433736801147\n",
      "Training loss for batch 244 : 0.006559113971889019\n",
      "Training loss for batch 245 : 0.15868031978607178\n",
      "Training loss for batch 246 : 0.0723455622792244\n",
      "Training loss for batch 247 : 0.028033869341015816\n",
      "Training loss for batch 248 : 0.0603654645383358\n",
      "Training loss for batch 249 : 0.22795341908931732\n",
      "Training loss for batch 250 : 0.05003475397825241\n",
      "Training loss for batch 251 : 0.0466473251581192\n",
      "Training loss for batch 252 : 0.1659654825925827\n",
      "Training loss for batch 253 : 0.10810558497905731\n",
      "Training loss for batch 254 : 0.12758219242095947\n",
      "Training loss for batch 255 : 0.12046478688716888\n",
      "Training loss for batch 256 : 0.09401417523622513\n",
      "Training loss for batch 257 : 0.1598455011844635\n",
      "Training loss for batch 258 : 0.1318226009607315\n",
      "Training loss for batch 259 : 0.06383933871984482\n",
      "Training loss for batch 260 : 0.07902128994464874\n",
      "Training loss for batch 261 : 0.20682352781295776\n",
      "Training loss for batch 262 : 0.034311484545469284\n",
      "Training loss for batch 263 : 0.09350999444723129\n",
      "Training loss for batch 264 : 0.022096728906035423\n",
      "Training loss for batch 265 : 0.1743009090423584\n",
      "Training loss for batch 266 : 0.04311106726527214\n",
      "Training loss for batch 267 : 0.03464081138372421\n",
      "Training loss for batch 268 : 0.017733201384544373\n",
      "Training loss for batch 269 : 0.14067798852920532\n",
      "Training loss for batch 270 : 0.05001688748598099\n",
      "Training loss for batch 271 : 0.07017099857330322\n",
      "Training loss for batch 272 : 0.048675455152988434\n",
      "Training loss for batch 273 : 0.06414691358804703\n",
      "Training loss for batch 274 : 0.11425568908452988\n",
      "Training loss for batch 275 : 0.08359072357416153\n",
      "Training loss for batch 276 : 0.026161974295973778\n",
      "Training loss for batch 277 : 0.19554740190505981\n",
      "Training loss for batch 278 : 0.09371098130941391\n",
      "Training loss for batch 279 : 0.05427154153585434\n",
      "Training loss for batch 280 : 0.10254455357789993\n",
      "Training loss for batch 281 : 0.033753108233213425\n",
      "Training loss for batch 282 : 0.043071944266557693\n",
      "Training loss for batch 283 : 0.08379485458135605\n",
      "Training loss for batch 284 : 0.08799651265144348\n",
      "Training loss for batch 285 : 0.17004820704460144\n",
      "Training loss for batch 286 : 0.13639186322689056\n",
      "Training loss for batch 287 : 0.2199144810438156\n",
      "Training loss for batch 288 : 0.029969070106744766\n",
      "Training loss for batch 289 : 0.07837198674678802\n",
      "Training loss for batch 290 : 0.06241815164685249\n",
      "Training loss for batch 291 : 0.05376331880688667\n",
      "Training loss for batch 292 : 0.07693877816200256\n",
      "Training loss for batch 293 : 0.24442660808563232\n",
      "Training loss for batch 294 : 0.10699649900197983\n",
      "Training loss for batch 295 : 0.010218554176390171\n",
      "Training loss for batch 296 : 0.03234955295920372\n",
      "Training loss for batch 297 : 0.05180325731635094\n",
      "Training loss for batch 298 : 0.08479251712560654\n",
      "Training loss for batch 299 : 0.13141581416130066\n",
      "Training loss for batch 300 : 0.098176509141922\n",
      "Training loss for batch 301 : 0.01610475964844227\n",
      "Training loss for batch 302 : 0.13156473636627197\n",
      "Training loss for batch 303 : 0.07720857113599777\n",
      "Training loss for batch 304 : 0.02343241311609745\n",
      "Training loss for batch 305 : 0.09821942448616028\n",
      "Training loss for batch 306 : 0.09641997516155243\n",
      "Training loss for batch 307 : 0.03976884111762047\n",
      "Training loss for batch 308 : 0.1230187714099884\n",
      "Training loss for batch 309 : 0.1269044429063797\n",
      "Training loss for batch 310 : 0.042258597910404205\n",
      "Training loss for batch 311 : 0.03499783203005791\n",
      "Training loss for batch 312 : 0.07795903086662292\n",
      "Training loss for batch 313 : 0.041566118597984314\n",
      "Training loss for batch 314 : 0.1989217847585678\n",
      "Training loss for batch 315 : 0.08929407596588135\n",
      "Training loss for batch 316 : 0.08850903809070587\n",
      "Training loss for batch 317 : 0.21072211861610413\n",
      "Training loss for batch 318 : 0.07935269176959991\n",
      "Training loss for batch 319 : 0.1552513986825943\n",
      "Training loss for batch 320 : 0.03825024887919426\n",
      "Training loss for batch 321 : 0.09944220632314682\n",
      "Training loss for batch 322 : 0.11226456612348557\n",
      "Training loss for batch 323 : 0.015127203427255154\n",
      "Training loss for batch 324 : 0.04073810949921608\n",
      "Training loss for batch 325 : 0.08097544312477112\n",
      "Training loss for batch 326 : 0.022551458328962326\n",
      "Training loss for batch 327 : 0.07279789447784424\n",
      "Training loss for batch 328 : 0.08231594413518906\n",
      "Training loss for batch 329 : 0.15005356073379517\n",
      "Training loss for batch 330 : 0.1777634620666504\n",
      "Training loss for batch 331 : 0.11783494055271149\n",
      "Training loss for batch 332 : 0.052431751042604446\n",
      "Training loss for batch 333 : 0.0382579006254673\n",
      "Training loss for batch 334 : 0.03638256713747978\n",
      "Training loss for batch 335 : 0.1743553727865219\n",
      "Training loss for batch 336 : 0.12572893500328064\n",
      "Training loss for batch 337 : 0.10849297046661377\n",
      "Training loss for batch 338 : 0.029431374743580818\n",
      "Training loss for batch 339 : 0.06505869328975677\n",
      "Training loss for batch 340 : 0.05650925263762474\n",
      "Training loss for batch 341 : 0.003983562812209129\n",
      "Training loss for batch 342 : 0.032463785260915756\n",
      "Training loss for batch 343 : 0.2258906364440918\n",
      "Training loss for batch 344 : 0.04532968997955322\n",
      "Training loss for batch 345 : 0.1649615615606308\n",
      "Training loss for batch 346 : 0.0\n",
      "Training loss for batch 347 : 0.06658882647752762\n",
      "Training loss for batch 348 : 0.14659832417964935\n",
      "Training loss for batch 349 : 0.030394908040761948\n",
      "Training loss for batch 350 : 0.2386026233434677\n",
      "Training loss for batch 351 : 0.13945327699184418\n",
      "Training loss for batch 352 : 0.06468846648931503\n",
      "Training loss for batch 353 : 0.036444541066884995\n",
      "Training loss for batch 354 : 0.09909878671169281\n",
      "Training loss for batch 355 : 0.01785988360643387\n",
      "Training loss for batch 356 : 0.11373582482337952\n",
      "Training loss for batch 357 : 0.05049442872405052\n",
      "Training loss for batch 358 : 0.1973986029624939\n",
      "Training loss for batch 359 : 0.0503704771399498\n",
      "Training loss for batch 360 : 0.05762333795428276\n",
      "Training loss for batch 361 : 0.028513817116618156\n",
      "Training loss for batch 362 : 0.08511404693126678\n",
      "Training loss for batch 363 : 0.06815872341394424\n",
      "Training loss for batch 364 : 0.009352813474833965\n",
      "Training loss for batch 365 : 0.08436858654022217\n",
      "Training loss for batch 366 : 0.03701287508010864\n",
      "Training loss for batch 367 : 0.10578262060880661\n",
      "Training loss for batch 368 : 0.011857206001877785\n",
      "Training loss for batch 369 : 0.141949862241745\n",
      "Training loss for batch 370 : 0.13794052600860596\n",
      "Training loss for batch 371 : 0.01693391054868698\n",
      "Training loss for batch 372 : 0.09408681094646454\n",
      "Training loss for batch 373 : 0.04961460456252098\n",
      "Training loss for batch 374 : 0.04110557213425636\n",
      "Training loss for batch 375 : 0.17183664441108704\n",
      "Training loss for batch 376 : 0.249083012342453\n",
      "Training loss for batch 377 : 0.06490602344274521\n",
      "Training loss for batch 378 : 0.04952433332800865\n",
      "Training loss for batch 379 : 0.04087699577212334\n",
      "Training loss for batch 380 : 0.060730766505002975\n",
      "Training loss for batch 381 : 0.19427013397216797\n",
      "Training loss for batch 382 : 0.027465786784887314\n",
      "Training loss for batch 383 : 0.0018054231768473983\n",
      "Training loss for batch 384 : 0.03074430115520954\n",
      "Training loss for batch 385 : 0.13764244318008423\n",
      "Training loss for batch 386 : 0.1626635640859604\n",
      "Training loss for batch 387 : 0.06838998198509216\n",
      "Training loss for batch 388 : 0.07664088904857635\n",
      "Training loss for batch 389 : 0.07238589972257614\n",
      "Training loss for batch 390 : 0.07803892344236374\n",
      "Training loss for batch 391 : 0.10211874544620514\n",
      "Training loss for batch 392 : 0.08523909002542496\n",
      "Training loss for batch 393 : 0.04449315741658211\n",
      "Training loss for batch 394 : 1.5215656290479274e-08\n",
      "Training loss for batch 395 : 0.06659689545631409\n",
      "Training loss for batch 396 : 0.050369881093502045\n",
      "Training loss for batch 397 : 0.04206310585141182\n",
      "Training loss for batch 398 : 0.1317765861749649\n",
      "Training loss for batch 399 : 0.1672898381948471\n",
      "Training loss for batch 400 : 0.1640440821647644\n",
      "Training loss for batch 401 : 0.050004325807094574\n",
      "Training loss for batch 402 : 0.040632568299770355\n",
      "Training loss for batch 403 : 0.1506497859954834\n",
      "Training loss for batch 404 : 0.39068132638931274\n",
      "Training loss for batch 405 : 0.06949347257614136\n",
      "Training loss for batch 406 : 0.1807677447795868\n",
      "Training loss for batch 407 : 0.22495107352733612\n",
      "Training loss for batch 408 : 0.19801229238510132\n",
      "Training loss for batch 409 : 0.11924027651548386\n",
      "Training loss for batch 410 : 0.03510967269539833\n",
      "Training loss for batch 411 : 0.1626317799091339\n",
      "Training loss for batch 412 : 0.13611207902431488\n",
      "Training loss for batch 413 : 0.05563230440020561\n",
      "Training loss for batch 414 : 0.06346651911735535\n",
      "Training loss for batch 415 : 0.06453324109315872\n",
      "Training loss for batch 416 : 0.09059818834066391\n",
      "Training loss for batch 417 : 0.12288104742765427\n",
      "Training loss for batch 418 : 0.07024252414703369\n",
      "Training loss for batch 419 : 0.0642198920249939\n",
      "Training loss for batch 420 : 0.006261808332055807\n",
      "Training loss for batch 421 : 0.049356453120708466\n",
      "Training loss for batch 422 : 0.11133885383605957\n",
      "Training loss for batch 423 : 0.3027743399143219\n",
      "Training loss for batch 424 : 0.05179218575358391\n",
      "Training loss for batch 425 : 0.08919880539178848\n",
      "Training loss for batch 426 : 0.19047951698303223\n",
      "Training loss for batch 427 : 0.11841461062431335\n",
      "Training loss for batch 428 : 0.17599371075630188\n",
      "Training loss for batch 429 : 0.12232159078121185\n",
      "Training loss for batch 430 : 0.0425594300031662\n",
      "Training loss for batch 431 : 0.1120859906077385\n",
      "Training loss for batch 432 : 0.18186084926128387\n",
      "Training loss for batch 433 : 0.0776236355304718\n",
      "Training loss for batch 434 : 0.23350617289543152\n",
      "Training loss for batch 435 : 0.0913388729095459\n",
      "Training loss for batch 436 : 0.11922510713338852\n",
      "Training loss for batch 437 : 0.10965567827224731\n",
      "Training loss for batch 438 : 0.12204079329967499\n",
      "Training loss for batch 439 : 0.10714682936668396\n",
      "Training loss for batch 440 : 0.0424279160797596\n",
      "Training loss for batch 441 : 0.020022202283143997\n",
      "Training loss for batch 442 : 0.13567472994327545\n",
      "Training loss for batch 443 : 0.04586106166243553\n",
      "Training loss for batch 444 : 0.04200505092740059\n",
      "Training loss for batch 445 : 0.006710648536682129\n",
      "Training loss for batch 446 : 0.17859964072704315\n",
      "Training loss for batch 447 : 0.06735622882843018\n",
      "Training loss for batch 448 : 0.046086132526397705\n",
      "Training loss for batch 449 : 0.07395480573177338\n",
      "Training loss for batch 450 : 0.10815319418907166\n",
      "Training loss for batch 451 : 0.07022279500961304\n",
      "Training loss for batch 452 : 1.4771990208828356e-07\n",
      "Training loss for batch 453 : 0.01796574704349041\n",
      "Training loss for batch 454 : 0.0024428304750472307\n",
      "Training loss for batch 455 : 0.030606241896748543\n",
      "Training loss for batch 456 : 0.14340350031852722\n",
      "Training loss for batch 457 : 0.04215717315673828\n",
      "Training loss for batch 458 : 0.25888508558273315\n",
      "Training loss for batch 459 : 0.04241098836064339\n",
      "Training loss for batch 460 : 0.018711157143115997\n",
      "Training loss for batch 461 : 0.0720115527510643\n",
      "Training loss for batch 462 : 0.05521988868713379\n",
      "Training loss for batch 463 : 0.12760862708091736\n",
      "Training loss for batch 464 : 0.048689909279346466\n",
      "Training loss for batch 465 : 0.13780184090137482\n",
      "Training loss for batch 466 : 0.09294488281011581\n",
      "Training loss for batch 467 : 0.13078294694423676\n",
      "Training loss for batch 468 : 0.01372943352907896\n",
      "Training loss for batch 469 : 0.07729826122522354\n",
      "Training loss for batch 470 : 0.05024590343236923\n",
      "Training loss for batch 471 : 0.05797848477959633\n",
      "Training loss for batch 472 : 0.09823846071958542\n",
      "Training loss for batch 473 : 0.1579381674528122\n",
      "Training loss for batch 474 : 0.19886574149131775\n",
      "Training loss for batch 475 : 0.04940512403845787\n",
      "Training loss for batch 476 : 0.03549705073237419\n",
      "Training loss for batch 477 : 0.03282509744167328\n",
      "Training loss for batch 478 : 0.10408888757228851\n",
      "Training loss for batch 479 : 0.03997619077563286\n",
      "Training loss for batch 480 : 0.15069907903671265\n",
      "Training loss for batch 481 : 0.2015169858932495\n",
      "Training loss for batch 482 : 0.09218387305736542\n",
      "Training loss for batch 483 : 0.04052320122718811\n",
      "Training loss for batch 484 : 0.03443821892142296\n",
      "Training loss for batch 485 : 0.13060177862644196\n",
      "Training loss for batch 486 : 0.11942772567272186\n",
      "Training loss for batch 487 : 0.09737769514322281\n",
      "Training loss for batch 488 : 0.07002782821655273\n",
      "Training loss for batch 489 : 0.008832773193717003\n",
      "Training loss for batch 490 : 0.11435205489397049\n",
      "Training loss for batch 491 : 0.08028044551610947\n",
      "Training loss for batch 492 : 0.12607309222221375\n",
      "Training loss for batch 493 : 0.07194653153419495\n",
      "Training loss for batch 494 : 0.07569600641727448\n",
      "Training loss for batch 495 : 0.04638637229800224\n",
      "Training loss for batch 496 : 0.09590111672878265\n",
      "Training loss for batch 497 : 0.08718059211969376\n",
      "Training loss for batch 498 : 0.047582391649484634\n",
      "Training loss for batch 499 : 0.009797039441764355\n",
      "Training loss for batch 500 : 0.18585629761219025\n",
      "Training loss for batch 501 : 0.018016576766967773\n",
      "Training loss for batch 502 : 0.11270536482334137\n",
      "Training loss for batch 503 : 0.06188809871673584\n",
      "Training loss for batch 504 : 0.039447590708732605\n",
      "Training loss for batch 505 : 0.20267479121685028\n",
      "Training loss for batch 506 : 0.018284916877746582\n",
      "Training loss for batch 507 : 0.04674119874835014\n",
      "Training loss for batch 508 : 0.10344448685646057\n",
      "Training loss for batch 509 : 0.03205610439181328\n",
      "Training loss for batch 510 : 0.024685131385922432\n",
      "Training loss for batch 511 : 0.028154421597719193\n",
      "Training loss for batch 512 : 0.0426790751516819\n",
      "Training loss for batch 513 : 0.09040287137031555\n",
      "Training loss for batch 514 : 0.10481160134077072\n",
      "Training loss for batch 515 : 0.0442202128469944\n",
      "Training loss for batch 516 : 0.04487881064414978\n",
      "Training loss for batch 517 : 0.01959838718175888\n",
      "Training loss for batch 518 : 0.0424661785364151\n",
      "Training loss for batch 519 : 0.019230499863624573\n",
      "Training loss for batch 520 : 0.13152244687080383\n",
      "Training loss for batch 521 : 0.034296222031116486\n",
      "Training loss for batch 522 : 0.0009114808635786176\n",
      "Training loss for batch 523 : 0.13506163656711578\n",
      "Training loss for batch 524 : 0.09332355111837387\n",
      "Training loss for batch 525 : 0.07394418120384216\n",
      "Training loss for batch 526 : 0.10517312586307526\n",
      "Training loss for batch 527 : 0.05393855646252632\n",
      "Training loss for batch 528 : 0.11367672681808472\n",
      "Training loss for batch 529 : 0.0036388353910297155\n",
      "Training loss for batch 530 : 0.032723501324653625\n",
      "Training loss for batch 531 : 0.04208052158355713\n",
      "Training loss for batch 532 : 0.0883345901966095\n",
      "Training loss for batch 533 : 0.036665476858615875\n",
      "Training loss for batch 534 : 0.1354837715625763\n",
      "Training loss for batch 535 : 0.09340549260377884\n",
      "Training loss for batch 536 : 0.0657077506184578\n",
      "Training loss for batch 537 : 0.02413882128894329\n",
      "Training loss for batch 538 : 0.2133159637451172\n",
      "Training loss for batch 539 : 0.08450303971767426\n",
      "Training loss for batch 540 : 0.09057067334651947\n",
      "Training loss for batch 541 : 0.07180105149745941\n",
      "Training loss for batch 542 : 0.0\n",
      "Training loss for batch 543 : 0.028571734204888344\n",
      "Training loss for batch 544 : 1.0380833259659994e-07\n",
      "Training loss for batch 545 : 0.14119312167167664\n",
      "Training loss for batch 546 : 0.1698940098285675\n",
      "Training loss for batch 547 : 0.27984172105789185\n",
      "Training loss for batch 548 : 0.030817655846476555\n",
      "Training loss for batch 549 : 0.0578300915658474\n",
      "Training loss for batch 550 : 0.12707917392253876\n",
      "Training loss for batch 551 : 0.0852079689502716\n",
      "Training loss for batch 552 : 0.10024519264698029\n",
      "Training loss for batch 553 : 0.2922469675540924\n",
      "Training loss for batch 554 : 0.22149790823459625\n",
      "Training loss for batch 555 : 0.07994929701089859\n",
      "Training loss for batch 556 : 0.1230541318655014\n",
      "Training loss for batch 557 : 0.0090766791254282\n",
      "Training loss for batch 558 : 0.030637631192803383\n",
      "Training loss for batch 559 : 0.085698202252388\n",
      "Training loss for batch 560 : 0.008090382441878319\n",
      "Training loss for batch 561 : 0.04719705507159233\n",
      "Training loss for batch 562 : 0.06016852334141731\n",
      "Training loss for batch 563 : 0.18927106261253357\n",
      "Training loss for batch 564 : 0.07848118990659714\n",
      "Training loss for batch 565 : 0.04847754165530205\n",
      "Training loss for batch 566 : 0.1526893973350525\n",
      "Training loss for batch 567 : 0.15810000896453857\n",
      "Training loss for batch 568 : 0.08729203045368195\n",
      "Training loss for batch 569 : 0.09668494015932083\n",
      "Training loss for batch 570 : 0.11927418410778046\n",
      "Training loss for batch 571 : 0.08278480917215347\n",
      "Training loss for batch 572 : 0.19293467700481415\n",
      "Training loss for batch 573 : 0.12576018273830414\n",
      "Training loss for batch 574 : 0.03349829837679863\n",
      "Training loss for batch 575 : 0.11815861612558365\n",
      "Training loss for batch 576 : 0.24074222147464752\n",
      "Training loss for batch 577 : 0.07929981499910355\n",
      "Training loss for batch 578 : 0.004154831171035767\n",
      "Training loss for batch 579 : 0.011817047372460365\n",
      "Training loss for batch 580 : 0.05365505442023277\n",
      "Training loss for batch 581 : 0.11249824613332748\n",
      "Training loss for batch 582 : 0.05608714744448662\n",
      "Training loss for batch 583 : 0.016326531767845154\n",
      "Training loss for batch 584 : 0.06568856537342072\n",
      "Training loss for batch 585 : 0.011580010876059532\n",
      "Training loss for batch 586 : 0.06791845709085464\n",
      "Training loss for batch 587 : 0.16657349467277527\n",
      "Training loss for batch 588 : 0.04326900839805603\n",
      "Training loss for batch 589 : 0.05012361332774162\n",
      "Training loss for batch 590 : 0.03756709396839142\n",
      "Training loss for batch 591 : 0.03948706015944481\n",
      "Training loss for batch 592 : 0.09797654300928116\n",
      "Training loss for batch 593 : 0.09187112003564835\n",
      "Training loss for batch 594 : 0.0946921706199646\n",
      "Training loss for batch 595 : 0.1260528266429901\n",
      "Training loss for batch 596 : 0.30772510170936584\n",
      "Training loss for batch 597 : 0.04644438251852989\n",
      "Training loss for batch 598 : 0.06366194039583206\n",
      "Training loss for batch 599 : 0.0812218189239502\n",
      "Training loss for batch 600 : 0.27814698219299316\n",
      "Training loss for batch 601 : 0.12657777965068817\n",
      "Training loss for batch 602 : 0.0706297904253006\n",
      "Training loss for batch 603 : 0.11427603662014008\n",
      "Training loss for batch 604 : 0.13695020973682404\n",
      "Training loss for batch 605 : 0.17931236326694489\n",
      "Training loss for batch 606 : 0.15639521181583405\n",
      "Training loss for batch 607 : 0.22818414866924286\n",
      "Training loss for batch 608 : 0.004805986303836107\n",
      "Training loss for batch 609 : 0.019692260771989822\n",
      "Training loss for batch 610 : 0.04919103533029556\n",
      "Training loss for batch 611 : 0.03289499878883362\n",
      "Training loss for batch 612 : 0.11412645131349564\n",
      "Training loss for batch 613 : 0.04434780031442642\n",
      "Training loss for batch 614 : 0.07068843394517899\n",
      "Training loss for batch 615 : 0.11931770294904709\n",
      "Training loss for batch 616 : 0.09270372241735458\n",
      "Training loss for batch 617 : 0.08240379393100739\n",
      "Training loss for batch 618 : 0.15525653958320618\n",
      "Training loss for batch 619 : 0.07540658861398697\n",
      "Training loss for batch 620 : 0.16455338895320892\n",
      "Training loss for batch 621 : 0.04205276444554329\n",
      "Training loss for batch 622 : 0.10633693635463715\n",
      "Training loss for batch 623 : 0.20268264412879944\n",
      "Training loss for batch 624 : 0.10689995437860489\n",
      "Training loss for batch 625 : 0.11928793787956238\n",
      "Training loss for batch 626 : 0.06201145052909851\n",
      "Training loss for batch 627 : 0.06517285853624344\n",
      "Training loss for batch 628 : 0.19810569286346436\n",
      "Training loss for batch 629 : 0.18455441296100616\n",
      "Training loss for batch 630 : 0.029860004782676697\n",
      "Training loss for batch 631 : 0.06205052137374878\n",
      "Training loss for batch 632 : 0.1232508197426796\n",
      "Training loss for batch 633 : 0.07204057276248932\n",
      "Training loss for batch 634 : 0.07553922384977341\n",
      "Training loss for batch 635 : 0.0269220732152462\n",
      "Training loss for batch 636 : 0.09940989315509796\n",
      "Training loss for batch 637 : 0.2434910088777542\n",
      "Training loss for batch 638 : 0.023862484842538834\n",
      "Training loss for batch 639 : 0.32078102231025696\n",
      "Training loss for batch 640 : 0.12289721518754959\n",
      "Training loss for batch 641 : 0.1418420672416687\n",
      "Training loss for batch 642 : 0.03555741533637047\n",
      "Training loss for batch 643 : 0.05870208516716957\n",
      "Training loss for batch 644 : 0.027565177530050278\n",
      "Training loss for batch 645 : 0.053850848227739334\n",
      "Training loss for batch 646 : 0.04076550900936127\n",
      "Training loss for batch 647 : 0.3092069625854492\n",
      "Training loss for batch 648 : 0.10876554250717163\n",
      "Training loss for batch 649 : 0.11400727927684784\n",
      "Training loss for batch 650 : 0.046772658824920654\n",
      "Training loss for batch 651 : 0.041352324187755585\n",
      "Training loss for batch 652 : 0.15331411361694336\n",
      "Training loss for batch 653 : 0.07424888014793396\n",
      "Training loss for batch 654 : 0.12910878658294678\n",
      "Training loss for batch 655 : 0.04169105738401413\n",
      "Training loss for batch 656 : 0.050151277333498\n",
      "Training loss for batch 657 : 0.06521813571453094\n",
      "Training loss for batch 658 : 0.239132821559906\n",
      "Training loss for batch 659 : 0.1889500468969345\n",
      "Training loss for batch 660 : 0.08338075131177902\n",
      "Training loss for batch 661 : 0.029316915199160576\n",
      "Training loss for batch 662 : 0.0009180779452435672\n",
      "Training loss for batch 663 : 0.17568248510360718\n",
      "Training loss for batch 664 : 0.17636296153068542\n",
      "Training loss for batch 665 : 0.11823969334363937\n",
      "Training loss for batch 666 : 0.05443219095468521\n",
      "Training loss for batch 667 : 0.18896295130252838\n",
      "Training loss for batch 668 : 0.04263670742511749\n",
      "Training loss for batch 669 : 0.061232879757881165\n",
      "Training loss for batch 670 : 0.06507280468940735\n",
      "Training loss for batch 671 : 0.32501018047332764\n",
      "Training loss for batch 672 : 0.05313652381300926\n",
      "Training loss for batch 673 : 0.07345564663410187\n",
      "Training loss for batch 674 : 0.03355323150753975\n",
      "Training loss for batch 675 : 0.0366959385573864\n",
      "Training loss for batch 676 : 0.14548635482788086\n",
      "Training loss for batch 677 : 0.03935180976986885\n",
      "Training loss for batch 678 : 0.06769267469644547\n",
      "Training loss for batch 679 : 0.08420123159885406\n",
      "Training loss for batch 680 : 0.0592087022960186\n",
      "Training loss for batch 681 : 0.017794907093048096\n",
      "Training loss for batch 682 : 0.13540540635585785\n",
      "Training loss for batch 683 : 0.04725220799446106\n",
      "Training loss for batch 684 : 0.1770356446504593\n",
      "Training loss for batch 685 : 0.053422711789608\n",
      "Training loss for batch 686 : 0.13558395206928253\n",
      "Training loss for batch 687 : 0.2048078030347824\n",
      "Training loss for batch 688 : 0.025651965290308\n",
      "Training loss for batch 689 : 0.09874344617128372\n",
      "Training loss for batch 690 : 0.040890082716941833\n",
      "Training loss for batch 691 : 0.05062373727560043\n",
      "Training loss for batch 692 : 0.027643628418445587\n",
      "Training loss for batch 693 : 0.07803408056497574\n",
      "Training loss for batch 694 : 0.08401837199926376\n",
      "Training loss for batch 695 : 4.895879612831777e-08\n",
      "Training loss for batch 696 : 0.021373316645622253\n",
      "Training loss for batch 697 : 0.026562990620732307\n",
      "Training loss for batch 698 : 0.04466293752193451\n",
      "Training loss for batch 699 : 0.2925523817539215\n",
      "Training loss for batch 700 : 0.1750779002904892\n",
      "Training loss for batch 701 : 0.1862841695547104\n",
      "Training loss for batch 702 : 0.13353504240512848\n",
      "Training loss for batch 703 : 0.12461470812559128\n",
      "Training loss for batch 704 : 0.07064416259527206\n",
      "Training loss for batch 705 : 0.04543187469244003\n",
      "Training loss for batch 706 : 0.00845564529299736\n",
      "Training loss for batch 707 : 0.21052488684654236\n",
      "Training loss for batch 708 : 0.08650034666061401\n",
      "Training loss for batch 709 : 0.23953446745872498\n",
      "Training loss for batch 710 : 0.131773442029953\n",
      "Training loss for batch 711 : 0.02685011737048626\n",
      "Training loss for batch 712 : 0.22450891137123108\n",
      "Training loss for batch 713 : 0.13147839903831482\n",
      "Training loss for batch 714 : 0.08935900032520294\n",
      "Training loss for batch 715 : 0.16974510252475739\n",
      "Training loss for batch 716 : 0.12778449058532715\n",
      "Training loss for batch 717 : 0.03785574436187744\n",
      "Training loss for batch 718 : 0.12070991843938828\n",
      "Training loss for batch 719 : 0.011188493110239506\n",
      "Training loss for batch 720 : 0.029360715299844742\n",
      "Training loss for batch 721 : 0.011530383490025997\n",
      "Training loss for batch 722 : 0.1715526431798935\n",
      "Training loss for batch 723 : 0.04857891798019409\n",
      "Training loss for batch 724 : 0.13396131992340088\n",
      "Training loss for batch 725 : 0.08025138080120087\n",
      "Training loss for batch 726 : 0.14198829233646393\n",
      "Training loss for batch 727 : 0.04867170751094818\n",
      "Training loss for batch 728 : 0.09205199033021927\n",
      "Training loss for batch 729 : 0.13324648141860962\n",
      "Training loss for batch 730 : 0.02985924482345581\n",
      "Training loss for batch 731 : 0.023157773539423943\n",
      "Training loss for batch 732 : 0.03870667889714241\n",
      "Training loss for batch 733 : 0.03852364793419838\n",
      "Training loss for batch 734 : 0.0516216941177845\n",
      "Training loss for batch 735 : 0.10744953155517578\n",
      "Training loss for batch 736 : 0.13543222844600677\n",
      "Training loss for batch 737 : 0.027427280321717262\n",
      "Training loss for batch 738 : 0.04643050953745842\n",
      "Training loss for batch 739 : 0.0890917032957077\n",
      "Training loss for batch 740 : 0.08649596571922302\n",
      "Training loss for batch 741 : 0.06151296943426132\n",
      "Training loss for batch 742 : 0.11542856693267822\n",
      "Training loss for batch 743 : 0.0753922089934349\n",
      "Training loss for batch 744 : 0.055590979754924774\n",
      "Training loss for batch 745 : 0.11620457470417023\n",
      "Training loss for batch 746 : 0.03895411640405655\n",
      "Training loss for batch 747 : 0.1756640076637268\n",
      "Training loss for batch 748 : 0.04269775003194809\n",
      "Training loss for batch 749 : 0.05701399967074394\n",
      "Training loss for batch 750 : 0.09473665058612823\n",
      "Training loss for batch 751 : 0.12343776971101761\n",
      "Training loss for batch 752 : 0.10042773187160492\n",
      "Training loss for batch 753 : 0.16854511201381683\n",
      "Training loss for batch 754 : 0.020275214686989784\n",
      "Training loss for batch 755 : 0.06270559877157211\n",
      "Training loss for batch 756 : 0.05422462522983551\n",
      "Training loss for batch 757 : 0.052842121571302414\n",
      "Training loss for batch 758 : 0.08977541327476501\n",
      "Training loss for batch 759 : 0.11745698750019073\n",
      "Training loss for batch 760 : 0.15614186227321625\n",
      "Training loss for batch 761 : 0.10000500082969666\n",
      "Training loss for batch 762 : 7.748674590857263e-09\n",
      "Training loss for batch 763 : 0.14983126521110535\n",
      "Training loss for batch 764 : 0.05200933665037155\n",
      "Training loss for batch 765 : 0.2034871131181717\n",
      "Training loss for batch 766 : 0.1887185424566269\n",
      "Training loss for batch 767 : 0.1580803096294403\n",
      "Training loss for batch 768 : 0.033934708684682846\n",
      "Training loss for batch 769 : 0.09851190447807312\n",
      "Training loss for batch 770 : 0.1849880814552307\n",
      "Training loss for batch 771 : 0.028500502929091454\n",
      "Training loss for batch 772 : 0.021744217723608017\n",
      "Training loss for batch 773 : 0.15378494560718536\n",
      "Training loss for batch 774 : 0.07523626834154129\n",
      "Training loss for batch 775 : 0.03795693814754486\n",
      "Training loss for batch 776 : 0.04172050207853317\n",
      "Training loss for batch 777 : 0.024668686091899872\n",
      "Training loss for batch 778 : 0.2127770185470581\n",
      "Training loss for batch 779 : 0.16815169155597687\n",
      "Training loss for batch 780 : 0.07459846138954163\n",
      "Training loss for batch 781 : 0.034401483833789825\n",
      "Training loss for batch 782 : 0.11680088937282562\n",
      "Training loss for batch 783 : 0.15027400851249695\n",
      "Training loss for batch 784 : 0.04561489075422287\n",
      "Training loss for batch 785 : 0.08341182768344879\n",
      "Training loss for batch 786 : 0.07252208143472672\n",
      "Training loss for batch 787 : 0.03360442817211151\n",
      "Training loss for batch 788 : 0.12776346504688263\n",
      "Training loss for batch 789 : 0.03171003237366676\n",
      "Training loss for batch 790 : 0.103605717420578\n",
      "Training loss for batch 791 : 0.024209750816226006\n",
      "Training loss for batch 792 : 0.10101105272769928\n",
      "Training loss for batch 793 : 0.035425376147031784\n",
      "Training loss for batch 794 : 0.08850891888141632\n",
      "Training loss for batch 795 : 0.011564530432224274\n",
      "Training loss for batch 796 : 0.19430118799209595\n",
      "Training loss for batch 797 : 0.0400746613740921\n",
      "Training loss for batch 798 : 0.008650310337543488\n",
      "Training loss for batch 799 : 0.029025238007307053\n",
      "Training loss for batch 800 : 0.07195613533258438\n",
      "Training loss for batch 801 : 0.12315752357244492\n",
      "Training loss for batch 802 : 0.045794837176799774\n",
      "Training loss for batch 803 : 0.061677031219005585\n",
      "Training loss for batch 804 : 0.02601068839430809\n",
      "Training loss for batch 805 : 0.07560009509325027\n",
      "Training loss for batch 806 : 0.07087511569261551\n",
      "Training loss for batch 807 : 0.12475843727588654\n",
      "Training loss for batch 808 : 0.04478704556822777\n",
      "Training loss for batch 809 : 0.014318625442683697\n",
      "Training loss for batch 810 : 0.11226178705692291\n",
      "Training loss for batch 811 : 0.05204694718122482\n",
      "Training loss for batch 812 : 0.08049985766410828\n",
      "Training loss for batch 813 : 0.17709693312644958\n",
      "Training loss for batch 814 : 0.036705467849969864\n",
      "Training loss for batch 815 : 0.13418486714363098\n",
      "Training loss for batch 816 : 0.1947462111711502\n",
      "Training loss for batch 817 : 0.06405206024646759\n",
      "Training loss for batch 818 : 0.16165952384471893\n",
      "Training loss for batch 819 : 0.12206533551216125\n",
      "Training loss for batch 820 : 0.07012780755758286\n",
      "Training loss for batch 821 : 0.15709996223449707\n",
      "Training loss for batch 822 : 0.04922030493617058\n",
      "Training loss for batch 823 : 0.06252433359622955\n",
      "Training loss for batch 824 : 0.05903434753417969\n",
      "Training loss for batch 825 : 0.07786340266466141\n",
      "Training loss for batch 826 : 0.1280922293663025\n",
      "Training loss for batch 827 : 0.11447865515947342\n",
      "Training loss for batch 828 : 0.04510089382529259\n",
      "Training loss for batch 829 : 0.026011422276496887\n",
      "Training loss for batch 830 : 0.05637078732252121\n",
      "Training loss for batch 831 : 0.20656433701515198\n",
      "Training loss for batch 832 : 0.01717856340110302\n",
      "Training loss for batch 833 : 0.18410702049732208\n",
      "Training loss for batch 834 : 0.11704069375991821\n",
      "Training loss for batch 835 : 0.07918966561555862\n",
      "Training loss for batch 836 : 0.19470316171646118\n",
      "Training loss for batch 837 : 0.12472154200077057\n",
      "Training loss for batch 838 : 0.015261700376868248\n",
      "Training loss for batch 839 : 0.11619485169649124\n",
      "Training loss for batch 840 : 0.06261032819747925\n",
      "Training loss for batch 841 : 0.08152898401021957\n",
      "Training loss for batch 842 : 0.02040589042007923\n",
      "Training loss for batch 843 : 0.0774456262588501\n",
      "Training loss for batch 844 : 0.09898457676172256\n",
      "Training loss for batch 845 : 0.3013684153556824\n",
      "Training loss for batch 846 : 0.35251495242118835\n",
      "Training loss for batch 847 : 0.04828707128763199\n",
      "Training loss for batch 848 : 0.16803082823753357\n",
      "Training loss for batch 849 : 0.03755933418869972\n",
      "Training loss for batch 850 : 0.13760364055633545\n",
      "Training loss for batch 851 : 0.15749140083789825\n",
      "Training loss for batch 852 : 0.12008343636989594\n",
      "Training loss for batch 853 : 0.04925420507788658\n",
      "Training loss for batch 854 : 0.2363935112953186\n",
      "Training loss for batch 855 : 0.0741204246878624\n",
      "Training loss for batch 856 : 0.08445164561271667\n",
      "Training loss for batch 857 : 0.06636340916156769\n",
      "Training loss for batch 858 : 0.07941201329231262\n",
      "Training loss for batch 859 : 0.11437954008579254\n",
      "Training loss for batch 860 : 0.05910908058285713\n",
      "Training loss for batch 861 : 0.0625305250287056\n",
      "Training loss for batch 862 : 0.040912460535764694\n",
      "Training loss for batch 863 : 0.14572705328464508\n",
      "Training loss for batch 864 : 0.08499571681022644\n",
      "Training loss for batch 865 : 0.08543829619884491\n",
      "Training loss for batch 866 : 0.1436261683702469\n",
      "Training loss for batch 867 : 0.11140463501214981\n",
      "Training loss for batch 868 : 0.06499318033456802\n",
      "Training loss for batch 869 : 0.12333566695451736\n",
      "Training loss for batch 870 : 0.07590670883655548\n",
      "Training loss for batch 871 : 0.13515697419643402\n",
      "Training loss for batch 872 : 0.1083226203918457\n",
      "Training loss for batch 873 : 0.08171506226062775\n",
      "Training loss for batch 874 : 0.05040717124938965\n",
      "Training loss for batch 875 : 0.18042507767677307\n",
      "Training loss for batch 876 : 0.1706564724445343\n",
      "Training loss for batch 877 : 0.11522383987903595\n",
      "Training loss for batch 878 : 0.2277614325284958\n",
      "Training loss for batch 879 : 0.04278567433357239\n",
      "Training loss for batch 880 : 0.23661905527114868\n",
      "Training loss for batch 881 : 0.141046941280365\n",
      "Training loss for batch 882 : 0.14409968256950378\n",
      "Training loss for batch 883 : 0.045020438730716705\n",
      "Training loss for batch 884 : 0.1295812875032425\n",
      "Training loss for batch 885 : 0.06950026750564575\n",
      "Training loss for batch 886 : 0.08308424800634384\n",
      "Training loss for batch 887 : 0.1272038221359253\n",
      "Training loss for batch 888 : 0.029283655807375908\n",
      "Training loss for batch 889 : 0.08330456912517548\n",
      "Training loss for batch 890 : 0.0016648538876324892\n",
      "Training loss for batch 891 : 0.029810616746544838\n",
      "Training loss for batch 892 : 0.08944142609834671\n",
      "Training loss for batch 893 : 0.0779455155134201\n",
      "Training loss for batch 894 : 0.013831543736159801\n",
      "Training loss for batch 895 : 0.12223561853170395\n",
      "Training loss for batch 896 : 0.11092278361320496\n",
      "Training loss for batch 897 : 0.0009731451864354312\n",
      "Training loss for batch 898 : 0.15966399013996124\n",
      "Training loss for batch 899 : 0.06139454245567322\n",
      "Training loss for batch 900 : 0.18720082938671112\n",
      "Training loss for batch 901 : 0.21750974655151367\n",
      "Training loss for batch 902 : 0.0011695564026013017\n",
      "Training loss for batch 903 : 0.01318309735506773\n",
      "Training loss for batch 904 : 0.15662531554698944\n",
      "Training loss for batch 905 : 0.039479635655879974\n",
      "Training loss for batch 906 : 0.024978604167699814\n",
      "Training loss for batch 907 : 0.13082255423069\n",
      "Training loss for batch 908 : 0.07587024569511414\n",
      "Training loss for batch 909 : 0.017589325085282326\n",
      "Training loss for batch 910 : 0.12173581123352051\n",
      "Training loss for batch 911 : 0.21494236588478088\n",
      "Training loss for batch 912 : 0.09892461448907852\n",
      "Training loss for batch 913 : 0.0029613166116178036\n",
      "Training loss for batch 914 : 0.0690767839550972\n",
      "Training loss for batch 915 : 0.008357684127986431\n",
      "Training loss for batch 916 : 0.12226873636245728\n",
      "Training loss for batch 917 : 0.12458007782697678\n",
      "Training loss for batch 918 : 0.17533661425113678\n",
      "Training loss for batch 919 : 0.022673077881336212\n",
      "Training loss for batch 920 : 0.042013075202703476\n",
      "Training loss for batch 921 : 0.09766419976949692\n",
      "Training loss for batch 922 : 0.001736953854560852\n",
      "Training loss for batch 923 : 0.07563985884189606\n",
      "Training loss for batch 924 : 0.05203799158334732\n",
      "Training loss for batch 925 : 0.01645752042531967\n",
      "Training loss for batch 926 : 0.06946761161088943\n",
      "Training loss for batch 927 : 0.20112542808055878\n",
      "Training loss for batch 928 : 0.04364023730158806\n",
      "Training loss for batch 929 : 0.07884252071380615\n",
      "Training loss for batch 930 : 0.055247291922569275\n",
      "Training loss for batch 931 : 0.1259634643793106\n",
      "Training loss for batch 932 : 0.08795907348394394\n",
      "Training loss for batch 933 : 0.009620974771678448\n",
      "Training loss for batch 934 : 0.051622603088617325\n",
      "Training loss for batch 935 : 0.21467360854148865\n",
      "Training loss for batch 936 : 0.04714704304933548\n",
      "Training loss for batch 937 : 0.20758891105651855\n",
      "Training loss for batch 938 : 0.13495950400829315\n",
      "Training loss for batch 939 : 0.25778189301490784\n",
      "Training loss for batch 940 : 0.0023577313404530287\n",
      "Training loss for batch 941 : 0.05694010108709335\n",
      "Training loss for batch 942 : 0.059399910271167755\n",
      "Training loss for batch 943 : 0.13996699452400208\n",
      "Training loss for batch 944 : 0.03384993225336075\n",
      "Training loss for batch 945 : 0.06410368531942368\n",
      "Training loss for batch 946 : 0.04934140667319298\n",
      "Training loss for batch 947 : 0.12688755989074707\n",
      "Training loss for batch 948 : 0.1387753188610077\n",
      "Training loss for batch 949 : 0.015566976740956306\n",
      "Training loss for batch 950 : 0.1519085168838501\n",
      "Training loss for batch 951 : 0.08651034533977509\n",
      "Training loss for batch 952 : 0.14477623999118805\n",
      "Training loss for batch 953 : 0.06545006483793259\n",
      "Training loss for batch 954 : 0.012775581330060959\n",
      "Training loss for batch 955 : 0.14350301027297974\n",
      "Training loss for batch 956 : 0.16290129721164703\n",
      "Training loss for batch 957 : 0.14093568921089172\n",
      "Training loss for batch 958 : 0.2158939242362976\n",
      "Training loss for batch 959 : 0.07453843951225281\n",
      "Training loss for batch 960 : 0.006557574961334467\n",
      "Training loss for batch 961 : 0.062370482832193375\n",
      "Training loss for batch 962 : 0.006971832364797592\n",
      "Training loss for batch 963 : 0.2662433087825775\n",
      "Training loss for batch 964 : 0.2301647812128067\n",
      "Training loss for batch 965 : 0.038036465644836426\n",
      "Training loss for batch 966 : 0.14893093705177307\n",
      "Training loss for batch 967 : 0.052751656621694565\n",
      "Training loss for batch 968 : 0.02351973205804825\n",
      "Training loss for batch 969 : 0.1581626534461975\n",
      "Training loss for batch 970 : 0.12095458060503006\n",
      "Training loss for batch 971 : 0.03572119399905205\n",
      "Training loss for batch 972 : 0.026240356266498566\n",
      "Training loss for batch 973 : 0.07647326588630676\n",
      "Training loss for batch 974 : 0.06277812272310257\n",
      "Training loss for batch 975 : 0.030907826498150826\n",
      "Training loss for batch 976 : 0.03623019531369209\n",
      "Training loss for batch 977 : 0.1283016949892044\n",
      "Training loss for batch 978 : 0.033825259655714035\n",
      "Training loss for batch 979 : 0.16143041849136353\n",
      "Training loss for batch 980 : 0.053589947521686554\n",
      "Training loss for batch 981 : 0.12522682547569275\n",
      "Training loss for batch 982 : 0.04979463666677475\n",
      "Training loss for batch 983 : 0.08896264433860779\n",
      "Training loss for batch 984 : 0.004075139760971069\n",
      "Training loss for batch 985 : 0.032164353877305984\n",
      "Training loss for batch 986 : 0.023202043026685715\n",
      "Training loss for batch 987 : 0.20785321295261383\n",
      "Training loss for batch 988 : 0.027190927416086197\n",
      "Training loss for batch 989 : 0.09652692824602127\n",
      "Training loss for batch 990 : 0.08697206526994705\n",
      "Training loss for batch 991 : 0.011633983813226223\n",
      "Training loss for batch 992 : 0.12418262660503387\n",
      "Training loss for batch 993 : 0.09762536734342575\n",
      "Training loss for batch 994 : 0.24970857799053192\n",
      "Training loss for batch 995 : 0.025747012346982956\n",
      "Training loss for batch 996 : 0.017722783610224724\n",
      "Training loss for batch 997 : 0.10596781969070435\n",
      "Training loss for batch 998 : 0.08081343024969101\n",
      "Training loss for batch 999 : 0.02618933655321598\n",
      "Training loss for batch 1000 : 0.08655150979757309\n",
      "Training loss for batch 1001 : 0.03721531853079796\n",
      "Training loss for batch 1002 : 0.04018951952457428\n",
      "Training loss for batch 1003 : 0.006045933347195387\n",
      "Training loss for batch 1004 : 0.20258501172065735\n",
      "Training loss for batch 1005 : 0.03611728921532631\n",
      "Training loss for batch 1006 : 0.29281681776046753\n",
      "Training loss for batch 1007 : 0.2550033926963806\n",
      "Training loss for batch 1008 : 0.04951242357492447\n",
      "Training loss for batch 1009 : 0.0956893265247345\n",
      "Training loss for batch 1010 : 0.22846591472625732\n",
      "Training loss for batch 1011 : 0.049732837826013565\n",
      "Training loss for batch 1012 : 0.1410718858242035\n",
      "Training loss for batch 1013 : 0.026211895048618317\n",
      "Training loss for batch 1014 : 0.04125484451651573\n",
      "Training loss for batch 1015 : 0.04426266998052597\n",
      "Training loss for batch 1016 : 0.16870911419391632\n",
      "Training loss for batch 1017 : 0.024104490876197815\n",
      "Training loss for batch 1018 : 0.10491622239351273\n",
      "Training loss for batch 1019 : 0.07556081563234329\n",
      "Training loss for batch 1020 : 0.056218937039375305\n",
      "Training loss for batch 1021 : 0.06992917507886887\n",
      "Training loss for batch 1022 : 0.04643343761563301\n",
      "Training loss for batch 1023 : 0.07573813199996948\n",
      "Training loss for batch 1024 : 0.031961839646101\n",
      "Training loss for batch 1025 : 0.19617141783237457\n",
      "Training loss for batch 1026 : 0.11725589632987976\n",
      "Training loss for batch 1027 : 0.05802781879901886\n",
      "Training loss for batch 1028 : 0.02394714020192623\n",
      "Training loss for batch 1029 : 0.009559313766658306\n",
      "Training loss for batch 1030 : 0.2655958831310272\n",
      "Training loss for batch 1031 : 0.20715507864952087\n",
      "Training loss for batch 1032 : 0.06836733222007751\n",
      "Training loss for batch 1033 : 0.05879981070756912\n",
      "Training loss for batch 1034 : 0.04544883221387863\n",
      "Training loss for batch 1035 : 0.12374011427164078\n",
      "Training loss for batch 1036 : 0.03738454356789589\n",
      "Training loss for batch 1037 : 0.12470662593841553\n",
      "Training loss for batch 1038 : 0.17119759321212769\n",
      "Training loss for batch 1039 : 0.057144198566675186\n",
      "Training loss for batch 1040 : 0.026367904618382454\n",
      "Training loss for batch 1041 : 0.0919729694724083\n",
      "Training loss for batch 1042 : 0.02800850011408329\n",
      "Training loss for batch 1043 : 0.13760685920715332\n",
      "Training loss for batch 1044 : 0.04658738523721695\n",
      "Training loss for batch 1045 : 0.07092536985874176\n",
      "Training loss for batch 1046 : 0.054657816886901855\n",
      "Training loss for batch 1047 : 0.07193110883235931\n",
      "Training loss for batch 1048 : 0.03028860315680504\n",
      "Training loss for batch 1049 : 0.05716656520962715\n",
      "Training loss for batch 1050 : 0.13185790181159973\n",
      "Training loss for batch 1051 : 0.20437614619731903\n",
      "Training loss for batch 1052 : 0.10114259272813797\n",
      "Training loss for batch 1053 : 0.060529742389917374\n",
      "Training loss for batch 1054 : 0.07684752345085144\n",
      "Training loss for batch 1055 : 0.025905871763825417\n",
      "Training loss for batch 1056 : 0.2641999423503876\n",
      "Training loss for batch 1057 : 0.09148016571998596\n",
      "Training loss for batch 1058 : 0.023931460455060005\n",
      "Training loss for batch 1059 : 0.0073353503830730915\n",
      "Training loss for batch 1060 : 0.03479848429560661\n",
      "Training loss for batch 1061 : 0.03238971158862114\n",
      "Training loss for batch 1062 : 0.15805256366729736\n",
      "Training loss for batch 1063 : 0.01579955965280533\n",
      "Training loss for batch 1064 : 0.03221185505390167\n",
      "Training loss for batch 1065 : 0.011887033469974995\n",
      "Training loss for batch 1066 : 0.040784042328596115\n",
      "Training loss for batch 1067 : 0.16137966513633728\n",
      "Training loss for batch 1068 : 0.0252227783203125\n",
      "Training loss for batch 1069 : 0.027585750445723534\n",
      "Training loss for batch 1070 : 0.43417468667030334\n",
      "Training loss for batch 1071 : 0.225682333111763\n",
      "Training loss for batch 1072 : 0.10107439011335373\n",
      "Training loss for batch 1073 : 0.05794154107570648\n",
      "Training loss for batch 1074 : 0.12998665869235992\n",
      "Training loss for batch 1075 : 0.01865638606250286\n",
      "Training loss for batch 1076 : 0.07826004177331924\n",
      "Training loss for batch 1077 : 0.16218094527721405\n",
      "Training loss for batch 1078 : 0.02308559976518154\n",
      "Training loss for batch 1079 : 0.023432975634932518\n",
      "Training loss for batch 1080 : 0.28910312056541443\n",
      "Training loss for batch 1081 : 0.11427831649780273\n",
      "Training loss for batch 1082 : 0.05727747082710266\n",
      "Training loss for batch 1083 : 0.11153881996870041\n",
      "Training loss for batch 1084 : 0.11234533786773682\n",
      "Training loss for batch 1085 : 0.1350637525320053\n",
      "Training loss for batch 1086 : 0.12527690827846527\n",
      "Training loss for batch 1087 : 0.3274080455303192\n",
      "Training loss for batch 1088 : 0.01709158718585968\n",
      "Training loss for batch 1089 : 0.05205283313989639\n",
      "Training loss for batch 1090 : 0.0598764568567276\n",
      "Training loss for batch 1091 : 0.0556703582406044\n",
      "Training loss for batch 1092 : 0.03077808767557144\n",
      "Training loss for batch 1093 : 0.22683799266815186\n",
      "Training loss for batch 1094 : 0.15072260797023773\n",
      "Training loss for batch 1095 : 0.21324235200881958\n",
      "Training loss for batch 1096 : 0.19483964145183563\n",
      "Training loss for batch 1097 : 0.18160425126552582\n",
      "Training loss for batch 1098 : 0.2243349254131317\n",
      "Training loss for batch 1099 : 0.06270809471607208\n",
      "Training loss for batch 1100 : 0.08371655642986298\n",
      "Training loss for batch 1101 : 0.0015080280136317015\n",
      "Training loss for batch 1102 : 0.048758335411548615\n",
      "Training loss for batch 1103 : 0.030107907950878143\n",
      "Training loss for batch 1104 : 0.1638985425233841\n",
      "Training loss for batch 1105 : 0.06194443628191948\n",
      "Training loss for batch 1106 : 0.03551773354411125\n",
      "Training loss for batch 1107 : 0.12901881337165833\n",
      "Training loss for batch 1108 : 0.029084455221891403\n",
      "Training loss for batch 1109 : 0.0404285192489624\n",
      "Training loss for batch 1110 : 0.026971658691763878\n",
      "Training loss for batch 1111 : 0.014594526961445808\n",
      "Training loss for batch 1112 : 1.4911565315856024e-08\n",
      "Training loss for batch 1113 : 0.012187101878225803\n",
      "Training loss for batch 1114 : 0.01473415456712246\n",
      "Training loss for batch 1115 : 0.019476786255836487\n",
      "Training loss for batch 1116 : 0.28873294591903687\n",
      "Training loss for batch 1117 : 0.15463712811470032\n",
      "Training loss for batch 1118 : 0.060601454228162766\n",
      "Training loss for batch 1119 : 0.23608115315437317\n",
      "Training loss for batch 1120 : 0.10960812866687775\n",
      "Training loss for batch 1121 : 0.04849638417363167\n",
      "Training loss for batch 1122 : 0.03530268371105194\n",
      "Training loss for batch 1123 : 0.011422722600400448\n",
      "Training loss for batch 1124 : 0.028961731120944023\n",
      "Training loss for batch 1125 : 0.07800544053316116\n",
      "Training loss for batch 1126 : 0.10767160356044769\n",
      "Training loss for batch 1127 : 0.20085616409778595\n",
      "Training loss for batch 1128 : 0.07299167662858963\n",
      "Training loss for batch 1129 : 0.06389492750167847\n",
      "Training loss for batch 1130 : 0.028320688754320145\n",
      "Training loss for batch 1131 : 0.0054607754573225975\n",
      "Training loss for batch 1132 : 0.08405853807926178\n",
      "Training loss for batch 1133 : 0.013656876981258392\n",
      "Training loss for batch 1134 : 0.029474884271621704\n",
      "Training loss for batch 1135 : 0.07491272687911987\n",
      "Training loss for batch 1136 : 0.030357664451003075\n",
      "Training loss for batch 1137 : 0.005894673056900501\n",
      "Training loss for batch 1138 : 0.07851213216781616\n",
      "Training loss for batch 1139 : 0.11721435189247131\n",
      "Training loss for batch 1140 : 0.11429741233587265\n",
      "Training loss for batch 1141 : 0.04670243337750435\n",
      "Training loss for batch 1142 : 0.12933889031410217\n",
      "Training loss for batch 1143 : 0.09921956062316895\n",
      "Training loss for batch 1144 : 0.1589229702949524\n",
      "Training loss for batch 1145 : 0.019890587776899338\n",
      "Training loss for batch 1146 : 0.04047006368637085\n",
      "Training loss for batch 1147 : 0.01739393174648285\n",
      "Training loss for batch 1148 : 0.029543837532401085\n",
      "Training loss for batch 1149 : 0.18969056010246277\n",
      "Training loss for batch 1150 : 0.07405641674995422\n",
      "Training loss for batch 1151 : 0.23666560649871826\n",
      "Training loss for batch 1152 : 0.06983989477157593\n",
      "Training loss for batch 1153 : 0.1129581406712532\n",
      "Training loss for batch 1154 : 0.16791632771492004\n",
      "Training loss for batch 1155 : 0.27075040340423584\n",
      "Training loss for batch 1156 : 0.11413878202438354\n",
      "Training loss for batch 1157 : 0.06112030893564224\n",
      "Training loss for batch 1158 : 0.2033967822790146\n",
      "Training loss for batch 1159 : 0.043515998870134354\n",
      "Training loss for batch 1160 : 0.04964783415198326\n",
      "Training loss for batch 1161 : 0.09072339534759521\n",
      "Training loss for batch 1162 : 0.24647049605846405\n",
      "Training loss for batch 1163 : 0.11535749584436417\n",
      "Training loss for batch 1164 : 0.025653546676039696\n",
      "Training loss for batch 1165 : 0.05174931138753891\n",
      "Training loss for batch 1166 : 0.12345538288354874\n",
      "Training loss for batch 1167 : 0.07572294771671295\n",
      "Training loss for batch 1168 : 0.021165842190384865\n",
      "Training loss for batch 1169 : 0.22132349014282227\n",
      "Training loss for batch 1170 : 0.057913463562726974\n",
      "Training loss for batch 1171 : 0.020081641152501106\n",
      "Training loss for batch 1172 : 0.03635723516345024\n",
      "Training loss for batch 1173 : 0.08727777004241943\n",
      "Training loss for batch 1174 : 0.0007921283831819892\n",
      "Training loss for batch 1175 : 0.11676035821437836\n",
      "Training loss for batch 1176 : 0.04816710576415062\n",
      "Training loss for batch 1177 : 0.26804980635643005\n",
      "Training loss for batch 1178 : 0.04579851031303406\n",
      "Training loss for batch 1179 : 0.0160007793456316\n",
      "Training loss for batch 1180 : 0.001561666140332818\n",
      "Training loss for batch 1181 : 0.14100918173789978\n",
      "Training loss for batch 1182 : 0.20384007692337036\n",
      "Training loss for batch 1183 : 0.24222175776958466\n",
      "Training loss for batch 1184 : 0.11361651122570038\n",
      "Training loss for batch 1185 : 0.025735309347510338\n",
      "Training loss for batch 1186 : 0.0\n",
      "Training loss for batch 1187 : 0.044178031384944916\n",
      "Training loss for batch 1188 : 0.05254089832305908\n",
      "Training loss for batch 1189 : 0.186659038066864\n",
      "Training loss for batch 1190 : 0.00845518708229065\n",
      "Training loss for batch 1191 : 0.039764825254678726\n",
      "Training loss for batch 1192 : 0.11198446899652481\n",
      "Training loss for batch 1193 : 0.002385927364230156\n",
      "Training loss for batch 1194 : 0.12758684158325195\n",
      "Training loss for batch 1195 : 0.06452105194330215\n",
      "Training loss for batch 1196 : 0.11459210515022278\n",
      "Training loss for batch 1197 : 0.2505589723587036\n",
      "Training loss for batch 1198 : 0.06810606271028519\n",
      "Training loss for batch 1199 : 0.10087960213422775\n",
      "Training loss for batch 1200 : 0.1454915702342987\n",
      "Training loss for batch 1201 : 0.020374981686472893\n",
      "Training loss for batch 1202 : 0.008094718679785728\n",
      "Training loss for batch 1203 : 0.12228497117757797\n",
      "Training loss for batch 1204 : 0.06843399256467819\n",
      "Training loss for batch 1205 : 0.1259395331144333\n",
      "Training loss for batch 1206 : 0.16130544245243073\n",
      "Training loss for batch 1207 : 0.10780436545610428\n",
      "Training loss for batch 1208 : 0.013896074146032333\n",
      "Training loss for batch 1209 : 0.3734418749809265\n",
      "Training loss for batch 1210 : 0.1481882780790329\n",
      "Training loss for batch 1211 : 0.006229609251022339\n",
      "Training loss for batch 1212 : 0.0771450400352478\n",
      "Training loss for batch 1213 : 0.12420184165239334\n",
      "Training loss for batch 1214 : 0.048276856541633606\n",
      "Training loss for batch 1215 : 0.09113473445177078\n",
      "Training loss for batch 1216 : 0.1534186601638794\n",
      "Training loss for batch 1217 : 0.04508865252137184\n",
      "Training loss for batch 1218 : 0.03870541229844093\n",
      "Training loss for batch 1219 : 0.06837595999240875\n",
      "Training loss for batch 1220 : 0.07364552468061447\n",
      "Training loss for batch 1221 : 0.22854198515415192\n",
      "Training loss for batch 1222 : 0.09870150685310364\n",
      "Training loss for batch 1223 : 0.06501650810241699\n",
      "Training loss for batch 1224 : 0.080632783472538\n",
      "Training loss for batch 1225 : 0.06016845256090164\n",
      "Training loss for batch 1226 : 0.00828782469034195\n",
      "Training loss for batch 1227 : 0.09668724983930588\n",
      "Training loss for batch 1228 : 0.1584390550851822\n",
      "Training loss for batch 1229 : 0.061225637793540955\n",
      "Training loss for batch 1230 : 0.0783335417509079\n",
      "Training loss for batch 1231 : 0.07008447498083115\n",
      "Training loss for batch 1232 : 0.009373844601213932\n",
      "Training loss for batch 1233 : 0.1287679374217987\n",
      "Training loss for batch 1234 : 0.07921648770570755\n",
      "Training loss for batch 1235 : 0.10889501124620438\n",
      "Training loss for batch 1236 : 0.12478956580162048\n",
      "Training loss for batch 1237 : 0.10209251195192337\n",
      "Training loss for batch 1238 : 0.030677594244480133\n",
      "Training loss for batch 1239 : 0.02674594707787037\n",
      "Training loss for batch 1240 : 0.11076182872056961\n",
      "Training loss for batch 1241 : 0.051162540912628174\n",
      "Training loss for batch 1242 : 0.025736674666404724\n",
      "Training loss for batch 1243 : 0.05066019669175148\n",
      "Training loss for batch 1244 : 0.03960170969367027\n",
      "Training loss for batch 1245 : 0.06264334917068481\n",
      "Training loss for batch 1246 : 0.159823477268219\n",
      "Training loss for batch 1247 : 0.18870361149311066\n",
      "Training loss for batch 1248 : 0.02175949700176716\n",
      "Training loss for batch 1249 : 0.11140106618404388\n",
      "Training loss for batch 1250 : 0.057212118059396744\n",
      "Training loss for batch 1251 : 0.06961093097925186\n",
      "Training loss for batch 1252 : 0.02975170873105526\n",
      "Training loss for batch 1253 : 0.06762515753507614\n",
      "Training loss for batch 1254 : 0.032674115151166916\n",
      "Training loss for batch 1255 : 0.04678119346499443\n",
      "Training loss for batch 1256 : 0.002609963295981288\n",
      "Training loss for batch 1257 : 0.10913146287202835\n",
      "Training loss for batch 1258 : 0.050373394042253494\n",
      "Training loss for batch 1259 : 0.2316165715456009\n",
      "Training loss for batch 1260 : 0.12275388091802597\n",
      "Training loss for batch 1261 : 0.15381793677806854\n",
      "Training loss for batch 1262 : 0.0\n",
      "Training loss for batch 1263 : 0.08020856976509094\n",
      "Training loss for batch 1264 : 0.10614493489265442\n",
      "Training loss for batch 1265 : 0.19255223870277405\n",
      "Training loss for batch 1266 : 0.03483150526881218\n",
      "Training loss for batch 1267 : 0.002857048064470291\n",
      "Training loss for batch 1268 : 0.16036930680274963\n",
      "Training loss for batch 1269 : 0.13562898337841034\n",
      "Training loss for batch 1270 : 0.08755837380886078\n",
      "Training loss for batch 1271 : 0.12086886912584305\n",
      "Training loss for batch 1272 : 0.27538442611694336\n",
      "Training loss for batch 1273 : 0.16198071837425232\n",
      "Training loss for batch 1274 : 0.10921662300825119\n",
      "Training loss for batch 1275 : 0.026025570929050446\n",
      "Training loss for batch 1276 : 0.11764739453792572\n",
      "Training loss for batch 1277 : 0.046576786786317825\n",
      "Training loss for batch 1278 : 0.18398045003414154\n",
      "Training loss for batch 1279 : 0.15022996068000793\n",
      "Training loss for batch 1280 : 0.0\n",
      "Training loss for batch 1281 : 0.03673215210437775\n",
      "Training loss for batch 1282 : 0.13867685198783875\n",
      "Training loss for batch 1283 : 0.11678536981344223\n",
      "Training loss for batch 1284 : 0.32807815074920654\n",
      "Training loss for batch 1285 : 0.011544373817741871\n",
      "Training loss for batch 1286 : 0.10750249028205872\n",
      "Training loss for batch 1287 : 0.04974114149808884\n",
      "Training loss for batch 1288 : 0.08556480705738068\n",
      "Training loss for batch 1289 : 0.16841880977153778\n",
      "Training loss for batch 1290 : 0.16070754826068878\n",
      "Training loss for batch 1291 : 0.060670170933008194\n",
      "Training loss for batch 1292 : 0.019812868908047676\n",
      "Training loss for batch 1293 : 0.0937516912817955\n",
      "Training loss for batch 1294 : 0.06057006120681763\n",
      "Training loss for batch 1295 : 0.07298204302787781\n",
      "Training loss for batch 1296 : 0.04026908800005913\n",
      "Training loss for batch 1297 : 0.15170951187610626\n",
      "Training loss for batch 1298 : 0.13470134139060974\n",
      "Training loss for batch 1299 : 0.11435851454734802\n",
      "Training loss for batch 1300 : 0.09972583502531052\n",
      "Training loss for batch 1301 : 0.028721319511532784\n",
      "Training loss for batch 1302 : 0.025702446699142456\n",
      "Training loss for batch 1303 : 0.16298215091228485\n",
      "Training loss for batch 1304 : 0.20899318158626556\n",
      "Training loss for batch 1305 : 0.02158614620566368\n",
      "Training loss for batch 1306 : 0.05314042419195175\n",
      "Training loss for batch 1307 : 0.03163227066397667\n",
      "Training loss for batch 1308 : 0.013679247349500656\n",
      "Training loss for batch 1309 : 0.18963798880577087\n",
      "Training loss for batch 1310 : 0.10444929450750351\n",
      "Training loss for batch 1311 : 0.062217991799116135\n",
      "Training loss for batch 1312 : 0.038130953907966614\n",
      "Training loss for batch 1313 : 0.14416660368442535\n",
      "Training loss for batch 1314 : 0.08694124966859818\n",
      "Training loss for batch 1315 : 0.09966740012168884\n",
      "Training loss for batch 1316 : 0.15979702770709991\n",
      "Training loss for batch 1317 : 0.17107030749320984\n",
      "Training loss for batch 1318 : 0.11302763223648071\n",
      "Training loss for batch 1319 : 0.09048794209957123\n",
      "Training loss for batch 1320 : 0.09065583348274231\n",
      "Training loss for batch 1321 : 0.04890929535031319\n",
      "Training loss for batch 1322 : 0.019913794472813606\n",
      "Training loss for batch 1323 : 0.11825716495513916\n",
      "Training loss for batch 1324 : 0.02809445932507515\n",
      "Training loss for batch 1325 : 0.12522396445274353\n",
      "Training loss for batch 1326 : 0.03133074939250946\n",
      "Training loss for batch 1327 : 0.14824329316616058\n",
      "Training loss for batch 1328 : 0.14579840004444122\n",
      "Training loss for batch 1329 : 0.08538395911455154\n",
      "Training loss for batch 1330 : 0.042242687195539474\n",
      "Training loss for batch 1331 : 0.07916615903377533\n",
      "Training loss for batch 1332 : 0.06345895677804947\n",
      "Training loss for batch 1333 : 0.07247093319892883\n",
      "Training loss for batch 1334 : 0.08612682670354843\n",
      "Training loss for batch 1335 : 0.04867536947131157\n",
      "Training loss for batch 1336 : 0.09233127534389496\n",
      "Training loss for batch 1337 : 0.09634570777416229\n",
      "Training loss for batch 1338 : 0.06970503181219101\n",
      "Training loss for batch 1339 : 0.15936456620693207\n",
      "Training loss for batch 1340 : 0.12528324127197266\n",
      "Training loss for batch 1341 : 0.08865541219711304\n",
      "Training loss for batch 1342 : 0.14984068274497986\n",
      "Training loss for batch 1343 : 0.10165271908044815\n",
      "Training loss for batch 1344 : 0.12154071778059006\n",
      "Training loss for batch 1345 : 0.09030856192111969\n",
      "Training loss for batch 1346 : 0.03989031910896301\n",
      "Training loss for batch 1347 : 0.12688207626342773\n",
      "Training loss for batch 1348 : 0.11752809584140778\n",
      "Training loss for batch 1349 : 0.1265861690044403\n",
      "Training loss for batch 1350 : 0.2003852128982544\n",
      "Training loss for batch 1351 : 0.052673883736133575\n",
      "Training loss for batch 1352 : 0.021981701254844666\n",
      "Training loss for batch 1353 : 0.08571454882621765\n",
      "Training loss for batch 1354 : 0.012346204370260239\n",
      "Training loss for batch 1355 : 0.056677430868148804\n",
      "Training loss for batch 1356 : 0.07570929080247879\n",
      "Training loss for batch 1357 : 0.0\n",
      "Training loss for batch 1358 : 0.030966637656092644\n",
      "Training loss for batch 1359 : 0.09335587173700333\n",
      "Training loss for batch 1360 : 0.08203394711017609\n",
      "Training loss for batch 1361 : 0.14952395856380463\n",
      "Training loss for batch 1362 : 0.04823101684451103\n",
      "Training loss for batch 1363 : 0.11703895777463913\n",
      "Training loss for batch 1364 : 0.19470594823360443\n",
      "Training loss for batch 1365 : 0.1403268277645111\n",
      "Training loss for batch 1366 : 0.18009760975837708\n",
      "Training loss for batch 1367 : 0.06491569429636002\n",
      "Training loss for batch 1368 : 0.07634034007787704\n",
      "Training loss for batch 1369 : 0.044534385204315186\n",
      "Training loss for batch 1370 : 0.07874634861946106\n",
      "Training loss for batch 1371 : 0.22361379861831665\n",
      "Training loss for batch 1372 : 0.2715011239051819\n",
      "Training loss for batch 1373 : 0.08419273793697357\n",
      "Training loss for batch 1374 : 0.2575256824493408\n",
      "Training loss for batch 1375 : 0.08238343149423599\n",
      "Training loss for batch 1376 : 0.04132908955216408\n",
      "Training loss for batch 1377 : 0.13057230412960052\n",
      "Training loss for batch 1378 : 0.012717656791210175\n",
      "Training loss for batch 1379 : 0.08286722004413605\n",
      "Training loss for batch 1380 : 0.06961911916732788\n",
      "Training loss for batch 1381 : 0.06627470254898071\n",
      "Training loss for batch 1382 : 0.13134263455867767\n",
      "Training loss for batch 1383 : 0.07133011519908905\n",
      "Training loss for batch 1384 : 0.06417743861675262\n",
      "Training loss for batch 1385 : 0.09789051115512848\n",
      "Training loss for batch 1386 : 0.10444237291812897\n",
      "Training loss for batch 1387 : 0.022676115855574608\n",
      "Training loss for batch 1388 : 0.022586816921830177\n",
      "Training loss for batch 1389 : 0.0371328741312027\n",
      "Training loss for batch 1390 : 0.05349664017558098\n",
      "Training loss for batch 1391 : 0.05838654190301895\n",
      "Training loss for batch 1392 : 0.17185825109481812\n",
      "Training loss for batch 1393 : 0.11594586074352264\n",
      "Training loss for batch 1394 : 0.018458278849720955\n",
      "Training loss for batch 1395 : 0.24315005540847778\n",
      "Training loss for batch 1396 : 0.10154052078723907\n",
      "Training loss for batch 1397 : 0.025468645617365837\n",
      "Training loss for batch 1398 : 0.023214200511574745\n",
      "Training loss for batch 1399 : 0.1191675141453743\n",
      "Training loss for batch 1400 : 0.0458117350935936\n",
      "Training loss for batch 1401 : 0.17513510584831238\n",
      "Training loss for batch 1402 : 0.09190010279417038\n",
      "Training loss for batch 1403 : 0.21086113154888153\n",
      "Training loss for batch 1404 : 0.11734879016876221\n",
      "Training loss for batch 1405 : 0.03407704830169678\n",
      "Training loss for batch 1406 : 0.19112037122249603\n",
      "Training loss for batch 1407 : 0.06746363639831543\n",
      "Training loss for batch 1408 : 0.062139544636011124\n",
      "Training loss for batch 1409 : 0.12956620752811432\n",
      "Training loss for batch 1410 : 0.025746408849954605\n",
      "Training loss for batch 1411 : 0.08590589463710785\n",
      "Training loss for batch 1412 : 0.17311836779117584\n",
      "Training loss for batch 1413 : 0.03701070696115494\n",
      "Training loss for batch 1414 : 0.13851135969161987\n",
      "Training loss for batch 1415 : 0.17074958980083466\n",
      "Training loss for batch 1416 : 0.06549853086471558\n",
      "Training loss for batch 1417 : 0.03453429415822029\n",
      "Training loss for batch 1418 : 0.08475879579782486\n",
      "Training loss for batch 1419 : 0.12926296889781952\n",
      "Training loss for batch 1420 : 0.08196753263473511\n",
      "Training loss for batch 1421 : 0.026731234043836594\n",
      "Training loss for batch 1422 : 0.02040926367044449\n",
      "Training loss for batch 1423 : 0.10050903260707855\n",
      "Training loss for batch 1424 : 0.13342544436454773\n",
      "Training loss for batch 1425 : 0.019505919888615608\n",
      "Training loss for batch 1426 : 0.06655357033014297\n",
      "Training loss for batch 1427 : 0.05679674074053764\n",
      "Training loss for batch 1428 : 0.010456664487719536\n",
      "Training loss for batch 1429 : 0.014358009211719036\n",
      "Training loss for batch 1430 : 0.14426273107528687\n",
      "Training loss for batch 1431 : 0.13885332643985748\n",
      "Training loss for batch 1432 : 0.03370576351881027\n",
      "Training loss for batch 1433 : 0.09330439567565918\n",
      "Training loss for batch 1434 : 0.12853185832500458\n",
      "Training loss for batch 1435 : 0.20248496532440186\n",
      "Training loss for batch 1436 : 0.023260734975337982\n",
      "Training loss for batch 1437 : 0.07329382747411728\n",
      "Training loss for batch 1438 : 0.023073112592101097\n",
      "Training loss for batch 1439 : 0.2081335037946701\n",
      "Training loss for batch 1440 : 0.16508789360523224\n",
      "Training loss for batch 1441 : 0.030265623703598976\n",
      "Training loss for batch 1442 : 0.1985844075679779\n",
      "Training loss for batch 1443 : 0.08159801363945007\n",
      "Training loss for batch 1444 : 0.1629902571439743\n",
      "Training loss for batch 1445 : 0.07818516343832016\n",
      "Training loss for batch 1446 : 0.09482429921627045\n",
      "Training loss for batch 1447 : 0.02436632104218006\n",
      "Training loss for batch 1448 : 0.08851699531078339\n",
      "Training loss for batch 1449 : 0.11875779926776886\n",
      "Training loss for batch 1450 : 0.01806734874844551\n",
      "Training loss for batch 1451 : 0.06229892000555992\n",
      "Training loss for batch 1452 : 0.16128447651863098\n",
      "Training loss for batch 1453 : 0.20699353516101837\n",
      "Training loss for batch 1454 : 0.061314888298511505\n",
      "Training loss for batch 1455 : 0.04902096092700958\n",
      "Training loss for batch 1456 : 0.01988459751009941\n",
      "Training loss for batch 1457 : 0.17799171805381775\n",
      "Training loss for batch 1458 : 0.029781769961118698\n",
      "Training loss for batch 1459 : 0.15718767046928406\n",
      "Training loss for batch 1460 : 0.10906701534986496\n",
      "Training loss for batch 1461 : 0.14283296465873718\n",
      "Training loss for batch 1462 : 0.08015158027410507\n",
      "Training loss for batch 1463 : 0.10522650182247162\n",
      "Training loss for batch 1464 : 0.0033599352464079857\n",
      "Training loss for batch 1465 : 0.04276842251420021\n",
      "Training loss for batch 1466 : 0.08992522209882736\n",
      "Training loss for batch 1467 : 0.10473044216632843\n",
      "Training loss for batch 1468 : 0.1488121896982193\n",
      "Training loss for batch 1469 : 0.0030182551126927137\n",
      "Training loss for batch 1470 : 0.06395280361175537\n",
      "Training loss for batch 1471 : 0.06063748151063919\n",
      "Training loss for batch 1472 : 0.07176250219345093\n",
      "Training loss for batch 1473 : 0.011151127517223358\n",
      "Training loss for batch 1474 : 0.01604745350778103\n",
      "Training loss for batch 1475 : 0.07562574744224548\n",
      "Training loss for batch 1476 : 0.06452327221632004\n",
      "Training loss for batch 1477 : 0.011253713630139828\n",
      "Training loss for batch 1478 : 0.16389398276805878\n",
      "Training loss for batch 1479 : 0.0605614110827446\n",
      "Training loss for batch 1480 : 0.019365834072232246\n",
      "Training loss for batch 1481 : 0.04310751333832741\n",
      "Training loss for batch 1482 : 1.4721154961705452e-08\n",
      "Training loss for batch 1483 : 0.04359941929578781\n",
      "Training loss for batch 1484 : 0.0377039834856987\n",
      "Training loss for batch 1485 : 0.024307627230882645\n",
      "Training loss for batch 1486 : 0.021183021366596222\n",
      "Training loss for batch 1487 : 0.034571390599012375\n",
      "Training loss for batch 1488 : 0.12054838985204697\n",
      "Training loss for batch 1489 : 0.04706854745745659\n",
      "Training loss for batch 1490 : 0.035075146704912186\n",
      "Training loss for batch 1491 : 0.21755677461624146\n",
      "Training loss for batch 1492 : 0.053579580038785934\n",
      "Training loss for batch 1493 : 0.010309654287993908\n",
      "Training loss for batch 1494 : 0.12905536592006683\n",
      "Training loss for batch 1495 : 0.11385440826416016\n",
      "Training loss for batch 1496 : 0.08578666299581528\n",
      "Training loss for batch 1497 : 0.05384131520986557\n",
      "Training loss for batch 1498 : 0.02235645055770874\n",
      "Training loss for batch 1499 : 0.12216772139072418\n",
      "Training loss for batch 1500 : 0.015559185296297073\n",
      "Training loss for batch 1501 : 0.02169935591518879\n",
      "Training loss for batch 1502 : 0.01663653925061226\n",
      "Training loss for batch 1503 : 0.07690548151731491\n",
      "Training loss for batch 1504 : 0.16725391149520874\n",
      "Training loss for batch 1505 : 0.045271310955286026\n",
      "Training loss for batch 1506 : 0.26763564348220825\n",
      "Training loss for batch 1507 : 0.10701827704906464\n",
      "Training loss for batch 1508 : 0.08291100710630417\n",
      "Training loss for batch 1509 : 0.022971289232373238\n",
      "Training loss for batch 1510 : 0.06911106407642365\n",
      "Training loss for batch 1511 : 0.2019214779138565\n",
      "Training loss for batch 1512 : 0.20096711814403534\n",
      "Training loss for batch 1513 : 0.1609087735414505\n",
      "Training loss for batch 1514 : 0.006856225430965424\n",
      "Training loss for batch 1515 : 0.23014074563980103\n",
      "Training loss for batch 1516 : 0.0854460671544075\n",
      "Training loss for batch 1517 : 0.14151763916015625\n",
      "Training loss for batch 1518 : 0.0477488748729229\n",
      "Training loss for batch 1519 : 0.014284576289355755\n",
      "Training loss for batch 1520 : 0.0032600509002804756\n",
      "Training loss for batch 1521 : 0.13285498321056366\n",
      "Training loss for batch 1522 : 0.09865306317806244\n",
      "Training loss for batch 1523 : 0.142609640955925\n",
      "Training loss for batch 1524 : 0.059920113533735275\n",
      "Training loss for batch 1525 : 0.08590833842754364\n",
      "Training loss for batch 1526 : 0.0029432475566864014\n",
      "Training loss for batch 1527 : 0.0\n",
      "Training loss for batch 1528 : 0.03044210374355316\n",
      "Training loss for batch 1529 : 0.11696959286928177\n",
      "Training loss for batch 1530 : 0.03477664291858673\n",
      "Training loss for batch 1531 : 0.02689521387219429\n",
      "Training loss for batch 1532 : 0.043295618146657944\n",
      "Training loss for batch 1533 : 0.21221716701984406\n",
      "Training loss for batch 1534 : 0.0765569806098938\n",
      "Training loss for batch 1535 : 0.13511188328266144\n",
      "Training loss for batch 1536 : 0.0\n",
      "Training loss for batch 1537 : 0.03389270603656769\n",
      "Training loss for batch 1538 : 0.020705655217170715\n",
      "Training loss for batch 1539 : 0.022806379944086075\n",
      "Training loss for batch 1540 : 0.08687711507081985\n",
      "Training loss for batch 1541 : 0.03206036239862442\n",
      "Training loss for batch 1542 : 7.286767100822544e-08\n",
      "Training loss for batch 1543 : 0.20965509116649628\n",
      "Training loss for batch 1544 : 0.29225781559944153\n",
      "Training loss for batch 1545 : 0.16498397290706635\n",
      "Training loss for batch 1546 : 0.02188251167535782\n",
      "Training loss for batch 1547 : 0.002690796507522464\n",
      "Training loss for batch 1548 : 0.10137739032506943\n",
      "Training loss for batch 1549 : 0.06468220055103302\n",
      "Training loss for batch 1550 : 0.013072339817881584\n",
      "Training loss for batch 1551 : 0.1375287026166916\n",
      "Training loss for batch 1552 : 0.10080167651176453\n",
      "Training loss for batch 1553 : 0.00680654076859355\n",
      "Training loss for batch 1554 : 0.038559481501579285\n",
      "Training loss for batch 1555 : 0.23247899115085602\n",
      "Training loss for batch 1556 : 0.16189216077327728\n",
      "Training loss for batch 1557 : 0.2840574085712433\n",
      "Training loss for batch 1558 : 0.07570292800664902\n",
      "Training loss for batch 1559 : 0.08728820830583572\n",
      "Training loss for batch 1560 : 0.11464210599660873\n",
      "Training loss for batch 1561 : 0.02520674094557762\n",
      "Training loss for batch 1562 : 0.09773807227611542\n",
      "Training loss for batch 1563 : 0.07860096544027328\n",
      "Training loss for batch 1564 : 0.10982877761125565\n",
      "Training loss for batch 1565 : 0.04679923132061958\n",
      "Training loss for batch 1566 : 0.11684499680995941\n",
      "Training loss for batch 1567 : 0.0006106075015850365\n",
      "Training loss for batch 1568 : 0.08216623961925507\n",
      "Training loss for batch 1569 : 0.09741519391536713\n",
      "Training loss for batch 1570 : 0.07185149937868118\n",
      "Training loss for batch 1571 : 0.16636890172958374\n",
      "Training loss for batch 1572 : 0.08506055921316147\n",
      "Training loss for batch 1573 : 0.10839882493019104\n",
      "Training loss for batch 1574 : 0.060079801827669144\n",
      "Training loss for batch 1575 : 0.07185114920139313\n",
      "Training loss for batch 1576 : 0.027186959981918335\n",
      "Training loss for batch 1577 : 0.18004943430423737\n",
      "Training loss for batch 1578 : 0.067467600107193\n",
      "Training loss for batch 1579 : 0.13345777988433838\n",
      "Training loss for batch 1580 : 0.14860324561595917\n",
      "Training loss for batch 1581 : 0.08245232701301575\n",
      "Training loss for batch 1582 : 0.08750059455633163\n",
      "Training loss for batch 1583 : 0.07939060032367706\n",
      "Training loss for batch 1584 : 0.17730851471424103\n",
      "Training loss for batch 1585 : 0.18705524504184723\n",
      "Training loss for batch 1586 : 0.03756735846400261\n",
      "Training loss for batch 1587 : 0.12429709732532501\n",
      "Training loss for batch 1588 : 0.11278393119573593\n",
      "Training loss for batch 1589 : 0.07754635810852051\n",
      "Training loss for batch 1590 : 0.051738593727350235\n",
      "Training loss for batch 1591 : 0.24683305621147156\n",
      "Training loss for batch 1592 : 0.1122409850358963\n",
      "Training loss for batch 1593 : 0.049742747098207474\n",
      "Training loss for batch 1594 : 0.07692039012908936\n",
      "Training loss for batch 1595 : 0.04674140363931656\n",
      "Training loss for batch 1596 : 0.0729348212480545\n",
      "Training loss for batch 1597 : 0.16010555624961853\n",
      "Training loss for batch 1598 : 0.00932872761040926\n",
      "Training loss for batch 1599 : 0.03740262612700462\n",
      "Training loss for batch 1600 : 0.020047351717948914\n",
      "Training loss for batch 1601 : 0.08709654211997986\n",
      "Training loss for batch 1602 : 0.15750420093536377\n",
      "Training loss for batch 1603 : 0.10012786090373993\n",
      "Training loss for batch 1604 : 0.05916336551308632\n",
      "Training loss for batch 1605 : 0.0479142926633358\n",
      "Training loss for batch 1606 : 0.025169342756271362\n",
      "Training loss for batch 1607 : 0.0014317474560812116\n",
      "Training loss for batch 1608 : 0.09681010246276855\n",
      "Training loss for batch 1609 : 0.0762965977191925\n",
      "Training loss for batch 1610 : 0.01656499318778515\n",
      "Training loss for batch 1611 : 0.07646971195936203\n",
      "Training loss for batch 1612 : 0.11080732196569443\n",
      "Training loss for batch 1613 : 0.06383129209280014\n",
      "Training loss for batch 1614 : 0.043422307819128036\n",
      "Training loss for batch 1615 : 0.05417875945568085\n",
      "Training loss for batch 1616 : 0.03541148081421852\n",
      "Training loss for batch 1617 : 0.22970682382583618\n",
      "Training loss for batch 1618 : 0.043681249022483826\n",
      "Training loss for batch 1619 : 0.0221837367862463\n",
      "Training loss for batch 1620 : 0.06614696234464645\n",
      "Training loss for batch 1621 : 0.09630052000284195\n",
      "Training loss for batch 1622 : 0.08525645732879639\n",
      "Training loss for batch 1623 : 0.13400216400623322\n",
      "Training loss for batch 1624 : 0.1750439554452896\n",
      "Training loss for batch 1625 : 0.10271141678094864\n",
      "Training loss for batch 1626 : 0.09934377670288086\n",
      "Training loss for batch 1627 : 0.07247745245695114\n",
      "Training loss for batch 1628 : 0.025639649480581284\n",
      "Training loss for batch 1629 : 0.09836988151073456\n",
      "Training loss for batch 1630 : 0.06649057567119598\n",
      "Training loss for batch 1631 : 0.0993691235780716\n",
      "Training loss for batch 1632 : 0.11313054710626602\n",
      "Training loss for batch 1633 : 0.07561448216438293\n",
      "Training loss for batch 1634 : 0.010715802200138569\n",
      "Training loss for batch 1635 : 0.0668979063630104\n",
      "Training loss for batch 1636 : 0.053908009082078934\n",
      "Training loss for batch 1637 : 0.024953845888376236\n",
      "Training loss for batch 1638 : 0.2071368545293808\n",
      "Training loss for batch 1639 : 0.12831911444664001\n",
      "Training loss for batch 1640 : 0.2049664705991745\n",
      "Training loss for batch 1641 : 0.13659769296646118\n",
      "Training loss for batch 1642 : 0.05682624131441116\n",
      "Training loss for batch 1643 : 0.04815422371029854\n",
      "Training loss for batch 1644 : 0.2227974683046341\n",
      "Training loss for batch 1645 : 0.04820108413696289\n",
      "Training loss for batch 1646 : 0.10941212624311447\n",
      "Training loss for batch 1647 : 0.024952132254838943\n",
      "Training loss for batch 1648 : 0.11897741258144379\n",
      "Training loss for batch 1649 : 0.08027955144643784\n",
      "Training loss for batch 1650 : 0.019277963787317276\n",
      "Training loss for batch 1651 : 0.057406436651945114\n",
      "Training loss for batch 1652 : 0.141320139169693\n",
      "Training loss for batch 1653 : 0.0907059907913208\n",
      "Training loss for batch 1654 : 0.022674424573779106\n",
      "Training loss for batch 1655 : 0.17251500487327576\n",
      "Training loss for batch 1656 : 0.12132304161787033\n",
      "Training loss for batch 1657 : 0.02953779511153698\n",
      "Training loss for batch 1658 : 0.07697255909442902\n",
      "Training loss for batch 1659 : 0.0773666724562645\n",
      "Training loss for batch 1660 : 0.03410523012280464\n",
      "Training loss for batch 1661 : 0.03769255056977272\n",
      "Training loss for batch 1662 : 0.02478868141770363\n",
      "Training loss for batch 1663 : 0.0450839102268219\n",
      "Training loss for batch 1664 : 0.1353248655796051\n",
      "Training loss for batch 1665 : 0.1953868418931961\n",
      "Training loss for batch 1666 : 0.042266156524419785\n",
      "Training loss for batch 1667 : 0.202371284365654\n",
      "Training loss for batch 1668 : 0.20107614994049072\n",
      "Training loss for batch 1669 : 0.004621964879333973\n",
      "Training loss for batch 1670 : 0.031192639842629433\n",
      "Training loss for batch 1671 : 0.05978899821639061\n",
      "Training loss for batch 1672 : 0.04205050691962242\n",
      "Training loss for batch 1673 : 0.19438041746616364\n",
      "Training loss for batch 1674 : 0.03743424266576767\n",
      "Training loss for batch 1675 : 0.1746104210615158\n",
      "Training loss for batch 1676 : 0.07852721214294434\n",
      "Training loss for batch 1677 : 0.12690524756908417\n",
      "Training loss for batch 1678 : 0.032501187175512314\n",
      "Training loss for batch 1679 : 0.12181451171636581\n",
      "Training loss for batch 1680 : 0.16129553318023682\n",
      "Training loss for batch 1681 : 0.11403112858533859\n",
      "Training loss for batch 1682 : 0.03300803154706955\n",
      "Training loss for batch 1683 : 0.12395454198122025\n",
      "Training loss for batch 1684 : 0.18688541650772095\n",
      "Training loss for batch 1685 : 0.17848485708236694\n",
      "Training loss for batch 1686 : 0.13875725865364075\n",
      "Training loss for batch 1687 : 0.012730143964290619\n",
      "Training loss for batch 1688 : 0.058458685874938965\n",
      "Training loss for batch 1689 : 0.051387231796979904\n",
      "Training loss for batch 1690 : 0.12871378660202026\n",
      "Training loss for batch 1691 : 0.157886803150177\n",
      "Training loss for batch 1692 : 0.28643664717674255\n",
      "Training loss for batch 1693 : 0.1624298244714737\n",
      "Training loss for batch 1694 : 0.07966943830251694\n",
      "Training loss for batch 1695 : 0.052240267395973206\n",
      "Training loss for batch 1696 : 0.03903096541762352\n",
      "Training loss for batch 1697 : 0.02853490598499775\n",
      "Training loss for batch 1698 : 0.06286250054836273\n",
      "Training loss for batch 1699 : 0.2472340315580368\n",
      "Training loss for batch 1700 : 0.0197940431535244\n",
      "Training loss for batch 1701 : 0.0005214264383539557\n",
      "Training loss for batch 1702 : 0.07744099944829941\n",
      "Training loss for batch 1703 : 0.10244201123714447\n",
      "Training loss for batch 1704 : 0.06572888791561127\n",
      "Training loss for batch 1705 : 0.23280781507492065\n",
      "Training loss for batch 1706 : 0.15127591788768768\n",
      "Training loss for batch 1707 : 0.16642653942108154\n",
      "Training loss for batch 1708 : 0.15489189326763153\n",
      "Training loss for batch 1709 : 0.11274700611829758\n",
      "Training loss for batch 1710 : 0.06988293677568436\n",
      "Training loss for batch 1711 : 0.03185031935572624\n",
      "Training loss for batch 1712 : 0.20304040610790253\n",
      "Training loss for batch 1713 : 0.17101186513900757\n",
      "Training loss for batch 1714 : 0.12455043941736221\n",
      "Training loss for batch 1715 : 0.19086876511573792\n",
      "Training loss for batch 1716 : 0.09233929216861725\n",
      "Training loss for batch 1717 : 0.0881267711520195\n",
      "Training loss for batch 1718 : 0.016185125336050987\n",
      "Training loss for batch 1719 : 0.21080806851387024\n",
      "Training loss for batch 1720 : 0.03794398158788681\n",
      "Training loss for batch 1721 : 0.02187473140656948\n",
      "Training loss for batch 1722 : 0.25110965967178345\n",
      "Training loss for batch 1723 : 0.039509713649749756\n",
      "Training loss for batch 1724 : 0.0860758051276207\n",
      "Training loss for batch 1725 : 0.10075197368860245\n",
      "Training loss for batch 1726 : 0.09103444218635559\n",
      "Training loss for batch 1727 : 0.037532951682806015\n",
      "Training loss for batch 1728 : 0.04313338175415993\n",
      "Training loss for batch 1729 : 0.05502445995807648\n",
      "Training loss for batch 1730 : 0.03120691142976284\n",
      "Training loss for batch 1731 : 0.013803316280245781\n",
      "Training loss for batch 1732 : 0.1061868965625763\n",
      "Training loss for batch 1733 : 0.006160323973745108\n",
      "Training loss for batch 1734 : 0.07388656586408615\n",
      "Training loss for batch 1735 : 0.05355935916304588\n",
      "Training loss for batch 1736 : 0.01304077822715044\n",
      "Training loss for batch 1737 : 0.048799797892570496\n",
      "Training loss for batch 1738 : 0.1707933396100998\n",
      "Training loss for batch 1739 : 0.12533588707447052\n",
      "Training loss for batch 1740 : 0.3152737021446228\n",
      "Training loss for batch 1741 : 0.012843464501202106\n",
      "Training loss for batch 1742 : 0.11653492599725723\n",
      "Training loss for batch 1743 : 0.07417308539152145\n",
      "Training loss for batch 1744 : 0.06893882155418396\n",
      "Training loss for batch 1745 : 0.13655616343021393\n",
      "Training loss for batch 1746 : 0.018814396113157272\n",
      "Training loss for batch 1747 : 0.0583006851375103\n",
      "Training loss for batch 1748 : 0.13899897038936615\n",
      "Training loss for batch 1749 : 0.1726706624031067\n",
      "Training loss for batch 1750 : 0.08273222297430038\n",
      "Training loss for batch 1751 : 0.03571803495287895\n",
      "Training loss for batch 1752 : 0.007397410925477743\n",
      "Training loss for batch 1753 : 0.03828758746385574\n",
      "Training loss for batch 1754 : 0.25608575344085693\n",
      "Training loss for batch 1755 : 0.17063415050506592\n",
      "Training loss for batch 1756 : 0.15846280753612518\n",
      "Training loss for batch 1757 : 0.14628654718399048\n",
      "Training loss for batch 1758 : 0.09062252938747406\n",
      "Training loss for batch 1759 : 0.15377643704414368\n",
      "Training loss for batch 1760 : 0.0006308213341981173\n",
      "Training loss for batch 1761 : 0.011399464681744576\n",
      "Training loss for batch 1762 : 0.028470413759350777\n",
      "Training loss for batch 1763 : 0.2232551872730255\n",
      "Training loss for batch 1764 : 0.08898904174566269\n",
      "Training loss for batch 1765 : 0.08703070878982544\n",
      "Training loss for batch 1766 : 0.14465360343456268\n",
      "Training loss for batch 1767 : 0.10140153020620346\n",
      "Training loss for batch 1768 : 0.16311699151992798\n",
      "Training loss for batch 1769 : 0.0988975316286087\n",
      "Training loss for batch 1770 : 0.16644030809402466\n",
      "Training loss for batch 1771 : 0.07172152400016785\n",
      "Training loss for batch 1772 : 0.06141000986099243\n",
      "Training loss for batch 1773 : 0.017002638429403305\n",
      "Training loss for batch 1774 : 0.052938543260097504\n",
      "Training loss for batch 1775 : 0.051574937999248505\n",
      "Training loss for batch 1776 : 0.1622639298439026\n",
      "Training loss for batch 1777 : 0.12200799584388733\n",
      "Training loss for batch 1778 : 8.983401578177563e-09\n",
      "Training loss for batch 1779 : 0.022524971514940262\n",
      "Training loss for batch 1780 : 0.13471053540706635\n",
      "Training loss for batch 1781 : 0.03523997589945793\n",
      "Training loss for batch 1782 : 0.046754080802202225\n",
      "Training loss for batch 1783 : 0.01411360688507557\n",
      "Training loss for batch 1784 : 0.1850501149892807\n",
      "Training loss for batch 1785 : 0.10588539391756058\n",
      "Training loss for batch 1786 : 0.1511930227279663\n",
      "Training loss for batch 1787 : 0.049689628183841705\n",
      "Training loss for batch 1788 : 0.08537331968545914\n",
      "Training loss for batch 1789 : 0.058727022260427475\n",
      "Training loss for batch 1790 : 0.03931557759642601\n",
      "Training loss for batch 1791 : 0.019175341352820396\n",
      "Training loss for batch 1792 : 0.23448942601680756\n",
      "Training loss for batch 1793 : 0.04582112282514572\n",
      "Training loss for batch 1794 : 0.05628946051001549\n",
      "Training loss for batch 1795 : 0.11077096313238144\n",
      "Training loss for batch 1796 : 0.029132921248674393\n",
      "Training loss for batch 1797 : 0.025640785694122314\n",
      "Training loss for batch 1798 : 0.11862893402576447\n",
      "Training loss for batch 1799 : 0.024733632802963257\n",
      "Training loss for batch 1800 : 0.07385295629501343\n",
      "Training loss for batch 1801 : 0.10502278059720993\n",
      "Training loss for batch 1802 : 0.0754832923412323\n",
      "Training loss for batch 1803 : 0.07155205309391022\n",
      "Training loss for batch 1804 : 0.15176448225975037\n",
      "Training loss for batch 1805 : 0.030462540686130524\n",
      "Training loss for batch 1806 : 0.13579998910427094\n",
      "Training loss for batch 1807 : 0.028021128848195076\n",
      "Training loss for batch 1808 : 0.10855518281459808\n",
      "Training loss for batch 1809 : 0.1338413953781128\n",
      "Training loss for batch 1810 : 0.2094191610813141\n",
      "Training loss for batch 1811 : 0.1986723095178604\n",
      "Training loss for batch 1812 : 0.18149171769618988\n",
      "Training loss for batch 1813 : 0.2552483081817627\n",
      "Training loss for batch 1814 : 0.4035889506340027\n",
      "Training loss for batch 1815 : 0.16380883753299713\n",
      "Training loss for batch 1816 : 0.10244350135326385\n",
      "Training loss for batch 1817 : 0.10631164908409119\n",
      "Training loss for batch 1818 : 0.039471715688705444\n",
      "Training loss for batch 1819 : 0.07685412466526031\n",
      "Training loss for batch 1820 : 0.03384784981608391\n",
      "Training loss for batch 1821 : 0.013291716575622559\n",
      "Training loss for batch 1822 : 0.10219448804855347\n",
      "Training loss for batch 1823 : 0.045335832983255386\n",
      "Training loss for batch 1824 : 0.06387636810541153\n",
      "Training loss for batch 1825 : 0.1742076724767685\n",
      "Training loss for batch 1826 : 0.0851534828543663\n",
      "Training loss for batch 1827 : 0.03290340676903725\n",
      "Training loss for batch 1828 : 0.16660700738430023\n",
      "Training loss for batch 1829 : 0.06520044058561325\n",
      "Training loss for batch 1830 : 0.022942641749978065\n",
      "Training loss for batch 1831 : 0.023688562214374542\n",
      "Training loss for batch 1832 : 0.11587586253881454\n",
      "Training loss for batch 1833 : 0.02430328167974949\n",
      "Training loss for batch 1834 : 0.017734240740537643\n",
      "Training loss for batch 1835 : 0.11270876228809357\n",
      "Training loss for batch 1836 : 0.07021456956863403\n",
      "Training loss for batch 1837 : 0.07836370915174484\n",
      "Training loss for batch 1838 : 0.08917385339736938\n",
      "Training loss for batch 1839 : 0.06322155892848969\n",
      "Training loss for batch 1840 : 0.08613143116235733\n",
      "Training loss for batch 1841 : 0.056002795696258545\n",
      "Training loss for batch 1842 : 0.05674579739570618\n",
      "Training loss for batch 1843 : 0.08918800204992294\n",
      "Training loss for batch 1844 : 0.18838827311992645\n",
      "Training loss for batch 1845 : 0.014959292486310005\n",
      "Training loss for batch 1846 : 0.023559002205729485\n",
      "Training loss for batch 1847 : 0.030826354399323463\n",
      "Training loss for batch 1848 : 0.08397389948368073\n",
      "Training loss for batch 1849 : 0.3040076494216919\n",
      "Training loss for batch 1850 : 0.053986188024282455\n",
      "Training loss for batch 1851 : 0.051875703036785126\n",
      "Training loss for batch 1852 : 0.06373847275972366\n",
      "Training loss for batch 1853 : 0.1945265680551529\n",
      "Training loss for batch 1854 : 0.05054091662168503\n",
      "Training loss for batch 1855 : 0.10329385846853256\n",
      "Training loss for batch 1856 : 0.17804613709449768\n",
      "Training loss for batch 1857 : 0.024173252284526825\n",
      "Training loss for batch 1858 : 0.10135574638843536\n",
      "Training loss for batch 1859 : 0.02371743507683277\n",
      "Training loss for batch 1860 : 0.04756590351462364\n",
      "Training loss for batch 1861 : 0.12285753339529037\n",
      "Training loss for batch 1862 : 0.0690680667757988\n",
      "Training loss for batch 1863 : 0.18036332726478577\n",
      "Training loss for batch 1864 : 0.02484540082514286\n",
      "Training loss for batch 1865 : 0.19610512256622314\n",
      "Training loss for batch 1866 : 0.07726342976093292\n",
      "Training loss for batch 1867 : 0.05121597275137901\n",
      "Training loss for batch 1868 : 3.3711046398821054e-08\n",
      "Training loss for batch 1869 : 0.10144945234060287\n",
      "Training loss for batch 1870 : 0.10139691084623337\n",
      "Training loss for batch 1871 : 0.09979337453842163\n",
      "Training loss for batch 1872 : 0.13447274267673492\n",
      "Training loss for batch 1873 : 0.045320428907871246\n",
      "Training loss for batch 1874 : 0.05365057662129402\n",
      "Training loss for batch 1875 : 0.036193788051605225\n",
      "Training loss for batch 1876 : 0.16780883073806763\n",
      "Training loss for batch 1877 : 0.19763189554214478\n",
      "Training loss for batch 1878 : 0.09633427113294601\n",
      "Training loss for batch 1879 : 0.16757221519947052\n",
      "Training loss for batch 1880 : 0.20278528332710266\n",
      "Training loss for batch 1881 : 0.1903323531150818\n",
      "Training loss for batch 1882 : 0.21869900822639465\n",
      "Training loss for batch 1883 : 0.045888543128967285\n",
      "Training loss for batch 1884 : 0.0031414006371051073\n",
      "Training loss for batch 1885 : 0.16746081411838531\n",
      "Training loss for batch 1886 : 0.15105436742305756\n",
      "Training loss for batch 1887 : 0.3274513781070709\n",
      "Training loss for batch 1888 : 0.2596488893032074\n",
      "Training loss for batch 1889 : 0.0973561629652977\n",
      "Training loss for batch 1890 : 0.009148397482931614\n",
      "Training loss for batch 1891 : 0.08856965601444244\n",
      "Training loss for batch 1892 : 0.06092073395848274\n",
      "Training loss for batch 1893 : 0.1306205689907074\n",
      "Training loss for batch 1894 : 0.017759550362825394\n",
      "Training loss for batch 1895 : 0.09419730305671692\n",
      "Training loss for batch 1896 : 0.10601984709501266\n",
      "Training loss for batch 1897 : 0.011859056539833546\n",
      "Training loss for batch 1898 : 0.04229060187935829\n",
      "Training loss for batch 1899 : 0.10224296897649765\n",
      "Training loss for batch 1900 : 0.019695375114679337\n",
      "Training loss for batch 1901 : 0.054114311933517456\n",
      "Training loss for batch 1902 : 0.21469241380691528\n",
      "Training loss for batch 1903 : 0.14960643649101257\n",
      "Training loss for batch 1904 : 0.11505180597305298\n",
      "Training loss for batch 1905 : 0.028681619092822075\n",
      "Training loss for batch 1906 : 0.25444114208221436\n",
      "Training loss for batch 1907 : 0.07743953913450241\n",
      "Training loss for batch 1908 : 0.1111021488904953\n",
      "Training loss for batch 1909 : 0.1157345101237297\n",
      "Training loss for batch 1910 : 0.038417037576436996\n",
      "Training loss for batch 1911 : 0.04277980700135231\n",
      "Training loss for batch 1912 : 0.024314861744642258\n",
      "Training loss for batch 1913 : 0.04210235923528671\n",
      "Training loss for batch 1914 : 0.08601891249418259\n",
      "Training loss for batch 1915 : 0.03967023268342018\n",
      "Training loss for batch 1916 : 0.08896134048700333\n",
      "Training loss for batch 1917 : 0.055235665291547775\n",
      "Training loss for batch 1918 : 0.09429819136857986\n",
      "Training loss for batch 1919 : 0.07184845209121704\n",
      "Training loss for batch 1920 : 0.07988544553518295\n",
      "Training loss for batch 1921 : 0.04908702149987221\n",
      "Training loss for batch 1922 : 0.0476105771958828\n",
      "Training loss for batch 1923 : 0.1974046379327774\n",
      "Training loss for batch 1924 : 0.13419966399669647\n",
      "Training loss for batch 1925 : 0.12273778766393661\n",
      "Training loss for batch 1926 : 0.03982557728886604\n",
      "Training loss for batch 1927 : 0.06207795441150665\n",
      "Training loss for batch 1928 : 0.11619586497545242\n",
      "Training loss for batch 1929 : 0.08339343965053558\n",
      "Training loss for batch 1930 : 0.08991120010614395\n",
      "Training loss for batch 1931 : 0.11594589054584503\n",
      "Training loss for batch 1932 : 0.03010060079395771\n",
      "Training loss for batch 1933 : 0.02219046838581562\n",
      "Training loss for batch 1934 : 0.2545161545276642\n",
      "Training loss for batch 1935 : 0.03833875432610512\n",
      "Training loss for batch 1936 : 0.16133709251880646\n",
      "Training loss for batch 1937 : 0.02289280667901039\n",
      "Training loss for batch 1938 : 0.05815209075808525\n",
      "Training loss for batch 1939 : 0.005174264311790466\n",
      "Training loss for batch 1940 : 0.194094717502594\n",
      "Training loss for batch 1941 : 0.050166767090559006\n",
      "Training loss for batch 1942 : 0.10283326357603073\n",
      "Training loss for batch 1943 : 0.00019224866991862655\n",
      "Training loss for batch 1944 : 0.09039893001317978\n",
      "Training loss for batch 1945 : 0.07702051848173141\n",
      "Training loss for batch 1946 : 0.00768475653603673\n",
      "Training loss for batch 1947 : 0.041786931455135345\n",
      "Training loss for batch 1948 : 0.09141895920038223\n",
      "Training loss for batch 1949 : 0.11488087475299835\n",
      "Training loss for batch 1950 : 0.07323698699474335\n",
      "Training loss for batch 1951 : 0.15288321673870087\n",
      "Training loss for batch 1952 : 0.08005248755216599\n",
      "Training loss for batch 1953 : 0.09708064794540405\n",
      "Training loss for batch 1954 : 0.02351664938032627\n",
      "Training loss for batch 1955 : 0.27398616075515747\n",
      "Training loss for batch 1956 : 0.07726337760686874\n",
      "Training loss for batch 1957 : 0.08596790581941605\n",
      "Training loss for batch 1958 : 0.09706609696149826\n",
      "Training loss for batch 1959 : 0.1861841082572937\n",
      "Training loss for batch 1960 : 0.0727839320898056\n",
      "Training loss for batch 1961 : 0.0879247710108757\n",
      "Training loss for batch 1962 : 0.04004231467843056\n",
      "Training loss for batch 1963 : 0.171820268034935\n",
      "Training loss for batch 1964 : 0.055483683943748474\n",
      "Training loss for batch 1965 : 0.012560147792100906\n",
      "Training loss for batch 1966 : 0.15116795897483826\n",
      "Training loss for batch 1967 : 0.0653407946228981\n",
      "Training loss for batch 1968 : 0.10020150989294052\n",
      "Training loss for batch 1969 : 0.06129799038171768\n",
      "Training loss for batch 1970 : 0.046014249324798584\n",
      "Training loss for batch 1971 : 0.05047760531306267\n",
      "Training loss for batch 1972 : 0.24875974655151367\n",
      "Training loss for batch 1973 : 0.04735451936721802\n",
      "Training loss for batch 1974 : 0.06400839239358902\n",
      "Training loss for batch 1975 : 0.11046194285154343\n",
      "Training loss for batch 1976 : 0.20435664057731628\n",
      "Training loss for batch 1977 : 0.09147286415100098\n",
      "Training loss for batch 1978 : 0.36114415526390076\n",
      "Training loss for batch 1979 : 0.1436225324869156\n",
      "Training loss for batch 1980 : 0.0697425901889801\n",
      "Training loss for batch 1981 : 0.026954734697937965\n",
      "Training loss for batch 1982 : 0.1468130499124527\n",
      "Training loss for batch 1983 : 0.04900714010000229\n",
      "Training loss for batch 1984 : 0.0413103848695755\n",
      "Training loss for batch 1985 : 0.1041145920753479\n",
      "Training loss for batch 1986 : 0.03053748793900013\n",
      "Training loss for batch 1987 : 0.060277316719293594\n",
      "Training loss for batch 1988 : 0.04905490204691887\n",
      "Training loss for batch 1989 : 0.03166498243808746\n",
      "Training loss for batch 1990 : 0.03302233666181564\n",
      "Training loss for batch 1991 : 0.09361555427312851\n",
      "Training loss for batch 1992 : 0.17945167422294617\n",
      "Training loss for batch 1993 : 0.13501329720020294\n",
      "Training loss for batch 1994 : 0.05055708810687065\n",
      "Training loss for batch 1995 : 0.17707791924476624\n",
      "Training loss for batch 1996 : 0.08681424707174301\n",
      "Training loss for batch 1997 : 0.12870962917804718\n",
      "Training loss for batch 1998 : 0.01214674487709999\n",
      "Training loss for batch 1999 : 0.22087500989437103\n",
      "Training loss for batch 2000 : 0.13030104339122772\n",
      "Training loss for batch 2001 : 0.0\n",
      "Training loss for batch 2002 : 0.08186102658510208\n",
      "Training loss for batch 2003 : 0.061260927468538284\n",
      "Training loss for batch 2004 : 0.05742964148521423\n",
      "Training loss for batch 2005 : 0.08556553721427917\n",
      "Training loss for batch 2006 : 0.07154116034507751\n",
      "Training loss for batch 2007 : 0.1516195684671402\n",
      "Training loss for batch 2008 : 0.06733446568250656\n",
      "Training loss for batch 2009 : 0.09392949938774109\n",
      "Training loss for batch 2010 : 0.05989118292927742\n",
      "Training loss for batch 2011 : 0.11565456539392471\n",
      "Training loss for batch 2012 : 0.0014858582289889455\n",
      "Training loss for batch 2013 : 0.10386619716882706\n",
      "Training loss for batch 2014 : 0.1916881799697876\n",
      "Training loss for batch 2015 : 0.17034786939620972\n",
      "Training loss for batch 2016 : 0.10591947287321091\n",
      "Training loss for batch 2017 : 0.1062578335404396\n",
      "Training loss for batch 2018 : 0.0524369515478611\n",
      "Training loss for batch 2019 : 0.1492612510919571\n",
      "Training loss for batch 2020 : 0.11072807759046555\n",
      "Training loss for batch 2021 : 0.04633476212620735\n",
      "Training loss for batch 2022 : 0.06391126662492752\n",
      "Training loss for batch 2023 : 0.1491849273443222\n",
      "Training loss for batch 2024 : 0.07299011200666428\n",
      "Training loss for batch 2025 : 0.14857110381126404\n",
      "Training loss for batch 2026 : 0.17387591302394867\n",
      "Training loss for batch 2027 : 0.08086637407541275\n",
      "Training loss for batch 2028 : 0.12716607749462128\n",
      "Training loss for batch 2029 : 0.1333369016647339\n",
      "Training loss for batch 2030 : 0.040477391332387924\n",
      "Training loss for batch 2031 : 0.022895358502864838\n",
      "Training loss for batch 2032 : 0.08082034438848495\n",
      "Training loss for batch 2033 : 0.01922987774014473\n",
      "Training loss for batch 2034 : 0.14123333990573883\n",
      "Training loss for batch 2035 : 0.007580666337162256\n",
      "Training loss for batch 2036 : 0.09083051234483719\n",
      "Training loss for batch 2037 : 0.09332265704870224\n",
      "Training loss for batch 2038 : 0.0236168522387743\n",
      "Training loss for batch 2039 : 0.17188529670238495\n",
      "Training loss for batch 2040 : 0.04392528533935547\n",
      "Training loss for batch 2041 : 0.11941126734018326\n",
      "Training loss for batch 2042 : 0.05938390642404556\n",
      "Training loss for batch 2043 : 0.0\n",
      "Training loss for batch 2044 : 0.08065111935138702\n",
      "Training loss for batch 2045 : 0.15121571719646454\n",
      "Training loss for batch 2046 : 0.09991932660341263\n",
      "Training loss for batch 2047 : 0.042740512639284134\n",
      "Training loss for batch 2048 : 0.0012682527303695679\n",
      "Training loss for batch 2049 : 0.12390502542257309\n",
      "Training loss for batch 2050 : 0.053226687014102936\n",
      "Training loss for batch 2051 : 0.14814434945583344\n",
      "Training loss for batch 2052 : 0.1088879257440567\n",
      "Training loss for batch 2053 : 0.18359389901161194\n",
      "Training loss for batch 2054 : 0.12284082174301147\n",
      "Training loss for batch 2055 : 0.01803225837647915\n",
      "Training loss for batch 2056 : 0.0006418377161026001\n",
      "Training loss for batch 2057 : 0.03619344159960747\n",
      "Training loss for batch 2058 : 0.19822783768177032\n",
      "Training loss for batch 2059 : 0.046045269817113876\n",
      "Training loss for batch 2060 : 0.21150526404380798\n",
      "Training loss for batch 2061 : 0.002686287509277463\n",
      "Training loss for batch 2062 : 0.26952481269836426\n",
      "Training loss for batch 2063 : 0.05071194842457771\n",
      "Training loss for batch 2064 : 0.1233556792140007\n",
      "Training loss for batch 2065 : 0.07158120721578598\n",
      "Training loss for batch 2066 : 0.16258947551250458\n",
      "Training loss for batch 2067 : 0.047394540160894394\n",
      "Training loss for batch 2068 : 0.062072183936834335\n",
      "Training loss for batch 2069 : 0.09465449303388596\n",
      "Training loss for batch 2070 : 0.07598049193620682\n",
      "Training loss for batch 2071 : 0.06667863577604294\n",
      "Training loss for batch 2072 : 0.029111871495842934\n",
      "Training loss for batch 2073 : 0.13576404750347137\n",
      "Training loss for batch 2074 : 0.0743584930896759\n",
      "Training loss for batch 2075 : 0.1569632589817047\n",
      "Training loss for batch 2076 : 0.2858959436416626\n",
      "Training loss for batch 2077 : 0.13298195600509644\n",
      "Training loss for batch 2078 : 0.13077762722969055\n",
      "Training loss for batch 2079 : 0.016291314736008644\n",
      "Training loss for batch 2080 : 0.15846484899520874\n",
      "Training loss for batch 2081 : 0.03339315950870514\n",
      "Training loss for batch 2082 : 0.22287365794181824\n",
      "Training loss for batch 2083 : 0.09648477286100388\n",
      "Training loss for batch 2084 : 0.0029943338595330715\n",
      "Training loss for batch 2085 : 0.12830200791358948\n",
      "Training loss for batch 2086 : 0.2276129424571991\n",
      "Training loss for batch 2087 : 0.04082970693707466\n",
      "Training loss for batch 2088 : 0.11016895622015\n",
      "Training loss for batch 2089 : 0.2535637617111206\n",
      "Training loss for batch 2090 : 0.09110762923955917\n",
      "Training loss for batch 2091 : 0.10226555913686752\n",
      "Training loss for batch 2092 : 0.11701738089323044\n",
      "Training loss for batch 2093 : 0.016607725992798805\n",
      "Training loss for batch 2094 : 0.16512277722358704\n",
      "Training loss for batch 2095 : 0.020211827009916306\n",
      "Training loss for batch 2096 : 0.1756410449743271\n",
      "Training loss for batch 2097 : 0.14240357279777527\n",
      "Training loss for batch 2098 : 0.05080782249569893\n",
      "Training loss for batch 2099 : 0.09861160814762115\n",
      "Training loss for batch 2100 : 0.05584004521369934\n",
      "Training loss for batch 2101 : 0.11882364004850388\n",
      "Training loss for batch 2102 : 0.05794006958603859\n",
      "Training loss for batch 2103 : 0.0280850101262331\n",
      "Training loss for batch 2104 : 0.22206246852874756\n",
      "Training loss for batch 2105 : 0.14546865224838257\n",
      "Training loss for batch 2106 : 0.21802476048469543\n",
      "Training loss for batch 2107 : 0.1785075068473816\n",
      "Training loss for batch 2108 : 0.2943565845489502\n",
      "Training loss for batch 2109 : 0.09001363813877106\n",
      "Training loss for batch 2110 : 0.025423474609851837\n",
      "Training loss for batch 2111 : 0.05109342932701111\n",
      "Training loss for batch 2112 : 0.006769274361431599\n",
      "Training loss for batch 2113 : 0.0729985162615776\n",
      "Training loss for batch 2114 : 0.1739146113395691\n",
      "Training loss for batch 2115 : 0.03562692180275917\n",
      "Training loss for batch 2116 : 0.17737065255641937\n",
      "Training loss for batch 2117 : 0.0331907644867897\n",
      "Training loss for batch 2118 : 0.1661839783191681\n",
      "Training loss for batch 2119 : 0.14698393642902374\n",
      "Training loss for batch 2120 : 0.11344800144433975\n",
      "Training loss for batch 2121 : 0.0738801583647728\n",
      "Training loss for batch 2122 : 0.025102872401475906\n",
      "Training loss for batch 2123 : 0.19750802218914032\n",
      "Training loss for batch 2124 : 0.1200549304485321\n",
      "Training loss for batch 2125 : 0.05757756531238556\n",
      "Training loss for batch 2126 : 0.1453762948513031\n",
      "Training loss for batch 2127 : 0.07905730605125427\n",
      "Training loss for batch 2128 : 0.17266975343227386\n",
      "Training loss for batch 2129 : 0.08911759406328201\n",
      "Training loss for batch 2130 : 0.04635551571846008\n",
      "Training loss for batch 2131 : 0.20339012145996094\n",
      "Training loss for batch 2132 : 0.10883086919784546\n",
      "Training loss for batch 2133 : 0.05896158888936043\n",
      "Training loss for batch 2134 : 0.04937482997775078\n",
      "Training loss for batch 2135 : 0.10187523812055588\n",
      "Training loss for batch 2136 : 0.129034161567688\n",
      "Training loss for batch 2137 : 0.0036594318225979805\n",
      "Training loss for batch 2138 : 0.1267375499010086\n",
      "Training loss for batch 2139 : 0.051723409444093704\n",
      "Training loss for batch 2140 : 0.10983029007911682\n",
      "Training loss for batch 2141 : 0.052266258746385574\n",
      "Training loss for batch 2142 : 0.04858558624982834\n",
      "Training loss for batch 2143 : 0.028464902192354202\n",
      "Training loss for batch 2144 : 0.04760543629527092\n",
      "Training loss for batch 2145 : 0.16125856339931488\n",
      "Training loss for batch 2146 : 0.06079733744263649\n",
      "Training loss for batch 2147 : 0.04534060135483742\n",
      "Training loss for batch 2148 : 0.0513632670044899\n",
      "Training loss for batch 2149 : 0.16111692786216736\n",
      "Training loss for batch 2150 : 0.05456243455410004\n",
      "Training loss for batch 2151 : 0.09276126325130463\n",
      "Training loss for batch 2152 : 0.06702592968940735\n",
      "Training loss for batch 2153 : 0.12794099748134613\n",
      "Training loss for batch 2154 : 0.14114752411842346\n",
      "Training loss for batch 2155 : 0.21717189252376556\n",
      "Training loss for batch 2156 : 0.027117004618048668\n",
      "Training loss for batch 2157 : 0.23887057602405548\n",
      "Training loss for batch 2158 : 0.0888204574584961\n",
      "Training loss for batch 2159 : 0.0652233436703682\n",
      "Training loss for batch 2160 : 0.06056276336312294\n",
      "Training loss for batch 2161 : 0.030499953776597977\n",
      "Training loss for batch 2162 : 0.043279316276311874\n",
      "Training loss for batch 2163 : 0.08249863982200623\n",
      "Training loss for batch 2164 : 0.053044192492961884\n",
      "Training loss for batch 2165 : 0.04163753613829613\n",
      "Training loss for batch 2166 : 0.04117660969495773\n",
      "Training loss for batch 2167 : 0.15606731176376343\n",
      "Training loss for batch 2168 : 0.15890106558799744\n",
      "Training loss for batch 2169 : 0.1272713541984558\n",
      "Training loss for batch 2170 : 0.13390105962753296\n",
      "Training loss for batch 2171 : 0.007166081108152866\n",
      "Training loss for batch 2172 : 0.06062300503253937\n",
      "Training loss for batch 2173 : 0.13764835894107819\n",
      "Training loss for batch 2174 : 0.1566227227449417\n",
      "Training loss for batch 2175 : 0.18515709042549133\n",
      "Training loss for batch 2176 : 0.002644573338329792\n",
      "Training loss for batch 2177 : 0.14108526706695557\n",
      "Training loss for batch 2178 : 0.05575382336974144\n",
      "Training loss for batch 2179 : 0.05038268119096756\n",
      "Training loss for batch 2180 : 0.12034457921981812\n",
      "Training loss for batch 2181 : 0.047918885946273804\n",
      "Training loss for batch 2182 : 0.09037374705076218\n",
      "Training loss for batch 2183 : 0.2891993224620819\n",
      "Training loss for batch 2184 : 0.08931808173656464\n",
      "Training loss for batch 2185 : 0.03912600129842758\n",
      "Training loss for batch 2186 : 0.1463591605424881\n",
      "Training loss for batch 2187 : 0.02624882012605667\n",
      "Training loss for batch 2188 : 0.12688526511192322\n",
      "Training loss for batch 2189 : 0.06941070407629013\n",
      "Training loss for batch 2190 : 0.03779226914048195\n",
      "Training loss for batch 2191 : 0.15531450510025024\n",
      "Training loss for batch 2192 : 0.12673434615135193\n",
      "Training loss for batch 2193 : 0.11709126085042953\n",
      "Training loss for batch 2194 : 0.19365374743938446\n",
      "Training loss for batch 2195 : 0.03498798981308937\n",
      "Training loss for batch 2196 : 0.08466514945030212\n",
      "Training loss for batch 2197 : 0.049248430877923965\n",
      "Training loss for batch 2198 : 0.15066607296466827\n",
      "Training loss for batch 2199 : 0.1097189337015152\n",
      "Training loss for batch 2200 : 0.0850827693939209\n",
      "Training loss for batch 2201 : 0.22178184986114502\n",
      "Training loss for batch 2202 : 0.13199745118618011\n",
      "Training loss for batch 2203 : 0.18521648645401\n",
      "Training loss for batch 2204 : 0.21703240275382996\n",
      "Training loss for batch 2205 : 0.12003125250339508\n",
      "Training loss for batch 2206 : 0.07265667617321014\n",
      "Training loss for batch 2207 : 0.1722976118326187\n",
      "Training loss for batch 2208 : 0.07258041203022003\n",
      "Training loss for batch 2209 : 0.020367968827486038\n",
      "Training loss for batch 2210 : 0.03790464624762535\n",
      "Training loss for batch 2211 : 0.1963283270597458\n",
      "Training loss for batch 2212 : 0.08642590045928955\n",
      "Training loss for batch 2213 : 0.25365278124809265\n",
      "Training loss for batch 2214 : 0.05518575757741928\n",
      "Training loss for batch 2215 : 0.056284137070178986\n",
      "Training loss for batch 2216 : 0.07390522956848145\n",
      "Training loss for batch 2217 : 0.24476851522922516\n",
      "Training loss for batch 2218 : 0.1966506391763687\n",
      "Training loss for batch 2219 : 0.11918925493955612\n",
      "Training loss for batch 2220 : 0.05422614514827728\n",
      "Training loss for batch 2221 : 0.012303183786571026\n",
      "Training loss for batch 2222 : 0.07157421112060547\n",
      "Training loss for batch 2223 : 0.10476904362440109\n",
      "Training loss for batch 2224 : 0.21485604345798492\n",
      "Training loss for batch 2225 : 0.05592595413327217\n",
      "Training loss for batch 2226 : 0.04970230907201767\n",
      "Training loss for batch 2227 : 0.13657011091709137\n",
      "Training loss for batch 2228 : 0.03110101819038391\n",
      "Training loss for batch 2229 : 0.18235614895820618\n",
      "Training loss for batch 2230 : 0.06425151228904724\n",
      "Training loss for batch 2231 : 0.033445026725530624\n",
      "Training loss for batch 2232 : 0.05639732629060745\n",
      "Training loss for batch 2233 : 0.014224858954548836\n",
      "Training loss for batch 2234 : 0.10568439960479736\n",
      "Training loss for batch 2235 : 0.11720581352710724\n",
      "Training loss for batch 2236 : 0.11758042871952057\n",
      "Training loss for batch 2237 : 0.13767196238040924\n",
      "Training loss for batch 2238 : 0.09561115503311157\n",
      "Training loss for batch 2239 : 0.06676933914422989\n",
      "Training loss for batch 2240 : 0.02988496795296669\n",
      "Training loss for batch 2241 : 0.11725244671106339\n",
      "Training loss for batch 2242 : 0.07858902961015701\n",
      "Training loss for batch 2243 : 0.13720177114009857\n",
      "Training loss for batch 2244 : 0.0661129578948021\n",
      "Training loss for batch 2245 : 0.03588050231337547\n",
      "Training loss for batch 2246 : 0.14119811356067657\n",
      "Training loss for batch 2247 : 0.09510613232851028\n",
      "Training loss for batch 2248 : 0.06132436543703079\n",
      "Training loss for batch 2249 : 0.1467735767364502\n",
      "Training loss for batch 2250 : 0.0872890055179596\n",
      "Training loss for batch 2251 : 0.0820506289601326\n",
      "Training loss for batch 2252 : 0.16929204761981964\n",
      "Training loss for batch 2253 : 0.08328426629304886\n",
      "Training loss for batch 2254 : 0.18225997686386108\n",
      "Training loss for batch 2255 : 0.0960112065076828\n",
      "Training loss for batch 2256 : 0.08282101154327393\n",
      "Training loss for batch 2257 : 0.24049173295497894\n",
      "Training loss for batch 2258 : 0.0945248231291771\n",
      "Training loss for batch 2259 : 0.10418397188186646\n",
      "Training loss for batch 2260 : 0.10555868595838547\n",
      "Training loss for batch 2261 : 0.20250998437404633\n",
      "Training loss for batch 2262 : 0.10371194034814835\n",
      "Training loss for batch 2263 : 0.14003439247608185\n",
      "Training loss for batch 2264 : 0.05568946525454521\n",
      "Training loss for batch 2265 : 0.0614754818379879\n",
      "Training loss for batch 2266 : 0.10663459450006485\n",
      "Training loss for batch 2267 : 0.06614693254232407\n",
      "Training loss for batch 2268 : 0.07311925292015076\n",
      "Training loss for batch 2269 : 0.0975487008690834\n",
      "Training loss for batch 2270 : 0.018709750846028328\n",
      "Training loss for batch 2271 : 0.12769117951393127\n",
      "Training loss for batch 2272 : 0.12152732908725739\n",
      "Training loss for batch 2273 : 0.29597195982933044\n",
      "Training loss for batch 2274 : 0.13671176135540009\n",
      "Training loss for batch 2275 : 0.04368297755718231\n",
      "Training loss for batch 2276 : 0.08418849855661392\n",
      "Training loss for batch 2277 : 0.04159229248762131\n",
      "Training loss for batch 2278 : 0.05999128520488739\n",
      "Training loss for batch 2279 : 0.20815221965312958\n",
      "Training loss for batch 2280 : 0.2498936504125595\n",
      "Training loss for batch 2281 : 0.004194085951894522\n",
      "Training loss for batch 2282 : 0.08944904804229736\n",
      "Training loss for batch 2283 : 0.013369645923376083\n",
      "Training loss for batch 2284 : 0.10600753128528595\n",
      "Training loss for batch 2285 : 0.14565923810005188\n",
      "Training loss for batch 2286 : 0.06954187899827957\n",
      "Training loss for batch 2287 : 0.20168335735797882\n",
      "Training loss for batch 2288 : 0.03560562804341316\n",
      "Training loss for batch 2289 : 0.26589301228523254\n",
      "Training loss for batch 2290 : 0.08305647224187851\n",
      "Training loss for batch 2291 : 0.02986595407128334\n",
      "Training loss for batch 2292 : 0.11136187613010406\n",
      "Training loss for batch 2293 : 0.02480968087911606\n",
      "Training loss for batch 2294 : 0.013266320340335369\n",
      "Training loss for batch 2295 : 0.12901225686073303\n",
      "Training loss for batch 2296 : 0.28332987427711487\n",
      "Training loss for batch 2297 : 0.10361923277378082\n",
      "Training loss for batch 2298 : 0.04218967258930206\n",
      "Training loss for batch 2299 : 0.005938409827649593\n",
      "Training loss for batch 2300 : 0.06283561140298843\n",
      "Training loss for batch 2301 : 0.16189689934253693\n",
      "Training loss for batch 2302 : 0.125472754240036\n",
      "Training loss for batch 2303 : 0.17307209968566895\n",
      "Training loss for batch 2304 : 0.16233298182487488\n",
      "Training loss for batch 2305 : 0.07961267977952957\n",
      "Training loss for batch 2306 : 0.20541998744010925\n",
      "Training loss for batch 2307 : 0.11032723635435104\n",
      "Training loss for batch 2308 : 0.10432741045951843\n",
      "Training loss for batch 2309 : 0.112310029566288\n",
      "Training loss for batch 2310 : 0.04046231508255005\n",
      "Training loss for batch 2311 : 0.2097136378288269\n",
      "Training loss for batch 2312 : 0.1419743299484253\n",
      "Training loss for batch 2313 : 0.03526335209608078\n",
      "Training loss for batch 2314 : 0.008322200737893581\n",
      "Training loss for batch 2315 : 0.01632840745151043\n",
      "Training loss for batch 2316 : 0.022746721282601357\n",
      "Training loss for batch 2317 : 0.01847408525645733\n",
      "Training loss for batch 2318 : 0.146015927195549\n",
      "Training loss for batch 2319 : 0.05665656924247742\n",
      "Training loss for batch 2320 : 0.2360679656267166\n",
      "Training loss for batch 2321 : 0.14425083994865417\n",
      "Training loss for batch 2322 : 0.11625625938177109\n",
      "Training loss for batch 2323 : 0.024718277156352997\n",
      "Training loss for batch 2324 : 0.022057723253965378\n",
      "Training loss for batch 2325 : 0.027051514014601707\n",
      "Training loss for batch 2326 : 0.024423841387033463\n",
      "Training loss for batch 2327 : 0.08750854432582855\n",
      "Training loss for batch 2328 : 0.11236687749624252\n",
      "Training loss for batch 2329 : 0.0482957661151886\n",
      "Training loss for batch 2330 : 0.052603304386138916\n",
      "Training loss for batch 2331 : 0.14167124032974243\n",
      "Training loss for batch 2332 : 0.11416158825159073\n",
      "Training loss for batch 2333 : 0.15757834911346436\n",
      "Training loss for batch 2334 : 0.034305643290281296\n",
      "Training loss for batch 2335 : 0.24821779131889343\n",
      "Training loss for batch 2336 : 0.12930914759635925\n",
      "Training loss for batch 2337 : 0.25911247730255127\n",
      "Training loss for batch 2338 : 0.18309634923934937\n",
      "Training loss for batch 2339 : 0.10326328873634338\n",
      "Training loss for batch 2340 : 0.046051543205976486\n",
      "Training loss for batch 2341 : 0.14221014082431793\n",
      "Training loss for batch 2342 : 0.011349889449775219\n",
      "Training loss for batch 2343 : 0.04437814652919769\n",
      "Training loss for batch 2344 : 0.026661399751901627\n",
      "Training loss for batch 2345 : 0.07608535885810852\n",
      "Training loss for batch 2346 : 0.16353611648082733\n",
      "Training loss for batch 2347 : 0.14941999316215515\n",
      "Training loss for batch 2348 : 0.06718280911445618\n",
      "Training loss for batch 2349 : 0.28496548533439636\n",
      "Training loss for batch 2350 : 0.031311485916376114\n",
      "Training loss for batch 2351 : 0.03566008061170578\n",
      "Training loss for batch 2352 : 0.07545162737369537\n",
      "Training loss for batch 2353 : 0.10249011218547821\n",
      "Training loss for batch 2354 : 0.2713235914707184\n",
      "Training loss for batch 2355 : 0.06475631147623062\n",
      "Training loss for batch 2356 : 0.10997188091278076\n",
      "Training loss for batch 2357 : 0.13814744353294373\n",
      "Training loss for batch 2358 : 0.06516752392053604\n",
      "Training loss for batch 2359 : 0.11879497021436691\n",
      "Training loss for batch 2360 : 0.1512535661458969\n",
      "Training loss for batch 2361 : 0.07410034537315369\n",
      "Training loss for batch 2362 : 0.15876832604408264\n",
      "Training loss for batch 2363 : 0.09752704203128815\n",
      "Training loss for batch 2364 : 0.08747003972530365\n",
      "Training loss for batch 2365 : 0.01365569606423378\n",
      "Training loss for batch 2366 : 0.13400079309940338\n",
      "Training loss for batch 2367 : 0.18739299476146698\n",
      "Training loss for batch 2368 : 0.015244985930621624\n",
      "Training loss for batch 2369 : 0.1487150937318802\n",
      "Training loss for batch 2370 : 0.02352614887058735\n",
      "Training loss for batch 2371 : 0.03042646497488022\n",
      "Training loss for batch 2372 : 0.11611243337392807\n",
      "Training loss for batch 2373 : 0.014381113462150097\n",
      "Training loss for batch 2374 : 0.05059913918375969\n",
      "Training loss for batch 2375 : 0.22761312127113342\n",
      "Training loss for batch 2376 : 0.08107388019561768\n",
      "Training loss for batch 2377 : 0.11571849882602692\n",
      "Training loss for batch 2378 : 0.07196658104658127\n",
      "Training loss for batch 2379 : 0.048317939043045044\n",
      "Training loss for batch 2380 : 0.06500084698200226\n",
      "Training loss for batch 2381 : 0.1928764432668686\n",
      "Training loss for batch 2382 : 0.031966641545295715\n",
      "Training loss for batch 2383 : 0.01829853281378746\n",
      "Training loss for batch 2384 : 0.008933433331549168\n",
      "Training loss for batch 2385 : 0.049792855978012085\n",
      "Training loss for batch 2386 : 0.06970061361789703\n",
      "Training loss for batch 2387 : 0.12052229791879654\n",
      "Training loss for batch 2388 : 0.006747089326381683\n",
      "Training loss for batch 2389 : 0.14690959453582764\n",
      "Training loss for batch 2390 : 0.17746129631996155\n",
      "Training loss for batch 2391 : 0.05305710807442665\n",
      "Training loss for batch 2392 : 0.02001308836042881\n",
      "Training loss for batch 2393 : 0.16753530502319336\n",
      "Training loss for batch 2394 : 0.057229623198509216\n",
      "Training loss for batch 2395 : 0.0911940261721611\n",
      "Training loss for batch 2396 : 0.0868087187409401\n",
      "Training loss for batch 2397 : 0.1574316769838333\n",
      "Training loss for batch 2398 : 0.3577345907688141\n",
      "Training loss for batch 2399 : 0.011762799695134163\n",
      "Training loss for batch 2400 : 0.13712608814239502\n",
      "Training loss for batch 2401 : 0.03548836335539818\n",
      "Training loss for batch 2402 : 0.10079608857631683\n",
      "Training loss for batch 2403 : 0.009791809134185314\n",
      "Training loss for batch 2404 : 0.08948573470115662\n",
      "Training loss for batch 2405 : 0.12154792994260788\n",
      "Training loss for batch 2406 : 0.053320158272981644\n",
      "Training loss for batch 2407 : 0.05605994164943695\n",
      "Training loss for batch 2408 : 0.10237345099449158\n",
      "Training loss for batch 2409 : 0.040273770689964294\n",
      "Training loss for batch 2410 : 0.09696652740240097\n",
      "Training loss for batch 2411 : 0.11936230212450027\n",
      "Training loss for batch 2412 : 0.0796658992767334\n",
      "Training loss for batch 2413 : 0.06854370981454849\n",
      "Training loss for batch 2414 : 0.18714436888694763\n",
      "Training loss for batch 2415 : 0.09933817386627197\n",
      "Training loss for batch 2416 : 0.017834076657891273\n",
      "Training loss for batch 2417 : 0.01621330715715885\n",
      "Training loss for batch 2418 : 0.07831360399723053\n",
      "Training loss for batch 2419 : 0.07168294489383698\n",
      "Training loss for batch 2420 : 0.07275953143835068\n",
      "Training loss for batch 2421 : 0.14773766696453094\n",
      "Training loss for batch 2422 : 0.036237481981515884\n",
      "Training loss for batch 2423 : 0.08431132137775421\n",
      "Training loss for batch 2424 : 0.11671527475118637\n",
      "Training loss for batch 2425 : 0.1092357188463211\n",
      "Training loss for batch 2426 : 0.06055554002523422\n",
      "Training loss for batch 2427 : 0.16539613902568817\n",
      "Training loss for batch 2428 : 0.24653460085391998\n",
      "Training loss for batch 2429 : 0.1470964401960373\n",
      "Training loss for batch 2430 : 0.12354197353124619\n",
      "Training loss for batch 2431 : 0.0\n",
      "Training loss for batch 2432 : 0.14270524680614471\n",
      "Training loss for batch 2433 : 0.18913820385932922\n",
      "Training loss for batch 2434 : 0.009867899119853973\n",
      "Training loss for batch 2435 : 0.06533637642860413\n",
      "Training loss for batch 2436 : 0.12642380595207214\n",
      "Training loss for batch 2437 : 0.12737016379833221\n",
      "Training loss for batch 2438 : 0.054901573807001114\n",
      "Training loss for batch 2439 : 0.0918317511677742\n",
      "Training loss for batch 2440 : 0.05603954568505287\n",
      "Training loss for batch 2441 : 0.06197419390082359\n",
      "Training loss for batch 2442 : 0.11930038779973984\n",
      "Training loss for batch 2443 : 0.0788026675581932\n",
      "Training loss for batch 2444 : 0.17782895267009735\n",
      "Training loss for batch 2445 : 0.12130923569202423\n",
      "Training loss for batch 2446 : 0.009505847468972206\n",
      "Training loss for batch 2447 : 0.18077002465724945\n",
      "Training loss for batch 2448 : 0.18092431128025055\n",
      "Training loss for batch 2449 : 0.07636504620313644\n",
      "Training loss for batch 2450 : 0.09790828078985214\n",
      "Training loss for batch 2451 : 0.23047633469104767\n",
      "Training loss for batch 2452 : 0.08509208261966705\n",
      "Training loss for batch 2453 : 0.07628608494997025\n",
      "Training loss for batch 2454 : 0.15633095800876617\n",
      "Training loss for batch 2455 : 0.08258691430091858\n",
      "Training loss for batch 2456 : 0.11574198305606842\n",
      "Training loss for batch 2457 : 0.04857484996318817\n",
      "Training loss for batch 2458 : 0.28766655921936035\n",
      "Training loss for batch 2459 : 0.05654069408774376\n",
      "Training loss for batch 2460 : 0.10249758511781693\n",
      "Training loss for batch 2461 : 0.12007340788841248\n",
      "Training loss for batch 2462 : 0.12303890287876129\n",
      "Training loss for batch 2463 : 0.11496463418006897\n",
      "Training loss for batch 2464 : 0.024531563743948936\n",
      "Training loss for batch 2465 : 0.12547266483306885\n",
      "Training loss for batch 2466 : 0.15479978919029236\n",
      "Training loss for batch 2467 : 0.16936196386814117\n",
      "Training loss for batch 2468 : 0.08244342356920242\n",
      "Training loss for batch 2469 : 0.12807802855968475\n",
      "Training loss for batch 2470 : 0.19908973574638367\n",
      "Training loss for batch 2471 : 0.15272203087806702\n",
      "Training loss for batch 2472 : 0.08957445621490479\n",
      "Training loss for batch 2473 : 0.07338923215866089\n",
      "Training loss for batch 2474 : 0.09167206287384033\n",
      "Training loss for batch 2475 : 0.16907019913196564\n",
      "Training loss for batch 2476 : 0.06562074273824692\n",
      "Training loss for batch 2477 : 0.03247087821364403\n",
      "Training loss for batch 2478 : 0.08896568417549133\n",
      "Training loss for batch 2479 : 0.019561992958188057\n",
      "Training loss for batch 2480 : 0.009656856767833233\n",
      "Training loss for batch 2481 : 0.22538059949874878\n",
      "Training loss for batch 2482 : 0.0979577898979187\n",
      "Training loss for batch 2483 : 0.06481330841779709\n",
      "Training loss for batch 2484 : 0.19890432059764862\n",
      "Training loss for batch 2485 : 0.027328256517648697\n",
      "Training loss for batch 2486 : 0.05466965213418007\n",
      "Training loss for batch 2487 : 0.10759688913822174\n",
      "Training loss for batch 2488 : 0.06371385604143143\n",
      "Training loss for batch 2489 : 0.1570674479007721\n",
      "Training loss for batch 2490 : 0.0917491465806961\n",
      "Training loss for batch 2491 : 0.049466054886579514\n",
      "Training loss for batch 2492 : 0.10623495280742645\n",
      "Training loss for batch 2493 : 0.19683972001075745\n",
      "Training loss for batch 2494 : 0.1365358978509903\n",
      "Training loss for batch 2495 : 0.09476853907108307\n",
      "Training loss for batch 2496 : 0.07447289675474167\n",
      "Training loss for batch 2497 : 0.18608641624450684\n",
      "Training loss for batch 2498 : 0.0899398997426033\n",
      "Training loss for batch 2499 : 0.3542897403240204\n",
      "Training loss for batch 2500 : 0.2849521338939667\n",
      "Training loss for batch 2501 : 0.148543581366539\n",
      "Training loss for batch 2502 : 0.10866334289312363\n",
      "Training loss for batch 2503 : 0.05757731944322586\n",
      "Training loss for batch 2504 : 0.0973542183637619\n",
      "Training loss for batch 2505 : 0.17891187965869904\n",
      "Training loss for batch 2506 : 0.04910096898674965\n",
      "Training loss for batch 2507 : 0.011327994987368584\n",
      "Training loss for batch 2508 : 0.0531880259513855\n",
      "Training loss for batch 2509 : 0.13815830647945404\n",
      "Training loss for batch 2510 : 0.09709503501653671\n",
      "Training loss for batch 2511 : 0.06933877617120743\n",
      "Training loss for batch 2512 : 0.18118953704833984\n",
      "Training loss for batch 2513 : 0.02166239358484745\n",
      "Training loss for batch 2514 : 0.12228647619485855\n",
      "Training loss for batch 2515 : 0.06898728013038635\n",
      "Training loss for batch 2516 : 0.01669333316385746\n",
      "Training loss for batch 2517 : 0.11313124001026154\n",
      "Training loss for batch 2518 : 0.02217949740588665\n",
      "Training loss for batch 2519 : 0.15007981657981873\n",
      "Training loss for batch 2520 : 0.11234936863183975\n",
      "Training loss for batch 2521 : 0.19516028463840485\n",
      "Training loss for batch 2522 : 0.06806829571723938\n",
      "Training loss for batch 2523 : 0.13234597444534302\n",
      "Training loss for batch 2524 : 0.248114213347435\n",
      "Training loss for batch 2525 : 0.1313498467206955\n",
      "Training loss for batch 2526 : 0.08359868824481964\n",
      "Training loss for batch 2527 : 0.1523141711950302\n",
      "Training loss for batch 2528 : 0.17290613055229187\n",
      "Training loss for batch 2529 : 0.03763994202017784\n",
      "Training loss for batch 2530 : 0.270605206489563\n",
      "Training loss for batch 2531 : 0.020847029983997345\n",
      "Training loss for batch 2532 : 0.04245791956782341\n",
      "Training loss for batch 2533 : 0.09015484899282455\n",
      "Training loss for batch 2534 : 0.11730026453733444\n",
      "Training loss for batch 2535 : 0.12341009080410004\n",
      "Training loss for batch 2536 : 0.15293973684310913\n",
      "Training loss for batch 2537 : 0.03761819005012512\n",
      "Training loss for batch 2538 : 0.014651288278400898\n",
      "Training loss for batch 2539 : 0.17787910997867584\n",
      "Training loss for batch 2540 : 0.07815028727054596\n",
      "Training loss for batch 2541 : 0.15756171941757202\n",
      "Training loss for batch 2542 : 0.28380513191223145\n",
      "Training loss for batch 2543 : 0.045058488845825195\n",
      "Training loss for batch 2544 : 0.09495362639427185\n",
      "Training loss for batch 2545 : 0.039886463433504105\n",
      "Training loss for batch 2546 : 0.09431104362010956\n",
      "Training loss for batch 2547 : 0.15611869096755981\n",
      "Training loss for batch 2548 : 0.055705100297927856\n",
      "Training loss for batch 2549 : 0.1158352643251419\n",
      "Training loss for batch 2550 : 0.014808883890509605\n",
      "Training loss for batch 2551 : 0.010563469491899014\n",
      "Training loss for batch 2552 : 0.03347636014223099\n",
      "Training loss for batch 2553 : 0.18510137498378754\n",
      "Training loss for batch 2554 : 0.13506297767162323\n",
      "Training loss for batch 2555 : 0.027920374646782875\n",
      "Training loss for batch 2556 : 0.14176912605762482\n",
      "Training loss for batch 2557 : 0.08192987740039825\n",
      "Training loss for batch 2558 : 0.1279938817024231\n",
      "Training loss for batch 2559 : 0.17128100991249084\n",
      "Training loss for batch 2560 : 0.06276603043079376\n",
      "Training loss for batch 2561 : 0.02266417071223259\n",
      "Training loss for batch 2562 : 0.11236557364463806\n",
      "Training loss for batch 2563 : 0.22438761591911316\n",
      "Training loss for batch 2564 : 0.07292269915342331\n",
      "Training loss for batch 2565 : 0.08169396966695786\n",
      "Training loss for batch 2566 : 0.00981266051530838\n",
      "Training loss for batch 2567 : 0.12592224776744843\n",
      "Training loss for batch 2568 : 0.020447256043553352\n",
      "Training loss for batch 2569 : 0.08233676105737686\n",
      "Training loss for batch 2570 : 0.0692000612616539\n",
      "Training loss for batch 2571 : 0.10073243826627731\n",
      "Training loss for batch 2572 : 0.15040937066078186\n",
      "Training loss for batch 2573 : 0.053286612033843994\n",
      "Training loss for batch 2574 : 0.07392455637454987\n",
      "Training loss for batch 2575 : 0.22344502806663513\n",
      "Training loss for batch 2576 : 0.0019635730423033237\n",
      "Training loss for batch 2577 : 0.12190095335245132\n",
      "Training loss for batch 2578 : 0.05026651546359062\n",
      "Training loss for batch 2579 : 0.03890354931354523\n",
      "Training loss for batch 2580 : 0.11789468675851822\n",
      "Training loss for batch 2581 : 0.1936214417219162\n",
      "Training loss for batch 2582 : 0.17988763749599457\n",
      "Training loss for batch 2583 : 0.10918625444173813\n",
      "Training loss for batch 2584 : 0.045084398239851\n",
      "Training loss for batch 2585 : 0.15292339026927948\n",
      "Training loss for batch 2586 : 0.06053348630666733\n",
      "Training loss for batch 2587 : 0.279231995344162\n",
      "Training loss for batch 2588 : 0.04346140846610069\n",
      "Training loss for batch 2589 : 0.030192159116268158\n",
      "Training loss for batch 2590 : 0.11783476918935776\n",
      "Training loss for batch 2591 : 0.06397532671689987\n",
      "Training loss for batch 2592 : 0.07497438043355942\n",
      "Training loss for batch 2593 : 0.10502927005290985\n",
      "Training loss for batch 2594 : 0.08966490626335144\n",
      "Training loss for batch 2595 : 0.05308961123228073\n",
      "Training loss for batch 2596 : 0.0013946841936558485\n",
      "Training loss for batch 2597 : 0.10279543697834015\n",
      "Training loss for batch 2598 : 0.04832633584737778\n",
      "Training loss for batch 2599 : 0.10115377604961395\n",
      "Training loss for batch 2600 : 0.20238326489925385\n",
      "Training loss for batch 2601 : 0.035708196461200714\n",
      "Training loss for batch 2602 : 0.16539329290390015\n",
      "Training loss for batch 2603 : 0.3008677363395691\n",
      "Training loss for batch 2604 : 0.010408959351480007\n",
      "Training loss for batch 2605 : 0.17618310451507568\n",
      "Training loss for batch 2606 : 0.04273555055260658\n",
      "Training loss for batch 2607 : 0.21356065571308136\n",
      "Training loss for batch 2608 : 0.21547019481658936\n",
      "Training loss for batch 2609 : 0.046009231358766556\n",
      "Training loss for batch 2610 : 0.09678786993026733\n",
      "Training loss for batch 2611 : 0.017616672441363335\n",
      "Training loss for batch 2612 : 0.11327076703310013\n",
      "Training loss for batch 2613 : 0.08452809602022171\n",
      "Training loss for batch 2614 : 0.06314481049776077\n",
      "Training loss for batch 2615 : 0.09234153479337692\n",
      "Training loss for batch 2616 : 0.11800570785999298\n",
      "Training loss for batch 2617 : 0.14683513343334198\n",
      "Training loss for batch 2618 : 0.22136037051677704\n",
      "Training loss for batch 2619 : 0.10408724099397659\n",
      "Training loss for batch 2620 : 0.0700131356716156\n",
      "Training loss for batch 2621 : 0.21580207347869873\n",
      "Training loss for batch 2622 : 0.13438844680786133\n",
      "Training loss for batch 2623 : 0.10898188501596451\n",
      "Training loss for batch 2624 : 0.1443130075931549\n",
      "Training loss for batch 2625 : 0.04045766592025757\n",
      "Training loss for batch 2626 : 0.17876450717449188\n",
      "Training loss for batch 2627 : 0.15642914175987244\n",
      "Training loss for batch 2628 : 0.04535408318042755\n",
      "Training loss for batch 2629 : 0.024321293458342552\n",
      "Training loss for batch 2630 : 0.033275000751018524\n",
      "Training loss for batch 2631 : 0.14735203981399536\n",
      "Training loss for batch 2632 : 0.14016559720039368\n",
      "Training loss for batch 2633 : 0.010839888826012611\n",
      "Training loss for batch 2634 : 0.09235347807407379\n",
      "Training loss for batch 2635 : 0.1627722531557083\n",
      "Training loss for batch 2636 : 0.13180091977119446\n",
      "Training loss for batch 2637 : 0.0920967385172844\n",
      "Training loss for batch 2638 : 0.08579026907682419\n",
      "Training loss for batch 2639 : 0.03639465197920799\n",
      "Training loss for batch 2640 : 0.14826034009456635\n",
      "Training loss for batch 2641 : 0.02375120483338833\n",
      "Training loss for batch 2642 : 0.10285943001508713\n",
      "Training loss for batch 2643 : 0.028456086292862892\n",
      "Training loss for batch 2644 : 0.1507500410079956\n",
      "Training loss for batch 2645 : 0.04525095596909523\n",
      "Training loss for batch 2646 : 0.08058590441942215\n",
      "Training loss for batch 2647 : 0.06836895644664764\n",
      "Training loss for batch 2648 : 0.10077391564846039\n",
      "Training loss for batch 2649 : 0.13286755979061127\n",
      "Training loss for batch 2650 : 0.037148524075746536\n",
      "Training loss for batch 2651 : 0.1083960011601448\n",
      "Training loss for batch 2652 : 0.1275804489850998\n",
      "Training loss for batch 2653 : 0.0006575560546480119\n",
      "Training loss for batch 2654 : 0.12877236306667328\n",
      "Training loss for batch 2655 : 0.14630155265331268\n",
      "Training loss for batch 2656 : 0.024685915559530258\n",
      "Training loss for batch 2657 : 0.02347077988088131\n",
      "Training loss for batch 2658 : 0.1513892263174057\n",
      "Training loss for batch 2659 : 0.11958838999271393\n",
      "Training loss for batch 2660 : 0.14392732083797455\n",
      "Training loss for batch 2661 : 0.2115592211484909\n",
      "Training loss for batch 2662 : 0.1857142150402069\n",
      "Training loss for batch 2663 : 0.17267724871635437\n",
      "Training loss for batch 2664 : 0.05583304539322853\n",
      "Training loss for batch 2665 : 0.09756501019001007\n",
      "Training loss for batch 2666 : 0.13942617177963257\n",
      "Training loss for batch 2667 : 0.1550082415342331\n",
      "Training loss for batch 2668 : 0.1259489804506302\n",
      "Training loss for batch 2669 : 0.3399121165275574\n",
      "Training loss for batch 2670 : 0.022009342908859253\n",
      "Training loss for batch 2671 : 0.009229406714439392\n",
      "Training loss for batch 2672 : 0.04297856613993645\n",
      "Training loss for batch 2673 : 0.04827428236603737\n",
      "Training loss for batch 2674 : 0.08696535229682922\n",
      "Training loss for batch 2675 : 0.04672703146934509\n",
      "Training loss for batch 2676 : 0.08685056865215302\n",
      "Training loss for batch 2677 : 0.3108134865760803\n",
      "Training loss for batch 2678 : 0.0\n",
      "Training loss for batch 2679 : 0.045481376349925995\n",
      "Training loss for batch 2680 : 0.08139205724000931\n",
      "Training loss for batch 2681 : 0.05718930438160896\n",
      "Training loss for batch 2682 : 0.05793922767043114\n",
      "Training loss for batch 2683 : 0.15128594636917114\n",
      "Training loss for batch 2684 : 0.05499199405312538\n",
      "Training loss for batch 2685 : 0.041778165847063065\n",
      "Training loss for batch 2686 : 0.19097504019737244\n",
      "Training loss for batch 2687 : 0.13533182442188263\n",
      "Training loss for batch 2688 : 0.21065334975719452\n",
      "Training loss for batch 2689 : 0.1088641956448555\n",
      "Training loss for batch 2690 : 0.16543737053871155\n",
      "Training loss for batch 2691 : 0.05740188807249069\n",
      "Training loss for batch 2692 : 0.09127137809991837\n",
      "Training loss for batch 2693 : 0.07818889617919922\n",
      "Training loss for batch 2694 : 0.08213718235492706\n",
      "Training loss for batch 2695 : 0.14018471539020538\n",
      "Training loss for batch 2696 : 0.0019450833788141608\n",
      "Training loss for batch 2697 : 0.09625913947820663\n",
      "Training loss for batch 2698 : 0.1968214064836502\n",
      "Training loss for batch 2699 : 0.02224380150437355\n",
      "Training loss for batch 2700 : 0.2579977810382843\n",
      "Training loss for batch 2701 : 0.05342666804790497\n",
      "Training loss for batch 2702 : 0.2711339294910431\n",
      "Training loss for batch 2703 : 0.06246178224682808\n",
      "Training loss for batch 2704 : 0.10920675843954086\n",
      "Training loss for batch 2705 : 0.009234029799699783\n",
      "Training loss for batch 2706 : 0.07962064445018768\n",
      "Training loss for batch 2707 : 0.2513274550437927\n",
      "Training loss for batch 2708 : 0.24932609498500824\n",
      "Training loss for batch 2709 : 0.15935979783535004\n",
      "Training loss for batch 2710 : 0.17598578333854675\n",
      "Training loss for batch 2711 : 0.08282512426376343\n",
      "Training loss for batch 2712 : 0.09081827849149704\n",
      "Training loss for batch 2713 : 0.2565038204193115\n",
      "Training loss for batch 2714 : 0.34012696146965027\n",
      "Training loss for batch 2715 : 0.028858259320259094\n",
      "Training loss for batch 2716 : 0.11160778999328613\n",
      "Training loss for batch 2717 : 0.06342408806085587\n",
      "Training loss for batch 2718 : 0.14007136225700378\n",
      "Training loss for batch 2719 : 0.2198285311460495\n",
      "Training loss for batch 2720 : 0.11738062649965286\n",
      "Training loss for batch 2721 : 0.23148389160633087\n",
      "Training loss for batch 2722 : 0.08669252693653107\n",
      "Training loss for batch 2723 : 0.3298113942146301\n",
      "Training loss for batch 2724 : 0.03918338939547539\n",
      "Training loss for batch 2725 : 0.2120540291070938\n",
      "Training loss for batch 2726 : 0.2213178128004074\n",
      "Training loss for batch 2727 : 0.08295629918575287\n",
      "Training loss for batch 2728 : 0.19988851249217987\n",
      "Training loss for batch 2729 : 0.10146655142307281\n",
      "Training loss for batch 2730 : 0.05775413289666176\n",
      "Training loss for batch 2731 : 0.0865442082285881\n",
      "Training loss for batch 2732 : 0.08230054378509521\n",
      "Training loss for batch 2733 : 0.08500450104475021\n",
      "Training loss for batch 2734 : 0.11019663512706757\n",
      "Training loss for batch 2735 : 0.09563088417053223\n",
      "Training loss for batch 2736 : 0.08507246524095535\n",
      "Training loss for batch 2737 : 0.05329875648021698\n",
      "Training loss for batch 2738 : 0.22747495770454407\n",
      "Training loss for batch 2739 : 0.055439867079257965\n",
      "Training loss for batch 2740 : 0.11997146159410477\n",
      "Training loss for batch 2741 : 0.07481857389211655\n",
      "Training loss for batch 2742 : 0.16195261478424072\n",
      "Training loss for batch 2743 : 0.05304964631795883\n",
      "Training loss for batch 2744 : 0.13009048998355865\n",
      "Training loss for batch 2745 : 0.046192318201065063\n",
      "Training loss for batch 2746 : 0.1262580156326294\n",
      "Training loss for batch 2747 : 0.11313211172819138\n",
      "Training loss for batch 2748 : 0.15561875700950623\n",
      "Training loss for batch 2749 : 0.056866198778152466\n",
      "Training loss for batch 2750 : 0.09039469808340073\n",
      "Training loss for batch 2751 : 0.06808153539896011\n",
      "Training loss for batch 2752 : 0.18714001774787903\n",
      "Training loss for batch 2753 : 0.09930752962827682\n",
      "Training loss for batch 2754 : 0.027579128742218018\n",
      "Training loss for batch 2755 : 0.2002386748790741\n",
      "Training loss for batch 2756 : 0.12451693415641785\n",
      "Training loss for batch 2757 : 0.1800088733434677\n",
      "Training loss for batch 2758 : 0.18868638575077057\n",
      "Training loss for batch 2759 : 0.009138097055256367\n",
      "Training loss for batch 2760 : 0.11703446507453918\n",
      "Training loss for batch 2761 : 0.06722339987754822\n",
      "Training loss for batch 2762 : 0.06507962197065353\n",
      "Training loss for batch 2763 : 0.11086219549179077\n",
      "Training loss for batch 2764 : 0.05973640829324722\n",
      "Training loss for batch 2765 : 0.14181996881961823\n",
      "Training loss for batch 2766 : 0.011503130197525024\n",
      "Training loss for batch 2767 : 0.17614644765853882\n",
      "Training loss for batch 2768 : 0.31859493255615234\n",
      "Training loss for batch 2769 : 0.02227196842432022\n",
      "Training loss for batch 2770 : 0.0575660839676857\n",
      "Training loss for batch 2771 : 0.10001266002655029\n",
      "Training loss for batch 2772 : 0.06227128952741623\n",
      "Training loss for batch 2773 : 0.09612907469272614\n",
      "Training loss for batch 2774 : 0.026248857378959656\n",
      "Training loss for batch 2775 : 0.027486102655529976\n",
      "Training loss for batch 2776 : 0.03841057047247887\n",
      "Training loss for batch 2777 : 0.1328539252281189\n",
      "Training loss for batch 2778 : 0.30747583508491516\n",
      "Training loss for batch 2779 : 0.13399618864059448\n",
      "Training loss for batch 2780 : 0.189973846077919\n",
      "Training loss for batch 2781 : 0.1050441637635231\n",
      "Training loss for batch 2782 : 0.06556426733732224\n",
      "Training loss for batch 2783 : 0.18513135612010956\n",
      "Training loss for batch 2784 : 0.3325033485889435\n",
      "Training loss for batch 2785 : 0.049395691603422165\n",
      "Training loss for batch 2786 : 0.03991273418068886\n",
      "Training loss for batch 2787 : 0.13079868257045746\n",
      "Training loss for batch 2788 : 0.0932646319270134\n",
      "Training loss for batch 2789 : 0.1015324592590332\n",
      "Training loss for batch 2790 : 0.15703605115413666\n",
      "Training loss for batch 2791 : 0.12124322354793549\n",
      "Training loss for batch 2792 : 0.060437917709350586\n",
      "Training loss for batch 2793 : 0.10855834931135178\n",
      "Training loss for batch 2794 : 0.04343320056796074\n",
      "Training loss for batch 2795 : 0.06998488306999207\n",
      "Training loss for batch 2796 : 0.07392716407775879\n",
      "Training loss for batch 2797 : 0.06906233727931976\n",
      "Training loss for batch 2798 : 0.14615963399410248\n",
      "Training loss for batch 2799 : 0.3253098130226135\n",
      "Training loss for batch 2800 : 0.08402464538812637\n",
      "Training loss for batch 2801 : 0.0659070611000061\n",
      "Training loss for batch 2802 : 0.003162756562232971\n",
      "Training loss for batch 2803 : 0.25749993324279785\n",
      "Training loss for batch 2804 : 0.08855807781219482\n",
      "Training loss for batch 2805 : 0.08940888941287994\n",
      "Training loss for batch 2806 : 0.09307239204645157\n",
      "Training loss for batch 2807 : 0.11329495906829834\n",
      "Training loss for batch 2808 : 0.03805486857891083\n",
      "Training loss for batch 2809 : 0.12898004055023193\n",
      "Training loss for batch 2810 : 0.11583331972360611\n",
      "Training loss for batch 2811 : 0.06910312175750732\n",
      "Training loss for batch 2812 : 0.27622565627098083\n",
      "Training loss for batch 2813 : 0.19907890260219574\n",
      "Training loss for batch 2814 : 0.08214207738637924\n",
      "Training loss for batch 2815 : 0.06101351976394653\n",
      "Training loss for batch 2816 : 0.20433366298675537\n",
      "Training loss for batch 2817 : 0.18463975191116333\n",
      "Training loss for batch 2818 : 0.09444800764322281\n",
      "Training loss for batch 2819 : 0.08741895854473114\n",
      "Training loss for batch 2820 : 0.05690959095954895\n",
      "Training loss for batch 2821 : 0.15138192474842072\n",
      "Training loss for batch 2822 : 0.08089800924062729\n",
      "Training loss for batch 2823 : 0.17816130816936493\n",
      "Training loss for batch 2824 : 0.039689887315034866\n",
      "Training loss for batch 2825 : 0.19077832996845245\n",
      "Training loss for batch 2826 : 0.1286996603012085\n",
      "Training loss for batch 2827 : 0.21253950893878937\n",
      "Training loss for batch 2828 : 0.12070769816637039\n",
      "Training loss for batch 2829 : 0.10266099125146866\n",
      "Training loss for batch 2830 : 0.0394529290497303\n",
      "Training loss for batch 2831 : 0.15839828550815582\n",
      "Training loss for batch 2832 : 0.002003992209210992\n",
      "Training loss for batch 2833 : 0.10226350277662277\n",
      "Training loss for batch 2834 : 0.15123943984508514\n",
      "Training loss for batch 2835 : 0.11545617878437042\n",
      "Training loss for batch 2836 : 0.18298448622226715\n",
      "Training loss for batch 2837 : 0.06837056577205658\n",
      "Training loss for batch 2838 : 0.16394564509391785\n",
      "Training loss for batch 2839 : 0.1352570652961731\n",
      "Training loss for batch 2840 : 0.10977337509393692\n",
      "Training loss for batch 2841 : 0.014451924711465836\n",
      "Training loss for batch 2842 : 0.0647568553686142\n",
      "Training loss for batch 2843 : 0.03852938488125801\n",
      "Training loss for batch 2844 : 0.08366797864437103\n",
      "Training loss for batch 2845 : 0.11684427410364151\n",
      "Training loss for batch 2846 : 0.01302027702331543\n",
      "Training loss for batch 2847 : 0.02515856921672821\n",
      "Training loss for batch 2848 : 0.14985013008117676\n",
      "Training loss for batch 2849 : 0.1350785195827484\n",
      "Training loss for batch 2850 : 0.07719315588474274\n",
      "Training loss for batch 2851 : 0.10924775898456573\n",
      "Training loss for batch 2852 : 0.09400401264429092\n",
      "Training loss for batch 2853 : 0.16718091070652008\n",
      "Training loss for batch 2854 : 0.06592053174972534\n",
      "Training loss for batch 2855 : 0.09385023266077042\n",
      "Training loss for batch 2856 : 0.032017894089221954\n",
      "Training loss for batch 2857 : 0.094346784055233\n",
      "Training loss for batch 2858 : 0.025394050404429436\n",
      "Training loss for batch 2859 : 0.006892680656164885\n",
      "Training loss for batch 2860 : 0.10871414840221405\n",
      "Training loss for batch 2861 : 0.012392568401992321\n",
      "Training loss for batch 2862 : 0.08666134625673294\n",
      "Training loss for batch 2863 : 0.2347302883863449\n",
      "Training loss for batch 2864 : 0.09482130408287048\n",
      "Training loss for batch 2865 : 0.05977732315659523\n",
      "Training loss for batch 2866 : 0.024717723950743675\n",
      "Training loss for batch 2867 : 0.17151905596256256\n",
      "Training loss for batch 2868 : 0.05718960985541344\n",
      "Training loss for batch 2869 : 0.07922239601612091\n",
      "Training loss for batch 2870 : 0.09038069099187851\n",
      "Training loss for batch 2871 : 0.040092162787914276\n",
      "Training loss for batch 2872 : 0.028137315064668655\n",
      "Training loss for batch 2873 : 0.10852131992578506\n",
      "Training loss for batch 2874 : 0.008880655281245708\n",
      "Training loss for batch 2875 : 0.062114469707012177\n",
      "Training loss for batch 2876 : 0.14334841072559357\n",
      "Training loss for batch 2877 : 0.03540114685893059\n",
      "Training loss for batch 2878 : 0.04216349497437477\n",
      "Training loss for batch 2879 : 0.19234172999858856\n",
      "Training loss for batch 2880 : 0.0018338412046432495\n",
      "Training loss for batch 2881 : 0.05751069635152817\n",
      "Training loss for batch 2882 : 0.022044243291020393\n",
      "Training loss for batch 2883 : 0.11663369834423065\n",
      "Training loss for batch 2884 : 0.2945122718811035\n",
      "Training loss for batch 2885 : 0.05789395049214363\n",
      "Training loss for batch 2886 : 0.060046400874853134\n",
      "Training loss for batch 2887 : 0.024492306634783745\n",
      "Training loss for batch 2888 : 0.06347525119781494\n",
      "Training loss for batch 2889 : 0.09056691080331802\n",
      "Training loss for batch 2890 : 0.00352325476706028\n",
      "Training loss for batch 2891 : 0.0926918089389801\n",
      "Training loss for batch 2892 : 0.2535799741744995\n",
      "Training loss for batch 2893 : 0.31132766604423523\n",
      "Training loss for batch 2894 : 0.08928115665912628\n",
      "Training loss for batch 2895 : 0.10151348263025284\n",
      "Training loss for batch 2896 : 0.11513708531856537\n",
      "Training loss for batch 2897 : 0.14188522100448608\n",
      "Training loss for batch 2898 : 0.11489836871623993\n",
      "Training loss for batch 2899 : 0.013663851656019688\n",
      "Training loss for batch 2900 : 0.029911043122410774\n",
      "Training loss for batch 2901 : 0.05392162501811981\n",
      "Training loss for batch 2902 : 0.10527072101831436\n",
      "Training loss for batch 2903 : 0.07115795463323593\n",
      "Training loss for batch 2904 : 0.0015428613405674696\n",
      "Training loss for batch 2905 : 0.020369520410895348\n",
      "Training loss for batch 2906 : 0.04421898350119591\n",
      "Training loss for batch 2907 : 0.06368490308523178\n",
      "Training loss for batch 2908 : 0.1825139820575714\n",
      "Training loss for batch 2909 : 0.2744094431400299\n",
      "Training loss for batch 2910 : 0.08811192959547043\n",
      "Training loss for batch 2911 : 0.11516434699296951\n",
      "Training loss for batch 2912 : 0.00868731364607811\n",
      "Training loss for batch 2913 : 0.1800287365913391\n",
      "Training loss for batch 2914 : 0.05866372585296631\n",
      "Training loss for batch 2915 : 0.04920249432325363\n",
      "Training loss for batch 2916 : 0.027061833068728447\n",
      "Training loss for batch 2917 : 0.1738845258951187\n",
      "Training loss for batch 2918 : 0.10646750032901764\n",
      "Training loss for batch 2919 : 0.0757274180650711\n",
      "Training loss for batch 2920 : 0.08790936321020126\n",
      "Training loss for batch 2921 : 0.07980143278837204\n",
      "Training loss for batch 2922 : 0.153814435005188\n",
      "Training loss for batch 2923 : 0.04144534468650818\n",
      "Training loss for batch 2924 : 0.154848113656044\n",
      "Training loss for batch 2925 : 0.08536373823881149\n",
      "Training loss for batch 2926 : 0.04891246557235718\n",
      "Training loss for batch 2927 : 0.2382507175207138\n",
      "Training loss for batch 2928 : 0.044623736292123795\n",
      "Training loss for batch 2929 : 0.04259217157959938\n",
      "Training loss for batch 2930 : 0.011263666674494743\n",
      "Training loss for batch 2931 : 0.08693919330835342\n",
      "Training loss for batch 2932 : 0.036974843591451645\n",
      "Training loss for batch 2933 : 0.2399555891752243\n",
      "Training loss for batch 2934 : 0.033365242183208466\n",
      "Training loss for batch 2935 : 0.11628106981515884\n",
      "Training loss for batch 2936 : 0.1484244465827942\n",
      "Training loss for batch 2937 : 0.13881750404834747\n",
      "Training loss for batch 2938 : 0.0429878830909729\n",
      "Training loss for batch 2939 : 0.013268878683447838\n",
      "Training loss for batch 2940 : 0.12368140369653702\n",
      "Training loss for batch 2941 : 0.09635274857282639\n",
      "Training loss for batch 2942 : 0.1652706414461136\n",
      "Training loss for batch 2943 : 0.11903277039527893\n",
      "Training loss for batch 2944 : 0.14992757141590118\n",
      "Training loss for batch 2945 : 0.05605755373835564\n",
      "Training loss for batch 2946 : 0.06359844654798508\n",
      "Training loss for batch 2947 : 0.07219469547271729\n",
      "Training loss for batch 2948 : 0.2526205778121948\n",
      "Training loss for batch 2949 : 0.05347805842757225\n",
      "Training loss for batch 2950 : 0.16043350100517273\n",
      "Training loss for batch 2951 : 0.024006441235542297\n",
      "Training loss for batch 2952 : 0.05546386167407036\n",
      "Training loss for batch 2953 : 0.28926414251327515\n",
      "Training loss for batch 2954 : 0.07132765650749207\n",
      "Training loss for batch 2955 : 0.07667245715856552\n",
      "Training loss for batch 2956 : 0.02452845126390457\n",
      "Training loss for batch 2957 : 0.08252282440662384\n",
      "Training loss for batch 2958 : 0.03844036906957626\n",
      "Training loss for batch 2959 : 0.21474945545196533\n",
      "Training loss for batch 2960 : 0.18357618153095245\n",
      "Training loss for batch 2961 : 0.15742622315883636\n",
      "Training loss for batch 2962 : 0.03047053888440132\n",
      "Training loss for batch 2963 : 0.09676498919725418\n",
      "Training loss for batch 2964 : 0.16436201333999634\n",
      "Training loss for batch 2965 : 0.22154536843299866\n",
      "Training loss for batch 2966 : 0.01911705546081066\n",
      "Training loss for batch 2967 : 0.08834829926490784\n",
      "Training loss for batch 2968 : 0.21621981263160706\n",
      "Training loss for batch 2969 : 0.041821300983428955\n",
      "Training loss for batch 2970 : 0.09001100808382034\n",
      "Training loss for batch 2971 : 0.1853390634059906\n",
      "Training loss for batch 2972 : 0.07855687290430069\n",
      "Training loss for batch 2973 : 0.32124802470207214\n",
      "Training loss for batch 2974 : 0.10067057609558105\n",
      "Training loss for batch 2975 : 0.1801718920469284\n",
      "Training loss for batch 2976 : 0.23684890568256378\n",
      "Training loss for batch 2977 : 0.04277360066771507\n",
      "Training loss for batch 2978 : 0.28455832600593567\n",
      "Training loss for batch 2979 : 0.01976875215768814\n",
      "Training loss for batch 2980 : 0.04163624718785286\n",
      "Training loss for batch 2981 : 0.11201132833957672\n",
      "Training loss for batch 2982 : 0.15212313830852509\n",
      "Training loss for batch 2983 : 0.12214930355548859\n",
      "Training loss for batch 2984 : 0.07316068559885025\n",
      "Training loss for batch 2985 : 0.12901008129119873\n",
      "Training loss for batch 2986 : 0.30704689025878906\n",
      "Training loss for batch 2987 : 0.05167475342750549\n",
      "Training loss for batch 2988 : 0.07081460952758789\n",
      "Training loss for batch 2989 : 0.31576839089393616\n",
      "Training loss for batch 2990 : 0.19985844194889069\n",
      "Training loss for batch 2991 : 0.0670662373304367\n",
      "Training loss for batch 2992 : 0.11395446956157684\n",
      "Training loss for batch 2993 : 0.18389925360679626\n",
      "Training loss for batch 2994 : 0.018168099224567413\n",
      "Training loss for batch 2995 : 0.12474590539932251\n",
      "Training loss for batch 2996 : 0.10355763137340546\n",
      "Training loss for batch 2997 : 0.19237501919269562\n",
      "Training loss for batch 2998 : 0.11041204631328583\n",
      "Training loss for batch 2999 : 0.07244975864887238\n",
      "Training loss for batch 3000 : 0.01814546063542366\n",
      "Training loss for batch 3001 : 0.012883603572845459\n",
      "Training loss for batch 3002 : 0.04271867498755455\n",
      "Training loss for batch 3003 : 0.1859765499830246\n",
      "Training loss for batch 3004 : 0.10056360065937042\n",
      "Training loss for batch 3005 : 0.16025854647159576\n",
      "Training loss for batch 3006 : 0.17425480484962463\n",
      "Training loss for batch 3007 : 0.15380749106407166\n",
      "Training loss for batch 3008 : 0.019155792891979218\n",
      "Training loss for batch 3009 : 0.036521632224321365\n",
      "Training loss for batch 3010 : 0.14984947443008423\n",
      "Training loss for batch 3011 : 0.06451026350259781\n",
      "Training loss for batch 3012 : 0.06343434751033783\n",
      "Training loss for batch 3013 : 0.12479285150766373\n",
      "Training loss for batch 3014 : 0.05155564844608307\n",
      "Training loss for batch 3015 : 0.16216208040714264\n",
      "Training loss for batch 3016 : 0.07969243824481964\n",
      "Training loss for batch 3017 : 0.189555823802948\n",
      "Training loss for batch 3018 : 0.07435079663991928\n",
      "Training loss for batch 3019 : 0.03891787678003311\n",
      "Training loss for batch 3020 : 0.06650061160326004\n",
      "Training loss for batch 3021 : 0.10546068847179413\n",
      "Training loss for batch 3022 : 0.054503120481967926\n",
      "Training loss for batch 3023 : 0.11749789118766785\n",
      "Training loss for batch 3024 : 0.13193202018737793\n",
      "Training loss for batch 3025 : 0.08010715246200562\n",
      "Training loss for batch 3026 : 0.22243943810462952\n",
      "Training loss for batch 3027 : 0.11205407977104187\n",
      "Training loss for batch 3028 : 0.3649446964263916\n",
      "Training loss for batch 3029 : 0.13360591232776642\n",
      "Training loss for batch 3030 : 0.1486913114786148\n",
      "Training loss for batch 3031 : 0.10675761848688126\n",
      "Training loss for batch 3032 : 0.10620559006929398\n",
      "Training loss for batch 3033 : 0.16159136593341827\n",
      "Training loss for batch 3034 : 0.033398352563381195\n",
      "Training loss for batch 3035 : 0.14898940920829773\n",
      "Training loss for batch 3036 : 0.09880731999874115\n",
      "Training loss for batch 3037 : 0.06912551075220108\n",
      "Training loss for batch 3038 : 0.04990359768271446\n",
      "Training loss for batch 3039 : 0.08626451343297958\n",
      "Training loss for batch 3040 : 0.08365688472986221\n",
      "Training loss for batch 3041 : 0.15228256583213806\n",
      "Training loss for batch 3042 : 0.09303544461727142\n",
      "Training loss for batch 3043 : 0.09963425248861313\n",
      "Training loss for batch 3044 : 0.15747328102588654\n",
      "Training loss for batch 3045 : 0.029475953429937363\n",
      "Training loss for batch 3046 : 0.19204667210578918\n",
      "Training loss for batch 3047 : 0.04438376426696777\n",
      "Training loss for batch 3048 : 0.15294258296489716\n",
      "Training loss for batch 3049 : 0.04274386167526245\n",
      "Training loss for batch 3050 : 0.13454453647136688\n",
      "Training loss for batch 3051 : 0.06432343274354935\n",
      "Training loss for batch 3052 : 0.24787981808185577\n",
      "Training loss for batch 3053 : 0.0672224685549736\n",
      "Training loss for batch 3054 : 0.03950256481766701\n",
      "Training loss for batch 3055 : 0.11958134919404984\n",
      "Training loss for batch 3056 : 0.035862427204847336\n",
      "Training loss for batch 3057 : 0.0947820246219635\n",
      "Training loss for batch 3058 : 0.060648247599601746\n",
      "Training loss for batch 3059 : 0.09958069771528244\n",
      "Training loss for batch 3060 : 0.06701112538576126\n",
      "Training loss for batch 3061 : 0.0734826996922493\n",
      "Training loss for batch 3062 : 0.20619289577007294\n",
      "Training loss for batch 3063 : 0.08836432546377182\n",
      "Training loss for batch 3064 : 0.16648288071155548\n",
      "Training loss for batch 3065 : 0.10901793092489243\n",
      "Training loss for batch 3066 : 0.07136347889900208\n",
      "Training loss for batch 3067 : 0.041181936860084534\n",
      "Training loss for batch 3068 : 0.0384681411087513\n",
      "Training loss for batch 3069 : 0.06620793044567108\n",
      "Training loss for batch 3070 : 0.014112088829278946\n",
      "Training loss for batch 3071 : 0.09436818957328796\n",
      "Training loss for batch 3072 : 0.2010050117969513\n",
      "Training loss for batch 3073 : 0.15277764201164246\n",
      "Training loss for batch 3074 : 0.01789204590022564\n",
      "Training loss for batch 3075 : 0.012226458638906479\n",
      "Training loss for batch 3076 : 0.09028036147356033\n",
      "Training loss for batch 3077 : 0.17642751336097717\n",
      "Training loss for batch 3078 : 0.06399253010749817\n",
      "Training loss for batch 3079 : 0.11444462835788727\n",
      "Training loss for batch 3080 : 0.11273667216300964\n",
      "Training loss for batch 3081 : 0.13319051265716553\n",
      "Training loss for batch 3082 : 0.032791052013635635\n",
      "Training loss for batch 3083 : 0.14121150970458984\n",
      "Training loss for batch 3084 : 0.06851692497730255\n",
      "Training loss for batch 3085 : 0.10454811900854111\n",
      "Training loss for batch 3086 : 0.09797564148902893\n",
      "Training loss for batch 3087 : 0.0800621509552002\n",
      "Training loss for batch 3088 : 0.03542404994368553\n",
      "Training loss for batch 3089 : 0.04532201588153839\n",
      "Training loss for batch 3090 : 0.0735088661313057\n",
      "Training loss for batch 3091 : 0.014468351379036903\n",
      "Training loss for batch 3092 : 0.04281012713909149\n",
      "Training loss for batch 3093 : 0.05942736193537712\n",
      "Training loss for batch 3094 : 0.04808437079191208\n",
      "Training loss for batch 3095 : 0.046304769814014435\n",
      "Training loss for batch 3096 : 0.08774203807115555\n",
      "Training loss for batch 3097 : 0.016147049143910408\n",
      "Training loss for batch 3098 : 0.03454217687249184\n",
      "Training loss for batch 3099 : 0.1470884382724762\n",
      "Training loss for batch 3100 : 0.15729022026062012\n",
      "Training loss for batch 3101 : 0.20361527800559998\n",
      "Training loss for batch 3102 : 0.1139637902379036\n",
      "Training loss for batch 3103 : 0.056901056319475174\n",
      "Training loss for batch 3104 : 0.09829490631818771\n",
      "Training loss for batch 3105 : 0.04649513214826584\n",
      "Training loss for batch 3106 : 0.14546287059783936\n",
      "Training loss for batch 3107 : 0.09678556770086288\n",
      "Training loss for batch 3108 : 0.043084800243377686\n",
      "Training loss for batch 3109 : 0.07557743042707443\n",
      "Training loss for batch 3110 : 0.1125548779964447\n",
      "Training loss for batch 3111 : 0.12012208998203278\n",
      "Training loss for batch 3112 : 0.09188099950551987\n",
      "Training loss for batch 3113 : 0.03681209310889244\n",
      "Training loss for batch 3114 : 0.03332473710179329\n",
      "Training loss for batch 3115 : 0.1509854942560196\n",
      "Training loss for batch 3116 : 0.14154179394245148\n",
      "Training loss for batch 3117 : 0.007848342880606651\n",
      "Training loss for batch 3118 : 0.07432953268289566\n",
      "Training loss for batch 3119 : 0.2754535973072052\n",
      "Training loss for batch 3120 : 0.19011710584163666\n",
      "Training loss for batch 3121 : 0.18451426923274994\n",
      "Training loss for batch 3122 : 0.15002712607383728\n",
      "Training loss for batch 3123 : 0.1340213567018509\n",
      "Training loss for batch 3124 : 0.14041493833065033\n",
      "Training loss for batch 3125 : 0.049598827958106995\n",
      "Training loss for batch 3126 : 0.011503512971103191\n",
      "Training loss for batch 3127 : 0.25333112478256226\n",
      "Training loss for batch 3128 : 0.15899401903152466\n",
      "Training loss for batch 3129 : 0.08679119497537613\n",
      "Training loss for batch 3130 : 0.1691853553056717\n",
      "Training loss for batch 3131 : 0.07519920915365219\n",
      "Training loss for batch 3132 : 0.03958859667181969\n",
      "Training loss for batch 3133 : 0.2824542820453644\n",
      "Training loss for batch 3134 : 0.0029612777289003134\n",
      "Training loss for batch 3135 : 0.05921509116888046\n",
      "Training loss for batch 3136 : 0.15427523851394653\n",
      "Training loss for batch 3137 : 0.06387899070978165\n",
      "Training loss for batch 3138 : 0.016763810068368912\n",
      "Training loss for batch 3139 : 0.21126729249954224\n",
      "Training loss for batch 3140 : 0.020955441519618034\n",
      "Training loss for batch 3141 : 0.1127902939915657\n",
      "Training loss for batch 3142 : 0.1294497847557068\n",
      "Training loss for batch 3143 : 0.0739893838763237\n",
      "Training loss for batch 3144 : 0.01708909682929516\n",
      "Training loss for batch 3145 : 0.2664266526699066\n",
      "Training loss for batch 3146 : 0.07373770326375961\n",
      "Training loss for batch 3147 : 0.11802951991558075\n",
      "Training loss for batch 3148 : 0.012138325721025467\n",
      "Training loss for batch 3149 : 0.20233899354934692\n",
      "Training loss for batch 3150 : 0.09070482105016708\n",
      "Training loss for batch 3151 : 0.171855628490448\n",
      "Training loss for batch 3152 : 0.11875703930854797\n",
      "Training loss for batch 3153 : 0.18606451153755188\n",
      "Training loss for batch 3154 : 0.03433642163872719\n",
      "Training loss for batch 3155 : 0.09014999121427536\n",
      "Training loss for batch 3156 : 0.03138135373592377\n",
      "Training loss for batch 3157 : 0.1642369031906128\n",
      "Training loss for batch 3158 : 0.22387047111988068\n",
      "Training loss for batch 3159 : 0.21430112421512604\n",
      "Training loss for batch 3160 : 0.047648586332798004\n",
      "Training loss for batch 3161 : 0.09941693395376205\n",
      "Training loss for batch 3162 : 0.053251031786203384\n",
      "Training loss for batch 3163 : 0.03545194864273071\n",
      "Training loss for batch 3164 : 0.06735993176698685\n",
      "Training loss for batch 3165 : 0.1317487508058548\n",
      "Training loss for batch 3166 : 0.13689671456813812\n",
      "Training loss for batch 3167 : 0.16746781766414642\n",
      "Training loss for batch 3168 : 0.11420167982578278\n",
      "Training loss for batch 3169 : 0.04603281617164612\n",
      "Training loss for batch 3170 : 0.0728495642542839\n",
      "Training loss for batch 3171 : 0.14255712926387787\n",
      "Training loss for batch 3172 : 0.1623247265815735\n",
      "Training loss for batch 3173 : 0.07583298534154892\n",
      "Training loss for batch 3174 : 0.05767665430903435\n",
      "Training loss for batch 3175 : 0.04557047784328461\n",
      "Training loss for batch 3176 : 0.10737024992704391\n",
      "Training loss for batch 3177 : 0.058135949075222015\n",
      "Training loss for batch 3178 : 0.036938976496458054\n",
      "Training loss for batch 3179 : 0.05725739896297455\n",
      "Training loss for batch 3180 : 0.09785224497318268\n",
      "Training loss for batch 3181 : 0.024385718628764153\n",
      "Training loss for batch 3182 : 0.03465082123875618\n",
      "Training loss for batch 3183 : 0.036258265376091\n",
      "Training loss for batch 3184 : 0.21794971823692322\n",
      "Training loss for batch 3185 : 0.11927460134029388\n",
      "Training loss for batch 3186 : 0.05701550096273422\n",
      "Training loss for batch 3187 : 0.0618099719285965\n",
      "Training loss for batch 3188 : 0.044149309396743774\n",
      "Training loss for batch 3189 : 0.10951105505228043\n",
      "Training loss for batch 3190 : 0.06080533564090729\n",
      "Training loss for batch 3191 : 0.13198566436767578\n",
      "Training loss for batch 3192 : 0.051986850798130035\n",
      "Training loss for batch 3193 : 0.0968647226691246\n",
      "Training loss for batch 3194 : 0.18682321906089783\n",
      "Training loss for batch 3195 : 0.015987074002623558\n",
      "Training loss for batch 3196 : 0.081743985414505\n",
      "Training loss for batch 3197 : 0.04089418426156044\n",
      "Training loss for batch 3198 : 0.09086142480373383\n",
      "Training loss for batch 3199 : 0.010513372719287872\n",
      "Training loss for batch 3200 : 0.12552684545516968\n",
      "Training loss for batch 3201 : 0.05365249142050743\n",
      "Training loss for batch 3202 : 0.1321871280670166\n",
      "Training loss for batch 3203 : 0.19033017754554749\n",
      "Training loss for batch 3204 : 0.10704417526721954\n",
      "Training loss for batch 3205 : 0.07889071851968765\n",
      "Training loss for batch 3206 : 0.04548745974898338\n",
      "Training loss for batch 3207 : 0.15800148248672485\n",
      "Training loss for batch 3208 : 0.28385481238365173\n",
      "Training loss for batch 3209 : 0.06822524219751358\n",
      "Training loss for batch 3210 : 0.022744817659258842\n",
      "Training loss for batch 3211 : 0.05060667544603348\n",
      "Training loss for batch 3212 : 0.010331020690500736\n",
      "Training loss for batch 3213 : 0.08666661381721497\n",
      "Training loss for batch 3214 : 0.22260189056396484\n",
      "Training loss for batch 3215 : 0.10349871963262558\n",
      "Training loss for batch 3216 : 0.09265271574258804\n",
      "Training loss for batch 3217 : 0.07582022249698639\n",
      "Training loss for batch 3218 : 0.2581166625022888\n",
      "Training loss for batch 3219 : 0.06465888768434525\n",
      "Training loss for batch 3220 : 0.2140161544084549\n",
      "Training loss for batch 3221 : 0.13370433449745178\n",
      "Training loss for batch 3222 : 0.13052332401275635\n",
      "Training loss for batch 3223 : 0.12121681123971939\n",
      "Training loss for batch 3224 : 0.2565380930900574\n",
      "Training loss for batch 3225 : 0.13732872903347015\n",
      "Training loss for batch 3226 : 0.04676181823015213\n",
      "Training loss for batch 3227 : 0.01289627980440855\n",
      "Training loss for batch 3228 : 0.05697043240070343\n",
      "Training loss for batch 3229 : 0.13751916587352753\n",
      "Training loss for batch 3230 : 0.004322923254221678\n",
      "Training loss for batch 3231 : 0.16873009502887726\n",
      "Training loss for batch 3232 : 0.14480547606945038\n",
      "Training loss for batch 3233 : 0.05197588726878166\n",
      "Training loss for batch 3234 : 0.010318907909095287\n",
      "Training loss for batch 3235 : 0.27958038449287415\n",
      "Training loss for batch 3236 : 0.10067585855722427\n",
      "Training loss for batch 3237 : 0.06522239744663239\n",
      "Training loss for batch 3238 : 0.16923114657402039\n",
      "Training loss for batch 3239 : 0.09139915555715561\n",
      "Training loss for batch 3240 : 0.01609465479850769\n",
      "Training loss for batch 3241 : 0.0498274564743042\n",
      "Training loss for batch 3242 : 0.19665086269378662\n",
      "Training loss for batch 3243 : 0.09267642349004745\n",
      "Training loss for batch 3244 : 0.01574011892080307\n",
      "Training loss for batch 3245 : 0.02086455188691616\n",
      "Training loss for batch 3246 : 0.2692892849445343\n",
      "Training loss for batch 3247 : 0.07843367010354996\n",
      "Training loss for batch 3248 : 0.1621873825788498\n",
      "Training loss for batch 3249 : 0.12005225569009781\n",
      "Training loss for batch 3250 : 0.08902215957641602\n",
      "Training loss for batch 3251 : 0.022824261337518692\n",
      "Training loss for batch 3252 : 0.09942464530467987\n",
      "Training loss for batch 3253 : 0.012543367221951485\n",
      "Training loss for batch 3254 : 0.06533853709697723\n",
      "Training loss for batch 3255 : 0.00968782790005207\n",
      "Training loss for batch 3256 : 0.0331183560192585\n",
      "Training loss for batch 3257 : 0.1880602389574051\n",
      "Training loss for batch 3258 : 0.2224704772233963\n",
      "Training loss for batch 3259 : 0.15411542356014252\n",
      "Training loss for batch 3260 : 0.17717157304286957\n",
      "Training loss for batch 3261 : 0.09085844457149506\n",
      "Training loss for batch 3262 : 0.04394137114286423\n",
      "Training loss for batch 3263 : 0.079076386988163\n",
      "Training loss for batch 3264 : 0.07006809860467911\n",
      "Training loss for batch 3265 : 0.09581689536571503\n",
      "Training loss for batch 3266 : 0.1898668110370636\n",
      "Training loss for batch 3267 : 0.09412350505590439\n",
      "Training loss for batch 3268 : 0.12137182056903839\n",
      "Training loss for batch 3269 : 0.13954786956310272\n",
      "Training loss for batch 3270 : 0.039102595299482346\n",
      "Training loss for batch 3271 : 0.13818524777889252\n",
      "Training loss for batch 3272 : 0.10643880069255829\n",
      "Training loss for batch 3273 : 0.09400149434804916\n",
      "Training loss for batch 3274 : 0.0986652746796608\n",
      "Training loss for batch 3275 : 0.15160346031188965\n",
      "Training loss for batch 3276 : 0.11983595043420792\n",
      "Training loss for batch 3277 : 0.012623772956430912\n",
      "Training loss for batch 3278 : 0.07121983170509338\n",
      "Training loss for batch 3279 : 0.039291009306907654\n",
      "Training loss for batch 3280 : 0.02353672683238983\n",
      "Training loss for batch 3281 : 0.07693304866552353\n",
      "Training loss for batch 3282 : 0.03251316770911217\n",
      "Training loss for batch 3283 : 0.12791496515274048\n",
      "Training loss for batch 3284 : 0.1256382316350937\n",
      "Training loss for batch 3285 : 0.07649090141057968\n",
      "Training loss for batch 3286 : 0.0022104631643742323\n",
      "Training loss for batch 3287 : 0.11998449265956879\n",
      "Training loss for batch 3288 : 0.09375832229852676\n",
      "Training loss for batch 3289 : 0.33469754457473755\n",
      "Training loss for batch 3290 : 0.0037627543788403273\n",
      "Training loss for batch 3291 : 0.026695936918258667\n",
      "Training loss for batch 3292 : 0.07318732142448425\n",
      "Training loss for batch 3293 : 0.1320778727531433\n",
      "Training loss for batch 3294 : 0.047392841428518295\n",
      "Training loss for batch 3295 : 0.1377030313014984\n",
      "Training loss for batch 3296 : 0.05251981317996979\n",
      "Training loss for batch 3297 : 0.2609882950782776\n",
      "Training loss for batch 3298 : 0.00329534150660038\n",
      "Training loss for batch 3299 : 0.06312594562768936\n",
      "Training loss for batch 3300 : 0.03543965145945549\n",
      "Training loss for batch 3301 : 0.04081171378493309\n",
      "Training loss for batch 3302 : 0.09724389016628265\n",
      "Training loss for batch 3303 : 0.12021001428365707\n",
      "Training loss for batch 3304 : 0.2686947286128998\n",
      "Training loss for batch 3305 : 0.08057456463575363\n",
      "Training loss for batch 3306 : 0.1536101996898651\n",
      "Training loss for batch 3307 : 0.014724845066666603\n",
      "Training loss for batch 3308 : 0.07089634239673615\n",
      "Training loss for batch 3309 : 0.14963988959789276\n",
      "Training loss for batch 3310 : 0.04497325047850609\n",
      "Training loss for batch 3311 : 0.0913904532790184\n",
      "Training loss for batch 3312 : 0.1761755645275116\n",
      "Training loss for batch 3313 : 0.25435084104537964\n",
      "Training loss for batch 3314 : 0.05181809887290001\n",
      "Training loss for batch 3315 : 0.2526125907897949\n",
      "Training loss for batch 3316 : 0.10334593057632446\n",
      "Training loss for batch 3317 : 0.058130908757448196\n",
      "Training loss for batch 3318 : 0.06436814367771149\n",
      "Training loss for batch 3319 : 0.027476372197270393\n",
      "Training loss for batch 3320 : 0.05100434273481369\n",
      "Training loss for batch 3321 : 0.09055114537477493\n",
      "Training loss for batch 3322 : 0.2409924864768982\n",
      "Training loss for batch 3323 : 0.18163004517555237\n",
      "Training loss for batch 3324 : 0.11195698380470276\n",
      "Training loss for batch 3325 : 0.021489350125193596\n",
      "Training loss for batch 3326 : 0.10506679117679596\n",
      "Training loss for batch 3327 : 0.07090749591588974\n",
      "Training loss for batch 3328 : 0.06990888714790344\n",
      "Training loss for batch 3329 : 0.2472367137670517\n",
      "Training loss for batch 3330 : 0.11399099975824356\n",
      "Training loss for batch 3331 : 0.040593430399894714\n",
      "Training loss for batch 3332 : 0.12162383645772934\n",
      "Training loss for batch 3333 : 0.15545742213726044\n",
      "Training loss for batch 3334 : 0.26773199439048767\n",
      "Training loss for batch 3335 : 0.08697284013032913\n",
      "Training loss for batch 3336 : 0.33250147104263306\n",
      "Training loss for batch 3337 : 0.12962274253368378\n",
      "Training loss for batch 3338 : 0.009369924664497375\n",
      "Training loss for batch 3339 : 0.22009436786174774\n",
      "Training loss for batch 3340 : 0.002439183183014393\n",
      "Training loss for batch 3341 : 0.2053796499967575\n",
      "Training loss for batch 3342 : 0.03129672631621361\n",
      "Training loss for batch 3343 : 0.23176763951778412\n",
      "Training loss for batch 3344 : 0.144044429063797\n",
      "Training loss for batch 3345 : 0.10281428694725037\n",
      "Training loss for batch 3346 : 0.05115425959229469\n",
      "Training loss for batch 3347 : 0.05768812075257301\n",
      "Training loss for batch 3348 : 0.04696233570575714\n",
      "Training loss for batch 3349 : 0.04268617928028107\n",
      "Training loss for batch 3350 : 0.11642234772443771\n",
      "Training loss for batch 3351 : 0.06608720868825912\n",
      "Training loss for batch 3352 : 0.13535349071025848\n",
      "Training loss for batch 3353 : 0.012265635654330254\n",
      "Training loss for batch 3354 : 0.0557803176343441\n",
      "Training loss for batch 3355 : 0.1265469342470169\n",
      "Training loss for batch 3356 : 0.0019374266266822815\n",
      "Training loss for batch 3357 : 0.07611778378486633\n",
      "Training loss for batch 3358 : 0.15591949224472046\n",
      "Training loss for batch 3359 : 0.031018882989883423\n",
      "Training loss for batch 3360 : 0.315535306930542\n",
      "Training loss for batch 3361 : 0.09493058174848557\n",
      "Training loss for batch 3362 : 0.29670828580856323\n",
      "Training loss for batch 3363 : 0.19123896956443787\n",
      "Training loss for batch 3364 : 0.028906142339110374\n",
      "Training loss for batch 3365 : 0.10624057054519653\n",
      "Training loss for batch 3366 : 0.09484891593456268\n",
      "Training loss for batch 3367 : 0.07638158649206161\n",
      "Training loss for batch 3368 : 0.07356908172369003\n",
      "Training loss for batch 3369 : 0.05876809358596802\n",
      "Training loss for batch 3370 : 0.19636118412017822\n",
      "Training loss for batch 3371 : 0.13984227180480957\n",
      "Training loss for batch 3372 : 0.01417053584009409\n",
      "Training loss for batch 3373 : 0.026624441146850586\n",
      "Training loss for batch 3374 : 0.14378541707992554\n",
      "Training loss for batch 3375 : 0.12606696784496307\n",
      "Training loss for batch 3376 : 0.22801660001277924\n",
      "Training loss for batch 3377 : 0.019903430715203285\n",
      "Training loss for batch 3378 : 0.13418935239315033\n",
      "Training loss for batch 3379 : 0.24741433560848236\n",
      "Training loss for batch 3380 : 0.2506958842277527\n",
      "Training loss for batch 3381 : 0.12466607987880707\n",
      "Training loss for batch 3382 : 0.15127083659172058\n",
      "Training loss for batch 3383 : 0.12357216328382492\n",
      "Training loss for batch 3384 : 0.14878931641578674\n",
      "Training loss for batch 3385 : 0.11697075515985489\n",
      "Training loss for batch 3386 : 0.1078653335571289\n",
      "Training loss for batch 3387 : 0.19016321003437042\n",
      "Training loss for batch 3388 : 0.17432540655136108\n",
      "Training loss for batch 3389 : 0.08569937944412231\n",
      "Training loss for batch 3390 : 0.19737227261066437\n",
      "Training loss for batch 3391 : 0.10481643676757812\n",
      "Training loss for batch 3392 : 0.15441006422042847\n",
      "Training loss for batch 3393 : 0.04109388217329979\n",
      "Training loss for batch 3394 : 0.161208838224411\n",
      "Training loss for batch 3395 : 0.10691528767347336\n",
      "Training loss for batch 3396 : 0.051898762583732605\n",
      "Training loss for batch 3397 : 0.22190344333648682\n",
      "Training loss for batch 3398 : 0.027386829257011414\n",
      "Training loss for batch 3399 : 0.08635575324296951\n",
      "Training loss for batch 3400 : 0.021945895627141\n",
      "Training loss for batch 3401 : 0.006354799959808588\n",
      "Training loss for batch 3402 : 0.046909432858228683\n",
      "Training loss for batch 3403 : 0.06566482782363892\n",
      "Training loss for batch 3404 : 0.12489423155784607\n",
      "Training loss for batch 3405 : 0.0976904109120369\n",
      "Training loss for batch 3406 : 0.11730784177780151\n",
      "Training loss for batch 3407 : 0.019740449264645576\n",
      "Training loss for batch 3408 : 0.12493111938238144\n",
      "Training loss for batch 3409 : 0.15619029104709625\n",
      "Training loss for batch 3410 : 0.08829239755868912\n",
      "Training loss for batch 3411 : 0.13044171035289764\n",
      "Training loss for batch 3412 : 0.09915110468864441\n",
      "Training loss for batch 3413 : 0.09154870361089706\n",
      "Training loss for batch 3414 : 0.09427912533283234\n",
      "Training loss for batch 3415 : 0.02970973402261734\n",
      "Training loss for batch 3416 : 0.04127664864063263\n",
      "Training loss for batch 3417 : 0.014590846374630928\n",
      "Training loss for batch 3418 : 0.06283988058567047\n",
      "Training loss for batch 3419 : 0.03792340308427811\n",
      "Training loss for batch 3420 : 0.20778696238994598\n",
      "Training loss for batch 3421 : 0.085647352039814\n",
      "Training loss for batch 3422 : 0.20840683579444885\n",
      "Training loss for batch 3423 : 0.22918719053268433\n",
      "Training loss for batch 3424 : 0.1696714162826538\n",
      "Training loss for batch 3425 : 0.11385679990053177\n",
      "Training loss for batch 3426 : 0.06227141618728638\n",
      "Training loss for batch 3427 : 0.030874233692884445\n",
      "Training loss for batch 3428 : 0.0376640260219574\n",
      "Training loss for batch 3429 : 0.1112205758690834\n",
      "Training loss for batch 3430 : 0.19275720417499542\n",
      "Training loss for batch 3431 : 0.08112813532352448\n",
      "Training loss for batch 3432 : 0.10888689011335373\n",
      "Training loss for batch 3433 : 0.06160502880811691\n",
      "Training loss for batch 3434 : 0.024916181340813637\n",
      "Training loss for batch 3435 : 0.2098262459039688\n",
      "Training loss for batch 3436 : 0.22805726528167725\n",
      "Training loss for batch 3437 : 0.0717998519539833\n",
      "Training loss for batch 3438 : 0.17218995094299316\n",
      "Training loss for batch 3439 : 0.10786290466785431\n",
      "Training loss for batch 3440 : 0.020771266892552376\n",
      "Training loss for batch 3441 : 0.09222637116909027\n",
      "Training loss for batch 3442 : 0.11900093406438828\n",
      "Training loss for batch 3443 : 0.14471878111362457\n",
      "Training loss for batch 3444 : 0.12996944785118103\n",
      "Training loss for batch 3445 : 0.25299209356307983\n",
      "Training loss for batch 3446 : 0.047820087522268295\n",
      "Training loss for batch 3447 : 0.09961332380771637\n",
      "Training loss for batch 3448 : 0.0914994552731514\n",
      "Training loss for batch 3449 : 0.09652404487133026\n",
      "Training loss for batch 3450 : 0.09303060173988342\n",
      "Training loss for batch 3451 : 0.08756601810455322\n",
      "Training loss for batch 3452 : 0.05096845701336861\n",
      "Training loss for batch 3453 : 0.20656076073646545\n",
      "Training loss for batch 3454 : 0.1735139787197113\n",
      "Training loss for batch 3455 : 0.22838392853736877\n",
      "Training loss for batch 3456 : 0.021254532039165497\n",
      "Training loss for batch 3457 : 0.07449226826429367\n",
      "Training loss for batch 3458 : 0.07696376740932465\n",
      "Training loss for batch 3459 : 0.1027582660317421\n",
      "Training loss for batch 3460 : 0.045337703078985214\n",
      "Training loss for batch 3461 : 0.026256227865815163\n",
      "Training loss for batch 3462 : 0.1771947145462036\n",
      "Training loss for batch 3463 : 0.08387206494808197\n",
      "Training loss for batch 3464 : 0.02282072976231575\n",
      "Training loss for batch 3465 : 0.20521149039268494\n",
      "Training loss for batch 3466 : 0.1694755256175995\n",
      "Training loss for batch 3467 : 0.053989477455616\n",
      "Training loss for batch 3468 : 0.03245626762509346\n",
      "Training loss for batch 3469 : 0.15596778690814972\n",
      "Training loss for batch 3470 : 0.108612060546875\n",
      "Training loss for batch 3471 : 0.107673779129982\n",
      "Training loss for batch 3472 : 0.05268460139632225\n",
      "Training loss for batch 3473 : 0.20568712055683136\n",
      "Training loss for batch 3474 : 0.08979924023151398\n",
      "Training loss for batch 3475 : 0.08637930452823639\n",
      "Training loss for batch 3476 : 0.04770132526755333\n",
      "Training loss for batch 3477 : 0.02725912630558014\n",
      "Training loss for batch 3478 : 0.06207801029086113\n",
      "Training loss for batch 3479 : 0.1169833242893219\n",
      "Training loss for batch 3480 : 0.05387603119015694\n",
      "Training loss for batch 3481 : 0.040859900414943695\n",
      "Training loss for batch 3482 : 0.10229551792144775\n",
      "Training loss for batch 3483 : 0.18238160014152527\n",
      "Training loss for batch 3484 : 0.027892502024769783\n",
      "Training loss for batch 3485 : 0.014288750477135181\n",
      "Training loss for batch 3486 : 0.03563917800784111\n",
      "Training loss for batch 3487 : 0.06898778676986694\n",
      "Training loss for batch 3488 : 0.05786055698990822\n",
      "Training loss for batch 3489 : 0.10442207753658295\n",
      "Training loss for batch 3490 : 0.05900314450263977\n",
      "Training loss for batch 3491 : 0.01000248733907938\n",
      "Training loss for batch 3492 : 0.1704757958650589\n",
      "Training loss for batch 3493 : 0.03022567182779312\n",
      "Training loss for batch 3494 : 0.08879294991493225\n",
      "Training loss for batch 3495 : 0.03658110648393631\n",
      "Training loss for batch 3496 : 0.03962070867419243\n",
      "Training loss for batch 3497 : 0.12822234630584717\n",
      "Training loss for batch 3498 : 0.11339009553194046\n",
      "Training loss for batch 3499 : 0.163296177983284\n",
      "Training loss for batch 3500 : 0.3415710926055908\n",
      "Training loss for batch 3501 : 0.1361308991909027\n",
      "Training loss for batch 3502 : 0.21475978195667267\n",
      "Training loss for batch 3503 : 0.023701678961515427\n",
      "Training loss for batch 3504 : 0.018231792375445366\n",
      "Training loss for batch 3505 : 0.05229642614722252\n",
      "Training loss for batch 3506 : 0.11055433005094528\n",
      "Training loss for batch 3507 : 0.11665594577789307\n",
      "Training loss for batch 3508 : 0.005757525097578764\n",
      "Training loss for batch 3509 : 0.07817842811346054\n",
      "Training loss for batch 3510 : 0.07005561143159866\n",
      "Training loss for batch 3511 : 0.22507232427597046\n",
      "Training loss for batch 3512 : 0.18891292810440063\n",
      "Training loss for batch 3513 : 0.17173904180526733\n",
      "Training loss for batch 3514 : 0.07907180488109589\n",
      "Training loss for batch 3515 : 0.05346241220831871\n",
      "Training loss for batch 3516 : 0.093301922082901\n",
      "Training loss for batch 3517 : 0.12448094040155411\n",
      "Training loss for batch 3518 : 0.007060435600578785\n",
      "Training loss for batch 3519 : 0.24855980277061462\n",
      "Training loss for batch 3520 : 0.05802071839570999\n",
      "Training loss for batch 3521 : 0.05850573256611824\n",
      "Training loss for batch 3522 : 0.28104662895202637\n",
      "Training loss for batch 3523 : 0.007284765597432852\n",
      "Training loss for batch 3524 : 0.04371482506394386\n",
      "Training loss for batch 3525 : 0.10400735586881638\n",
      "Training loss for batch 3526 : 0.21321921050548553\n",
      "Training loss for batch 3527 : 0.11132337152957916\n",
      "Training loss for batch 3528 : 0.20599353313446045\n",
      "Training loss for batch 3529 : 0.14153437316417694\n",
      "Training loss for batch 3530 : 0.0\n",
      "Training loss for batch 3531 : 0.160308375954628\n",
      "Training loss for batch 3532 : 0.06522797793149948\n",
      "Training loss for batch 3533 : 0.10848236083984375\n",
      "Training loss for batch 3534 : 0.08014126121997833\n",
      "Training loss for batch 3535 : 0.05934174358844757\n",
      "Training loss for batch 3536 : 0.11329323053359985\n",
      "Training loss for batch 3537 : 0.06718699634075165\n",
      "Training loss for batch 3538 : 0.14332635700702667\n",
      "Training loss for batch 3539 : 0.14686311781406403\n",
      "Training loss for batch 3540 : 0.11273995786905289\n",
      "Training loss for batch 3541 : 0.045579694211483\n",
      "Training loss for batch 3542 : 0.1991286277770996\n",
      "Training loss for batch 3543 : 0.02574523352086544\n",
      "Training loss for batch 3544 : 0.025069963186979294\n",
      "Training loss for batch 3545 : 0.18318824470043182\n",
      "Training loss for batch 3546 : 0.056046660989522934\n",
      "Training loss for batch 3547 : 0.10390766710042953\n",
      "Training loss for batch 3548 : 0.1597149819135666\n",
      "Training loss for batch 3549 : 0.09026332944631577\n",
      "Training loss for batch 3550 : 0.12211037427186966\n",
      "Training loss for batch 3551 : 0.11337491124868393\n",
      "Training loss for batch 3552 : 0.03576790541410446\n",
      "Training loss for batch 3553 : 0.202288419008255\n",
      "Training loss for batch 3554 : 0.22969502210617065\n",
      "Training loss for batch 3555 : 0.022283107042312622\n",
      "Training loss for batch 3556 : 0.2709991931915283\n",
      "Training loss for batch 3557 : 0.016551826149225235\n",
      "Training loss for batch 3558 : 0.1439182162284851\n",
      "Training loss for batch 3559 : 0.06930441409349442\n",
      "Training loss for batch 3560 : 0.034129124134778976\n",
      "Training loss for batch 3561 : 0.08209041506052017\n",
      "Training loss for batch 3562 : 0.037502165883779526\n",
      "Training loss for batch 3563 : 0.15091969072818756\n",
      "Training loss for batch 3564 : 0.14861024916172028\n",
      "Training loss for batch 3565 : 0.053030502051115036\n",
      "Training loss for batch 3566 : 0.03119145892560482\n",
      "Training loss for batch 3567 : 0.09672312438488007\n",
      "Training loss for batch 3568 : 0.047710418701171875\n",
      "Training loss for batch 3569 : 0.1894511878490448\n",
      "Training loss for batch 3570 : 0.002678742166608572\n",
      "Training loss for batch 3571 : 0.1622045338153839\n",
      "Training loss for batch 3572 : 0.17880035936832428\n",
      "Training loss for batch 3573 : 0.15712998807430267\n",
      "Training loss for batch 3574 : 0.0861804410815239\n",
      "Training loss for batch 3575 : 0.13643674552440643\n",
      "Training loss for batch 3576 : 0.003959521185606718\n",
      "Training loss for batch 3577 : 0.01007152535021305\n",
      "Training loss for batch 3578 : 0.11000136286020279\n",
      "Training loss for batch 3579 : 0.013723037205636501\n",
      "Training loss for batch 3580 : 0.16925300657749176\n",
      "Training loss for batch 3581 : 0.20141717791557312\n",
      "Training loss for batch 3582 : 0.08937187492847443\n",
      "Training loss for batch 3583 : 0.055156368762254715\n",
      "Training loss for batch 3584 : 0.11124276369810104\n",
      "Training loss for batch 3585 : 0.22351504862308502\n",
      "Training loss for batch 3586 : 0.46156004071235657\n",
      "Training loss for batch 3587 : 0.025279121473431587\n",
      "Training loss for batch 3588 : 0.027462169528007507\n",
      "Training loss for batch 3589 : 0.11690830439329147\n",
      "Training loss for batch 3590 : 0.05387036129832268\n",
      "Training loss for batch 3591 : 0.14158958196640015\n",
      "Training loss for batch 3592 : 0.08611013740301132\n",
      "Training loss for batch 3593 : 0.22087480127811432\n",
      "Training loss for batch 3594 : 0.006644587032496929\n",
      "Training loss for batch 3595 : 0.021455073729157448\n",
      "Training loss for batch 3596 : 0.1887560784816742\n",
      "Training loss for batch 3597 : 0.05904519930481911\n",
      "Training loss for batch 3598 : 0.1060946136713028\n",
      "Training loss for batch 3599 : 0.03488106280565262\n",
      "Training loss for batch 3600 : 0.12379733473062515\n",
      "Training loss for batch 3601 : 0.11378989368677139\n",
      "Training loss for batch 3602 : 0.05737040564417839\n",
      "Training loss for batch 3603 : 0.17469733953475952\n",
      "Training loss for batch 3604 : 0.10043982416391373\n",
      "Training loss for batch 3605 : 0.056067369878292084\n",
      "Training loss for batch 3606 : 0.12931713461875916\n",
      "Training loss for batch 3607 : 0.0713934376835823\n",
      "Training loss for batch 3608 : 0.03007252886891365\n",
      "Training loss for batch 3609 : 0.04136465862393379\n",
      "Training loss for batch 3610 : 0.018628569319844246\n",
      "Training loss for batch 3611 : 0.026845887303352356\n",
      "Training loss for batch 3612 : 0.014768446795642376\n",
      "Training loss for batch 3613 : 0.17420586943626404\n",
      "Training loss for batch 3614 : 0.22697053849697113\n",
      "Training loss for batch 3615 : 0.08561402559280396\n",
      "Training loss for batch 3616 : 0.1579340547323227\n",
      "Training loss for batch 3617 : 0.051876503974199295\n",
      "Training loss for batch 3618 : 0.2679441273212433\n",
      "Training loss for batch 3619 : 0.049722880125045776\n",
      "Training loss for batch 3620 : 0.15189479291439056\n",
      "Training loss for batch 3621 : 0.22628960013389587\n",
      "Training loss for batch 3622 : 0.12603816390037537\n",
      "Training loss for batch 3623 : 0.0778479352593422\n",
      "Training loss for batch 3624 : 0.017847826704382896\n",
      "Training loss for batch 3625 : 0.053047552704811096\n",
      "Training loss for batch 3626 : 0.07903856784105301\n",
      "Training loss for batch 3627 : 0.004930153489112854\n",
      "Training loss for batch 3628 : 0.18329733610153198\n",
      "Training loss for batch 3629 : 0.15310783684253693\n",
      "Training loss for batch 3630 : 0.0038432106375694275\n",
      "Training loss for batch 3631 : 0.040792323648929596\n",
      "Training loss for batch 3632 : 0.011351518332958221\n",
      "Training loss for batch 3633 : 0.0\n",
      "Training loss for batch 3634 : 0.07411687821149826\n",
      "Training loss for batch 3635 : 0.15940724313259125\n",
      "Training loss for batch 3636 : 0.18545426428318024\n",
      "Training loss for batch 3637 : 0.0973113626241684\n",
      "Training loss for batch 3638 : 0.14538906514644623\n",
      "Training loss for batch 3639 : 0.01496826857328415\n",
      "Training loss for batch 3640 : 0.07019416242837906\n",
      "Training loss for batch 3641 : 0.12348684668540955\n",
      "Training loss for batch 3642 : 0.42717573046684265\n",
      "Training loss for batch 3643 : 0.03423422574996948\n",
      "Training loss for batch 3644 : 0.09398629516363144\n",
      "Training loss for batch 3645 : 0.08855552971363068\n",
      "Training loss for batch 3646 : 0.052801720798015594\n",
      "Training loss for batch 3647 : 0.2182929515838623\n",
      "Training loss for batch 3648 : 0.1003032848238945\n",
      "Training loss for batch 3649 : 0.20056626200675964\n",
      "Training loss for batch 3650 : 0.07904960960149765\n",
      "Training loss for batch 3651 : 0.2673015296459198\n",
      "Training loss for batch 3652 : 0.10870366543531418\n",
      "Training loss for batch 3653 : 0.17112997174263\n",
      "Training loss for batch 3654 : 0.15080301463603973\n",
      "Training loss for batch 3655 : 0.08722532540559769\n",
      "Training loss for batch 3656 : 0.024781545624136925\n",
      "Training loss for batch 3657 : 0.05152270570397377\n",
      "Training loss for batch 3658 : 0.16520936787128448\n",
      "Training loss for batch 3659 : 0.19135455787181854\n",
      "Training loss for batch 3660 : 0.22911527752876282\n",
      "Training loss for batch 3661 : 0.04103880003094673\n",
      "Training loss for batch 3662 : 0.025100842118263245\n",
      "Training loss for batch 3663 : 0.059065647423267365\n",
      "Training loss for batch 3664 : 0.04455690458416939\n",
      "Training loss for batch 3665 : 0.10300610959529877\n",
      "Training loss for batch 3666 : 0.07296627014875412\n",
      "Training loss for batch 3667 : 0.0488959364593029\n",
      "Training loss for batch 3668 : 0.1428983211517334\n",
      "Training loss for batch 3669 : 0.09547027945518494\n",
      "Training loss for batch 3670 : 0.020768340677022934\n",
      "Training loss for batch 3671 : 0.028459811583161354\n",
      "Training loss for batch 3672 : 0.1500808447599411\n",
      "Training loss for batch 3673 : 0.19606386125087738\n",
      "Training loss for batch 3674 : 0.06896235793828964\n",
      "Training loss for batch 3675 : 0.07348334044218063\n",
      "Training loss for batch 3676 : 0.12980210781097412\n",
      "Training loss for batch 3677 : 0.18095873296260834\n",
      "Training loss for batch 3678 : 0.0695609450340271\n",
      "Training loss for batch 3679 : 0.25074538588523865\n",
      "Training loss for batch 3680 : 0.15694162249565125\n",
      "Training loss for batch 3681 : 0.23787377774715424\n",
      "Training loss for batch 3682 : 0.09806936979293823\n",
      "Training loss for batch 3683 : 0.1304875761270523\n",
      "Training loss for batch 3684 : 0.020142588764429092\n",
      "Training loss for batch 3685 : 0.04815153777599335\n",
      "Training loss for batch 3686 : 0.08084617555141449\n",
      "Training loss for batch 3687 : 0.08386625349521637\n",
      "Training loss for batch 3688 : 0.06388912349939346\n",
      "Training loss for batch 3689 : 0.04941020905971527\n",
      "Training loss for batch 3690 : 0.28084903955459595\n",
      "Training loss for batch 3691 : 0.13546769320964813\n",
      "Training loss for batch 3692 : 0.12766693532466888\n",
      "Training loss for batch 3693 : 0.11108437180519104\n",
      "Training loss for batch 3694 : 0.04778175428509712\n",
      "Training loss for batch 3695 : 0.030430883169174194\n",
      "Training loss for batch 3696 : 0.17053064703941345\n",
      "Training loss for batch 3697 : 0.14789792895317078\n",
      "Training loss for batch 3698 : 0.2512660324573517\n",
      "Training loss for batch 3699 : 0.03437361121177673\n",
      "Training loss for batch 3700 : 0.053575549274683\n",
      "Training loss for batch 3701 : 0.01318697165697813\n",
      "Training loss for batch 3702 : 0.03800168260931969\n",
      "Training loss for batch 3703 : 0.025140997022390366\n",
      "Training loss for batch 3704 : 0.11774661391973495\n",
      "Training loss for batch 3705 : 0.15588827431201935\n",
      "Training loss for batch 3706 : 0.1790103167295456\n",
      "Training loss for batch 3707 : 0.012901987880468369\n",
      "Training loss for batch 3708 : 0.16463984549045563\n",
      "Training loss for batch 3709 : 0.05951317399740219\n",
      "Training loss for batch 3710 : 0.07594240456819534\n",
      "Training loss for batch 3711 : 0.04601321369409561\n",
      "Training loss for batch 3712 : 0.001042410614900291\n",
      "Training loss for batch 3713 : 1.2362567858303919e-08\n",
      "Training loss for batch 3714 : 0.06789084523916245\n",
      "Training loss for batch 3715 : 0.170403853058815\n",
      "Training loss for batch 3716 : 0.04281957447528839\n",
      "Training loss for batch 3717 : 0.16711752116680145\n",
      "Training loss for batch 3718 : 0.17344090342521667\n",
      "Training loss for batch 3719 : 0.03622645512223244\n",
      "Training loss for batch 3720 : 0.1286529302597046\n",
      "Training loss for batch 3721 : 0.13776519894599915\n",
      "Training loss for batch 3722 : 0.02605368010699749\n",
      "Training loss for batch 3723 : 0.19458633661270142\n",
      "Training loss for batch 3724 : 0.09543956816196442\n",
      "Training loss for batch 3725 : 0.031052974984049797\n",
      "Training loss for batch 3726 : 0.03992175683379173\n",
      "Training loss for batch 3727 : 0.09295673668384552\n",
      "Training loss for batch 3728 : 0.1616220325231552\n",
      "Training loss for batch 3729 : 0.11937025189399719\n",
      "Training loss for batch 3730 : 0.00028477029991336167\n",
      "Training loss for batch 3731 : 0.24497061967849731\n",
      "Training loss for batch 3732 : 0.0441410094499588\n",
      "Training loss for batch 3733 : 0.05319445580244064\n",
      "Training loss for batch 3734 : 0.12460027635097504\n",
      "Training loss for batch 3735 : 0.019458163529634476\n",
      "Training loss for batch 3736 : 0.01261546928435564\n",
      "Training loss for batch 3737 : 0.07126007229089737\n",
      "Training loss for batch 3738 : 0.0956493690609932\n",
      "Training loss for batch 3739 : 0.1348007619380951\n",
      "Training loss for batch 3740 : 0.14906908571720123\n",
      "Training loss for batch 3741 : 0.033138204365968704\n",
      "Training loss for batch 3742 : 0.06175707280635834\n",
      "Training loss for batch 3743 : 0.08136266469955444\n",
      "Training loss for batch 3744 : 0.20187994837760925\n",
      "Training loss for batch 3745 : 0.1761827915906906\n",
      "Training loss for batch 3746 : 0.1391177475452423\n",
      "Training loss for batch 3747 : 0.2284078449010849\n",
      "Training loss for batch 3748 : 0.17237289249897003\n",
      "Training loss for batch 3749 : 0.0892300084233284\n",
      "Training loss for batch 3750 : 0.06236128509044647\n",
      "Training loss for batch 3751 : 0.11400105804204941\n",
      "Training loss for batch 3752 : 0.016493167728185654\n",
      "Training loss for batch 3753 : 0.22416037321090698\n",
      "Training loss for batch 3754 : 0.056697338819503784\n",
      "Training loss for batch 3755 : 0.08179803192615509\n",
      "Training loss for batch 3756 : 0.06286057084798813\n",
      "Training loss for batch 3757 : 0.0873480960726738\n",
      "Training loss for batch 3758 : 0.058352768421173096\n",
      "Training loss for batch 3759 : 0.06401561945676804\n",
      "Training loss for batch 3760 : 0.08286931365728378\n",
      "Training loss for batch 3761 : 0.1361386775970459\n",
      "Training loss for batch 3762 : 0.18580619990825653\n",
      "Training loss for batch 3763 : 0.1588943600654602\n",
      "Training loss for batch 3764 : 0.13918301463127136\n",
      "Training loss for batch 3765 : 0.032803118228912354\n",
      "Training loss for batch 3766 : 0.12326779961585999\n",
      "Training loss for batch 3767 : 0.12673047184944153\n",
      "Training loss for batch 3768 : 0.2043651044368744\n",
      "Training loss for batch 3769 : 0.04911724105477333\n",
      "Training loss for batch 3770 : 0.1933477371931076\n",
      "Training loss for batch 3771 : 0.03493674844503403\n",
      "Training loss for batch 3772 : 0.042739011347293854\n",
      "Training loss for batch 3773 : 0.1823488026857376\n",
      "Training loss for batch 3774 : 0.12248511612415314\n",
      "Training loss for batch 3775 : 0.13333193957805634\n",
      "Training loss for batch 3776 : 0.06955162435770035\n",
      "Training loss for batch 3777 : 0.10702965408563614\n",
      "Training loss for batch 3778 : 0.036458343267440796\n",
      "Training loss for batch 3779 : 0.09593050926923752\n",
      "Training loss for batch 3780 : 0.043067947030067444\n",
      "Training loss for batch 3781 : 0.023085428401827812\n",
      "Training loss for batch 3782 : 0.026036689057946205\n",
      "Training loss for batch 3783 : 0.15019844472408295\n",
      "Training loss for batch 3784 : 0.08801507949829102\n",
      "Training loss for batch 3785 : 0.27647215127944946\n",
      "Training loss for batch 3786 : 0.09275803714990616\n",
      "Training loss for batch 3787 : 0.012354536913335323\n",
      "Training loss for batch 3788 : 0.2999480962753296\n",
      "Training loss for batch 3789 : 0.03798697143793106\n",
      "Training loss for batch 3790 : 0.07683530449867249\n",
      "Training loss for batch 3791 : 0.2030622512102127\n",
      "Training loss for batch 3792 : 0.22327449917793274\n",
      "Training loss for batch 3793 : 0.09568792581558228\n",
      "Training loss for batch 3794 : 0.20694252848625183\n",
      "Training loss for batch 3795 : 0.2699158489704132\n",
      "Training loss for batch 3796 : 0.00581806106492877\n",
      "Training loss for batch 3797 : 0.018849804997444153\n",
      "Training loss for batch 3798 : 0.19216111302375793\n",
      "Training loss for batch 3799 : 0.24367398023605347\n",
      "Training loss for batch 3800 : 0.13314524292945862\n",
      "Training loss for batch 3801 : 0.06072216480970383\n",
      "Training loss for batch 3802 : 0.02252495102584362\n",
      "Training loss for batch 3803 : 0.09664426743984222\n",
      "Training loss for batch 3804 : 0.2012566775083542\n",
      "Training loss for batch 3805 : 0.16129066050052643\n",
      "Training loss for batch 3806 : 0.1212245300412178\n",
      "Training loss for batch 3807 : 0.036162637174129486\n",
      "Training loss for batch 3808 : 0.047463029623031616\n",
      "Training loss for batch 3809 : 0.10221797972917557\n",
      "Training loss for batch 3810 : 0.19153840839862823\n",
      "Training loss for batch 3811 : 0.16780759394168854\n",
      "Training loss for batch 3812 : 0.020833127200603485\n",
      "Training loss for batch 3813 : 0.11721667647361755\n",
      "Training loss for batch 3814 : 0.08341813832521439\n",
      "Training loss for batch 3815 : 0.03175041824579239\n",
      "Training loss for batch 3816 : 0.11032804101705551\n",
      "Training loss for batch 3817 : 0.10226841270923615\n",
      "Training loss for batch 3818 : 0.07135102152824402\n",
      "Training loss for batch 3819 : 0.10081614553928375\n",
      "Training loss for batch 3820 : 0.07524426281452179\n",
      "Training loss for batch 3821 : 0.0868559330701828\n",
      "Training loss for batch 3822 : 0.1000363752245903\n",
      "Training loss for batch 3823 : 0.02987515553832054\n",
      "Training loss for batch 3824 : 0.12537379562854767\n",
      "Training loss for batch 3825 : 0.009456976316869259\n",
      "Training loss for batch 3826 : 0.1509348452091217\n",
      "Training loss for batch 3827 : 0.05848844721913338\n",
      "Training loss for batch 3828 : 0.04329922795295715\n",
      "Training loss for batch 3829 : 0.0832083448767662\n",
      "Training loss for batch 3830 : 0.03303021565079689\n",
      "Training loss for batch 3831 : 0.05359187722206116\n",
      "Training loss for batch 3832 : 0.030943306162953377\n",
      "Training loss for batch 3833 : 0.03437941148877144\n",
      "Training loss for batch 3834 : 0.13044174015522003\n",
      "Training loss for batch 3835 : 0.2004583328962326\n",
      "Training loss for batch 3836 : 0.029376914724707603\n",
      "Training loss for batch 3837 : 0.02337561920285225\n",
      "Training loss for batch 3838 : 0.2903123199939728\n",
      "Training loss for batch 3839 : 0.0\n",
      "Training loss for batch 3840 : 0.18226128816604614\n",
      "Training loss for batch 3841 : 0.2259572297334671\n",
      "Training loss for batch 3842 : 0.10967609286308289\n",
      "Training loss for batch 3843 : 0.06690124422311783\n",
      "Training loss for batch 3844 : 0.0833766907453537\n",
      "Training loss for batch 3845 : 0.14526055753231049\n",
      "Training loss for batch 3846 : 0.08532872051000595\n",
      "Training loss for batch 3847 : 0.0939892828464508\n",
      "Training loss for batch 3848 : 0.08813876658678055\n",
      "Training loss for batch 3849 : 0.1488487273454666\n",
      "Training loss for batch 3850 : 0.23385179042816162\n",
      "Training loss for batch 3851 : 0.05139169469475746\n",
      "Training loss for batch 3852 : 0.1932622641324997\n",
      "Training loss for batch 3853 : 0.2264261692762375\n",
      "Training loss for batch 3854 : 0.033126894384622574\n",
      "Training loss for batch 3855 : 0.019773637875914574\n",
      "Training loss for batch 3856 : 0.2316213697195053\n",
      "Training loss for batch 3857 : 0.06774035841226578\n",
      "Training loss for batch 3858 : 0.04099319875240326\n",
      "Training loss for batch 3859 : 0.23440487682819366\n",
      "Training loss for batch 3860 : 0.0713203102350235\n",
      "Training loss for batch 3861 : 0.08919790387153625\n",
      "Training loss for batch 3862 : 0.01669183000922203\n",
      "Training loss for batch 3863 : 0.09964747726917267\n",
      "Training loss for batch 3864 : 0.0731557309627533\n",
      "Training loss for batch 3865 : 0.051704127341508865\n",
      "Training loss for batch 3866 : 0.07067149132490158\n",
      "Training loss for batch 3867 : 0.09921706467866898\n",
      "Training loss for batch 3868 : 0.059885311871767044\n",
      "Training loss for batch 3869 : 0.336906373500824\n",
      "Training loss for batch 3870 : 0.09868315607309341\n",
      "Training loss for batch 3871 : 0.1607898771762848\n",
      "Training loss for batch 3872 : 0.19582903385162354\n",
      "Training loss for batch 3873 : 0.11393066495656967\n",
      "Training loss for batch 3874 : 0.048497360199689865\n",
      "Training loss for batch 3875 : 0.31694620847702026\n",
      "Training loss for batch 3876 : 0.18952088057994843\n",
      "Training loss for batch 3877 : 0.2274937778711319\n",
      "Training loss for batch 3878 : 0.04848901554942131\n",
      "Training loss for batch 3879 : 0.10063201189041138\n",
      "Training loss for batch 3880 : 0.029023513197898865\n",
      "Training loss for batch 3881 : 0.18960189819335938\n",
      "Training loss for batch 3882 : 0.17846596240997314\n",
      "Training loss for batch 3883 : 0.04546733945608139\n",
      "Training loss for batch 3884 : 0.022356292232871056\n",
      "Training loss for batch 3885 : 0.05627401918172836\n",
      "Training loss for batch 3886 : 0.029917148873209953\n",
      "Training loss for batch 3887 : 0.08769585937261581\n",
      "Training loss for batch 3888 : 0.046428240835666656\n",
      "Training loss for batch 3889 : 0.22550763189792633\n",
      "Training loss for batch 3890 : 0.20612600445747375\n",
      "Training loss for batch 3891 : 0.10660481452941895\n",
      "Training loss for batch 3892 : 0.04637150838971138\n",
      "Training loss for batch 3893 : 4.497224814770107e-09\n",
      "Training loss for batch 3894 : 0.010624123737215996\n",
      "Training loss for batch 3895 : 0.0689643993973732\n",
      "Training loss for batch 3896 : 0.07921665906906128\n",
      "Training loss for batch 3897 : 0.14320828020572662\n",
      "Training loss for batch 3898 : 0.13282738626003265\n",
      "Training loss for batch 3899 : 0.10404946655035019\n",
      "Training loss for batch 3900 : 0.1836734563112259\n",
      "Training loss for batch 3901 : 0.11940646171569824\n",
      "Training loss for batch 3902 : 0.01654926687479019\n",
      "Training loss for batch 3903 : 0.09512247145175934\n",
      "Training loss for batch 3904 : 0.061732128262519836\n",
      "Training loss for batch 3905 : 0.18263226747512817\n",
      "Training loss for batch 3906 : 0.3005240261554718\n",
      "Training loss for batch 3907 : 0.14632639288902283\n",
      "Training loss for batch 3908 : 0.057655274868011475\n",
      "Training loss for batch 3909 : 0.0035879663191735744\n",
      "Training loss for batch 3910 : 0.044409528374671936\n",
      "Training loss for batch 3911 : 0.11344446986913681\n",
      "Training loss for batch 3912 : 0.1707664430141449\n",
      "Training loss for batch 3913 : 0.16533643007278442\n",
      "Training loss for batch 3914 : 0.04563106223940849\n",
      "Training loss for batch 3915 : 0.024963781237602234\n",
      "Training loss for batch 3916 : 0.10128019005060196\n",
      "Training loss for batch 3917 : 0.02855711616575718\n",
      "Training loss for batch 3918 : 0.15824849903583527\n",
      "Training loss for batch 3919 : 0.08559668809175491\n",
      "Training loss for batch 3920 : 0.11962772905826569\n",
      "Training loss for batch 3921 : 0.06791120022535324\n",
      "Training loss for batch 3922 : 0.14655587077140808\n",
      "Training loss for batch 3923 : 0.06272578239440918\n",
      "Training loss for batch 3924 : 0.20598874986171722\n",
      "Training loss for batch 3925 : 0.27061790227890015\n",
      "Training loss for batch 3926 : 0.07549033313989639\n",
      "Training loss for batch 3927 : 0.3045707941055298\n",
      "Training loss for batch 3928 : 0.2381712645292282\n",
      "Training loss for batch 3929 : 0.21032606065273285\n",
      "Training loss for batch 3930 : 0.19214516878128052\n",
      "Training loss for batch 3931 : 0.18271982669830322\n",
      "Training loss for batch 3932 : 0.05888686329126358\n",
      "Training loss for batch 3933 : 0.005063822027295828\n",
      "Training loss for batch 3934 : 0.1687123328447342\n",
      "Training loss for batch 3935 : 0.1494390219449997\n",
      "Training loss for batch 3936 : 0.08909208327531815\n",
      "Training loss for batch 3937 : 0.13560421764850616\n",
      "Training loss for batch 3938 : 0.107052281498909\n",
      "Training loss for batch 3939 : 0.15700770914554596\n",
      "Training loss for batch 3940 : 0.28900787234306335\n",
      "Training loss for batch 3941 : 0.10590066015720367\n",
      "Training loss for batch 3942 : 0.05787699669599533\n",
      "Training loss for batch 3943 : 0.07024606317281723\n",
      "Training loss for batch 3944 : 0.03336964175105095\n",
      "Training loss for batch 3945 : 0.14140886068344116\n",
      "Training loss for batch 3946 : 0.04688391089439392\n",
      "Training loss for batch 3947 : 0.03513225167989731\n",
      "Training loss for batch 3948 : 0.4487183690071106\n",
      "Training loss for batch 3949 : 0.1165245994925499\n",
      "Training loss for batch 3950 : 0.21126332879066467\n",
      "Training loss for batch 3951 : 0.18419139087200165\n",
      "Training loss for batch 3952 : 0.167838454246521\n",
      "Training loss for batch 3953 : 0.15283572673797607\n",
      "Training loss for batch 3954 : 0.018273070454597473\n",
      "Training loss for batch 3955 : 0.11115482449531555\n",
      "Training loss for batch 3956 : 0.08178766816854477\n",
      "Training loss for batch 3957 : 0.05519861727952957\n",
      "Training loss for batch 3958 : 0.1049765795469284\n",
      "Training loss for batch 3959 : 0.10028783977031708\n",
      "Training loss for batch 3960 : 0.08433165401220322\n",
      "Training loss for batch 3961 : 0.0739390179514885\n",
      "Training loss for batch 3962 : 0.12035879492759705\n",
      "Training loss for batch 3963 : 0.12269242852926254\n",
      "Training loss for batch 3964 : 0.23199962079524994\n",
      "Training loss for batch 3965 : 0.22853684425354004\n",
      "Training loss for batch 3966 : 0.2088192254304886\n",
      "Training loss for batch 3967 : 0.0010741651058197021\n",
      "Training loss for batch 3968 : 0.031168919056653976\n",
      "Training loss for batch 3969 : 0.054685816168785095\n",
      "Training loss for batch 3970 : 0.1925315111875534\n",
      "Training loss for batch 3971 : 0.051859404891729355\n",
      "Training loss for batch 3972 : 0.11622034758329391\n",
      "Training loss for batch 3973 : 0.147539883852005\n",
      "Training loss for batch 3974 : 0.13170795142650604\n",
      "Training loss for batch 3975 : 0.16803255677223206\n",
      "Training loss for batch 3976 : 0.047604937106370926\n",
      "Training loss for batch 3977 : 0.1982419788837433\n",
      "Training loss for batch 3978 : 0.11592697352170944\n",
      "Training loss for batch 3979 : 0.21544408798217773\n",
      "Training loss for batch 3980 : 0.03496600687503815\n",
      "Training loss for batch 3981 : 0.2056725025177002\n",
      "Training loss for batch 3982 : 0.22874456644058228\n",
      "Training loss for batch 3983 : 0.17326343059539795\n",
      "Training loss for batch 3984 : 0.06928586214780807\n",
      "Training loss for batch 3985 : 0.03929607570171356\n",
      "Training loss for batch 3986 : 0.12957531213760376\n",
      "Training loss for batch 3987 : 0.07479466497898102\n",
      "Training loss for batch 3988 : 0.05293554812669754\n",
      "Training loss for batch 3989 : 0.18209850788116455\n",
      "Training loss for batch 3990 : 0.012840691953897476\n",
      "Training loss for batch 3991 : 0.1485821008682251\n",
      "Training loss for batch 3992 : 0.14639988541603088\n",
      "Training loss for batch 3993 : 0.031484317034482956\n",
      "Training loss for batch 3994 : 0.1803063601255417\n",
      "Training loss for batch 3995 : 0.13479307293891907\n",
      "Training loss for batch 3996 : 0.09534081071615219\n",
      "Training loss for batch 3997 : 0.10753641277551651\n",
      "Training loss for batch 3998 : 0.13450723886489868\n",
      "Training loss for batch 3999 : 0.08120452612638474\n",
      "Training loss for batch 4000 : 0.09461712092161179\n",
      "Training loss for batch 4001 : 0.011867198161780834\n",
      "Training loss for batch 4002 : 0.21576493978500366\n",
      "Training loss for batch 4003 : 0.027824779972434044\n",
      "Training loss for batch 4004 : 0.07348360121250153\n",
      "Training loss for batch 4005 : 0.20291218161582947\n",
      "Training loss for batch 4006 : 0.04752987623214722\n",
      "Training loss for batch 4007 : 0.10138242691755295\n",
      "Training loss for batch 4008 : 0.030583810061216354\n",
      "Training loss for batch 4009 : 0.053742390125989914\n",
      "Training loss for batch 4010 : 0.012317778542637825\n",
      "Training loss for batch 4011 : 0.06749936193227768\n",
      "Training loss for batch 4012 : 0.01621788926422596\n",
      "Training loss for batch 4013 : 0.32276663184165955\n",
      "Training loss for batch 4014 : 0.07516758888959885\n",
      "Training loss for batch 4015 : 0.0950709730386734\n",
      "Training loss for batch 4016 : 0.06168236583471298\n",
      "Training loss for batch 4017 : 0.06893132627010345\n",
      "Training loss for batch 4018 : 0.0342998169362545\n",
      "Training loss for batch 4019 : 0.119937464594841\n",
      "Training loss for batch 4020 : 0.06378685683012009\n",
      "Training loss for batch 4021 : 0.044305864721536636\n",
      "Training loss for batch 4022 : 0.003971716854721308\n",
      "Training loss for batch 4023 : 0.05937950313091278\n",
      "Training loss for batch 4024 : 0.10002696514129639\n",
      "Training loss for batch 4025 : 0.09496524930000305\n",
      "Training loss for batch 4026 : 0.04693672060966492\n",
      "Training loss for batch 4027 : 0.01287313923239708\n",
      "Training loss for batch 4028 : 0.06466779857873917\n",
      "Training loss for batch 4029 : 0.10895276814699173\n",
      "Training loss for batch 4030 : 0.053243692964315414\n",
      "Training loss for batch 4031 : 0.11087719351053238\n",
      "Training loss for batch 4032 : 0.11388558149337769\n",
      "Training loss for batch 4033 : 0.10353700816631317\n",
      "Training loss for batch 4034 : 0.1447756290435791\n",
      "Training loss for batch 4035 : 0.02025139145553112\n",
      "Training loss for batch 4036 : 0.1531444489955902\n",
      "Training loss for batch 4037 : 0.056822244077920914\n",
      "Training loss for batch 4038 : 0.0764199048280716\n",
      "Training loss for batch 4039 : 0.17824800312519073\n",
      "Training loss for batch 4040 : 0.18833385407924652\n",
      "Training loss for batch 4041 : 0.08641321212053299\n",
      "Training loss for batch 4042 : 0.13134358823299408\n",
      "Training loss for batch 4043 : 0.03398078307509422\n",
      "Training loss for batch 4044 : 0.16208231449127197\n",
      "Training loss for batch 4045 : 0.06077684462070465\n",
      "Training loss for batch 4046 : 0.14490070939064026\n",
      "Training loss for batch 4047 : 0.188465416431427\n",
      "Training loss for batch 4048 : 0.03853593021631241\n",
      "Training loss for batch 4049 : 0.1496630162000656\n",
      "Training loss for batch 4050 : 0.12927675247192383\n",
      "Training loss for batch 4051 : 0.25757232308387756\n",
      "Training loss for batch 4052 : 0.11328738927841187\n",
      "Training loss for batch 4053 : 0.14970090985298157\n",
      "Training loss for batch 4054 : 0.09005026519298553\n",
      "Training loss for batch 4055 : 0.22256462275981903\n",
      "Training loss for batch 4056 : 0.22196821868419647\n",
      "Training loss for batch 4057 : 0.24225398898124695\n",
      "Training loss for batch 4058 : 0.04956180229783058\n",
      "Training loss for batch 4059 : 0.24823474884033203\n",
      "Training loss for batch 4060 : 0.09855981171131134\n",
      "Training loss for batch 4061 : 0.048465803265571594\n",
      "Training loss for batch 4062 : 0.19377441704273224\n",
      "Training loss for batch 4063 : 0.1290239840745926\n",
      "Training loss for batch 4064 : 0.22519654035568237\n",
      "Training loss for batch 4065 : 0.0875929743051529\n",
      "Training loss for batch 4066 : 0.04417818784713745\n",
      "Training loss for batch 4067 : 0.0017995914677157998\n",
      "Training loss for batch 4068 : 0.09502948075532913\n",
      "Training loss for batch 4069 : 0.05977947264909744\n",
      "Training loss for batch 4070 : 0.21055611968040466\n",
      "Training loss for batch 4071 : 0.29009512066841125\n",
      "Training loss for batch 4072 : 0.034657228738069534\n",
      "Training loss for batch 4073 : 0.09564665704965591\n",
      "Training loss for batch 4074 : 0.06339766085147858\n",
      "Training loss for batch 4075 : 0.03797517716884613\n",
      "Training loss for batch 4076 : 0.10179082304239273\n",
      "Training loss for batch 4077 : 0.07276634126901627\n",
      "Training loss for batch 4078 : 0.15405301749706268\n",
      "Training loss for batch 4079 : 0.10243380814790726\n",
      "Training loss for batch 4080 : 0.1302986592054367\n",
      "Training loss for batch 4081 : 0.09202010184526443\n",
      "Training loss for batch 4082 : 0.0674864649772644\n",
      "Training loss for batch 4083 : 0.13275058567523956\n",
      "Training loss for batch 4084 : 0.06085328757762909\n",
      "Training loss for batch 4085 : 0.134421244263649\n",
      "Training loss for batch 4086 : 0.10711342096328735\n",
      "Training loss for batch 4087 : 0.35200485587120056\n",
      "Training loss for batch 4088 : 0.2596128582954407\n",
      "Training loss for batch 4089 : 0.01184979546815157\n",
      "Training loss for batch 4090 : 0.002617417136207223\n",
      "Training loss for batch 4091 : 0.14270876348018646\n",
      "Training loss for batch 4092 : 0.10705608874559402\n",
      "Training loss for batch 4093 : 0.10744361579418182\n",
      "Training loss for batch 4094 : 0.1521535962820053\n",
      "Training loss for batch 4095 : 0.1748000830411911\n",
      "Training loss for batch 4096 : 0.08404861390590668\n",
      "Training loss for batch 4097 : 0.06354472786188126\n",
      "Training loss for batch 4098 : 0.09774752706289291\n",
      "Training loss for batch 4099 : 0.10425649583339691\n",
      "Training loss for batch 4100 : 0.11049939692020416\n",
      "Training loss for batch 4101 : 0.2559495270252228\n",
      "Training loss for batch 4102 : 0.06765365600585938\n",
      "Training loss for batch 4103 : 0.028544722124934196\n",
      "Training loss for batch 4104 : 0.10474824160337448\n",
      "Training loss for batch 4105 : 0.01971360296010971\n",
      "Training loss for batch 4106 : 0.08211792260408401\n",
      "Training loss for batch 4107 : 0.05827118456363678\n",
      "Training loss for batch 4108 : 0.10527582466602325\n",
      "Training loss for batch 4109 : 0.023470578715205193\n",
      "Training loss for batch 4110 : 0.014308424666523933\n",
      "Training loss for batch 4111 : 0.1559465527534485\n",
      "Training loss for batch 4112 : 0.10166451334953308\n",
      "Training loss for batch 4113 : 0.03868338465690613\n",
      "Training loss for batch 4114 : 0.10769493132829666\n",
      "Training loss for batch 4115 : 0.028432153165340424\n",
      "Training loss for batch 4116 : 0.20895585417747498\n",
      "Training loss for batch 4117 : 0.13169173896312714\n",
      "Training loss for batch 4118 : 0.09882906079292297\n",
      "Training loss for batch 4119 : 0.19998697936534882\n",
      "Training loss for batch 4120 : 0.05609143525362015\n",
      "Training loss for batch 4121 : 0.07725734263658524\n",
      "Training loss for batch 4122 : 0.013857855461537838\n",
      "Training loss for batch 4123 : 0.18572074174880981\n",
      "Training loss for batch 4124 : 0.07755953818559647\n",
      "Training loss for batch 4125 : 0.0742926150560379\n",
      "Training loss for batch 4126 : 0.07065986841917038\n",
      "Training loss for batch 4127 : 0.08509766310453415\n",
      "Training loss for batch 4128 : 0.0725826770067215\n",
      "Training loss for batch 4129 : 0.10110817849636078\n",
      "Training loss for batch 4130 : 0.1209879070520401\n",
      "Training loss for batch 4131 : 0.014277126640081406\n",
      "Training loss for batch 4132 : 0.22578683495521545\n",
      "Training loss for batch 4133 : 0.12478964775800705\n",
      "Training loss for batch 4134 : 0.022068684920668602\n",
      "Training loss for batch 4135 : 0.01893279142677784\n",
      "Training loss for batch 4136 : 0.22258464992046356\n",
      "Training loss for batch 4137 : 0.09139131009578705\n",
      "Training loss for batch 4138 : 0.0869792178273201\n",
      "Training loss for batch 4139 : 0.06767027825117111\n",
      "Training loss for batch 4140 : 0.09778475016355515\n",
      "Training loss for batch 4141 : 0.07185356318950653\n",
      "Training loss for batch 4142 : 0.09383586794137955\n",
      "Training loss for batch 4143 : 0.12739786505699158\n",
      "Training loss for batch 4144 : 0.28242313861846924\n",
      "Training loss for batch 4145 : 0.1681009978055954\n",
      "Training loss for batch 4146 : 0.1860993504524231\n",
      "Training loss for batch 4147 : 0.11308447271585464\n",
      "Training loss for batch 4148 : 0.04128022491931915\n",
      "Training loss for batch 4149 : 0.08788452297449112\n",
      "Training loss for batch 4150 : 0.043481938540935516\n",
      "Training loss for batch 4151 : 0.05285349115729332\n",
      "Training loss for batch 4152 : 0.04868236929178238\n",
      "Training loss for batch 4153 : 0.005832422524690628\n",
      "Parameter containing:\n",
      "tensor(-0.2614, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.0000100135803223\n",
      "Training loss for batch 1 : 1.0000100135803223\n",
      "Training loss for batch 2 : 1.0000098943710327\n",
      "Training loss for batch 3 : 1.0000100135803223\n",
      "Training loss for batch 4 : 1.0000100135803223\n",
      "Training loss for batch 5 : 1.0000100135803223\n",
      "Training loss for batch 6 : 1.0000100135803223\n",
      "Training loss for batch 7 : 1.0000100135803223\n",
      "Training loss for batch 8 : 1.0000100135803223\n",
      "Training loss for batch 9 : 1.0000100135803223\n",
      "Training loss for batch 10 : 1.0000100135803223\n",
      "Training loss for batch 11 : 1.0000100135803223\n",
      "Training loss for batch 12 : 0.17729532718658447\n",
      "Training loss for batch 13 : 0.044619329273700714\n",
      "Training loss for batch 14 : 0.06791601330041885\n",
      "Training loss for batch 15 : 0.23084861040115356\n",
      "Training loss for batch 16 : 0.0243210569024086\n",
      "Training loss for batch 17 : 0.05887545272707939\n",
      "Training loss for batch 18 : 0.0034910119138658047\n",
      "Training loss for batch 19 : 0.17803527414798737\n",
      "Training loss for batch 20 : 0.005397913511842489\n",
      "Training loss for batch 21 : 0.08506691455841064\n",
      "Training loss for batch 22 : 0.22195281088352203\n",
      "Training loss for batch 23 : 0.044736623764038086\n",
      "Training loss for batch 24 : 0.3445364832878113\n",
      "Training loss for batch 25 : 0.029593218117952347\n",
      "Training loss for batch 26 : 0.15929648280143738\n",
      "Training loss for batch 27 : 0.16434210538864136\n",
      "Training loss for batch 28 : 0.02725846879184246\n",
      "Training loss for batch 29 : 0.12024681270122528\n",
      "Training loss for batch 30 : 0.03477463126182556\n",
      "Training loss for batch 31 : 0.1561039388179779\n",
      "Training loss for batch 32 : 0.12429417669773102\n",
      "Training loss for batch 33 : 0.01248198002576828\n",
      "Training loss for batch 34 : 0.026489930227398872\n",
      "Training loss for batch 35 : 0.13596810400485992\n",
      "Training loss for batch 36 : 0.18390648066997528\n",
      "Training loss for batch 37 : 0.052987441420555115\n",
      "Training loss for batch 38 : 0.07530053704977036\n",
      "Training loss for batch 39 : 0.04452604800462723\n",
      "Training loss for batch 40 : 0.10444797575473785\n",
      "Training loss for batch 41 : 0.03623244911432266\n",
      "Training loss for batch 42 : 0.1752312332391739\n",
      "Training loss for batch 43 : 0.044381093233823776\n",
      "Training loss for batch 44 : 0.02730235457420349\n",
      "Training loss for batch 45 : 0.03136206790804863\n",
      "Training loss for batch 46 : 0.0050757997669279575\n",
      "Training loss for batch 47 : 0.0397346056997776\n",
      "Training loss for batch 48 : 0.0007976416382007301\n",
      "Training loss for batch 49 : 0.011438685469329357\n",
      "Training loss for batch 50 : 0.05963168293237686\n",
      "Training loss for batch 51 : 0.0005729157710447907\n",
      "Training loss for batch 52 : 0.03740379959344864\n",
      "Training loss for batch 53 : 0.08142121881246567\n",
      "Training loss for batch 54 : 0.054807838052511215\n",
      "Training loss for batch 55 : 0.05211670324206352\n",
      "Training loss for batch 56 : 0.03505546599626541\n",
      "Training loss for batch 57 : 0.08054831624031067\n",
      "Training loss for batch 58 : 0.13210280239582062\n",
      "Training loss for batch 59 : 0.006782010663300753\n",
      "Training loss for batch 60 : 0.199122354388237\n",
      "Training loss for batch 61 : 0.06272704154253006\n",
      "Training loss for batch 62 : 0.02807152271270752\n",
      "Training loss for batch 63 : 0.08270993083715439\n",
      "Training loss for batch 64 : 0.0037106650415807962\n",
      "Training loss for batch 65 : 0.05360649153590202\n",
      "Training loss for batch 66 : 0.07618926465511322\n",
      "Training loss for batch 67 : 0.10107051581144333\n",
      "Training loss for batch 68 : 0.12013396620750427\n",
      "Training loss for batch 69 : 0.122522734105587\n",
      "Training loss for batch 70 : 0.059978920966386795\n",
      "Training loss for batch 71 : 0.026361413300037384\n",
      "Training loss for batch 72 : 0.07971742004156113\n",
      "Training loss for batch 73 : 0.07146579027175903\n",
      "Training loss for batch 74 : 0.076424241065979\n",
      "Training loss for batch 75 : 0.05008726194500923\n",
      "Training loss for batch 76 : 0.01637893170118332\n",
      "Training loss for batch 77 : 0.07775680720806122\n",
      "Training loss for batch 78 : 0.09241790324449539\n",
      "Training loss for batch 79 : 0.060767628252506256\n",
      "Training loss for batch 80 : 0.08482443541288376\n",
      "Training loss for batch 81 : 0.1749059408903122\n",
      "Training loss for batch 82 : 0.1041572093963623\n",
      "Training loss for batch 83 : 0.19318526983261108\n",
      "Training loss for batch 84 : 0.15001782774925232\n",
      "Training loss for batch 85 : 0.09972771257162094\n",
      "Training loss for batch 86 : 0.05896357074379921\n",
      "Training loss for batch 87 : 0.056099068373441696\n",
      "Training loss for batch 88 : 0.05354085564613342\n",
      "Training loss for batch 89 : 0.09288720786571503\n",
      "Training loss for batch 90 : 0.035914622247219086\n",
      "Training loss for batch 91 : 0.09192007035017014\n",
      "Training loss for batch 92 : 0.036672789603471756\n",
      "Training loss for batch 93 : 0.21321330964565277\n",
      "Training loss for batch 94 : 0.19091998040676117\n",
      "Training loss for batch 95 : 0.11583691090345383\n",
      "Training loss for batch 96 : 0.12110204994678497\n",
      "Training loss for batch 97 : 0.018437830731272697\n",
      "Training loss for batch 98 : 0.06336922198534012\n",
      "Training loss for batch 99 : 0.12854114174842834\n",
      "Training loss for batch 100 : 0.10315793752670288\n",
      "Training loss for batch 101 : 0.06850165873765945\n",
      "Training loss for batch 102 : 0.03673124685883522\n",
      "Training loss for batch 103 : 5.676479730709616e-09\n",
      "Training loss for batch 104 : 0.13038834929466248\n",
      "Training loss for batch 105 : 0.01198743935674429\n",
      "Training loss for batch 106 : 0.0625990629196167\n",
      "Training loss for batch 107 : 0.1019795686006546\n",
      "Training loss for batch 108 : 0.18093274533748627\n",
      "Training loss for batch 109 : 0.11609922349452972\n",
      "Training loss for batch 110 : 0.14988715946674347\n",
      "Training loss for batch 111 : 0.10304614901542664\n",
      "Training loss for batch 112 : 0.07077273726463318\n",
      "Training loss for batch 113 : 0.1840711534023285\n",
      "Training loss for batch 114 : 0.05501707270741463\n",
      "Training loss for batch 115 : 0.03886561095714569\n",
      "Training loss for batch 116 : 0.18953365087509155\n",
      "Training loss for batch 117 : 0.06769546121358871\n",
      "Training loss for batch 118 : 0.41307514905929565\n",
      "Training loss for batch 119 : 0.14519447088241577\n",
      "Training loss for batch 120 : 0.008504725061357021\n",
      "Training loss for batch 121 : 0.031430844217538834\n",
      "Training loss for batch 122 : 0.10744225233793259\n",
      "Training loss for batch 123 : 0.11279300600290298\n",
      "Training loss for batch 124 : 0.012926925905048847\n",
      "Training loss for batch 125 : 0.05334807187318802\n",
      "Training loss for batch 126 : 0.022226549685001373\n",
      "Training loss for batch 127 : 0.05548057705163956\n",
      "Training loss for batch 128 : 0.09833529591560364\n",
      "Training loss for batch 129 : 0.21114017069339752\n",
      "Training loss for batch 130 : 0.0421159565448761\n",
      "Training loss for batch 131 : 0.06663499772548676\n",
      "Training loss for batch 132 : 0.13437925279140472\n",
      "Training loss for batch 133 : 0.08522217720746994\n",
      "Training loss for batch 134 : 0.1943456381559372\n",
      "Training loss for batch 135 : 0.1001078188419342\n",
      "Training loss for batch 136 : 0.027493005618453026\n",
      "Training loss for batch 137 : 0.09469294548034668\n",
      "Training loss for batch 138 : 0.00583262275904417\n",
      "Training loss for batch 139 : 0.09481435269117355\n",
      "Training loss for batch 140 : 0.09031333029270172\n",
      "Training loss for batch 141 : 0.15988901257514954\n",
      "Training loss for batch 142 : 0.07524049282073975\n",
      "Training loss for batch 143 : 0.16289588809013367\n",
      "Training loss for batch 144 : 0.09040962159633636\n",
      "Training loss for batch 145 : 0.05455496534705162\n",
      "Training loss for batch 146 : 0.06381172686815262\n",
      "Training loss for batch 147 : 0.04756919667124748\n",
      "Training loss for batch 148 : 0.02557563968002796\n",
      "Training loss for batch 149 : 0.033658467233181\n",
      "Training loss for batch 150 : 0.09773808717727661\n",
      "Training loss for batch 151 : 0.03063136711716652\n",
      "Training loss for batch 152 : 0.03196494281291962\n",
      "Training loss for batch 153 : 0.14705345034599304\n",
      "Training loss for batch 154 : 0.1229519471526146\n",
      "Training loss for batch 155 : 0.019822923466563225\n",
      "Training loss for batch 156 : 0.03801986575126648\n",
      "Training loss for batch 157 : 0.06546376645565033\n",
      "Training loss for batch 158 : 0.0077165113762021065\n",
      "Training loss for batch 159 : 0.15101224184036255\n",
      "Training loss for batch 160 : 0.004318729043006897\n",
      "Training loss for batch 161 : 0.13803404569625854\n",
      "Training loss for batch 162 : 0.1206195130944252\n",
      "Training loss for batch 163 : 0.13127481937408447\n",
      "Training loss for batch 164 : 0.07703178375959396\n",
      "Training loss for batch 165 : 0.07306109368801117\n",
      "Training loss for batch 166 : 0.027243738994002342\n",
      "Training loss for batch 167 : 0.08665888756513596\n",
      "Training loss for batch 168 : 0.05059272423386574\n",
      "Training loss for batch 169 : 0.11434450000524521\n",
      "Training loss for batch 170 : 0.020565642043948174\n",
      "Training loss for batch 171 : 0.1784505546092987\n",
      "Training loss for batch 172 : 0.029147887602448463\n",
      "Training loss for batch 173 : 0.10301028192043304\n",
      "Training loss for batch 174 : 0.06006450578570366\n",
      "Training loss for batch 175 : 0.13640891015529633\n",
      "Training loss for batch 176 : 0.0028986481484025717\n",
      "Training loss for batch 177 : 0.1260843724012375\n",
      "Training loss for batch 178 : 0.08750569820404053\n",
      "Training loss for batch 179 : 0.05453645437955856\n",
      "Training loss for batch 180 : 0.005414570681750774\n",
      "Training loss for batch 181 : 0.11876902729272842\n",
      "Training loss for batch 182 : 0.11232075840234756\n",
      "Training loss for batch 183 : 0.08657532185316086\n",
      "Training loss for batch 184 : 0.09726010262966156\n",
      "Training loss for batch 185 : 0.021777518093585968\n",
      "Training loss for batch 186 : 0.051654260605573654\n",
      "Training loss for batch 187 : 0.07249534130096436\n",
      "Training loss for batch 188 : 0.07377507537603378\n",
      "Training loss for batch 189 : 0.13968580961227417\n",
      "Training loss for batch 190 : 0.11854861676692963\n",
      "Training loss for batch 191 : 0.1647128015756607\n",
      "Training loss for batch 192 : 0.004477908369153738\n",
      "Training loss for batch 193 : 0.023466896265745163\n",
      "Training loss for batch 194 : 0.03745083883404732\n",
      "Training loss for batch 195 : 0.19833514094352722\n",
      "Training loss for batch 196 : 0.05915091559290886\n",
      "Training loss for batch 197 : 0.09037654101848602\n",
      "Training loss for batch 198 : 0.18300417065620422\n",
      "Training loss for batch 199 : 0.07122685760259628\n",
      "Training loss for batch 200 : 0.05759323388338089\n",
      "Training loss for batch 201 : 0.07240446656942368\n",
      "Training loss for batch 202 : 0.07378146797418594\n",
      "Training loss for batch 203 : 0.06597130000591278\n",
      "Training loss for batch 204 : 0.07028844207525253\n",
      "Training loss for batch 205 : 0.011435452848672867\n",
      "Training loss for batch 206 : 0.24819223582744598\n",
      "Training loss for batch 207 : 0.03006853722035885\n",
      "Training loss for batch 208 : 0.05216803774237633\n",
      "Training loss for batch 209 : 0.10742470622062683\n",
      "Training loss for batch 210 : 0.1701584905385971\n",
      "Training loss for batch 211 : 0.11126688122749329\n",
      "Training loss for batch 212 : 0.08860257267951965\n",
      "Training loss for batch 213 : 0.04473906382918358\n",
      "Training loss for batch 214 : 0.24834337830543518\n",
      "Training loss for batch 215 : 0.052440620958805084\n",
      "Training loss for batch 216 : 0.11404688656330109\n",
      "Training loss for batch 217 : 0.047919463366270065\n",
      "Training loss for batch 218 : 0.047818489372730255\n",
      "Training loss for batch 219 : 0.010635726153850555\n",
      "Training loss for batch 220 : 0.1878713071346283\n",
      "Training loss for batch 221 : 0.06815097481012344\n",
      "Training loss for batch 222 : 0.021438322961330414\n",
      "Training loss for batch 223 : 0.06982721388339996\n",
      "Training loss for batch 224 : 0.01089780405163765\n",
      "Training loss for batch 225 : 0.09532755613327026\n",
      "Training loss for batch 226 : 0.10121490061283112\n",
      "Training loss for batch 227 : 0.052681148052215576\n",
      "Training loss for batch 228 : 0.041160814464092255\n",
      "Training loss for batch 229 : 0.07894333451986313\n",
      "Training loss for batch 230 : 0.029524633660912514\n",
      "Training loss for batch 231 : 0.23078413307666779\n",
      "Training loss for batch 232 : 0.09496436268091202\n",
      "Training loss for batch 233 : 0.02186707779765129\n",
      "Training loss for batch 234 : 0.0932069793343544\n",
      "Training loss for batch 235 : 0.13451643288135529\n",
      "Training loss for batch 236 : 0.12004067003726959\n",
      "Training loss for batch 237 : 0.08982863277196884\n",
      "Training loss for batch 238 : 0.124931201338768\n",
      "Training loss for batch 239 : 0.0892891213297844\n",
      "Training loss for batch 240 : 0.013770676217973232\n",
      "Training loss for batch 241 : 0.0758032500743866\n",
      "Training loss for batch 242 : 0.21744735538959503\n",
      "Training loss for batch 243 : 0.017847275361418724\n",
      "Training loss for batch 244 : 0.04961089417338371\n",
      "Training loss for batch 245 : 0.09744932502508163\n",
      "Training loss for batch 246 : 0.016340631991624832\n",
      "Training loss for batch 247 : 0.04505461826920509\n",
      "Training loss for batch 248 : 0.0417456217110157\n",
      "Training loss for batch 249 : 0.06132395938038826\n",
      "Training loss for batch 250 : 0.005443203262984753\n",
      "Training loss for batch 251 : 0.052036527544260025\n",
      "Training loss for batch 252 : 0.09294218569993973\n",
      "Training loss for batch 253 : 0.06314944475889206\n",
      "Training loss for batch 254 : 0.23871777951717377\n",
      "Training loss for batch 255 : 0.039205215871334076\n",
      "Training loss for batch 256 : 0.10154440999031067\n",
      "Training loss for batch 257 : 0.06262462586164474\n",
      "Training loss for batch 258 : 0.025388767942786217\n",
      "Training loss for batch 259 : 0.07427386939525604\n",
      "Training loss for batch 260 : 0.0379168763756752\n",
      "Training loss for batch 261 : 0.06722669303417206\n",
      "Training loss for batch 262 : 0.09048677235841751\n",
      "Training loss for batch 263 : 0.13839316368103027\n",
      "Training loss for batch 264 : 0.043594665825366974\n",
      "Training loss for batch 265 : 0.11783166229724884\n",
      "Training loss for batch 266 : 0.024057995527982712\n",
      "Training loss for batch 267 : 0.05329793319106102\n",
      "Training loss for batch 268 : 0.056342702358961105\n",
      "Training loss for batch 269 : 0.0365675613284111\n",
      "Training loss for batch 270 : 0.08258398622274399\n",
      "Training loss for batch 271 : 0.02272757887840271\n",
      "Training loss for batch 272 : 0.14108939468860626\n",
      "Training loss for batch 273 : 0.05208371952176094\n",
      "Training loss for batch 274 : 0.10448967665433884\n",
      "Training loss for batch 275 : 0.10451976954936981\n",
      "Training loss for batch 276 : 0.0005169709911569953\n",
      "Training loss for batch 277 : 0.06520944088697433\n",
      "Training loss for batch 278 : 0.08343429118394852\n",
      "Training loss for batch 279 : 0.17146910727024078\n",
      "Training loss for batch 280 : 0.06076379865407944\n",
      "Training loss for batch 281 : 0.023735595867037773\n",
      "Training loss for batch 282 : 0.006208489183336496\n",
      "Training loss for batch 283 : 0.11559711396694183\n",
      "Training loss for batch 284 : 0.1956767588853836\n",
      "Training loss for batch 285 : 0.1716591864824295\n",
      "Training loss for batch 286 : 0.10927598923444748\n",
      "Training loss for batch 287 : 0.08916100114583969\n",
      "Training loss for batch 288 : 0.05092195048928261\n",
      "Training loss for batch 289 : 0.09901022166013718\n",
      "Training loss for batch 290 : 0.05325241759419441\n",
      "Training loss for batch 291 : 0.022316742688417435\n",
      "Training loss for batch 292 : 0.07946571707725525\n",
      "Training loss for batch 293 : 0.08102860301733017\n",
      "Training loss for batch 294 : 0.15454545617103577\n",
      "Training loss for batch 295 : 0.10884521901607513\n",
      "Training loss for batch 296 : 0.043952107429504395\n",
      "Training loss for batch 297 : 0.05726802721619606\n",
      "Training loss for batch 298 : 0.10767638683319092\n",
      "Training loss for batch 299 : 0.111853688955307\n",
      "Training loss for batch 300 : 0.030336927622556686\n",
      "Training loss for batch 301 : 0.07255534082651138\n",
      "Training loss for batch 302 : 0.20718854665756226\n",
      "Training loss for batch 303 : 0.11612971872091293\n",
      "Training loss for batch 304 : 0.0923212468624115\n",
      "Training loss for batch 305 : 0.1421329230070114\n",
      "Training loss for batch 306 : 0.1535319685935974\n",
      "Training loss for batch 307 : 0.026804639026522636\n",
      "Training loss for batch 308 : 0.12766452133655548\n",
      "Training loss for batch 309 : 0.12816397845745087\n",
      "Training loss for batch 310 : 0.054365552961826324\n",
      "Training loss for batch 311 : 0.1398046910762787\n",
      "Training loss for batch 312 : 0.013419217430055141\n",
      "Training loss for batch 313 : 0.11995765566825867\n",
      "Training loss for batch 314 : 0.14903207123279572\n",
      "Training loss for batch 315 : 0.06386358290910721\n",
      "Training loss for batch 316 : 0.06030230224132538\n",
      "Training loss for batch 317 : 0.05985056608915329\n",
      "Training loss for batch 318 : 0.04425479471683502\n",
      "Training loss for batch 319 : 0.11220298707485199\n",
      "Training loss for batch 320 : 0.08202535659074783\n",
      "Training loss for batch 321 : 0.1988144814968109\n",
      "Training loss for batch 322 : 0.1494881808757782\n",
      "Training loss for batch 323 : 0.020525606349110603\n",
      "Training loss for batch 324 : 0.08653812855482101\n",
      "Training loss for batch 325 : 0.07834678888320923\n",
      "Training loss for batch 326 : 0.04419371858239174\n",
      "Training loss for batch 327 : 0.02270941250026226\n",
      "Training loss for batch 328 : 0.13054485619068146\n",
      "Training loss for batch 329 : 0.24847649037837982\n",
      "Training loss for batch 330 : 0.06348491460084915\n",
      "Training loss for batch 331 : 0.076524518430233\n",
      "Training loss for batch 332 : 0.07681916654109955\n",
      "Training loss for batch 333 : 0.03379306569695473\n",
      "Training loss for batch 334 : 0.053713977336883545\n",
      "Training loss for batch 335 : 0.05994764715433121\n",
      "Training loss for batch 336 : 0.13229884207248688\n",
      "Training loss for batch 337 : 0.023466169834136963\n",
      "Training loss for batch 338 : 0.1504427045583725\n",
      "Training loss for batch 339 : 0.02289881370961666\n",
      "Training loss for batch 340 : 0.16060571372509003\n",
      "Training loss for batch 341 : 0.12905316054821014\n",
      "Training loss for batch 342 : 0.03760763630270958\n",
      "Training loss for batch 343 : 0.022306697443127632\n",
      "Training loss for batch 344 : 0.02842108905315399\n",
      "Training loss for batch 345 : 0.05764959380030632\n",
      "Training loss for batch 346 : 0.16010239720344543\n",
      "Training loss for batch 347 : 0.04740753024816513\n",
      "Training loss for batch 348 : 0.018476318567991257\n",
      "Training loss for batch 349 : 0.043598588556051254\n",
      "Training loss for batch 350 : 0.06426741927862167\n",
      "Training loss for batch 351 : 0.0679067000746727\n",
      "Training loss for batch 352 : 0.10353779792785645\n",
      "Training loss for batch 353 : 0.08070409297943115\n",
      "Training loss for batch 354 : 0.05584524944424629\n",
      "Training loss for batch 355 : 0.1551918089389801\n",
      "Training loss for batch 356 : 0.08859162032604218\n",
      "Training loss for batch 357 : 0.01907923072576523\n",
      "Training loss for batch 358 : 0.07582644373178482\n",
      "Training loss for batch 359 : 0.031971175223588943\n",
      "Training loss for batch 360 : 0.10294950008392334\n",
      "Training loss for batch 361 : 0.030800962820649147\n",
      "Training loss for batch 362 : 0.1259077489376068\n",
      "Training loss for batch 363 : 0.07692156732082367\n",
      "Training loss for batch 364 : 0.16125497221946716\n",
      "Training loss for batch 365 : 0.18165305256843567\n",
      "Training loss for batch 366 : 0.053517382591962814\n",
      "Training loss for batch 367 : 0.019786100834608078\n",
      "Training loss for batch 368 : 0.025166427716612816\n",
      "Training loss for batch 369 : 0.19307619333267212\n",
      "Training loss for batch 370 : 0.12303473800420761\n",
      "Training loss for batch 371 : 0.10584177076816559\n",
      "Training loss for batch 372 : 0.18243271112442017\n",
      "Training loss for batch 373 : 0.17009484767913818\n",
      "Training loss for batch 374 : 0.10875780880451202\n",
      "Training loss for batch 375 : 0.043623026460409164\n",
      "Training loss for batch 376 : 0.0919819176197052\n",
      "Training loss for batch 377 : 0.026959236711263657\n",
      "Training loss for batch 378 : 0.040271032601594925\n",
      "Training loss for batch 379 : 0.01529874186962843\n",
      "Training loss for batch 380 : 0.09833656996488571\n",
      "Training loss for batch 381 : 0.024211039766669273\n",
      "Training loss for batch 382 : 0.08800488710403442\n",
      "Training loss for batch 383 : 0.12171719968318939\n",
      "Training loss for batch 384 : 0.012829447165131569\n",
      "Training loss for batch 385 : 0.087253637611866\n",
      "Training loss for batch 386 : 0.007262793835252523\n",
      "Training loss for batch 387 : 0.11123377829790115\n",
      "Training loss for batch 388 : 0.08831591159105301\n",
      "Training loss for batch 389 : 0.11507081985473633\n",
      "Training loss for batch 390 : 0.11098642647266388\n",
      "Training loss for batch 391 : 0.012392560951411724\n",
      "Training loss for batch 392 : 0.018511448055505753\n",
      "Training loss for batch 393 : 0.029615139588713646\n",
      "Training loss for batch 394 : 0.08128578960895538\n",
      "Training loss for batch 395 : 0.06758260726928711\n",
      "Training loss for batch 396 : 0.07689803838729858\n",
      "Training loss for batch 397 : 0.21804659068584442\n",
      "Training loss for batch 398 : 0.12145622819662094\n",
      "Training loss for batch 399 : 0.05252721533179283\n",
      "Training loss for batch 400 : 0.0285604540258646\n",
      "Training loss for batch 401 : 0.07549235224723816\n",
      "Training loss for batch 402 : 0.16116294264793396\n",
      "Training loss for batch 403 : 0.061949074268341064\n",
      "Training loss for batch 404 : 0.04264140501618385\n",
      "Training loss for batch 405 : 0.0\n",
      "Training loss for batch 406 : 0.03905869647860527\n",
      "Training loss for batch 407 : 0.006636185571551323\n",
      "Training loss for batch 408 : 0.012767666019499302\n",
      "Training loss for batch 409 : 0.06580327451229095\n",
      "Training loss for batch 410 : 0.031895291060209274\n",
      "Training loss for batch 411 : 0.04083060100674629\n",
      "Training loss for batch 412 : 0.1009591817855835\n",
      "Training loss for batch 413 : 0.011065252125263214\n",
      "Training loss for batch 414 : 0.16694582998752594\n",
      "Training loss for batch 415 : 0.24022172391414642\n",
      "Training loss for batch 416 : 0.032734520733356476\n",
      "Training loss for batch 417 : 0.20950768887996674\n",
      "Training loss for batch 418 : 0.0005353299784474075\n",
      "Training loss for batch 419 : 0.08011800050735474\n",
      "Training loss for batch 420 : 0.09129846096038818\n",
      "Training loss for batch 421 : 0.07608965039253235\n",
      "Training loss for batch 422 : 0.06320609152317047\n",
      "Training loss for batch 423 : 0.08870603144168854\n",
      "Training loss for batch 424 : 0.08287932723760605\n",
      "Training loss for batch 425 : 0.07107774168252945\n",
      "Training loss for batch 426 : 0.037674855440855026\n",
      "Training loss for batch 427 : 0.0011633634567260742\n",
      "Training loss for batch 428 : 0.09257377684116364\n",
      "Training loss for batch 429 : 0.019930709153413773\n",
      "Training loss for batch 430 : 0.07217457890510559\n",
      "Training loss for batch 431 : 0.07605168223381042\n",
      "Training loss for batch 432 : 0.10612796247005463\n",
      "Training loss for batch 433 : 0.10890871286392212\n",
      "Training loss for batch 434 : 0.06780949234962463\n",
      "Training loss for batch 435 : 0.06918563693761826\n",
      "Training loss for batch 436 : 0.06125110760331154\n",
      "Training loss for batch 437 : 0.01602838933467865\n",
      "Training loss for batch 438 : 0.04157394543290138\n",
      "Training loss for batch 439 : 0.1070605218410492\n",
      "Training loss for batch 440 : 0.0620402991771698\n",
      "Training loss for batch 441 : 0.1766788810491562\n",
      "Training loss for batch 442 : 0.0408402681350708\n",
      "Training loss for batch 443 : 0.0396314300596714\n",
      "Training loss for batch 444 : 0.030589954927563667\n",
      "Training loss for batch 445 : 0.08071697503328323\n",
      "Training loss for batch 446 : 0.18351511657238007\n",
      "Training loss for batch 447 : 0.04039172828197479\n",
      "Training loss for batch 448 : 0.1359098106622696\n",
      "Training loss for batch 449 : 0.01535184308886528\n",
      "Training loss for batch 450 : 0.03832938149571419\n",
      "Training loss for batch 451 : 0.13708490133285522\n",
      "Training loss for batch 452 : 0.03241419419646263\n",
      "Training loss for batch 453 : 0.10726117342710495\n",
      "Training loss for batch 454 : 0.07451754063367844\n",
      "Training loss for batch 455 : 0.10354256629943848\n",
      "Training loss for batch 456 : 0.08528009057044983\n",
      "Training loss for batch 457 : 0.04033612832427025\n",
      "Training loss for batch 458 : 0.08064807951450348\n",
      "Training loss for batch 459 : 0.04712171480059624\n",
      "Training loss for batch 460 : 0.002022650558501482\n",
      "Training loss for batch 461 : 0.02000255696475506\n",
      "Training loss for batch 462 : 0.011022944934666157\n",
      "Training loss for batch 463 : 0.08059006184339523\n",
      "Training loss for batch 464 : 0.06524404138326645\n",
      "Training loss for batch 465 : 0.01964792236685753\n",
      "Training loss for batch 466 : 0.02871309593319893\n",
      "Training loss for batch 467 : 0.06778176128864288\n",
      "Training loss for batch 468 : 0.02171853557229042\n",
      "Training loss for batch 469 : 0.08499325811862946\n",
      "Training loss for batch 470 : 0.06767673045396805\n",
      "Training loss for batch 471 : 0.21912053227424622\n",
      "Training loss for batch 472 : 0.03190936893224716\n",
      "Training loss for batch 473 : 0.09247785806655884\n",
      "Training loss for batch 474 : 0.015455045737326145\n",
      "Training loss for batch 475 : 0.10606937855482101\n",
      "Training loss for batch 476 : 0.031118161976337433\n",
      "Training loss for batch 477 : 0.05402897670865059\n",
      "Training loss for batch 478 : 0.18673719465732574\n",
      "Training loss for batch 479 : 0.1578374207019806\n",
      "Training loss for batch 480 : 0.0657772347331047\n",
      "Training loss for batch 481 : 0.07256975769996643\n",
      "Training loss for batch 482 : 0.12053435295820236\n",
      "Training loss for batch 483 : 0.0409950353205204\n",
      "Training loss for batch 484 : 0.05437999218702316\n",
      "Training loss for batch 485 : 0.07223125547170639\n",
      "Training loss for batch 486 : 0.047635287046432495\n",
      "Training loss for batch 487 : 0.1318603903055191\n",
      "Training loss for batch 488 : 0.020237896591424942\n",
      "Training loss for batch 489 : 0.05178942158818245\n",
      "Training loss for batch 490 : 0.1532268226146698\n",
      "Training loss for batch 491 : 0.07399476319551468\n",
      "Training loss for batch 492 : 0.05832594633102417\n",
      "Training loss for batch 493 : 0.08753123879432678\n",
      "Training loss for batch 494 : 0.13184650242328644\n",
      "Training loss for batch 495 : 0.07368826866149902\n",
      "Training loss for batch 496 : 0.027640370652079582\n",
      "Training loss for batch 497 : 0.16373835504055023\n",
      "Training loss for batch 498 : 0.11129199713468552\n",
      "Training loss for batch 499 : 0.030443884432315826\n",
      "Training loss for batch 500 : 0.07730695605278015\n",
      "Training loss for batch 501 : 0.12824007868766785\n",
      "Training loss for batch 502 : 0.20364344120025635\n",
      "Training loss for batch 503 : 0.15995140373706818\n",
      "Training loss for batch 504 : 0.03596329689025879\n",
      "Training loss for batch 505 : 0.03746192902326584\n",
      "Training loss for batch 506 : 0.013407601974904537\n",
      "Training loss for batch 507 : 0.11497287452220917\n",
      "Training loss for batch 508 : 0.08938227593898773\n",
      "Training loss for batch 509 : 0.007044624071568251\n",
      "Training loss for batch 510 : 0.043724581599235535\n",
      "Training loss for batch 511 : 0.10411909222602844\n",
      "Training loss for batch 512 : 0.06288350373506546\n",
      "Training loss for batch 513 : 0.07524406909942627\n",
      "Training loss for batch 514 : 0.057613521814346313\n",
      "Training loss for batch 515 : 0.0366225428879261\n",
      "Training loss for batch 516 : 0.07336410880088806\n",
      "Training loss for batch 517 : 0.14059458673000336\n",
      "Training loss for batch 518 : 0.0001298879215028137\n",
      "Training loss for batch 519 : 0.053254272788763046\n",
      "Training loss for batch 520 : 0.18140307068824768\n",
      "Training loss for batch 521 : 0.05936918780207634\n",
      "Training loss for batch 522 : 0.1992630958557129\n",
      "Training loss for batch 523 : 0.04418071359395981\n",
      "Training loss for batch 524 : 0.18362438678741455\n",
      "Training loss for batch 525 : 0.053955260664224625\n",
      "Training loss for batch 526 : 0.05122248828411102\n",
      "Training loss for batch 527 : 0.005525756161659956\n",
      "Training loss for batch 528 : 0.004837114829570055\n",
      "Training loss for batch 529 : 0.04084780067205429\n",
      "Training loss for batch 530 : 0.21482601761817932\n",
      "Training loss for batch 531 : 0.0891333669424057\n",
      "Training loss for batch 532 : 0.1746257245540619\n",
      "Training loss for batch 533 : 0.018217815086245537\n",
      "Training loss for batch 534 : 0.08506407588720322\n",
      "Training loss for batch 535 : 0.016913753002882004\n",
      "Training loss for batch 536 : 0.14211049675941467\n",
      "Training loss for batch 537 : 0.08729905635118484\n",
      "Training loss for batch 538 : 0.08142861723899841\n",
      "Training loss for batch 539 : 0.09388010948896408\n",
      "Training loss for batch 540 : 0.08511492609977722\n",
      "Training loss for batch 541 : 0.15218766033649445\n",
      "Training loss for batch 542 : 0.06525172293186188\n",
      "Training loss for batch 543 : 0.08534184843301773\n",
      "Training loss for batch 544 : 0.01705487072467804\n",
      "Training loss for batch 545 : 0.002927422523498535\n",
      "Training loss for batch 546 : 0.017064442858099937\n",
      "Training loss for batch 547 : 0.19879792630672455\n",
      "Training loss for batch 548 : 0.08916999399662018\n",
      "Training loss for batch 549 : 0.08868105709552765\n",
      "Training loss for batch 550 : 0.15461961925029755\n",
      "Training loss for batch 551 : 0.02956976182758808\n",
      "Training loss for batch 552 : 0.13973596692085266\n",
      "Training loss for batch 553 : 0.044835414737463\n",
      "Training loss for batch 554 : 0.1361703723669052\n",
      "Training loss for batch 555 : 0.11757691949605942\n",
      "Training loss for batch 556 : 0.035092901438474655\n",
      "Training loss for batch 557 : 0.030413448810577393\n",
      "Training loss for batch 558 : 0.0292934849858284\n",
      "Training loss for batch 559 : 0.057279687374830246\n",
      "Training loss for batch 560 : 0.09701532870531082\n",
      "Training loss for batch 561 : 0.1079438179731369\n",
      "Training loss for batch 562 : 0.1437060534954071\n",
      "Training loss for batch 563 : 0.049433302134275436\n",
      "Training loss for batch 564 : 0.06004033610224724\n",
      "Training loss for batch 565 : 0.08959656208753586\n",
      "Training loss for batch 566 : 0.008475132286548615\n",
      "Training loss for batch 567 : 0.1032976359128952\n",
      "Training loss for batch 568 : 0.034426502883434296\n",
      "Training loss for batch 569 : 0.06723929941654205\n",
      "Training loss for batch 570 : 0.05757245048880577\n",
      "Training loss for batch 571 : 0.15559570491313934\n",
      "Training loss for batch 572 : 0.016849631443619728\n",
      "Training loss for batch 573 : 0.06663460284471512\n",
      "Training loss for batch 574 : 0.05050057917833328\n",
      "Training loss for batch 575 : 0.013372371904551983\n",
      "Training loss for batch 576 : 0.03132055699825287\n",
      "Training loss for batch 577 : 0.11118648201227188\n",
      "Training loss for batch 578 : 0.2606056034564972\n",
      "Training loss for batch 579 : 0.15721963346004486\n",
      "Training loss for batch 580 : 0.20073117315769196\n",
      "Training loss for batch 581 : 0.09215253591537476\n",
      "Training loss for batch 582 : 0.08837427198886871\n",
      "Training loss for batch 583 : 0.057855620980262756\n",
      "Training loss for batch 584 : 0.1451740562915802\n",
      "Training loss for batch 585 : 0.06735032796859741\n",
      "Training loss for batch 586 : 0.0991293266415596\n",
      "Training loss for batch 587 : 0.0513172522187233\n",
      "Training loss for batch 588 : 0.06422704458236694\n",
      "Training loss for batch 589 : 0.09259770810604095\n",
      "Training loss for batch 590 : 0.10633362829685211\n",
      "Training loss for batch 591 : 0.08453235775232315\n",
      "Training loss for batch 592 : 0.022566072642803192\n",
      "Training loss for batch 593 : 0.027789287269115448\n",
      "Training loss for batch 594 : 0.11608359217643738\n",
      "Training loss for batch 595 : 0.03687003254890442\n",
      "Training loss for batch 596 : 0.08947978913784027\n",
      "Training loss for batch 597 : 0.10166087746620178\n",
      "Training loss for batch 598 : 0.01170741580426693\n",
      "Training loss for batch 599 : 0.11020613461732864\n",
      "Training loss for batch 600 : 0.06038505211472511\n",
      "Training loss for batch 601 : 0.11468473821878433\n",
      "Training loss for batch 602 : 0.04339849203824997\n",
      "Training loss for batch 603 : 0.15503142774105072\n",
      "Training loss for batch 604 : 0.029787009581923485\n",
      "Training loss for batch 605 : 0.021603379398584366\n",
      "Training loss for batch 606 : 0.11108386516571045\n",
      "Training loss for batch 607 : 0.16183100640773773\n",
      "Training loss for batch 608 : 0.07066546380519867\n",
      "Training loss for batch 609 : 0.05927475914359093\n",
      "Training loss for batch 610 : 0.10802992433309555\n",
      "Training loss for batch 611 : 0.16639560461044312\n",
      "Training loss for batch 612 : 0.12096694856882095\n",
      "Training loss for batch 613 : 0.05600658059120178\n",
      "Training loss for batch 614 : 0.013231410644948483\n",
      "Training loss for batch 615 : 0.03089057095348835\n",
      "Training loss for batch 616 : 0.03779727965593338\n",
      "Training loss for batch 617 : 0.15239720046520233\n",
      "Training loss for batch 618 : 0.028667468577623367\n",
      "Training loss for batch 619 : 0.07738558202981949\n",
      "Training loss for batch 620 : 0.0006638368358835578\n",
      "Training loss for batch 621 : 0.023751763626933098\n",
      "Training loss for batch 622 : 0.0834474191069603\n",
      "Training loss for batch 623 : 0.05917494744062424\n",
      "Training loss for batch 624 : 0.21627676486968994\n",
      "Training loss for batch 625 : 0.055537376552820206\n",
      "Training loss for batch 626 : 0.07047711312770844\n",
      "Training loss for batch 627 : 0.041424889117479324\n",
      "Training loss for batch 628 : 0.03740973398089409\n",
      "Training loss for batch 629 : 0.17132112383842468\n",
      "Training loss for batch 630 : 0.16799047589302063\n",
      "Training loss for batch 631 : 0.07054875791072845\n",
      "Training loss for batch 632 : 0.11106623709201813\n",
      "Training loss for batch 633 : 0.2103634625673294\n",
      "Training loss for batch 634 : 0.02739332988858223\n",
      "Training loss for batch 635 : 0.0691225528717041\n",
      "Training loss for batch 636 : 0.010866990312933922\n",
      "Training loss for batch 637 : 0.021157504990696907\n",
      "Training loss for batch 638 : 0.18432751297950745\n",
      "Training loss for batch 639 : 0.019664093852043152\n",
      "Training loss for batch 640 : 0.052281029522418976\n",
      "Training loss for batch 641 : 0.10665654391050339\n",
      "Training loss for batch 642 : 0.045354634523391724\n",
      "Training loss for batch 643 : 0.07488155364990234\n",
      "Training loss for batch 644 : 0.04167596623301506\n",
      "Training loss for batch 645 : 0.1089426651597023\n",
      "Training loss for batch 646 : 0.06489810347557068\n",
      "Training loss for batch 647 : 0.08910714834928513\n",
      "Training loss for batch 648 : 0.09499452263116837\n",
      "Training loss for batch 649 : 0.06321045756340027\n",
      "Training loss for batch 650 : 0.17484402656555176\n",
      "Training loss for batch 651 : 0.017853153869509697\n",
      "Training loss for batch 652 : 0.047812994569540024\n",
      "Training loss for batch 653 : 0.0564928874373436\n",
      "Training loss for batch 654 : 0.06302624195814133\n",
      "Training loss for batch 655 : 0.053868312388658524\n",
      "Training loss for batch 656 : 0.10307613015174866\n",
      "Training loss for batch 657 : 0.021593092009425163\n",
      "Training loss for batch 658 : 0.13966114819049835\n",
      "Training loss for batch 659 : 0.009958583861589432\n",
      "Training loss for batch 660 : 0.21340562403202057\n",
      "Training loss for batch 661 : 0.026629850268363953\n",
      "Training loss for batch 662 : 0.10647928714752197\n",
      "Training loss for batch 663 : 0.1159205511212349\n",
      "Training loss for batch 664 : 0.16995762288570404\n",
      "Training loss for batch 665 : 0.09502533078193665\n",
      "Training loss for batch 666 : 0.013851344585418701\n",
      "Training loss for batch 667 : 0.08559078723192215\n",
      "Training loss for batch 668 : 0.02259141206741333\n",
      "Training loss for batch 669 : 0.02699870429933071\n",
      "Training loss for batch 670 : 0.028299488127231598\n",
      "Training loss for batch 671 : 0.026754792779684067\n",
      "Training loss for batch 672 : 0.054549820721149445\n",
      "Training loss for batch 673 : 0.03376934304833412\n",
      "Training loss for batch 674 : 0.0685136541724205\n",
      "Training loss for batch 675 : 0.03780384361743927\n",
      "Training loss for batch 676 : 0.06285127252340317\n",
      "Training loss for batch 677 : 0.053893908858299255\n",
      "Training loss for batch 678 : 0.06797578185796738\n",
      "Training loss for batch 679 : 0.07450710237026215\n",
      "Training loss for batch 680 : 0.08392007648944855\n",
      "Training loss for batch 681 : 0.10139581561088562\n",
      "Training loss for batch 682 : 0.078548863530159\n",
      "Training loss for batch 683 : 0.025087660178542137\n",
      "Training loss for batch 684 : 0.12794488668441772\n",
      "Training loss for batch 685 : 0.03756028786301613\n",
      "Training loss for batch 686 : 0.08091634511947632\n",
      "Training loss for batch 687 : 0.09196843951940536\n",
      "Training loss for batch 688 : 0.03323232755064964\n",
      "Training loss for batch 689 : 0.041053056716918945\n",
      "Training loss for batch 690 : 0.18658098578453064\n",
      "Training loss for batch 691 : 0.1954963058233261\n",
      "Training loss for batch 692 : 0.12690001726150513\n",
      "Training loss for batch 693 : 0.016697721555829048\n",
      "Training loss for batch 694 : 0.03993534296751022\n",
      "Training loss for batch 695 : 0.049410030245780945\n",
      "Training loss for batch 696 : 0.032640017569065094\n",
      "Training loss for batch 697 : 0.0868602767586708\n",
      "Training loss for batch 698 : 0.05074917897582054\n",
      "Training loss for batch 699 : 0.0384305864572525\n",
      "Training loss for batch 700 : 0.07938166707754135\n",
      "Training loss for batch 701 : 0.12825019657611847\n",
      "Training loss for batch 702 : 0.026695074513554573\n",
      "Training loss for batch 703 : 0.05356159806251526\n",
      "Training loss for batch 704 : 0.10188545286655426\n",
      "Training loss for batch 705 : 0.013278871774673462\n",
      "Training loss for batch 706 : 0.16402314603328705\n",
      "Training loss for batch 707 : 0.0150294229388237\n",
      "Training loss for batch 708 : 0.03230370208621025\n",
      "Training loss for batch 709 : 0.0285124983638525\n",
      "Training loss for batch 710 : 0.037055160850286484\n",
      "Training loss for batch 711 : 0.10158493369817734\n",
      "Training loss for batch 712 : 0.08866230398416519\n",
      "Training loss for batch 713 : 0.038476791232824326\n",
      "Training loss for batch 714 : 0.054039210081100464\n",
      "Training loss for batch 715 : 0.07151032984256744\n",
      "Training loss for batch 716 : 0.11203551292419434\n",
      "Training loss for batch 717 : 0.12510782480239868\n",
      "Training loss for batch 718 : 0.046559352427721024\n",
      "Training loss for batch 719 : 0.18670445680618286\n",
      "Training loss for batch 720 : 0.11883199959993362\n",
      "Training loss for batch 721 : 0.035998161882162094\n",
      "Training loss for batch 722 : 0.05738001689314842\n",
      "Training loss for batch 723 : 0.12923720479011536\n",
      "Training loss for batch 724 : 0.13671396672725677\n",
      "Training loss for batch 725 : 0.06557434797286987\n",
      "Training loss for batch 726 : 0.09328463673591614\n",
      "Training loss for batch 727 : 0.0425039604306221\n",
      "Training loss for batch 728 : 0.02595115825533867\n",
      "Training loss for batch 729 : 0.04837363585829735\n",
      "Training loss for batch 730 : 0.15446080267429352\n",
      "Training loss for batch 731 : 0.023972241207957268\n",
      "Training loss for batch 732 : 0.00803573802113533\n",
      "Training loss for batch 733 : 0.04823281988501549\n",
      "Training loss for batch 734 : 0.16456317901611328\n",
      "Training loss for batch 735 : 0.06494799256324768\n",
      "Training loss for batch 736 : 0.0880226120352745\n",
      "Training loss for batch 737 : 0.12291333079338074\n",
      "Training loss for batch 738 : 0.0734536424279213\n",
      "Training loss for batch 739 : 0.18027834594249725\n",
      "Training loss for batch 740 : 0.06550819426774979\n",
      "Training loss for batch 741 : 0.014935405924916267\n",
      "Training loss for batch 742 : 0.0014375995378941298\n",
      "Training loss for batch 743 : 0.16042429208755493\n",
      "Training loss for batch 744 : 0.09008798003196716\n",
      "Training loss for batch 745 : 0.005916541907936335\n",
      "Training loss for batch 746 : 0.18747006356716156\n",
      "Training loss for batch 747 : 0.18771156668663025\n",
      "Training loss for batch 748 : 0.00256588077172637\n",
      "Training loss for batch 749 : 0.06770306080579758\n",
      "Training loss for batch 750 : 0.07380528748035431\n",
      "Training loss for batch 751 : 0.06538163125514984\n",
      "Training loss for batch 752 : 0.03456122428178787\n",
      "Training loss for batch 753 : 0.04264608025550842\n",
      "Training loss for batch 754 : 0.04337343946099281\n",
      "Training loss for batch 755 : 0.08152402937412262\n",
      "Training loss for batch 756 : 0.07050123065710068\n",
      "Training loss for batch 757 : 0.03158080577850342\n",
      "Training loss for batch 758 : 0.10244009643793106\n",
      "Training loss for batch 759 : 0.09769995510578156\n",
      "Training loss for batch 760 : 0.2152768075466156\n",
      "Training loss for batch 761 : 0.007372178602963686\n",
      "Training loss for batch 762 : 0.05018063262104988\n",
      "Training loss for batch 763 : 0.07166261970996857\n",
      "Training loss for batch 764 : 0.04232574254274368\n",
      "Training loss for batch 765 : 0.03224422037601471\n",
      "Training loss for batch 766 : 0.06290151923894882\n",
      "Training loss for batch 767 : 0.045335885137319565\n",
      "Training loss for batch 768 : 0.03882099315524101\n",
      "Training loss for batch 769 : 0.04941682144999504\n",
      "Training loss for batch 770 : 0.03688148409128189\n",
      "Training loss for batch 771 : 0.12590472400188446\n",
      "Training loss for batch 772 : 0.06730161607265472\n",
      "Training loss for batch 773 : 0.04603094235062599\n",
      "Training loss for batch 774 : 0.06921986490488052\n",
      "Training loss for batch 775 : 0.1393294632434845\n",
      "Training loss for batch 776 : 0.033833373337984085\n",
      "Training loss for batch 777 : 0.041615091264247894\n",
      "Training loss for batch 778 : 0.0745595321059227\n",
      "Training loss for batch 779 : 0.045088522136211395\n",
      "Training loss for batch 780 : 0.003678873647004366\n",
      "Training loss for batch 781 : 0.04274606332182884\n",
      "Training loss for batch 782 : 0.06215384975075722\n",
      "Training loss for batch 783 : 0.006534206680953503\n",
      "Training loss for batch 784 : 0.1152719184756279\n",
      "Training loss for batch 785 : 0.09199521690607071\n",
      "Training loss for batch 786 : 0.08195168524980545\n",
      "Training loss for batch 787 : 0.013485838659107685\n",
      "Training loss for batch 788 : 0.0060561527498066425\n",
      "Training loss for batch 789 : 0.014760153368115425\n",
      "Training loss for batch 790 : 0.04900837317109108\n",
      "Training loss for batch 791 : 0.009049340151250362\n",
      "Training loss for batch 792 : 0.06590931117534637\n",
      "Training loss for batch 793 : 0.041785743087530136\n",
      "Training loss for batch 794 : 0.04332537204027176\n",
      "Training loss for batch 795 : 0.0650935247540474\n",
      "Training loss for batch 796 : 0.03080235980451107\n",
      "Training loss for batch 797 : 0.15961897373199463\n",
      "Training loss for batch 798 : 0.04907473176717758\n",
      "Training loss for batch 799 : 0.09114749729633331\n",
      "Training loss for batch 800 : 0.3468179404735565\n",
      "Training loss for batch 801 : 0.048688244074583054\n",
      "Training loss for batch 802 : 0.04046044126152992\n",
      "Training loss for batch 803 : 0.0947558656334877\n",
      "Training loss for batch 804 : 0.009906035847961903\n",
      "Training loss for batch 805 : 0.013463829644024372\n",
      "Training loss for batch 806 : 0.046787988394498825\n",
      "Training loss for batch 807 : 0.019898315891623497\n",
      "Training loss for batch 808 : 0.04773731157183647\n",
      "Training loss for batch 809 : 0.1595464050769806\n",
      "Training loss for batch 810 : 0.008742759935557842\n",
      "Training loss for batch 811 : 0.1898891180753708\n",
      "Training loss for batch 812 : 0.12832914292812347\n",
      "Training loss for batch 813 : 0.07588537782430649\n",
      "Training loss for batch 814 : 0.05499615892767906\n",
      "Training loss for batch 815 : 0.08091437816619873\n",
      "Training loss for batch 816 : 0.16141562163829803\n",
      "Training loss for batch 817 : 0.08310800790786743\n",
      "Training loss for batch 818 : 0.21061009168624878\n",
      "Training loss for batch 819 : 0.058380722999572754\n",
      "Training loss for batch 820 : 0.02768455259501934\n",
      "Training loss for batch 821 : 0.08372851461172104\n",
      "Training loss for batch 822 : 0.0977562963962555\n",
      "Training loss for batch 823 : 0.06561312079429626\n",
      "Training loss for batch 824 : 0.09430968016386032\n",
      "Training loss for batch 825 : 0.046019263565540314\n",
      "Training loss for batch 826 : 0.0984954982995987\n",
      "Training loss for batch 827 : 0.052777767181396484\n",
      "Training loss for batch 828 : 0.06921587139368057\n",
      "Training loss for batch 829 : 0.15908114612102509\n",
      "Training loss for batch 830 : 0.1414983868598938\n",
      "Training loss for batch 831 : 0.10261646658182144\n",
      "Training loss for batch 832 : 0.057930782437324524\n",
      "Training loss for batch 833 : 0.01820381172001362\n",
      "Training loss for batch 834 : 0.03214491903781891\n",
      "Training loss for batch 835 : 0.06435695290565491\n",
      "Training loss for batch 836 : 0.04649240896105766\n",
      "Training loss for batch 837 : 0.01384126115590334\n",
      "Training loss for batch 838 : 0.03784380108118057\n",
      "Training loss for batch 839 : 0.01480139046907425\n",
      "Training loss for batch 840 : 0.12386567890644073\n",
      "Training loss for batch 841 : 0.11030501872301102\n",
      "Training loss for batch 842 : 0.03383506089448929\n",
      "Training loss for batch 843 : 0.013963238336145878\n",
      "Training loss for batch 844 : 0.156422957777977\n",
      "Training loss for batch 845 : 0.13525795936584473\n",
      "Training loss for batch 846 : 0.1484745442867279\n",
      "Training loss for batch 847 : 0.04548252373933792\n",
      "Training loss for batch 848 : 0.04184101149439812\n",
      "Training loss for batch 849 : 0.10722339898347855\n",
      "Training loss for batch 850 : 0.0017391042783856392\n",
      "Training loss for batch 851 : 0.19327136874198914\n",
      "Training loss for batch 852 : 0.033674199134111404\n",
      "Training loss for batch 853 : 0.18384654819965363\n",
      "Training loss for batch 854 : 0.006819493602961302\n",
      "Training loss for batch 855 : 0.19214074313640594\n",
      "Training loss for batch 856 : 0.12898896634578705\n",
      "Training loss for batch 857 : 0.11053556948900223\n",
      "Training loss for batch 858 : 0.03649343177676201\n",
      "Training loss for batch 859 : 0.09127777814865112\n",
      "Training loss for batch 860 : 0.07366087287664413\n",
      "Training loss for batch 861 : 0.04911480471491814\n",
      "Training loss for batch 862 : 0.0895426794886589\n",
      "Training loss for batch 863 : 0.1124425157904625\n",
      "Training loss for batch 864 : 0.10019437223672867\n",
      "Training loss for batch 865 : 0.12647667527198792\n",
      "Training loss for batch 866 : 0.057951390743255615\n",
      "Training loss for batch 867 : 0.03740780055522919\n",
      "Training loss for batch 868 : 0.11071230471134186\n",
      "Training loss for batch 869 : 0.011628655716776848\n",
      "Training loss for batch 870 : 0.1596856266260147\n",
      "Training loss for batch 871 : 0.04505537822842598\n",
      "Training loss for batch 872 : 0.17603352665901184\n",
      "Training loss for batch 873 : 0.04821500927209854\n",
      "Training loss for batch 874 : 0.0713123083114624\n",
      "Training loss for batch 875 : 0.17213745415210724\n",
      "Training loss for batch 876 : 0.19256852567195892\n",
      "Training loss for batch 877 : 0.03239351138472557\n",
      "Training loss for batch 878 : 0.13822683691978455\n",
      "Training loss for batch 879 : 0.06260311603546143\n",
      "Training loss for batch 880 : 0.28624287247657776\n",
      "Training loss for batch 881 : 0.1779729425907135\n",
      "Training loss for batch 882 : 0.26507440209388733\n",
      "Training loss for batch 883 : 0.08241650462150574\n",
      "Training loss for batch 884 : 0.09986872971057892\n",
      "Training loss for batch 885 : 0.19060464203357697\n",
      "Training loss for batch 886 : 0.11159954965114594\n",
      "Training loss for batch 887 : 0.1434353142976761\n",
      "Training loss for batch 888 : 0.12059533596038818\n",
      "Training loss for batch 889 : 0.11452167481184006\n",
      "Training loss for batch 890 : 0.09681347757577896\n",
      "Training loss for batch 891 : 0.024931039661169052\n",
      "Training loss for batch 892 : 0.05458905175328255\n",
      "Training loss for batch 893 : 0.027728423476219177\n",
      "Training loss for batch 894 : 0.15776614844799042\n",
      "Training loss for batch 895 : 0.04070857912302017\n",
      "Training loss for batch 896 : 0.25527679920196533\n",
      "Training loss for batch 897 : 0.0763852670788765\n",
      "Training loss for batch 898 : 0.03152405843138695\n",
      "Training loss for batch 899 : 0.13142889738082886\n",
      "Training loss for batch 900 : 0.06178286299109459\n",
      "Training loss for batch 901 : 0.05008477345108986\n",
      "Training loss for batch 902 : 0.09534180909395218\n",
      "Training loss for batch 903 : 0.06995551288127899\n",
      "Training loss for batch 904 : 0.11489351093769073\n",
      "Training loss for batch 905 : 0.042234353721141815\n",
      "Training loss for batch 906 : 0.053661931306123734\n",
      "Training loss for batch 907 : 0.15305453538894653\n",
      "Training loss for batch 908 : 0.07605838775634766\n",
      "Training loss for batch 909 : 0.10901842266321182\n",
      "Training loss for batch 910 : 0.15288333594799042\n",
      "Training loss for batch 911 : 0.11022116243839264\n",
      "Training loss for batch 912 : 0.12095589190721512\n",
      "Training loss for batch 913 : 0.11604534089565277\n",
      "Training loss for batch 914 : 0.11716271936893463\n",
      "Training loss for batch 915 : 0.10174687951803207\n",
      "Training loss for batch 916 : 0.046510182321071625\n",
      "Training loss for batch 917 : 0.0856330543756485\n",
      "Training loss for batch 918 : 0.11893333494663239\n",
      "Training loss for batch 919 : 0.0943472757935524\n",
      "Training loss for batch 920 : 0.053857527673244476\n",
      "Training loss for batch 921 : 0.03645463287830353\n",
      "Training loss for batch 922 : 0.036714278161525726\n",
      "Training loss for batch 923 : 0.06970466673374176\n",
      "Training loss for batch 924 : 0.030372576788067818\n",
      "Training loss for batch 925 : 0.00657441234216094\n",
      "Training loss for batch 926 : 0.01638588309288025\n",
      "Training loss for batch 927 : 0.13085652887821198\n",
      "Training loss for batch 928 : 0.0022323743905872107\n",
      "Training loss for batch 929 : 0.07826016843318939\n",
      "Training loss for batch 930 : 0.07493049651384354\n",
      "Training loss for batch 931 : 0.04846549034118652\n",
      "Training loss for batch 932 : 0.04716503620147705\n",
      "Training loss for batch 933 : 0.17743541300296783\n",
      "Training loss for batch 934 : 0.02406827174127102\n",
      "Training loss for batch 935 : 0.08625900000333786\n",
      "Training loss for batch 936 : 0.0009503215551376343\n",
      "Training loss for batch 937 : 0.05209134891629219\n",
      "Training loss for batch 938 : 0.14451256394386292\n",
      "Training loss for batch 939 : 0.12031294405460358\n",
      "Training loss for batch 940 : 0.18502646684646606\n",
      "Training loss for batch 941 : 0.12997938692569733\n",
      "Training loss for batch 942 : 0.013652602210640907\n",
      "Training loss for batch 943 : 0.04404151812195778\n",
      "Training loss for batch 944 : 0.10445524752140045\n",
      "Training loss for batch 945 : 0.019368141889572144\n",
      "Training loss for batch 946 : 0.003617773065343499\n",
      "Training loss for batch 947 : 0.029513094574213028\n",
      "Training loss for batch 948 : 0.03449248895049095\n",
      "Training loss for batch 949 : 0.16493697464466095\n",
      "Training loss for batch 950 : 0.07364127784967422\n",
      "Training loss for batch 951 : 0.056132327765226364\n",
      "Training loss for batch 952 : 0.12636952102184296\n",
      "Training loss for batch 953 : 0.04258217662572861\n",
      "Training loss for batch 954 : 0.04435395449399948\n",
      "Training loss for batch 955 : 0.13016432523727417\n",
      "Training loss for batch 956 : 0.21200862526893616\n",
      "Training loss for batch 957 : 0.030276432633399963\n",
      "Training loss for batch 958 : 0.008223126642405987\n",
      "Training loss for batch 959 : 0.0\n",
      "Training loss for batch 960 : 0.08709649741649628\n",
      "Training loss for batch 961 : 0.2699738144874573\n",
      "Training loss for batch 962 : 0.11124169081449509\n",
      "Training loss for batch 963 : 0.2367125004529953\n",
      "Training loss for batch 964 : 0.1279158741235733\n",
      "Training loss for batch 965 : 0.0\n",
      "Training loss for batch 966 : 0.11230941116809845\n",
      "Training loss for batch 967 : 0.19102385640144348\n",
      "Training loss for batch 968 : 0.09717880934476852\n",
      "Training loss for batch 969 : 0.08621012419462204\n",
      "Training loss for batch 970 : 0.09987432509660721\n",
      "Training loss for batch 971 : 0.08036433160305023\n",
      "Training loss for batch 972 : 0.08355572819709778\n",
      "Training loss for batch 973 : 0.07085083425045013\n",
      "Training loss for batch 974 : 0.005069944076240063\n",
      "Training loss for batch 975 : 0.015129407867789268\n",
      "Training loss for batch 976 : 0.1710931360721588\n",
      "Training loss for batch 977 : 0.022825254127383232\n",
      "Training loss for batch 978 : 0.04893900454044342\n",
      "Training loss for batch 979 : 0.1042957454919815\n",
      "Training loss for batch 980 : 0.08093694597482681\n",
      "Training loss for batch 981 : 0.3386039137840271\n",
      "Training loss for batch 982 : 0.10465646535158157\n",
      "Training loss for batch 983 : 0.06717416644096375\n",
      "Training loss for batch 984 : 0.004142781253904104\n",
      "Training loss for batch 985 : 0.05145347863435745\n",
      "Training loss for batch 986 : 0.08016139268875122\n",
      "Training loss for batch 987 : 0.11146378517150879\n",
      "Training loss for batch 988 : 0.06641323864459991\n",
      "Training loss for batch 989 : 0.0283973291516304\n",
      "Training loss for batch 990 : 0.023792441934347153\n",
      "Training loss for batch 991 : 0.09386657178401947\n",
      "Training loss for batch 992 : 0.14180701971054077\n",
      "Training loss for batch 993 : 0.15987443923950195\n",
      "Training loss for batch 994 : 0.039273183792829514\n",
      "Training loss for batch 995 : 0.16925634443759918\n",
      "Training loss for batch 996 : 0.11494356393814087\n",
      "Training loss for batch 997 : 0.14410269260406494\n",
      "Training loss for batch 998 : 0.030717698857188225\n",
      "Training loss for batch 999 : 0.19074901938438416\n",
      "Training loss for batch 1000 : 0.057837389409542084\n",
      "Training loss for batch 1001 : 0.018444089218974113\n",
      "Training loss for batch 1002 : 0.08096364140510559\n",
      "Training loss for batch 1003 : 0.009663055650889874\n",
      "Training loss for batch 1004 : 0.07843024283647537\n",
      "Training loss for batch 1005 : 0.08910034596920013\n",
      "Training loss for batch 1006 : 0.038690533488988876\n",
      "Training loss for batch 1007 : 0.01334486622363329\n",
      "Training loss for batch 1008 : 0.08821592479944229\n",
      "Training loss for batch 1009 : 0.09566012024879456\n",
      "Training loss for batch 1010 : 0.16238194704055786\n",
      "Training loss for batch 1011 : 0.12552441656589508\n",
      "Training loss for batch 1012 : 0.08346138894557953\n",
      "Training loss for batch 1013 : 0.052527520805597305\n",
      "Training loss for batch 1014 : 0.02111443318426609\n",
      "Training loss for batch 1015 : 0.07756628096103668\n",
      "Training loss for batch 1016 : 0.07368388026952744\n",
      "Training loss for batch 1017 : 0.06384215503931046\n",
      "Training loss for batch 1018 : 0.10295351594686508\n",
      "Training loss for batch 1019 : 0.06651332229375839\n",
      "Training loss for batch 1020 : 0.08640089631080627\n",
      "Training loss for batch 1021 : 0.1592598557472229\n",
      "Training loss for batch 1022 : 0.036981645971536636\n",
      "Training loss for batch 1023 : 0.18567322194576263\n",
      "Training loss for batch 1024 : 0.023141762241721153\n",
      "Training loss for batch 1025 : 0.1266482025384903\n",
      "Training loss for batch 1026 : 0.16304294764995575\n",
      "Training loss for batch 1027 : 0.12180332839488983\n",
      "Training loss for batch 1028 : 0.09641837328672409\n",
      "Training loss for batch 1029 : 0.014571726322174072\n",
      "Training loss for batch 1030 : 0.1443980634212494\n",
      "Training loss for batch 1031 : 0.033333249390125275\n",
      "Training loss for batch 1032 : 0.044750697910785675\n",
      "Training loss for batch 1033 : 0.04544084519147873\n",
      "Training loss for batch 1034 : 0.09169340878725052\n",
      "Training loss for batch 1035 : 0.07517136633396149\n",
      "Training loss for batch 1036 : 0.15945588052272797\n",
      "Training loss for batch 1037 : 0.06437166035175323\n",
      "Training loss for batch 1038 : 0.12050233781337738\n",
      "Training loss for batch 1039 : 0.2973182499408722\n",
      "Training loss for batch 1040 : 0.07994292676448822\n",
      "Training loss for batch 1041 : 0.0860905647277832\n",
      "Training loss for batch 1042 : 0.12503795325756073\n",
      "Training loss for batch 1043 : 0.048568665981292725\n",
      "Training loss for batch 1044 : 0.0856693759560585\n",
      "Training loss for batch 1045 : 0.04026118665933609\n",
      "Training loss for batch 1046 : 0.008789021521806717\n",
      "Training loss for batch 1047 : 0.04079582542181015\n",
      "Training loss for batch 1048 : 0.027429942041635513\n",
      "Training loss for batch 1049 : 0.052149590104818344\n",
      "Training loss for batch 1050 : 0.012329712510108948\n",
      "Training loss for batch 1051 : 0.05063407123088837\n",
      "Training loss for batch 1052 : 0.1278141885995865\n",
      "Training loss for batch 1053 : 0.008563101291656494\n",
      "Training loss for batch 1054 : 0.017592014744877815\n",
      "Training loss for batch 1055 : 0.04935180023312569\n",
      "Training loss for batch 1056 : 0.009355208836495876\n",
      "Training loss for batch 1057 : 0.012480741366744041\n",
      "Training loss for batch 1058 : 0.057735081762075424\n",
      "Training loss for batch 1059 : 0.09285485744476318\n",
      "Training loss for batch 1060 : 0.16885505616664886\n",
      "Training loss for batch 1061 : 0.08850906789302826\n",
      "Training loss for batch 1062 : 0.13614509999752045\n",
      "Training loss for batch 1063 : 0.09548516571521759\n",
      "Training loss for batch 1064 : 0.006477891001850367\n",
      "Training loss for batch 1065 : 0.32272469997406006\n",
      "Training loss for batch 1066 : 0.08683709800243378\n",
      "Training loss for batch 1067 : 0.10885817557573318\n",
      "Training loss for batch 1068 : 0.051688097417354584\n",
      "Training loss for batch 1069 : 0.04957893490791321\n",
      "Training loss for batch 1070 : 0.03659676015377045\n",
      "Training loss for batch 1071 : 0.10782743245363235\n",
      "Training loss for batch 1072 : 0.00442764675244689\n",
      "Training loss for batch 1073 : 0.049663688987493515\n",
      "Training loss for batch 1074 : 0.07082962989807129\n",
      "Training loss for batch 1075 : 0.020246529951691628\n",
      "Training loss for batch 1076 : 0.21247179806232452\n",
      "Training loss for batch 1077 : 0.10834599286317825\n",
      "Training loss for batch 1078 : 0.14540086686611176\n",
      "Training loss for batch 1079 : 0.07348982989788055\n",
      "Training loss for batch 1080 : 0.0779990553855896\n",
      "Training loss for batch 1081 : 0.10508459061384201\n",
      "Training loss for batch 1082 : 0.10599218308925629\n",
      "Training loss for batch 1083 : 0.006452559493482113\n",
      "Training loss for batch 1084 : 0.007168787531554699\n",
      "Training loss for batch 1085 : 0.04849459230899811\n",
      "Training loss for batch 1086 : 0.14952227473258972\n",
      "Training loss for batch 1087 : 0.060120996087789536\n",
      "Training loss for batch 1088 : 0.0732075497508049\n",
      "Training loss for batch 1089 : 0.12486578524112701\n",
      "Training loss for batch 1090 : 0.033527933061122894\n",
      "Training loss for batch 1091 : 0.0678788498044014\n",
      "Training loss for batch 1092 : 0.16581185162067413\n",
      "Training loss for batch 1093 : 0.04006543010473251\n",
      "Training loss for batch 1094 : 0.1683567762374878\n",
      "Training loss for batch 1095 : 0.12856242060661316\n",
      "Training loss for batch 1096 : 0.05451145023107529\n",
      "Training loss for batch 1097 : 0.17108233273029327\n",
      "Training loss for batch 1098 : 0.04918073117733002\n",
      "Training loss for batch 1099 : 0.12758207321166992\n",
      "Training loss for batch 1100 : 0.15716633200645447\n",
      "Training loss for batch 1101 : 0.026784786954522133\n",
      "Training loss for batch 1102 : 0.160659059882164\n",
      "Training loss for batch 1103 : 0.12290018051862717\n",
      "Training loss for batch 1104 : 0.04887526109814644\n",
      "Training loss for batch 1105 : 0.08788922429084778\n",
      "Training loss for batch 1106 : 0.03830390051007271\n",
      "Training loss for batch 1107 : 0.11220278590917587\n",
      "Training loss for batch 1108 : 0.07763639092445374\n",
      "Training loss for batch 1109 : 0.1609136313199997\n",
      "Training loss for batch 1110 : 0.13680024445056915\n",
      "Training loss for batch 1111 : 0.02184775471687317\n",
      "Training loss for batch 1112 : 0.040895309299230576\n",
      "Training loss for batch 1113 : 0.01972799375653267\n",
      "Training loss for batch 1114 : 0.2240975797176361\n",
      "Training loss for batch 1115 : 0.02517622523009777\n",
      "Training loss for batch 1116 : 0.047098882496356964\n",
      "Training loss for batch 1117 : 0.11616387963294983\n",
      "Training loss for batch 1118 : 0.12373170256614685\n",
      "Training loss for batch 1119 : 0.09559817612171173\n",
      "Training loss for batch 1120 : 0.017501307651400566\n",
      "Training loss for batch 1121 : 0.041340421885252\n",
      "Training loss for batch 1122 : 0.15156860649585724\n",
      "Training loss for batch 1123 : 0.11180049926042557\n",
      "Training loss for batch 1124 : 0.06154432147741318\n",
      "Training loss for batch 1125 : 0.04945158585906029\n",
      "Training loss for batch 1126 : 0.058333128690719604\n",
      "Training loss for batch 1127 : 0.1397424042224884\n",
      "Training loss for batch 1128 : 0.18937551975250244\n",
      "Training loss for batch 1129 : 0.06762003898620605\n",
      "Training loss for batch 1130 : 0.05890728533267975\n",
      "Training loss for batch 1131 : 0.12475740164518356\n",
      "Training loss for batch 1132 : 0.018242113292217255\n",
      "Training loss for batch 1133 : 0.044384874403476715\n",
      "Training loss for batch 1134 : 0.08619778603315353\n",
      "Training loss for batch 1135 : 0.07333570718765259\n",
      "Training loss for batch 1136 : 0.0574834980070591\n",
      "Training loss for batch 1137 : 0.033517058938741684\n",
      "Training loss for batch 1138 : 0.054554861038923264\n",
      "Training loss for batch 1139 : 0.09978435188531876\n",
      "Training loss for batch 1140 : 0.0816265270113945\n",
      "Training loss for batch 1141 : 0.10014993697404861\n",
      "Training loss for batch 1142 : 0.06553607434034348\n",
      "Training loss for batch 1143 : 0.061021700501441956\n",
      "Training loss for batch 1144 : 0.11968227475881577\n",
      "Training loss for batch 1145 : 0.023587316274642944\n",
      "Training loss for batch 1146 : 0.07133431732654572\n",
      "Training loss for batch 1147 : 0.10018298774957657\n",
      "Training loss for batch 1148 : 0.08757241815328598\n",
      "Training loss for batch 1149 : 0.11976483464241028\n",
      "Training loss for batch 1150 : 0.1287013441324234\n",
      "Training loss for batch 1151 : 0.059655606746673584\n",
      "Training loss for batch 1152 : 0.0234138872474432\n",
      "Training loss for batch 1153 : 0.02948467992246151\n",
      "Training loss for batch 1154 : 0.009533261880278587\n",
      "Training loss for batch 1155 : 0.02400553598999977\n",
      "Training loss for batch 1156 : 0.11610890924930573\n",
      "Training loss for batch 1157 : 0.08995790034532547\n",
      "Training loss for batch 1158 : 0.15388396382331848\n",
      "Training loss for batch 1159 : 0.23062759637832642\n",
      "Training loss for batch 1160 : 0.015417370945215225\n",
      "Training loss for batch 1161 : 0.049277856945991516\n",
      "Training loss for batch 1162 : 0.12890049815177917\n",
      "Training loss for batch 1163 : 0.03819267079234123\n",
      "Training loss for batch 1164 : 0.008128487505018711\n",
      "Training loss for batch 1165 : 0.07088169455528259\n",
      "Training loss for batch 1166 : 0.03140828385949135\n",
      "Training loss for batch 1167 : 0.11654346436262131\n",
      "Training loss for batch 1168 : 0.08404082804918289\n",
      "Training loss for batch 1169 : 0.0326208733022213\n",
      "Training loss for batch 1170 : 0.2327568680047989\n",
      "Training loss for batch 1171 : 0.15196074545383453\n",
      "Training loss for batch 1172 : 0.2211216390132904\n",
      "Training loss for batch 1173 : 0.10600706189870834\n",
      "Training loss for batch 1174 : 0.004582249093800783\n",
      "Training loss for batch 1175 : 0.04923183470964432\n",
      "Training loss for batch 1176 : 0.08454009145498276\n",
      "Training loss for batch 1177 : 0.22770066559314728\n",
      "Training loss for batch 1178 : 0.054834671318531036\n",
      "Training loss for batch 1179 : 0.05158147215843201\n",
      "Training loss for batch 1180 : 0.10767851024866104\n",
      "Training loss for batch 1181 : 0.05103294551372528\n",
      "Training loss for batch 1182 : 0.04247426614165306\n",
      "Training loss for batch 1183 : 0.15058250725269318\n",
      "Training loss for batch 1184 : 0.022559570148587227\n",
      "Training loss for batch 1185 : 0.07774651795625687\n",
      "Training loss for batch 1186 : 0.0043056621216237545\n",
      "Training loss for batch 1187 : 0.1322881430387497\n",
      "Training loss for batch 1188 : 0.088615782558918\n",
      "Training loss for batch 1189 : 0.0657799243927002\n",
      "Training loss for batch 1190 : 0.07669778913259506\n",
      "Training loss for batch 1191 : 0.00539021659642458\n",
      "Training loss for batch 1192 : 0.11591368913650513\n",
      "Training loss for batch 1193 : 0.13020798563957214\n",
      "Training loss for batch 1194 : 0.04991283640265465\n",
      "Training loss for batch 1195 : 0.1892431080341339\n",
      "Training loss for batch 1196 : 0.06748630851507187\n",
      "Training loss for batch 1197 : 0.14348956942558289\n",
      "Training loss for batch 1198 : 0.11635999381542206\n",
      "Training loss for batch 1199 : 0.12588219344615936\n",
      "Training loss for batch 1200 : 0.1488530933856964\n",
      "Training loss for batch 1201 : 0.16904300451278687\n",
      "Training loss for batch 1202 : 0.034577272832393646\n",
      "Training loss for batch 1203 : 0.10772953927516937\n",
      "Training loss for batch 1204 : 0.010818403214216232\n",
      "Training loss for batch 1205 : 0.10224764049053192\n",
      "Training loss for batch 1206 : 0.15805283188819885\n",
      "Training loss for batch 1207 : 0.05495781823992729\n",
      "Training loss for batch 1208 : 0.09939514100551605\n",
      "Training loss for batch 1209 : 0.1336362063884735\n",
      "Training loss for batch 1210 : 0.18408441543579102\n",
      "Training loss for batch 1211 : 0.1592898964881897\n",
      "Training loss for batch 1212 : 0.14784015715122223\n",
      "Training loss for batch 1213 : 0.00530060613527894\n",
      "Training loss for batch 1214 : 0.19477881491184235\n",
      "Training loss for batch 1215 : 0.13823296129703522\n",
      "Training loss for batch 1216 : 0.010624426417052746\n",
      "Training loss for batch 1217 : 0.009064820595085621\n",
      "Training loss for batch 1218 : 0.0034514814615249634\n",
      "Training loss for batch 1219 : 0.15914592146873474\n",
      "Training loss for batch 1220 : 0.1613425463438034\n",
      "Training loss for batch 1221 : 0.10465630143880844\n",
      "Training loss for batch 1222 : 0.09015652537345886\n",
      "Training loss for batch 1223 : 0.020955054089426994\n",
      "Training loss for batch 1224 : 0.18866273760795593\n",
      "Training loss for batch 1225 : 0.023559922352433205\n",
      "Training loss for batch 1226 : 0.13070346415042877\n",
      "Training loss for batch 1227 : 0.07108022272586823\n",
      "Training loss for batch 1228 : 0.04031793400645256\n",
      "Training loss for batch 1229 : 0.17028675973415375\n",
      "Training loss for batch 1230 : 0.03417329117655754\n",
      "Training loss for batch 1231 : 0.13459856808185577\n",
      "Training loss for batch 1232 : 0.034176576882600784\n",
      "Training loss for batch 1233 : 0.031620047986507416\n",
      "Training loss for batch 1234 : 0.011547780595719814\n",
      "Training loss for batch 1235 : 0.24692107737064362\n",
      "Training loss for batch 1236 : 0.010406636632978916\n",
      "Training loss for batch 1237 : 0.0038809473626315594\n",
      "Training loss for batch 1238 : 0.1848953664302826\n",
      "Training loss for batch 1239 : 0.1766931116580963\n",
      "Training loss for batch 1240 : 0.055029500275850296\n",
      "Training loss for batch 1241 : 0.11176818609237671\n",
      "Training loss for batch 1242 : 0.13245682418346405\n",
      "Training loss for batch 1243 : 0.10647238045930862\n",
      "Training loss for batch 1244 : 0.0818982645869255\n",
      "Training loss for batch 1245 : 0.07517336308956146\n",
      "Training loss for batch 1246 : 0.06809555739164352\n",
      "Training loss for batch 1247 : 0.0853978767991066\n",
      "Training loss for batch 1248 : 0.09060785174369812\n",
      "Training loss for batch 1249 : 0.12204407155513763\n",
      "Training loss for batch 1250 : 0.01784295029938221\n",
      "Training loss for batch 1251 : 0.08049249649047852\n",
      "Training loss for batch 1252 : 0.021283769980072975\n",
      "Training loss for batch 1253 : 0.047649819403886795\n",
      "Training loss for batch 1254 : 0.17341665923595428\n",
      "Training loss for batch 1255 : 0.13148795068264008\n",
      "Training loss for batch 1256 : 0.11136342585086823\n",
      "Training loss for batch 1257 : 0.028108768165111542\n",
      "Training loss for batch 1258 : 0.0013713141670450568\n",
      "Training loss for batch 1259 : 0.09895973652601242\n",
      "Training loss for batch 1260 : 0.04077449440956116\n",
      "Training loss for batch 1261 : 0.06919438391923904\n",
      "Training loss for batch 1262 : 0.058978959918022156\n",
      "Training loss for batch 1263 : 0.10806430131196976\n",
      "Training loss for batch 1264 : 0.0555436797440052\n",
      "Training loss for batch 1265 : 0.1111946851015091\n",
      "Training loss for batch 1266 : 0.15182463824748993\n",
      "Training loss for batch 1267 : 0.06718304753303528\n",
      "Training loss for batch 1268 : 0.017757786437869072\n",
      "Training loss for batch 1269 : 0.0888337567448616\n",
      "Training loss for batch 1270 : 0.19303390383720398\n",
      "Training loss for batch 1271 : 0.1516069620847702\n",
      "Training loss for batch 1272 : 0.0617465004324913\n",
      "Training loss for batch 1273 : 0.11348823457956314\n",
      "Training loss for batch 1274 : 0.0463811457157135\n",
      "Training loss for batch 1275 : 0.045959170907735825\n",
      "Training loss for batch 1276 : 0.029322071000933647\n",
      "Training loss for batch 1277 : 0.16410435736179352\n",
      "Training loss for batch 1278 : 0.007877432741224766\n",
      "Training loss for batch 1279 : 0.11796178668737411\n",
      "Training loss for batch 1280 : 0.14505426585674286\n",
      "Training loss for batch 1281 : 0.04508049041032791\n",
      "Training loss for batch 1282 : 0.08380792289972305\n",
      "Training loss for batch 1283 : 0.024064067751169205\n",
      "Training loss for batch 1284 : 0.13467879593372345\n",
      "Training loss for batch 1285 : 0.08734215795993805\n",
      "Training loss for batch 1286 : 0.09087956696748734\n",
      "Training loss for batch 1287 : 0.16113942861557007\n",
      "Training loss for batch 1288 : 0.054060108959674835\n",
      "Training loss for batch 1289 : 0.258736252784729\n",
      "Training loss for batch 1290 : 0.05905107036232948\n",
      "Training loss for batch 1291 : 0.0790899395942688\n",
      "Training loss for batch 1292 : 0.1066456139087677\n",
      "Training loss for batch 1293 : 0.21310992538928986\n",
      "Training loss for batch 1294 : 0.04200052097439766\n",
      "Training loss for batch 1295 : 0.08619160205125809\n",
      "Training loss for batch 1296 : 0.10175879299640656\n",
      "Training loss for batch 1297 : 0.057864781469106674\n",
      "Training loss for batch 1298 : 0.027158061042428017\n",
      "Training loss for batch 1299 : 0.035746246576309204\n",
      "Training loss for batch 1300 : 0.07151778787374496\n",
      "Training loss for batch 1301 : 0.0181965883821249\n",
      "Training loss for batch 1302 : 0.005147005431354046\n",
      "Training loss for batch 1303 : 0.09337949752807617\n",
      "Training loss for batch 1304 : 0.05686058849096298\n",
      "Training loss for batch 1305 : 0.0381372831761837\n",
      "Training loss for batch 1306 : 0.12197275459766388\n",
      "Training loss for batch 1307 : 0.04186541959643364\n",
      "Training loss for batch 1308 : 0.03086545690894127\n",
      "Training loss for batch 1309 : 0.03632964566349983\n",
      "Training loss for batch 1310 : 0.0026919222436845303\n",
      "Training loss for batch 1311 : 0.15657959878444672\n",
      "Training loss for batch 1312 : 0.016677185893058777\n",
      "Training loss for batch 1313 : 0.08557570725679398\n",
      "Training loss for batch 1314 : 0.028810884803533554\n",
      "Training loss for batch 1315 : 0.09665364772081375\n",
      "Training loss for batch 1316 : 0.07562338560819626\n",
      "Training loss for batch 1317 : 0.029715241864323616\n",
      "Training loss for batch 1318 : 0.14942790567874908\n",
      "Training loss for batch 1319 : 0.05994341894984245\n",
      "Training loss for batch 1320 : 0.16237036883831024\n",
      "Training loss for batch 1321 : 0.14015838503837585\n",
      "Training loss for batch 1322 : 0.0\n",
      "Training loss for batch 1323 : 0.10335984081029892\n",
      "Training loss for batch 1324 : 0.0943342000246048\n",
      "Training loss for batch 1325 : 0.19624924659729004\n",
      "Training loss for batch 1326 : 0.14229169487953186\n",
      "Training loss for batch 1327 : 0.06300061196088791\n",
      "Training loss for batch 1328 : 0.3337060511112213\n",
      "Training loss for batch 1329 : 0.061467621475458145\n",
      "Training loss for batch 1330 : 0.04354124888777733\n",
      "Training loss for batch 1331 : 0.021851293742656708\n",
      "Training loss for batch 1332 : 0.0185487549751997\n",
      "Training loss for batch 1333 : 0.007569973357021809\n",
      "Training loss for batch 1334 : 0.13541124761104584\n",
      "Training loss for batch 1335 : 0.13222885131835938\n",
      "Training loss for batch 1336 : 0.0663694441318512\n",
      "Training loss for batch 1337 : 0.19766183197498322\n",
      "Training loss for batch 1338 : 0.049969132989645004\n",
      "Training loss for batch 1339 : 0.03787684068083763\n",
      "Training loss for batch 1340 : 0.08872471004724503\n",
      "Training loss for batch 1341 : 0.03490092232823372\n",
      "Training loss for batch 1342 : 0.061299048364162445\n",
      "Training loss for batch 1343 : 0.2019932121038437\n",
      "Training loss for batch 1344 : 0.0603044293820858\n",
      "Training loss for batch 1345 : 0.04993946850299835\n",
      "Training loss for batch 1346 : 0.26921704411506653\n",
      "Training loss for batch 1347 : 0.17596544325351715\n",
      "Training loss for batch 1348 : 0.09539379179477692\n",
      "Training loss for batch 1349 : 0.03399822488427162\n",
      "Training loss for batch 1350 : 0.1888335645198822\n",
      "Training loss for batch 1351 : 0.15334202349185944\n",
      "Training loss for batch 1352 : 0.09975366294384003\n",
      "Training loss for batch 1353 : 0.005224395543336868\n",
      "Training loss for batch 1354 : 0.051390066742897034\n",
      "Training loss for batch 1355 : 0.08330310881137848\n",
      "Training loss for batch 1356 : 0.06613955646753311\n",
      "Training loss for batch 1357 : 0.14704176783561707\n",
      "Training loss for batch 1358 : 0.04263101518154144\n",
      "Training loss for batch 1359 : 0.023975269868969917\n",
      "Training loss for batch 1360 : 0.0654706358909607\n",
      "Training loss for batch 1361 : 0.1542482078075409\n",
      "Training loss for batch 1362 : 0.11087341606616974\n",
      "Training loss for batch 1363 : 0.085988849401474\n",
      "Training loss for batch 1364 : 0.17109429836273193\n",
      "Training loss for batch 1365 : 0.09246984124183655\n",
      "Training loss for batch 1366 : 0.14668364822864532\n",
      "Training loss for batch 1367 : 0.06493330746889114\n",
      "Training loss for batch 1368 : 0.10299867391586304\n",
      "Training loss for batch 1369 : 0.27368199825286865\n",
      "Training loss for batch 1370 : 0.13846690952777863\n",
      "Training loss for batch 1371 : 0.05771959573030472\n",
      "Training loss for batch 1372 : 0.12683354318141937\n",
      "Training loss for batch 1373 : 0.022111643105745316\n",
      "Training loss for batch 1374 : 0.096500925719738\n",
      "Training loss for batch 1375 : 0.1302935928106308\n",
      "Training loss for batch 1376 : 0.122504822909832\n",
      "Training loss for batch 1377 : 0.17474108934402466\n",
      "Training loss for batch 1378 : 0.14399059116840363\n",
      "Training loss for batch 1379 : 0.0945918932557106\n",
      "Training loss for batch 1380 : 0.031583432108163834\n",
      "Training loss for batch 1381 : 0.03744804486632347\n",
      "Training loss for batch 1382 : 0.07162286341190338\n",
      "Training loss for batch 1383 : 0.25819164514541626\n",
      "Training loss for batch 1384 : 0.06500396877527237\n",
      "Training loss for batch 1385 : 0.050893425941467285\n",
      "Training loss for batch 1386 : 0.05413688346743584\n",
      "Training loss for batch 1387 : 0.0731259137392044\n",
      "Training loss for batch 1388 : 0.09579593688249588\n",
      "Training loss for batch 1389 : 0.03635234013199806\n",
      "Training loss for batch 1390 : 0.09806185960769653\n",
      "Training loss for batch 1391 : 0.10951144993305206\n",
      "Training loss for batch 1392 : 0.043979451060295105\n",
      "Training loss for batch 1393 : 0.04433958977460861\n",
      "Training loss for batch 1394 : 0.013831826858222485\n",
      "Training loss for batch 1395 : 0.21584588289260864\n",
      "Training loss for batch 1396 : 0.105341337621212\n",
      "Training loss for batch 1397 : 0.09472348541021347\n",
      "Training loss for batch 1398 : 0.015358013100922108\n",
      "Training loss for batch 1399 : 0.03777264431118965\n",
      "Training loss for batch 1400 : 0.08179488033056259\n",
      "Training loss for batch 1401 : 0.08435454964637756\n",
      "Training loss for batch 1402 : 0.12417054921388626\n",
      "Training loss for batch 1403 : 0.021862061694264412\n",
      "Training loss for batch 1404 : 0.0829453319311142\n",
      "Training loss for batch 1405 : 0.04576903581619263\n",
      "Training loss for batch 1406 : 0.16262935101985931\n",
      "Training loss for batch 1407 : 0.039500169456005096\n",
      "Training loss for batch 1408 : 0.042022742331027985\n",
      "Training loss for batch 1409 : 0.11082860082387924\n",
      "Training loss for batch 1410 : 0.041276127099990845\n",
      "Training loss for batch 1411 : 0.045956745743751526\n",
      "Training loss for batch 1412 : 0.06868286430835724\n",
      "Training loss for batch 1413 : 0.07710684090852737\n",
      "Training loss for batch 1414 : 0.04486934468150139\n",
      "Training loss for batch 1415 : 0.23405401408672333\n",
      "Training loss for batch 1416 : 0.16376447677612305\n",
      "Training loss for batch 1417 : 0.04666130244731903\n",
      "Training loss for batch 1418 : 0.1452448070049286\n",
      "Training loss for batch 1419 : 0.09637485444545746\n",
      "Training loss for batch 1420 : 0.03936739265918732\n",
      "Training loss for batch 1421 : 0.058902714401483536\n",
      "Training loss for batch 1422 : 0.01604810543358326\n",
      "Training loss for batch 1423 : 0.025554204359650612\n",
      "Training loss for batch 1424 : 0.028438381850719452\n",
      "Training loss for batch 1425 : 0.050189901143312454\n",
      "Training loss for batch 1426 : 0.06875517964363098\n",
      "Training loss for batch 1427 : 0.20148630440235138\n",
      "Training loss for batch 1428 : 0.07971958070993423\n",
      "Training loss for batch 1429 : 0.10445010662078857\n",
      "Training loss for batch 1430 : 0.23365968465805054\n",
      "Training loss for batch 1431 : 0.05381285399198532\n",
      "Training loss for batch 1432 : 0.028404319658875465\n",
      "Training loss for batch 1433 : 0.19767874479293823\n",
      "Training loss for batch 1434 : 0.2553335726261139\n",
      "Training loss for batch 1435 : 0.08565995842218399\n",
      "Training loss for batch 1436 : 0.17729096114635468\n",
      "Training loss for batch 1437 : 0.11601433902978897\n",
      "Training loss for batch 1438 : 0.13311292231082916\n",
      "Training loss for batch 1439 : 0.015840157866477966\n",
      "Training loss for batch 1440 : 0.09090226143598557\n",
      "Training loss for batch 1441 : 0.14070872962474823\n",
      "Training loss for batch 1442 : 0.15202593803405762\n",
      "Training loss for batch 1443 : 0.06995705515146255\n",
      "Training loss for batch 1444 : 0.09586239606142044\n",
      "Training loss for batch 1445 : 0.03427563235163689\n",
      "Training loss for batch 1446 : 0.07146327197551727\n",
      "Training loss for batch 1447 : 0.09596310555934906\n",
      "Training loss for batch 1448 : 0.017368163913488388\n",
      "Training loss for batch 1449 : 0.09427554160356522\n",
      "Training loss for batch 1450 : 0.0603102408349514\n",
      "Training loss for batch 1451 : 0.1403844803571701\n",
      "Training loss for batch 1452 : 0.13957469165325165\n",
      "Training loss for batch 1453 : 0.054526157677173615\n",
      "Training loss for batch 1454 : 0.0871303379535675\n",
      "Training loss for batch 1455 : 0.08257261663675308\n",
      "Training loss for batch 1456 : 0.03447667136788368\n",
      "Training loss for batch 1457 : 0.10648485273122787\n",
      "Training loss for batch 1458 : 0.10028235614299774\n",
      "Training loss for batch 1459 : 0.06490770727396011\n",
      "Training loss for batch 1460 : 0.05332168936729431\n",
      "Training loss for batch 1461 : 0.08579865843057632\n",
      "Training loss for batch 1462 : 0.09581851959228516\n",
      "Training loss for batch 1463 : 0.03864012658596039\n",
      "Training loss for batch 1464 : 0.04143694415688515\n",
      "Training loss for batch 1465 : 0.08110365271568298\n",
      "Training loss for batch 1466 : 0.005695641040802002\n",
      "Training loss for batch 1467 : 0.07480074465274811\n",
      "Training loss for batch 1468 : 0.03949424996972084\n",
      "Training loss for batch 1469 : 0.010280720889568329\n",
      "Training loss for batch 1470 : 0.03864773362874985\n",
      "Training loss for batch 1471 : 0.08191396296024323\n",
      "Training loss for batch 1472 : 0.133917436003685\n",
      "Training loss for batch 1473 : 0.10434333980083466\n",
      "Training loss for batch 1474 : 0.14709243178367615\n",
      "Training loss for batch 1475 : 0.1321723610162735\n",
      "Training loss for batch 1476 : 0.011401152238249779\n",
      "Training loss for batch 1477 : 0.034388184547424316\n",
      "Training loss for batch 1478 : 0.04129577800631523\n",
      "Training loss for batch 1479 : 0.09894977509975433\n",
      "Training loss for batch 1480 : 0.026321694254875183\n",
      "Training loss for batch 1481 : 0.11289432644844055\n",
      "Training loss for batch 1482 : 0.019039710983633995\n",
      "Training loss for batch 1483 : 0.07015040516853333\n",
      "Training loss for batch 1484 : 0.08391018211841583\n",
      "Training loss for batch 1485 : 0.13656997680664062\n",
      "Training loss for batch 1486 : 0.07212387025356293\n",
      "Training loss for batch 1487 : 0.09280446916818619\n",
      "Training loss for batch 1488 : 0.039814386516809464\n",
      "Training loss for batch 1489 : 0.19000181555747986\n",
      "Training loss for batch 1490 : 0.09986981749534607\n",
      "Training loss for batch 1491 : 0.07007936388254166\n",
      "Training loss for batch 1492 : 0.20078891515731812\n",
      "Training loss for batch 1493 : 0.019442835822701454\n",
      "Training loss for batch 1494 : 0.06448640674352646\n",
      "Training loss for batch 1495 : 0.19196759164333344\n",
      "Training loss for batch 1496 : 0.1252039670944214\n",
      "Training loss for batch 1497 : 0.1783149093389511\n",
      "Training loss for batch 1498 : 0.12385112047195435\n",
      "Training loss for batch 1499 : 0.07796907424926758\n",
      "Training loss for batch 1500 : 0.09036615490913391\n",
      "Training loss for batch 1501 : 0.1696833074092865\n",
      "Training loss for batch 1502 : 0.1317170411348343\n",
      "Training loss for batch 1503 : 0.0600699745118618\n",
      "Training loss for batch 1504 : 0.06491299718618393\n",
      "Training loss for batch 1505 : 0.07958678901195526\n",
      "Training loss for batch 1506 : 0.01873096078634262\n",
      "Training loss for batch 1507 : 0.06246505305171013\n",
      "Training loss for batch 1508 : 0.09269586205482483\n",
      "Training loss for batch 1509 : 0.12294495850801468\n",
      "Training loss for batch 1510 : 0.17687638103961945\n",
      "Training loss for batch 1511 : 0.05418006330728531\n",
      "Training loss for batch 1512 : 0.18264412879943848\n",
      "Training loss for batch 1513 : 0.03884650021791458\n",
      "Training loss for batch 1514 : 0.13612718880176544\n",
      "Training loss for batch 1515 : 0.05391523241996765\n",
      "Training loss for batch 1516 : 0.1066010519862175\n",
      "Training loss for batch 1517 : 0.07349671423435211\n",
      "Training loss for batch 1518 : 0.08112303912639618\n",
      "Training loss for batch 1519 : 0.07699461281299591\n",
      "Training loss for batch 1520 : 0.10525385290384293\n",
      "Training loss for batch 1521 : 0.09265630692243576\n",
      "Training loss for batch 1522 : 0.17359952628612518\n",
      "Training loss for batch 1523 : 0.05111147090792656\n",
      "Training loss for batch 1524 : 0.008074061945080757\n",
      "Training loss for batch 1525 : 0.1966875195503235\n",
      "Training loss for batch 1526 : 0.01663186401128769\n",
      "Training loss for batch 1527 : 0.038512445986270905\n",
      "Training loss for batch 1528 : 0.08058803528547287\n",
      "Training loss for batch 1529 : 0.08997359126806259\n",
      "Training loss for batch 1530 : 0.055383358150720596\n",
      "Training loss for batch 1531 : 0.09410836547613144\n",
      "Training loss for batch 1532 : 0.1022128313779831\n",
      "Training loss for batch 1533 : 0.10065004974603653\n",
      "Training loss for batch 1534 : 0.05692877992987633\n",
      "Training loss for batch 1535 : 0.09300320595502853\n",
      "Training loss for batch 1536 : 0.06184614822268486\n",
      "Training loss for batch 1537 : 0.08633196353912354\n",
      "Training loss for batch 1538 : 0.0756087377667427\n",
      "Training loss for batch 1539 : 0.1309727430343628\n",
      "Training loss for batch 1540 : 0.06404786556959152\n",
      "Training loss for batch 1541 : 0.0430758073925972\n",
      "Training loss for batch 1542 : 0.11012094467878342\n",
      "Training loss for batch 1543 : 0.06743451952934265\n",
      "Training loss for batch 1544 : 0.06216893345117569\n",
      "Training loss for batch 1545 : 0.07239661365747452\n",
      "Training loss for batch 1546 : 0.02311520278453827\n",
      "Training loss for batch 1547 : 0.009202445857226849\n",
      "Training loss for batch 1548 : 0.051191508769989014\n",
      "Training loss for batch 1549 : 0.03793604299426079\n",
      "Training loss for batch 1550 : 0.12750013172626495\n",
      "Training loss for batch 1551 : 0.05295298993587494\n",
      "Training loss for batch 1552 : 0.11729690432548523\n",
      "Training loss for batch 1553 : 0.017312144860625267\n",
      "Training loss for batch 1554 : 0.13339248299598694\n",
      "Training loss for batch 1555 : 0.018588760867714882\n",
      "Training loss for batch 1556 : 0.07797478139400482\n",
      "Training loss for batch 1557 : 0.0324699729681015\n",
      "Training loss for batch 1558 : 0.047022607177495956\n",
      "Training loss for batch 1559 : 0.227663055062294\n",
      "Training loss for batch 1560 : 0.16580799221992493\n",
      "Training loss for batch 1561 : 0.06928274035453796\n",
      "Training loss for batch 1562 : 0.07739140838384628\n",
      "Training loss for batch 1563 : 0.07580605894327164\n",
      "Training loss for batch 1564 : 0.034747444093227386\n",
      "Training loss for batch 1565 : 0.011595260351896286\n",
      "Training loss for batch 1566 : 0.3290485441684723\n",
      "Training loss for batch 1567 : 0.04038732498884201\n",
      "Training loss for batch 1568 : 0.1548623889684677\n",
      "Training loss for batch 1569 : 0.018884725868701935\n",
      "Training loss for batch 1570 : 0.05566704645752907\n",
      "Training loss for batch 1571 : 0.11411753296852112\n",
      "Training loss for batch 1572 : 0.09879414737224579\n",
      "Training loss for batch 1573 : 0.07187528908252716\n",
      "Training loss for batch 1574 : 0.10462997108697891\n",
      "Training loss for batch 1575 : 0.1661236584186554\n",
      "Training loss for batch 1576 : 0.2921355366706848\n",
      "Training loss for batch 1577 : 0.13699333369731903\n",
      "Training loss for batch 1578 : 0.07347051799297333\n",
      "Training loss for batch 1579 : 0.10523873567581177\n",
      "Training loss for batch 1580 : 0.1353607326745987\n",
      "Training loss for batch 1581 : 0.09141503274440765\n",
      "Training loss for batch 1582 : 0.05761419236660004\n",
      "Training loss for batch 1583 : 0.1257573962211609\n",
      "Training loss for batch 1584 : 0.15421408414840698\n",
      "Training loss for batch 1585 : 0.04602401703596115\n",
      "Training loss for batch 1586 : 0.1831870973110199\n",
      "Training loss for batch 1587 : 0.05667855963110924\n",
      "Training loss for batch 1588 : 0.007851410657167435\n",
      "Training loss for batch 1589 : 0.1214432641863823\n",
      "Training loss for batch 1590 : 0.28097596764564514\n",
      "Training loss for batch 1591 : 0.10051294416189194\n",
      "Training loss for batch 1592 : 0.08824057132005692\n",
      "Training loss for batch 1593 : 0.21442732214927673\n",
      "Training loss for batch 1594 : 0.18690167367458344\n",
      "Training loss for batch 1595 : 0.07586856186389923\n",
      "Training loss for batch 1596 : 0.16543936729431152\n",
      "Training loss for batch 1597 : 0.1016484797000885\n",
      "Training loss for batch 1598 : 0.06444555521011353\n",
      "Training loss for batch 1599 : 0.1882464587688446\n",
      "Training loss for batch 1600 : 0.043110743165016174\n",
      "Training loss for batch 1601 : 0.06850054115056992\n",
      "Training loss for batch 1602 : 0.07872876524925232\n",
      "Training loss for batch 1603 : 0.021654443815350533\n",
      "Training loss for batch 1604 : 0.0552835650742054\n",
      "Training loss for batch 1605 : 0.19365477561950684\n",
      "Training loss for batch 1606 : 0.04245641455054283\n",
      "Training loss for batch 1607 : 0.10283268988132477\n",
      "Training loss for batch 1608 : 0.11880706250667572\n",
      "Training loss for batch 1609 : 0.031069844961166382\n",
      "Training loss for batch 1610 : 0.08504757285118103\n",
      "Training loss for batch 1611 : 0.09185115993022919\n",
      "Training loss for batch 1612 : 0.05221382528543472\n",
      "Training loss for batch 1613 : 0.04420939087867737\n",
      "Training loss for batch 1614 : 0.09228823333978653\n",
      "Training loss for batch 1615 : 0.17690199613571167\n",
      "Training loss for batch 1616 : 0.08703593909740448\n",
      "Training loss for batch 1617 : 0.06867451965808868\n",
      "Training loss for batch 1618 : 0.11889831721782684\n",
      "Training loss for batch 1619 : 0.07987266033887863\n",
      "Training loss for batch 1620 : 0.1217130795121193\n",
      "Training loss for batch 1621 : 0.12927749752998352\n",
      "Training loss for batch 1622 : 0.021236475557088852\n",
      "Training loss for batch 1623 : 0.1032494455575943\n",
      "Training loss for batch 1624 : 0.07331964373588562\n",
      "Training loss for batch 1625 : 0.12921753525733948\n",
      "Training loss for batch 1626 : 0.15255211293697357\n",
      "Training loss for batch 1627 : 0.021417509764432907\n",
      "Training loss for batch 1628 : 0.01303276140242815\n",
      "Training loss for batch 1629 : 0.03377831354737282\n",
      "Training loss for batch 1630 : 0.05687350034713745\n",
      "Training loss for batch 1631 : 0.07428021728992462\n",
      "Training loss for batch 1632 : 0.19253093004226685\n",
      "Training loss for batch 1633 : 0.10193346440792084\n",
      "Training loss for batch 1634 : 0.027694322168827057\n",
      "Training loss for batch 1635 : 0.11425397545099258\n",
      "Training loss for batch 1636 : 0.02488158643245697\n",
      "Training loss for batch 1637 : 0.13348276913166046\n",
      "Training loss for batch 1638 : 0.02843732200562954\n",
      "Training loss for batch 1639 : 0.10435163974761963\n",
      "Training loss for batch 1640 : 0.1491832137107849\n",
      "Training loss for batch 1641 : 0.09003736823797226\n",
      "Training loss for batch 1642 : 0.15837065875530243\n",
      "Training loss for batch 1643 : 0.021373799070715904\n",
      "Training loss for batch 1644 : 0.11736579984426498\n",
      "Training loss for batch 1645 : 0.16516755521297455\n",
      "Training loss for batch 1646 : 0.11282670497894287\n",
      "Training loss for batch 1647 : 0.13064874708652496\n",
      "Training loss for batch 1648 : 0.15261632204055786\n",
      "Training loss for batch 1649 : 0.024122204631567\n",
      "Training loss for batch 1650 : 0.1117214560508728\n",
      "Training loss for batch 1651 : 0.03189833462238312\n",
      "Training loss for batch 1652 : 0.13685716688632965\n",
      "Training loss for batch 1653 : 0.10050801187753677\n",
      "Training loss for batch 1654 : 0.10108835250139236\n",
      "Training loss for batch 1655 : 0.04522590711712837\n",
      "Training loss for batch 1656 : 0.1448606550693512\n",
      "Training loss for batch 1657 : 0.06733794510364532\n",
      "Training loss for batch 1658 : 0.0331852026283741\n",
      "Training loss for batch 1659 : 0.020867107436060905\n",
      "Training loss for batch 1660 : 0.07626023888587952\n",
      "Training loss for batch 1661 : 0.031150206923484802\n",
      "Training loss for batch 1662 : 0.07580067962408066\n",
      "Training loss for batch 1663 : 0.04707449674606323\n",
      "Training loss for batch 1664 : 0.0761747732758522\n",
      "Training loss for batch 1665 : 0.10370667278766632\n",
      "Training loss for batch 1666 : 0.007554469630122185\n",
      "Training loss for batch 1667 : 0.20063026249408722\n",
      "Training loss for batch 1668 : 0.052245911210775375\n",
      "Training loss for batch 1669 : 0.01816253364086151\n",
      "Training loss for batch 1670 : 0.03169748932123184\n",
      "Training loss for batch 1671 : 0.09244618564844131\n",
      "Training loss for batch 1672 : 0.09435625374317169\n",
      "Training loss for batch 1673 : 0.058751270174980164\n",
      "Training loss for batch 1674 : 0.23084883391857147\n",
      "Training loss for batch 1675 : 0.012432845309376717\n",
      "Training loss for batch 1676 : 0.10110284388065338\n",
      "Training loss for batch 1677 : 0.04518992826342583\n",
      "Training loss for batch 1678 : 0.02828541025519371\n",
      "Training loss for batch 1679 : 0.1518486738204956\n",
      "Training loss for batch 1680 : 0.15495765209197998\n",
      "Training loss for batch 1681 : 0.0460502915084362\n",
      "Training loss for batch 1682 : 0.06848211586475372\n",
      "Training loss for batch 1683 : 0.12259221076965332\n",
      "Training loss for batch 1684 : 0.11642421782016754\n",
      "Training loss for batch 1685 : 0.23584352433681488\n",
      "Training loss for batch 1686 : 0.06222224980592728\n",
      "Training loss for batch 1687 : 0.1282663345336914\n",
      "Training loss for batch 1688 : 0.13498225808143616\n",
      "Training loss for batch 1689 : 0.09282530099153519\n",
      "Training loss for batch 1690 : 0.03826019912958145\n",
      "Training loss for batch 1691 : 0.04250074550509453\n",
      "Training loss for batch 1692 : 0.02865881472826004\n",
      "Training loss for batch 1693 : 0.06798974424600601\n",
      "Training loss for batch 1694 : 0.00554079283028841\n",
      "Training loss for batch 1695 : 0.12573163211345673\n",
      "Training loss for batch 1696 : 0.11255426704883575\n",
      "Training loss for batch 1697 : 0.1012953519821167\n",
      "Training loss for batch 1698 : 0.1428447663784027\n",
      "Training loss for batch 1699 : 0.025364244356751442\n",
      "Training loss for batch 1700 : 0.01928858645260334\n",
      "Training loss for batch 1701 : 0.06965465843677521\n",
      "Training loss for batch 1702 : 0.11625203490257263\n",
      "Training loss for batch 1703 : 0.10563618689775467\n",
      "Training loss for batch 1704 : 0.12413650006055832\n",
      "Training loss for batch 1705 : 0.11254994571208954\n",
      "Training loss for batch 1706 : 0.09831566363573074\n",
      "Training loss for batch 1707 : 0.0809965431690216\n",
      "Training loss for batch 1708 : 0.01420129369944334\n",
      "Training loss for batch 1709 : 0.07944795489311218\n",
      "Training loss for batch 1710 : 0.025228198617696762\n",
      "Training loss for batch 1711 : 0.04531042277812958\n",
      "Training loss for batch 1712 : 0.059722237288951874\n",
      "Training loss for batch 1713 : 0.11234800517559052\n",
      "Training loss for batch 1714 : 0.06325545907020569\n",
      "Training loss for batch 1715 : 0.011765128001570702\n",
      "Training loss for batch 1716 : 0.019361985847353935\n",
      "Training loss for batch 1717 : 0.0494660921394825\n",
      "Training loss for batch 1718 : 0.11794184148311615\n",
      "Training loss for batch 1719 : 0.10289021581411362\n",
      "Training loss for batch 1720 : 0.1280716210603714\n",
      "Training loss for batch 1721 : 0.16860240697860718\n",
      "Training loss for batch 1722 : 0.07272972166538239\n",
      "Training loss for batch 1723 : 0.13733850419521332\n",
      "Training loss for batch 1724 : 0.24942755699157715\n",
      "Training loss for batch 1725 : 0.027757816016674042\n",
      "Training loss for batch 1726 : 0.13581722974777222\n",
      "Training loss for batch 1727 : 0.053054146468639374\n",
      "Training loss for batch 1728 : 0.005457856692373753\n",
      "Training loss for batch 1729 : 0.022796623408794403\n",
      "Training loss for batch 1730 : 0.011414892971515656\n",
      "Training loss for batch 1731 : 0.10654060542583466\n",
      "Training loss for batch 1732 : 0.07592955231666565\n",
      "Training loss for batch 1733 : 0.1310272067785263\n",
      "Training loss for batch 1734 : 0.10292224586009979\n",
      "Training loss for batch 1735 : 0.11789930611848831\n",
      "Training loss for batch 1736 : 0.12763454020023346\n",
      "Training loss for batch 1737 : 0.18709313869476318\n",
      "Training loss for batch 1738 : 0.04575680196285248\n",
      "Training loss for batch 1739 : 0.09392772614955902\n",
      "Training loss for batch 1740 : 0.05096503719687462\n",
      "Training loss for batch 1741 : 0.13761110603809357\n",
      "Training loss for batch 1742 : 0.09631279110908508\n",
      "Training loss for batch 1743 : 0.08221491426229477\n",
      "Training loss for batch 1744 : 0.06461339443922043\n",
      "Training loss for batch 1745 : 0.1623140275478363\n",
      "Training loss for batch 1746 : 3.061740372345412e-08\n",
      "Training loss for batch 1747 : 0.03142336755990982\n",
      "Training loss for batch 1748 : 0.05110638961195946\n",
      "Training loss for batch 1749 : 0.02431558631360531\n",
      "Training loss for batch 1750 : 0.017407309263944626\n",
      "Training loss for batch 1751 : 0.08844916522502899\n",
      "Training loss for batch 1752 : 0.11735238134860992\n",
      "Training loss for batch 1753 : 0.12251878529787064\n",
      "Training loss for batch 1754 : 0.020153282210230827\n",
      "Training loss for batch 1755 : 0.12622573971748352\n",
      "Training loss for batch 1756 : 0.14942514896392822\n",
      "Training loss for batch 1757 : 0.01857023872435093\n",
      "Training loss for batch 1758 : 0.11359091848134995\n",
      "Training loss for batch 1759 : 0.14575986564159393\n",
      "Training loss for batch 1760 : 0.02035403624176979\n",
      "Training loss for batch 1761 : 0.06543385237455368\n",
      "Training loss for batch 1762 : 0.021468952298164368\n",
      "Training loss for batch 1763 : 0.05886414274573326\n",
      "Training loss for batch 1764 : 0.05355580523610115\n",
      "Training loss for batch 1765 : 0.1710423082113266\n",
      "Training loss for batch 1766 : 0.10956353694200516\n",
      "Training loss for batch 1767 : 0.04961935803294182\n",
      "Training loss for batch 1768 : 0.07477071136236191\n",
      "Training loss for batch 1769 : 0.07854588329792023\n",
      "Training loss for batch 1770 : 0.07780129462480545\n",
      "Training loss for batch 1771 : 0.08287174254655838\n",
      "Training loss for batch 1772 : 0.1311074048280716\n",
      "Training loss for batch 1773 : 0.10577934235334396\n",
      "Training loss for batch 1774 : 0.07306447625160217\n",
      "Training loss for batch 1775 : 0.013144736178219318\n",
      "Training loss for batch 1776 : 0.04765228182077408\n",
      "Training loss for batch 1777 : 0.12490956485271454\n",
      "Training loss for batch 1778 : 0.02217273786664009\n",
      "Training loss for batch 1779 : 0.2639855444431305\n",
      "Training loss for batch 1780 : 0.03901142627000809\n",
      "Training loss for batch 1781 : 0.06818947941064835\n",
      "Training loss for batch 1782 : 0.09529075771570206\n",
      "Training loss for batch 1783 : 0.21052689850330353\n",
      "Training loss for batch 1784 : 0.0620448961853981\n",
      "Training loss for batch 1785 : 0.022994546219706535\n",
      "Training loss for batch 1786 : 0.04752117767930031\n",
      "Training loss for batch 1787 : 0.07124574482440948\n",
      "Training loss for batch 1788 : 0.052607446908950806\n",
      "Training loss for batch 1789 : 0.07309916615486145\n",
      "Training loss for batch 1790 : 0.059161387383937836\n",
      "Training loss for batch 1791 : 0.044470492750406265\n",
      "Training loss for batch 1792 : 0.04141052067279816\n",
      "Training loss for batch 1793 : 0.027252279222011566\n",
      "Training loss for batch 1794 : 0.0029469518922269344\n",
      "Training loss for batch 1795 : 0.07353907078504562\n",
      "Training loss for batch 1796 : 0.10043580085039139\n",
      "Training loss for batch 1797 : 0.17881396412849426\n",
      "Training loss for batch 1798 : 0.02227882109582424\n",
      "Training loss for batch 1799 : 0.09915237873792648\n",
      "Training loss for batch 1800 : 0.03540247678756714\n",
      "Training loss for batch 1801 : 0.031160585582256317\n",
      "Training loss for batch 1802 : 0.16294939815998077\n",
      "Training loss for batch 1803 : 0.05542543902993202\n",
      "Training loss for batch 1804 : 0.35518360137939453\n",
      "Training loss for batch 1805 : 0.08821495622396469\n",
      "Training loss for batch 1806 : 0.10621950030326843\n",
      "Training loss for batch 1807 : 0.04610016196966171\n",
      "Training loss for batch 1808 : 0.17417632043361664\n",
      "Training loss for batch 1809 : 0.11421982198953629\n",
      "Training loss for batch 1810 : 0.1173970177769661\n",
      "Training loss for batch 1811 : 0.12747542560100555\n",
      "Training loss for batch 1812 : 0.1641562283039093\n",
      "Training loss for batch 1813 : 0.1390165537595749\n",
      "Training loss for batch 1814 : 0.1078566387295723\n",
      "Training loss for batch 1815 : 0.25476157665252686\n",
      "Training loss for batch 1816 : 0.07622598856687546\n",
      "Training loss for batch 1817 : 0.09427978098392487\n",
      "Training loss for batch 1818 : 0.03857534006237984\n",
      "Training loss for batch 1819 : 0.2778557240962982\n",
      "Training loss for batch 1820 : 0.09611409902572632\n",
      "Training loss for batch 1821 : 0.02391030266880989\n",
      "Training loss for batch 1822 : 0.05583389103412628\n",
      "Training loss for batch 1823 : 0.05636408552527428\n",
      "Training loss for batch 1824 : 0.03465922921895981\n",
      "Training loss for batch 1825 : 0.11394822597503662\n",
      "Training loss for batch 1826 : 0.06272731721401215\n",
      "Training loss for batch 1827 : 0.01555639412254095\n",
      "Training loss for batch 1828 : 0.03729292005300522\n",
      "Training loss for batch 1829 : 0.02426784113049507\n",
      "Training loss for batch 1830 : 0.03769813850522041\n",
      "Training loss for batch 1831 : 0.013031496666371822\n",
      "Training loss for batch 1832 : 0.1121860072016716\n",
      "Training loss for batch 1833 : 0.03774847835302353\n",
      "Training loss for batch 1834 : 0.06776871532201767\n",
      "Training loss for batch 1835 : 0.03168635815382004\n",
      "Training loss for batch 1836 : 0.1783168613910675\n",
      "Training loss for batch 1837 : 0.13273262977600098\n",
      "Training loss for batch 1838 : 0.019008690491318703\n",
      "Training loss for batch 1839 : 0.06309874355792999\n",
      "Training loss for batch 1840 : 0.029766304418444633\n",
      "Training loss for batch 1841 : 0.07784700393676758\n",
      "Training loss for batch 1842 : 0.07240691781044006\n",
      "Training loss for batch 1843 : 0.045510318130254745\n",
      "Training loss for batch 1844 : 0.17140167951583862\n",
      "Training loss for batch 1845 : 0.1583881378173828\n",
      "Training loss for batch 1846 : 0.14946430921554565\n",
      "Training loss for batch 1847 : 0.13809114694595337\n",
      "Training loss for batch 1848 : 0.06984015554189682\n",
      "Training loss for batch 1849 : 0.06720451265573502\n",
      "Training loss for batch 1850 : 0.04629797860980034\n",
      "Training loss for batch 1851 : 0.014884360134601593\n",
      "Training loss for batch 1852 : 0.007715200539678335\n",
      "Training loss for batch 1853 : 0.14469698071479797\n",
      "Training loss for batch 1854 : 0.12009921669960022\n",
      "Training loss for batch 1855 : 0.015691513195633888\n",
      "Training loss for batch 1856 : 0.017561854794621468\n",
      "Training loss for batch 1857 : 0.037181418389081955\n",
      "Training loss for batch 1858 : 0.09402697533369064\n",
      "Training loss for batch 1859 : 0.1160692572593689\n",
      "Training loss for batch 1860 : 0.019214021041989326\n",
      "Training loss for batch 1861 : 0.04717468470335007\n",
      "Training loss for batch 1862 : 0.012880195863544941\n",
      "Training loss for batch 1863 : 0.030372826382517815\n",
      "Training loss for batch 1864 : 0.15713469684123993\n",
      "Training loss for batch 1865 : 0.07463221251964569\n",
      "Training loss for batch 1866 : 0.0\n",
      "Training loss for batch 1867 : 0.068964883685112\n",
      "Training loss for batch 1868 : 0.03925514221191406\n",
      "Training loss for batch 1869 : 0.01648084446787834\n",
      "Training loss for batch 1870 : 0.16602341830730438\n",
      "Training loss for batch 1871 : 0.07056308537721634\n",
      "Training loss for batch 1872 : 0.10493038594722748\n",
      "Training loss for batch 1873 : 0.08133664727210999\n",
      "Training loss for batch 1874 : 0.09196591377258301\n",
      "Training loss for batch 1875 : 0.10297146439552307\n",
      "Training loss for batch 1876 : 0.10873427987098694\n",
      "Training loss for batch 1877 : 0.07171579450368881\n",
      "Training loss for batch 1878 : 0.1409551501274109\n",
      "Training loss for batch 1879 : 0.132074773311615\n",
      "Training loss for batch 1880 : 0.04558755084872246\n",
      "Training loss for batch 1881 : 0.03737534582614899\n",
      "Training loss for batch 1882 : 0.02447992004454136\n",
      "Training loss for batch 1883 : 0.0660620778799057\n",
      "Training loss for batch 1884 : 0.048536960035562515\n",
      "Training loss for batch 1885 : 0.028894608840346336\n",
      "Training loss for batch 1886 : 0.017112642526626587\n",
      "Training loss for batch 1887 : 0.03495334833860397\n",
      "Training loss for batch 1888 : 0.004988045431673527\n",
      "Training loss for batch 1889 : 0.21374502778053284\n",
      "Training loss for batch 1890 : 0.02533479779958725\n",
      "Training loss for batch 1891 : 0.1732373833656311\n",
      "Training loss for batch 1892 : 0.05619923770427704\n",
      "Training loss for batch 1893 : 0.2843186855316162\n",
      "Training loss for batch 1894 : 0.09997107833623886\n",
      "Training loss for batch 1895 : 0.014656197279691696\n",
      "Training loss for batch 1896 : 0.05062796548008919\n",
      "Training loss for batch 1897 : 0.053545139729976654\n",
      "Training loss for batch 1898 : 0.10960602760314941\n",
      "Training loss for batch 1899 : 0.11138886213302612\n",
      "Training loss for batch 1900 : 0.009022989310324192\n",
      "Training loss for batch 1901 : 0.12608039379119873\n",
      "Training loss for batch 1902 : 0.1647026389837265\n",
      "Training loss for batch 1903 : 0.012411151081323624\n",
      "Training loss for batch 1904 : 0.16715897619724274\n",
      "Training loss for batch 1905 : 0.24845199286937714\n",
      "Training loss for batch 1906 : 0.043429896235466\n",
      "Training loss for batch 1907 : 0.085024394094944\n",
      "Training loss for batch 1908 : 0.027776522561907768\n",
      "Training loss for batch 1909 : 0.10567710548639297\n",
      "Training loss for batch 1910 : 0.1158331111073494\n",
      "Training loss for batch 1911 : 0.1282627135515213\n",
      "Training loss for batch 1912 : 0.1180829107761383\n",
      "Training loss for batch 1913 : 0.10193564742803574\n",
      "Training loss for batch 1914 : 0.005003069993108511\n",
      "Training loss for batch 1915 : 0.059999018907547\n",
      "Training loss for batch 1916 : 0.08244273066520691\n",
      "Training loss for batch 1917 : 0.080048106610775\n",
      "Training loss for batch 1918 : 0.02294696494936943\n",
      "Training loss for batch 1919 : 0.08976486325263977\n",
      "Training loss for batch 1920 : 0.055938564240932465\n",
      "Training loss for batch 1921 : 0.28644752502441406\n",
      "Training loss for batch 1922 : 0.026528574526309967\n",
      "Training loss for batch 1923 : 0.09677325189113617\n",
      "Training loss for batch 1924 : 0.13433198630809784\n",
      "Training loss for batch 1925 : 0.07126739621162415\n",
      "Training loss for batch 1926 : 0.1767076700925827\n",
      "Training loss for batch 1927 : 0.018968800082802773\n",
      "Training loss for batch 1928 : 0.08194312453269958\n",
      "Training loss for batch 1929 : 0.03924578055739403\n",
      "Training loss for batch 1930 : 0.1355174481868744\n",
      "Training loss for batch 1931 : 0.18883639574050903\n",
      "Training loss for batch 1932 : 0.08920057862997055\n",
      "Training loss for batch 1933 : 0.08388581871986389\n",
      "Training loss for batch 1934 : 0.06887225061655045\n",
      "Training loss for batch 1935 : 0.15623515844345093\n",
      "Training loss for batch 1936 : 0.042481839656829834\n",
      "Training loss for batch 1937 : 0.03533945605158806\n",
      "Training loss for batch 1938 : 0.1634584218263626\n",
      "Training loss for batch 1939 : 0.11673867702484131\n",
      "Training loss for batch 1940 : 0.06706941872835159\n",
      "Training loss for batch 1941 : 0.0748385488986969\n",
      "Training loss for batch 1942 : 0.10960502177476883\n",
      "Training loss for batch 1943 : 0.11387734115123749\n",
      "Training loss for batch 1944 : 0.016041887924075127\n",
      "Training loss for batch 1945 : 0.03618072345852852\n",
      "Training loss for batch 1946 : 0.19371549785137177\n",
      "Training loss for batch 1947 : 0.035591840744018555\n",
      "Training loss for batch 1948 : 0.11069566756486893\n",
      "Training loss for batch 1949 : 0.05537169426679611\n",
      "Training loss for batch 1950 : 0.023495595902204514\n",
      "Training loss for batch 1951 : 0.06807675957679749\n",
      "Training loss for batch 1952 : 0.14634357392787933\n",
      "Training loss for batch 1953 : 0.10677628964185715\n",
      "Training loss for batch 1954 : 0.07147760689258575\n",
      "Training loss for batch 1955 : 0.0718320906162262\n",
      "Training loss for batch 1956 : 0.09661650657653809\n",
      "Training loss for batch 1957 : 0.13969269394874573\n",
      "Training loss for batch 1958 : 0.030488045886158943\n",
      "Training loss for batch 1959 : 0.10239697247743607\n",
      "Training loss for batch 1960 : 0.06971178948879242\n",
      "Training loss for batch 1961 : 0.16936752200126648\n",
      "Training loss for batch 1962 : 0.10125542432069778\n",
      "Training loss for batch 1963 : 0.1497204303741455\n",
      "Training loss for batch 1964 : 0.15336544811725616\n",
      "Training loss for batch 1965 : 0.047853145748376846\n",
      "Training loss for batch 1966 : 0.06239262968301773\n",
      "Training loss for batch 1967 : 0.12427869439125061\n",
      "Training loss for batch 1968 : 0.09153169393539429\n",
      "Training loss for batch 1969 : 0.024668646976351738\n",
      "Training loss for batch 1970 : 0.033550821244716644\n",
      "Training loss for batch 1971 : 0.045570436865091324\n",
      "Training loss for batch 1972 : 0.07074464857578278\n",
      "Training loss for batch 1973 : 0.08584874868392944\n",
      "Training loss for batch 1974 : 0.02364981919527054\n",
      "Training loss for batch 1975 : 0.14854241907596588\n",
      "Training loss for batch 1976 : 0.021035922691226006\n",
      "Training loss for batch 1977 : 0.10486198961734772\n",
      "Training loss for batch 1978 : 0.027134248986840248\n",
      "Training loss for batch 1979 : 0.08255720883607864\n",
      "Training loss for batch 1980 : 0.05897253751754761\n",
      "Training loss for batch 1981 : 0.09005635231733322\n",
      "Training loss for batch 1982 : 0.07234417647123337\n",
      "Training loss for batch 1983 : 0.04371834173798561\n",
      "Training loss for batch 1984 : 0.21521694958209991\n",
      "Training loss for batch 1985 : 0.053849607706069946\n",
      "Training loss for batch 1986 : 0.17453311383724213\n",
      "Training loss for batch 1987 : 0.0012857158435508609\n",
      "Training loss for batch 1988 : 0.033697210252285004\n",
      "Training loss for batch 1989 : 0.06023323908448219\n",
      "Training loss for batch 1990 : 0.13406825065612793\n",
      "Training loss for batch 1991 : 0.19687263667583466\n",
      "Training loss for batch 1992 : 0.11004970967769623\n",
      "Training loss for batch 1993 : 0.0957828164100647\n",
      "Training loss for batch 1994 : 0.04429003596305847\n",
      "Training loss for batch 1995 : 0.07450409978628159\n",
      "Training loss for batch 1996 : 0.042792271822690964\n",
      "Training loss for batch 1997 : 0.197999969124794\n",
      "Training loss for batch 1998 : 0.07819535583257675\n",
      "Training loss for batch 1999 : 0.20541569590568542\n",
      "Training loss for batch 2000 : 0.08605176210403442\n",
      "Training loss for batch 2001 : 0.07922777533531189\n",
      "Training loss for batch 2002 : 0.02772602066397667\n",
      "Training loss for batch 2003 : 0.09280906617641449\n",
      "Training loss for batch 2004 : 0.09021773189306259\n",
      "Training loss for batch 2005 : 0.04064178094267845\n",
      "Training loss for batch 2006 : 0.04539038985967636\n",
      "Training loss for batch 2007 : 0.011938552372157574\n",
      "Training loss for batch 2008 : 0.08791173249483109\n",
      "Training loss for batch 2009 : 0.0936545878648758\n",
      "Training loss for batch 2010 : 0.13207930326461792\n",
      "Training loss for batch 2011 : 0.017118802294135094\n",
      "Training loss for batch 2012 : 0.06376601755619049\n",
      "Training loss for batch 2013 : 0.2383435219526291\n",
      "Training loss for batch 2014 : 0.30016767978668213\n",
      "Training loss for batch 2015 : 0.07444153726100922\n",
      "Training loss for batch 2016 : 0.24316516518592834\n",
      "Training loss for batch 2017 : 0.12274029850959778\n",
      "Training loss for batch 2018 : 0.14512421190738678\n",
      "Training loss for batch 2019 : 0.08190733939409256\n",
      "Training loss for batch 2020 : 0.03774000704288483\n",
      "Training loss for batch 2021 : 0.053514547646045685\n",
      "Training loss for batch 2022 : 0.19250433146953583\n",
      "Training loss for batch 2023 : 0.034301429986953735\n",
      "Training loss for batch 2024 : 0.09339946508407593\n",
      "Training loss for batch 2025 : 0.08446122705936432\n",
      "Training loss for batch 2026 : 0.07875441014766693\n",
      "Training loss for batch 2027 : 0.16165988147258759\n",
      "Training loss for batch 2028 : 0.10256331413984299\n",
      "Training loss for batch 2029 : 0.1560269296169281\n",
      "Training loss for batch 2030 : 0.04586051031947136\n",
      "Training loss for batch 2031 : 0.04880953207612038\n",
      "Training loss for batch 2032 : 0.08230779320001602\n",
      "Training loss for batch 2033 : 0.024619445204734802\n",
      "Training loss for batch 2034 : 0.1120300367474556\n",
      "Training loss for batch 2035 : 0.08081531524658203\n",
      "Training loss for batch 2036 : 0.20009468495845795\n",
      "Training loss for batch 2037 : 0.11714169383049011\n",
      "Training loss for batch 2038 : 0.19708608090877533\n",
      "Training loss for batch 2039 : 0.28030896186828613\n",
      "Training loss for batch 2040 : 0.03263722360134125\n",
      "Training loss for batch 2041 : 0.061536479741334915\n",
      "Training loss for batch 2042 : 0.025730418041348457\n",
      "Training loss for batch 2043 : 0.0032918681390583515\n",
      "Training loss for batch 2044 : 0.13061150908470154\n",
      "Training loss for batch 2045 : 0.3436198830604553\n",
      "Training loss for batch 2046 : 0.11287253350019455\n",
      "Training loss for batch 2047 : 0.06627444177865982\n",
      "Training loss for batch 2048 : 0.12094675004482269\n",
      "Training loss for batch 2049 : 0.03488820046186447\n",
      "Training loss for batch 2050 : 0.0032730798702687025\n",
      "Training loss for batch 2051 : 0.26269036531448364\n",
      "Training loss for batch 2052 : 0.021457338705658913\n",
      "Training loss for batch 2053 : 0.03916168212890625\n",
      "Training loss for batch 2054 : 0.044637806713581085\n",
      "Training loss for batch 2055 : 0.10335510969161987\n",
      "Training loss for batch 2056 : 0.0934629887342453\n",
      "Training loss for batch 2057 : 0.14480826258659363\n",
      "Training loss for batch 2058 : 0.10840591043233871\n",
      "Training loss for batch 2059 : 0.12012929469347\n",
      "Training loss for batch 2060 : 0.16538986563682556\n",
      "Training loss for batch 2061 : 0.08532306551933289\n",
      "Training loss for batch 2062 : 0.06305830925703049\n",
      "Training loss for batch 2063 : 0.03554355353116989\n",
      "Training loss for batch 2064 : 0.21315497159957886\n",
      "Training loss for batch 2065 : 0.029620645567774773\n",
      "Training loss for batch 2066 : 0.13508668541908264\n",
      "Training loss for batch 2067 : 0.006942500825971365\n",
      "Training loss for batch 2068 : 0.10515854507684708\n",
      "Training loss for batch 2069 : 0.041674789041280746\n",
      "Training loss for batch 2070 : 0.15426094830036163\n",
      "Training loss for batch 2071 : 0.054711584001779556\n",
      "Training loss for batch 2072 : 0.0637255534529686\n",
      "Training loss for batch 2073 : 0.08810684829950333\n",
      "Training loss for batch 2074 : 0.07113095372915268\n",
      "Training loss for batch 2075 : 0.04456823319196701\n",
      "Training loss for batch 2076 : 0.0645461156964302\n",
      "Training loss for batch 2077 : 0.026850521564483643\n",
      "Training loss for batch 2078 : 0.06422926485538483\n",
      "Training loss for batch 2079 : 0.05593627318739891\n",
      "Training loss for batch 2080 : 0.008209899067878723\n",
      "Training loss for batch 2081 : 0.007699234411120415\n",
      "Training loss for batch 2082 : 0.035436924546957016\n",
      "Training loss for batch 2083 : 0.10999060422182083\n",
      "Training loss for batch 2084 : 0.1567484587430954\n",
      "Training loss for batch 2085 : 0.0742790624499321\n",
      "Training loss for batch 2086 : 0.039544492959976196\n",
      "Training loss for batch 2087 : 0.09407173097133636\n",
      "Training loss for batch 2088 : 0.027490848675370216\n",
      "Training loss for batch 2089 : 0.015240932814776897\n",
      "Training loss for batch 2090 : 0.09909891337156296\n",
      "Training loss for batch 2091 : 0.027199212461709976\n",
      "Training loss for batch 2092 : 0.009875261224806309\n",
      "Training loss for batch 2093 : 0.21296270191669464\n",
      "Training loss for batch 2094 : 0.16686318814754486\n",
      "Training loss for batch 2095 : 0.1035521849989891\n",
      "Training loss for batch 2096 : 0.09582380950450897\n",
      "Training loss for batch 2097 : 0.035601213574409485\n",
      "Training loss for batch 2098 : 0.027779556810855865\n",
      "Training loss for batch 2099 : 0.1093374565243721\n",
      "Training loss for batch 2100 : 0.03714399412274361\n",
      "Training loss for batch 2101 : 0.10976842790842056\n",
      "Training loss for batch 2102 : 0.06310220807790756\n",
      "Training loss for batch 2103 : 0.1494368314743042\n",
      "Training loss for batch 2104 : 0.13451436161994934\n",
      "Training loss for batch 2105 : 0.15423285961151123\n",
      "Training loss for batch 2106 : 0.09135796129703522\n",
      "Training loss for batch 2107 : 0.0703480914235115\n",
      "Training loss for batch 2108 : 0.09369940310716629\n",
      "Training loss for batch 2109 : 0.01798785850405693\n",
      "Training loss for batch 2110 : 0.08709149062633514\n",
      "Training loss for batch 2111 : 0.06095217168331146\n",
      "Training loss for batch 2112 : 0.0922926813364029\n",
      "Training loss for batch 2113 : 0.07930417358875275\n",
      "Training loss for batch 2114 : 0.04526504874229431\n",
      "Training loss for batch 2115 : 0.031268708407878876\n",
      "Training loss for batch 2116 : 0.06304743885993958\n",
      "Training loss for batch 2117 : 0.03523110970854759\n",
      "Training loss for batch 2118 : 0.055290862917900085\n",
      "Training loss for batch 2119 : 0.1579822301864624\n",
      "Training loss for batch 2120 : 0.04939521849155426\n",
      "Training loss for batch 2121 : 0.04480450600385666\n",
      "Training loss for batch 2122 : 0.07544653117656708\n",
      "Training loss for batch 2123 : 0.10447338223457336\n",
      "Training loss for batch 2124 : 0.057212673127651215\n",
      "Training loss for batch 2125 : 0.12908071279525757\n",
      "Training loss for batch 2126 : 0.08287742733955383\n",
      "Training loss for batch 2127 : 0.12190862745046616\n",
      "Training loss for batch 2128 : 0.125551238656044\n",
      "Training loss for batch 2129 : 0.09321998059749603\n",
      "Training loss for batch 2130 : 0.05713176727294922\n",
      "Training loss for batch 2131 : 0.12099644541740417\n",
      "Training loss for batch 2132 : 0.02768043242394924\n",
      "Training loss for batch 2133 : 0.03440369293093681\n",
      "Training loss for batch 2134 : 0.1009116843342781\n",
      "Training loss for batch 2135 : 0.09600469470024109\n",
      "Training loss for batch 2136 : 0.08747360855340958\n",
      "Training loss for batch 2137 : 0.16232313215732574\n",
      "Training loss for batch 2138 : 0.12201409041881561\n",
      "Training loss for batch 2139 : 0.06563011556863785\n",
      "Training loss for batch 2140 : 0.10093522071838379\n",
      "Training loss for batch 2141 : 0.027150578796863556\n",
      "Training loss for batch 2142 : 0.13459716737270355\n",
      "Training loss for batch 2143 : 0.024942412972450256\n",
      "Training loss for batch 2144 : 0.06094866245985031\n",
      "Training loss for batch 2145 : 0.05820779874920845\n",
      "Training loss for batch 2146 : 0.11962825804948807\n",
      "Training loss for batch 2147 : 0.12352557480335236\n",
      "Training loss for batch 2148 : 0.12128230184316635\n",
      "Training loss for batch 2149 : 0.039638713002204895\n",
      "Training loss for batch 2150 : 0.03254268318414688\n",
      "Training loss for batch 2151 : 0.04427729547023773\n",
      "Training loss for batch 2152 : 0.018281713128089905\n",
      "Training loss for batch 2153 : 0.041471537202596664\n",
      "Training loss for batch 2154 : 0.14476923644542694\n",
      "Training loss for batch 2155 : 0.049180272966623306\n",
      "Training loss for batch 2156 : 0.09540029615163803\n",
      "Training loss for batch 2157 : 0.09542319923639297\n",
      "Training loss for batch 2158 : 0.10279616713523865\n",
      "Training loss for batch 2159 : 0.12080380320549011\n",
      "Training loss for batch 2160 : 0.060320716351270676\n",
      "Training loss for batch 2161 : 0.2048213928937912\n",
      "Training loss for batch 2162 : 0.1262299120426178\n",
      "Training loss for batch 2163 : 0.046649228781461716\n",
      "Training loss for batch 2164 : 0.12744557857513428\n",
      "Training loss for batch 2165 : 0.024680985137820244\n",
      "Training loss for batch 2166 : 0.041847918182611465\n",
      "Training loss for batch 2167 : 0.05864451453089714\n",
      "Training loss for batch 2168 : 0.057266466319561005\n",
      "Training loss for batch 2169 : 0.2608516812324524\n",
      "Training loss for batch 2170 : 0.05920160561800003\n",
      "Training loss for batch 2171 : 0.07820311933755875\n",
      "Training loss for batch 2172 : 0.1347663253545761\n",
      "Training loss for batch 2173 : 0.16805246472358704\n",
      "Training loss for batch 2174 : 0.10450070351362228\n",
      "Training loss for batch 2175 : 0.07690861076116562\n",
      "Training loss for batch 2176 : 0.15305405855178833\n",
      "Training loss for batch 2177 : 0.16504338383674622\n",
      "Training loss for batch 2178 : 0.05329566076397896\n",
      "Training loss for batch 2179 : 0.003931393381208181\n",
      "Training loss for batch 2180 : 0.03965308517217636\n",
      "Training loss for batch 2181 : 0.03160950914025307\n",
      "Training loss for batch 2182 : 0.03556590527296066\n",
      "Training loss for batch 2183 : 0.0939708724617958\n",
      "Training loss for batch 2184 : 0.10380798578262329\n",
      "Training loss for batch 2185 : 0.06887593120336533\n",
      "Training loss for batch 2186 : 0.09418215602636337\n",
      "Training loss for batch 2187 : 0.06486451625823975\n",
      "Training loss for batch 2188 : 0.07977009564638138\n",
      "Training loss for batch 2189 : 0.028675688430666924\n",
      "Training loss for batch 2190 : 0.08380839973688126\n",
      "Training loss for batch 2191 : 0.13185515999794006\n",
      "Training loss for batch 2192 : 0.035841211676597595\n",
      "Training loss for batch 2193 : 0.08825775980949402\n",
      "Training loss for batch 2194 : 9.329109218469966e-08\n",
      "Training loss for batch 2195 : 0.17662863433361053\n",
      "Training loss for batch 2196 : 0.05950123071670532\n",
      "Training loss for batch 2197 : 0.14909696578979492\n",
      "Training loss for batch 2198 : 0.01951547898352146\n",
      "Training loss for batch 2199 : 0.00870832335203886\n",
      "Training loss for batch 2200 : 0.04421490058302879\n",
      "Training loss for batch 2201 : 0.13382568955421448\n",
      "Training loss for batch 2202 : 0.1772024780511856\n",
      "Training loss for batch 2203 : 0.03700084611773491\n",
      "Training loss for batch 2204 : 0.03331678360700607\n",
      "Training loss for batch 2205 : 0.04254031926393509\n",
      "Training loss for batch 2206 : 0.07557962089776993\n",
      "Training loss for batch 2207 : 0.024558773264288902\n",
      "Training loss for batch 2208 : 0.10704577714204788\n",
      "Training loss for batch 2209 : 0.002646045060828328\n",
      "Training loss for batch 2210 : 0.1886531263589859\n",
      "Training loss for batch 2211 : 0.08995510637760162\n",
      "Training loss for batch 2212 : 0.07975207269191742\n",
      "Training loss for batch 2213 : 0.02099643461406231\n",
      "Training loss for batch 2214 : 0.09173397719860077\n",
      "Training loss for batch 2215 : 0.054127611219882965\n",
      "Training loss for batch 2216 : 0.015992991626262665\n",
      "Training loss for batch 2217 : 0.0833134651184082\n",
      "Training loss for batch 2218 : 0.1610674411058426\n",
      "Training loss for batch 2219 : 0.08002237230539322\n",
      "Training loss for batch 2220 : 0.03944633901119232\n",
      "Training loss for batch 2221 : 0.10446832329034805\n",
      "Training loss for batch 2222 : 0.2420947253704071\n",
      "Training loss for batch 2223 : 0.032319363206624985\n",
      "Training loss for batch 2224 : 0.029991766437888145\n",
      "Training loss for batch 2225 : 0.04477667808532715\n",
      "Training loss for batch 2226 : 0.1903602033853531\n",
      "Training loss for batch 2227 : 0.0\n",
      "Training loss for batch 2228 : 0.0509609617292881\n",
      "Training loss for batch 2229 : 0.04124600067734718\n",
      "Training loss for batch 2230 : 0.03353964164853096\n",
      "Training loss for batch 2231 : 0.1641327440738678\n",
      "Training loss for batch 2232 : 0.05907975882291794\n",
      "Training loss for batch 2233 : 0.047853995114564896\n",
      "Training loss for batch 2234 : 0.17256249487400055\n",
      "Training loss for batch 2235 : 0.042786501348018646\n",
      "Training loss for batch 2236 : 0.06273224949836731\n",
      "Training loss for batch 2237 : 0.18050070106983185\n",
      "Training loss for batch 2238 : 0.3449104130268097\n",
      "Training loss for batch 2239 : 0.2697245478630066\n",
      "Training loss for batch 2240 : 0.228885218501091\n",
      "Training loss for batch 2241 : 0.10791004449129105\n",
      "Training loss for batch 2242 : 0.019805092364549637\n",
      "Training loss for batch 2243 : 0.04771667346358299\n",
      "Training loss for batch 2244 : 0.0924236923456192\n",
      "Training loss for batch 2245 : 0.1068773940205574\n",
      "Training loss for batch 2246 : 0.045519329607486725\n",
      "Training loss for batch 2247 : 0.06781677901744843\n",
      "Training loss for batch 2248 : 0.12433256208896637\n",
      "Training loss for batch 2249 : 0.13713335990905762\n",
      "Training loss for batch 2250 : 0.17806962132453918\n",
      "Training loss for batch 2251 : 0.07552787661552429\n",
      "Training loss for batch 2252 : 0.018011020496487617\n",
      "Training loss for batch 2253 : 0.04283944144845009\n",
      "Training loss for batch 2254 : 0.11506985127925873\n",
      "Training loss for batch 2255 : 0.1502387970685959\n",
      "Training loss for batch 2256 : 0.12176267802715302\n",
      "Training loss for batch 2257 : 0.060807328671216965\n",
      "Training loss for batch 2258 : 0.10672927647829056\n",
      "Training loss for batch 2259 : 0.0514560304582119\n",
      "Training loss for batch 2260 : 0.20859700441360474\n",
      "Training loss for batch 2261 : 0.06354241073131561\n",
      "Training loss for batch 2262 : 0.042695268988609314\n",
      "Training loss for batch 2263 : 0.0346984900534153\n",
      "Training loss for batch 2264 : 0.07920687645673752\n",
      "Training loss for batch 2265 : 0.23191988468170166\n",
      "Training loss for batch 2266 : 0.07650581002235413\n",
      "Training loss for batch 2267 : 0.07183780521154404\n",
      "Training loss for batch 2268 : 0.08822321146726608\n",
      "Training loss for batch 2269 : 0.1938456892967224\n",
      "Training loss for batch 2270 : 0.06414463371038437\n",
      "Training loss for batch 2271 : 0.03304971754550934\n",
      "Training loss for batch 2272 : 0.16665513813495636\n",
      "Training loss for batch 2273 : 0.12690356373786926\n",
      "Training loss for batch 2274 : 0.2252938598394394\n",
      "Training loss for batch 2275 : 0.12070973217487335\n",
      "Training loss for batch 2276 : 0.5470593571662903\n",
      "Training loss for batch 2277 : 0.27349889278411865\n",
      "Training loss for batch 2278 : 0.10746379941701889\n",
      "Training loss for batch 2279 : 0.033159393817186356\n",
      "Training loss for batch 2280 : 0.022552676498889923\n",
      "Training loss for batch 2281 : 0.027113506570458412\n",
      "Training loss for batch 2282 : 0.04071857035160065\n",
      "Training loss for batch 2283 : 0.0324082151055336\n",
      "Training loss for batch 2284 : 0.19116650521755219\n",
      "Training loss for batch 2285 : 0.08054063469171524\n",
      "Training loss for batch 2286 : 0.04285392165184021\n",
      "Training loss for batch 2287 : 0.018926609307527542\n",
      "Training loss for batch 2288 : 0.115202896296978\n",
      "Training loss for batch 2289 : 0.09972068667411804\n",
      "Training loss for batch 2290 : 0.08930956572294235\n",
      "Training loss for batch 2291 : 0.012822362594306469\n",
      "Training loss for batch 2292 : 0.11622782051563263\n",
      "Training loss for batch 2293 : 0.10728117823600769\n",
      "Training loss for batch 2294 : 0.011604559607803822\n",
      "Training loss for batch 2295 : 0.01928795874118805\n",
      "Training loss for batch 2296 : 0.1828654259443283\n",
      "Training loss for batch 2297 : 0.04355217516422272\n",
      "Training loss for batch 2298 : 0.04941518232226372\n",
      "Training loss for batch 2299 : 0.2738574743270874\n",
      "Training loss for batch 2300 : 0.09266619384288788\n",
      "Training loss for batch 2301 : 0.061857640743255615\n",
      "Training loss for batch 2302 : 0.08563253283500671\n",
      "Training loss for batch 2303 : 0.018795672804117203\n",
      "Training loss for batch 2304 : 0.04682864621281624\n",
      "Training loss for batch 2305 : 0.11884677410125732\n",
      "Training loss for batch 2306 : 0.07131068408489227\n",
      "Training loss for batch 2307 : 0.1832512617111206\n",
      "Training loss for batch 2308 : 0.11786848306655884\n",
      "Training loss for batch 2309 : 0.15464268624782562\n",
      "Training loss for batch 2310 : 0.05512973666191101\n",
      "Training loss for batch 2311 : 0.021524062380194664\n",
      "Training loss for batch 2312 : 0.09677805751562119\n",
      "Training loss for batch 2313 : 0.049029391258955\n",
      "Training loss for batch 2314 : 0.19347141683101654\n",
      "Training loss for batch 2315 : 0.005803635809570551\n",
      "Training loss for batch 2316 : 0.03426545113325119\n",
      "Training loss for batch 2317 : 0.06676501780748367\n",
      "Training loss for batch 2318 : 0.23674720525741577\n",
      "Training loss for batch 2319 : 0.1605856865644455\n",
      "Training loss for batch 2320 : 0.05980028212070465\n",
      "Training loss for batch 2321 : 0.13864390552043915\n",
      "Training loss for batch 2322 : 0.19891317188739777\n",
      "Training loss for batch 2323 : 0.11823911219835281\n",
      "Training loss for batch 2324 : 0.0823892131447792\n",
      "Training loss for batch 2325 : 0.027253273874521255\n",
      "Training loss for batch 2326 : 0.12279897183179855\n",
      "Training loss for batch 2327 : 0.11407172679901123\n",
      "Training loss for batch 2328 : 0.17697829008102417\n",
      "Training loss for batch 2329 : 0.131947323679924\n",
      "Training loss for batch 2330 : 0.05734394118189812\n",
      "Training loss for batch 2331 : 0.12544232606887817\n",
      "Training loss for batch 2332 : 0.1258639395236969\n",
      "Training loss for batch 2333 : 0.10349642485380173\n",
      "Training loss for batch 2334 : 0.05590274930000305\n",
      "Training loss for batch 2335 : 0.07188113033771515\n",
      "Training loss for batch 2336 : 0.09428387135267258\n",
      "Training loss for batch 2337 : 0.127976655960083\n",
      "Training loss for batch 2338 : 0.0736188143491745\n",
      "Training loss for batch 2339 : 0.055588845163583755\n",
      "Training loss for batch 2340 : 0.1077885702252388\n",
      "Training loss for batch 2341 : 0.04889463260769844\n",
      "Training loss for batch 2342 : 0.029595108702778816\n",
      "Training loss for batch 2343 : 0.04711366817355156\n",
      "Training loss for batch 2344 : 0.15925680100917816\n",
      "Training loss for batch 2345 : 0.08196718990802765\n",
      "Training loss for batch 2346 : 0.009181367233395576\n",
      "Training loss for batch 2347 : 0.13454629480838776\n",
      "Training loss for batch 2348 : 0.04649946466088295\n",
      "Training loss for batch 2349 : 0.02658616565167904\n",
      "Training loss for batch 2350 : 0.05169617757201195\n",
      "Training loss for batch 2351 : 0.11679515987634659\n",
      "Training loss for batch 2352 : 0.22767451405525208\n",
      "Training loss for batch 2353 : 0.14976492524147034\n",
      "Training loss for batch 2354 : 0.11737727373838425\n",
      "Training loss for batch 2355 : 0.12228798866271973\n",
      "Training loss for batch 2356 : 0.10224892199039459\n",
      "Training loss for batch 2357 : 0.01602906547486782\n",
      "Training loss for batch 2358 : 0.036054033786058426\n",
      "Training loss for batch 2359 : 0.15478618443012238\n",
      "Training loss for batch 2360 : 0.08188553899526596\n",
      "Training loss for batch 2361 : 0.014049632474780083\n",
      "Training loss for batch 2362 : 0.12268467992544174\n",
      "Training loss for batch 2363 : 0.012410139665007591\n",
      "Training loss for batch 2364 : 0.0453590489923954\n",
      "Training loss for batch 2365 : 0.06861612945795059\n",
      "Training loss for batch 2366 : 0.10387635976076126\n",
      "Training loss for batch 2367 : 0.06089986488223076\n",
      "Training loss for batch 2368 : 0.12879841029644012\n",
      "Training loss for batch 2369 : 0.06554968655109406\n",
      "Training loss for batch 2370 : 0.0901830643415451\n",
      "Training loss for batch 2371 : 0.050002168864011765\n",
      "Training loss for batch 2372 : 0.04928477108478546\n",
      "Training loss for batch 2373 : 0.12896986305713654\n",
      "Training loss for batch 2374 : 0.0983886793255806\n",
      "Training loss for batch 2375 : 0.04003741219639778\n",
      "Training loss for batch 2376 : 0.034740474075078964\n",
      "Training loss for batch 2377 : 0.028976833447813988\n",
      "Training loss for batch 2378 : 0.1966225951910019\n",
      "Training loss for batch 2379 : 0.07282716780900955\n",
      "Training loss for batch 2380 : 0.043697088956832886\n",
      "Training loss for batch 2381 : 0.2014136165380478\n",
      "Training loss for batch 2382 : 0.055562619119882584\n",
      "Training loss for batch 2383 : 0.10411807149648666\n",
      "Training loss for batch 2384 : 0.042751625180244446\n",
      "Training loss for batch 2385 : 0.18501892685890198\n",
      "Training loss for batch 2386 : 0.11759693920612335\n",
      "Training loss for batch 2387 : 0.1819295585155487\n",
      "Training loss for batch 2388 : 0.1358334720134735\n",
      "Training loss for batch 2389 : 0.07526224851608276\n",
      "Training loss for batch 2390 : 0.22887371480464935\n",
      "Training loss for batch 2391 : 0.1881723254919052\n",
      "Training loss for batch 2392 : 0.34110379219055176\n",
      "Training loss for batch 2393 : 0.05193401873111725\n",
      "Training loss for batch 2394 : 0.03735670819878578\n",
      "Training loss for batch 2395 : 0.10254240781068802\n",
      "Training loss for batch 2396 : 0.026398515328764915\n",
      "Training loss for batch 2397 : 0.1873050183057785\n",
      "Training loss for batch 2398 : 0.09594549983739853\n",
      "Training loss for batch 2399 : 0.22744905948638916\n",
      "Training loss for batch 2400 : 0.0786338597536087\n",
      "Training loss for batch 2401 : 0.039303116500377655\n",
      "Training loss for batch 2402 : 0.05503253638744354\n",
      "Training loss for batch 2403 : 0.0307408906519413\n",
      "Training loss for batch 2404 : 0.03858364745974541\n",
      "Training loss for batch 2405 : 0.05867941305041313\n",
      "Training loss for batch 2406 : 0.10055990517139435\n",
      "Training loss for batch 2407 : 0.062181077897548676\n",
      "Training loss for batch 2408 : 0.04925059154629707\n",
      "Training loss for batch 2409 : 0.04885535687208176\n",
      "Training loss for batch 2410 : 0.1646731048822403\n",
      "Training loss for batch 2411 : 0.1269017904996872\n",
      "Training loss for batch 2412 : 0.07183605432510376\n",
      "Training loss for batch 2413 : 0.11070767790079117\n",
      "Training loss for batch 2414 : 0.09679225832223892\n",
      "Training loss for batch 2415 : 0.13355720043182373\n",
      "Training loss for batch 2416 : 0.1041954830288887\n",
      "Training loss for batch 2417 : 0.21727201342582703\n",
      "Training loss for batch 2418 : 0.1519699990749359\n",
      "Training loss for batch 2419 : 0.0992269441485405\n",
      "Training loss for batch 2420 : 0.13848812878131866\n",
      "Training loss for batch 2421 : 0.19582659006118774\n",
      "Training loss for batch 2422 : 0.1592351794242859\n",
      "Training loss for batch 2423 : 0.0615609735250473\n",
      "Training loss for batch 2424 : 0.003144709160551429\n",
      "Training loss for batch 2425 : 0.15887406468391418\n",
      "Training loss for batch 2426 : 0.12595513463020325\n",
      "Training loss for batch 2427 : 0.15498577058315277\n",
      "Training loss for batch 2428 : 0.10437531024217606\n",
      "Training loss for batch 2429 : 0.08327143639326096\n",
      "Training loss for batch 2430 : 0.023399673402309418\n",
      "Training loss for batch 2431 : 0.1017279401421547\n",
      "Training loss for batch 2432 : 0.027844419702887535\n",
      "Training loss for batch 2433 : 0.2796576917171478\n",
      "Training loss for batch 2434 : 0.14238698780536652\n",
      "Training loss for batch 2435 : 0.059292204678058624\n",
      "Training loss for batch 2436 : 0.04231947660446167\n",
      "Training loss for batch 2437 : 0.19070029258728027\n",
      "Training loss for batch 2438 : 0.31811845302581787\n",
      "Training loss for batch 2439 : 0.022851713001728058\n",
      "Training loss for batch 2440 : 0.20372191071510315\n",
      "Training loss for batch 2441 : 0.0786166787147522\n",
      "Training loss for batch 2442 : 0.035354290157556534\n",
      "Training loss for batch 2443 : 0.09864342957735062\n",
      "Training loss for batch 2444 : 0.06387537717819214\n",
      "Training loss for batch 2445 : 0.02964639477431774\n",
      "Training loss for batch 2446 : 1.313738096087036e-07\n",
      "Training loss for batch 2447 : 0.06445939093828201\n",
      "Training loss for batch 2448 : 0.17869937419891357\n",
      "Training loss for batch 2449 : 0.1286717802286148\n",
      "Training loss for batch 2450 : 0.21002733707427979\n",
      "Training loss for batch 2451 : 0.1943788230419159\n",
      "Training loss for batch 2452 : 0.0673619881272316\n",
      "Training loss for batch 2453 : 0.10559145361185074\n",
      "Training loss for batch 2454 : 0.04543682187795639\n",
      "Training loss for batch 2455 : 0.049349017441272736\n",
      "Training loss for batch 2456 : 0.14170284569263458\n",
      "Training loss for batch 2457 : 0.05867529287934303\n",
      "Training loss for batch 2458 : 0.03928808122873306\n",
      "Training loss for batch 2459 : 0.043425798416137695\n",
      "Training loss for batch 2460 : 0.1617610901594162\n",
      "Training loss for batch 2461 : 0.12119398266077042\n",
      "Training loss for batch 2462 : 0.0530095100402832\n",
      "Training loss for batch 2463 : 0.12094148248434067\n",
      "Training loss for batch 2464 : 0.25910356640815735\n",
      "Training loss for batch 2465 : 0.0832865759730339\n",
      "Training loss for batch 2466 : 0.05728156119585037\n",
      "Training loss for batch 2467 : 0.01524533424526453\n",
      "Training loss for batch 2468 : 0.03863028809428215\n",
      "Training loss for batch 2469 : 0.11221693456172943\n",
      "Training loss for batch 2470 : 0.042752813547849655\n",
      "Training loss for batch 2471 : 0.08326254040002823\n",
      "Training loss for batch 2472 : 0.09860651940107346\n",
      "Training loss for batch 2473 : 0.00844278559088707\n",
      "Training loss for batch 2474 : 0.1019960269331932\n",
      "Training loss for batch 2475 : 0.05022606998682022\n",
      "Training loss for batch 2476 : 0.1292744129896164\n",
      "Training loss for batch 2477 : 0.0998198464512825\n",
      "Training loss for batch 2478 : 0.18391141295433044\n",
      "Training loss for batch 2479 : 0.12389716506004333\n",
      "Training loss for batch 2480 : 0.011130603961646557\n",
      "Training loss for batch 2481 : 0.06380356848239899\n",
      "Training loss for batch 2482 : 0.09633579105138779\n",
      "Training loss for batch 2483 : 0.10001005232334137\n",
      "Training loss for batch 2484 : 0.07824099063873291\n",
      "Training loss for batch 2485 : 0.054114971309900284\n",
      "Training loss for batch 2486 : 0.16596922278404236\n",
      "Training loss for batch 2487 : 0.08327744901180267\n",
      "Training loss for batch 2488 : 0.07719167321920395\n",
      "Training loss for batch 2489 : 0.0009089839295484126\n",
      "Training loss for batch 2490 : 0.03664444386959076\n",
      "Training loss for batch 2491 : 0.0595824159681797\n",
      "Training loss for batch 2492 : 0.0622914619743824\n",
      "Training loss for batch 2493 : 0.0161603894084692\n",
      "Training loss for batch 2494 : 0.09750055521726608\n",
      "Training loss for batch 2495 : 0.007128863129764795\n",
      "Training loss for batch 2496 : 0.07887856662273407\n",
      "Training loss for batch 2497 : 0.06604420393705368\n",
      "Training loss for batch 2498 : 0.11575473845005035\n",
      "Training loss for batch 2499 : 0.12100305408239365\n",
      "Training loss for batch 2500 : 0.029000481590628624\n",
      "Training loss for batch 2501 : 0.05273236706852913\n",
      "Training loss for batch 2502 : 0.026964828372001648\n",
      "Training loss for batch 2503 : 0.07354959100484848\n",
      "Training loss for batch 2504 : 0.04149523004889488\n",
      "Training loss for batch 2505 : 0.015632662922143936\n",
      "Training loss for batch 2506 : 0.06761619448661804\n",
      "Training loss for batch 2507 : 0.0007533385069109499\n",
      "Training loss for batch 2508 : 0.22464457154273987\n",
      "Training loss for batch 2509 : 0.17191316187381744\n",
      "Training loss for batch 2510 : 0.06814780086278915\n",
      "Training loss for batch 2511 : 0.06713426858186722\n",
      "Training loss for batch 2512 : 0.14018934965133667\n",
      "Training loss for batch 2513 : 0.07863258570432663\n",
      "Training loss for batch 2514 : 0.0578642413020134\n",
      "Training loss for batch 2515 : 0.14805914461612701\n",
      "Training loss for batch 2516 : 0.1107560396194458\n",
      "Training loss for batch 2517 : 0.21588684618473053\n",
      "Training loss for batch 2518 : 0.043422047048807144\n",
      "Training loss for batch 2519 : 0.06237449124455452\n",
      "Training loss for batch 2520 : 0.1272253692150116\n",
      "Training loss for batch 2521 : 0.14991585910320282\n",
      "Training loss for batch 2522 : 0.04560227319598198\n",
      "Training loss for batch 2523 : 0.18324363231658936\n",
      "Training loss for batch 2524 : 0.16393859684467316\n",
      "Training loss for batch 2525 : 0.031152091920375824\n",
      "Training loss for batch 2526 : 0.07079040259122849\n",
      "Training loss for batch 2527 : 0.1489923596382141\n",
      "Training loss for batch 2528 : 0.22614699602127075\n",
      "Training loss for batch 2529 : 0.1265249103307724\n",
      "Training loss for batch 2530 : 0.050011713057756424\n",
      "Training loss for batch 2531 : 0.014099910855293274\n",
      "Training loss for batch 2532 : 0.16798783838748932\n",
      "Training loss for batch 2533 : 0.18274210393428802\n",
      "Training loss for batch 2534 : 0.15643872320652008\n",
      "Training loss for batch 2535 : 0.06398152559995651\n",
      "Training loss for batch 2536 : 0.1204865574836731\n",
      "Training loss for batch 2537 : 0.06153533607721329\n",
      "Training loss for batch 2538 : 0.03331059589982033\n",
      "Training loss for batch 2539 : 0.021826492622494698\n",
      "Training loss for batch 2540 : 0.10253310948610306\n",
      "Training loss for batch 2541 : 0.09520424157381058\n",
      "Training loss for batch 2542 : 0.024239666759967804\n",
      "Training loss for batch 2543 : 0.009044692851603031\n",
      "Training loss for batch 2544 : 0.037293318659067154\n",
      "Training loss for batch 2545 : 0.08904659748077393\n",
      "Training loss for batch 2546 : 0.08111289143562317\n",
      "Training loss for batch 2547 : 0.09309407323598862\n",
      "Training loss for batch 2548 : 0.10219638049602509\n",
      "Training loss for batch 2549 : 0.06432143598794937\n",
      "Training loss for batch 2550 : 0.021238766610622406\n",
      "Training loss for batch 2551 : 0.024861158803105354\n",
      "Training loss for batch 2552 : 0.03298890218138695\n",
      "Training loss for batch 2553 : 0.05479653179645538\n",
      "Training loss for batch 2554 : 0.11924974620342255\n",
      "Training loss for batch 2555 : 0.09948321431875229\n",
      "Training loss for batch 2556 : 0.11478220671415329\n",
      "Training loss for batch 2557 : 0.09390651434659958\n",
      "Training loss for batch 2558 : 0.06641779094934464\n",
      "Training loss for batch 2559 : 0.0659315288066864\n",
      "Training loss for batch 2560 : 0.13986852765083313\n",
      "Training loss for batch 2561 : 0.10540976375341415\n",
      "Training loss for batch 2562 : 0.10924433171749115\n",
      "Training loss for batch 2563 : 0.08230173587799072\n",
      "Training loss for batch 2564 : 0.14501726627349854\n",
      "Training loss for batch 2565 : 0.0720796138048172\n",
      "Training loss for batch 2566 : 0.1898374706506729\n",
      "Training loss for batch 2567 : 0.09401346743106842\n",
      "Training loss for batch 2568 : 0.0791744589805603\n",
      "Training loss for batch 2569 : 0.15226498246192932\n",
      "Training loss for batch 2570 : 0.07984507083892822\n",
      "Training loss for batch 2571 : 0.014748608693480492\n",
      "Training loss for batch 2572 : 0.1826878935098648\n",
      "Training loss for batch 2573 : 0.057850342243909836\n",
      "Training loss for batch 2574 : 0.0\n",
      "Training loss for batch 2575 : 0.20104290544986725\n",
      "Training loss for batch 2576 : 0.04426693916320801\n",
      "Training loss for batch 2577 : 0.042366646230220795\n",
      "Training loss for batch 2578 : 0.0736953392624855\n",
      "Training loss for batch 2579 : 0.059253938496112823\n",
      "Training loss for batch 2580 : 0.025291666388511658\n",
      "Training loss for batch 2581 : 0.060640428215265274\n",
      "Training loss for batch 2582 : 0.1535005122423172\n",
      "Training loss for batch 2583 : 0.10746750235557556\n",
      "Training loss for batch 2584 : 0.13464847207069397\n",
      "Training loss for batch 2585 : 0.022138435393571854\n",
      "Training loss for batch 2586 : 0.012133144773542881\n",
      "Training loss for batch 2587 : 0.01801382191479206\n",
      "Training loss for batch 2588 : 0.11648928374052048\n",
      "Training loss for batch 2589 : 0.008023180067539215\n",
      "Training loss for batch 2590 : 0.10655652731657028\n",
      "Training loss for batch 2591 : 0.07161781191825867\n",
      "Training loss for batch 2592 : 0.11573793739080429\n",
      "Training loss for batch 2593 : 0.07549410313367844\n",
      "Training loss for batch 2594 : 0.020620325580239296\n",
      "Training loss for batch 2595 : 0.10589294880628586\n",
      "Training loss for batch 2596 : 0.07603772729635239\n",
      "Training loss for batch 2597 : 0.03467662259936333\n",
      "Training loss for batch 2598 : 0.02086465433239937\n",
      "Training loss for batch 2599 : 0.058297693729400635\n",
      "Training loss for batch 2600 : 0.11438320577144623\n",
      "Training loss for batch 2601 : 0.017339562997221947\n",
      "Training loss for batch 2602 : 0.0963367149233818\n",
      "Training loss for batch 2603 : 0.10828829556703568\n",
      "Training loss for batch 2604 : 0.07853040844202042\n",
      "Training loss for batch 2605 : 0.06729449331760406\n",
      "Training loss for batch 2606 : 0.13615159690380096\n",
      "Training loss for batch 2607 : 0.015409714542329311\n",
      "Training loss for batch 2608 : 0.07280487567186356\n",
      "Training loss for batch 2609 : 0.10521888732910156\n",
      "Training loss for batch 2610 : 0.13721701502799988\n",
      "Training loss for batch 2611 : 0.06426896899938583\n",
      "Training loss for batch 2612 : 0.030125698074698448\n",
      "Training loss for batch 2613 : 0.050267040729522705\n",
      "Training loss for batch 2614 : 0.0643734559416771\n",
      "Training loss for batch 2615 : 9.060023131723938e-08\n",
      "Training loss for batch 2616 : 0.07651254534721375\n",
      "Training loss for batch 2617 : 0.1816219985485077\n",
      "Training loss for batch 2618 : 0.1612975150346756\n",
      "Training loss for batch 2619 : 0.05591881275177002\n",
      "Training loss for batch 2620 : 0.0035074311308562756\n",
      "Training loss for batch 2621 : 0.13650397956371307\n",
      "Training loss for batch 2622 : 0.07686331868171692\n",
      "Training loss for batch 2623 : 0.12260901927947998\n",
      "Training loss for batch 2624 : 0.08322691917419434\n",
      "Training loss for batch 2625 : 0.03616181015968323\n",
      "Training loss for batch 2626 : 0.19254200160503387\n",
      "Training loss for batch 2627 : 0.06995480507612228\n",
      "Training loss for batch 2628 : 0.017398593947291374\n",
      "Training loss for batch 2629 : 0.0731273740530014\n",
      "Training loss for batch 2630 : 0.0633789449930191\n",
      "Training loss for batch 2631 : 0.11264177411794662\n",
      "Training loss for batch 2632 : 0.00690827053040266\n",
      "Training loss for batch 2633 : 0.05564656853675842\n",
      "Training loss for batch 2634 : 0.013566394336521626\n",
      "Training loss for batch 2635 : 0.03210500627756119\n",
      "Training loss for batch 2636 : 0.06540524959564209\n",
      "Training loss for batch 2637 : 0.07186710834503174\n",
      "Training loss for batch 2638 : 0.06451416015625\n",
      "Training loss for batch 2639 : 0.24897746741771698\n",
      "Training loss for batch 2640 : 0.00377662037499249\n",
      "Training loss for batch 2641 : 0.07663936913013458\n",
      "Training loss for batch 2642 : 0.15643788874149323\n",
      "Training loss for batch 2643 : 0.10611632466316223\n",
      "Training loss for batch 2644 : 0.05287085101008415\n",
      "Training loss for batch 2645 : 0.07745519280433655\n",
      "Training loss for batch 2646 : 0.025525201112031937\n",
      "Training loss for batch 2647 : 0.10460980981588364\n",
      "Training loss for batch 2648 : 0.056268688291311264\n",
      "Training loss for batch 2649 : 0.13894011080265045\n",
      "Training loss for batch 2650 : 0.06108596920967102\n",
      "Training loss for batch 2651 : 0.10555510222911835\n",
      "Training loss for batch 2652 : 0.04972757399082184\n",
      "Training loss for batch 2653 : 0.08957207947969437\n",
      "Training loss for batch 2654 : 0.2714480459690094\n",
      "Training loss for batch 2655 : 0.12542934715747833\n",
      "Training loss for batch 2656 : 0.009808877483010292\n",
      "Training loss for batch 2657 : 0.05214338377118111\n",
      "Training loss for batch 2658 : 0.11070320010185242\n",
      "Training loss for batch 2659 : 0.13189518451690674\n",
      "Training loss for batch 2660 : 0.04966163635253906\n",
      "Training loss for batch 2661 : 0.13463237881660461\n",
      "Training loss for batch 2662 : 0.11324171721935272\n",
      "Training loss for batch 2663 : 0.13575109839439392\n",
      "Training loss for batch 2664 : 0.07524609565734863\n",
      "Training loss for batch 2665 : 0.08466958999633789\n",
      "Training loss for batch 2666 : 0.06359537690877914\n",
      "Training loss for batch 2667 : 0.1419154852628708\n",
      "Training loss for batch 2668 : 0.24718059599399567\n",
      "Training loss for batch 2669 : 0.05014852061867714\n",
      "Training loss for batch 2670 : 0.05432117357850075\n",
      "Training loss for batch 2671 : 0.14069899916648865\n",
      "Training loss for batch 2672 : 0.277321994304657\n",
      "Training loss for batch 2673 : 0.10744234919548035\n",
      "Training loss for batch 2674 : 0.0644097700715065\n",
      "Training loss for batch 2675 : 0.09228290617465973\n",
      "Training loss for batch 2676 : 0.10595564544200897\n",
      "Training loss for batch 2677 : 0.10084730386734009\n",
      "Training loss for batch 2678 : 0.014692677184939384\n",
      "Training loss for batch 2679 : 0.10614486783742905\n",
      "Training loss for batch 2680 : 0.0227603018283844\n",
      "Training loss for batch 2681 : 0.044398900121450424\n",
      "Training loss for batch 2682 : 0.22881664335727692\n",
      "Training loss for batch 2683 : 0.029988598078489304\n",
      "Training loss for batch 2684 : 0.010181905701756477\n",
      "Training loss for batch 2685 : 0.09113513678312302\n",
      "Training loss for batch 2686 : 0.1330995112657547\n",
      "Training loss for batch 2687 : 0.17047274112701416\n",
      "Training loss for batch 2688 : 0.1545952707529068\n",
      "Training loss for batch 2689 : 0.08012882620096207\n",
      "Training loss for batch 2690 : 0.009689475409686565\n",
      "Training loss for batch 2691 : 0.11009233444929123\n",
      "Training loss for batch 2692 : 0.03161313757300377\n",
      "Training loss for batch 2693 : 0.10146183520555496\n",
      "Training loss for batch 2694 : 0.1391187608242035\n",
      "Training loss for batch 2695 : 0.047737184911966324\n",
      "Training loss for batch 2696 : 0.08762243390083313\n",
      "Training loss for batch 2697 : 0.1121484711766243\n",
      "Training loss for batch 2698 : 0.21776118874549866\n",
      "Training loss for batch 2699 : 0.015258913859724998\n",
      "Training loss for batch 2700 : 0.03517472743988037\n",
      "Training loss for batch 2701 : 0.17507727444171906\n",
      "Training loss for batch 2702 : 0.16547763347625732\n",
      "Training loss for batch 2703 : 0.16304808855056763\n",
      "Training loss for batch 2704 : 0.05696246773004532\n",
      "Training loss for batch 2705 : 0.09159185737371445\n",
      "Training loss for batch 2706 : 0.09006418287754059\n",
      "Training loss for batch 2707 : 0.13018718361854553\n",
      "Training loss for batch 2708 : 0.11820759624242783\n",
      "Training loss for batch 2709 : 0.11809565126895905\n",
      "Training loss for batch 2710 : 0.002313370583578944\n",
      "Training loss for batch 2711 : 0.04616554453969002\n",
      "Training loss for batch 2712 : 0.14690445363521576\n",
      "Training loss for batch 2713 : 0.06508193910121918\n",
      "Training loss for batch 2714 : 0.17444804310798645\n",
      "Training loss for batch 2715 : 0.25206661224365234\n",
      "Training loss for batch 2716 : 0.04509687423706055\n",
      "Training loss for batch 2717 : 0.07077261060476303\n",
      "Training loss for batch 2718 : 0.049884404987096786\n",
      "Training loss for batch 2719 : 0.06740246713161469\n",
      "Training loss for batch 2720 : 0.06341218948364258\n",
      "Training loss for batch 2721 : 0.07177900522947311\n",
      "Training loss for batch 2722 : 0.1086384728550911\n",
      "Training loss for batch 2723 : 0.10647479444742203\n",
      "Training loss for batch 2724 : 0.16909314692020416\n",
      "Training loss for batch 2725 : 0.07166560739278793\n",
      "Training loss for batch 2726 : 0.060601238161325455\n",
      "Training loss for batch 2727 : 0.0002908818132709712\n",
      "Training loss for batch 2728 : 0.1119222491979599\n",
      "Training loss for batch 2729 : 0.10244639217853546\n",
      "Training loss for batch 2730 : 0.09240133315324783\n",
      "Training loss for batch 2731 : 0.12333562225103378\n",
      "Training loss for batch 2732 : 0.21390677988529205\n",
      "Training loss for batch 2733 : 0.06805945187807083\n",
      "Training loss for batch 2734 : 6.363141835663555e-08\n",
      "Training loss for batch 2735 : 0.13243578374385834\n",
      "Training loss for batch 2736 : 0.04650275409221649\n",
      "Training loss for batch 2737 : 0.14992181956768036\n",
      "Training loss for batch 2738 : 0.08496229350566864\n",
      "Training loss for batch 2739 : 0.13801699876785278\n",
      "Training loss for batch 2740 : 0.07139267772436142\n",
      "Training loss for batch 2741 : 0.18583418428897858\n",
      "Training loss for batch 2742 : 0.03628005459904671\n",
      "Training loss for batch 2743 : 0.0924554094672203\n",
      "Training loss for batch 2744 : 0.05027847737073898\n",
      "Training loss for batch 2745 : 0.0029311380349099636\n",
      "Training loss for batch 2746 : 0.05819237232208252\n",
      "Training loss for batch 2747 : 0.05560503154993057\n",
      "Training loss for batch 2748 : 0.14094841480255127\n",
      "Training loss for batch 2749 : 0.20710746943950653\n",
      "Training loss for batch 2750 : 0.07761038094758987\n",
      "Training loss for batch 2751 : 0.009077517315745354\n",
      "Training loss for batch 2752 : 0.0032223353628069162\n",
      "Training loss for batch 2753 : 0.18064263463020325\n",
      "Training loss for batch 2754 : 0.012131077237427235\n",
      "Training loss for batch 2755 : 0.022961750626564026\n",
      "Training loss for batch 2756 : 0.13482071459293365\n",
      "Training loss for batch 2757 : 0.02831585891544819\n",
      "Training loss for batch 2758 : 0.017773691564798355\n",
      "Training loss for batch 2759 : 0.09278000891208649\n",
      "Training loss for batch 2760 : 0.07074947655200958\n",
      "Training loss for batch 2761 : 0.2369639128446579\n",
      "Training loss for batch 2762 : 0.10861145704984665\n",
      "Training loss for batch 2763 : 0.11828378587961197\n",
      "Training loss for batch 2764 : 0.2779556214809418\n",
      "Training loss for batch 2765 : 0.0159014705568552\n",
      "Training loss for batch 2766 : 0.15051496028900146\n",
      "Training loss for batch 2767 : 0.03894083946943283\n",
      "Training loss for batch 2768 : 0.08612800389528275\n",
      "Training loss for batch 2769 : 0.17240695655345917\n",
      "Training loss for batch 2770 : 0.1024097427725792\n",
      "Training loss for batch 2771 : 0.0\n",
      "Training loss for batch 2772 : 0.10154951363801956\n",
      "Training loss for batch 2773 : 0.08308086544275284\n",
      "Training loss for batch 2774 : 0.0370989590883255\n",
      "Training loss for batch 2775 : 0.06438706070184708\n",
      "Training loss for batch 2776 : 0.05794967710971832\n",
      "Training loss for batch 2777 : 0.039859071373939514\n",
      "Training loss for batch 2778 : 0.240889772772789\n",
      "Training loss for batch 2779 : 0.0769328773021698\n",
      "Training loss for batch 2780 : 0.00410885363817215\n",
      "Training loss for batch 2781 : 0.04240534454584122\n",
      "Training loss for batch 2782 : 0.021013299003243446\n",
      "Training loss for batch 2783 : 0.03547076880931854\n",
      "Training loss for batch 2784 : 0.04206005111336708\n",
      "Training loss for batch 2785 : 0.056074149906635284\n",
      "Training loss for batch 2786 : 0.16953711211681366\n",
      "Training loss for batch 2787 : 0.015194260515272617\n",
      "Training loss for batch 2788 : 0.07248881459236145\n",
      "Training loss for batch 2789 : 0.09878407418727875\n",
      "Training loss for batch 2790 : 0.07419959455728531\n",
      "Training loss for batch 2791 : 0.043643705546855927\n",
      "Training loss for batch 2792 : 0.14535830914974213\n",
      "Training loss for batch 2793 : 0.20889517664909363\n",
      "Training loss for batch 2794 : 0.11422145366668701\n",
      "Training loss for batch 2795 : 0.025491079315543175\n",
      "Training loss for batch 2796 : 0.06485538929700851\n",
      "Training loss for batch 2797 : 0.17554064095020294\n",
      "Training loss for batch 2798 : 0.055740319192409515\n",
      "Training loss for batch 2799 : 0.056810565292835236\n",
      "Training loss for batch 2800 : 0.04892111197113991\n",
      "Training loss for batch 2801 : 0.031939029693603516\n",
      "Training loss for batch 2802 : 0.03749695047736168\n",
      "Training loss for batch 2803 : 0.1218898668885231\n",
      "Training loss for batch 2804 : 0.2058388739824295\n",
      "Training loss for batch 2805 : 0.029552198946475983\n",
      "Training loss for batch 2806 : 0.11807357519865036\n",
      "Training loss for batch 2807 : 0.09152546525001526\n",
      "Training loss for batch 2808 : 0.030680319294333458\n",
      "Training loss for batch 2809 : 0.14583861827850342\n",
      "Training loss for batch 2810 : 0.1408143937587738\n",
      "Training loss for batch 2811 : 0.13474763929843903\n",
      "Training loss for batch 2812 : 0.09341452270746231\n",
      "Training loss for batch 2813 : 0.02689208649098873\n",
      "Training loss for batch 2814 : 0.09789883345365524\n",
      "Training loss for batch 2815 : 0.06884267926216125\n",
      "Training loss for batch 2816 : 0.07768881320953369\n",
      "Training loss for batch 2817 : 0.0979219377040863\n",
      "Training loss for batch 2818 : 0.05213882401585579\n",
      "Training loss for batch 2819 : 0.0814172700047493\n",
      "Training loss for batch 2820 : 0.01578744873404503\n",
      "Training loss for batch 2821 : 0.20268960297107697\n",
      "Training loss for batch 2822 : 0.2466191202402115\n",
      "Training loss for batch 2823 : 0.12856116890907288\n",
      "Training loss for batch 2824 : 0.06105474755167961\n",
      "Training loss for batch 2825 : 0.07363613694906235\n",
      "Training loss for batch 2826 : 0.14925920963287354\n",
      "Training loss for batch 2827 : 0.08387818187475204\n",
      "Training loss for batch 2828 : 0.04809706658124924\n",
      "Training loss for batch 2829 : 0.048347096890211105\n",
      "Training loss for batch 2830 : 0.09871755540370941\n",
      "Training loss for batch 2831 : 0.1859840452671051\n",
      "Training loss for batch 2832 : 0.0\n",
      "Training loss for batch 2833 : 0.08246371150016785\n",
      "Training loss for batch 2834 : 0.13918398320674896\n",
      "Training loss for batch 2835 : 0.09317900240421295\n",
      "Training loss for batch 2836 : 0.11831549555063248\n",
      "Training loss for batch 2837 : 0.17999178171157837\n",
      "Training loss for batch 2838 : 0.09482499212026596\n",
      "Training loss for batch 2839 : 0.12481078505516052\n",
      "Training loss for batch 2840 : 0.017367776483297348\n",
      "Training loss for batch 2841 : 0.058567438274621964\n",
      "Training loss for batch 2842 : 0.1419888436794281\n",
      "Training loss for batch 2843 : 0.16658206284046173\n",
      "Training loss for batch 2844 : 0.15858088433742523\n",
      "Training loss for batch 2845 : 0.0386662520468235\n",
      "Training loss for batch 2846 : 0.20758122205734253\n",
      "Training loss for batch 2847 : 0.04549318552017212\n",
      "Training loss for batch 2848 : 0.12398579716682434\n",
      "Training loss for batch 2849 : 0.15273846685886383\n",
      "Training loss for batch 2850 : 0.007266909349709749\n",
      "Training loss for batch 2851 : 0.06388639658689499\n",
      "Training loss for batch 2852 : 0.1554235816001892\n",
      "Training loss for batch 2853 : 0.16982106864452362\n",
      "Training loss for batch 2854 : 0.018067704513669014\n",
      "Training loss for batch 2855 : 0.043096646666526794\n",
      "Training loss for batch 2856 : 0.031912777572870255\n",
      "Training loss for batch 2857 : 0.061417173594236374\n",
      "Training loss for batch 2858 : 0.0942816361784935\n",
      "Training loss for batch 2859 : 0.09762491285800934\n",
      "Training loss for batch 2860 : 0.2018185257911682\n",
      "Training loss for batch 2861 : 0.09978530555963516\n",
      "Training loss for batch 2862 : 0.041153520345687866\n",
      "Training loss for batch 2863 : 0.09070628881454468\n",
      "Training loss for batch 2864 : 0.15761788189411163\n",
      "Training loss for batch 2865 : 0.15695902705192566\n",
      "Training loss for batch 2866 : 0.05052897334098816\n",
      "Training loss for batch 2867 : 0.0814695954322815\n",
      "Training loss for batch 2868 : 0.16503076255321503\n",
      "Training loss for batch 2869 : 0.09226589649915695\n",
      "Training loss for batch 2870 : 0.066793292760849\n",
      "Training loss for batch 2871 : 0.1318557858467102\n",
      "Training loss for batch 2872 : 0.15692876279354095\n",
      "Training loss for batch 2873 : 0.13530835509300232\n",
      "Training loss for batch 2874 : 0.17329686880111694\n",
      "Training loss for batch 2875 : 0.040266409516334534\n",
      "Training loss for batch 2876 : 0.045833297073841095\n",
      "Training loss for batch 2877 : 0.06318981945514679\n",
      "Training loss for batch 2878 : 0.07633771747350693\n",
      "Training loss for batch 2879 : 0.10700880736112595\n",
      "Training loss for batch 2880 : 0.019308988004922867\n",
      "Training loss for batch 2881 : 0.10529172420501709\n",
      "Training loss for batch 2882 : 0.10982271283864975\n",
      "Training loss for batch 2883 : 0.13117004930973053\n",
      "Training loss for batch 2884 : 0.023390933871269226\n",
      "Training loss for batch 2885 : 0.11493014544248581\n",
      "Training loss for batch 2886 : 0.08566956222057343\n",
      "Training loss for batch 2887 : 0.007174771279096603\n",
      "Training loss for batch 2888 : 0.11261867731809616\n",
      "Training loss for batch 2889 : 0.0030472143553197384\n",
      "Training loss for batch 2890 : 0.11733505129814148\n",
      "Training loss for batch 2891 : 0.022964460775256157\n",
      "Training loss for batch 2892 : 0.16290004551410675\n",
      "Training loss for batch 2893 : 0.04492711275815964\n",
      "Training loss for batch 2894 : 0.04132339730858803\n",
      "Training loss for batch 2895 : 0.09033701568841934\n",
      "Training loss for batch 2896 : 0.1842893809080124\n",
      "Training loss for batch 2897 : 0.09969007223844528\n",
      "Training loss for batch 2898 : 0.054506152868270874\n",
      "Training loss for batch 2899 : 0.024247515946626663\n",
      "Training loss for batch 2900 : 0.21113190054893494\n",
      "Training loss for batch 2901 : 0.02164144441485405\n",
      "Training loss for batch 2902 : 0.12216533720493317\n",
      "Training loss for batch 2903 : 0.032504837960004807\n",
      "Training loss for batch 2904 : 0.1557309627532959\n",
      "Training loss for batch 2905 : 0.025115396827459335\n",
      "Training loss for batch 2906 : 0.043114639818668365\n",
      "Training loss for batch 2907 : 0.0\n",
      "Training loss for batch 2908 : 0.028551388531923294\n",
      "Training loss for batch 2909 : 0.05932449549436569\n",
      "Training loss for batch 2910 : 0.18369780480861664\n",
      "Training loss for batch 2911 : 0.2069442719221115\n",
      "Training loss for batch 2912 : 0.09004813432693481\n",
      "Training loss for batch 2913 : 0.06399063020944595\n",
      "Training loss for batch 2914 : 0.0\n",
      "Training loss for batch 2915 : 0.04088577255606651\n",
      "Training loss for batch 2916 : 0.03948771581053734\n",
      "Training loss for batch 2917 : 0.244704931974411\n",
      "Training loss for batch 2918 : 0.2801271677017212\n",
      "Training loss for batch 2919 : 0.06902216374874115\n",
      "Training loss for batch 2920 : 0.22779236733913422\n",
      "Training loss for batch 2921 : 0.062090691179037094\n",
      "Training loss for batch 2922 : 0.03551364690065384\n",
      "Training loss for batch 2923 : 0.06198708340525627\n",
      "Training loss for batch 2924 : 0.02475823275744915\n",
      "Training loss for batch 2925 : 0.15494295954704285\n",
      "Training loss for batch 2926 : 0.0617002435028553\n",
      "Training loss for batch 2927 : 0.07345611602067947\n",
      "Training loss for batch 2928 : 0.051769956946372986\n",
      "Training loss for batch 2929 : 0.09197579324245453\n",
      "Training loss for batch 2930 : 0.1120099425315857\n",
      "Training loss for batch 2931 : 0.1776740849018097\n",
      "Training loss for batch 2932 : 0.08340321481227875\n",
      "Training loss for batch 2933 : 0.13541345298290253\n",
      "Training loss for batch 2934 : 0.08412875235080719\n",
      "Training loss for batch 2935 : 0.13100114464759827\n",
      "Training loss for batch 2936 : 0.06082981079816818\n",
      "Training loss for batch 2937 : 0.08370853215456009\n",
      "Training loss for batch 2938 : 0.15652625262737274\n",
      "Training loss for batch 2939 : 0.08168994635343552\n",
      "Training loss for batch 2940 : 0.1506938338279724\n",
      "Training loss for batch 2941 : 0.1185307726264\n",
      "Training loss for batch 2942 : 0.14998820424079895\n",
      "Training loss for batch 2943 : 0.1849704533815384\n",
      "Training loss for batch 2944 : 0.13891378045082092\n",
      "Training loss for batch 2945 : 0.0610882043838501\n",
      "Training loss for batch 2946 : 0.07573080062866211\n",
      "Training loss for batch 2947 : 0.22572478652000427\n",
      "Training loss for batch 2948 : 0.11707933992147446\n",
      "Training loss for batch 2949 : 0.09270141273736954\n",
      "Training loss for batch 2950 : 0.16918298602104187\n",
      "Training loss for batch 2951 : 0.0908396765589714\n",
      "Training loss for batch 2952 : 0.12956734001636505\n",
      "Training loss for batch 2953 : 0.042938556522130966\n",
      "Training loss for batch 2954 : 0.004799331538379192\n",
      "Training loss for batch 2955 : 0.09271597862243652\n",
      "Training loss for batch 2956 : 0.062498051673173904\n",
      "Training loss for batch 2957 : 0.07833658158779144\n",
      "Training loss for batch 2958 : 0.0706387385725975\n",
      "Training loss for batch 2959 : 0.020398026332259178\n",
      "Training loss for batch 2960 : 0.0374794565141201\n",
      "Training loss for batch 2961 : 0.07601097226142883\n",
      "Training loss for batch 2962 : 0.11402013897895813\n",
      "Training loss for batch 2963 : 1.5730410041214782e-07\n",
      "Training loss for batch 2964 : 0.08273179829120636\n",
      "Training loss for batch 2965 : 0.14494171738624573\n",
      "Training loss for batch 2966 : 0.07244697958230972\n",
      "Training loss for batch 2967 : 0.052936047315597534\n",
      "Training loss for batch 2968 : 0.04128071665763855\n",
      "Training loss for batch 2969 : 0.05634675547480583\n",
      "Training loss for batch 2970 : 0.1057075709104538\n",
      "Training loss for batch 2971 : 0.04713456705212593\n",
      "Training loss for batch 2972 : 0.2779657244682312\n",
      "Training loss for batch 2973 : 0.03808094188570976\n",
      "Training loss for batch 2974 : 0.07994276285171509\n",
      "Training loss for batch 2975 : 0.05423498526215553\n",
      "Training loss for batch 2976 : 0.1398831605911255\n",
      "Training loss for batch 2977 : 0.0037432238459587097\n",
      "Training loss for batch 2978 : 0.07019919157028198\n",
      "Training loss for batch 2979 : 0.10252393037080765\n",
      "Training loss for batch 2980 : 0.1433173567056656\n",
      "Training loss for batch 2981 : 0.11701107770204544\n",
      "Training loss for batch 2982 : 0.09120360761880875\n",
      "Training loss for batch 2983 : 0.031101806089282036\n",
      "Training loss for batch 2984 : 0.14572325348854065\n",
      "Training loss for batch 2985 : 0.03941365331411362\n",
      "Training loss for batch 2986 : 0.20159122347831726\n",
      "Training loss for batch 2987 : 0.010668352246284485\n",
      "Training loss for batch 2988 : 0.03862203285098076\n",
      "Training loss for batch 2989 : 0.024736829102039337\n",
      "Training loss for batch 2990 : 0.04055948927998543\n",
      "Training loss for batch 2991 : 0.025823991745710373\n",
      "Training loss for batch 2992 : 0.21923165023326874\n",
      "Training loss for batch 2993 : 0.024154184386134148\n",
      "Training loss for batch 2994 : 0.026598691940307617\n",
      "Training loss for batch 2995 : 0.07248987257480621\n",
      "Training loss for batch 2996 : 0.036704301834106445\n",
      "Training loss for batch 2997 : 0.040234845131635666\n",
      "Training loss for batch 2998 : 0.05966459587216377\n",
      "Training loss for batch 2999 : 0.06302191317081451\n",
      "Training loss for batch 3000 : 0.14098230004310608\n",
      "Training loss for batch 3001 : 0.0056893485598266125\n",
      "Training loss for batch 3002 : 0.0569734126329422\n",
      "Training loss for batch 3003 : 0.11913899332284927\n",
      "Training loss for batch 3004 : 0.07242707908153534\n",
      "Training loss for batch 3005 : 0.15053223073482513\n",
      "Training loss for batch 3006 : 0.05181555822491646\n",
      "Training loss for batch 3007 : 0.044344738125801086\n",
      "Training loss for batch 3008 : 0.19967898726463318\n",
      "Training loss for batch 3009 : 0.05976035073399544\n",
      "Training loss for batch 3010 : 0.24876989424228668\n",
      "Training loss for batch 3011 : 0.09184924513101578\n",
      "Training loss for batch 3012 : 0.01785573735833168\n",
      "Training loss for batch 3013 : 0.1680666208267212\n",
      "Training loss for batch 3014 : 0.018428202718496323\n",
      "Training loss for batch 3015 : 0.05655936151742935\n",
      "Training loss for batch 3016 : 0.03967750072479248\n",
      "Training loss for batch 3017 : 0.14402420818805695\n",
      "Training loss for batch 3018 : 0.017073553055524826\n",
      "Training loss for batch 3019 : 0.1045190766453743\n",
      "Training loss for batch 3020 : 0.07799350470304489\n",
      "Training loss for batch 3021 : 0.04500598460435867\n",
      "Training loss for batch 3022 : 0.0\n",
      "Training loss for batch 3023 : 0.01661921851336956\n",
      "Training loss for batch 3024 : 0.16734051704406738\n",
      "Training loss for batch 3025 : 0.0815199762582779\n",
      "Training loss for batch 3026 : 0.1797819286584854\n",
      "Training loss for batch 3027 : 0.13229432702064514\n",
      "Training loss for batch 3028 : 0.24023032188415527\n",
      "Training loss for batch 3029 : 0.11615363508462906\n",
      "Training loss for batch 3030 : 0.02777577005326748\n",
      "Training loss for batch 3031 : 0.30037692189216614\n",
      "Training loss for batch 3032 : 0.1865588277578354\n",
      "Training loss for batch 3033 : 0.012236414477229118\n",
      "Training loss for batch 3034 : 0.07488629966974258\n",
      "Training loss for batch 3035 : 0.1791173219680786\n",
      "Training loss for batch 3036 : 0.09380512684583664\n",
      "Training loss for batch 3037 : 0.01930135115981102\n",
      "Training loss for batch 3038 : 0.1527162343263626\n",
      "Training loss for batch 3039 : 0.10519617050886154\n",
      "Training loss for batch 3040 : 0.033775318413972855\n",
      "Training loss for batch 3041 : 0.04630828648805618\n",
      "Training loss for batch 3042 : 0.11261433362960815\n",
      "Training loss for batch 3043 : 0.1866539865732193\n",
      "Training loss for batch 3044 : 0.10253579914569855\n",
      "Training loss for batch 3045 : 0.14613975584506989\n",
      "Training loss for batch 3046 : 0.02537650056183338\n",
      "Training loss for batch 3047 : 0.02031421847641468\n",
      "Training loss for batch 3048 : 0.1053004339337349\n",
      "Training loss for batch 3049 : 0.05284302309155464\n",
      "Training loss for batch 3050 : 0.0565948486328125\n",
      "Training loss for batch 3051 : 0.05777367204427719\n",
      "Training loss for batch 3052 : 0.1550367772579193\n",
      "Training loss for batch 3053 : 0.09586527198553085\n",
      "Training loss for batch 3054 : 0.19213050603866577\n",
      "Training loss for batch 3055 : 0.09322493523359299\n",
      "Training loss for batch 3056 : 0.03379782289266586\n",
      "Training loss for batch 3057 : 0.04520073160529137\n",
      "Training loss for batch 3058 : 0.10564189404249191\n",
      "Training loss for batch 3059 : 0.24639639258384705\n",
      "Training loss for batch 3060 : 0.15788115561008453\n",
      "Training loss for batch 3061 : 0.08303913474082947\n",
      "Training loss for batch 3062 : 0.061797693371772766\n",
      "Training loss for batch 3063 : 0.19865916669368744\n",
      "Training loss for batch 3064 : 0.25409501791000366\n",
      "Training loss for batch 3065 : 0.08117854595184326\n",
      "Training loss for batch 3066 : 0.18469417095184326\n",
      "Training loss for batch 3067 : 0.14718075096607208\n",
      "Training loss for batch 3068 : 0.09139908850193024\n",
      "Training loss for batch 3069 : 0.11049919575452805\n",
      "Training loss for batch 3070 : 0.09361294656991959\n",
      "Training loss for batch 3071 : 0.0622122548520565\n",
      "Training loss for batch 3072 : 0.13888563215732574\n",
      "Training loss for batch 3073 : 0.034408099949359894\n",
      "Training loss for batch 3074 : 0.07722707092761993\n",
      "Training loss for batch 3075 : 0.07975631952285767\n",
      "Training loss for batch 3076 : 0.017711510881781578\n",
      "Training loss for batch 3077 : 0.17970439791679382\n",
      "Training loss for batch 3078 : 0.09995294362306595\n",
      "Training loss for batch 3079 : 0.0987786129117012\n",
      "Training loss for batch 3080 : 0.05117198824882507\n",
      "Training loss for batch 3081 : 0.07452306151390076\n",
      "Training loss for batch 3082 : 0.0481436550617218\n",
      "Training loss for batch 3083 : 0.12781764566898346\n",
      "Training loss for batch 3084 : 0.062206026166677475\n",
      "Training loss for batch 3085 : 0.2503093481063843\n",
      "Training loss for batch 3086 : 0.07624226063489914\n",
      "Training loss for batch 3087 : 0.1166125237941742\n",
      "Training loss for batch 3088 : 0.09357959032058716\n",
      "Training loss for batch 3089 : 0.0790141299366951\n",
      "Training loss for batch 3090 : 0.05003059655427933\n",
      "Training loss for batch 3091 : 0.07125183939933777\n",
      "Training loss for batch 3092 : 0.2056080400943756\n",
      "Training loss for batch 3093 : 0.08467844128608704\n",
      "Training loss for batch 3094 : 0.127790167927742\n",
      "Training loss for batch 3095 : 0.07174970954656601\n",
      "Training loss for batch 3096 : 0.021321294829249382\n",
      "Training loss for batch 3097 : 0.03323495388031006\n",
      "Training loss for batch 3098 : 0.04762905463576317\n",
      "Training loss for batch 3099 : 0.09192235767841339\n",
      "Training loss for batch 3100 : 0.07726726680994034\n",
      "Training loss for batch 3101 : 0.06319741904735565\n",
      "Training loss for batch 3102 : 0.054781459271907806\n",
      "Training loss for batch 3103 : 0.03917372599244118\n",
      "Training loss for batch 3104 : 0.04553601145744324\n",
      "Training loss for batch 3105 : 0.08317932486534119\n",
      "Training loss for batch 3106 : 0.17573724687099457\n",
      "Training loss for batch 3107 : 0.05620921030640602\n",
      "Training loss for batch 3108 : 0.15191102027893066\n",
      "Training loss for batch 3109 : 0.048598192632198334\n",
      "Training loss for batch 3110 : 0.039002589881420135\n",
      "Training loss for batch 3111 : 0.13154588639736176\n",
      "Training loss for batch 3112 : 0.1381739228963852\n",
      "Training loss for batch 3113 : 0.013130556792020798\n",
      "Training loss for batch 3114 : 0.16586442291736603\n",
      "Training loss for batch 3115 : 0.01952388696372509\n",
      "Training loss for batch 3116 : 0.12412475794553757\n",
      "Training loss for batch 3117 : 0.02721327543258667\n",
      "Training loss for batch 3118 : 0.19594746828079224\n",
      "Training loss for batch 3119 : 0.030715832486748695\n",
      "Training loss for batch 3120 : 0.05276492238044739\n",
      "Training loss for batch 3121 : 0.039313193410634995\n",
      "Training loss for batch 3122 : 0.06261705607175827\n",
      "Training loss for batch 3123 : 0.0330810621380806\n",
      "Training loss for batch 3124 : 0.08770323544740677\n",
      "Training loss for batch 3125 : 0.3477797508239746\n",
      "Training loss for batch 3126 : 0.07868311554193497\n",
      "Training loss for batch 3127 : 0.1540975123643875\n",
      "Training loss for batch 3128 : 0.12480436265468597\n",
      "Training loss for batch 3129 : 0.011729157529771328\n",
      "Training loss for batch 3130 : 0.17809635400772095\n",
      "Training loss for batch 3131 : 0.12764406204223633\n",
      "Training loss for batch 3132 : 0.07568778097629547\n",
      "Training loss for batch 3133 : 0.21474872529506683\n",
      "Training loss for batch 3134 : 0.03857552260160446\n",
      "Training loss for batch 3135 : 0.20407864451408386\n",
      "Training loss for batch 3136 : 0.013531198725104332\n",
      "Training loss for batch 3137 : 0.03794163838028908\n",
      "Training loss for batch 3138 : 0.001007984159514308\n",
      "Training loss for batch 3139 : 0.09777600318193436\n",
      "Training loss for batch 3140 : 0.2208806872367859\n",
      "Training loss for batch 3141 : 0.07428714632987976\n",
      "Training loss for batch 3142 : 0.009106564335525036\n",
      "Training loss for batch 3143 : 0.08070209622383118\n",
      "Training loss for batch 3144 : 0.038315705955028534\n",
      "Training loss for batch 3145 : 0.039431944489479065\n",
      "Training loss for batch 3146 : 0.021058296784758568\n",
      "Training loss for batch 3147 : 0.19513782858848572\n",
      "Training loss for batch 3148 : 0.09722886979579926\n",
      "Training loss for batch 3149 : 0.06007317826151848\n",
      "Training loss for batch 3150 : 0.09580951184034348\n",
      "Training loss for batch 3151 : 0.0199727863073349\n",
      "Training loss for batch 3152 : 0.10081621259450912\n",
      "Training loss for batch 3153 : 0.13584740459918976\n",
      "Training loss for batch 3154 : 0.008647930808365345\n",
      "Training loss for batch 3155 : 0.1585095077753067\n",
      "Training loss for batch 3156 : 0.0786169245839119\n",
      "Training loss for batch 3157 : 0.056063827127218246\n",
      "Training loss for batch 3158 : 0.07289065420627594\n",
      "Training loss for batch 3159 : 0.05891208350658417\n",
      "Training loss for batch 3160 : 0.05343693867325783\n",
      "Training loss for batch 3161 : 0.135325625538826\n",
      "Training loss for batch 3162 : 0.06111612915992737\n",
      "Training loss for batch 3163 : 0.05657342076301575\n",
      "Training loss for batch 3164 : 0.11675450205802917\n",
      "Training loss for batch 3165 : 0.040293991565704346\n",
      "Training loss for batch 3166 : 0.16612394154071808\n",
      "Training loss for batch 3167 : 0.03379850462079048\n",
      "Training loss for batch 3168 : 0.10925684124231339\n",
      "Training loss for batch 3169 : 0.05537722259759903\n",
      "Training loss for batch 3170 : 0.06397545337677002\n",
      "Training loss for batch 3171 : 0.06618839502334595\n",
      "Training loss for batch 3172 : 0.0023241175804287195\n",
      "Training loss for batch 3173 : 0.10405216366052628\n",
      "Training loss for batch 3174 : 0.12547357380390167\n",
      "Training loss for batch 3175 : 0.29327285289764404\n",
      "Training loss for batch 3176 : 0.004242143128067255\n",
      "Training loss for batch 3177 : 0.024050982668995857\n",
      "Training loss for batch 3178 : 0.056556228548288345\n",
      "Training loss for batch 3179 : 0.10776767134666443\n",
      "Training loss for batch 3180 : 0.021276814863085747\n",
      "Training loss for batch 3181 : 0.23178903758525848\n",
      "Training loss for batch 3182 : 0.07443835586309433\n",
      "Training loss for batch 3183 : 0.05803186073899269\n",
      "Training loss for batch 3184 : 0.17673727869987488\n",
      "Training loss for batch 3185 : 0.0846184641122818\n",
      "Training loss for batch 3186 : 0.11999529600143433\n",
      "Training loss for batch 3187 : 0.027676712721586227\n",
      "Training loss for batch 3188 : 0.09709136933088303\n",
      "Training loss for batch 3189 : 0.048353973776102066\n",
      "Training loss for batch 3190 : 0.04786965250968933\n",
      "Training loss for batch 3191 : 0.169621080160141\n",
      "Training loss for batch 3192 : 0.09850455820560455\n",
      "Training loss for batch 3193 : 0.10611745715141296\n",
      "Training loss for batch 3194 : 0.059054046869277954\n",
      "Training loss for batch 3195 : 0.056711599230766296\n",
      "Training loss for batch 3196 : 0.04276363551616669\n",
      "Training loss for batch 3197 : 0.05300651863217354\n",
      "Training loss for batch 3198 : 0.06457921117544174\n",
      "Training loss for batch 3199 : 0.08044610917568207\n",
      "Training loss for batch 3200 : 0.0655883327126503\n",
      "Training loss for batch 3201 : 0.1712370365858078\n",
      "Training loss for batch 3202 : 0.17651695013046265\n",
      "Training loss for batch 3203 : 0.009247447364032269\n",
      "Training loss for batch 3204 : 0.00840485468506813\n",
      "Training loss for batch 3205 : 0.11666151881217957\n",
      "Training loss for batch 3206 : 0.0563955195248127\n",
      "Training loss for batch 3207 : 0.11262726038694382\n",
      "Training loss for batch 3208 : 0.009556850418448448\n",
      "Training loss for batch 3209 : 0.02636531926691532\n",
      "Training loss for batch 3210 : 0.08037346601486206\n",
      "Training loss for batch 3211 : 0.04708213359117508\n",
      "Training loss for batch 3212 : 0.09897230565547943\n",
      "Training loss for batch 3213 : 0.1024385541677475\n",
      "Training loss for batch 3214 : 0.04501605033874512\n",
      "Training loss for batch 3215 : 0.0857657864689827\n",
      "Training loss for batch 3216 : 0.0924258604645729\n",
      "Training loss for batch 3217 : 0.15099070966243744\n",
      "Training loss for batch 3218 : 0.07553493231534958\n",
      "Training loss for batch 3219 : 1.1068021876781131e-07\n",
      "Training loss for batch 3220 : 0.08477146923542023\n",
      "Training loss for batch 3221 : 0.10047078877687454\n",
      "Training loss for batch 3222 : 0.14023032784461975\n",
      "Training loss for batch 3223 : 0.16355329751968384\n",
      "Training loss for batch 3224 : 0.1128799244761467\n",
      "Training loss for batch 3225 : 0.1251923143863678\n",
      "Training loss for batch 3226 : 0.18934977054595947\n",
      "Training loss for batch 3227 : 0.22946356236934662\n",
      "Training loss for batch 3228 : 0.3208009898662567\n",
      "Training loss for batch 3229 : 0.200751394033432\n",
      "Training loss for batch 3230 : 0.09802982956171036\n",
      "Training loss for batch 3231 : 0.07847052812576294\n",
      "Training loss for batch 3232 : 0.08787541836500168\n",
      "Training loss for batch 3233 : 0.049743279814720154\n",
      "Training loss for batch 3234 : 0.1912125051021576\n",
      "Training loss for batch 3235 : 0.016242872923612595\n",
      "Training loss for batch 3236 : 0.06343499571084976\n",
      "Training loss for batch 3237 : 0.19442281126976013\n",
      "Training loss for batch 3238 : 0.08190157264471054\n",
      "Training loss for batch 3239 : 0.07416810840368271\n",
      "Training loss for batch 3240 : 0.04518444463610649\n",
      "Training loss for batch 3241 : 0.07746309787034988\n",
      "Training loss for batch 3242 : 0.041173480451107025\n",
      "Training loss for batch 3243 : 0.18351560831069946\n",
      "Training loss for batch 3244 : 0.0930795818567276\n",
      "Training loss for batch 3245 : 0.051016706973314285\n",
      "Training loss for batch 3246 : 0.012029069475829601\n",
      "Training loss for batch 3247 : 0.0467069037258625\n",
      "Training loss for batch 3248 : 0.060766126960515976\n",
      "Training loss for batch 3249 : 0.06293822079896927\n",
      "Training loss for batch 3250 : 0.0811782106757164\n",
      "Training loss for batch 3251 : 0.03540380671620369\n",
      "Training loss for batch 3252 : 0.13755853474140167\n",
      "Training loss for batch 3253 : 0.0531446635723114\n",
      "Training loss for batch 3254 : 0.0282144695520401\n",
      "Training loss for batch 3255 : 0.23428018391132355\n",
      "Training loss for batch 3256 : 0.06234661489725113\n",
      "Training loss for batch 3257 : 0.07063739746809006\n",
      "Training loss for batch 3258 : 0.09462037682533264\n",
      "Training loss for batch 3259 : 0.05793658643960953\n",
      "Training loss for batch 3260 : 0.060249801725149155\n",
      "Training loss for batch 3261 : 0.07608979195356369\n",
      "Training loss for batch 3262 : 0.18634381890296936\n",
      "Training loss for batch 3263 : 0.09134618937969208\n",
      "Training loss for batch 3264 : 0.019296923652291298\n",
      "Training loss for batch 3265 : 0.16686995327472687\n",
      "Training loss for batch 3266 : 0.08403799682855606\n",
      "Training loss for batch 3267 : 0.0729551762342453\n",
      "Training loss for batch 3268 : 0.26370665431022644\n",
      "Training loss for batch 3269 : 0.075156569480896\n",
      "Training loss for batch 3270 : 0.03438519313931465\n",
      "Training loss for batch 3271 : 0.050017792731523514\n",
      "Training loss for batch 3272 : 0.11432605236768723\n",
      "Training loss for batch 3273 : 0.07995347678661346\n",
      "Training loss for batch 3274 : 0.09348008036613464\n",
      "Training loss for batch 3275 : 0.040466997772455215\n",
      "Training loss for batch 3276 : 0.009661233052611351\n",
      "Training loss for batch 3277 : 0.040558312088251114\n",
      "Training loss for batch 3278 : 0.15397390723228455\n",
      "Training loss for batch 3279 : 0.15637284517288208\n",
      "Training loss for batch 3280 : 0.0870162770152092\n",
      "Training loss for batch 3281 : 0.14693650603294373\n",
      "Training loss for batch 3282 : 0.08451560139656067\n",
      "Training loss for batch 3283 : 0.10220974683761597\n",
      "Training loss for batch 3284 : 0.021328981965780258\n",
      "Training loss for batch 3285 : 0.07440832257270813\n",
      "Training loss for batch 3286 : 0.08390028774738312\n",
      "Training loss for batch 3287 : 0.14948353171348572\n",
      "Training loss for batch 3288 : 0.09622377157211304\n",
      "Training loss for batch 3289 : 0.1006520465016365\n",
      "Training loss for batch 3290 : 0.08109243214130402\n",
      "Training loss for batch 3291 : 0.08439702540636063\n",
      "Training loss for batch 3292 : 0.14945222437381744\n",
      "Training loss for batch 3293 : 0.07757080346345901\n",
      "Training loss for batch 3294 : 0.0348924919962883\n",
      "Training loss for batch 3295 : 0.0623062439262867\n",
      "Training loss for batch 3296 : 0.05695372819900513\n",
      "Training loss for batch 3297 : 0.05739389732480049\n",
      "Training loss for batch 3298 : 0.10683457553386688\n",
      "Training loss for batch 3299 : 0.05983158200979233\n",
      "Training loss for batch 3300 : 0.03789043799042702\n",
      "Training loss for batch 3301 : 0.1545485556125641\n",
      "Training loss for batch 3302 : 0.09623061120510101\n",
      "Training loss for batch 3303 : 0.05543183162808418\n",
      "Training loss for batch 3304 : 0.02640516124665737\n",
      "Training loss for batch 3305 : 0.055831100791692734\n",
      "Training loss for batch 3306 : 0.026414556428790092\n",
      "Training loss for batch 3307 : 0.045889485627412796\n",
      "Training loss for batch 3308 : 0.05889240279793739\n",
      "Training loss for batch 3309 : 0.11940452456474304\n",
      "Training loss for batch 3310 : 0.0038343470077961683\n",
      "Training loss for batch 3311 : 0.14702662825584412\n",
      "Training loss for batch 3312 : 0.0385676771402359\n",
      "Training loss for batch 3313 : 0.1044541448354721\n",
      "Training loss for batch 3314 : 0.014507397077977657\n",
      "Training loss for batch 3315 : 0.08287311345338821\n",
      "Training loss for batch 3316 : 0.035205405205488205\n",
      "Training loss for batch 3317 : 0.09418903291225433\n",
      "Training loss for batch 3318 : 0.07808545976877213\n",
      "Training loss for batch 3319 : 0.10341480374336243\n",
      "Training loss for batch 3320 : 0.09569720923900604\n",
      "Training loss for batch 3321 : 0.11055166274309158\n",
      "Training loss for batch 3322 : 0.057868946343660355\n",
      "Training loss for batch 3323 : 0.14062358438968658\n",
      "Training loss for batch 3324 : 0.06894642114639282\n",
      "Training loss for batch 3325 : 0.011302377097308636\n",
      "Training loss for batch 3326 : 0.06540791690349579\n",
      "Training loss for batch 3327 : 0.0\n",
      "Training loss for batch 3328 : 0.1180189773440361\n",
      "Training loss for batch 3329 : 0.019951770082116127\n",
      "Training loss for batch 3330 : 0.10126238316297531\n",
      "Training loss for batch 3331 : 0.21225138008594513\n",
      "Training loss for batch 3332 : 0.04608980566263199\n",
      "Training loss for batch 3333 : 0.004811118356883526\n",
      "Training loss for batch 3334 : 0.016327235847711563\n",
      "Training loss for batch 3335 : 0.0761609673500061\n",
      "Training loss for batch 3336 : 0.0041039977222681046\n",
      "Training loss for batch 3337 : 0.14705447852611542\n",
      "Training loss for batch 3338 : 0.028041472658514977\n",
      "Training loss for batch 3339 : 0.22930185496807098\n",
      "Training loss for batch 3340 : 0.055714093148708344\n",
      "Training loss for batch 3341 : 0.029279861599206924\n",
      "Training loss for batch 3342 : 0.06290306150913239\n",
      "Training loss for batch 3343 : 0.1611124873161316\n",
      "Training loss for batch 3344 : 0.03519459441304207\n",
      "Training loss for batch 3345 : 0.045745085924863815\n",
      "Training loss for batch 3346 : 0.08087769895792007\n",
      "Training loss for batch 3347 : 0.029717715457081795\n",
      "Training loss for batch 3348 : 0.16861745715141296\n",
      "Training loss for batch 3349 : 0.12133383750915527\n",
      "Training loss for batch 3350 : 0.04652132838964462\n",
      "Training loss for batch 3351 : 0.06375919282436371\n",
      "Training loss for batch 3352 : 0.12204050272703171\n",
      "Training loss for batch 3353 : 0.08957374095916748\n",
      "Training loss for batch 3354 : 0.11745871603488922\n",
      "Training loss for batch 3355 : 0.16621695458889008\n",
      "Training loss for batch 3356 : 0.026657238602638245\n",
      "Training loss for batch 3357 : 0.10265212506055832\n",
      "Training loss for batch 3358 : 0.022709595039486885\n",
      "Training loss for batch 3359 : 0.185377836227417\n",
      "Training loss for batch 3360 : 0.03685867413878441\n",
      "Training loss for batch 3361 : 0.01811819337308407\n",
      "Training loss for batch 3362 : 0.0587993785738945\n",
      "Training loss for batch 3363 : 0.21700729429721832\n",
      "Training loss for batch 3364 : 0.021854929625988007\n",
      "Training loss for batch 3365 : 0.11857620626688004\n",
      "Training loss for batch 3366 : 0.013352849520742893\n",
      "Training loss for batch 3367 : 0.05103448033332825\n",
      "Training loss for batch 3368 : 0.0725870355963707\n",
      "Training loss for batch 3369 : 0.047087062150239944\n",
      "Training loss for batch 3370 : 0.13873980939388275\n",
      "Training loss for batch 3371 : 0.1015254408121109\n",
      "Training loss for batch 3372 : 0.029807519167661667\n",
      "Training loss for batch 3373 : 0.0683828815817833\n",
      "Training loss for batch 3374 : 0.0809173658490181\n",
      "Training loss for batch 3375 : 5.509280853743803e-08\n",
      "Training loss for batch 3376 : 0.20067887008190155\n",
      "Training loss for batch 3377 : 0.08430929481983185\n",
      "Training loss for batch 3378 : 0.044736456125974655\n",
      "Training loss for batch 3379 : 0.012194558046758175\n",
      "Training loss for batch 3380 : 0.18582558631896973\n",
      "Training loss for batch 3381 : 0.18325035274028778\n",
      "Training loss for batch 3382 : 0.0005390493315644562\n",
      "Training loss for batch 3383 : 0.050711292773485184\n",
      "Training loss for batch 3384 : 0.17956781387329102\n",
      "Training loss for batch 3385 : 0.06614460796117783\n",
      "Training loss for batch 3386 : 0.02334422431886196\n",
      "Training loss for batch 3387 : 0.14525766670703888\n",
      "Training loss for batch 3388 : 0.0330609530210495\n",
      "Training loss for batch 3389 : 0.028274642303586006\n",
      "Training loss for batch 3390 : 0.08799200505018234\n",
      "Training loss for batch 3391 : 0.029445098713040352\n",
      "Training loss for batch 3392 : 0.1082463189959526\n",
      "Training loss for batch 3393 : 0.011415704153478146\n",
      "Training loss for batch 3394 : 0.11761446297168732\n",
      "Training loss for batch 3395 : 0.0\n",
      "Training loss for batch 3396 : 0.0259709469974041\n",
      "Training loss for batch 3397 : 0.3422427177429199\n",
      "Training loss for batch 3398 : 0.09082846343517303\n",
      "Training loss for batch 3399 : 0.18181252479553223\n",
      "Training loss for batch 3400 : 0.06849639117717743\n",
      "Training loss for batch 3401 : 0.047369323670864105\n",
      "Training loss for batch 3402 : 0.004875305108726025\n",
      "Training loss for batch 3403 : 0.09835776686668396\n",
      "Training loss for batch 3404 : 0.17494797706604004\n",
      "Training loss for batch 3405 : 0.08612825721502304\n",
      "Training loss for batch 3406 : 0.011612302623689175\n",
      "Training loss for batch 3407 : 0.04053599014878273\n",
      "Training loss for batch 3408 : 0.04942449927330017\n",
      "Training loss for batch 3409 : 0.07339221984148026\n",
      "Training loss for batch 3410 : 0.03407438471913338\n",
      "Training loss for batch 3411 : 0.10655143111944199\n",
      "Training loss for batch 3412 : 0.008933713659644127\n",
      "Training loss for batch 3413 : 0.0484333336353302\n",
      "Training loss for batch 3414 : 0.10582456737756729\n",
      "Training loss for batch 3415 : 0.2140924036502838\n",
      "Training loss for batch 3416 : 0.04086054116487503\n",
      "Training loss for batch 3417 : 0.17122295498847961\n",
      "Training loss for batch 3418 : 0.10820553451776505\n",
      "Training loss for batch 3419 : 0.2039879560470581\n",
      "Training loss for batch 3420 : 0.06316717714071274\n",
      "Training loss for batch 3421 : 0.06092948466539383\n",
      "Training loss for batch 3422 : 0.04025983437895775\n",
      "Training loss for batch 3423 : 0.10339251160621643\n",
      "Training loss for batch 3424 : 0.05076274648308754\n",
      "Training loss for batch 3425 : 0.06897546350955963\n",
      "Training loss for batch 3426 : 0.1050419881939888\n",
      "Training loss for batch 3427 : 0.1307717114686966\n",
      "Training loss for batch 3428 : 0.058003634214401245\n",
      "Training loss for batch 3429 : 0.03617723658680916\n",
      "Training loss for batch 3430 : 0.020849846303462982\n",
      "Training loss for batch 3431 : 0.2734939754009247\n",
      "Training loss for batch 3432 : 0.252035915851593\n",
      "Training loss for batch 3433 : 0.1394597589969635\n",
      "Training loss for batch 3434 : 0.09157933294773102\n",
      "Training loss for batch 3435 : 0.26369357109069824\n",
      "Training loss for batch 3436 : 0.15651004016399384\n",
      "Training loss for batch 3437 : 0.04495331272482872\n",
      "Training loss for batch 3438 : 0.07569848746061325\n",
      "Training loss for batch 3439 : 0.07687409967184067\n",
      "Training loss for batch 3440 : 0.09690915048122406\n",
      "Training loss for batch 3441 : 0.17699837684631348\n",
      "Training loss for batch 3442 : 0.1574975848197937\n",
      "Training loss for batch 3443 : 0.046606242656707764\n",
      "Training loss for batch 3444 : 0.2282911241054535\n",
      "Training loss for batch 3445 : 0.16839320957660675\n",
      "Training loss for batch 3446 : 0.057722508907318115\n",
      "Training loss for batch 3447 : 0.17867955565452576\n",
      "Training loss for batch 3448 : 0.166634202003479\n",
      "Training loss for batch 3449 : 0.25777724385261536\n",
      "Training loss for batch 3450 : 0.036859236657619476\n",
      "Training loss for batch 3451 : 0.05783688649535179\n",
      "Training loss for batch 3452 : 0.12858325242996216\n",
      "Training loss for batch 3453 : 0.03448278084397316\n",
      "Training loss for batch 3454 : 0.06649065762758255\n",
      "Training loss for batch 3455 : 0.0815017893910408\n",
      "Training loss for batch 3456 : 0.02691795490682125\n",
      "Training loss for batch 3457 : 0.02094021439552307\n",
      "Training loss for batch 3458 : 0.10615507513284683\n",
      "Training loss for batch 3459 : 0.04952499642968178\n",
      "Training loss for batch 3460 : 0.10620128363370895\n",
      "Training loss for batch 3461 : 0.051733341068029404\n",
      "Training loss for batch 3462 : 0.00901589635759592\n",
      "Training loss for batch 3463 : 0.1819538176059723\n",
      "Training loss for batch 3464 : 0.0688031017780304\n",
      "Training loss for batch 3465 : 0.08602234721183777\n",
      "Training loss for batch 3466 : 0.18514297902584076\n",
      "Training loss for batch 3467 : 0.13696058094501495\n",
      "Training loss for batch 3468 : 0.2249937802553177\n",
      "Training loss for batch 3469 : 0.11671276390552521\n",
      "Training loss for batch 3470 : 0.04605698585510254\n",
      "Training loss for batch 3471 : 0.0874645933508873\n",
      "Training loss for batch 3472 : 0.15789219737052917\n",
      "Training loss for batch 3473 : 0.02297983132302761\n",
      "Training loss for batch 3474 : 0.09354223310947418\n",
      "Training loss for batch 3475 : 0.008903327398002148\n",
      "Training loss for batch 3476 : 0.0774373784661293\n",
      "Training loss for batch 3477 : 0.11311843246221542\n",
      "Training loss for batch 3478 : 0.05317665636539459\n",
      "Training loss for batch 3479 : 0.014140190556645393\n",
      "Training loss for batch 3480 : 0.017076639458537102\n",
      "Training loss for batch 3481 : 0.07435934245586395\n",
      "Training loss for batch 3482 : 0.15884670615196228\n",
      "Training loss for batch 3483 : 0.3124784529209137\n",
      "Training loss for batch 3484 : 0.20365984737873077\n",
      "Training loss for batch 3485 : 0.05415397137403488\n",
      "Training loss for batch 3486 : 0.07215388864278793\n",
      "Training loss for batch 3487 : 0.1370653510093689\n",
      "Training loss for batch 3488 : 0.15784679353237152\n",
      "Training loss for batch 3489 : 0.048212889581918716\n",
      "Training loss for batch 3490 : 0.13161948323249817\n",
      "Training loss for batch 3491 : 0.012435652315616608\n",
      "Training loss for batch 3492 : 0.19903793931007385\n",
      "Training loss for batch 3493 : 0.07827042043209076\n",
      "Training loss for batch 3494 : 0.020380862057209015\n",
      "Training loss for batch 3495 : 0.06930916756391525\n",
      "Training loss for batch 3496 : 0.13046440482139587\n",
      "Training loss for batch 3497 : 0.13088229298591614\n",
      "Training loss for batch 3498 : 0.14209626615047455\n",
      "Training loss for batch 3499 : 0.12263523787260056\n",
      "Training loss for batch 3500 : 0.14458826184272766\n",
      "Training loss for batch 3501 : 0.05320604890584946\n",
      "Training loss for batch 3502 : 0.05596964433789253\n",
      "Training loss for batch 3503 : 0.06809737533330917\n",
      "Training loss for batch 3504 : 0.0722474530339241\n",
      "Training loss for batch 3505 : 0.04949885234236717\n",
      "Training loss for batch 3506 : 0.05923591926693916\n",
      "Training loss for batch 3507 : 0.1346197873353958\n",
      "Training loss for batch 3508 : 0.1549968123435974\n",
      "Training loss for batch 3509 : 0.14290593564510345\n",
      "Training loss for batch 3510 : 0.1857127845287323\n",
      "Training loss for batch 3511 : 0.14468984305858612\n",
      "Training loss for batch 3512 : 0.07242999225854874\n",
      "Training loss for batch 3513 : 0.06806666404008865\n",
      "Training loss for batch 3514 : 0.041733209043741226\n",
      "Training loss for batch 3515 : 0.06584174931049347\n",
      "Training loss for batch 3516 : 0.1624818742275238\n",
      "Training loss for batch 3517 : 0.1338653266429901\n",
      "Training loss for batch 3518 : 0.06495015323162079\n",
      "Training loss for batch 3519 : 0.015085764229297638\n",
      "Training loss for batch 3520 : 0.07504512369632721\n",
      "Training loss for batch 3521 : 0.09876462817192078\n",
      "Training loss for batch 3522 : 0.10962370783090591\n",
      "Training loss for batch 3523 : 0.09654165804386139\n",
      "Training loss for batch 3524 : 0.06318337470293045\n",
      "Training loss for batch 3525 : 0.09064342081546783\n",
      "Training loss for batch 3526 : 0.028522292152047157\n",
      "Training loss for batch 3527 : 0.12615764141082764\n",
      "Training loss for batch 3528 : 0.026781294494867325\n",
      "Training loss for batch 3529 : 0.08680643141269684\n",
      "Training loss for batch 3530 : 0.06847018748521805\n",
      "Training loss for batch 3531 : 0.027504289522767067\n",
      "Training loss for batch 3532 : 0.0736495777964592\n",
      "Training loss for batch 3533 : 0.03351074084639549\n",
      "Training loss for batch 3534 : 0.16360755264759064\n",
      "Training loss for batch 3535 : 0.08561093360185623\n",
      "Training loss for batch 3536 : 0.020889131352305412\n",
      "Training loss for batch 3537 : 0.11771754920482635\n",
      "Training loss for batch 3538 : 0.1911858320236206\n",
      "Training loss for batch 3539 : 0.03113916888833046\n",
      "Training loss for batch 3540 : 0.15085741877555847\n",
      "Training loss for batch 3541 : 0.015535065904259682\n",
      "Training loss for batch 3542 : 0.08323723077774048\n",
      "Training loss for batch 3543 : 0.043229881674051285\n",
      "Training loss for batch 3544 : 0.014829039573669434\n",
      "Training loss for batch 3545 : 0.08469071984291077\n",
      "Training loss for batch 3546 : 0.2080085575580597\n",
      "Training loss for batch 3547 : 0.11218378692865372\n",
      "Training loss for batch 3548 : 0.0941089615225792\n",
      "Training loss for batch 3549 : 0.19240714609622955\n",
      "Training loss for batch 3550 : 0.07356277108192444\n",
      "Training loss for batch 3551 : 0.03718186914920807\n",
      "Training loss for batch 3552 : 0.04278036206960678\n",
      "Training loss for batch 3553 : 0.0012102227192372084\n",
      "Training loss for batch 3554 : 0.12326303869485855\n",
      "Training loss for batch 3555 : 0.10738097876310349\n",
      "Training loss for batch 3556 : 0.018654515966773033\n",
      "Training loss for batch 3557 : 0.020315898582339287\n",
      "Training loss for batch 3558 : 0.08337178081274033\n",
      "Training loss for batch 3559 : 0.041604965925216675\n",
      "Training loss for batch 3560 : 0.03828469291329384\n",
      "Training loss for batch 3561 : 0.06922662258148193\n",
      "Training loss for batch 3562 : 0.12263386696577072\n",
      "Training loss for batch 3563 : 0.004149056971073151\n",
      "Training loss for batch 3564 : 0.16051185131072998\n",
      "Training loss for batch 3565 : 0.10305236279964447\n",
      "Training loss for batch 3566 : 0.07949794083833694\n",
      "Training loss for batch 3567 : 0.02823922038078308\n",
      "Training loss for batch 3568 : 0.023590900003910065\n",
      "Training loss for batch 3569 : 0.02610790729522705\n",
      "Training loss for batch 3570 : 0.05940880998969078\n",
      "Training loss for batch 3571 : 0.04187865927815437\n",
      "Training loss for batch 3572 : 0.10165980458259583\n",
      "Training loss for batch 3573 : 0.037324558943510056\n",
      "Training loss for batch 3574 : 0.028773227706551552\n",
      "Training loss for batch 3575 : 0.1328134983778\n",
      "Training loss for batch 3576 : 0.02032437175512314\n",
      "Training loss for batch 3577 : 0.07788019627332687\n",
      "Training loss for batch 3578 : 0.19825436174869537\n",
      "Training loss for batch 3579 : 0.12623342871665955\n",
      "Training loss for batch 3580 : 0.206751748919487\n",
      "Training loss for batch 3581 : 0.13206017017364502\n",
      "Training loss for batch 3582 : 0.016755148768424988\n",
      "Training loss for batch 3583 : 0.03701048344373703\n",
      "Training loss for batch 3584 : 0.040447041392326355\n",
      "Training loss for batch 3585 : 0.1767074465751648\n",
      "Training loss for batch 3586 : 0.06478045135736465\n",
      "Training loss for batch 3587 : 0.07312925159931183\n",
      "Training loss for batch 3588 : 0.049063947051763535\n",
      "Training loss for batch 3589 : 0.11302857100963593\n",
      "Training loss for batch 3590 : 0.01954852044582367\n",
      "Training loss for batch 3591 : 0.021604696288704872\n",
      "Training loss for batch 3592 : 0.08341211825609207\n",
      "Training loss for batch 3593 : 0.04731753468513489\n",
      "Training loss for batch 3594 : 0.14747750759124756\n",
      "Training loss for batch 3595 : 0.01566707156598568\n",
      "Training loss for batch 3596 : 0.03015105426311493\n",
      "Training loss for batch 3597 : 0.018617641180753708\n",
      "Training loss for batch 3598 : 0.014083768241107464\n",
      "Training loss for batch 3599 : 0.13853983581066132\n",
      "Training loss for batch 3600 : 0.0\n",
      "Training loss for batch 3601 : 0.14990335702896118\n",
      "Training loss for batch 3602 : 0.053046151995658875\n",
      "Training loss for batch 3603 : 0.17089655995368958\n",
      "Training loss for batch 3604 : 0.14087045192718506\n",
      "Training loss for batch 3605 : 0.011777393519878387\n",
      "Training loss for batch 3606 : 0.17704327404499054\n",
      "Training loss for batch 3607 : 0.07873982191085815\n",
      "Training loss for batch 3608 : 0.19425225257873535\n",
      "Training loss for batch 3609 : 0.22384724020957947\n",
      "Training loss for batch 3610 : 0.060335446149110794\n",
      "Training loss for batch 3611 : 0.004884492605924606\n",
      "Training loss for batch 3612 : 0.1123942956328392\n",
      "Training loss for batch 3613 : 0.16133177280426025\n",
      "Training loss for batch 3614 : 0.1749281883239746\n",
      "Training loss for batch 3615 : 0.06767982989549637\n",
      "Training loss for batch 3616 : 0.07000990957021713\n",
      "Training loss for batch 3617 : 0.002534484025090933\n",
      "Training loss for batch 3618 : 0.0018252655863761902\n",
      "Training loss for batch 3619 : 0.04049914702773094\n",
      "Training loss for batch 3620 : 0.055460259318351746\n",
      "Training loss for batch 3621 : 0.1365082710981369\n",
      "Training loss for batch 3622 : 0.09891882538795471\n",
      "Training loss for batch 3623 : 0.20414137840270996\n",
      "Training loss for batch 3624 : 0.12226995080709457\n",
      "Training loss for batch 3625 : 0.05519312247633934\n",
      "Training loss for batch 3626 : 0.017333224415779114\n",
      "Training loss for batch 3627 : 0.199202761054039\n",
      "Training loss for batch 3628 : 0.035898298025131226\n",
      "Training loss for batch 3629 : 0.04647196829319\n",
      "Training loss for batch 3630 : 0.14644792675971985\n",
      "Training loss for batch 3631 : 0.011251314543187618\n",
      "Training loss for batch 3632 : 0.07947354018688202\n",
      "Training loss for batch 3633 : 0.0658721923828125\n",
      "Training loss for batch 3634 : 0.031841326504945755\n",
      "Training loss for batch 3635 : 0.019672611728310585\n",
      "Training loss for batch 3636 : 0.06984961032867432\n",
      "Training loss for batch 3637 : 0.10728923976421356\n",
      "Training loss for batch 3638 : 0.05258515849709511\n",
      "Training loss for batch 3639 : 0.32661888003349304\n",
      "Training loss for batch 3640 : 0.1705804169178009\n",
      "Training loss for batch 3641 : 0.027006391435861588\n",
      "Training loss for batch 3642 : 0.21345940232276917\n",
      "Training loss for batch 3643 : 0.050722185522317886\n",
      "Training loss for batch 3644 : 0.027438407763838768\n",
      "Training loss for batch 3645 : 0.1267753541469574\n",
      "Training loss for batch 3646 : 0.053165502846241\n",
      "Training loss for batch 3647 : 5.352460696883554e-09\n",
      "Training loss for batch 3648 : 0.051249872893095016\n",
      "Training loss for batch 3649 : 0.02518501691520214\n",
      "Training loss for batch 3650 : 0.2660108804702759\n",
      "Training loss for batch 3651 : 0.09952055662870407\n",
      "Training loss for batch 3652 : 0.22930482029914856\n",
      "Training loss for batch 3653 : 0.08422700315713882\n",
      "Training loss for batch 3654 : 0.14851278066635132\n",
      "Training loss for batch 3655 : 0.05091487243771553\n",
      "Training loss for batch 3656 : 0.11553631722927094\n",
      "Training loss for batch 3657 : 0.18891148269176483\n",
      "Training loss for batch 3658 : 0.010088402777910233\n",
      "Training loss for batch 3659 : 0.07380866259336472\n",
      "Training loss for batch 3660 : 0.12940038740634918\n",
      "Training loss for batch 3661 : 0.1580554097890854\n",
      "Training loss for batch 3662 : 0.1538887321949005\n",
      "Training loss for batch 3663 : 0.07326129078865051\n",
      "Training loss for batch 3664 : 0.06672952324151993\n",
      "Training loss for batch 3665 : 0.2111479938030243\n",
      "Training loss for batch 3666 : 0.08219575881958008\n",
      "Training loss for batch 3667 : 0.06320206075906754\n",
      "Training loss for batch 3668 : 0.019448991864919662\n",
      "Training loss for batch 3669 : 0.08213799446821213\n",
      "Training loss for batch 3670 : 0.02332165092229843\n",
      "Training loss for batch 3671 : 0.022569481283426285\n",
      "Training loss for batch 3672 : 0.14933480322360992\n",
      "Training loss for batch 3673 : 0.04676562920212746\n",
      "Training loss for batch 3674 : 0.05545421317219734\n",
      "Training loss for batch 3675 : 0.018506111577153206\n",
      "Training loss for batch 3676 : 0.027464648708701134\n",
      "Training loss for batch 3677 : 0.06829395145177841\n",
      "Training loss for batch 3678 : 0.19437076151371002\n",
      "Training loss for batch 3679 : 0.12560103833675385\n",
      "Training loss for batch 3680 : 0.08612022548913956\n",
      "Training loss for batch 3681 : 0.11598251014947891\n",
      "Training loss for batch 3682 : 0.14195699989795685\n",
      "Training loss for batch 3683 : 0.08656967431306839\n",
      "Training loss for batch 3684 : 0.020137256011366844\n",
      "Training loss for batch 3685 : 0.19052310287952423\n",
      "Training loss for batch 3686 : 0.012604236602783203\n",
      "Training loss for batch 3687 : 0.1215294599533081\n",
      "Training loss for batch 3688 : 0.06460852921009064\n",
      "Training loss for batch 3689 : 0.1320052146911621\n",
      "Training loss for batch 3690 : 0.27724406123161316\n",
      "Training loss for batch 3691 : 0.04581555351614952\n",
      "Training loss for batch 3692 : 0.022180214524269104\n",
      "Training loss for batch 3693 : 0.11951997131109238\n",
      "Training loss for batch 3694 : 0.12225815653800964\n",
      "Training loss for batch 3695 : 0.05733133852481842\n",
      "Training loss for batch 3696 : 0.0465395487844944\n",
      "Training loss for batch 3697 : 0.09057056903839111\n",
      "Training loss for batch 3698 : 0.10399935394525528\n",
      "Training loss for batch 3699 : 0.4322039484977722\n",
      "Training loss for batch 3700 : 0.1979808211326599\n",
      "Training loss for batch 3701 : 0.05470862239599228\n",
      "Training loss for batch 3702 : 0.05661921575665474\n",
      "Training loss for batch 3703 : 0.15347442030906677\n",
      "Training loss for batch 3704 : 0.19626839458942413\n",
      "Training loss for batch 3705 : 0.04343928024172783\n",
      "Training loss for batch 3706 : 0.03256014734506607\n",
      "Training loss for batch 3707 : 0.05611873045563698\n",
      "Training loss for batch 3708 : 0.06717938929796219\n",
      "Training loss for batch 3709 : 0.009784023277461529\n",
      "Training loss for batch 3710 : 0.028107574209570885\n",
      "Training loss for batch 3711 : 0.15933825075626373\n",
      "Training loss for batch 3712 : 0.04479163512587547\n",
      "Training loss for batch 3713 : 0.07478311657905579\n",
      "Training loss for batch 3714 : 0.1544036865234375\n",
      "Training loss for batch 3715 : 0.023475471884012222\n",
      "Training loss for batch 3716 : 0.08621388673782349\n",
      "Training loss for batch 3717 : 0.14139550924301147\n",
      "Training loss for batch 3718 : 0.10489099472761154\n",
      "Training loss for batch 3719 : 0.15841977298259735\n",
      "Training loss for batch 3720 : 0.15179814398288727\n",
      "Training loss for batch 3721 : 0.1167282834649086\n",
      "Training loss for batch 3722 : 0.16798976063728333\n",
      "Training loss for batch 3723 : 0.07359620183706284\n",
      "Training loss for batch 3724 : 0.06446173042058945\n",
      "Training loss for batch 3725 : 0.024195700883865356\n",
      "Training loss for batch 3726 : 0.1330592930316925\n",
      "Training loss for batch 3727 : 0.029125716537237167\n",
      "Training loss for batch 3728 : 0.026661638170480728\n",
      "Training loss for batch 3729 : 0.213886559009552\n",
      "Training loss for batch 3730 : 0.14764411747455597\n",
      "Training loss for batch 3731 : 0.15198375284671783\n",
      "Training loss for batch 3732 : 0.01868019625544548\n",
      "Training loss for batch 3733 : 0.07434632629156113\n",
      "Training loss for batch 3734 : 0.0741829201579094\n",
      "Training loss for batch 3735 : 0.1061888113617897\n",
      "Training loss for batch 3736 : 0.054036423563957214\n",
      "Training loss for batch 3737 : 0.18040955066680908\n",
      "Training loss for batch 3738 : 0.15563294291496277\n",
      "Training loss for batch 3739 : 0.14940868318080902\n",
      "Training loss for batch 3740 : 0.11207031458616257\n",
      "Training loss for batch 3741 : 0.0766606256365776\n",
      "Training loss for batch 3742 : 0.19999809563159943\n",
      "Training loss for batch 3743 : 0.22586247324943542\n",
      "Training loss for batch 3744 : 0.005646442528814077\n",
      "Training loss for batch 3745 : 0.046482641249895096\n",
      "Training loss for batch 3746 : 0.03664373978972435\n",
      "Training loss for batch 3747 : 0.0302251186221838\n",
      "Training loss for batch 3748 : 0.10993277281522751\n",
      "Training loss for batch 3749 : 0.2440827488899231\n",
      "Training loss for batch 3750 : 0.12979064881801605\n",
      "Training loss for batch 3751 : 0.08006396889686584\n",
      "Training loss for batch 3752 : 0.0993533656001091\n",
      "Training loss for batch 3753 : 0.006651381962001324\n",
      "Training loss for batch 3754 : 0.0979509949684143\n",
      "Training loss for batch 3755 : 0.15884360671043396\n",
      "Training loss for batch 3756 : 0.13056960701942444\n",
      "Training loss for batch 3757 : 0.02819358929991722\n",
      "Training loss for batch 3758 : 0.09477392584085464\n",
      "Training loss for batch 3759 : 0.13811025023460388\n",
      "Training loss for batch 3760 : 0.13100504875183105\n",
      "Training loss for batch 3761 : 1.419794948276376e-09\n",
      "Training loss for batch 3762 : 0.16438481211662292\n",
      "Training loss for batch 3763 : 0.1050150990486145\n",
      "Training loss for batch 3764 : 0.0270627960562706\n",
      "Training loss for batch 3765 : 0.12785688042640686\n",
      "Training loss for batch 3766 : 0.10630393028259277\n",
      "Training loss for batch 3767 : 0.17417588829994202\n",
      "Training loss for batch 3768 : 0.08861108869314194\n",
      "Training loss for batch 3769 : 0.2085694521665573\n",
      "Training loss for batch 3770 : 0.1093357652425766\n",
      "Training loss for batch 3771 : 0.13845103979110718\n",
      "Training loss for batch 3772 : 0.1286732405424118\n",
      "Training loss for batch 3773 : 0.009155028499662876\n",
      "Training loss for batch 3774 : 0.11138953268527985\n",
      "Training loss for batch 3775 : 0.06899372488260269\n",
      "Training loss for batch 3776 : 0.06928980350494385\n",
      "Training loss for batch 3777 : 0.137887105345726\n",
      "Training loss for batch 3778 : 0.09244626760482788\n",
      "Training loss for batch 3779 : 0.24353808164596558\n",
      "Training loss for batch 3780 : 0.03573383018374443\n",
      "Training loss for batch 3781 : 0.012592903338372707\n",
      "Training loss for batch 3782 : 0.05162699893116951\n",
      "Training loss for batch 3783 : 0.14564940333366394\n",
      "Training loss for batch 3784 : 0.06722519546747208\n",
      "Training loss for batch 3785 : 0.1271897405385971\n",
      "Training loss for batch 3786 : 0.06313847005367279\n",
      "Training loss for batch 3787 : 0.08827828615903854\n",
      "Training loss for batch 3788 : 0.3001232147216797\n",
      "Training loss for batch 3789 : 0.030721213668584824\n",
      "Training loss for batch 3790 : 0.16885994374752045\n",
      "Training loss for batch 3791 : 0.08189211785793304\n",
      "Training loss for batch 3792 : 0.02992585301399231\n",
      "Training loss for batch 3793 : 0.13488930463790894\n",
      "Training loss for batch 3794 : 0.1149211898446083\n",
      "Training loss for batch 3795 : 0.1317206472158432\n",
      "Training loss for batch 3796 : 0.1332542598247528\n",
      "Training loss for batch 3797 : 0.027751771733164787\n",
      "Training loss for batch 3798 : 0.0781535878777504\n",
      "Training loss for batch 3799 : 0.05981818959116936\n",
      "Training loss for batch 3800 : 0.1505749374628067\n",
      "Training loss for batch 3801 : 0.0945063903927803\n",
      "Training loss for batch 3802 : 0.00837613269686699\n",
      "Training loss for batch 3803 : 0.06047133356332779\n",
      "Training loss for batch 3804 : 0.10793139785528183\n",
      "Training loss for batch 3805 : 0.01675255410373211\n",
      "Training loss for batch 3806 : 0.12042055279016495\n",
      "Training loss for batch 3807 : 0.08936183899641037\n",
      "Training loss for batch 3808 : 0.035503536462783813\n",
      "Training loss for batch 3809 : 0.1301705539226532\n",
      "Training loss for batch 3810 : 0.07165586948394775\n",
      "Training loss for batch 3811 : 0.0768495425581932\n",
      "Training loss for batch 3812 : 0.17839454114437103\n",
      "Training loss for batch 3813 : 0.030941959470510483\n",
      "Training loss for batch 3814 : 0.19502083957195282\n",
      "Training loss for batch 3815 : 0.1893128603696823\n",
      "Training loss for batch 3816 : 0.017046475782990456\n",
      "Training loss for batch 3817 : 0.04750271141529083\n",
      "Training loss for batch 3818 : 0.0749235600233078\n",
      "Training loss for batch 3819 : 0.14564305543899536\n",
      "Training loss for batch 3820 : 0.11821317672729492\n",
      "Training loss for batch 3821 : 0.1882251501083374\n",
      "Training loss for batch 3822 : 0.010277542285621166\n",
      "Training loss for batch 3823 : 0.0\n",
      "Training loss for batch 3824 : 0.12704779207706451\n",
      "Training loss for batch 3825 : 0.05332526937127113\n",
      "Training loss for batch 3826 : 0.01902598887681961\n",
      "Training loss for batch 3827 : 0.0030700056813657284\n",
      "Training loss for batch 3828 : 0.026654861867427826\n",
      "Training loss for batch 3829 : 0.07053893804550171\n",
      "Training loss for batch 3830 : 0.10983939468860626\n",
      "Training loss for batch 3831 : 0.027077466249465942\n",
      "Training loss for batch 3832 : 0.18296493589878082\n",
      "Training loss for batch 3833 : 0.233281210064888\n",
      "Training loss for batch 3834 : 0.17861638963222504\n",
      "Training loss for batch 3835 : 0.13121245801448822\n",
      "Training loss for batch 3836 : 0.17709283530712128\n",
      "Training loss for batch 3837 : 0.006448240019381046\n",
      "Training loss for batch 3838 : 0.1422126293182373\n",
      "Training loss for batch 3839 : 0.10973602533340454\n",
      "Training loss for batch 3840 : 0.029218178242444992\n",
      "Training loss for batch 3841 : 0.017547238618135452\n",
      "Training loss for batch 3842 : 0.014740628190338612\n",
      "Training loss for batch 3843 : 0.13305522501468658\n",
      "Training loss for batch 3844 : 0.2572092115879059\n",
      "Training loss for batch 3845 : 0.17753811180591583\n",
      "Training loss for batch 3846 : 0.23300477862358093\n",
      "Training loss for batch 3847 : 0.08748973160982132\n",
      "Training loss for batch 3848 : 0.027909671887755394\n",
      "Training loss for batch 3849 : 0.06861390173435211\n",
      "Training loss for batch 3850 : 0.1438058465719223\n",
      "Training loss for batch 3851 : 0.0717124417424202\n",
      "Training loss for batch 3852 : 0.07424180954694748\n",
      "Training loss for batch 3853 : 0.06821701675653458\n",
      "Training loss for batch 3854 : 0.06229604780673981\n",
      "Training loss for batch 3855 : 0.09815865755081177\n",
      "Training loss for batch 3856 : 0.08411157876253128\n",
      "Training loss for batch 3857 : 0.10850906372070312\n",
      "Training loss for batch 3858 : 0.11127200722694397\n",
      "Training loss for batch 3859 : 0.09332894533872604\n",
      "Training loss for batch 3860 : 0.057723354548215866\n",
      "Training loss for batch 3861 : 0.0452156737446785\n",
      "Training loss for batch 3862 : 0.12276645749807358\n",
      "Training loss for batch 3863 : 0.05897726118564606\n",
      "Training loss for batch 3864 : 0.03193710744380951\n",
      "Training loss for batch 3865 : 0.0516982264816761\n",
      "Training loss for batch 3866 : 0.06964045763015747\n",
      "Training loss for batch 3867 : 0.05802135169506073\n",
      "Training loss for batch 3868 : 0.025100480765104294\n",
      "Training loss for batch 3869 : 0.07136628031730652\n",
      "Training loss for batch 3870 : 0.0483640618622303\n",
      "Training loss for batch 3871 : 0.07937800884246826\n",
      "Training loss for batch 3872 : 0.17328089475631714\n",
      "Training loss for batch 3873 : 0.14845161139965057\n",
      "Training loss for batch 3874 : 0.07266993820667267\n",
      "Training loss for batch 3875 : 0.022625019773840904\n",
      "Training loss for batch 3876 : 0.2167634814977646\n",
      "Training loss for batch 3877 : 0.16440066695213318\n",
      "Training loss for batch 3878 : 0.10787167400121689\n",
      "Training loss for batch 3879 : 0.052296385169029236\n",
      "Training loss for batch 3880 : 0.1091223731637001\n",
      "Training loss for batch 3881 : 0.04146989434957504\n",
      "Training loss for batch 3882 : 0.03468068316578865\n",
      "Training loss for batch 3883 : 0.034534890204668045\n",
      "Training loss for batch 3884 : 0.12240716069936752\n",
      "Training loss for batch 3885 : 0.08167488873004913\n",
      "Training loss for batch 3886 : 0.12313836067914963\n",
      "Training loss for batch 3887 : 0.034341827034950256\n",
      "Training loss for batch 3888 : 0.0251043438911438\n",
      "Training loss for batch 3889 : 0.13300472497940063\n",
      "Training loss for batch 3890 : 0.13098382949829102\n",
      "Training loss for batch 3891 : 0.04922002926468849\n",
      "Training loss for batch 3892 : 0.08523403853178024\n",
      "Training loss for batch 3893 : 0.03463040292263031\n",
      "Training loss for batch 3894 : 0.12023364007472992\n",
      "Training loss for batch 3895 : 0.14137212932109833\n",
      "Training loss for batch 3896 : 0.035770490765571594\n",
      "Training loss for batch 3897 : 0.03218758478760719\n",
      "Training loss for batch 3898 : 0.06237809732556343\n",
      "Training loss for batch 3899 : 0.18983890116214752\n",
      "Training loss for batch 3900 : 0.042814936488866806\n",
      "Training loss for batch 3901 : 0.0820666030049324\n",
      "Training loss for batch 3902 : 0.07490904629230499\n",
      "Training loss for batch 3903 : 0.0696147233247757\n",
      "Training loss for batch 3904 : 0.0958707183599472\n",
      "Training loss for batch 3905 : 0.017125599086284637\n",
      "Training loss for batch 3906 : 0.024475693702697754\n",
      "Training loss for batch 3907 : 0.037855472415685654\n",
      "Training loss for batch 3908 : 0.05721161887049675\n",
      "Training loss for batch 3909 : 0.061240892857313156\n",
      "Training loss for batch 3910 : 0.16548845171928406\n",
      "Training loss for batch 3911 : 0.18531706929206848\n",
      "Training loss for batch 3912 : 0.025528928264975548\n",
      "Training loss for batch 3913 : 0.00846150517463684\n",
      "Training loss for batch 3914 : 0.09052934497594833\n",
      "Training loss for batch 3915 : 0.021134676411747932\n",
      "Training loss for batch 3916 : 0.18363730609416962\n",
      "Training loss for batch 3917 : 0.04512261599302292\n",
      "Training loss for batch 3918 : 0.2155056893825531\n",
      "Training loss for batch 3919 : 0.04327789694070816\n",
      "Training loss for batch 3920 : 0.03942094370722771\n",
      "Training loss for batch 3921 : 0.0626039057970047\n",
      "Training loss for batch 3922 : 0.018856868147850037\n",
      "Training loss for batch 3923 : 0.21098420023918152\n",
      "Training loss for batch 3924 : 0.02536057122051716\n",
      "Training loss for batch 3925 : 0.11436107009649277\n",
      "Training loss for batch 3926 : 0.039413657039403915\n",
      "Training loss for batch 3927 : 0.047154948115348816\n",
      "Training loss for batch 3928 : 0.1342737227678299\n",
      "Training loss for batch 3929 : 0.22270739078521729\n",
      "Training loss for batch 3930 : 0.014478515833616257\n",
      "Training loss for batch 3931 : 0.07226826250553131\n",
      "Training loss for batch 3932 : 0.04278261959552765\n",
      "Training loss for batch 3933 : 0.13249880075454712\n",
      "Training loss for batch 3934 : 0.16677196323871613\n",
      "Training loss for batch 3935 : 0.14712399244308472\n",
      "Training loss for batch 3936 : 0.09048943966627121\n",
      "Training loss for batch 3937 : 0.10867303609848022\n",
      "Training loss for batch 3938 : 0.21303832530975342\n",
      "Training loss for batch 3939 : 0.07970337569713593\n",
      "Training loss for batch 3940 : 0.1628822535276413\n",
      "Training loss for batch 3941 : 0.13272756338119507\n",
      "Training loss for batch 3942 : 0.03733520954847336\n",
      "Training loss for batch 3943 : 0.16127286851406097\n",
      "Training loss for batch 3944 : 0.018543994054198265\n",
      "Training loss for batch 3945 : 0.04615788534283638\n",
      "Training loss for batch 3946 : 0.08498146384954453\n",
      "Training loss for batch 3947 : 0.04050053283572197\n",
      "Training loss for batch 3948 : 0.2315712869167328\n",
      "Training loss for batch 3949 : 0.05760347470641136\n",
      "Training loss for batch 3950 : 0.1232587918639183\n",
      "Training loss for batch 3951 : 0.010819097980856895\n",
      "Training loss for batch 3952 : 0.13468310236930847\n",
      "Training loss for batch 3953 : 0.07110218703746796\n",
      "Training loss for batch 3954 : 0.05790543183684349\n",
      "Training loss for batch 3955 : 0.15192219614982605\n",
      "Training loss for batch 3956 : 0.0033940644934773445\n",
      "Training loss for batch 3957 : 0.24779699742794037\n",
      "Training loss for batch 3958 : 0.07054293155670166\n",
      "Training loss for batch 3959 : 0.2108881026506424\n",
      "Training loss for batch 3960 : 0.06328722089529037\n",
      "Training loss for batch 3961 : 0.26354432106018066\n",
      "Training loss for batch 3962 : 0.03355554863810539\n",
      "Training loss for batch 3963 : 0.0853797197341919\n",
      "Training loss for batch 3964 : 0.1311781108379364\n",
      "Training loss for batch 3965 : 0.14129188656806946\n",
      "Training loss for batch 3966 : 0.11321165412664413\n",
      "Training loss for batch 3967 : 0.027272485196590424\n",
      "Training loss for batch 3968 : 4.297025313348968e-08\n",
      "Training loss for batch 3969 : 0.12499617785215378\n",
      "Training loss for batch 3970 : 0.0258163008838892\n",
      "Training loss for batch 3971 : 0.12908510863780975\n",
      "Training loss for batch 3972 : 0.17856086790561676\n",
      "Training loss for batch 3973 : 0.057072557508945465\n",
      "Training loss for batch 3974 : 0.05587005987763405\n",
      "Training loss for batch 3975 : 0.09761150926351547\n",
      "Training loss for batch 3976 : 0.09344404190778732\n",
      "Training loss for batch 3977 : 0.018704351037740707\n",
      "Training loss for batch 3978 : 0.3678208291530609\n",
      "Training loss for batch 3979 : 0.22387659549713135\n",
      "Training loss for batch 3980 : 0.1825304627418518\n",
      "Training loss for batch 3981 : 0.19250862300395966\n",
      "Training loss for batch 3982 : 0.19187381863594055\n",
      "Training loss for batch 3983 : 0.028792450204491615\n",
      "Training loss for batch 3984 : 0.025332752615213394\n",
      "Training loss for batch 3985 : 0.09343883395195007\n",
      "Training loss for batch 3986 : 0.06556635349988937\n",
      "Training loss for batch 3987 : 0.05017988011240959\n",
      "Training loss for batch 3988 : 0.0798858106136322\n",
      "Training loss for batch 3989 : 0.047174934297800064\n",
      "Training loss for batch 3990 : 0.03893061354756355\n",
      "Training loss for batch 3991 : 0.07291921228170395\n",
      "Training loss for batch 3992 : 0.01618357002735138\n",
      "Training loss for batch 3993 : 0.018535209819674492\n",
      "Training loss for batch 3994 : 0.0378577746450901\n",
      "Training loss for batch 3995 : 0.053562890738248825\n",
      "Training loss for batch 3996 : 0.17712385952472687\n",
      "Training loss for batch 3997 : 0.12042935192584991\n",
      "Training loss for batch 3998 : 0.10032739490270615\n",
      "Training loss for batch 3999 : 0.12194385379552841\n",
      "Training loss for batch 4000 : 0.16517174243927002\n",
      "Training loss for batch 4001 : 0.20313504338264465\n",
      "Training loss for batch 4002 : 0.03411749005317688\n",
      "Training loss for batch 4003 : 0.1507991999387741\n",
      "Training loss for batch 4004 : 0.12054348737001419\n",
      "Training loss for batch 4005 : 0.06464692950248718\n",
      "Training loss for batch 4006 : 0.04104664921760559\n",
      "Training loss for batch 4007 : 0.16787812113761902\n",
      "Training loss for batch 4008 : 0.09020192176103592\n",
      "Training loss for batch 4009 : 0.09316711127758026\n",
      "Training loss for batch 4010 : 0.014209621585905552\n",
      "Training loss for batch 4011 : 0.12290188670158386\n",
      "Training loss for batch 4012 : 0.1165674701333046\n",
      "Training loss for batch 4013 : 0.008746584877371788\n",
      "Training loss for batch 4014 : 0.14328885078430176\n",
      "Training loss for batch 4015 : 0.19089728593826294\n",
      "Training loss for batch 4016 : 0.04474402964115143\n",
      "Training loss for batch 4017 : 0.16029924154281616\n",
      "Training loss for batch 4018 : 0.04696251451969147\n",
      "Training loss for batch 4019 : 0.02650631032884121\n",
      "Training loss for batch 4020 : 0.0057891542091965675\n",
      "Training loss for batch 4021 : 0.15867309272289276\n",
      "Training loss for batch 4022 : 0.04419426620006561\n",
      "Training loss for batch 4023 : 0.05917935073375702\n",
      "Training loss for batch 4024 : 0.09054888784885406\n",
      "Training loss for batch 4025 : 0.05238780006766319\n",
      "Training loss for batch 4026 : 0.04406386613845825\n",
      "Training loss for batch 4027 : 0.030527906492352486\n",
      "Training loss for batch 4028 : 0.15259066224098206\n",
      "Training loss for batch 4029 : 0.03145263344049454\n",
      "Training loss for batch 4030 : 0.04984845221042633\n",
      "Training loss for batch 4031 : 0.0586744099855423\n",
      "Training loss for batch 4032 : 0.035330262035131454\n",
      "Training loss for batch 4033 : 0.14476348459720612\n",
      "Training loss for batch 4034 : 0.0975072905421257\n",
      "Training loss for batch 4035 : 0.18469476699829102\n",
      "Training loss for batch 4036 : 0.2521842122077942\n",
      "Training loss for batch 4037 : 0.07897678017616272\n",
      "Training loss for batch 4038 : 0.0827021449804306\n",
      "Training loss for batch 4039 : 0.14521953463554382\n",
      "Training loss for batch 4040 : 0.1722879260778427\n",
      "Training loss for batch 4041 : 0.047375649213790894\n",
      "Training loss for batch 4042 : 0.10246973484754562\n",
      "Training loss for batch 4043 : 0.24909763038158417\n",
      "Training loss for batch 4044 : 0.09027930349111557\n",
      "Training loss for batch 4045 : 0.0596502386033535\n",
      "Training loss for batch 4046 : 0.1521628350019455\n",
      "Training loss for batch 4047 : 0.03367624059319496\n",
      "Training loss for batch 4048 : 0.0153458621352911\n",
      "Training loss for batch 4049 : 0.11297588050365448\n",
      "Training loss for batch 4050 : 6.599840673970903e-08\n",
      "Training loss for batch 4051 : 0.05267004296183586\n",
      "Training loss for batch 4052 : 0.07305966317653656\n",
      "Training loss for batch 4053 : 0.07095768302679062\n",
      "Training loss for batch 4054 : 0.04874107986688614\n",
      "Training loss for batch 4055 : 0.0994754433631897\n",
      "Training loss for batch 4056 : 0.17189288139343262\n",
      "Training loss for batch 4057 : 0.1844571828842163\n",
      "Training loss for batch 4058 : 0.019218655303120613\n",
      "Training loss for batch 4059 : 0.19483371078968048\n",
      "Training loss for batch 4060 : 0.18419083952903748\n",
      "Training loss for batch 4061 : 0.3620811700820923\n",
      "Training loss for batch 4062 : 0.12001506239175797\n",
      "Training loss for batch 4063 : 0.17302969098091125\n",
      "Training loss for batch 4064 : 0.04990405961871147\n",
      "Training loss for batch 4065 : 0.1813974827528\n",
      "Training loss for batch 4066 : 0.12349455803632736\n",
      "Training loss for batch 4067 : 0.020634286105632782\n",
      "Training loss for batch 4068 : 0.1067105084657669\n",
      "Training loss for batch 4069 : 0.13601279258728027\n",
      "Training loss for batch 4070 : 0.2719091475009918\n",
      "Training loss for batch 4071 : 0.1703188270330429\n",
      "Training loss for batch 4072 : 0.060837119817733765\n",
      "Training loss for batch 4073 : 0.11160318553447723\n",
      "Training loss for batch 4074 : 0.19842690229415894\n",
      "Training loss for batch 4075 : 0.0355696901679039\n",
      "Training loss for batch 4076 : 0.1712290495634079\n",
      "Training loss for batch 4077 : 0.1296447366476059\n",
      "Training loss for batch 4078 : 0.08082642406225204\n",
      "Training loss for batch 4079 : 0.040765777230262756\n",
      "Training loss for batch 4080 : 0.12638942897319794\n",
      "Training loss for batch 4081 : 0.048472821712493896\n",
      "Training loss for batch 4082 : 0.12366143614053726\n",
      "Training loss for batch 4083 : 0.018448134884238243\n",
      "Training loss for batch 4084 : 0.11664783209562302\n",
      "Training loss for batch 4085 : 0.18638773262500763\n",
      "Training loss for batch 4086 : 0.05158190429210663\n",
      "Training loss for batch 4087 : 0.027762243524193764\n",
      "Training loss for batch 4088 : 0.041788745671510696\n",
      "Training loss for batch 4089 : 0.06300158053636551\n",
      "Training loss for batch 4090 : 0.21126066148281097\n",
      "Training loss for batch 4091 : 0.04261193796992302\n",
      "Training loss for batch 4092 : 0.08791446685791016\n",
      "Training loss for batch 4093 : 0.20205534994602203\n",
      "Training loss for batch 4094 : 0.054089754819869995\n",
      "Training loss for batch 4095 : 0.2223873734474182\n",
      "Training loss for batch 4096 : 0.0816541463136673\n",
      "Training loss for batch 4097 : 0.09909162670373917\n",
      "Training loss for batch 4098 : 0.060854434967041016\n",
      "Training loss for batch 4099 : 0.036897964775562286\n",
      "Training loss for batch 4100 : 0.01901761069893837\n",
      "Training loss for batch 4101 : 0.08413265645503998\n",
      "Training loss for batch 4102 : 0.10168293118476868\n",
      "Training loss for batch 4103 : 0.08478803187608719\n",
      "Training loss for batch 4104 : 0.19049575924873352\n",
      "Training loss for batch 4105 : 0.030898023396730423\n",
      "Training loss for batch 4106 : 0.033098574727773666\n",
      "Training loss for batch 4107 : 0.12635788321495056\n",
      "Training loss for batch 4108 : 0.1248444989323616\n",
      "Training loss for batch 4109 : 0.053843021392822266\n",
      "Training loss for batch 4110 : 0.06794712692499161\n",
      "Training loss for batch 4111 : 0.033627793192863464\n",
      "Training loss for batch 4112 : 0.16728077828884125\n",
      "Training loss for batch 4113 : 0.12159240245819092\n",
      "Training loss for batch 4114 : 0.10514809936285019\n",
      "Training loss for batch 4115 : 0.08896228671073914\n",
      "Training loss for batch 4116 : 0.05465986952185631\n",
      "Training loss for batch 4117 : 0.054375749081373215\n",
      "Training loss for batch 4118 : 0.10312164574861526\n",
      "Training loss for batch 4119 : 0.0417938157916069\n",
      "Training loss for batch 4120 : 0.07039397209882736\n",
      "Training loss for batch 4121 : 0.20519016683101654\n",
      "Training loss for batch 4122 : 0.08477949351072311\n",
      "Training loss for batch 4123 : 0.03424821421504021\n",
      "Training loss for batch 4124 : 0.17503619194030762\n",
      "Training loss for batch 4125 : 0.11141219735145569\n",
      "Training loss for batch 4126 : 0.12928152084350586\n",
      "Training loss for batch 4127 : 0.06385154277086258\n",
      "Training loss for batch 4128 : 0.1229032650589943\n",
      "Training loss for batch 4129 : 0.13989189267158508\n",
      "Training loss for batch 4130 : 0.11614492535591125\n",
      "Training loss for batch 4131 : 0.16769200563430786\n",
      "Training loss for batch 4132 : 0.008810169994831085\n",
      "Training loss for batch 4133 : 0.01414579525589943\n",
      "Training loss for batch 4134 : 0.035678379237651825\n",
      "Training loss for batch 4135 : 0.06961590051651001\n",
      "Training loss for batch 4136 : 0.20858679711818695\n",
      "Training loss for batch 4137 : 0.05817587301135063\n",
      "Training loss for batch 4138 : 0.0016566995764151216\n",
      "Training loss for batch 4139 : 0.10039893537759781\n",
      "Training loss for batch 4140 : 0.12210075557231903\n",
      "Training loss for batch 4141 : 0.030035924166440964\n",
      "Training loss for batch 4142 : 0.06412677466869354\n",
      "Training loss for batch 4143 : 0.24001099169254303\n",
      "Training loss for batch 4144 : 0.012607992626726627\n",
      "Training loss for batch 4145 : 0.06833069771528244\n",
      "Training loss for batch 4146 : 0.01156648900359869\n",
      "Training loss for batch 4147 : 0.05136120691895485\n",
      "Training loss for batch 4148 : 0.12075743824243546\n",
      "Training loss for batch 4149 : 0.16054262220859528\n",
      "Training loss for batch 4150 : 0.05447394400835037\n",
      "Training loss for batch 4151 : 0.08082731813192368\n",
      "Training loss for batch 4152 : 0.0489315502345562\n",
      "Training loss for batch 4153 : 0.06194319948554039\n",
      "Parameter containing:\n",
      "tensor(-0.2614, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.0000100135803223\n",
      "Training loss for batch 1 : 1.0000100135803223\n",
      "Training loss for batch 2 : 1.0000098943710327\n",
      "Training loss for batch 3 : 1.0000098943710327\n",
      "Training loss for batch 4 : 1.0000100135803223\n",
      "Training loss for batch 5 : 1.0000100135803223\n",
      "Training loss for batch 6 : 1.0000100135803223\n",
      "Training loss for batch 7 : 1.0000098943710327\n",
      "Training loss for batch 8 : 1.0000100135803223\n",
      "Training loss for batch 9 : 1.0000100135803223\n",
      "Training loss for batch 10 : 1.0000100135803223\n",
      "Training loss for batch 11 : 1.0000100135803223\n",
      "Training loss for batch 12 : 0.08317359536886215\n",
      "Training loss for batch 13 : 0.0007919917698018253\n",
      "Training loss for batch 14 : 0.06152559444308281\n",
      "Training loss for batch 15 : 0.015527511946856976\n",
      "Training loss for batch 16 : 0.1433417797088623\n",
      "Training loss for batch 17 : 0.016956215724349022\n",
      "Training loss for batch 18 : 0.07564503699541092\n",
      "Training loss for batch 19 : 0.022613465785980225\n",
      "Training loss for batch 20 : 0.01839672401547432\n",
      "Training loss for batch 21 : 0.013469705358147621\n",
      "Training loss for batch 22 : 0.01568783074617386\n",
      "Training loss for batch 23 : 0.10975243896245956\n",
      "Training loss for batch 24 : 0.09920462965965271\n",
      "Training loss for batch 25 : 0.02903762087225914\n",
      "Training loss for batch 26 : 0.07291297614574432\n",
      "Training loss for batch 27 : 0.02551763691008091\n",
      "Training loss for batch 28 : 0.17690643668174744\n",
      "Training loss for batch 29 : 0.08001674711704254\n",
      "Training loss for batch 30 : 0.09623740613460541\n",
      "Training loss for batch 31 : 0.07231417298316956\n",
      "Training loss for batch 32 : 0.02605610340833664\n",
      "Training loss for batch 33 : 0.09308401495218277\n",
      "Training loss for batch 34 : 0.031382810324430466\n",
      "Training loss for batch 35 : 0.0762457400560379\n",
      "Training loss for batch 36 : 0.1429549604654312\n",
      "Training loss for batch 37 : 0.04961516708135605\n",
      "Training loss for batch 38 : 0.16510964930057526\n",
      "Training loss for batch 39 : 0.12260590493679047\n",
      "Training loss for batch 40 : 0.08281570672988892\n",
      "Training loss for batch 41 : 0.006136365234851837\n",
      "Training loss for batch 42 : 0.2220619022846222\n",
      "Training loss for batch 43 : 0.12511245906352997\n",
      "Training loss for batch 44 : 0.21698881685733795\n",
      "Training loss for batch 45 : 0.02442336641252041\n",
      "Training loss for batch 46 : 0.051171112805604935\n",
      "Training loss for batch 47 : 0.02158825471997261\n",
      "Training loss for batch 48 : 0.021043969318270683\n",
      "Training loss for batch 49 : 0.04050878807902336\n",
      "Training loss for batch 50 : 0.0561165027320385\n",
      "Training loss for batch 51 : 0.0400036945939064\n",
      "Training loss for batch 52 : 0.07267920672893524\n",
      "Training loss for batch 53 : 0.09631634503602982\n",
      "Training loss for batch 54 : 0.03337855264544487\n",
      "Training loss for batch 55 : 0.08489172905683517\n",
      "Training loss for batch 56 : 0.02045949175953865\n",
      "Training loss for batch 57 : 0.0022974959574639797\n",
      "Training loss for batch 58 : 0.005527692846953869\n",
      "Training loss for batch 59 : 0.058922555297613144\n",
      "Training loss for batch 60 : 0.04849264398217201\n",
      "Training loss for batch 61 : 0.06095871329307556\n",
      "Training loss for batch 62 : 0.04193153232336044\n",
      "Training loss for batch 63 : 0.07976279407739639\n",
      "Training loss for batch 64 : 0.0021337387152016163\n",
      "Training loss for batch 65 : 0.013490527868270874\n",
      "Training loss for batch 66 : 0.03132602199912071\n",
      "Training loss for batch 67 : 0.07053404301404953\n",
      "Training loss for batch 68 : 0.03530163690447807\n",
      "Training loss for batch 69 : 0.06180040165781975\n",
      "Training loss for batch 70 : 0.09360044449567795\n",
      "Training loss for batch 71 : 0.07538118958473206\n",
      "Training loss for batch 72 : 0.05462755262851715\n",
      "Training loss for batch 73 : 0.03625361621379852\n",
      "Training loss for batch 74 : 0.04199213534593582\n",
      "Training loss for batch 75 : 0.026078738272190094\n",
      "Training loss for batch 76 : 0.029512695968151093\n",
      "Training loss for batch 77 : 0.07724575698375702\n",
      "Training loss for batch 78 : 0.07283716648817062\n",
      "Training loss for batch 79 : 0.14354237914085388\n",
      "Training loss for batch 80 : 0.018007084727287292\n",
      "Training loss for batch 81 : 0.016998566687107086\n",
      "Training loss for batch 82 : 0.10571714490652084\n",
      "Training loss for batch 83 : 0.14725102484226227\n",
      "Training loss for batch 84 : 0.05142655968666077\n",
      "Training loss for batch 85 : 0.08813916891813278\n",
      "Training loss for batch 86 : 0.17166033387184143\n",
      "Training loss for batch 87 : 0.017062265425920486\n",
      "Training loss for batch 88 : 0.08146502077579498\n",
      "Training loss for batch 89 : 0.09528279304504395\n",
      "Training loss for batch 90 : 0.11821626126766205\n",
      "Training loss for batch 91 : 0.08281949907541275\n",
      "Training loss for batch 92 : 0.040816135704517365\n",
      "Training loss for batch 93 : 0.03797106817364693\n",
      "Training loss for batch 94 : 0.007921756245195866\n",
      "Training loss for batch 95 : 0.0009020402794703841\n",
      "Training loss for batch 96 : 0.037632428109645844\n",
      "Training loss for batch 97 : 0.15026149153709412\n",
      "Training loss for batch 98 : 0.07974530756473541\n",
      "Training loss for batch 99 : 0.023960387334227562\n",
      "Training loss for batch 100 : 5.981061690363276e-08\n",
      "Training loss for batch 101 : 0.007408848498016596\n",
      "Training loss for batch 102 : 0.031664032489061356\n",
      "Training loss for batch 103 : 0.11529139429330826\n",
      "Training loss for batch 104 : 0.03605056554079056\n",
      "Training loss for batch 105 : 0.08379141986370087\n",
      "Training loss for batch 106 : 0.05381687358021736\n",
      "Training loss for batch 107 : 0.03995335474610329\n",
      "Training loss for batch 108 : 0.18684327602386475\n",
      "Training loss for batch 109 : 0.0589788481593132\n",
      "Training loss for batch 110 : 0.07596032321453094\n",
      "Training loss for batch 111 : 4.661510999426355e-08\n",
      "Training loss for batch 112 : 0.07226637005805969\n",
      "Training loss for batch 113 : 0.09068956226110458\n",
      "Training loss for batch 114 : 0.05747818201780319\n",
      "Training loss for batch 115 : 0.013768259435892105\n",
      "Training loss for batch 116 : 0.04069191589951515\n",
      "Training loss for batch 117 : 0.028405306860804558\n",
      "Training loss for batch 118 : 0.16126449406147003\n",
      "Training loss for batch 119 : 0.01414105761796236\n",
      "Training loss for batch 120 : 0.00628427742049098\n",
      "Training loss for batch 121 : 0.022867726162075996\n",
      "Training loss for batch 122 : 0.1300089806318283\n",
      "Training loss for batch 123 : 0.09131140261888504\n",
      "Training loss for batch 124 : 0.07181873917579651\n",
      "Training loss for batch 125 : 0.05052937567234039\n",
      "Training loss for batch 126 : 0.01458489615470171\n",
      "Training loss for batch 127 : 0.01381704956293106\n",
      "Training loss for batch 128 : 0.02645339071750641\n",
      "Training loss for batch 129 : 0.047314491122961044\n",
      "Training loss for batch 130 : 0.15691298246383667\n",
      "Training loss for batch 131 : 0.09680210053920746\n",
      "Training loss for batch 132 : 0.007789356168359518\n",
      "Training loss for batch 133 : 0.027188420295715332\n",
      "Training loss for batch 134 : 0.15543648600578308\n",
      "Training loss for batch 135 : 0.1381155252456665\n",
      "Training loss for batch 136 : 0.023377101868391037\n",
      "Training loss for batch 137 : 0.10578317195177078\n",
      "Training loss for batch 138 : 0.07120417803525925\n",
      "Training loss for batch 139 : 0.11526460945606232\n",
      "Training loss for batch 140 : 0.1651468575000763\n",
      "Training loss for batch 141 : 0.06967137008905411\n",
      "Training loss for batch 142 : 0.05818936601281166\n",
      "Training loss for batch 143 : 0.027057409286499023\n",
      "Training loss for batch 144 : 6.98099245077799e-10\n",
      "Training loss for batch 145 : 0.02638358436524868\n",
      "Training loss for batch 146 : 0.012421863153576851\n",
      "Training loss for batch 147 : 0.02711915411055088\n",
      "Training loss for batch 148 : 0.08056358993053436\n",
      "Training loss for batch 149 : 0.03237989544868469\n",
      "Training loss for batch 150 : 0.14263859391212463\n",
      "Training loss for batch 151 : 0.01822783425450325\n",
      "Training loss for batch 152 : 0.043188851326704025\n",
      "Training loss for batch 153 : 0.046917419880628586\n",
      "Training loss for batch 154 : 0.08941879123449326\n",
      "Training loss for batch 155 : 0.07368820160627365\n",
      "Training loss for batch 156 : 0.019478050991892815\n",
      "Training loss for batch 157 : 0.05556482449173927\n",
      "Training loss for batch 158 : 0.08496702462434769\n",
      "Training loss for batch 159 : 0.049491818994283676\n",
      "Training loss for batch 160 : 0.08872228860855103\n",
      "Training loss for batch 161 : 0.16191118955612183\n",
      "Training loss for batch 162 : 0.039452504366636276\n",
      "Training loss for batch 163 : 0.0965232402086258\n",
      "Training loss for batch 164 : 0.12929408252239227\n",
      "Training loss for batch 165 : 0.07872448861598969\n",
      "Training loss for batch 166 : 0.05585147812962532\n",
      "Training loss for batch 167 : 0.10046983510255814\n",
      "Training loss for batch 168 : 0.01762501336634159\n",
      "Training loss for batch 169 : 0.1256590187549591\n",
      "Training loss for batch 170 : 0.06560028344392776\n",
      "Training loss for batch 171 : 0.13534297049045563\n",
      "Training loss for batch 172 : 0.11080995947122574\n",
      "Training loss for batch 173 : 0.038567546755075455\n",
      "Training loss for batch 174 : 0.07031216472387314\n",
      "Training loss for batch 175 : 0.07971493899822235\n",
      "Training loss for batch 176 : 0.10114315152168274\n",
      "Training loss for batch 177 : 0.0010118186473846436\n",
      "Training loss for batch 178 : 0.013226831331849098\n",
      "Training loss for batch 179 : 0.1328068971633911\n",
      "Training loss for batch 180 : 0.09054791927337646\n",
      "Training loss for batch 181 : 0.08511295169591904\n",
      "Training loss for batch 182 : 0.06058298051357269\n",
      "Training loss for batch 183 : 0.07249940931797028\n",
      "Training loss for batch 184 : 0.11981718987226486\n",
      "Training loss for batch 185 : 0.04507854953408241\n",
      "Training loss for batch 186 : 0.029394596815109253\n",
      "Training loss for batch 187 : 0.07898019999265671\n",
      "Training loss for batch 188 : 0.1723226010799408\n",
      "Training loss for batch 189 : 0.06358931213617325\n",
      "Training loss for batch 190 : 0.20489606261253357\n",
      "Training loss for batch 191 : 0.061157021671533585\n",
      "Training loss for batch 192 : 0.08386753499507904\n",
      "Training loss for batch 193 : 0.13879260420799255\n",
      "Training loss for batch 194 : 0.14803409576416016\n",
      "Training loss for batch 195 : 0.17176252603530884\n",
      "Training loss for batch 196 : 0.06644455343484879\n",
      "Training loss for batch 197 : 0.03659185767173767\n",
      "Training loss for batch 198 : 0.014999216422438622\n",
      "Training loss for batch 199 : 0.060455936938524246\n",
      "Training loss for batch 200 : 0.011742664501070976\n",
      "Training loss for batch 201 : 0.14039723575115204\n",
      "Training loss for batch 202 : 0.03454121947288513\n",
      "Training loss for batch 203 : 0.010283201932907104\n",
      "Training loss for batch 204 : 0.14710713922977448\n",
      "Training loss for batch 205 : 0.015520187094807625\n",
      "Training loss for batch 206 : 0.0502510741353035\n",
      "Training loss for batch 207 : 0.009971823543310165\n",
      "Training loss for batch 208 : 0.04789658635854721\n",
      "Training loss for batch 209 : 0.15974360704421997\n",
      "Training loss for batch 210 : 0.10510639101266861\n",
      "Training loss for batch 211 : 0.17550699412822723\n",
      "Training loss for batch 212 : 0.03673632815480232\n",
      "Training loss for batch 213 : 0.08645620942115784\n",
      "Training loss for batch 214 : 0.11348088830709457\n",
      "Training loss for batch 215 : 0.06422099471092224\n",
      "Training loss for batch 216 : 0.033870965242385864\n",
      "Training loss for batch 217 : 0.0011588067281991243\n",
      "Training loss for batch 218 : 0.10996300727128983\n",
      "Training loss for batch 219 : 0.03764333948493004\n",
      "Training loss for batch 220 : 0.11641634255647659\n",
      "Training loss for batch 221 : 0.05706941708922386\n",
      "Training loss for batch 222 : 0.05349492281675339\n",
      "Training loss for batch 223 : 0.027637403458356857\n",
      "Training loss for batch 224 : 0.16204699873924255\n",
      "Training loss for batch 225 : 0.03033514693379402\n",
      "Training loss for batch 226 : 0.1041746735572815\n",
      "Training loss for batch 227 : 0.025069214403629303\n",
      "Training loss for batch 228 : 0.018197862431406975\n",
      "Training loss for batch 229 : 0.16622599959373474\n",
      "Training loss for batch 230 : 0.05339645966887474\n",
      "Training loss for batch 231 : 0.06058185175061226\n",
      "Training loss for batch 232 : 0.0352424681186676\n",
      "Training loss for batch 233 : 0.05355628952383995\n",
      "Training loss for batch 234 : 0.14621227979660034\n",
      "Training loss for batch 235 : 0.08988775312900543\n",
      "Training loss for batch 236 : 0.060547418892383575\n",
      "Training loss for batch 237 : 0.07135000824928284\n",
      "Training loss for batch 238 : 0.09153201431035995\n",
      "Training loss for batch 239 : 0.14270824193954468\n",
      "Training loss for batch 240 : 0.052250418812036514\n",
      "Training loss for batch 241 : 0.056897953152656555\n",
      "Training loss for batch 242 : 0.03889881819486618\n",
      "Training loss for batch 243 : 0.047391489148139954\n",
      "Training loss for batch 244 : 0.08792611956596375\n",
      "Training loss for batch 245 : 0.08340290933847427\n",
      "Training loss for batch 246 : 0.07834891974925995\n",
      "Training loss for batch 247 : 0.12990310788154602\n",
      "Training loss for batch 248 : 0.013532129116356373\n",
      "Training loss for batch 249 : 0.020379869267344475\n",
      "Training loss for batch 250 : 0.1232372596859932\n",
      "Training loss for batch 251 : 0.05608966946601868\n",
      "Training loss for batch 252 : 0.051703792065382004\n",
      "Training loss for batch 253 : 0.02530301734805107\n",
      "Training loss for batch 254 : 0.08874982595443726\n",
      "Training loss for batch 255 : 0.08015961945056915\n",
      "Training loss for batch 256 : 0.022414790466427803\n",
      "Training loss for batch 257 : 0.2053365260362625\n",
      "Training loss for batch 258 : 0.10019498318433762\n",
      "Training loss for batch 259 : 0.0009408643236383796\n",
      "Training loss for batch 260 : 0.019135335460305214\n",
      "Training loss for batch 261 : 0.14688557386398315\n",
      "Training loss for batch 262 : 0.006020173896104097\n",
      "Training loss for batch 263 : 0.0639234185218811\n",
      "Training loss for batch 264 : 0.00577876903116703\n",
      "Training loss for batch 265 : 0.020651187747716904\n",
      "Training loss for batch 266 : 0.130670428276062\n",
      "Training loss for batch 267 : 0.05740054324269295\n",
      "Training loss for batch 268 : 0.07694283872842789\n",
      "Training loss for batch 269 : 0.07543673366308212\n",
      "Training loss for batch 270 : 0.07910172641277313\n",
      "Training loss for batch 271 : 0.11562594026327133\n",
      "Training loss for batch 272 : 0.0403330996632576\n",
      "Training loss for batch 273 : 0.08035819977521896\n",
      "Training loss for batch 274 : 0.07952176779508591\n",
      "Training loss for batch 275 : 0.012143392115831375\n",
      "Training loss for batch 276 : 0.011996620334684849\n",
      "Training loss for batch 277 : 0.03779680281877518\n",
      "Training loss for batch 278 : 0.13046588003635406\n",
      "Training loss for batch 279 : 0.04644298180937767\n",
      "Training loss for batch 280 : 0.10771162807941437\n",
      "Training loss for batch 281 : 0.0297468900680542\n",
      "Training loss for batch 282 : 0.07681269943714142\n",
      "Training loss for batch 283 : 0.1819976568222046\n",
      "Training loss for batch 284 : 0.06467287987470627\n",
      "Training loss for batch 285 : 0.13296784460544586\n",
      "Training loss for batch 286 : 0.12422741204500198\n",
      "Training loss for batch 287 : 0.09228143095970154\n",
      "Training loss for batch 288 : 0.05091729760169983\n",
      "Training loss for batch 289 : 0.10631442815065384\n",
      "Training loss for batch 290 : 0.07479061931371689\n",
      "Training loss for batch 291 : 0.09150753915309906\n",
      "Training loss for batch 292 : 0.09932620078325272\n",
      "Training loss for batch 293 : 0.002263782313093543\n",
      "Training loss for batch 294 : 0.053211238235235214\n",
      "Training loss for batch 295 : 0.034799542278051376\n",
      "Training loss for batch 296 : 0.12054989486932755\n",
      "Training loss for batch 297 : 0.04504229128360748\n",
      "Training loss for batch 298 : 0.1207432895898819\n",
      "Training loss for batch 299 : 0.020643984898924828\n",
      "Training loss for batch 300 : 0.14592871069908142\n",
      "Training loss for batch 301 : 0.12645410001277924\n",
      "Training loss for batch 302 : 0.11216982454061508\n",
      "Training loss for batch 303 : 0.1500207781791687\n",
      "Training loss for batch 304 : 0.05147985368967056\n",
      "Training loss for batch 305 : 0.06922659277915955\n",
      "Training loss for batch 306 : 0.04486525058746338\n",
      "Training loss for batch 307 : 0.014869295060634613\n",
      "Training loss for batch 308 : 0.09857315570116043\n",
      "Training loss for batch 309 : 0.0629076212644577\n",
      "Training loss for batch 310 : 0.054933883249759674\n",
      "Training loss for batch 311 : 0.01937900111079216\n",
      "Training loss for batch 312 : 0.018732717260718346\n",
      "Training loss for batch 313 : 0.02389933541417122\n",
      "Training loss for batch 314 : 0.006416465155780315\n",
      "Training loss for batch 315 : 0.04386074095964432\n",
      "Training loss for batch 316 : 0.04913853481411934\n",
      "Training loss for batch 317 : 0.10699588060379028\n",
      "Training loss for batch 318 : 0.08314415067434311\n",
      "Training loss for batch 319 : 0.0859900414943695\n",
      "Training loss for batch 320 : 0.12489230185747147\n",
      "Training loss for batch 321 : 0.06448366492986679\n",
      "Training loss for batch 322 : 0.05682429298758507\n",
      "Training loss for batch 323 : 0.043668169528245926\n",
      "Training loss for batch 324 : 0.04125993326306343\n",
      "Training loss for batch 325 : 0.07578571140766144\n",
      "Training loss for batch 326 : 0.05232518911361694\n",
      "Training loss for batch 327 : 0.09841795265674591\n",
      "Training loss for batch 328 : 0.021979548037052155\n",
      "Training loss for batch 329 : 0.08568418771028519\n",
      "Training loss for batch 330 : 0.036967117339372635\n",
      "Training loss for batch 331 : 0.15613503754138947\n",
      "Training loss for batch 332 : 0.02926463633775711\n",
      "Training loss for batch 333 : 0.009336589835584164\n",
      "Training loss for batch 334 : 0.07872248440980911\n",
      "Training loss for batch 335 : 0.041407790035009384\n",
      "Training loss for batch 336 : 0.15376164019107819\n",
      "Training loss for batch 337 : 0.110775887966156\n",
      "Training loss for batch 338 : 0.011623295955359936\n",
      "Training loss for batch 339 : 0.060426585376262665\n",
      "Training loss for batch 340 : 0.009265427477657795\n",
      "Training loss for batch 341 : 0.09565756469964981\n",
      "Training loss for batch 342 : 0.029384594410657883\n",
      "Training loss for batch 343 : 0.1543867588043213\n",
      "Training loss for batch 344 : 0.010077662765979767\n",
      "Training loss for batch 345 : 0.057490572333335876\n",
      "Training loss for batch 346 : 0.004020721185952425\n",
      "Training loss for batch 347 : 0.05925288423895836\n",
      "Training loss for batch 348 : 0.042431581765413284\n",
      "Training loss for batch 349 : 0.1345987468957901\n",
      "Training loss for batch 350 : 0.07240001857280731\n",
      "Training loss for batch 351 : 0.038155484944581985\n",
      "Training loss for batch 352 : 0.136030375957489\n",
      "Training loss for batch 353 : 0.11472027003765106\n",
      "Training loss for batch 354 : 0.04142427816987038\n",
      "Training loss for batch 355 : 0.01512474101036787\n",
      "Training loss for batch 356 : 0.0020773163996636868\n",
      "Training loss for batch 357 : 0.0817972794175148\n",
      "Training loss for batch 358 : 0.0471765473484993\n",
      "Training loss for batch 359 : 0.12097723037004471\n",
      "Training loss for batch 360 : 0.07281874865293503\n",
      "Training loss for batch 361 : 0.056004855781793594\n",
      "Training loss for batch 362 : 0.08364124596118927\n",
      "Training loss for batch 363 : 0.022182362154126167\n",
      "Training loss for batch 364 : 0.04403280094265938\n",
      "Training loss for batch 365 : 0.028503717854619026\n",
      "Training loss for batch 366 : 0.08386438339948654\n",
      "Training loss for batch 367 : 0.08157837390899658\n",
      "Training loss for batch 368 : 0.029799744486808777\n",
      "Training loss for batch 369 : 0.05108685791492462\n",
      "Training loss for batch 370 : 0.06253016740083694\n",
      "Training loss for batch 371 : 0.04127771407365799\n",
      "Training loss for batch 372 : 0.012952228076756\n",
      "Training loss for batch 373 : 0.09649018198251724\n",
      "Training loss for batch 374 : 0.09161499887704849\n",
      "Training loss for batch 375 : 0.08051534742116928\n",
      "Training loss for batch 376 : 0.0433264896273613\n",
      "Training loss for batch 377 : 0.05463099479675293\n",
      "Training loss for batch 378 : 0.031101247295737267\n",
      "Training loss for batch 379 : 0.08015280961990356\n",
      "Training loss for batch 380 : 0.042245782911777496\n",
      "Training loss for batch 381 : 0.015210751444101334\n",
      "Training loss for batch 382 : 0.012899110093712807\n",
      "Training loss for batch 383 : 0.1648956835269928\n",
      "Training loss for batch 384 : 0.03314073756337166\n",
      "Training loss for batch 385 : 0.013411497697234154\n",
      "Training loss for batch 386 : 0.20733189582824707\n",
      "Training loss for batch 387 : 0.05259724706411362\n",
      "Training loss for batch 388 : 0.020183568820357323\n",
      "Training loss for batch 389 : 0.03323161602020264\n",
      "Training loss for batch 390 : 0.028161851689219475\n",
      "Training loss for batch 391 : 0.10818155854940414\n",
      "Training loss for batch 392 : 0.03117194212973118\n",
      "Training loss for batch 393 : 0.062038637697696686\n",
      "Training loss for batch 394 : 0.0640435740351677\n",
      "Training loss for batch 395 : 0.06844346225261688\n",
      "Training loss for batch 396 : 0.04688018932938576\n",
      "Training loss for batch 397 : 0.10175436735153198\n",
      "Training loss for batch 398 : 0.12398141622543335\n",
      "Training loss for batch 399 : 0.12191543728113174\n",
      "Training loss for batch 400 : 0.138616681098938\n",
      "Training loss for batch 401 : 0.11821098625659943\n",
      "Training loss for batch 402 : 0.026501154527068138\n",
      "Training loss for batch 403 : 0.10156698524951935\n",
      "Training loss for batch 404 : 0.04142940416932106\n",
      "Training loss for batch 405 : 0.02240336872637272\n",
      "Training loss for batch 406 : 0.07411137223243713\n",
      "Training loss for batch 407 : 0.0830373764038086\n",
      "Training loss for batch 408 : 0.0504896342754364\n",
      "Training loss for batch 409 : 0.01565444841980934\n",
      "Training loss for batch 410 : 0.0068872240372002125\n",
      "Training loss for batch 411 : 0.17442457377910614\n",
      "Training loss for batch 412 : 0.04090236499905586\n",
      "Training loss for batch 413 : 0.032946523278951645\n",
      "Training loss for batch 414 : 0.14433234930038452\n",
      "Training loss for batch 415 : 0.10772071778774261\n",
      "Training loss for batch 416 : 0.055569689720869064\n",
      "Training loss for batch 417 : 0.011043709702789783\n",
      "Training loss for batch 418 : 0.011151253245770931\n",
      "Training loss for batch 419 : 0.07650519162416458\n",
      "Training loss for batch 420 : 0.0032199621200561523\n",
      "Training loss for batch 421 : 0.022019842639565468\n",
      "Training loss for batch 422 : 0.08910709619522095\n",
      "Training loss for batch 423 : 0.05720726400613785\n",
      "Training loss for batch 424 : 0.13191723823547363\n",
      "Training loss for batch 425 : 0.09338875114917755\n",
      "Training loss for batch 426 : 0.0661383792757988\n",
      "Training loss for batch 427 : 0.016713734716176987\n",
      "Training loss for batch 428 : 0.03124585561454296\n",
      "Training loss for batch 429 : 0.04995788261294365\n",
      "Training loss for batch 430 : 0.04299738258123398\n",
      "Training loss for batch 431 : 0.017313461750745773\n",
      "Training loss for batch 432 : 0.016494637355208397\n",
      "Training loss for batch 433 : 0.14507997035980225\n",
      "Training loss for batch 434 : 0.018109485507011414\n",
      "Training loss for batch 435 : 0.053334712982177734\n",
      "Training loss for batch 436 : 0.17228963971138\n",
      "Training loss for batch 437 : 0.009735620580613613\n",
      "Training loss for batch 438 : 0.01966654509305954\n",
      "Training loss for batch 439 : 0.02779191918671131\n",
      "Training loss for batch 440 : 0.1447255164384842\n",
      "Training loss for batch 441 : 0.03695770353078842\n",
      "Training loss for batch 442 : 0.04645126685500145\n",
      "Training loss for batch 443 : 0.0580834299325943\n",
      "Training loss for batch 444 : 0.0748613253235817\n",
      "Training loss for batch 445 : 0.008864280767738819\n",
      "Training loss for batch 446 : 0.07756680995225906\n",
      "Training loss for batch 447 : 0.1374901533126831\n",
      "Training loss for batch 448 : 0.02996320277452469\n",
      "Training loss for batch 449 : 0.03588727116584778\n",
      "Training loss for batch 450 : 0.05507349222898483\n",
      "Training loss for batch 451 : 0.033165376633405685\n",
      "Training loss for batch 452 : 0.11245544999837875\n",
      "Training loss for batch 453 : 0.10383516550064087\n",
      "Training loss for batch 454 : 0.02604449726641178\n",
      "Training loss for batch 455 : 0.048989187926054\n",
      "Training loss for batch 456 : 0.12910571694374084\n",
      "Training loss for batch 457 : 0.10540159046649933\n",
      "Training loss for batch 458 : 0.00451078824698925\n",
      "Training loss for batch 459 : 0.12073327600955963\n",
      "Training loss for batch 460 : 0.050184596329927444\n",
      "Training loss for batch 461 : 0.1633184850215912\n",
      "Training loss for batch 462 : 0.18062560260295868\n",
      "Training loss for batch 463 : 0.03010604716837406\n",
      "Training loss for batch 464 : 0.011249810457229614\n",
      "Training loss for batch 465 : 0.1264505386352539\n",
      "Training loss for batch 466 : 0.08528666943311691\n",
      "Training loss for batch 467 : 0.05930629372596741\n",
      "Training loss for batch 468 : 0.0859362781047821\n",
      "Training loss for batch 469 : 0.09715303778648376\n",
      "Training loss for batch 470 : 0.09432059526443481\n",
      "Training loss for batch 471 : 0.06161896884441376\n",
      "Training loss for batch 472 : 0.04543808847665787\n",
      "Training loss for batch 473 : 0.03050670400261879\n",
      "Training loss for batch 474 : 0.0324765220284462\n",
      "Training loss for batch 475 : 0.06867581605911255\n",
      "Training loss for batch 476 : 0.10538338124752045\n",
      "Training loss for batch 477 : 0.10353965312242508\n",
      "Training loss for batch 478 : 0.0671539232134819\n",
      "Training loss for batch 479 : 0.01633681356906891\n",
      "Training loss for batch 480 : 0.021114833652973175\n",
      "Training loss for batch 481 : 0.030082374811172485\n",
      "Training loss for batch 482 : 0.029327457770705223\n",
      "Training loss for batch 483 : 0.018671510741114616\n",
      "Training loss for batch 484 : 0.0529039166867733\n",
      "Training loss for batch 485 : 0.06838411837816238\n",
      "Training loss for batch 486 : 0.07604050636291504\n",
      "Training loss for batch 487 : 0.09068454802036285\n",
      "Training loss for batch 488 : 0.10183804482221603\n",
      "Training loss for batch 489 : 0.06834094226360321\n",
      "Training loss for batch 490 : 0.006614039186388254\n",
      "Training loss for batch 491 : 0.10452992469072342\n",
      "Training loss for batch 492 : 0.07973679155111313\n",
      "Training loss for batch 493 : 0.03470289334654808\n",
      "Training loss for batch 494 : 0.10248364508152008\n",
      "Training loss for batch 495 : 0.03386338800191879\n",
      "Training loss for batch 496 : 0.05482769384980202\n",
      "Training loss for batch 497 : 0.034198254346847534\n",
      "Training loss for batch 498 : 0.23588672280311584\n",
      "Training loss for batch 499 : 0.09284751117229462\n",
      "Training loss for batch 500 : 0.06308730691671371\n",
      "Training loss for batch 501 : 0.023848464712500572\n",
      "Training loss for batch 502 : 0.025458477437496185\n",
      "Training loss for batch 503 : 0.07210174202919006\n",
      "Training loss for batch 504 : 0.0006344914436340332\n",
      "Training loss for batch 505 : 0.009924452751874924\n",
      "Training loss for batch 506 : 0.00041775405406951904\n",
      "Training loss for batch 507 : 0.03090132214128971\n",
      "Training loss for batch 508 : 0.18690651655197144\n",
      "Training loss for batch 509 : 0.03136564791202545\n",
      "Training loss for batch 510 : 0.1507265567779541\n",
      "Training loss for batch 511 : 0.06634809821844101\n",
      "Training loss for batch 512 : 0.08880110830068588\n",
      "Training loss for batch 513 : 0.0676356703042984\n",
      "Training loss for batch 514 : 0.003481989959254861\n",
      "Training loss for batch 515 : 0.03663043677806854\n",
      "Training loss for batch 516 : 0.006516561843454838\n",
      "Training loss for batch 517 : 0.06841383129358292\n",
      "Training loss for batch 518 : 0.08347208052873611\n",
      "Training loss for batch 519 : 0.12565281987190247\n",
      "Training loss for batch 520 : 0.07148413360118866\n",
      "Training loss for batch 521 : 0.0689784362912178\n",
      "Training loss for batch 522 : 0.002414051676169038\n",
      "Training loss for batch 523 : 0.10211838781833649\n",
      "Training loss for batch 524 : 0.08303889632225037\n",
      "Training loss for batch 525 : 0.0018154780846089125\n",
      "Training loss for batch 526 : 0.027266422286629677\n",
      "Training loss for batch 527 : 0.19484452903270721\n",
      "Training loss for batch 528 : 0.04954191669821739\n",
      "Training loss for batch 529 : 0.051624953746795654\n",
      "Training loss for batch 530 : 0.09284837543964386\n",
      "Training loss for batch 531 : 0.12933002412319183\n",
      "Training loss for batch 532 : 0.02955525927245617\n",
      "Training loss for batch 533 : 0.23545436561107635\n",
      "Training loss for batch 534 : 0.004662004765123129\n",
      "Training loss for batch 535 : 0.034838248044252396\n",
      "Training loss for batch 536 : 0.040124427527189255\n",
      "Training loss for batch 537 : 0.23376677930355072\n",
      "Training loss for batch 538 : 0.10346630960702896\n",
      "Training loss for batch 539 : 0.16824263334274292\n",
      "Training loss for batch 540 : 0.07142052054405212\n",
      "Training loss for batch 541 : 0.04299077391624451\n",
      "Training loss for batch 542 : 0.08304335176944733\n",
      "Training loss for batch 543 : 0.14235250651836395\n",
      "Training loss for batch 544 : 0.06277400255203247\n",
      "Training loss for batch 545 : 0.0653872936964035\n",
      "Training loss for batch 546 : 0.13256381452083588\n",
      "Training loss for batch 547 : 0.12527190148830414\n",
      "Training loss for batch 548 : 0.07720723003149033\n",
      "Training loss for batch 549 : 0.05455935001373291\n",
      "Training loss for batch 550 : 0.012725500389933586\n",
      "Training loss for batch 551 : 0.06807596236467361\n",
      "Training loss for batch 552 : 0.01130794920027256\n",
      "Training loss for batch 553 : 0.1147497370839119\n",
      "Training loss for batch 554 : 0.0179856289178133\n",
      "Training loss for batch 555 : 0.12652361392974854\n",
      "Training loss for batch 556 : 0.01351426262408495\n",
      "Training loss for batch 557 : 0.0206588264554739\n",
      "Training loss for batch 558 : 0.12251890450716019\n",
      "Training loss for batch 559 : 0.08670718222856522\n",
      "Training loss for batch 560 : 0.02473626844584942\n",
      "Training loss for batch 561 : 0.07532776892185211\n",
      "Training loss for batch 562 : 0.13181935250759125\n",
      "Training loss for batch 563 : 0.1024925634264946\n",
      "Training loss for batch 564 : 0.08819340169429779\n",
      "Training loss for batch 565 : 0.14176177978515625\n",
      "Training loss for batch 566 : 0.021347559988498688\n",
      "Training loss for batch 567 : 0.010071845725178719\n",
      "Training loss for batch 568 : 0.026976246386766434\n",
      "Training loss for batch 569 : 0.051979053765535355\n",
      "Training loss for batch 570 : 0.04608161002397537\n",
      "Training loss for batch 571 : 0.09071267396211624\n",
      "Training loss for batch 572 : 0.060481809079647064\n",
      "Training loss for batch 573 : 0.1358531415462494\n",
      "Training loss for batch 574 : 0.009879260323941708\n",
      "Training loss for batch 575 : 0.04663968086242676\n",
      "Training loss for batch 576 : 0.0\n",
      "Training loss for batch 577 : 0.010760074481368065\n",
      "Training loss for batch 578 : 0.16599935293197632\n",
      "Training loss for batch 579 : 0.007634168956428766\n",
      "Training loss for batch 580 : 0.06335484981536865\n",
      "Training loss for batch 581 : 0.024090180173516273\n",
      "Training loss for batch 582 : 0.18885447084903717\n",
      "Training loss for batch 583 : 0.06558908522129059\n",
      "Training loss for batch 584 : 0.05996700003743172\n",
      "Training loss for batch 585 : 0.058561403304338455\n",
      "Training loss for batch 586 : 0.15807868540287018\n",
      "Training loss for batch 587 : 0.026186179369688034\n",
      "Training loss for batch 588 : 0.03458762168884277\n",
      "Training loss for batch 589 : 0.08633961528539658\n",
      "Training loss for batch 590 : 0.018977604806423187\n",
      "Training loss for batch 591 : 0.08618958294391632\n",
      "Training loss for batch 592 : 0.11830991506576538\n",
      "Training loss for batch 593 : 0.01830964908003807\n",
      "Training loss for batch 594 : 0.03764364495873451\n",
      "Training loss for batch 595 : 0.14031435549259186\n",
      "Training loss for batch 596 : 0.03398331627249718\n",
      "Training loss for batch 597 : 0.08740435540676117\n",
      "Training loss for batch 598 : 0.04097053408622742\n",
      "Training loss for batch 599 : 5.083325049781706e-08\n",
      "Training loss for batch 600 : 0.23391489684581757\n",
      "Training loss for batch 601 : 0.058921605348587036\n",
      "Training loss for batch 602 : 0.02591034583747387\n",
      "Training loss for batch 603 : 0.06741072237491608\n",
      "Training loss for batch 604 : 0.02884817123413086\n",
      "Training loss for batch 605 : 0.03637883812189102\n",
      "Training loss for batch 606 : 0.068080835044384\n",
      "Training loss for batch 607 : 0.19629541039466858\n",
      "Training loss for batch 608 : 0.02595168724656105\n",
      "Training loss for batch 609 : 0.065724678337574\n",
      "Training loss for batch 610 : 0.09138453006744385\n",
      "Training loss for batch 611 : 0.06947983056306839\n",
      "Training loss for batch 612 : 0.04792089760303497\n",
      "Training loss for batch 613 : 0.08729679882526398\n",
      "Training loss for batch 614 : 0.02894953079521656\n",
      "Training loss for batch 615 : 0.07472763955593109\n",
      "Training loss for batch 616 : 0.004841324873268604\n",
      "Training loss for batch 617 : 0.011302015744149685\n",
      "Training loss for batch 618 : 0.03837835416197777\n",
      "Training loss for batch 619 : 0.1577775627374649\n",
      "Training loss for batch 620 : 0.008744086138904095\n",
      "Training loss for batch 621 : 0.09095574170351028\n",
      "Training loss for batch 622 : 0.058929309248924255\n",
      "Training loss for batch 623 : 0.061585597693920135\n",
      "Training loss for batch 624 : 0.15558207035064697\n",
      "Training loss for batch 625 : 0.10834810882806778\n",
      "Training loss for batch 626 : 0.06249333545565605\n",
      "Training loss for batch 627 : 0.05124508589506149\n",
      "Training loss for batch 628 : 0.03336132690310478\n",
      "Training loss for batch 629 : 0.09395966678857803\n",
      "Training loss for batch 630 : 0.08040771633386612\n",
      "Training loss for batch 631 : 0.08268828690052032\n",
      "Training loss for batch 632 : 0.16764988005161285\n",
      "Training loss for batch 633 : 0.09814831614494324\n",
      "Training loss for batch 634 : 0.13427267968654633\n",
      "Training loss for batch 635 : 0.00534586887806654\n",
      "Training loss for batch 636 : 0.009992130100727081\n",
      "Training loss for batch 637 : 0.08581455796957016\n",
      "Training loss for batch 638 : 0.07913593202829361\n",
      "Training loss for batch 639 : 0.11275520920753479\n",
      "Training loss for batch 640 : 0.06801022589206696\n",
      "Training loss for batch 641 : 0.05847768113017082\n",
      "Training loss for batch 642 : 0.05115557461977005\n",
      "Training loss for batch 643 : 0.056045740842819214\n",
      "Training loss for batch 644 : 0.06474433094263077\n",
      "Training loss for batch 645 : 0.07922118157148361\n",
      "Training loss for batch 646 : 0.0011037664953619242\n",
      "Training loss for batch 647 : 0.08063101768493652\n",
      "Training loss for batch 648 : 1.373972491336417e-08\n",
      "Training loss for batch 649 : 0.0752524882555008\n",
      "Training loss for batch 650 : 0.027149301022291183\n",
      "Training loss for batch 651 : 0.011513792909681797\n",
      "Training loss for batch 652 : 0.011384089477360249\n",
      "Training loss for batch 653 : 0.019254453480243683\n",
      "Training loss for batch 654 : 0.057175297290086746\n",
      "Training loss for batch 655 : 0.06060205399990082\n",
      "Training loss for batch 656 : 0.12576133012771606\n",
      "Training loss for batch 657 : 0.04170083999633789\n",
      "Training loss for batch 658 : 0.09046308696269989\n",
      "Training loss for batch 659 : 0.08020475506782532\n",
      "Training loss for batch 660 : 0.03513519465923309\n",
      "Training loss for batch 661 : 0.06508535891771317\n",
      "Training loss for batch 662 : 0.06177760288119316\n",
      "Training loss for batch 663 : 0.027287235483527184\n",
      "Training loss for batch 664 : 0.07528612017631531\n",
      "Training loss for batch 665 : 0.10061556845903397\n",
      "Training loss for batch 666 : 0.007638415787369013\n",
      "Training loss for batch 667 : 0.057264797389507294\n",
      "Training loss for batch 668 : 0.02707422338426113\n",
      "Training loss for batch 669 : 0.14511989057064056\n",
      "Training loss for batch 670 : 0.01661796309053898\n",
      "Training loss for batch 671 : 0.1161617785692215\n",
      "Training loss for batch 672 : 0.07002674043178558\n",
      "Training loss for batch 673 : 0.23273371160030365\n",
      "Training loss for batch 674 : 0.002282381057739258\n",
      "Training loss for batch 675 : 0.04851908981800079\n",
      "Training loss for batch 676 : 0.07982749491930008\n",
      "Training loss for batch 677 : 0.061853207647800446\n",
      "Training loss for batch 678 : 0.01957360841333866\n",
      "Training loss for batch 679 : 0.1377187818288803\n",
      "Training loss for batch 680 : 0.03160657733678818\n",
      "Training loss for batch 681 : 0.1755918711423874\n",
      "Training loss for batch 682 : 0.21819061040878296\n",
      "Training loss for batch 683 : 0.034060023725032806\n",
      "Training loss for batch 684 : 0.08453070372343063\n",
      "Training loss for batch 685 : 0.0\n",
      "Training loss for batch 686 : 0.1729717254638672\n",
      "Training loss for batch 687 : 0.049853239208459854\n",
      "Training loss for batch 688 : 0.07020886987447739\n",
      "Training loss for batch 689 : 0.031675342470407486\n",
      "Training loss for batch 690 : 0.11025893688201904\n",
      "Training loss for batch 691 : 0.09740972518920898\n",
      "Training loss for batch 692 : 1.0453923060538273e-08\n",
      "Training loss for batch 693 : 0.06873126327991486\n",
      "Training loss for batch 694 : 0.05018070712685585\n",
      "Training loss for batch 695 : 0.18646322190761566\n",
      "Training loss for batch 696 : 0.014207493513822556\n",
      "Training loss for batch 697 : 0.037005264312028885\n",
      "Training loss for batch 698 : 0.04571192339062691\n",
      "Training loss for batch 699 : 0.0017952894559130073\n",
      "Training loss for batch 700 : 0.06146683916449547\n",
      "Training loss for batch 701 : 0.06986518204212189\n",
      "Training loss for batch 702 : 0.04210685193538666\n",
      "Training loss for batch 703 : 0.05693608522415161\n",
      "Training loss for batch 704 : 0.04710109159350395\n",
      "Training loss for batch 705 : 0.012080315500497818\n",
      "Training loss for batch 706 : 0.034402139484882355\n",
      "Training loss for batch 707 : 0.05857238546013832\n",
      "Training loss for batch 708 : 0.056178975850343704\n",
      "Training loss for batch 709 : 0.12043710798025131\n",
      "Training loss for batch 710 : 0.04235631972551346\n",
      "Training loss for batch 711 : 0.04324682056903839\n",
      "Training loss for batch 712 : 0.048583708703517914\n",
      "Training loss for batch 713 : 0.1533280313014984\n",
      "Training loss for batch 714 : 0.09465658664703369\n",
      "Training loss for batch 715 : 0.11786449700593948\n",
      "Training loss for batch 716 : 0.10458708554506302\n",
      "Training loss for batch 717 : 0.0695277750492096\n",
      "Training loss for batch 718 : 0.10254289954900742\n",
      "Training loss for batch 719 : 0.16299781203269958\n",
      "Training loss for batch 720 : 0.05518035963177681\n",
      "Training loss for batch 721 : 0.001351243001408875\n",
      "Training loss for batch 722 : 0.030103664845228195\n",
      "Training loss for batch 723 : 0.0\n",
      "Training loss for batch 724 : 0.1373414844274521\n",
      "Training loss for batch 725 : 0.07248757034540176\n",
      "Training loss for batch 726 : 0.0649905800819397\n",
      "Training loss for batch 727 : 0.07910700887441635\n",
      "Training loss for batch 728 : 0.038021720945835114\n",
      "Training loss for batch 729 : 0.06934410333633423\n",
      "Training loss for batch 730 : 0.07821517437696457\n",
      "Training loss for batch 731 : 0.0596202090382576\n",
      "Training loss for batch 732 : 0.07507841289043427\n",
      "Training loss for batch 733 : 0.1270494908094406\n",
      "Training loss for batch 734 : 0.02815689519047737\n",
      "Training loss for batch 735 : 0.08284139633178711\n",
      "Training loss for batch 736 : 0.08619223535060883\n",
      "Training loss for batch 737 : 0.002827230142429471\n",
      "Training loss for batch 738 : 0.09282219409942627\n",
      "Training loss for batch 739 : 0.04647250846028328\n",
      "Training loss for batch 740 : 0.06463725864887238\n",
      "Training loss for batch 741 : 0.06852524727582932\n",
      "Training loss for batch 742 : 0.05534573271870613\n",
      "Training loss for batch 743 : 0.01708795689046383\n",
      "Training loss for batch 744 : 0.11761830747127533\n",
      "Training loss for batch 745 : 0.11309847235679626\n",
      "Training loss for batch 746 : 0.02949167601764202\n",
      "Training loss for batch 747 : 0.17781874537467957\n",
      "Training loss for batch 748 : 0.02105039171874523\n",
      "Training loss for batch 749 : 0.13466843962669373\n",
      "Training loss for batch 750 : 0.04958372190594673\n",
      "Training loss for batch 751 : 0.05401891842484474\n",
      "Training loss for batch 752 : 0.09503846615552902\n",
      "Training loss for batch 753 : 0.06387779861688614\n",
      "Training loss for batch 754 : 0.02004680410027504\n",
      "Training loss for batch 755 : 0.13898135721683502\n",
      "Training loss for batch 756 : 0.057863328605890274\n",
      "Training loss for batch 757 : 0.06960733979940414\n",
      "Training loss for batch 758 : 0.14853844046592712\n",
      "Training loss for batch 759 : 0.034189920872449875\n",
      "Training loss for batch 760 : 0.1357647329568863\n",
      "Training loss for batch 761 : 0.055310286581516266\n",
      "Training loss for batch 762 : 0.09633191674947739\n",
      "Training loss for batch 763 : 0.02104596234858036\n",
      "Training loss for batch 764 : 0.04106521978974342\n",
      "Training loss for batch 765 : 0.03719198703765869\n",
      "Training loss for batch 766 : 0.12344303727149963\n",
      "Training loss for batch 767 : 0.12838153541088104\n",
      "Training loss for batch 768 : 0.08781472593545914\n",
      "Training loss for batch 769 : 0.05232757329940796\n",
      "Training loss for batch 770 : 0.27729305624961853\n",
      "Training loss for batch 771 : 0.04717034474015236\n",
      "Training loss for batch 772 : 0.07040118426084518\n",
      "Training loss for batch 773 : 0.033962663263082504\n",
      "Training loss for batch 774 : 0.0273506511002779\n",
      "Training loss for batch 775 : 0.06187009811401367\n",
      "Training loss for batch 776 : 0.11833406984806061\n",
      "Training loss for batch 777 : 0.03891924396157265\n",
      "Training loss for batch 778 : 0.03144027292728424\n",
      "Training loss for batch 779 : 0.02252129837870598\n",
      "Training loss for batch 780 : 0.11779502034187317\n",
      "Training loss for batch 781 : 0.07031094282865524\n",
      "Training loss for batch 782 : 0.04157111421227455\n",
      "Training loss for batch 783 : 0.10460063070058823\n",
      "Training loss for batch 784 : 0.04816896840929985\n",
      "Training loss for batch 785 : 0.03684040158987045\n",
      "Training loss for batch 786 : 0.025339050218462944\n",
      "Training loss for batch 787 : 0.004713119938969612\n",
      "Training loss for batch 788 : 0.019183756783604622\n",
      "Training loss for batch 789 : 0.0998668447136879\n",
      "Training loss for batch 790 : 0.004507869947701693\n",
      "Training loss for batch 791 : 0.08429088443517685\n",
      "Training loss for batch 792 : 0.047793976962566376\n",
      "Training loss for batch 793 : 0.04448501393198967\n",
      "Training loss for batch 794 : 0.029432518407702446\n",
      "Training loss for batch 795 : 0.11235764622688293\n",
      "Training loss for batch 796 : 0.07023995369672775\n",
      "Training loss for batch 797 : 0.07252107560634613\n",
      "Training loss for batch 798 : 0.14548128843307495\n",
      "Training loss for batch 799 : 0.13003668189048767\n",
      "Training loss for batch 800 : 0.04409771412611008\n",
      "Training loss for batch 801 : 0.10894360393285751\n",
      "Training loss for batch 802 : 0.20350047945976257\n",
      "Training loss for batch 803 : 0.13900256156921387\n",
      "Training loss for batch 804 : 0.14898359775543213\n",
      "Training loss for batch 805 : 0.024494953453540802\n",
      "Training loss for batch 806 : 0.01208635326474905\n",
      "Training loss for batch 807 : 0.08566594123840332\n",
      "Training loss for batch 808 : 0.049044009298086166\n",
      "Training loss for batch 809 : 0.031268712133169174\n",
      "Training loss for batch 810 : 0.00414531072601676\n",
      "Training loss for batch 811 : 0.05638711154460907\n",
      "Training loss for batch 812 : 0.11753188073635101\n",
      "Training loss for batch 813 : 0.07366099208593369\n",
      "Training loss for batch 814 : 0.06198497861623764\n",
      "Training loss for batch 815 : 0.08881638199090958\n",
      "Training loss for batch 816 : 0.12843121588230133\n",
      "Training loss for batch 817 : 0.0341978557407856\n",
      "Training loss for batch 818 : 0.010256420820951462\n",
      "Training loss for batch 819 : 0.011187434196472168\n",
      "Training loss for batch 820 : 0.08404912799596786\n",
      "Training loss for batch 821 : 0.07805567979812622\n",
      "Training loss for batch 822 : 0.014422992244362831\n",
      "Training loss for batch 823 : 0.08401867747306824\n",
      "Training loss for batch 824 : 0.059531547129154205\n",
      "Training loss for batch 825 : 0.0072550540789961815\n",
      "Training loss for batch 826 : 0.0458076149225235\n",
      "Training loss for batch 827 : 0.04376434534788132\n",
      "Training loss for batch 828 : 0.06596579402685165\n",
      "Training loss for batch 829 : 0.012474747374653816\n",
      "Training loss for batch 830 : 0.08501078933477402\n",
      "Training loss for batch 831 : 0.1347292810678482\n",
      "Training loss for batch 832 : 0.008189098909497261\n",
      "Training loss for batch 833 : 0.09600440412759781\n",
      "Training loss for batch 834 : 0.046189721673727036\n",
      "Training loss for batch 835 : 0.025966573506593704\n",
      "Training loss for batch 836 : 0.009977788664400578\n",
      "Training loss for batch 837 : 0.08042412996292114\n",
      "Training loss for batch 838 : 0.019820744171738625\n",
      "Training loss for batch 839 : 0.028170045465230942\n",
      "Training loss for batch 840 : 0.23194560408592224\n",
      "Training loss for batch 841 : 0.0389789454638958\n",
      "Training loss for batch 842 : 0.05101841688156128\n",
      "Training loss for batch 843 : 0.0898965373635292\n",
      "Training loss for batch 844 : 0.20756033062934875\n",
      "Training loss for batch 845 : 0.031489115208387375\n",
      "Training loss for batch 846 : 0.042288586497306824\n",
      "Training loss for batch 847 : 0.13775737583637238\n",
      "Training loss for batch 848 : 0.052686188369989395\n",
      "Training loss for batch 849 : 0.06899089366197586\n",
      "Training loss for batch 850 : 0.019075214862823486\n",
      "Training loss for batch 851 : 0.025278514251112938\n",
      "Training loss for batch 852 : 0.04092510789632797\n",
      "Training loss for batch 853 : 0.018237018957734108\n",
      "Training loss for batch 854 : 0.1370190680027008\n",
      "Training loss for batch 855 : 0.1753278374671936\n",
      "Training loss for batch 856 : 0.09296602755784988\n",
      "Training loss for batch 857 : 0.046140894293785095\n",
      "Training loss for batch 858 : 0.02155132032930851\n",
      "Training loss for batch 859 : 0.021295737475156784\n",
      "Training loss for batch 860 : 0.030568154528737068\n",
      "Training loss for batch 861 : 0.089499332010746\n",
      "Training loss for batch 862 : 0.12090331315994263\n",
      "Training loss for batch 863 : 0.06994026154279709\n",
      "Training loss for batch 864 : 0.02653166465461254\n",
      "Training loss for batch 865 : 0.035560764372348785\n",
      "Training loss for batch 866 : 0.13084088265895844\n",
      "Training loss for batch 867 : 0.10320748388767242\n",
      "Training loss for batch 868 : 0.00874329824000597\n",
      "Training loss for batch 869 : 0.0479348748922348\n",
      "Training loss for batch 870 : 0.05035373568534851\n",
      "Training loss for batch 871 : 0.014418193139135838\n",
      "Training loss for batch 872 : 0.1864316612482071\n",
      "Training loss for batch 873 : 0.08797462284564972\n",
      "Training loss for batch 874 : 0.13275952637195587\n",
      "Training loss for batch 875 : 0.07198265194892883\n",
      "Training loss for batch 876 : 0.17419618368148804\n",
      "Training loss for batch 877 : 0.08138726651668549\n",
      "Training loss for batch 878 : 0.08835037797689438\n",
      "Training loss for batch 879 : 0.0059709991328418255\n",
      "Training loss for batch 880 : 0.23740608990192413\n",
      "Training loss for batch 881 : 0.2087446004152298\n",
      "Training loss for batch 882 : 0.0591149665415287\n",
      "Training loss for batch 883 : 0.05414257198572159\n",
      "Training loss for batch 884 : 0.05921341851353645\n",
      "Training loss for batch 885 : 0.08163446187973022\n",
      "Training loss for batch 886 : 0.0012247466947883368\n",
      "Training loss for batch 887 : 0.05609501153230667\n",
      "Training loss for batch 888 : 0.061921533197164536\n",
      "Training loss for batch 889 : 0.07060357928276062\n",
      "Training loss for batch 890 : 0.033501554280519485\n",
      "Training loss for batch 891 : 0.02598663978278637\n",
      "Training loss for batch 892 : 0.07920873910188675\n",
      "Training loss for batch 893 : 0.07207511365413666\n",
      "Training loss for batch 894 : 0.04497004300355911\n",
      "Training loss for batch 895 : 0.04771336540579796\n",
      "Training loss for batch 896 : 0.06954652816057205\n",
      "Training loss for batch 897 : 0.10909334570169449\n",
      "Training loss for batch 898 : 0.0021384386345744133\n",
      "Training loss for batch 899 : 0.06364250928163528\n",
      "Training loss for batch 900 : 0.13287284970283508\n",
      "Training loss for batch 901 : 0.11156102269887924\n",
      "Training loss for batch 902 : 0.11480219662189484\n",
      "Training loss for batch 903 : 0.028930483385920525\n",
      "Training loss for batch 904 : 0.13241209089756012\n",
      "Training loss for batch 905 : 0.062289174646139145\n",
      "Training loss for batch 906 : 0.041913338005542755\n",
      "Training loss for batch 907 : 0.0991426557302475\n",
      "Training loss for batch 908 : 0.04482603073120117\n",
      "Training loss for batch 909 : 0.030766218900680542\n",
      "Training loss for batch 910 : 0.20422500371932983\n",
      "Training loss for batch 911 : 0.0009302049875259399\n",
      "Training loss for batch 912 : 0.051182202994823456\n",
      "Training loss for batch 913 : 0.07343574613332748\n",
      "Training loss for batch 914 : 0.001022349577397108\n",
      "Training loss for batch 915 : 0.04481973126530647\n",
      "Training loss for batch 916 : 0.022110842168331146\n",
      "Training loss for batch 917 : 0.10436417907476425\n",
      "Training loss for batch 918 : 0.08935478329658508\n",
      "Training loss for batch 919 : 0.0\n",
      "Training loss for batch 920 : 0.030529286712408066\n",
      "Training loss for batch 921 : 0.10935436934232712\n",
      "Training loss for batch 922 : 0.0880637913942337\n",
      "Training loss for batch 923 : 0.08281572163105011\n",
      "Training loss for batch 924 : 0.1469331681728363\n",
      "Training loss for batch 925 : 0.011606855317950249\n",
      "Training loss for batch 926 : 0.0688837543129921\n",
      "Training loss for batch 927 : 0.07697790116071701\n",
      "Training loss for batch 928 : 0.04494297504425049\n",
      "Training loss for batch 929 : 0.009357132948935032\n",
      "Training loss for batch 930 : 0.1257980614900589\n",
      "Training loss for batch 931 : 0.10182023048400879\n",
      "Training loss for batch 932 : 0.01668359711766243\n",
      "Training loss for batch 933 : 0.055506568402051926\n",
      "Training loss for batch 934 : 0.04212648421525955\n",
      "Training loss for batch 935 : 0.03245428577065468\n",
      "Training loss for batch 936 : 0.09913254529237747\n",
      "Training loss for batch 937 : 0.0014194201212376356\n",
      "Training loss for batch 938 : 0.00514154601842165\n",
      "Training loss for batch 939 : 0.06472078710794449\n",
      "Training loss for batch 940 : 0.020375864580273628\n",
      "Training loss for batch 941 : 0.0562015026807785\n",
      "Training loss for batch 942 : 0.10222508758306503\n",
      "Training loss for batch 943 : 0.05550766736268997\n",
      "Training loss for batch 944 : 0.027075065299868584\n",
      "Training loss for batch 945 : 0.09077295660972595\n",
      "Training loss for batch 946 : 0.03332515433430672\n",
      "Training loss for batch 947 : 0.004039937164634466\n",
      "Training loss for batch 948 : 0.06448529660701752\n",
      "Training loss for batch 949 : 0.08650612831115723\n",
      "Training loss for batch 950 : 0.003635646076872945\n",
      "Training loss for batch 951 : 0.051361311227083206\n",
      "Training loss for batch 952 : 0.012452520430088043\n",
      "Training loss for batch 953 : 0.10139475017786026\n",
      "Training loss for batch 954 : 0.2643314301967621\n",
      "Training loss for batch 955 : 0.0\n",
      "Training loss for batch 956 : 0.16528567671775818\n",
      "Training loss for batch 957 : 0.13959088921546936\n",
      "Training loss for batch 958 : 0.07761204242706299\n",
      "Training loss for batch 959 : 0.018936382606625557\n",
      "Training loss for batch 960 : 0.007767999079078436\n",
      "Training loss for batch 961 : 0.07687855511903763\n",
      "Training loss for batch 962 : 0.04727385938167572\n",
      "Training loss for batch 963 : 0.11650563031435013\n",
      "Training loss for batch 964 : 0.2109532207250595\n",
      "Training loss for batch 965 : 0.06543854624032974\n",
      "Training loss for batch 966 : 0.0018964664777740836\n",
      "Training loss for batch 967 : 0.05619967728853226\n",
      "Training loss for batch 968 : 0.01213238574564457\n",
      "Training loss for batch 969 : 0.1140919104218483\n",
      "Training loss for batch 970 : 0.16044531762599945\n",
      "Training loss for batch 971 : 0.005389196798205376\n",
      "Training loss for batch 972 : 0.19824150204658508\n",
      "Training loss for batch 973 : 0.07312022149562836\n",
      "Training loss for batch 974 : 0.004106894601136446\n",
      "Training loss for batch 975 : 0.06962926685810089\n",
      "Training loss for batch 976 : 0.04463781788945198\n",
      "Training loss for batch 977 : 0.11815349012613297\n",
      "Training loss for batch 978 : 0.027758995071053505\n",
      "Training loss for batch 979 : 0.08028112351894379\n",
      "Training loss for batch 980 : 0.03847403824329376\n",
      "Training loss for batch 981 : 0.047557491809129715\n",
      "Training loss for batch 982 : 0.07996967434883118\n",
      "Training loss for batch 983 : 0.039987627416849136\n",
      "Training loss for batch 984 : 0.16849148273468018\n",
      "Training loss for batch 985 : 0.003367335768416524\n",
      "Training loss for batch 986 : 0.027635518461465836\n",
      "Training loss for batch 987 : 0.022058140486478806\n",
      "Training loss for batch 988 : 0.0827183648943901\n",
      "Training loss for batch 989 : 0.1279008984565735\n",
      "Training loss for batch 990 : 0.08066785335540771\n",
      "Training loss for batch 991 : 0.023583928123116493\n",
      "Training loss for batch 992 : 0.018559129908680916\n",
      "Training loss for batch 993 : 0.07017005234956741\n",
      "Training loss for batch 994 : 0.05648690089583397\n",
      "Training loss for batch 995 : 0.16267810761928558\n",
      "Training loss for batch 996 : 0.050951551645994186\n",
      "Training loss for batch 997 : 0.09551233798265457\n",
      "Training loss for batch 998 : 0.012167860753834248\n",
      "Training loss for batch 999 : 0.11667481064796448\n",
      "Training loss for batch 1000 : 0.01843952015042305\n",
      "Training loss for batch 1001 : 0.0011011792812496424\n",
      "Training loss for batch 1002 : 0.13210220634937286\n",
      "Training loss for batch 1003 : 0.13839054107666016\n",
      "Training loss for batch 1004 : 0.16053009033203125\n",
      "Training loss for batch 1005 : 0.0579795315861702\n",
      "Training loss for batch 1006 : 0.10226666182279587\n",
      "Training loss for batch 1007 : 0.08821969479322433\n",
      "Training loss for batch 1008 : 0.04701477661728859\n",
      "Training loss for batch 1009 : 0.1373075246810913\n",
      "Training loss for batch 1010 : 0.09485622495412827\n",
      "Training loss for batch 1011 : 0.14371061325073242\n",
      "Training loss for batch 1012 : 0.1545572131872177\n",
      "Training loss for batch 1013 : 0.14781275391578674\n",
      "Training loss for batch 1014 : 0.07463523000478745\n",
      "Training loss for batch 1015 : 0.04266902059316635\n",
      "Training loss for batch 1016 : 0.05799092724919319\n",
      "Training loss for batch 1017 : 0.159576877951622\n",
      "Training loss for batch 1018 : 0.03530773147940636\n",
      "Training loss for batch 1019 : 0.11116237193346024\n",
      "Training loss for batch 1020 : 0.013180430047214031\n",
      "Training loss for batch 1021 : 0.01941983588039875\n",
      "Training loss for batch 1022 : 0.17099283635616302\n",
      "Training loss for batch 1023 : 0.039232973009347916\n",
      "Training loss for batch 1024 : 0.04628482460975647\n",
      "Training loss for batch 1025 : 0.05459088832139969\n",
      "Training loss for batch 1026 : 0.01644531637430191\n",
      "Training loss for batch 1027 : 0.037579894065856934\n",
      "Training loss for batch 1028 : 0.07231350988149643\n",
      "Training loss for batch 1029 : 0.058886073529720306\n",
      "Training loss for batch 1030 : 0.01224589068442583\n",
      "Training loss for batch 1031 : 0.06426449120044708\n",
      "Training loss for batch 1032 : 0.06267596781253815\n",
      "Training loss for batch 1033 : 0.04459010064601898\n",
      "Training loss for batch 1034 : 0.0365571491420269\n",
      "Training loss for batch 1035 : 0.10669304430484772\n",
      "Training loss for batch 1036 : 0.07919829338788986\n",
      "Training loss for batch 1037 : 0.038376953452825546\n",
      "Training loss for batch 1038 : 0.10659004747867584\n",
      "Training loss for batch 1039 : 0.05372937396168709\n",
      "Training loss for batch 1040 : 0.05938510596752167\n",
      "Training loss for batch 1041 : 0.02487087994813919\n",
      "Training loss for batch 1042 : 0.03417646139860153\n",
      "Training loss for batch 1043 : 0.17793051898479462\n",
      "Training loss for batch 1044 : 0.11131668090820312\n",
      "Training loss for batch 1045 : 0.034724485129117966\n",
      "Training loss for batch 1046 : 0.03311792388558388\n",
      "Training loss for batch 1047 : 0.055317558348178864\n",
      "Training loss for batch 1048 : 0.028922589495778084\n",
      "Training loss for batch 1049 : 0.16589030623435974\n",
      "Training loss for batch 1050 : 0.03272946923971176\n",
      "Training loss for batch 1051 : 0.1321466863155365\n",
      "Training loss for batch 1052 : 0.04155337065458298\n",
      "Training loss for batch 1053 : 0.035788558423519135\n",
      "Training loss for batch 1054 : 0.08741731196641922\n",
      "Training loss for batch 1055 : 0.09333125501871109\n",
      "Training loss for batch 1056 : 0.032150086015462875\n",
      "Training loss for batch 1057 : 0.018044913187623024\n",
      "Training loss for batch 1058 : 0.021390918642282486\n",
      "Training loss for batch 1059 : 0.03651054948568344\n",
      "Training loss for batch 1060 : 0.05900855362415314\n",
      "Training loss for batch 1061 : 0.0946698859333992\n",
      "Training loss for batch 1062 : 0.22598326206207275\n",
      "Training loss for batch 1063 : 0.19456666707992554\n",
      "Training loss for batch 1064 : 0.0667898952960968\n",
      "Training loss for batch 1065 : 0.011308427900075912\n",
      "Training loss for batch 1066 : 0.06092158332467079\n",
      "Training loss for batch 1067 : 0.06403838843107224\n",
      "Training loss for batch 1068 : 0.020594462752342224\n",
      "Training loss for batch 1069 : 0.0932479202747345\n",
      "Training loss for batch 1070 : 0.0024019458796828985\n",
      "Training loss for batch 1071 : 0.07187686115503311\n",
      "Training loss for batch 1072 : 0.06010835990309715\n",
      "Training loss for batch 1073 : 0.04290614277124405\n",
      "Training loss for batch 1074 : 0.00863577052950859\n",
      "Training loss for batch 1075 : 0.15534144639968872\n",
      "Training loss for batch 1076 : 0.03966988995671272\n",
      "Training loss for batch 1077 : 0.026139184832572937\n",
      "Training loss for batch 1078 : 0.08779806643724442\n",
      "Training loss for batch 1079 : 0.032350655645132065\n",
      "Training loss for batch 1080 : 0.033437956124544144\n",
      "Training loss for batch 1081 : 0.17235594987869263\n",
      "Training loss for batch 1082 : 0.17489050328731537\n",
      "Training loss for batch 1083 : 0.09758489578962326\n",
      "Training loss for batch 1084 : 0.015528207644820213\n",
      "Training loss for batch 1085 : 0.104494109749794\n",
      "Training loss for batch 1086 : 0.02548537589609623\n",
      "Training loss for batch 1087 : 0.07421178370714188\n",
      "Training loss for batch 1088 : 0.024545978754758835\n",
      "Training loss for batch 1089 : 0.17687657475471497\n",
      "Training loss for batch 1090 : 0.08634716272354126\n",
      "Training loss for batch 1091 : 0.09710609912872314\n",
      "Training loss for batch 1092 : 0.15816056728363037\n",
      "Training loss for batch 1093 : 0.10675422847270966\n",
      "Training loss for batch 1094 : 0.2571812868118286\n",
      "Training loss for batch 1095 : 0.10999464988708496\n",
      "Training loss for batch 1096 : 0.1723870187997818\n",
      "Training loss for batch 1097 : 0.1081792488694191\n",
      "Training loss for batch 1098 : 0.11006411910057068\n",
      "Training loss for batch 1099 : 0.0036208799574524164\n",
      "Training loss for batch 1100 : 0.04742886498570442\n",
      "Training loss for batch 1101 : 0.011851423420011997\n",
      "Training loss for batch 1102 : 0.10643209517002106\n",
      "Training loss for batch 1103 : 0.07293219864368439\n",
      "Training loss for batch 1104 : 0.1114482656121254\n",
      "Training loss for batch 1105 : 0.12822893261909485\n",
      "Training loss for batch 1106 : 0.09408438205718994\n",
      "Training loss for batch 1107 : 0.08122328668832779\n",
      "Training loss for batch 1108 : 0.04629652202129364\n",
      "Training loss for batch 1109 : 0.09695212543010712\n",
      "Training loss for batch 1110 : 0.08312086015939713\n",
      "Training loss for batch 1111 : 0.09457868337631226\n",
      "Training loss for batch 1112 : 0.14495499432086945\n",
      "Training loss for batch 1113 : 0.07588542252779007\n",
      "Training loss for batch 1114 : 0.1078612357378006\n",
      "Training loss for batch 1115 : 0.002888903021812439\n",
      "Training loss for batch 1116 : 0.05449819192290306\n",
      "Training loss for batch 1117 : 0.10607799142599106\n",
      "Training loss for batch 1118 : 0.1121109277009964\n",
      "Training loss for batch 1119 : 0.057697080075740814\n",
      "Training loss for batch 1120 : 0.14479172229766846\n",
      "Training loss for batch 1121 : 0.043195243924856186\n",
      "Training loss for batch 1122 : 0.04760526865720749\n",
      "Training loss for batch 1123 : 0.015153287909924984\n",
      "Training loss for batch 1124 : 0.0701555386185646\n",
      "Training loss for batch 1125 : 0.0436304546892643\n",
      "Training loss for batch 1126 : 0.030659988522529602\n",
      "Training loss for batch 1127 : 0.05980725958943367\n",
      "Training loss for batch 1128 : 0.013713648542761803\n",
      "Training loss for batch 1129 : 0.15530772507190704\n",
      "Training loss for batch 1130 : 0.22622089087963104\n",
      "Training loss for batch 1131 : 0.007265856489539146\n",
      "Training loss for batch 1132 : 0.05717603489756584\n",
      "Training loss for batch 1133 : 0.03958221897482872\n",
      "Training loss for batch 1134 : 0.056474719196558\n",
      "Training loss for batch 1135 : 0.041811343282461166\n",
      "Training loss for batch 1136 : 0.051216304302215576\n",
      "Training loss for batch 1137 : 0.03149862214922905\n",
      "Training loss for batch 1138 : 0.03066885657608509\n",
      "Training loss for batch 1139 : 0.10430276393890381\n",
      "Training loss for batch 1140 : 0.0038876186590641737\n",
      "Training loss for batch 1141 : 0.041525617241859436\n",
      "Training loss for batch 1142 : 0.03650226071476936\n",
      "Training loss for batch 1143 : 0.031810883432626724\n",
      "Training loss for batch 1144 : 0.13300640881061554\n",
      "Training loss for batch 1145 : 0.03909271955490112\n",
      "Training loss for batch 1146 : 0.0006378713878802955\n",
      "Training loss for batch 1147 : 0.0305886622518301\n",
      "Training loss for batch 1148 : 0.057046908885240555\n",
      "Training loss for batch 1149 : 0.07433570176362991\n",
      "Training loss for batch 1150 : 0.1109575405716896\n",
      "Training loss for batch 1151 : 0.08791869878768921\n",
      "Training loss for batch 1152 : 0.05569128692150116\n",
      "Training loss for batch 1153 : 0.04541614651679993\n",
      "Training loss for batch 1154 : 0.048364296555519104\n",
      "Training loss for batch 1155 : 0.08941540122032166\n",
      "Training loss for batch 1156 : 0.030498472973704338\n",
      "Training loss for batch 1157 : 0.12824532389640808\n",
      "Training loss for batch 1158 : 0.05208483338356018\n",
      "Training loss for batch 1159 : 8.255137800006196e-05\n",
      "Training loss for batch 1160 : 0.09577840566635132\n",
      "Training loss for batch 1161 : 0.0038517783395946026\n",
      "Training loss for batch 1162 : 0.034909408539533615\n",
      "Training loss for batch 1163 : 0.05141332373023033\n",
      "Training loss for batch 1164 : 0.04646372050046921\n",
      "Training loss for batch 1165 : 0.07227359712123871\n",
      "Training loss for batch 1166 : 0.030803244560956955\n",
      "Training loss for batch 1167 : 0.14158625900745392\n",
      "Training loss for batch 1168 : 0.13531287014484406\n",
      "Training loss for batch 1169 : 0.166536346077919\n",
      "Training loss for batch 1170 : 0.078291155397892\n",
      "Training loss for batch 1171 : 0.174518421292305\n",
      "Training loss for batch 1172 : 0.00768698938190937\n",
      "Training loss for batch 1173 : 0.014043557457625866\n",
      "Training loss for batch 1174 : 0.049300290644168854\n",
      "Training loss for batch 1175 : 0.0881953164935112\n",
      "Training loss for batch 1176 : 0.039318133145570755\n",
      "Training loss for batch 1177 : 0.03711361065506935\n",
      "Training loss for batch 1178 : 0.04488953948020935\n",
      "Training loss for batch 1179 : 0.06797727942466736\n",
      "Training loss for batch 1180 : 0.19639956951141357\n",
      "Training loss for batch 1181 : 0.06607048958539963\n",
      "Training loss for batch 1182 : 0.024640338495373726\n",
      "Training loss for batch 1183 : 0.1289932280778885\n",
      "Training loss for batch 1184 : 0.03148746117949486\n",
      "Training loss for batch 1185 : 0.029086345806717873\n",
      "Training loss for batch 1186 : 0.12334601581096649\n",
      "Training loss for batch 1187 : 0.21025507152080536\n",
      "Training loss for batch 1188 : 0.09354940801858902\n",
      "Training loss for batch 1189 : 0.05545410141348839\n",
      "Training loss for batch 1190 : 0.1258503496646881\n",
      "Training loss for batch 1191 : 0.09326093643903732\n",
      "Training loss for batch 1192 : 0.2007366120815277\n",
      "Training loss for batch 1193 : 0.07312240451574326\n",
      "Training loss for batch 1194 : 0.04287181422114372\n",
      "Training loss for batch 1195 : 0.18020370602607727\n",
      "Training loss for batch 1196 : 0.010250558145344257\n",
      "Training loss for batch 1197 : 0.08317483216524124\n",
      "Training loss for batch 1198 : 0.06999785453081131\n",
      "Training loss for batch 1199 : 0.03937218338251114\n",
      "Training loss for batch 1200 : 0.04340040311217308\n",
      "Training loss for batch 1201 : 0.25787168741226196\n",
      "Training loss for batch 1202 : 0.09842818230390549\n",
      "Training loss for batch 1203 : 0.05815805494785309\n",
      "Training loss for batch 1204 : 0.06152110546827316\n",
      "Training loss for batch 1205 : 0.05036145821213722\n",
      "Training loss for batch 1206 : 0.07091930508613586\n",
      "Training loss for batch 1207 : 0.021085640415549278\n",
      "Training loss for batch 1208 : 0.06598763167858124\n",
      "Training loss for batch 1209 : 0.09978023916482925\n",
      "Training loss for batch 1210 : 0.1423862725496292\n",
      "Training loss for batch 1211 : 0.0347251333296299\n",
      "Training loss for batch 1212 : 0.07626087218523026\n",
      "Training loss for batch 1213 : 0.16963361203670502\n",
      "Training loss for batch 1214 : 0.10960328578948975\n",
      "Training loss for batch 1215 : 0.008829990401864052\n",
      "Training loss for batch 1216 : 0.0065369815565645695\n",
      "Training loss for batch 1217 : 0.27306050062179565\n",
      "Training loss for batch 1218 : 0.007336360402405262\n",
      "Training loss for batch 1219 : 0.07829354703426361\n",
      "Training loss for batch 1220 : 0.01786310225725174\n",
      "Training loss for batch 1221 : 0.005594540853053331\n",
      "Training loss for batch 1222 : 0.12542003393173218\n",
      "Training loss for batch 1223 : 0.15919384360313416\n",
      "Training loss for batch 1224 : 0.03963843360543251\n",
      "Training loss for batch 1225 : 0.024498486891388893\n",
      "Training loss for batch 1226 : 0.2587992250919342\n",
      "Training loss for batch 1227 : 0.0776335671544075\n",
      "Training loss for batch 1228 : 0.05416339635848999\n",
      "Training loss for batch 1229 : 0.048373401165008545\n",
      "Training loss for batch 1230 : 0.02962595410645008\n",
      "Training loss for batch 1231 : 0.1024952307343483\n",
      "Training loss for batch 1232 : 0.06921457499265671\n",
      "Training loss for batch 1233 : 0.014270921237766743\n",
      "Training loss for batch 1234 : 0.0120397312566638\n",
      "Training loss for batch 1235 : 0.08010300993919373\n",
      "Training loss for batch 1236 : 0.07471352815628052\n",
      "Training loss for batch 1237 : 0.13069747388362885\n",
      "Training loss for batch 1238 : 0.0695093423128128\n",
      "Training loss for batch 1239 : 0.06130170449614525\n",
      "Training loss for batch 1240 : 0.057318828999996185\n",
      "Training loss for batch 1241 : 0.1694987714290619\n",
      "Training loss for batch 1242 : 0.04723585397005081\n",
      "Training loss for batch 1243 : 0.15245261788368225\n",
      "Training loss for batch 1244 : 0.15445582568645477\n",
      "Training loss for batch 1245 : 0.037787891924381256\n",
      "Training loss for batch 1246 : 0.08842012286186218\n",
      "Training loss for batch 1247 : 0.07035796344280243\n",
      "Training loss for batch 1248 : 0.12412449717521667\n",
      "Training loss for batch 1249 : 0.013964599929749966\n",
      "Training loss for batch 1250 : 0.17472219467163086\n",
      "Training loss for batch 1251 : 0.07926199585199356\n",
      "Training loss for batch 1252 : 0.04631051421165466\n",
      "Training loss for batch 1253 : 0.060293592512607574\n",
      "Training loss for batch 1254 : 0.25049716234207153\n",
      "Training loss for batch 1255 : 0.1282195746898651\n",
      "Training loss for batch 1256 : 0.030659573152661324\n",
      "Training loss for batch 1257 : 0.10230329632759094\n",
      "Training loss for batch 1258 : 0.04714319482445717\n",
      "Training loss for batch 1259 : 0.028780383989214897\n",
      "Training loss for batch 1260 : 0.23301492631435394\n",
      "Training loss for batch 1261 : 0.09772755950689316\n",
      "Training loss for batch 1262 : 0.13814328610897064\n",
      "Training loss for batch 1263 : 0.0015576531877741218\n",
      "Training loss for batch 1264 : 0.08479531854391098\n",
      "Training loss for batch 1265 : 0.17269645631313324\n",
      "Training loss for batch 1266 : 0.2009132206439972\n",
      "Training loss for batch 1267 : 0.07732497155666351\n",
      "Training loss for batch 1268 : 0.21466755867004395\n",
      "Training loss for batch 1269 : 0.03631461411714554\n",
      "Training loss for batch 1270 : 0.11405882984399796\n",
      "Training loss for batch 1271 : 0.06222878023982048\n",
      "Training loss for batch 1272 : 0.15353311598300934\n",
      "Training loss for batch 1273 : 0.060539569705724716\n",
      "Training loss for batch 1274 : 0.12364964187145233\n",
      "Training loss for batch 1275 : 0.14415325224399567\n",
      "Training loss for batch 1276 : 0.01703633926808834\n",
      "Training loss for batch 1277 : 0.016904164105653763\n",
      "Training loss for batch 1278 : 0.014047808013856411\n",
      "Training loss for batch 1279 : 0.08699173480272293\n",
      "Training loss for batch 1280 : 0.0642690435051918\n",
      "Training loss for batch 1281 : 0.06304817646741867\n",
      "Training loss for batch 1282 : 0.0697101429104805\n",
      "Training loss for batch 1283 : 0.06671249866485596\n",
      "Training loss for batch 1284 : 0.13550829887390137\n",
      "Training loss for batch 1285 : 0.0025328313931822777\n",
      "Training loss for batch 1286 : 0.0843939483165741\n",
      "Training loss for batch 1287 : 0.01487622782588005\n",
      "Training loss for batch 1288 : 0.0903622955083847\n",
      "Training loss for batch 1289 : 0.0309524517506361\n",
      "Training loss for batch 1290 : 0.08511760085821152\n",
      "Training loss for batch 1291 : 0.10333172231912613\n",
      "Training loss for batch 1292 : 0.01901923306286335\n",
      "Training loss for batch 1293 : 0.1314602941274643\n",
      "Training loss for batch 1294 : 0.004876052960753441\n",
      "Training loss for batch 1295 : 0.08240802586078644\n",
      "Training loss for batch 1296 : 0.08058186620473862\n",
      "Training loss for batch 1297 : 0.025003565475344658\n",
      "Training loss for batch 1298 : 0.07889154553413391\n",
      "Training loss for batch 1299 : 0.08869455009698868\n",
      "Training loss for batch 1300 : 0.09305085986852646\n",
      "Training loss for batch 1301 : 0.05351791903376579\n",
      "Training loss for batch 1302 : 0.21472975611686707\n",
      "Training loss for batch 1303 : 0.05715172737836838\n",
      "Training loss for batch 1304 : 0.07198567688465118\n",
      "Training loss for batch 1305 : 0.02293366752564907\n",
      "Training loss for batch 1306 : 0.07652492821216583\n",
      "Training loss for batch 1307 : 0.10022132098674774\n",
      "Training loss for batch 1308 : 0.1281379759311676\n",
      "Training loss for batch 1309 : 0.11344167590141296\n",
      "Training loss for batch 1310 : 0.10140520334243774\n",
      "Training loss for batch 1311 : 0.0\n",
      "Training loss for batch 1312 : 0.0052842749282717705\n",
      "Training loss for batch 1313 : 0.036205556243658066\n",
      "Training loss for batch 1314 : 0.016565177589654922\n",
      "Training loss for batch 1315 : 0.06467125564813614\n",
      "Training loss for batch 1316 : 0.04411904886364937\n",
      "Training loss for batch 1317 : 0.0093739228323102\n",
      "Training loss for batch 1318 : 0.03731944411993027\n",
      "Training loss for batch 1319 : 0.024066999554634094\n",
      "Training loss for batch 1320 : 0.0036647971719503403\n",
      "Training loss for batch 1321 : 0.037299126386642456\n",
      "Training loss for batch 1322 : 0.011970116756856441\n",
      "Training loss for batch 1323 : 0.05676385387778282\n",
      "Training loss for batch 1324 : 0.007241717539727688\n",
      "Training loss for batch 1325 : 0.14763708412647247\n",
      "Training loss for batch 1326 : 0.11529213190078735\n",
      "Training loss for batch 1327 : 0.1650998741388321\n",
      "Training loss for batch 1328 : 0.10961010307073593\n",
      "Training loss for batch 1329 : 0.052881401032209396\n",
      "Training loss for batch 1330 : 8.291247866054618e-08\n",
      "Training loss for batch 1331 : 0.03621930256485939\n",
      "Training loss for batch 1332 : 0.04940664395689964\n",
      "Training loss for batch 1333 : 0.029470866546034813\n",
      "Training loss for batch 1334 : 0.1311475932598114\n",
      "Training loss for batch 1335 : 0.032096654176712036\n",
      "Training loss for batch 1336 : 0.11399871855974197\n",
      "Training loss for batch 1337 : 0.14903141558170319\n",
      "Training loss for batch 1338 : 0.025066738948225975\n",
      "Training loss for batch 1339 : 0.024846430867910385\n",
      "Training loss for batch 1340 : 0.019009819254279137\n",
      "Training loss for batch 1341 : 0.0782189667224884\n",
      "Training loss for batch 1342 : 0.016866568475961685\n",
      "Training loss for batch 1343 : 0.05379717797040939\n",
      "Training loss for batch 1344 : 0.10941196233034134\n",
      "Training loss for batch 1345 : 0.031443770974874496\n",
      "Training loss for batch 1346 : 0.027555231004953384\n",
      "Training loss for batch 1347 : 0.05744687467813492\n",
      "Training loss for batch 1348 : 0.03113744407892227\n",
      "Training loss for batch 1349 : 0.03828325495123863\n",
      "Training loss for batch 1350 : 0.019201360642910004\n",
      "Training loss for batch 1351 : 0.12463987618684769\n",
      "Training loss for batch 1352 : 0.00982176885008812\n",
      "Training loss for batch 1353 : 3.0311195587273687e-05\n",
      "Training loss for batch 1354 : 0.0365481972694397\n",
      "Training loss for batch 1355 : 0.05633075535297394\n",
      "Training loss for batch 1356 : 0.07204855978488922\n",
      "Training loss for batch 1357 : 0.08039101958274841\n",
      "Training loss for batch 1358 : 0.04345276951789856\n",
      "Training loss for batch 1359 : 0.022770432755351067\n",
      "Training loss for batch 1360 : 0.08479093015193939\n",
      "Training loss for batch 1361 : 0.016873298212885857\n",
      "Training loss for batch 1362 : 0.07316838949918747\n",
      "Training loss for batch 1363 : 0.10887026786804199\n",
      "Training loss for batch 1364 : 0.032705698162317276\n",
      "Training loss for batch 1365 : 0.07111860811710358\n",
      "Training loss for batch 1366 : 0.16009217500686646\n",
      "Training loss for batch 1367 : 0.25125598907470703\n",
      "Training loss for batch 1368 : 0.0141940051689744\n",
      "Training loss for batch 1369 : 0.06500758230686188\n",
      "Training loss for batch 1370 : 0.006692669820040464\n",
      "Training loss for batch 1371 : 0.02404557168483734\n",
      "Training loss for batch 1372 : 0.006806754972785711\n",
      "Training loss for batch 1373 : 0.10466509312391281\n",
      "Training loss for batch 1374 : 0.13581101596355438\n",
      "Training loss for batch 1375 : 0.03198646754026413\n",
      "Training loss for batch 1376 : 0.029916979372501373\n",
      "Training loss for batch 1377 : 0.0697108656167984\n",
      "Training loss for batch 1378 : 0.03455639258027077\n",
      "Training loss for batch 1379 : 0.022834375500679016\n",
      "Training loss for batch 1380 : 0.15536737442016602\n",
      "Training loss for batch 1381 : 0.07486608624458313\n",
      "Training loss for batch 1382 : 0.07421150803565979\n",
      "Training loss for batch 1383 : 0.03564617410302162\n",
      "Training loss for batch 1384 : 0.0009052902460098267\n",
      "Training loss for batch 1385 : 0.08484935760498047\n",
      "Training loss for batch 1386 : 0.06772656738758087\n",
      "Training loss for batch 1387 : 0.05072566121816635\n",
      "Training loss for batch 1388 : 0.037730876356363297\n",
      "Training loss for batch 1389 : 0.054581813514232635\n",
      "Training loss for batch 1390 : 0.01077983994036913\n",
      "Training loss for batch 1391 : 0.04174899309873581\n",
      "Training loss for batch 1392 : 0.04082177206873894\n",
      "Training loss for batch 1393 : 0.09425127506256104\n",
      "Training loss for batch 1394 : 0.0769442468881607\n",
      "Training loss for batch 1395 : 0.055049967020750046\n",
      "Training loss for batch 1396 : 0.10017082840204239\n",
      "Training loss for batch 1397 : 0.06240300461649895\n",
      "Training loss for batch 1398 : 0.08057160675525665\n",
      "Training loss for batch 1399 : 0.084742970764637\n",
      "Training loss for batch 1400 : 0.06910765916109085\n",
      "Training loss for batch 1401 : 0.026395946741104126\n",
      "Training loss for batch 1402 : 0.059673961251974106\n",
      "Training loss for batch 1403 : 0.05190673843026161\n",
      "Training loss for batch 1404 : 0.021760238334536552\n",
      "Training loss for batch 1405 : 1.2688165895724524e-07\n",
      "Training loss for batch 1406 : 0.13007213175296783\n",
      "Training loss for batch 1407 : 0.04512668401002884\n",
      "Training loss for batch 1408 : 0.1004786565899849\n",
      "Training loss for batch 1409 : 0.04724092036485672\n",
      "Training loss for batch 1410 : 0.08854588866233826\n",
      "Training loss for batch 1411 : 0.03664768859744072\n",
      "Training loss for batch 1412 : 0.1165398508310318\n",
      "Training loss for batch 1413 : 0.044644057750701904\n",
      "Training loss for batch 1414 : 0.13620489835739136\n",
      "Training loss for batch 1415 : 0.13530287146568298\n",
      "Training loss for batch 1416 : 0.0474960021674633\n",
      "Training loss for batch 1417 : 0.1372823417186737\n",
      "Training loss for batch 1418 : 0.0\n",
      "Training loss for batch 1419 : 0.21632172167301178\n",
      "Training loss for batch 1420 : 0.05432813987135887\n",
      "Training loss for batch 1421 : 0.1785125583410263\n",
      "Training loss for batch 1422 : 0.12492862343788147\n",
      "Training loss for batch 1423 : 0.19747097790241241\n",
      "Training loss for batch 1424 : 0.14307916164398193\n",
      "Training loss for batch 1425 : 0.10132697224617004\n",
      "Training loss for batch 1426 : 0.11711739748716354\n",
      "Training loss for batch 1427 : 0.02139406092464924\n",
      "Training loss for batch 1428 : 0.08968110382556915\n",
      "Training loss for batch 1429 : 0.14900089800357819\n",
      "Training loss for batch 1430 : 0.17779530584812164\n",
      "Training loss for batch 1431 : 0.06655681133270264\n",
      "Training loss for batch 1432 : 0.06770391762256622\n",
      "Training loss for batch 1433 : 0.17410317063331604\n",
      "Training loss for batch 1434 : 3.355829036877367e-08\n",
      "Training loss for batch 1435 : 0.07569439709186554\n",
      "Training loss for batch 1436 : 0.08542804419994354\n",
      "Training loss for batch 1437 : 0.21661065518856049\n",
      "Training loss for batch 1438 : 0.09176043421030045\n",
      "Training loss for batch 1439 : 0.14471574127674103\n",
      "Training loss for batch 1440 : 0.10538196563720703\n",
      "Training loss for batch 1441 : 0.007173776626586914\n",
      "Training loss for batch 1442 : 0.0682893693447113\n",
      "Training loss for batch 1443 : 0.16755028069019318\n",
      "Training loss for batch 1444 : 0.0009969979291781783\n",
      "Training loss for batch 1445 : 0.03939490765333176\n",
      "Training loss for batch 1446 : 0.06180969625711441\n",
      "Training loss for batch 1447 : 0.11447010189294815\n",
      "Training loss for batch 1448 : 0.032134320586919785\n",
      "Training loss for batch 1449 : 0.03256996348500252\n",
      "Training loss for batch 1450 : 0.04064661264419556\n",
      "Training loss for batch 1451 : 0.12554293870925903\n",
      "Training loss for batch 1452 : 0.09818864613771439\n",
      "Training loss for batch 1453 : 0.010123740881681442\n",
      "Training loss for batch 1454 : 0.02408231981098652\n",
      "Training loss for batch 1455 : 0.04917566850781441\n",
      "Training loss for batch 1456 : 0.027757497504353523\n",
      "Training loss for batch 1457 : 0.159528449177742\n",
      "Training loss for batch 1458 : 0.13829904794692993\n",
      "Training loss for batch 1459 : 0.0264238640666008\n",
      "Training loss for batch 1460 : 0.026136338710784912\n",
      "Training loss for batch 1461 : 0.05716180056333542\n",
      "Training loss for batch 1462 : 0.06723224371671677\n",
      "Training loss for batch 1463 : 0.2147061824798584\n",
      "Training loss for batch 1464 : 0.16200488805770874\n",
      "Training loss for batch 1465 : 0.03601105883717537\n",
      "Training loss for batch 1466 : 0.05515673756599426\n",
      "Training loss for batch 1467 : 0.11043766885995865\n",
      "Training loss for batch 1468 : 0.02211201936006546\n",
      "Training loss for batch 1469 : 0.007529707625508308\n",
      "Training loss for batch 1470 : 0.0986257866024971\n",
      "Training loss for batch 1471 : 0.06180119141936302\n",
      "Training loss for batch 1472 : 0.06466972827911377\n",
      "Training loss for batch 1473 : 0.02853703871369362\n",
      "Training loss for batch 1474 : 0.11512214690446854\n",
      "Training loss for batch 1475 : 0.1445021778345108\n",
      "Training loss for batch 1476 : 0.04597320407629013\n",
      "Training loss for batch 1477 : 0.15123490989208221\n",
      "Training loss for batch 1478 : 0.002354682656005025\n",
      "Training loss for batch 1479 : 0.09715411067008972\n",
      "Training loss for batch 1480 : 0.011740938760340214\n",
      "Training loss for batch 1481 : 0.15301883220672607\n",
      "Training loss for batch 1482 : 0.21022161841392517\n",
      "Training loss for batch 1483 : 0.10273805260658264\n",
      "Training loss for batch 1484 : 0.03595073148608208\n",
      "Training loss for batch 1485 : 0.02016936056315899\n",
      "Training loss for batch 1486 : 0.12236309051513672\n",
      "Training loss for batch 1487 : 0.16473981738090515\n",
      "Training loss for batch 1488 : 0.09403704851865768\n",
      "Training loss for batch 1489 : 0.03459848836064339\n",
      "Training loss for batch 1490 : 0.19816993176937103\n",
      "Training loss for batch 1491 : 0.09095395356416702\n",
      "Training loss for batch 1492 : 0.08693388104438782\n",
      "Training loss for batch 1493 : 0.08966568857431412\n",
      "Training loss for batch 1494 : 0.08466710895299911\n",
      "Training loss for batch 1495 : 0.008655871264636517\n",
      "Training loss for batch 1496 : 0.0893847793340683\n",
      "Training loss for batch 1497 : 0.02764267846941948\n",
      "Training loss for batch 1498 : 0.13561277091503143\n",
      "Training loss for batch 1499 : 0.17880578339099884\n",
      "Training loss for batch 1500 : 0.020170634612441063\n",
      "Training loss for batch 1501 : 0.06028496474027634\n",
      "Training loss for batch 1502 : 0.07295378297567368\n",
      "Training loss for batch 1503 : 0.06608898937702179\n",
      "Training loss for batch 1504 : 0.09925084561109543\n",
      "Training loss for batch 1505 : 0.013272062875330448\n",
      "Training loss for batch 1506 : 0.05485968291759491\n",
      "Training loss for batch 1507 : 0.09467589110136032\n",
      "Training loss for batch 1508 : 0.014685445465147495\n",
      "Training loss for batch 1509 : 0.10797878354787827\n",
      "Training loss for batch 1510 : 0.07719136774539948\n",
      "Training loss for batch 1511 : 0.06730426102876663\n",
      "Training loss for batch 1512 : 0.06183063983917236\n",
      "Training loss for batch 1513 : 0.11887983232736588\n",
      "Training loss for batch 1514 : 0.049086764454841614\n",
      "Training loss for batch 1515 : 0.08890392631292343\n",
      "Training loss for batch 1516 : 0.06065916270017624\n",
      "Training loss for batch 1517 : 0.05154501274228096\n",
      "Training loss for batch 1518 : 0.08411441743373871\n",
      "Training loss for batch 1519 : 0.14277997612953186\n",
      "Training loss for batch 1520 : 0.007670348044484854\n",
      "Training loss for batch 1521 : 0.10589759796857834\n",
      "Training loss for batch 1522 : 0.06005259230732918\n",
      "Training loss for batch 1523 : 0.017594072967767715\n",
      "Training loss for batch 1524 : 0.04375869035720825\n",
      "Training loss for batch 1525 : 0.07749419659376144\n",
      "Training loss for batch 1526 : 0.014534269459545612\n",
      "Training loss for batch 1527 : 0.010678283870220184\n",
      "Training loss for batch 1528 : 0.12790082395076752\n",
      "Training loss for batch 1529 : 0.16441631317138672\n",
      "Training loss for batch 1530 : 0.0604604110121727\n",
      "Training loss for batch 1531 : 0.08206994086503983\n",
      "Training loss for batch 1532 : 0.028678901493549347\n",
      "Training loss for batch 1533 : 0.07484764605760574\n",
      "Training loss for batch 1534 : 0.02356692962348461\n",
      "Training loss for batch 1535 : 0.025402769446372986\n",
      "Training loss for batch 1536 : 0.11710330098867416\n",
      "Training loss for batch 1537 : 0.12278793752193451\n",
      "Training loss for batch 1538 : 0.03921106457710266\n",
      "Training loss for batch 1539 : 0.03692232444882393\n",
      "Training loss for batch 1540 : 0.08262937515974045\n",
      "Training loss for batch 1541 : 0.07576969265937805\n",
      "Training loss for batch 1542 : 0.01727115549147129\n",
      "Training loss for batch 1543 : 0.073537677526474\n",
      "Training loss for batch 1544 : 0.12253622710704803\n",
      "Training loss for batch 1545 : 0.11611321568489075\n",
      "Training loss for batch 1546 : 0.032603081315755844\n",
      "Training loss for batch 1547 : 0.1136871948838234\n",
      "Training loss for batch 1548 : 0.08631448447704315\n",
      "Training loss for batch 1549 : 0.13976940512657166\n",
      "Training loss for batch 1550 : 0.052182815968990326\n",
      "Training loss for batch 1551 : 0.2692912817001343\n",
      "Training loss for batch 1552 : 0.04725896939635277\n",
      "Training loss for batch 1553 : 0.04518625885248184\n",
      "Training loss for batch 1554 : 0.09611102193593979\n",
      "Training loss for batch 1555 : 0.036397092044353485\n",
      "Training loss for batch 1556 : 0.07135661691427231\n",
      "Training loss for batch 1557 : 0.10412009805440903\n",
      "Training loss for batch 1558 : 0.0381057932972908\n",
      "Training loss for batch 1559 : 0.08409897983074188\n",
      "Training loss for batch 1560 : 0.007614925503730774\n",
      "Training loss for batch 1561 : 0.09976967424154282\n",
      "Training loss for batch 1562 : 0.054984983056783676\n",
      "Training loss for batch 1563 : 0.06496722251176834\n",
      "Training loss for batch 1564 : 0.017502179369330406\n",
      "Training loss for batch 1565 : 0.10593632608652115\n",
      "Training loss for batch 1566 : 0.02899109572172165\n",
      "Training loss for batch 1567 : 0.01271808985620737\n",
      "Training loss for batch 1568 : 0.0297299325466156\n",
      "Training loss for batch 1569 : 0.021259859204292297\n",
      "Training loss for batch 1570 : 0.1200607568025589\n",
      "Training loss for batch 1571 : 0.02449626661837101\n",
      "Training loss for batch 1572 : 0.07764297723770142\n",
      "Training loss for batch 1573 : 0.003334667533636093\n",
      "Training loss for batch 1574 : 0.029570750892162323\n",
      "Training loss for batch 1575 : 0.08109879493713379\n",
      "Training loss for batch 1576 : 0.1084938645362854\n",
      "Training loss for batch 1577 : 0.08749720454216003\n",
      "Training loss for batch 1578 : 0.06839750707149506\n",
      "Training loss for batch 1579 : 0.10297644883394241\n",
      "Training loss for batch 1580 : 0.11971575766801834\n",
      "Training loss for batch 1581 : 0.10972081124782562\n",
      "Training loss for batch 1582 : 0.038607627153396606\n",
      "Training loss for batch 1583 : 0.04992973059415817\n",
      "Training loss for batch 1584 : 0.05198075622320175\n",
      "Training loss for batch 1585 : 0.1815725564956665\n",
      "Training loss for batch 1586 : 0.06026725471019745\n",
      "Training loss for batch 1587 : 0.11955569684505463\n",
      "Training loss for batch 1588 : 0.08150401711463928\n",
      "Training loss for batch 1589 : 0.05893296003341675\n",
      "Training loss for batch 1590 : 0.1651046872138977\n",
      "Training loss for batch 1591 : 0.018664797767996788\n",
      "Training loss for batch 1592 : 0.0324159674346447\n",
      "Training loss for batch 1593 : 0.07986831665039062\n",
      "Training loss for batch 1594 : 0.06217939779162407\n",
      "Training loss for batch 1595 : 1.0505154790507731e-07\n",
      "Training loss for batch 1596 : 0.06011127308011055\n",
      "Training loss for batch 1597 : 0.0959722176194191\n",
      "Training loss for batch 1598 : 0.013395107351243496\n",
      "Training loss for batch 1599 : 0.04492732509970665\n",
      "Training loss for batch 1600 : 0.09820333123207092\n",
      "Training loss for batch 1601 : 0.21026894450187683\n",
      "Training loss for batch 1602 : 0.10625719279050827\n",
      "Training loss for batch 1603 : 0.08917123824357986\n",
      "Training loss for batch 1604 : 0.02391333505511284\n",
      "Training loss for batch 1605 : 0.051758285611867905\n",
      "Training loss for batch 1606 : 0.13165399432182312\n",
      "Training loss for batch 1607 : 0.019947420805692673\n",
      "Training loss for batch 1608 : 0.04381684958934784\n",
      "Training loss for batch 1609 : 0.03589688986539841\n",
      "Training loss for batch 1610 : 0.08509767055511475\n",
      "Training loss for batch 1611 : 0.05526936054229736\n",
      "Training loss for batch 1612 : 0.10858548432588577\n",
      "Training loss for batch 1613 : 0.020569952204823494\n",
      "Training loss for batch 1614 : 0.16210801899433136\n",
      "Training loss for batch 1615 : 0.022402163594961166\n",
      "Training loss for batch 1616 : 0.12547177076339722\n",
      "Training loss for batch 1617 : 0.051549654453992844\n",
      "Training loss for batch 1618 : 0.051153331995010376\n",
      "Training loss for batch 1619 : 0.06720691919326782\n",
      "Training loss for batch 1620 : 0.11419608443975449\n",
      "Training loss for batch 1621 : 0.05098855122923851\n",
      "Training loss for batch 1622 : 0.18814563751220703\n",
      "Training loss for batch 1623 : 0.07115007936954498\n",
      "Training loss for batch 1624 : 0.037643756717443466\n",
      "Training loss for batch 1625 : 0.10280388593673706\n",
      "Training loss for batch 1626 : 0.16724644601345062\n",
      "Training loss for batch 1627 : 0.03280036151409149\n",
      "Training loss for batch 1628 : 0.04363114386796951\n",
      "Training loss for batch 1629 : 0.10455132275819778\n",
      "Training loss for batch 1630 : 0.03541329875588417\n",
      "Training loss for batch 1631 : 0.07810067385435104\n",
      "Training loss for batch 1632 : 0.20367403328418732\n",
      "Training loss for batch 1633 : 0.17570289969444275\n",
      "Training loss for batch 1634 : 0.022004498168826103\n",
      "Training loss for batch 1635 : 0.14823414385318756\n",
      "Training loss for batch 1636 : 0.17661313712596893\n",
      "Training loss for batch 1637 : 0.0028366807382553816\n",
      "Training loss for batch 1638 : 0.03253612294793129\n",
      "Training loss for batch 1639 : 0.04330633208155632\n",
      "Training loss for batch 1640 : 0.05571755766868591\n",
      "Training loss for batch 1641 : 0.03197208046913147\n",
      "Training loss for batch 1642 : 0.12665043771266937\n",
      "Training loss for batch 1643 : 0.04018900543451309\n",
      "Training loss for batch 1644 : 0.036542560905218124\n",
      "Training loss for batch 1645 : 0.11148503422737122\n",
      "Training loss for batch 1646 : 0.1429854929447174\n",
      "Training loss for batch 1647 : 0.04736930504441261\n",
      "Training loss for batch 1648 : 0.04315223544836044\n",
      "Training loss for batch 1649 : 0.07220826297998428\n",
      "Training loss for batch 1650 : 0.03919040411710739\n",
      "Training loss for batch 1651 : 0.09205524623394012\n",
      "Training loss for batch 1652 : 0.09415542334318161\n",
      "Training loss for batch 1653 : 0.010123567655682564\n",
      "Training loss for batch 1654 : 0.013470973819494247\n",
      "Training loss for batch 1655 : 0.0723213478922844\n",
      "Training loss for batch 1656 : 0.10804374516010284\n",
      "Training loss for batch 1657 : 0.03421147167682648\n",
      "Training loss for batch 1658 : 0.04149411991238594\n",
      "Training loss for batch 1659 : 0.0626826286315918\n",
      "Training loss for batch 1660 : 0.07594551146030426\n",
      "Training loss for batch 1661 : 0.020546479150652885\n",
      "Training loss for batch 1662 : 0.03127756342291832\n",
      "Training loss for batch 1663 : 0.19442303478717804\n",
      "Training loss for batch 1664 : 0.06132059916853905\n",
      "Training loss for batch 1665 : 0.011860968545079231\n",
      "Training loss for batch 1666 : 0.05803867056965828\n",
      "Training loss for batch 1667 : 0.13098357617855072\n",
      "Training loss for batch 1668 : 0.04243387281894684\n",
      "Training loss for batch 1669 : 0.030884426087141037\n",
      "Training loss for batch 1670 : 0.06205027177929878\n",
      "Training loss for batch 1671 : 0.05085131153464317\n",
      "Training loss for batch 1672 : 0.05076116696000099\n",
      "Training loss for batch 1673 : 0.07249969244003296\n",
      "Training loss for batch 1674 : 0.08028381317853928\n",
      "Training loss for batch 1675 : 0.024752037599682808\n",
      "Training loss for batch 1676 : 0.10363216698169708\n",
      "Training loss for batch 1677 : 0.07470303028821945\n",
      "Training loss for batch 1678 : 0.03527771309018135\n",
      "Training loss for batch 1679 : 0.05079091712832451\n",
      "Training loss for batch 1680 : 0.08290746808052063\n",
      "Training loss for batch 1681 : 0.029617223888635635\n",
      "Training loss for batch 1682 : 0.10940466821193695\n",
      "Training loss for batch 1683 : 0.023512577638030052\n",
      "Training loss for batch 1684 : 0.016723064705729485\n",
      "Training loss for batch 1685 : 0.2239842712879181\n",
      "Training loss for batch 1686 : 0.027859479188919067\n",
      "Training loss for batch 1687 : 0.10413321107625961\n",
      "Training loss for batch 1688 : 0.13385798037052155\n",
      "Training loss for batch 1689 : 0.03364741802215576\n",
      "Training loss for batch 1690 : 0.0643741637468338\n",
      "Training loss for batch 1691 : 0.029185323044657707\n",
      "Training loss for batch 1692 : 0.09701225906610489\n",
      "Training loss for batch 1693 : 0.05058325082063675\n",
      "Training loss for batch 1694 : 0.09673740714788437\n",
      "Training loss for batch 1695 : 0.07872593402862549\n",
      "Training loss for batch 1696 : 0.09235242754220963\n",
      "Training loss for batch 1697 : 0.10042955726385117\n",
      "Training loss for batch 1698 : 0.1076224222779274\n",
      "Training loss for batch 1699 : 0.11985635757446289\n",
      "Training loss for batch 1700 : 0.01689743623137474\n",
      "Training loss for batch 1701 : 0.04603857174515724\n",
      "Training loss for batch 1702 : 0.06588815897703171\n",
      "Training loss for batch 1703 : 0.044989027082920074\n",
      "Training loss for batch 1704 : 0.13667532801628113\n",
      "Training loss for batch 1705 : 0.05801758915185928\n",
      "Training loss for batch 1706 : 0.12087985128164291\n",
      "Training loss for batch 1707 : 0.03877490758895874\n",
      "Training loss for batch 1708 : 0.18375082314014435\n",
      "Training loss for batch 1709 : 0.12095901370048523\n",
      "Training loss for batch 1710 : 0.11100675165653229\n",
      "Training loss for batch 1711 : 0.058673348277807236\n",
      "Training loss for batch 1712 : 0.0242813378572464\n",
      "Training loss for batch 1713 : 0.04336656630039215\n",
      "Training loss for batch 1714 : 0.1608363687992096\n",
      "Training loss for batch 1715 : 0.01762765273451805\n",
      "Training loss for batch 1716 : 0.0453253909945488\n",
      "Training loss for batch 1717 : 0.1265215426683426\n",
      "Training loss for batch 1718 : 0.10455816984176636\n",
      "Training loss for batch 1719 : 0.14843927323818207\n",
      "Training loss for batch 1720 : 0.062097158282995224\n",
      "Training loss for batch 1721 : 0.10286463052034378\n",
      "Training loss for batch 1722 : 0.04940782114863396\n",
      "Training loss for batch 1723 : 0.002923071850091219\n",
      "Training loss for batch 1724 : 0.12686856091022491\n",
      "Training loss for batch 1725 : 0.08801568299531937\n",
      "Training loss for batch 1726 : 0.08906850963830948\n",
      "Training loss for batch 1727 : 0.2226734161376953\n",
      "Training loss for batch 1728 : 0.11861689388751984\n",
      "Training loss for batch 1729 : 0.008062140084803104\n",
      "Training loss for batch 1730 : 0.08549114316701889\n",
      "Training loss for batch 1731 : 0.08769300580024719\n",
      "Training loss for batch 1732 : 0.017995143309235573\n",
      "Training loss for batch 1733 : 0.10534760355949402\n",
      "Training loss for batch 1734 : 0.13138820230960846\n",
      "Training loss for batch 1735 : 0.1495877206325531\n",
      "Training loss for batch 1736 : 0.03917217254638672\n",
      "Training loss for batch 1737 : 0.19532358646392822\n",
      "Training loss for batch 1738 : 0.03962467238306999\n",
      "Training loss for batch 1739 : 0.008237261325120926\n",
      "Training loss for batch 1740 : 0.15782976150512695\n",
      "Training loss for batch 1741 : 0.03419525548815727\n",
      "Training loss for batch 1742 : 0.09504151344299316\n",
      "Training loss for batch 1743 : 0.17162540555000305\n",
      "Training loss for batch 1744 : 0.14005330204963684\n",
      "Training loss for batch 1745 : 0.12312152236700058\n",
      "Training loss for batch 1746 : 0.07860307395458221\n",
      "Training loss for batch 1747 : 0.04933718219399452\n",
      "Training loss for batch 1748 : 0.05930691212415695\n",
      "Training loss for batch 1749 : 0.09340003877878189\n",
      "Training loss for batch 1750 : 0.04029855132102966\n",
      "Training loss for batch 1751 : 0.1112532839179039\n",
      "Training loss for batch 1752 : 0.09969231486320496\n",
      "Training loss for batch 1753 : 0.08616933226585388\n",
      "Training loss for batch 1754 : 0.0978562980890274\n",
      "Training loss for batch 1755 : 0.08658678829669952\n",
      "Training loss for batch 1756 : 0.16795825958251953\n",
      "Training loss for batch 1757 : 0.04598900303244591\n",
      "Training loss for batch 1758 : 0.06525930017232895\n",
      "Training loss for batch 1759 : 0.0734400674700737\n",
      "Training loss for batch 1760 : 0.05937729775905609\n",
      "Training loss for batch 1761 : 0.019081952050328255\n",
      "Training loss for batch 1762 : 0.09408111125230789\n",
      "Training loss for batch 1763 : 0.12441948801279068\n",
      "Training loss for batch 1764 : 0.13163934648036957\n",
      "Training loss for batch 1765 : 0.04355182871222496\n",
      "Training loss for batch 1766 : 0.12365216016769409\n",
      "Training loss for batch 1767 : 0.00022580008953809738\n",
      "Training loss for batch 1768 : 0.055751509964466095\n",
      "Training loss for batch 1769 : 0.06037024408578873\n",
      "Training loss for batch 1770 : 0.1916119009256363\n",
      "Training loss for batch 1771 : 0.07363320142030716\n",
      "Training loss for batch 1772 : 0.08313662558794022\n",
      "Training loss for batch 1773 : 0.0700995922088623\n",
      "Training loss for batch 1774 : 0.12746044993400574\n",
      "Training loss for batch 1775 : 0.047744087874889374\n",
      "Training loss for batch 1776 : 0.09869446605443954\n",
      "Training loss for batch 1777 : 0.20209063589572906\n",
      "Training loss for batch 1778 : 0.050271693617105484\n",
      "Training loss for batch 1779 : 0.09459096193313599\n",
      "Training loss for batch 1780 : 0.09929429739713669\n",
      "Training loss for batch 1781 : 0.016984840855002403\n",
      "Training loss for batch 1782 : 0.024324744939804077\n",
      "Training loss for batch 1783 : 0.08397385478019714\n",
      "Training loss for batch 1784 : 0.05713852494955063\n",
      "Training loss for batch 1785 : 0.07045003771781921\n",
      "Training loss for batch 1786 : 0.10742772370576859\n",
      "Training loss for batch 1787 : 0.05047667771577835\n",
      "Training loss for batch 1788 : 0.01930939592421055\n",
      "Training loss for batch 1789 : 0.07346223294734955\n",
      "Training loss for batch 1790 : 0.06811563670635223\n",
      "Training loss for batch 1791 : 0.032137345522642136\n",
      "Training loss for batch 1792 : 0.05468118563294411\n",
      "Training loss for batch 1793 : 0.18080982565879822\n",
      "Training loss for batch 1794 : 0.1069706454873085\n",
      "Training loss for batch 1795 : 0.09975610673427582\n",
      "Training loss for batch 1796 : 0.03908924385905266\n",
      "Training loss for batch 1797 : 0.05703943967819214\n",
      "Training loss for batch 1798 : 0.040672216564416885\n",
      "Training loss for batch 1799 : 0.05904490500688553\n",
      "Training loss for batch 1800 : 0.07331249117851257\n",
      "Training loss for batch 1801 : 0.10171724110841751\n",
      "Training loss for batch 1802 : 0.1309415102005005\n",
      "Training loss for batch 1803 : 0.057576052844524384\n",
      "Training loss for batch 1804 : 0.04135880619287491\n",
      "Training loss for batch 1805 : 0.06843291223049164\n",
      "Training loss for batch 1806 : 0.04324140399694443\n",
      "Training loss for batch 1807 : 0.010644570924341679\n",
      "Training loss for batch 1808 : 0.07427991181612015\n",
      "Training loss for batch 1809 : 0.053754404187202454\n",
      "Training loss for batch 1810 : 0.09405267983675003\n",
      "Training loss for batch 1811 : 0.015360736288130283\n",
      "Training loss for batch 1812 : 0.025328556075692177\n",
      "Training loss for batch 1813 : 0.14761129021644592\n",
      "Training loss for batch 1814 : 0.03814256936311722\n",
      "Training loss for batch 1815 : 0.07023531943559647\n",
      "Training loss for batch 1816 : 0.012412939220666885\n",
      "Training loss for batch 1817 : 0.0038107053842395544\n",
      "Training loss for batch 1818 : 0.01762974262237549\n",
      "Training loss for batch 1819 : 0.01833954267203808\n",
      "Training loss for batch 1820 : 0.09432349354028702\n",
      "Training loss for batch 1821 : 0.01881987601518631\n",
      "Training loss for batch 1822 : 0.15266333520412445\n",
      "Training loss for batch 1823 : 0.09605560451745987\n",
      "Training loss for batch 1824 : 0.05803598463535309\n",
      "Training loss for batch 1825 : 0.09780178219079971\n",
      "Training loss for batch 1826 : 0.08834685385227203\n",
      "Training loss for batch 1827 : 0.0022996379993855953\n",
      "Training loss for batch 1828 : 0.06582068651914597\n",
      "Training loss for batch 1829 : 0.12189354002475739\n",
      "Training loss for batch 1830 : 0.12441752105951309\n",
      "Training loss for batch 1831 : 0.08800389617681503\n",
      "Training loss for batch 1832 : 0.0501888245344162\n",
      "Training loss for batch 1833 : 0.038540858775377274\n",
      "Training loss for batch 1834 : 0.010072882287204266\n",
      "Training loss for batch 1835 : 0.048120930790901184\n",
      "Training loss for batch 1836 : 0.07525093108415604\n",
      "Training loss for batch 1837 : 0.027991537004709244\n",
      "Training loss for batch 1838 : 0.027019565925002098\n",
      "Training loss for batch 1839 : 0.05738873779773712\n",
      "Training loss for batch 1840 : 0.07018612325191498\n",
      "Training loss for batch 1841 : 0.09348496794700623\n",
      "Training loss for batch 1842 : 0.07001832127571106\n",
      "Training loss for batch 1843 : 0.03442923352122307\n",
      "Training loss for batch 1844 : 0.09404748678207397\n",
      "Training loss for batch 1845 : 0.11560362577438354\n",
      "Training loss for batch 1846 : 0.03194015473127365\n",
      "Training loss for batch 1847 : 0.017249157652258873\n",
      "Training loss for batch 1848 : 0.04595378413796425\n",
      "Training loss for batch 1849 : 0.04587201774120331\n",
      "Training loss for batch 1850 : 0.2221241146326065\n",
      "Training loss for batch 1851 : 0.054808758199214935\n",
      "Training loss for batch 1852 : 0.013941004872322083\n",
      "Training loss for batch 1853 : 0.09941381216049194\n",
      "Training loss for batch 1854 : 0.06964387744665146\n",
      "Training loss for batch 1855 : 0.1924222707748413\n",
      "Training loss for batch 1856 : 0.04320337995886803\n",
      "Training loss for batch 1857 : 6.414041854441166e-05\n",
      "Training loss for batch 1858 : 0.09425832331180573\n",
      "Training loss for batch 1859 : 0.042674411088228226\n",
      "Training loss for batch 1860 : 0.07004553079605103\n",
      "Training loss for batch 1861 : 0.00030778846121393144\n",
      "Training loss for batch 1862 : 0.0343489944934845\n",
      "Training loss for batch 1863 : 0.08980856835842133\n",
      "Training loss for batch 1864 : 0.031030109152197838\n",
      "Training loss for batch 1865 : 0.03695228323340416\n",
      "Training loss for batch 1866 : 0.1831490397453308\n",
      "Training loss for batch 1867 : 0.11025145649909973\n",
      "Training loss for batch 1868 : 0.0681367889046669\n",
      "Training loss for batch 1869 : 0.03521335497498512\n",
      "Training loss for batch 1870 : 0.15179169178009033\n",
      "Training loss for batch 1871 : 0.08259936422109604\n",
      "Training loss for batch 1872 : 0.027392663061618805\n",
      "Training loss for batch 1873 : 6.452058443073838e-08\n",
      "Training loss for batch 1874 : 0.07941629737615585\n",
      "Training loss for batch 1875 : 0.14932364225387573\n",
      "Training loss for batch 1876 : 0.00028455755091272295\n",
      "Training loss for batch 1877 : 0.20088177919387817\n",
      "Training loss for batch 1878 : 0.04309381544589996\n",
      "Training loss for batch 1879 : 0.12374378740787506\n",
      "Training loss for batch 1880 : 0.06522117555141449\n",
      "Training loss for batch 1881 : 0.10972569137811661\n",
      "Training loss for batch 1882 : 0.012890372425317764\n",
      "Training loss for batch 1883 : 0.20963601768016815\n",
      "Training loss for batch 1884 : 0.10083893686532974\n",
      "Training loss for batch 1885 : 0.06884744763374329\n",
      "Training loss for batch 1886 : 0.06563745439052582\n",
      "Training loss for batch 1887 : 0.050302427262067795\n",
      "Training loss for batch 1888 : 0.07538183778524399\n",
      "Training loss for batch 1889 : 0.05117810517549515\n",
      "Training loss for batch 1890 : 0.12360294908285141\n",
      "Training loss for batch 1891 : 0.05221395194530487\n",
      "Training loss for batch 1892 : 0.004711697343736887\n",
      "Training loss for batch 1893 : 0.03761319816112518\n",
      "Training loss for batch 1894 : 0.009635703638195992\n",
      "Training loss for batch 1895 : 0.06668791174888611\n",
      "Training loss for batch 1896 : 0.07022980600595474\n",
      "Training loss for batch 1897 : 0.022969746962189674\n",
      "Training loss for batch 1898 : 0.11479320377111435\n",
      "Training loss for batch 1899 : 0.10725310444831848\n",
      "Training loss for batch 1900 : 3.0115494098481577e-08\n",
      "Training loss for batch 1901 : 0.004659947007894516\n",
      "Training loss for batch 1902 : 0.012875374406576157\n",
      "Training loss for batch 1903 : 0.03388635069131851\n",
      "Training loss for batch 1904 : 0.07549887895584106\n",
      "Training loss for batch 1905 : 0.10923721641302109\n",
      "Training loss for batch 1906 : 0.06703194975852966\n",
      "Training loss for batch 1907 : 0.05146632343530655\n",
      "Training loss for batch 1908 : 0.2032972127199173\n",
      "Training loss for batch 1909 : 0.10493398457765579\n",
      "Training loss for batch 1910 : 0.0073729935102164745\n",
      "Training loss for batch 1911 : 0.009168362244963646\n",
      "Training loss for batch 1912 : 0.08072937279939651\n",
      "Training loss for batch 1913 : 0.05452636629343033\n",
      "Training loss for batch 1914 : 0.1404242217540741\n",
      "Training loss for batch 1915 : 0.04499881714582443\n",
      "Training loss for batch 1916 : 0.08809337019920349\n",
      "Training loss for batch 1917 : 0.0097562987357378\n",
      "Training loss for batch 1918 : 0.02651367336511612\n",
      "Training loss for batch 1919 : 0.19122782349586487\n",
      "Training loss for batch 1920 : 0.025169340893626213\n",
      "Training loss for batch 1921 : 0.20725899934768677\n",
      "Training loss for batch 1922 : 0.012573515996336937\n",
      "Training loss for batch 1923 : 0.06512826681137085\n",
      "Training loss for batch 1924 : 0.07993284612894058\n",
      "Training loss for batch 1925 : 0.09538654237985611\n",
      "Training loss for batch 1926 : 0.0944691076874733\n",
      "Training loss for batch 1927 : 0.14360205829143524\n",
      "Training loss for batch 1928 : 0.0574338324368\n",
      "Training loss for batch 1929 : 0.01688641682267189\n",
      "Training loss for batch 1930 : 0.009362413547933102\n",
      "Training loss for batch 1931 : 0.03439450263977051\n",
      "Training loss for batch 1932 : 0.07795243710279465\n",
      "Training loss for batch 1933 : 0.022548608481884003\n",
      "Training loss for batch 1934 : 0.02091020718216896\n",
      "Training loss for batch 1935 : 0.033743903040885925\n",
      "Training loss for batch 1936 : 0.09153766185045242\n",
      "Training loss for batch 1937 : 0.08997221291065216\n",
      "Training loss for batch 1938 : 0.01901589147746563\n",
      "Training loss for batch 1939 : 0.20141750574111938\n",
      "Training loss for batch 1940 : 0.1500704437494278\n",
      "Training loss for batch 1941 : 0.16926631331443787\n",
      "Training loss for batch 1942 : 0.0809946283698082\n",
      "Training loss for batch 1943 : 0.1512252241373062\n",
      "Training loss for batch 1944 : 0.18059614300727844\n",
      "Training loss for batch 1945 : 0.16106852889060974\n",
      "Training loss for batch 1946 : 0.027746601030230522\n",
      "Training loss for batch 1947 : 0.05643117055296898\n",
      "Training loss for batch 1948 : 0.08376550674438477\n",
      "Training loss for batch 1949 : 0.07203099876642227\n",
      "Training loss for batch 1950 : 0.12284631282091141\n",
      "Training loss for batch 1951 : 0.05736300349235535\n",
      "Training loss for batch 1952 : 0.095951609313488\n",
      "Training loss for batch 1953 : 0.031527865678071976\n",
      "Training loss for batch 1954 : 0.02891397289931774\n",
      "Training loss for batch 1955 : 0.06852638721466064\n",
      "Training loss for batch 1956 : 0.018923001363873482\n",
      "Training loss for batch 1957 : 0.1075417622923851\n",
      "Training loss for batch 1958 : 0.008282696828246117\n",
      "Training loss for batch 1959 : 0.11990219354629517\n",
      "Training loss for batch 1960 : 0.1072174683213234\n",
      "Training loss for batch 1961 : 0.10253752768039703\n",
      "Training loss for batch 1962 : 0.033505190163850784\n",
      "Training loss for batch 1963 : 0.04738375172019005\n",
      "Training loss for batch 1964 : 0.1768723428249359\n",
      "Training loss for batch 1965 : 0.1470758318901062\n",
      "Training loss for batch 1966 : 0.03398578241467476\n",
      "Training loss for batch 1967 : 0.06070363149046898\n",
      "Training loss for batch 1968 : 0.1366819143295288\n",
      "Training loss for batch 1969 : 0.07078801840543747\n",
      "Training loss for batch 1970 : 0.1880260854959488\n",
      "Training loss for batch 1971 : 0.10860250890254974\n",
      "Training loss for batch 1972 : 0.1499635875225067\n",
      "Training loss for batch 1973 : 0.03668280318379402\n",
      "Training loss for batch 1974 : 0.05761014297604561\n",
      "Training loss for batch 1975 : 0.004740342032164335\n",
      "Training loss for batch 1976 : 0.07895971834659576\n",
      "Training loss for batch 1977 : 0.21859224140644073\n",
      "Training loss for batch 1978 : 0.02629786729812622\n",
      "Training loss for batch 1979 : 0.036712273955345154\n",
      "Training loss for batch 1980 : 0.07835854589939117\n",
      "Training loss for batch 1981 : 0.13769811391830444\n",
      "Training loss for batch 1982 : 0.0346468947827816\n",
      "Training loss for batch 1983 : 0.03456106781959534\n",
      "Training loss for batch 1984 : 0.09156961739063263\n",
      "Training loss for batch 1985 : 0.10311231762170792\n",
      "Training loss for batch 1986 : 0.003918666858226061\n",
      "Training loss for batch 1987 : 0.0411415658891201\n",
      "Training loss for batch 1988 : 0.07597852498292923\n",
      "Training loss for batch 1989 : 0.09042053669691086\n",
      "Training loss for batch 1990 : 0.12680362164974213\n",
      "Training loss for batch 1991 : 0.06563982367515564\n",
      "Training loss for batch 1992 : 0.1694146692752838\n",
      "Training loss for batch 1993 : 0.0630171075463295\n",
      "Training loss for batch 1994 : 0.12483029067516327\n",
      "Training loss for batch 1995 : 0.04693344235420227\n",
      "Training loss for batch 1996 : 0.1562509685754776\n",
      "Training loss for batch 1997 : 0.0521065890789032\n",
      "Training loss for batch 1998 : 0.03899474814534187\n",
      "Training loss for batch 1999 : 0.028236521407961845\n",
      "Training loss for batch 2000 : 0.07853473722934723\n",
      "Training loss for batch 2001 : 0.04590728506445885\n",
      "Training loss for batch 2002 : 0.1292206197977066\n",
      "Training loss for batch 2003 : 0.17066523432731628\n",
      "Training loss for batch 2004 : 0.20941056311130524\n",
      "Training loss for batch 2005 : 0.039993949234485626\n",
      "Training loss for batch 2006 : 0.0392640084028244\n",
      "Training loss for batch 2007 : 0.020564034581184387\n",
      "Training loss for batch 2008 : 0.03173493593931198\n",
      "Training loss for batch 2009 : 0.041968293488025665\n",
      "Training loss for batch 2010 : 0.0883324071764946\n",
      "Training loss for batch 2011 : 0.09113341569900513\n",
      "Training loss for batch 2012 : 0.11832307279109955\n",
      "Training loss for batch 2013 : 0.18582899868488312\n",
      "Training loss for batch 2014 : 0.048547275364398956\n",
      "Training loss for batch 2015 : 0.08972718566656113\n",
      "Training loss for batch 2016 : 0.08142750710248947\n",
      "Training loss for batch 2017 : 0.042127370834350586\n",
      "Training loss for batch 2018 : 0.07370396703481674\n",
      "Training loss for batch 2019 : 0.07402866333723068\n",
      "Training loss for batch 2020 : 0.05249427631497383\n",
      "Training loss for batch 2021 : 0.07537859678268433\n",
      "Training loss for batch 2022 : 0.07129572331905365\n",
      "Training loss for batch 2023 : 0.03272286802530289\n",
      "Training loss for batch 2024 : 0.075388103723526\n",
      "Training loss for batch 2025 : 0.06456425040960312\n",
      "Training loss for batch 2026 : 0.04359707981348038\n",
      "Training loss for batch 2027 : 0.1832025945186615\n",
      "Training loss for batch 2028 : 0.0024866226594895124\n",
      "Training loss for batch 2029 : 0.026301102712750435\n",
      "Training loss for batch 2030 : 0.09540855139493942\n",
      "Training loss for batch 2031 : 0.12466776371002197\n",
      "Training loss for batch 2032 : 0.06164226308465004\n",
      "Training loss for batch 2033 : 0.11975549906492233\n",
      "Training loss for batch 2034 : 0.07410762459039688\n",
      "Training loss for batch 2035 : 0.04069509729743004\n",
      "Training loss for batch 2036 : 0.03595566749572754\n",
      "Training loss for batch 2037 : 0.14161549508571625\n",
      "Training loss for batch 2038 : 0.08775868266820908\n",
      "Training loss for batch 2039 : 0.10446368157863617\n",
      "Training loss for batch 2040 : 0.061981119215488434\n",
      "Training loss for batch 2041 : 0.03380578011274338\n",
      "Training loss for batch 2042 : 0.04806544631719589\n",
      "Training loss for batch 2043 : 0.20098376274108887\n",
      "Training loss for batch 2044 : 0.013306663371622562\n",
      "Training loss for batch 2045 : 0.11741335690021515\n",
      "Training loss for batch 2046 : 0.05966169387102127\n",
      "Training loss for batch 2047 : 0.10040674358606339\n",
      "Training loss for batch 2048 : 0.1303483247756958\n",
      "Training loss for batch 2049 : 0.01825016550719738\n",
      "Training loss for batch 2050 : 0.027666429057717323\n",
      "Training loss for batch 2051 : 0.0685194805264473\n",
      "Training loss for batch 2052 : 0.24787016212940216\n",
      "Training loss for batch 2053 : 0.08958974480628967\n",
      "Training loss for batch 2054 : 0.19819071888923645\n",
      "Training loss for batch 2055 : 0.06270243972539902\n",
      "Training loss for batch 2056 : 0.031544528901576996\n",
      "Training loss for batch 2057 : 0.011853436939418316\n",
      "Training loss for batch 2058 : 0.042706798762083054\n",
      "Training loss for batch 2059 : 0.18257606029510498\n",
      "Training loss for batch 2060 : 0.03308571130037308\n",
      "Training loss for batch 2061 : 0.11942917853593826\n",
      "Training loss for batch 2062 : 0.13379113376140594\n",
      "Training loss for batch 2063 : 0.008469254709780216\n",
      "Training loss for batch 2064 : 0.03231760859489441\n",
      "Training loss for batch 2065 : 0.0886974185705185\n",
      "Training loss for batch 2066 : 0.026144107803702354\n",
      "Training loss for batch 2067 : 0.17872467637062073\n",
      "Training loss for batch 2068 : 0.0066246394999325275\n",
      "Training loss for batch 2069 : 0.003318925853818655\n",
      "Training loss for batch 2070 : 0.06590020656585693\n",
      "Training loss for batch 2071 : 0.18841640651226044\n",
      "Training loss for batch 2072 : 0.023346569389104843\n",
      "Training loss for batch 2073 : 0.12641127407550812\n",
      "Training loss for batch 2074 : 0.04122111573815346\n",
      "Training loss for batch 2075 : 0.05491946265101433\n",
      "Training loss for batch 2076 : 0.12627790868282318\n",
      "Training loss for batch 2077 : 0.0025166363921016455\n",
      "Training loss for batch 2078 : 0.014057221822440624\n",
      "Training loss for batch 2079 : 0.21979421377182007\n",
      "Training loss for batch 2080 : 0.056907836347818375\n",
      "Training loss for batch 2081 : 0.006287380121648312\n",
      "Training loss for batch 2082 : 0.032328031957149506\n",
      "Training loss for batch 2083 : 0.12429729849100113\n",
      "Training loss for batch 2084 : 0.05167950689792633\n",
      "Training loss for batch 2085 : 0.045830752700567245\n",
      "Training loss for batch 2086 : 0.1120348572731018\n",
      "Training loss for batch 2087 : 0.06793944537639618\n",
      "Training loss for batch 2088 : 0.012623811140656471\n",
      "Training loss for batch 2089 : 0.027370581403374672\n",
      "Training loss for batch 2090 : 0.0619095154106617\n",
      "Training loss for batch 2091 : 0.19279338419437408\n",
      "Training loss for batch 2092 : 0.014510993845760822\n",
      "Training loss for batch 2093 : 0.009849783964455128\n",
      "Training loss for batch 2094 : 0.024594519287347794\n",
      "Training loss for batch 2095 : 0.07839540392160416\n",
      "Training loss for batch 2096 : 0.05999882146716118\n",
      "Training loss for batch 2097 : 0.02979796938598156\n",
      "Training loss for batch 2098 : 0.1060408279299736\n",
      "Training loss for batch 2099 : 0.024131419137120247\n",
      "Training loss for batch 2100 : 0.05142172425985336\n",
      "Training loss for batch 2101 : 0.27427396178245544\n",
      "Training loss for batch 2102 : 0.03448697552084923\n",
      "Training loss for batch 2103 : 0.09897433966398239\n",
      "Training loss for batch 2104 : 0.07145306468009949\n",
      "Training loss for batch 2105 : 0.09534260630607605\n",
      "Training loss for batch 2106 : 0.1362573802471161\n",
      "Training loss for batch 2107 : 0.03736165165901184\n",
      "Training loss for batch 2108 : 0.06463161110877991\n",
      "Training loss for batch 2109 : 0.025311509147286415\n",
      "Training loss for batch 2110 : 0.06574305891990662\n",
      "Training loss for batch 2111 : 0.07080154865980148\n",
      "Training loss for batch 2112 : 0.09649831056594849\n",
      "Training loss for batch 2113 : 0.02168438211083412\n",
      "Training loss for batch 2114 : 0.07457561045885086\n",
      "Training loss for batch 2115 : 0.06764332205057144\n",
      "Training loss for batch 2116 : 0.09425778687000275\n",
      "Training loss for batch 2117 : 0.013047974556684494\n",
      "Training loss for batch 2118 : 0.05636434257030487\n",
      "Training loss for batch 2119 : 0.06862784177064896\n",
      "Training loss for batch 2120 : 0.046430084854364395\n",
      "Training loss for batch 2121 : 0.08753062784671783\n",
      "Training loss for batch 2122 : 0.016772141680121422\n",
      "Training loss for batch 2123 : 0.012891463935375214\n",
      "Training loss for batch 2124 : 0.006939033977687359\n",
      "Training loss for batch 2125 : 0.11834796518087387\n",
      "Training loss for batch 2126 : 0.04575767740607262\n",
      "Training loss for batch 2127 : 0.1154288649559021\n",
      "Training loss for batch 2128 : 0.09061738848686218\n",
      "Training loss for batch 2129 : 0.043469227850437164\n",
      "Training loss for batch 2130 : 0.13722699880599976\n",
      "Training loss for batch 2131 : 0.09158036857843399\n",
      "Training loss for batch 2132 : 0.02198748290538788\n",
      "Training loss for batch 2133 : 0.020036742091178894\n",
      "Training loss for batch 2134 : 0.034947287291288376\n",
      "Training loss for batch 2135 : 0.032621219754219055\n",
      "Training loss for batch 2136 : 0.02557990327477455\n",
      "Training loss for batch 2137 : 0.06177405267953873\n",
      "Training loss for batch 2138 : 0.09733940660953522\n",
      "Training loss for batch 2139 : 0.03907995671033859\n",
      "Training loss for batch 2140 : 0.04875193163752556\n",
      "Training loss for batch 2141 : 0.0038465196266770363\n",
      "Training loss for batch 2142 : 0.15188157558441162\n",
      "Training loss for batch 2143 : 0.003617695299908519\n",
      "Training loss for batch 2144 : 0.0971553698182106\n",
      "Training loss for batch 2145 : 0.018666496500372887\n",
      "Training loss for batch 2146 : 0.04506171867251396\n",
      "Training loss for batch 2147 : 0.10340379923582077\n",
      "Training loss for batch 2148 : 0.15511001646518707\n",
      "Training loss for batch 2149 : 0.06946168094873428\n",
      "Training loss for batch 2150 : 0.003473257180303335\n",
      "Training loss for batch 2151 : 0.06356848776340485\n",
      "Training loss for batch 2152 : 0.17151807248592377\n",
      "Training loss for batch 2153 : 0.02672085538506508\n",
      "Training loss for batch 2154 : 0.16509032249450684\n",
      "Training loss for batch 2155 : 0.21261204779148102\n",
      "Training loss for batch 2156 : 0.06350011378526688\n",
      "Training loss for batch 2157 : 0.11755891144275665\n",
      "Training loss for batch 2158 : 0.19407887756824493\n",
      "Training loss for batch 2159 : 0.03430166468024254\n",
      "Training loss for batch 2160 : 0.09374551475048065\n",
      "Training loss for batch 2161 : 0.09713082760572433\n",
      "Training loss for batch 2162 : 0.008935555815696716\n",
      "Training loss for batch 2163 : 0.13079477846622467\n",
      "Training loss for batch 2164 : 0.05276172608137131\n",
      "Training loss for batch 2165 : 0.058540359139442444\n",
      "Training loss for batch 2166 : 0.02501627616584301\n",
      "Training loss for batch 2167 : 0.14551520347595215\n",
      "Training loss for batch 2168 : 0.03488263860344887\n",
      "Training loss for batch 2169 : 0.0015553086996078491\n",
      "Training loss for batch 2170 : 0.033686984330415726\n",
      "Training loss for batch 2171 : 0.08284331113100052\n",
      "Training loss for batch 2172 : 0.17578408122062683\n",
      "Training loss for batch 2173 : 0.06708942353725433\n",
      "Training loss for batch 2174 : 0.08249044418334961\n",
      "Training loss for batch 2175 : 0.16079933941364288\n",
      "Training loss for batch 2176 : 0.04083843156695366\n",
      "Training loss for batch 2177 : 0.09083837270736694\n",
      "Training loss for batch 2178 : 0.07180404663085938\n",
      "Training loss for batch 2179 : 0.10593216121196747\n",
      "Training loss for batch 2180 : 0.21111755073070526\n",
      "Training loss for batch 2181 : 0.08980386704206467\n",
      "Training loss for batch 2182 : 0.0006929281516931951\n",
      "Training loss for batch 2183 : 0.02369048073887825\n",
      "Training loss for batch 2184 : 0.05016171559691429\n",
      "Training loss for batch 2185 : 0.22259770333766937\n",
      "Training loss for batch 2186 : 0.1635429859161377\n",
      "Training loss for batch 2187 : 0.045056793838739395\n",
      "Training loss for batch 2188 : 0.1393403261899948\n",
      "Training loss for batch 2189 : 0.15125645697116852\n",
      "Training loss for batch 2190 : 0.1413027048110962\n",
      "Training loss for batch 2191 : 0.13221974670886993\n",
      "Training loss for batch 2192 : 0.1704511195421219\n",
      "Training loss for batch 2193 : 0.06355632841587067\n",
      "Training loss for batch 2194 : 0.07513558864593506\n",
      "Training loss for batch 2195 : 0.11228080093860626\n",
      "Training loss for batch 2196 : 0.06385575979948044\n",
      "Training loss for batch 2197 : 0.059346724301576614\n",
      "Training loss for batch 2198 : 0.04395989328622818\n",
      "Training loss for batch 2199 : 0.03373593091964722\n",
      "Training loss for batch 2200 : 0.21683605015277863\n",
      "Training loss for batch 2201 : 0.03341444209218025\n",
      "Training loss for batch 2202 : 0.09582005441188812\n",
      "Training loss for batch 2203 : 0.14353948831558228\n",
      "Training loss for batch 2204 : 0.024858642369508743\n",
      "Training loss for batch 2205 : 0.08290715515613556\n",
      "Training loss for batch 2206 : 0.07432880997657776\n",
      "Training loss for batch 2207 : 0.030561242252588272\n",
      "Training loss for batch 2208 : 0.0965820699930191\n",
      "Training loss for batch 2209 : 0.05712972953915596\n",
      "Training loss for batch 2210 : 0.019150134176015854\n",
      "Training loss for batch 2211 : 0.07226358354091644\n",
      "Training loss for batch 2212 : 0.010935433208942413\n",
      "Training loss for batch 2213 : 0.1034592017531395\n",
      "Training loss for batch 2214 : 0.0833636149764061\n",
      "Training loss for batch 2215 : 0.03381885588169098\n",
      "Training loss for batch 2216 : 0.13612565398216248\n",
      "Training loss for batch 2217 : 0.1728174388408661\n",
      "Training loss for batch 2218 : 0.022518333047628403\n",
      "Training loss for batch 2219 : 0.11652577668428421\n",
      "Training loss for batch 2220 : 0.0998673364520073\n",
      "Training loss for batch 2221 : 0.046187371015548706\n",
      "Training loss for batch 2222 : 0.027823150157928467\n",
      "Training loss for batch 2223 : 0.1538453847169876\n",
      "Training loss for batch 2224 : 0.04746318981051445\n",
      "Training loss for batch 2225 : 0.09713821113109589\n",
      "Training loss for batch 2226 : 0.0714963749051094\n",
      "Training loss for batch 2227 : 0.16864268481731415\n",
      "Training loss for batch 2228 : 0.032309286296367645\n",
      "Training loss for batch 2229 : 0.013919465243816376\n",
      "Training loss for batch 2230 : 0.03591946139931679\n",
      "Training loss for batch 2231 : 0.21023069322109222\n",
      "Training loss for batch 2232 : 0.0360657274723053\n",
      "Training loss for batch 2233 : 0.0674714520573616\n",
      "Training loss for batch 2234 : 0.0061402167193591595\n",
      "Training loss for batch 2235 : 0.07661449909210205\n",
      "Training loss for batch 2236 : 0.10817023366689682\n",
      "Training loss for batch 2237 : 0.15450029075145721\n",
      "Training loss for batch 2238 : 0.017235739156603813\n",
      "Training loss for batch 2239 : 0.013442838564515114\n",
      "Training loss for batch 2240 : 0.06390194594860077\n",
      "Training loss for batch 2241 : 0.10748374462127686\n",
      "Training loss for batch 2242 : 0.09766940772533417\n",
      "Training loss for batch 2243 : 0.1490417718887329\n",
      "Training loss for batch 2244 : 0.046667106449604034\n",
      "Training loss for batch 2245 : 0.06122833862900734\n",
      "Training loss for batch 2246 : 0.12656214833259583\n",
      "Training loss for batch 2247 : 0.0203732717782259\n",
      "Training loss for batch 2248 : 0.15804481506347656\n",
      "Training loss for batch 2249 : 0.10415332764387131\n",
      "Training loss for batch 2250 : 0.15375491976737976\n",
      "Training loss for batch 2251 : 0.011839129962027073\n",
      "Training loss for batch 2252 : 0.13418087363243103\n",
      "Training loss for batch 2253 : 0.1492442935705185\n",
      "Training loss for batch 2254 : 0.028827866539359093\n",
      "Training loss for batch 2255 : 0.13576017320156097\n",
      "Training loss for batch 2256 : 0.2117864340543747\n",
      "Training loss for batch 2257 : 0.00950570683926344\n",
      "Training loss for batch 2258 : 0.2512308657169342\n",
      "Training loss for batch 2259 : 0.04252859577536583\n",
      "Training loss for batch 2260 : 0.03382442146539688\n",
      "Training loss for batch 2261 : 0.17561478912830353\n",
      "Training loss for batch 2262 : 0.017076293006539345\n",
      "Training loss for batch 2263 : 0.06033671274781227\n",
      "Training loss for batch 2264 : 0.09071429818868637\n",
      "Training loss for batch 2265 : 0.05385313183069229\n",
      "Training loss for batch 2266 : 0.10711777210235596\n",
      "Training loss for batch 2267 : 0.09743840247392654\n",
      "Training loss for batch 2268 : 0.19974200427532196\n",
      "Training loss for batch 2269 : 0.09359031170606613\n",
      "Training loss for batch 2270 : 0.14568768441677094\n",
      "Training loss for batch 2271 : 0.0809139758348465\n",
      "Training loss for batch 2272 : 0.052715107798576355\n",
      "Training loss for batch 2273 : 0.15496826171875\n",
      "Training loss for batch 2274 : 0.021104099228978157\n",
      "Training loss for batch 2275 : 0.06474985927343369\n",
      "Training loss for batch 2276 : 0.06521221995353699\n",
      "Training loss for batch 2277 : 0.11284735053777695\n",
      "Training loss for batch 2278 : 0.044033538550138474\n",
      "Training loss for batch 2279 : 0.018778366968035698\n",
      "Training loss for batch 2280 : 0.1399488002061844\n",
      "Training loss for batch 2281 : 0.19803258776664734\n",
      "Training loss for batch 2282 : 0.07873440533876419\n",
      "Training loss for batch 2283 : 0.012875907123088837\n",
      "Training loss for batch 2284 : 0.052139826118946075\n",
      "Training loss for batch 2285 : 0.0813918337225914\n",
      "Training loss for batch 2286 : 0.03019152581691742\n",
      "Training loss for batch 2287 : 0.01723501831293106\n",
      "Training loss for batch 2288 : 0.10824119299650192\n",
      "Training loss for batch 2289 : 0.02011796645820141\n",
      "Training loss for batch 2290 : 0.035106975585222244\n",
      "Training loss for batch 2291 : 0.13353531062602997\n",
      "Training loss for batch 2292 : 0.052965205162763596\n",
      "Training loss for batch 2293 : 0.1592661738395691\n",
      "Training loss for batch 2294 : 0.08584608882665634\n",
      "Training loss for batch 2295 : 0.07502608746290207\n",
      "Training loss for batch 2296 : 0.15091808140277863\n",
      "Training loss for batch 2297 : 0.07366480678319931\n",
      "Training loss for batch 2298 : 0.03126269206404686\n",
      "Training loss for batch 2299 : 0.029531383886933327\n",
      "Training loss for batch 2300 : 0.12101183831691742\n",
      "Training loss for batch 2301 : 0.021404027938842773\n",
      "Training loss for batch 2302 : 0.12743735313415527\n",
      "Training loss for batch 2303 : 0.07087987661361694\n",
      "Training loss for batch 2304 : 0.04423101246356964\n",
      "Training loss for batch 2305 : 0.0\n",
      "Training loss for batch 2306 : 0.06639166176319122\n",
      "Training loss for batch 2307 : 0.18345467746257782\n",
      "Training loss for batch 2308 : 0.0027984234038740396\n",
      "Training loss for batch 2309 : 0.033259470015764236\n",
      "Training loss for batch 2310 : 0.1866401731967926\n",
      "Training loss for batch 2311 : 0.011438350193202496\n",
      "Training loss for batch 2312 : 0.09266963601112366\n",
      "Training loss for batch 2313 : 0.027402378618717194\n",
      "Training loss for batch 2314 : 0.06484261155128479\n",
      "Training loss for batch 2315 : 0.06662152707576752\n",
      "Training loss for batch 2316 : 0.04027524217963219\n",
      "Training loss for batch 2317 : 0.05231907591223717\n",
      "Training loss for batch 2318 : 0.0\n",
      "Training loss for batch 2319 : 0.11037078499794006\n",
      "Training loss for batch 2320 : 0.03571949526667595\n",
      "Training loss for batch 2321 : 0.1110910028219223\n",
      "Training loss for batch 2322 : 0.026775918900966644\n",
      "Training loss for batch 2323 : 0.05173754692077637\n",
      "Training loss for batch 2324 : 0.08403162658214569\n",
      "Training loss for batch 2325 : 0.19714966416358948\n",
      "Training loss for batch 2326 : 0.07943454384803772\n",
      "Training loss for batch 2327 : 0.058741990476846695\n",
      "Training loss for batch 2328 : 0.054648254066705704\n",
      "Training loss for batch 2329 : 0.04578467085957527\n",
      "Training loss for batch 2330 : 0.1576792597770691\n",
      "Training loss for batch 2331 : 0.23068030178546906\n",
      "Training loss for batch 2332 : 0.14530164003372192\n",
      "Training loss for batch 2333 : 0.054060790687799454\n",
      "Training loss for batch 2334 : 0.07242824882268906\n",
      "Training loss for batch 2335 : 0.10980981588363647\n",
      "Training loss for batch 2336 : 0.1634194701910019\n",
      "Training loss for batch 2337 : 0.0693109929561615\n",
      "Training loss for batch 2338 : 0.08993703871965408\n",
      "Training loss for batch 2339 : 0.12602302432060242\n",
      "Training loss for batch 2340 : 0.07166764885187149\n",
      "Training loss for batch 2341 : 0.01374139916151762\n",
      "Training loss for batch 2342 : 0.07221990078687668\n",
      "Training loss for batch 2343 : 0.022967267781496048\n",
      "Training loss for batch 2344 : 0.15612393617630005\n",
      "Training loss for batch 2345 : 0.17588545382022858\n",
      "Training loss for batch 2346 : 0.13015124201774597\n",
      "Training loss for batch 2347 : 0.11346644163131714\n",
      "Training loss for batch 2348 : 0.06171819567680359\n",
      "Training loss for batch 2349 : 0.03166090324521065\n",
      "Training loss for batch 2350 : 0.10804244875907898\n",
      "Training loss for batch 2351 : 0.03488065302371979\n",
      "Training loss for batch 2352 : 0.0907161608338356\n",
      "Training loss for batch 2353 : 0.1933738738298416\n",
      "Training loss for batch 2354 : 0.06451071053743362\n",
      "Training loss for batch 2355 : 0.02735835686326027\n",
      "Training loss for batch 2356 : 0.07076434791088104\n",
      "Training loss for batch 2357 : 0.09711325168609619\n",
      "Training loss for batch 2358 : 0.0504106767475605\n",
      "Training loss for batch 2359 : 0.0663151741027832\n",
      "Training loss for batch 2360 : 0.034761663526296616\n",
      "Training loss for batch 2361 : 0.04503185302019119\n",
      "Training loss for batch 2362 : 0.003862599143758416\n",
      "Training loss for batch 2363 : 0.04843439161777496\n",
      "Training loss for batch 2364 : 0.1736975908279419\n",
      "Training loss for batch 2365 : 0.004947912413626909\n",
      "Training loss for batch 2366 : 0.004964758642017841\n",
      "Training loss for batch 2367 : 0.061254095286130905\n",
      "Training loss for batch 2368 : 0.04418513923883438\n",
      "Training loss for batch 2369 : 0.01012224331498146\n",
      "Training loss for batch 2370 : 0.022521955892443657\n",
      "Training loss for batch 2371 : 0.200377956032753\n",
      "Training loss for batch 2372 : 1.7798726048567914e-08\n",
      "Training loss for batch 2373 : 0.039969298988580704\n",
      "Training loss for batch 2374 : 0.06634747982025146\n",
      "Training loss for batch 2375 : 0.05931713432073593\n",
      "Training loss for batch 2376 : 0.0014684415655210614\n",
      "Training loss for batch 2377 : 0.17743346095085144\n",
      "Training loss for batch 2378 : 0.12132423371076584\n",
      "Training loss for batch 2379 : 0.01541200466454029\n",
      "Training loss for batch 2380 : 0.05989458039402962\n",
      "Training loss for batch 2381 : 0.09334135800600052\n",
      "Training loss for batch 2382 : 0.022996794432401657\n",
      "Training loss for batch 2383 : 0.09558488428592682\n",
      "Training loss for batch 2384 : 0.06349842995405197\n",
      "Training loss for batch 2385 : 0.15906238555908203\n",
      "Training loss for batch 2386 : 0.001608586055226624\n",
      "Training loss for batch 2387 : 0.1080968827009201\n",
      "Training loss for batch 2388 : 0.20094585418701172\n",
      "Training loss for batch 2389 : 0.029294708743691444\n",
      "Training loss for batch 2390 : 0.07399572432041168\n",
      "Training loss for batch 2391 : 1.1042570235986204e-07\n",
      "Training loss for batch 2392 : 0.049085281789302826\n",
      "Training loss for batch 2393 : 0.003139724489301443\n",
      "Training loss for batch 2394 : 0.11156582832336426\n",
      "Training loss for batch 2395 : 0.12788145244121552\n",
      "Training loss for batch 2396 : 0.0402664951980114\n",
      "Training loss for batch 2397 : 0.08633828908205032\n",
      "Training loss for batch 2398 : 0.03777977451682091\n",
      "Training loss for batch 2399 : 0.29254457354545593\n",
      "Training loss for batch 2400 : 0.03126346692442894\n",
      "Training loss for batch 2401 : 0.14680014550685883\n",
      "Training loss for batch 2402 : 0.019580379128456116\n",
      "Training loss for batch 2403 : 0.03039977326989174\n",
      "Training loss for batch 2404 : 0.0647096037864685\n",
      "Training loss for batch 2405 : 0.08166024088859558\n",
      "Training loss for batch 2406 : 0.027955221012234688\n",
      "Training loss for batch 2407 : 0.06684032827615738\n",
      "Training loss for batch 2408 : 0.06612462550401688\n",
      "Training loss for batch 2409 : 0.06213473528623581\n",
      "Training loss for batch 2410 : 0.05335830897092819\n",
      "Training loss for batch 2411 : 0.12402050197124481\n",
      "Training loss for batch 2412 : 0.008493205532431602\n",
      "Training loss for batch 2413 : 0.07644522190093994\n",
      "Training loss for batch 2414 : 0.13436588644981384\n",
      "Training loss for batch 2415 : 0.12067379057407379\n",
      "Training loss for batch 2416 : 0.017463266849517822\n",
      "Training loss for batch 2417 : 0.0579577311873436\n",
      "Training loss for batch 2418 : 0.01554507203400135\n",
      "Training loss for batch 2419 : 0.042815499007701874\n",
      "Training loss for batch 2420 : 0.09345423430204391\n",
      "Training loss for batch 2421 : 0.019726505503058434\n",
      "Training loss for batch 2422 : 0.2015533298254013\n",
      "Training loss for batch 2423 : 0.09487409144639969\n",
      "Training loss for batch 2424 : 0.014763113111257553\n",
      "Training loss for batch 2425 : 0.1502843052148819\n",
      "Training loss for batch 2426 : 0.05148058012127876\n",
      "Training loss for batch 2427 : 0.015498751774430275\n",
      "Training loss for batch 2428 : 0.010391396470367908\n",
      "Training loss for batch 2429 : 0.1171472817659378\n",
      "Training loss for batch 2430 : 0.06125160679221153\n",
      "Training loss for batch 2431 : 0.11340540647506714\n",
      "Training loss for batch 2432 : 0.04238768666982651\n",
      "Training loss for batch 2433 : 0.05261813849210739\n",
      "Training loss for batch 2434 : 0.027027340605854988\n",
      "Training loss for batch 2435 : 0.16006051003932953\n",
      "Training loss for batch 2436 : 0.03371527045965195\n",
      "Training loss for batch 2437 : 0.02366727963089943\n",
      "Training loss for batch 2438 : 0.01191507838666439\n",
      "Training loss for batch 2439 : 0.2143368124961853\n",
      "Training loss for batch 2440 : 0.14572732150554657\n",
      "Training loss for batch 2441 : 0.042682018131017685\n",
      "Training loss for batch 2442 : 0.022350572049617767\n",
      "Training loss for batch 2443 : 0.15498612821102142\n",
      "Training loss for batch 2444 : 0.07315818965435028\n",
      "Training loss for batch 2445 : 0.20752201974391937\n",
      "Training loss for batch 2446 : 0.1308862715959549\n",
      "Training loss for batch 2447 : 0.08287977427244186\n",
      "Training loss for batch 2448 : 0.01837722212076187\n",
      "Training loss for batch 2449 : 0.024257762357592583\n",
      "Training loss for batch 2450 : 0.18481981754302979\n",
      "Training loss for batch 2451 : 0.1001828983426094\n",
      "Training loss for batch 2452 : 0.02273053675889969\n",
      "Training loss for batch 2453 : 0.02147369645535946\n",
      "Training loss for batch 2454 : 0.08859354257583618\n",
      "Training loss for batch 2455 : 0.0328005775809288\n",
      "Training loss for batch 2456 : 0.1843409240245819\n",
      "Training loss for batch 2457 : 0.04097020626068115\n",
      "Training loss for batch 2458 : 0.06967030465602875\n",
      "Training loss for batch 2459 : 0.02464195527136326\n",
      "Training loss for batch 2460 : 0.024856753647327423\n",
      "Training loss for batch 2461 : 0.044109102338552475\n",
      "Training loss for batch 2462 : 0.05546224117279053\n",
      "Training loss for batch 2463 : 0.018191887065768242\n",
      "Training loss for batch 2464 : 0.24765188992023468\n",
      "Training loss for batch 2465 : 0.03892246633768082\n",
      "Training loss for batch 2466 : 0.11503544449806213\n",
      "Training loss for batch 2467 : 0.0\n",
      "Training loss for batch 2468 : 0.09552743285894394\n",
      "Training loss for batch 2469 : 0.0713234469294548\n",
      "Training loss for batch 2470 : 0.1809493750333786\n",
      "Training loss for batch 2471 : 0.0\n",
      "Training loss for batch 2472 : 0.20228460431098938\n",
      "Training loss for batch 2473 : 0.08492894470691681\n",
      "Training loss for batch 2474 : 0.07729021459817886\n",
      "Training loss for batch 2475 : 0.20550082623958588\n",
      "Training loss for batch 2476 : 0.045661382377147675\n",
      "Training loss for batch 2477 : 0.0\n",
      "Training loss for batch 2478 : 0.05193711444735527\n",
      "Training loss for batch 2479 : 0.14365077018737793\n",
      "Training loss for batch 2480 : 0.07599685341119766\n",
      "Training loss for batch 2481 : 0.09547502547502518\n",
      "Training loss for batch 2482 : 0.020602000877261162\n",
      "Training loss for batch 2483 : 0.04231918230652809\n",
      "Training loss for batch 2484 : 0.02685919962823391\n",
      "Training loss for batch 2485 : 0.08249630033969879\n",
      "Training loss for batch 2486 : 0.06193786486983299\n",
      "Training loss for batch 2487 : 0.15309971570968628\n",
      "Training loss for batch 2488 : 0.0\n",
      "Training loss for batch 2489 : 0.08305464684963226\n",
      "Training loss for batch 2490 : 0.14942920207977295\n",
      "Training loss for batch 2491 : 0.09525436162948608\n",
      "Training loss for batch 2492 : 0.04279844090342522\n",
      "Training loss for batch 2493 : 0.10309997946023941\n",
      "Training loss for batch 2494 : 0.07418596744537354\n",
      "Training loss for batch 2495 : 0.08105870336294174\n",
      "Training loss for batch 2496 : 0.07271403074264526\n",
      "Training loss for batch 2497 : 0.06396669149398804\n",
      "Training loss for batch 2498 : 0.094101183116436\n",
      "Training loss for batch 2499 : 0.20485781133174896\n",
      "Training loss for batch 2500 : 0.08692282438278198\n",
      "Training loss for batch 2501 : 0.16771255433559418\n",
      "Training loss for batch 2502 : 0.04290090501308441\n",
      "Training loss for batch 2503 : 0.019661031663417816\n",
      "Training loss for batch 2504 : 0.10176317393779755\n",
      "Training loss for batch 2505 : 0.02457832545042038\n",
      "Training loss for batch 2506 : 0.06410053372383118\n",
      "Training loss for batch 2507 : 0.005327147431671619\n",
      "Training loss for batch 2508 : 0.08132150024175644\n",
      "Training loss for batch 2509 : 0.014982923865318298\n",
      "Training loss for batch 2510 : 0.11057595908641815\n",
      "Training loss for batch 2511 : 0.08932992815971375\n",
      "Training loss for batch 2512 : 0.19611001014709473\n",
      "Training loss for batch 2513 : 0.02037074789404869\n",
      "Training loss for batch 2514 : 0.07513965666294098\n",
      "Training loss for batch 2515 : 0.018081817775964737\n",
      "Training loss for batch 2516 : 0.029617872089147568\n",
      "Training loss for batch 2517 : 0.023219360038638115\n",
      "Training loss for batch 2518 : 0.09121812880039215\n",
      "Training loss for batch 2519 : 0.12587518990039825\n",
      "Training loss for batch 2520 : 0.004006680101156235\n",
      "Training loss for batch 2521 : 0.1174086183309555\n",
      "Training loss for batch 2522 : 0.04374856874346733\n",
      "Training loss for batch 2523 : 0.03488945960998535\n",
      "Training loss for batch 2524 : 0.12016462534666061\n",
      "Training loss for batch 2525 : 0.10145047307014465\n",
      "Training loss for batch 2526 : 0.12771742045879364\n",
      "Training loss for batch 2527 : 0.08333779871463776\n",
      "Training loss for batch 2528 : 0.0748339369893074\n",
      "Training loss for batch 2529 : 0.03862142190337181\n",
      "Training loss for batch 2530 : 0.09609740972518921\n",
      "Training loss for batch 2531 : 0.11962897330522537\n",
      "Training loss for batch 2532 : 0.11914760619401932\n",
      "Training loss for batch 2533 : 0.09259006381034851\n",
      "Training loss for batch 2534 : 0.16379022598266602\n",
      "Training loss for batch 2535 : 0.018242117017507553\n",
      "Training loss for batch 2536 : 0.08421768993139267\n",
      "Training loss for batch 2537 : 0.13527987897396088\n",
      "Training loss for batch 2538 : 0.02813807874917984\n",
      "Training loss for batch 2539 : 0.09408103674650192\n",
      "Training loss for batch 2540 : 0.13990890979766846\n",
      "Training loss for batch 2541 : 0.05054555833339691\n",
      "Training loss for batch 2542 : 0.052664246410131454\n",
      "Training loss for batch 2543 : 0.01779242977499962\n",
      "Training loss for batch 2544 : 0.024311572313308716\n",
      "Training loss for batch 2545 : 0.048425789922475815\n",
      "Training loss for batch 2546 : 0.22477759420871735\n",
      "Training loss for batch 2547 : 0.023804496973752975\n",
      "Training loss for batch 2548 : 0.047563135623931885\n",
      "Training loss for batch 2549 : 0.020670762285590172\n",
      "Training loss for batch 2550 : 0.05397019907832146\n",
      "Training loss for batch 2551 : 0.0854826346039772\n",
      "Training loss for batch 2552 : 0.23946207761764526\n",
      "Training loss for batch 2553 : 0.002776890993118286\n",
      "Training loss for batch 2554 : 0.12202245742082596\n",
      "Training loss for batch 2555 : 0.07034461200237274\n",
      "Training loss for batch 2556 : 0.02847236767411232\n",
      "Training loss for batch 2557 : 0.12148695439100266\n",
      "Training loss for batch 2558 : 0.08401024341583252\n",
      "Training loss for batch 2559 : 0.14366810023784637\n",
      "Training loss for batch 2560 : 0.07784289121627808\n",
      "Training loss for batch 2561 : 0.03508793190121651\n",
      "Training loss for batch 2562 : 0.008561378344893456\n",
      "Training loss for batch 2563 : 0.07032620161771774\n",
      "Training loss for batch 2564 : 0.0463622622191906\n",
      "Training loss for batch 2565 : 0.19900181889533997\n",
      "Training loss for batch 2566 : 0.014670025557279587\n",
      "Training loss for batch 2567 : 0.1312033087015152\n",
      "Training loss for batch 2568 : 0.018150774762034416\n",
      "Training loss for batch 2569 : 0.059596553444862366\n",
      "Training loss for batch 2570 : 0.14480751752853394\n",
      "Training loss for batch 2571 : 0.0609554722905159\n",
      "Training loss for batch 2572 : 0.12759418785572052\n",
      "Training loss for batch 2573 : 0.02068798802793026\n",
      "Training loss for batch 2574 : 0.052653416991233826\n",
      "Training loss for batch 2575 : 0.07565523684024811\n",
      "Training loss for batch 2576 : 0.08566690981388092\n",
      "Training loss for batch 2577 : 0.05056266486644745\n",
      "Training loss for batch 2578 : 0.029603807255625725\n",
      "Training loss for batch 2579 : 0.1192825511097908\n",
      "Training loss for batch 2580 : 0.09344174712896347\n",
      "Training loss for batch 2581 : 0.038194235414266586\n",
      "Training loss for batch 2582 : 0.09564433991909027\n",
      "Training loss for batch 2583 : 0.02598445490002632\n",
      "Training loss for batch 2584 : 0.005214052740484476\n",
      "Training loss for batch 2585 : 0.04410756006836891\n",
      "Training loss for batch 2586 : 0.0889449417591095\n",
      "Training loss for batch 2587 : 0.014956659637391567\n",
      "Training loss for batch 2588 : 0.0478573776781559\n",
      "Training loss for batch 2589 : 0.09672579169273376\n",
      "Training loss for batch 2590 : 0.05303791165351868\n",
      "Training loss for batch 2591 : 0.010036681778728962\n",
      "Training loss for batch 2592 : 0.09242662042379379\n",
      "Training loss for batch 2593 : 0.012318914756178856\n",
      "Training loss for batch 2594 : 0.1549772322177887\n",
      "Training loss for batch 2595 : 0.057527218014001846\n",
      "Training loss for batch 2596 : 0.008013619109988213\n",
      "Training loss for batch 2597 : 0.07695600390434265\n",
      "Training loss for batch 2598 : 0.005473675671964884\n",
      "Training loss for batch 2599 : 0.07462114840745926\n",
      "Training loss for batch 2600 : 0.08000534772872925\n",
      "Training loss for batch 2601 : 0.01237616129219532\n",
      "Training loss for batch 2602 : 0.13164006173610687\n",
      "Training loss for batch 2603 : 0.03329768404364586\n",
      "Training loss for batch 2604 : 0.04087649658322334\n",
      "Training loss for batch 2605 : 0.13489653170108795\n",
      "Training loss for batch 2606 : 0.06558407098054886\n",
      "Training loss for batch 2607 : 0.13750892877578735\n",
      "Training loss for batch 2608 : 0.09637860208749771\n",
      "Training loss for batch 2609 : 3.689694594299908e-08\n",
      "Training loss for batch 2610 : 0.19481506943702698\n",
      "Training loss for batch 2611 : 0.06513363122940063\n",
      "Training loss for batch 2612 : 0.21760980784893036\n",
      "Training loss for batch 2613 : 0.025224028155207634\n",
      "Training loss for batch 2614 : 0.03398287296295166\n",
      "Training loss for batch 2615 : 0.0772523507475853\n",
      "Training loss for batch 2616 : 0.040300771594047546\n",
      "Training loss for batch 2617 : 0.049284279346466064\n",
      "Training loss for batch 2618 : 0.16212914884090424\n",
      "Training loss for batch 2619 : 0.16782072186470032\n",
      "Training loss for batch 2620 : 0.006822465918958187\n",
      "Training loss for batch 2621 : 0.03072519600391388\n",
      "Training loss for batch 2622 : 0.04168838635087013\n",
      "Training loss for batch 2623 : 0.07473576813936234\n",
      "Training loss for batch 2624 : 0.06363043189048767\n",
      "Training loss for batch 2625 : 0.01889846660196781\n",
      "Training loss for batch 2626 : 0.036389295011758804\n",
      "Training loss for batch 2627 : 0.09423068165779114\n",
      "Training loss for batch 2628 : 0.09274694323539734\n",
      "Training loss for batch 2629 : 0.1257753074169159\n",
      "Training loss for batch 2630 : 0.2626284658908844\n",
      "Training loss for batch 2631 : 0.07474541664123535\n",
      "Training loss for batch 2632 : 0.05198173597455025\n",
      "Training loss for batch 2633 : 0.02744792401790619\n",
      "Training loss for batch 2634 : 0.018855949863791466\n",
      "Training loss for batch 2635 : 0.09631826728582382\n",
      "Training loss for batch 2636 : 0.05682193860411644\n",
      "Training loss for batch 2637 : 0.05752355232834816\n",
      "Training loss for batch 2638 : 0.02539055608212948\n",
      "Training loss for batch 2639 : 0.23278702795505524\n",
      "Training loss for batch 2640 : 0.044278137385845184\n",
      "Training loss for batch 2641 : 0.02047516591846943\n",
      "Training loss for batch 2642 : 0.029346702620387077\n",
      "Training loss for batch 2643 : 0.02913202904164791\n",
      "Training loss for batch 2644 : 0.08685199916362762\n",
      "Training loss for batch 2645 : 0.11394502967596054\n",
      "Training loss for batch 2646 : 0.10247929394245148\n",
      "Training loss for batch 2647 : 0.212187722325325\n",
      "Training loss for batch 2648 : 0.09809817373752594\n",
      "Training loss for batch 2649 : 0.010932689532637596\n",
      "Training loss for batch 2650 : 0.06726466119289398\n",
      "Training loss for batch 2651 : 0.013980931602418423\n",
      "Training loss for batch 2652 : 0.07712971419095993\n",
      "Training loss for batch 2653 : 0.0361870639026165\n",
      "Training loss for batch 2654 : 0.13851819932460785\n",
      "Training loss for batch 2655 : 0.15032294392585754\n",
      "Training loss for batch 2656 : 0.025981532409787178\n",
      "Training loss for batch 2657 : 0.13392353057861328\n",
      "Training loss for batch 2658 : 0.007327680941671133\n",
      "Training loss for batch 2659 : 0.10027964413166046\n",
      "Training loss for batch 2660 : 0.12935540080070496\n",
      "Training loss for batch 2661 : 0.11042261123657227\n",
      "Training loss for batch 2662 : 0.1246718317270279\n",
      "Training loss for batch 2663 : 0.08195716887712479\n",
      "Training loss for batch 2664 : 0.05422663688659668\n",
      "Training loss for batch 2665 : 0.04333464801311493\n",
      "Training loss for batch 2666 : 0.03910985589027405\n",
      "Training loss for batch 2667 : 0.023934368044137955\n",
      "Training loss for batch 2668 : 0.019207658246159554\n",
      "Training loss for batch 2669 : 0.017865270376205444\n",
      "Training loss for batch 2670 : 0.16344211995601654\n",
      "Training loss for batch 2671 : 0.11002875119447708\n",
      "Training loss for batch 2672 : 0.05523080751299858\n",
      "Training loss for batch 2673 : 0.04127948358654976\n",
      "Training loss for batch 2674 : 0.25137823820114136\n",
      "Training loss for batch 2675 : 0.14733806252479553\n",
      "Training loss for batch 2676 : 0.06078343093395233\n",
      "Training loss for batch 2677 : 0.015896042808890343\n",
      "Training loss for batch 2678 : 0.0278796199709177\n",
      "Training loss for batch 2679 : 0.05600595101714134\n",
      "Training loss for batch 2680 : 0.11862827837467194\n",
      "Training loss for batch 2681 : 0.028906969353556633\n",
      "Training loss for batch 2682 : 0.14244921505451202\n",
      "Training loss for batch 2683 : 0.03738633915781975\n",
      "Training loss for batch 2684 : 0.01519134733825922\n",
      "Training loss for batch 2685 : 0.13797469437122345\n",
      "Training loss for batch 2686 : 0.039858128875494\n",
      "Training loss for batch 2687 : 3.779264901027091e-08\n",
      "Training loss for batch 2688 : 0.11002123355865479\n",
      "Training loss for batch 2689 : 0.15795789659023285\n",
      "Training loss for batch 2690 : 0.1422499418258667\n",
      "Training loss for batch 2691 : 0.15811921656131744\n",
      "Training loss for batch 2692 : 0.1421404480934143\n",
      "Training loss for batch 2693 : 0.07681657373905182\n",
      "Training loss for batch 2694 : 0.21221411228179932\n",
      "Training loss for batch 2695 : 0.001073962775990367\n",
      "Training loss for batch 2696 : 0.12240460515022278\n",
      "Training loss for batch 2697 : 0.01203966699540615\n",
      "Training loss for batch 2698 : 0.2142890989780426\n",
      "Training loss for batch 2699 : 0.038372866809368134\n",
      "Training loss for batch 2700 : 0.003399427980184555\n",
      "Training loss for batch 2701 : 0.1764814853668213\n",
      "Training loss for batch 2702 : 0.016018075868487358\n",
      "Training loss for batch 2703 : 0.020739810541272163\n",
      "Training loss for batch 2704 : 0.0025137364864349365\n",
      "Training loss for batch 2705 : 0.053760528564453125\n",
      "Training loss for batch 2706 : 0.2764544188976288\n",
      "Training loss for batch 2707 : 0.17379620671272278\n",
      "Training loss for batch 2708 : 0.059456873685121536\n",
      "Training loss for batch 2709 : 0.19499608874320984\n",
      "Training loss for batch 2710 : 0.00635136105120182\n",
      "Training loss for batch 2711 : 0.057937141507864\n",
      "Training loss for batch 2712 : 0.12251950800418854\n",
      "Training loss for batch 2713 : 0.08167929947376251\n",
      "Training loss for batch 2714 : 0.06512603908777237\n",
      "Training loss for batch 2715 : 0.017709894105792046\n",
      "Training loss for batch 2716 : 0.1865355670452118\n",
      "Training loss for batch 2717 : 0.06724019348621368\n",
      "Training loss for batch 2718 : 0.010524467565119267\n",
      "Training loss for batch 2719 : 0.1514824479818344\n",
      "Training loss for batch 2720 : 0.16869975626468658\n",
      "Training loss for batch 2721 : 0.045892201364040375\n",
      "Training loss for batch 2722 : 0.00016674424114171416\n",
      "Training loss for batch 2723 : 0.03173884004354477\n",
      "Training loss for batch 2724 : 0.08501198142766953\n",
      "Training loss for batch 2725 : 0.0469876267015934\n",
      "Training loss for batch 2726 : 0.20558679103851318\n",
      "Training loss for batch 2727 : 6.857018161099404e-05\n",
      "Training loss for batch 2728 : 0.03104313649237156\n",
      "Training loss for batch 2729 : 0.03352104872465134\n",
      "Training loss for batch 2730 : 0.025387674570083618\n",
      "Training loss for batch 2731 : 0.08927102386951447\n",
      "Training loss for batch 2732 : 0.16870658099651337\n",
      "Training loss for batch 2733 : 0.06900092959403992\n",
      "Training loss for batch 2734 : 0.02430696412920952\n",
      "Training loss for batch 2735 : 0.09055826812982559\n",
      "Training loss for batch 2736 : 0.1297019124031067\n",
      "Training loss for batch 2737 : 0.012193134054541588\n",
      "Training loss for batch 2738 : 0.07617168128490448\n",
      "Training loss for batch 2739 : 0.01886017434298992\n",
      "Training loss for batch 2740 : 0.05807504057884216\n",
      "Training loss for batch 2741 : 0.00881450716406107\n",
      "Training loss for batch 2742 : 0.05282455310225487\n",
      "Training loss for batch 2743 : 0.16094787418842316\n",
      "Training loss for batch 2744 : 0.0\n",
      "Training loss for batch 2745 : 0.08857601881027222\n",
      "Training loss for batch 2746 : 0.08821792155504227\n",
      "Training loss for batch 2747 : 0.14745482802391052\n",
      "Training loss for batch 2748 : 0.04780736193060875\n",
      "Training loss for batch 2749 : 0.12779437005519867\n",
      "Training loss for batch 2750 : 0.1651691198348999\n",
      "Training loss for batch 2751 : 0.08962182700634003\n",
      "Training loss for batch 2752 : 0.2391396313905716\n",
      "Training loss for batch 2753 : 0.02996458299458027\n",
      "Training loss for batch 2754 : 0.14794234931468964\n",
      "Training loss for batch 2755 : 0.11078178137540817\n",
      "Training loss for batch 2756 : 0.06473056972026825\n",
      "Training loss for batch 2757 : 0.044186223298311234\n",
      "Training loss for batch 2758 : 0.19321762025356293\n",
      "Training loss for batch 2759 : 0.23488594591617584\n",
      "Training loss for batch 2760 : 0.06349470466375351\n",
      "Training loss for batch 2761 : 0.030763093382120132\n",
      "Training loss for batch 2762 : 0.12770619988441467\n",
      "Training loss for batch 2763 : 0.016890058293938637\n",
      "Training loss for batch 2764 : 0.0658162534236908\n",
      "Training loss for batch 2765 : 0.054307375103235245\n",
      "Training loss for batch 2766 : 0.027545327320694923\n",
      "Training loss for batch 2767 : 0.08857181668281555\n",
      "Training loss for batch 2768 : 0.07445354014635086\n",
      "Training loss for batch 2769 : 0.025871818885207176\n",
      "Training loss for batch 2770 : 0.19286580383777618\n",
      "Training loss for batch 2771 : 0.06981714814901352\n",
      "Training loss for batch 2772 : 0.07916323095560074\n",
      "Training loss for batch 2773 : 0.032207220792770386\n",
      "Training loss for batch 2774 : 0.08230958133935928\n",
      "Training loss for batch 2775 : 0.10584480315446854\n",
      "Training loss for batch 2776 : 0.029224934056401253\n",
      "Training loss for batch 2777 : 0.018975866958498955\n",
      "Training loss for batch 2778 : 0.044044848531484604\n",
      "Training loss for batch 2779 : 0.04911383241415024\n",
      "Training loss for batch 2780 : 0.00358399236574769\n",
      "Training loss for batch 2781 : 0.02940140664577484\n",
      "Training loss for batch 2782 : 0.06399662792682648\n",
      "Training loss for batch 2783 : 0.06895331293344498\n",
      "Training loss for batch 2784 : 0.21080267429351807\n",
      "Training loss for batch 2785 : 0.03785701096057892\n",
      "Training loss for batch 2786 : 0.1127898320555687\n",
      "Training loss for batch 2787 : 0.013272671028971672\n",
      "Training loss for batch 2788 : 0.09375554323196411\n",
      "Training loss for batch 2789 : 0.045841049402952194\n",
      "Training loss for batch 2790 : 0.02223219722509384\n",
      "Training loss for batch 2791 : 0.046472739428281784\n",
      "Training loss for batch 2792 : 0.16399283707141876\n",
      "Training loss for batch 2793 : 0.18626634776592255\n",
      "Training loss for batch 2794 : 0.08249587565660477\n",
      "Training loss for batch 2795 : 0.08234778046607971\n",
      "Training loss for batch 2796 : 0.05709410831332207\n",
      "Training loss for batch 2797 : 0.10038135945796967\n",
      "Training loss for batch 2798 : 0.13226379454135895\n",
      "Training loss for batch 2799 : 0.12030927836894989\n",
      "Training loss for batch 2800 : 0.0318208783864975\n",
      "Training loss for batch 2801 : 0.09177819639444351\n",
      "Training loss for batch 2802 : 0.02421235293149948\n",
      "Training loss for batch 2803 : 0.10497699677944183\n",
      "Training loss for batch 2804 : 0.09105419367551804\n",
      "Training loss for batch 2805 : 0.05469458922743797\n",
      "Training loss for batch 2806 : 0.07195311039686203\n",
      "Training loss for batch 2807 : 0.0031971835996955633\n",
      "Training loss for batch 2808 : 0.23071938753128052\n",
      "Training loss for batch 2809 : 0.14716827869415283\n",
      "Training loss for batch 2810 : 0.07163595408201218\n",
      "Training loss for batch 2811 : 0.05713966488838196\n",
      "Training loss for batch 2812 : 1.835068097477688e-08\n",
      "Training loss for batch 2813 : 0.09496838599443436\n",
      "Training loss for batch 2814 : 0.20056568086147308\n",
      "Training loss for batch 2815 : 0.2553926706314087\n",
      "Training loss for batch 2816 : 0.06797836720943451\n",
      "Training loss for batch 2817 : 0.09947603940963745\n",
      "Training loss for batch 2818 : 0.08437588065862656\n",
      "Training loss for batch 2819 : 0.008540828712284565\n",
      "Training loss for batch 2820 : 0.06965041905641556\n",
      "Training loss for batch 2821 : 0.1646016389131546\n",
      "Training loss for batch 2822 : 0.12213396281003952\n",
      "Training loss for batch 2823 : 0.003837727941572666\n",
      "Training loss for batch 2824 : 0.06790267676115036\n",
      "Training loss for batch 2825 : 0.21838273108005524\n",
      "Training loss for batch 2826 : 0.26703476905822754\n",
      "Training loss for batch 2827 : 0.1035311371088028\n",
      "Training loss for batch 2828 : 0.06725527346134186\n",
      "Training loss for batch 2829 : 0.1386166363954544\n",
      "Training loss for batch 2830 : 0.05565152317285538\n",
      "Training loss for batch 2831 : 0.03207405284047127\n",
      "Training loss for batch 2832 : 0.011454835534095764\n",
      "Training loss for batch 2833 : 0.12059735506772995\n",
      "Training loss for batch 2834 : 0.0819569081068039\n",
      "Training loss for batch 2835 : 0.08058902621269226\n",
      "Training loss for batch 2836 : 0.09529290348291397\n",
      "Training loss for batch 2837 : 0.047261256724596024\n",
      "Training loss for batch 2838 : 0.06569705903530121\n",
      "Training loss for batch 2839 : 0.04183583706617355\n",
      "Training loss for batch 2840 : 0.020725352689623833\n",
      "Training loss for batch 2841 : 0.06957422196865082\n",
      "Training loss for batch 2842 : 0.04956423491239548\n",
      "Training loss for batch 2843 : 0.05537417531013489\n",
      "Training loss for batch 2844 : 0.09002400189638138\n",
      "Training loss for batch 2845 : 0.048784445971250534\n",
      "Training loss for batch 2846 : 0.004358356352895498\n",
      "Training loss for batch 2847 : 0.14568959176540375\n",
      "Training loss for batch 2848 : 0.06829188019037247\n",
      "Training loss for batch 2849 : 0.034377872943878174\n",
      "Training loss for batch 2850 : 0.052676428109407425\n",
      "Training loss for batch 2851 : 0.07390748709440231\n",
      "Training loss for batch 2852 : 0.019934767857193947\n",
      "Training loss for batch 2853 : 0.17479169368743896\n",
      "Training loss for batch 2854 : 0.046926602721214294\n",
      "Training loss for batch 2855 : 0.02561105601489544\n",
      "Training loss for batch 2856 : 0.20929846167564392\n",
      "Training loss for batch 2857 : 0.11197496205568314\n",
      "Training loss for batch 2858 : 8.637037751668686e-09\n",
      "Training loss for batch 2859 : 0.12960176169872284\n",
      "Training loss for batch 2860 : 0.08199328929185867\n",
      "Training loss for batch 2861 : 0.05822914466261864\n",
      "Training loss for batch 2862 : 0.021110450848937035\n",
      "Training loss for batch 2863 : 0.15108396112918854\n",
      "Training loss for batch 2864 : 0.06962747871875763\n",
      "Training loss for batch 2865 : 0.04987018555402756\n",
      "Training loss for batch 2866 : 0.20811374485492706\n",
      "Training loss for batch 2867 : 0.09343001991510391\n",
      "Training loss for batch 2868 : 0.05760767310857773\n",
      "Training loss for batch 2869 : 0.16721388697624207\n",
      "Training loss for batch 2870 : 0.1032891571521759\n",
      "Training loss for batch 2871 : 0.07269130647182465\n",
      "Training loss for batch 2872 : 0.05924254655838013\n",
      "Training loss for batch 2873 : 0.07944095134735107\n",
      "Training loss for batch 2874 : 0.07475599646568298\n",
      "Training loss for batch 2875 : 6.110738581810438e-09\n",
      "Training loss for batch 2876 : 0.1395053118467331\n",
      "Training loss for batch 2877 : 0.18451489508152008\n",
      "Training loss for batch 2878 : 0.10619983822107315\n",
      "Training loss for batch 2879 : 0.10468873381614685\n",
      "Training loss for batch 2880 : 0.02735016494989395\n",
      "Training loss for batch 2881 : 0.28485244512557983\n",
      "Training loss for batch 2882 : 0.11656108498573303\n",
      "Training loss for batch 2883 : 0.09791743755340576\n",
      "Training loss for batch 2884 : 0.23017990589141846\n",
      "Training loss for batch 2885 : 0.04465942457318306\n",
      "Training loss for batch 2886 : 0.12562252581119537\n",
      "Training loss for batch 2887 : 0.05740595608949661\n",
      "Training loss for batch 2888 : 0.07849866151809692\n",
      "Training loss for batch 2889 : 0.12030396610498428\n",
      "Training loss for batch 2890 : 0.07740515470504761\n",
      "Training loss for batch 2891 : 0.07756495475769043\n",
      "Training loss for batch 2892 : 0.13539014756679535\n",
      "Training loss for batch 2893 : 0.01013927161693573\n",
      "Training loss for batch 2894 : 0.04139238968491554\n",
      "Training loss for batch 2895 : 0.07665270566940308\n",
      "Training loss for batch 2896 : 0.1817309707403183\n",
      "Training loss for batch 2897 : 0.10524183511734009\n",
      "Training loss for batch 2898 : 0.0347236692905426\n",
      "Training loss for batch 2899 : 0.10040272027254105\n",
      "Training loss for batch 2900 : 0.031420640647411346\n",
      "Training loss for batch 2901 : 0.03262009099125862\n",
      "Training loss for batch 2902 : 0.10644982755184174\n",
      "Training loss for batch 2903 : 0.06324824690818787\n",
      "Training loss for batch 2904 : 0.04943084344267845\n",
      "Training loss for batch 2905 : 0.007032230030745268\n",
      "Training loss for batch 2906 : 0.048440370708703995\n",
      "Training loss for batch 2907 : 0.16188913583755493\n",
      "Training loss for batch 2908 : 0.0669044628739357\n",
      "Training loss for batch 2909 : 0.024175480008125305\n",
      "Training loss for batch 2910 : 0.05032029375433922\n",
      "Training loss for batch 2911 : 0.04613202437758446\n",
      "Training loss for batch 2912 : 0.1316000521183014\n",
      "Training loss for batch 2913 : 0.04976731538772583\n",
      "Training loss for batch 2914 : 0.14272408187389374\n",
      "Training loss for batch 2915 : 0.0277874693274498\n",
      "Training loss for batch 2916 : 0.053451359272003174\n",
      "Training loss for batch 2917 : 0.08920950442552567\n",
      "Training loss for batch 2918 : 0.1856437623500824\n",
      "Training loss for batch 2919 : 0.055805753916502\n",
      "Training loss for batch 2920 : 0.09208615869283676\n",
      "Training loss for batch 2921 : 0.03225138783454895\n",
      "Training loss for batch 2922 : 0.2208300083875656\n",
      "Training loss for batch 2923 : 0.09411638230085373\n",
      "Training loss for batch 2924 : 0.14446432888507843\n",
      "Training loss for batch 2925 : 0.10532303154468536\n",
      "Training loss for batch 2926 : 0.14709483087062836\n",
      "Training loss for batch 2927 : 0.003034797729924321\n",
      "Training loss for batch 2928 : 0.01434365939348936\n",
      "Training loss for batch 2929 : 0.028502164408564568\n",
      "Training loss for batch 2930 : 0.06708717346191406\n",
      "Training loss for batch 2931 : 0.039456095546483994\n",
      "Training loss for batch 2932 : 0.061436086893081665\n",
      "Training loss for batch 2933 : 0.07976919412612915\n",
      "Training loss for batch 2934 : 0.031225932762026787\n",
      "Training loss for batch 2935 : 0.009989814832806587\n",
      "Training loss for batch 2936 : 0.12755639851093292\n",
      "Training loss for batch 2937 : 0.07854253053665161\n",
      "Training loss for batch 2938 : 0.005183289758861065\n",
      "Training loss for batch 2939 : 0.031060228124260902\n",
      "Training loss for batch 2940 : 0.06434129923582077\n",
      "Training loss for batch 2941 : 0.1901252567768097\n",
      "Training loss for batch 2942 : 0.18283778429031372\n",
      "Training loss for batch 2943 : 0.09703348577022552\n",
      "Training loss for batch 2944 : 0.13194037973880768\n",
      "Training loss for batch 2945 : 0.02405797503888607\n",
      "Training loss for batch 2946 : 0.12774749100208282\n",
      "Training loss for batch 2947 : 0.08096001297235489\n",
      "Training loss for batch 2948 : 0.025803187862038612\n",
      "Training loss for batch 2949 : 0.03071325644850731\n",
      "Training loss for batch 2950 : 0.00574328051880002\n",
      "Training loss for batch 2951 : 0.06644616276025772\n",
      "Training loss for batch 2952 : 0.17780183255672455\n",
      "Training loss for batch 2953 : 0.07083310931921005\n",
      "Training loss for batch 2954 : 0.04618435725569725\n",
      "Training loss for batch 2955 : 0.08661971986293793\n",
      "Training loss for batch 2956 : 0.06089303269982338\n",
      "Training loss for batch 2957 : 0.09273661673069\n",
      "Training loss for batch 2958 : 0.03723354637622833\n",
      "Training loss for batch 2959 : 0.10998226702213287\n",
      "Training loss for batch 2960 : 0.13894079625606537\n",
      "Training loss for batch 2961 : 0.06312472373247147\n",
      "Training loss for batch 2962 : 0.05715015530586243\n",
      "Training loss for batch 2963 : 0.007739198859781027\n",
      "Training loss for batch 2964 : 0.028565222397446632\n",
      "Training loss for batch 2965 : 0.0886329710483551\n",
      "Training loss for batch 2966 : 0.051677118986845016\n",
      "Training loss for batch 2967 : 0.12389010190963745\n",
      "Training loss for batch 2968 : 0.03374222293496132\n",
      "Training loss for batch 2969 : 0.013810044154524803\n",
      "Training loss for batch 2970 : 0.12978821992874146\n",
      "Training loss for batch 2971 : 0.05384836345911026\n",
      "Training loss for batch 2972 : 0.17578212916851044\n",
      "Training loss for batch 2973 : 0.07276909053325653\n",
      "Training loss for batch 2974 : 0.20726945996284485\n",
      "Training loss for batch 2975 : 0.1360405832529068\n",
      "Training loss for batch 2976 : 0.023129012435674667\n",
      "Training loss for batch 2977 : 0.05038680508732796\n",
      "Training loss for batch 2978 : 0.22365692257881165\n",
      "Training loss for batch 2979 : 0.04792887344956398\n",
      "Training loss for batch 2980 : 0.03910427168011665\n",
      "Training loss for batch 2981 : 0.0396004393696785\n",
      "Training loss for batch 2982 : 0.12007024884223938\n",
      "Training loss for batch 2983 : 0.05403219163417816\n",
      "Training loss for batch 2984 : 0.11083358526229858\n",
      "Training loss for batch 2985 : 0.054728589951992035\n",
      "Training loss for batch 2986 : 0.059972260147333145\n",
      "Training loss for batch 2987 : 0.08035755157470703\n",
      "Training loss for batch 2988 : 0.033196378499269485\n",
      "Training loss for batch 2989 : 0.0736740380525589\n",
      "Training loss for batch 2990 : 0.017245326191186905\n",
      "Training loss for batch 2991 : 0.09627994149923325\n",
      "Training loss for batch 2992 : 0.05975132808089256\n",
      "Training loss for batch 2993 : 0.10666189342737198\n",
      "Training loss for batch 2994 : 0.17780658602714539\n",
      "Training loss for batch 2995 : 0.055203232914209366\n",
      "Training loss for batch 2996 : 0.021062320098280907\n",
      "Training loss for batch 2997 : 0.010748961009085178\n",
      "Training loss for batch 2998 : 0.13376279175281525\n",
      "Training loss for batch 2999 : 0.0326114147901535\n",
      "Training loss for batch 3000 : 0.02915801852941513\n",
      "Training loss for batch 3001 : 0.003967513795942068\n",
      "Training loss for batch 3002 : 0.06047062575817108\n",
      "Training loss for batch 3003 : 0.0582246258854866\n",
      "Training loss for batch 3004 : 0.02496311254799366\n",
      "Training loss for batch 3005 : 0.10263551026582718\n",
      "Training loss for batch 3006 : 0.002002070425078273\n",
      "Training loss for batch 3007 : 0.013997992500662804\n",
      "Training loss for batch 3008 : 0.07800638675689697\n",
      "Training loss for batch 3009 : 0.016612181439995766\n",
      "Training loss for batch 3010 : 0.01962919533252716\n",
      "Training loss for batch 3011 : 0.09899228811264038\n",
      "Training loss for batch 3012 : 0.1763787865638733\n",
      "Training loss for batch 3013 : 0.0360097661614418\n",
      "Training loss for batch 3014 : 0.08285805583000183\n",
      "Training loss for batch 3015 : 0.03883121535181999\n",
      "Training loss for batch 3016 : 0.07727483659982681\n",
      "Training loss for batch 3017 : 0.07871615141630173\n",
      "Training loss for batch 3018 : 0.10562769323587418\n",
      "Training loss for batch 3019 : 0.030382497236132622\n",
      "Training loss for batch 3020 : 0.05586471036076546\n",
      "Training loss for batch 3021 : 0.18894654512405396\n",
      "Training loss for batch 3022 : 0.02123347297310829\n",
      "Training loss for batch 3023 : 0.03342026472091675\n",
      "Training loss for batch 3024 : 0.07496365159749985\n",
      "Training loss for batch 3025 : 0.016628073528409004\n",
      "Training loss for batch 3026 : 0.181381955742836\n",
      "Training loss for batch 3027 : 0.13662280142307281\n",
      "Training loss for batch 3028 : 0.1021585762500763\n",
      "Training loss for batch 3029 : 0.10219371318817139\n",
      "Training loss for batch 3030 : 0.12281431257724762\n",
      "Training loss for batch 3031 : 0.19928082823753357\n",
      "Training loss for batch 3032 : 0.025800179690122604\n",
      "Training loss for batch 3033 : 0.04093845933675766\n",
      "Training loss for batch 3034 : 0.07042918354272842\n",
      "Training loss for batch 3035 : 0.04549160972237587\n",
      "Training loss for batch 3036 : 0.15729206800460815\n",
      "Training loss for batch 3037 : 0.018223412334918976\n",
      "Training loss for batch 3038 : 0.263178288936615\n",
      "Training loss for batch 3039 : 0.018042052164673805\n",
      "Training loss for batch 3040 : 0.2651733160018921\n",
      "Training loss for batch 3041 : 0.12201361358165741\n",
      "Training loss for batch 3042 : 0.12167797982692719\n",
      "Training loss for batch 3043 : 0.06369764357805252\n",
      "Training loss for batch 3044 : 0.19809487462043762\n",
      "Training loss for batch 3045 : 0.1294744312763214\n",
      "Training loss for batch 3046 : 0.043963123112916946\n",
      "Training loss for batch 3047 : 0.007643220946192741\n",
      "Training loss for batch 3048 : 0.12900814414024353\n",
      "Training loss for batch 3049 : 0.01996167004108429\n",
      "Training loss for batch 3050 : 0.054483719170093536\n",
      "Training loss for batch 3051 : 0.039922844618558884\n",
      "Training loss for batch 3052 : 0.05543290823698044\n",
      "Training loss for batch 3053 : 0.013161815702915192\n",
      "Training loss for batch 3054 : 0.21132414042949677\n",
      "Training loss for batch 3055 : 0.08742287755012512\n",
      "Training loss for batch 3056 : 0.09369495511054993\n",
      "Training loss for batch 3057 : 0.01818828284740448\n",
      "Training loss for batch 3058 : 0.020158326253294945\n",
      "Training loss for batch 3059 : 0.09152017533779144\n",
      "Training loss for batch 3060 : 0.14642846584320068\n",
      "Training loss for batch 3061 : 0.15902042388916016\n",
      "Training loss for batch 3062 : 0.08752134442329407\n",
      "Training loss for batch 3063 : 0.10055956989526749\n",
      "Training loss for batch 3064 : 0.04435987025499344\n",
      "Training loss for batch 3065 : 0.0028395738918334246\n",
      "Training loss for batch 3066 : 0.01650512032210827\n",
      "Training loss for batch 3067 : 0.02911456860601902\n",
      "Training loss for batch 3068 : 0.1049816831946373\n",
      "Training loss for batch 3069 : 0.0776669904589653\n",
      "Training loss for batch 3070 : 0.13931576907634735\n",
      "Training loss for batch 3071 : 0.0953553318977356\n",
      "Training loss for batch 3072 : 0.0821954533457756\n",
      "Training loss for batch 3073 : 0.04514074698090553\n",
      "Training loss for batch 3074 : 0.127472922205925\n",
      "Training loss for batch 3075 : 0.09321153163909912\n",
      "Training loss for batch 3076 : 0.016038941219449043\n",
      "Training loss for batch 3077 : 0.16015568375587463\n",
      "Training loss for batch 3078 : 0.07340379059314728\n",
      "Training loss for batch 3079 : 0.21737653017044067\n",
      "Training loss for batch 3080 : 0.09422967582941055\n",
      "Training loss for batch 3081 : 0.19746439158916473\n",
      "Training loss for batch 3082 : 0.12104230374097824\n",
      "Training loss for batch 3083 : 0.2985254228115082\n",
      "Training loss for batch 3084 : 0.07391519099473953\n",
      "Training loss for batch 3085 : 0.05442260578274727\n",
      "Training loss for batch 3086 : 0.0335889607667923\n",
      "Training loss for batch 3087 : 0.13523730635643005\n",
      "Training loss for batch 3088 : 0.02951507456600666\n",
      "Training loss for batch 3089 : 0.007527845446020365\n",
      "Training loss for batch 3090 : 0.20219436287879944\n",
      "Training loss for batch 3091 : 0.050497300922870636\n",
      "Training loss for batch 3092 : 0.10210669785737991\n",
      "Training loss for batch 3093 : 0.1137821301817894\n",
      "Training loss for batch 3094 : 0.06596739590167999\n",
      "Training loss for batch 3095 : 0.03588838502764702\n",
      "Training loss for batch 3096 : 0.18462525308132172\n",
      "Training loss for batch 3097 : 0.02226775698363781\n",
      "Training loss for batch 3098 : 0.0756150633096695\n",
      "Training loss for batch 3099 : 0.03894345089793205\n",
      "Training loss for batch 3100 : 0.05471479520201683\n",
      "Training loss for batch 3101 : 0.10540787875652313\n",
      "Training loss for batch 3102 : 0.026122568175196648\n",
      "Training loss for batch 3103 : 0.06328576803207397\n",
      "Training loss for batch 3104 : 0.09504252672195435\n",
      "Training loss for batch 3105 : 0.03812123462557793\n",
      "Training loss for batch 3106 : 0.0988747775554657\n",
      "Training loss for batch 3107 : 0.012116248719394207\n",
      "Training loss for batch 3108 : 0.06506679952144623\n",
      "Training loss for batch 3109 : 0.16034069657325745\n",
      "Training loss for batch 3110 : 0.13145574927330017\n",
      "Training loss for batch 3111 : 0.007574447896331549\n",
      "Training loss for batch 3112 : 0.15225200355052948\n",
      "Training loss for batch 3113 : 0.058203745633363724\n",
      "Training loss for batch 3114 : 0.023697303608059883\n",
      "Training loss for batch 3115 : 0.00014970700431149453\n",
      "Training loss for batch 3116 : 0.044519081711769104\n",
      "Training loss for batch 3117 : 0.18521785736083984\n",
      "Training loss for batch 3118 : 0.061589550226926804\n",
      "Training loss for batch 3119 : 0.20308147370815277\n",
      "Training loss for batch 3120 : 0.1006486713886261\n",
      "Training loss for batch 3121 : 0.028031524270772934\n",
      "Training loss for batch 3122 : 0.11551990360021591\n",
      "Training loss for batch 3123 : 0.14636152982711792\n",
      "Training loss for batch 3124 : 0.13761799037456512\n",
      "Training loss for batch 3125 : 0.04878310486674309\n",
      "Training loss for batch 3126 : 0.04563816264271736\n",
      "Training loss for batch 3127 : 0.2051878720521927\n",
      "Training loss for batch 3128 : 0.1357184797525406\n",
      "Training loss for batch 3129 : 0.051916200667619705\n",
      "Training loss for batch 3130 : 0.08129767328500748\n",
      "Training loss for batch 3131 : 0.009471814148128033\n",
      "Training loss for batch 3132 : 0.03723973035812378\n",
      "Training loss for batch 3133 : 0.1104726567864418\n",
      "Training loss for batch 3134 : 0.15664546191692352\n",
      "Training loss for batch 3135 : 0.17746028304100037\n",
      "Training loss for batch 3136 : 0.00023359557962976396\n",
      "Training loss for batch 3137 : 0.03784584999084473\n",
      "Training loss for batch 3138 : 0.023917870596051216\n",
      "Training loss for batch 3139 : 0.10611207038164139\n",
      "Training loss for batch 3140 : 0.09435141831636429\n",
      "Training loss for batch 3141 : 0.04183107614517212\n",
      "Training loss for batch 3142 : 0.09999627619981766\n",
      "Training loss for batch 3143 : 0.03892768546938896\n",
      "Training loss for batch 3144 : 0.10855281352996826\n",
      "Training loss for batch 3145 : 0.09269828349351883\n",
      "Training loss for batch 3146 : 0.04332553222775459\n",
      "Training loss for batch 3147 : 0.047094352543354034\n",
      "Training loss for batch 3148 : 0.07881124317646027\n",
      "Training loss for batch 3149 : 0.06535891443490982\n",
      "Training loss for batch 3150 : 0.11730591952800751\n",
      "Training loss for batch 3151 : 0.026818590238690376\n",
      "Training loss for batch 3152 : 0.03630026802420616\n",
      "Training loss for batch 3153 : 0.09760438650846481\n",
      "Training loss for batch 3154 : 0.2493221014738083\n",
      "Training loss for batch 3155 : 0.21959102153778076\n",
      "Training loss for batch 3156 : 0.035981159657239914\n",
      "Training loss for batch 3157 : 0.026822203770279884\n",
      "Training loss for batch 3158 : 0.06889614462852478\n",
      "Training loss for batch 3159 : 0.016179654747247696\n",
      "Training loss for batch 3160 : 0.0678216964006424\n",
      "Training loss for batch 3161 : 0.0207237396389246\n",
      "Training loss for batch 3162 : 0.12533965706825256\n",
      "Training loss for batch 3163 : 0.15669183433055878\n",
      "Training loss for batch 3164 : 0.06671745330095291\n",
      "Training loss for batch 3165 : 0.017937688156962395\n",
      "Training loss for batch 3166 : 0.02209516242146492\n",
      "Training loss for batch 3167 : 0.09714342653751373\n",
      "Training loss for batch 3168 : 0.09524495899677277\n",
      "Training loss for batch 3169 : 0.0756882056593895\n",
      "Training loss for batch 3170 : 0.20214170217514038\n",
      "Training loss for batch 3171 : 0.01774730533361435\n",
      "Training loss for batch 3172 : 0.15308959782123566\n",
      "Training loss for batch 3173 : 0.02584182843565941\n",
      "Training loss for batch 3174 : 0.0\n",
      "Training loss for batch 3175 : 0.06328944861888885\n",
      "Training loss for batch 3176 : 0.07637657970190048\n",
      "Training loss for batch 3177 : 0.0016733756056055427\n",
      "Training loss for batch 3178 : 0.12124255299568176\n",
      "Training loss for batch 3179 : 0.1239844262599945\n",
      "Training loss for batch 3180 : 0.08620984107255936\n",
      "Training loss for batch 3181 : 0.15827937424182892\n",
      "Training loss for batch 3182 : 0.1470155268907547\n",
      "Training loss for batch 3183 : 0.03576831892132759\n",
      "Training loss for batch 3184 : 0.02675369381904602\n",
      "Training loss for batch 3185 : 0.08924587815999985\n",
      "Training loss for batch 3186 : 0.09502933919429779\n",
      "Training loss for batch 3187 : 0.21234826743602753\n",
      "Training loss for batch 3188 : 0.0502997487783432\n",
      "Training loss for batch 3189 : 0.06795180588960648\n",
      "Training loss for batch 3190 : 0.04583951085805893\n",
      "Training loss for batch 3191 : 0.2586435079574585\n",
      "Training loss for batch 3192 : 0.0779624953866005\n",
      "Training loss for batch 3193 : 0.11091853678226471\n",
      "Training loss for batch 3194 : 0.010479721240699291\n",
      "Training loss for batch 3195 : 0.06935004889965057\n",
      "Training loss for batch 3196 : 0.010249080136418343\n",
      "Training loss for batch 3197 : 0.12594860792160034\n",
      "Training loss for batch 3198 : 0.09293097257614136\n",
      "Training loss for batch 3199 : 0.0\n",
      "Training loss for batch 3200 : 0.08489850163459778\n",
      "Training loss for batch 3201 : 0.301037073135376\n",
      "Training loss for batch 3202 : 0.012465151026844978\n",
      "Training loss for batch 3203 : 0.03445606306195259\n",
      "Training loss for batch 3204 : 0.1368938684463501\n",
      "Training loss for batch 3205 : 0.021493488922715187\n",
      "Training loss for batch 3206 : 0.11490099877119064\n",
      "Training loss for batch 3207 : 0.13243360817432404\n",
      "Training loss for batch 3208 : 0.0253254733979702\n",
      "Training loss for batch 3209 : 0.030730459839105606\n",
      "Training loss for batch 3210 : 0.030748400837183\n",
      "Training loss for batch 3211 : 0.04182437062263489\n",
      "Training loss for batch 3212 : 0.011690270155668259\n",
      "Training loss for batch 3213 : 0.09607220441102982\n",
      "Training loss for batch 3214 : 0.03288820758461952\n",
      "Training loss for batch 3215 : 0.03690221160650253\n",
      "Training loss for batch 3216 : 0.03594008460640907\n",
      "Training loss for batch 3217 : 0.08080647885799408\n",
      "Training loss for batch 3218 : 0.032242629677057266\n",
      "Training loss for batch 3219 : 0.03255608305335045\n",
      "Training loss for batch 3220 : 0.10002496838569641\n",
      "Training loss for batch 3221 : 0.079159215092659\n",
      "Training loss for batch 3222 : 0.1363885998725891\n",
      "Training loss for batch 3223 : 0.07945870608091354\n",
      "Training loss for batch 3224 : 0.014433088712394238\n",
      "Training loss for batch 3225 : 0.18484899401664734\n",
      "Training loss for batch 3226 : 0.0037939678877592087\n",
      "Training loss for batch 3227 : 0.05770121142268181\n",
      "Training loss for batch 3228 : 0.07521779835224152\n",
      "Training loss for batch 3229 : 0.13012126088142395\n",
      "Training loss for batch 3230 : 0.04064690321683884\n",
      "Training loss for batch 3231 : 0.12480194121599197\n",
      "Training loss for batch 3232 : 0.02991599030792713\n",
      "Training loss for batch 3233 : 0.2117307037115097\n",
      "Training loss for batch 3234 : 0.07557108998298645\n",
      "Training loss for batch 3235 : 0.040332239121198654\n",
      "Training loss for batch 3236 : 0.08444744348526001\n",
      "Training loss for batch 3237 : 0.005049938801676035\n",
      "Training loss for batch 3238 : 0.04304055869579315\n",
      "Training loss for batch 3239 : 0.00238325959071517\n",
      "Training loss for batch 3240 : 0.053567852824926376\n",
      "Training loss for batch 3241 : 0.0968715101480484\n",
      "Training loss for batch 3242 : 0.145847886800766\n",
      "Training loss for batch 3243 : 0.04396510496735573\n",
      "Training loss for batch 3244 : 0.07087551057338715\n",
      "Training loss for batch 3245 : 0.18826808035373688\n",
      "Training loss for batch 3246 : 0.0764559954404831\n",
      "Training loss for batch 3247 : 0.021054163575172424\n",
      "Training loss for batch 3248 : 0.08702323585748672\n",
      "Training loss for batch 3249 : 0.0029559044633060694\n",
      "Training loss for batch 3250 : 0.04218796640634537\n",
      "Training loss for batch 3251 : 0.027292970567941666\n",
      "Training loss for batch 3252 : 0.04139629751443863\n",
      "Training loss for batch 3253 : 0.11777178943157196\n",
      "Training loss for batch 3254 : 0.13529406487941742\n",
      "Training loss for batch 3255 : 0.22699356079101562\n",
      "Training loss for batch 3256 : 0.05197178199887276\n",
      "Training loss for batch 3257 : 0.08533180505037308\n",
      "Training loss for batch 3258 : 0.0720619261264801\n",
      "Training loss for batch 3259 : 0.06317252665758133\n",
      "Training loss for batch 3260 : 0.04324346035718918\n",
      "Training loss for batch 3261 : 0.06827879697084427\n",
      "Training loss for batch 3262 : 0.06267829984426498\n",
      "Training loss for batch 3263 : 0.017085714265704155\n",
      "Training loss for batch 3264 : 0.1974668949842453\n",
      "Training loss for batch 3265 : 0.1306312084197998\n",
      "Training loss for batch 3266 : 0.018260905519127846\n",
      "Training loss for batch 3267 : 0.15568730235099792\n",
      "Training loss for batch 3268 : 0.07655647397041321\n",
      "Training loss for batch 3269 : 0.05762851610779762\n",
      "Training loss for batch 3270 : 0.02277965098619461\n",
      "Training loss for batch 3271 : 0.03243304044008255\n",
      "Training loss for batch 3272 : 5.2060389776897864e-08\n",
      "Training loss for batch 3273 : 0.08842819184064865\n",
      "Training loss for batch 3274 : 0.03795977681875229\n",
      "Training loss for batch 3275 : 0.15562552213668823\n",
      "Training loss for batch 3276 : 0.07058888673782349\n",
      "Training loss for batch 3277 : 0.0846053883433342\n",
      "Training loss for batch 3278 : 0.13444989919662476\n",
      "Training loss for batch 3279 : 0.15314054489135742\n",
      "Training loss for batch 3280 : 0.05129373446106911\n",
      "Training loss for batch 3281 : 0.024377955123782158\n",
      "Training loss for batch 3282 : 0.030708713456988335\n",
      "Training loss for batch 3283 : 0.026629121974110603\n",
      "Training loss for batch 3284 : 0.13942082226276398\n",
      "Training loss for batch 3285 : 0.005809118505567312\n",
      "Training loss for batch 3286 : 0.04553302004933357\n",
      "Training loss for batch 3287 : 0.10077744722366333\n",
      "Training loss for batch 3288 : 0.10257051140069962\n",
      "Training loss for batch 3289 : 0.03318929299712181\n",
      "Training loss for batch 3290 : 0.03154883533716202\n",
      "Training loss for batch 3291 : 0.02431555837392807\n",
      "Training loss for batch 3292 : 0.026688922196626663\n",
      "Training loss for batch 3293 : 0.2078593224287033\n",
      "Training loss for batch 3294 : 0.26827067136764526\n",
      "Training loss for batch 3295 : 0.08396191895008087\n",
      "Training loss for batch 3296 : 0.07041513919830322\n",
      "Training loss for batch 3297 : 0.0829867348074913\n",
      "Training loss for batch 3298 : 0.08876216411590576\n",
      "Training loss for batch 3299 : 0.01332907285541296\n",
      "Training loss for batch 3300 : 0.009463392198085785\n",
      "Training loss for batch 3301 : 0.09793784469366074\n",
      "Training loss for batch 3302 : 0.010654142126441002\n",
      "Training loss for batch 3303 : 0.07176777720451355\n",
      "Training loss for batch 3304 : 0.15508705377578735\n",
      "Training loss for batch 3305 : 0.05657842010259628\n",
      "Training loss for batch 3306 : 0.025455432012677193\n",
      "Training loss for batch 3307 : 0.026878995820879936\n",
      "Training loss for batch 3308 : 0.05252944678068161\n",
      "Training loss for batch 3309 : 0.09917774796485901\n",
      "Training loss for batch 3310 : 0.12993168830871582\n",
      "Training loss for batch 3311 : 0.1786351203918457\n",
      "Training loss for batch 3312 : 0.07155746221542358\n",
      "Training loss for batch 3313 : 0.012819135561585426\n",
      "Training loss for batch 3314 : 0.032756175845861435\n",
      "Training loss for batch 3315 : 0.015115141868591309\n",
      "Training loss for batch 3316 : 0.0078726252540946\n",
      "Training loss for batch 3317 : 0.04321093484759331\n",
      "Training loss for batch 3318 : 0.08331043273210526\n",
      "Training loss for batch 3319 : 0.08320731669664383\n",
      "Training loss for batch 3320 : 0.10228705406188965\n",
      "Training loss for batch 3321 : 0.09912270307540894\n",
      "Training loss for batch 3322 : 0.08847137540578842\n",
      "Training loss for batch 3323 : 0.11215977370738983\n",
      "Training loss for batch 3324 : 0.056643567979335785\n",
      "Training loss for batch 3325 : 0.046689994633197784\n",
      "Training loss for batch 3326 : 0.15934865176677704\n",
      "Training loss for batch 3327 : 0.04133182764053345\n",
      "Training loss for batch 3328 : 0.019965652376413345\n",
      "Training loss for batch 3329 : 0.23994062840938568\n",
      "Training loss for batch 3330 : 0.05699760839343071\n",
      "Training loss for batch 3331 : 0.06061909720301628\n",
      "Training loss for batch 3332 : 0.07113179564476013\n",
      "Training loss for batch 3333 : 0.023896947503089905\n",
      "Training loss for batch 3334 : 0.08425799012184143\n",
      "Training loss for batch 3335 : 0.26104143261909485\n",
      "Training loss for batch 3336 : 0.009920503944158554\n",
      "Training loss for batch 3337 : 0.04628400877118111\n",
      "Training loss for batch 3338 : 0.08915113657712936\n",
      "Training loss for batch 3339 : 0.04047286510467529\n",
      "Training loss for batch 3340 : 0.03454460948705673\n",
      "Training loss for batch 3341 : 0.06902381777763367\n",
      "Training loss for batch 3342 : 0.062419384717941284\n",
      "Training loss for batch 3343 : 0.02750977873802185\n",
      "Training loss for batch 3344 : 0.06422200053930283\n",
      "Training loss for batch 3345 : 4.111679441365368e-09\n",
      "Training loss for batch 3346 : 0.009086714126169682\n",
      "Training loss for batch 3347 : 0.058016158640384674\n",
      "Training loss for batch 3348 : 0.04326104745268822\n",
      "Training loss for batch 3349 : 0.08804741501808167\n",
      "Training loss for batch 3350 : 0.07060333341360092\n",
      "Training loss for batch 3351 : 0.12987209856510162\n",
      "Training loss for batch 3352 : 0.06782464683055878\n",
      "Training loss for batch 3353 : 0.05146333575248718\n",
      "Training loss for batch 3354 : 0.2025650441646576\n",
      "Training loss for batch 3355 : 0.06216111779212952\n",
      "Training loss for batch 3356 : 0.04472489655017853\n",
      "Training loss for batch 3357 : 0.03694764897227287\n",
      "Training loss for batch 3358 : 0.1491325944662094\n",
      "Training loss for batch 3359 : 0.04220950976014137\n",
      "Training loss for batch 3360 : 0.052073728293180466\n",
      "Training loss for batch 3361 : 0.1291891187429428\n",
      "Training loss for batch 3362 : 0.06782553344964981\n",
      "Training loss for batch 3363 : 0.08223305642604828\n",
      "Training loss for batch 3364 : 0.09097740054130554\n",
      "Training loss for batch 3365 : 0.15016146004199982\n",
      "Training loss for batch 3366 : 0.0730944573879242\n",
      "Training loss for batch 3367 : 0.0533553883433342\n",
      "Training loss for batch 3368 : 0.09956750273704529\n",
      "Training loss for batch 3369 : 0.18433326482772827\n",
      "Training loss for batch 3370 : 0.18382211029529572\n",
      "Training loss for batch 3371 : 0.05602683871984482\n",
      "Training loss for batch 3372 : 0.0932324081659317\n",
      "Training loss for batch 3373 : 0.09089004248380661\n",
      "Training loss for batch 3374 : 0.22384001314640045\n",
      "Training loss for batch 3375 : 0.05137897655367851\n",
      "Training loss for batch 3376 : 0.10218977183103561\n",
      "Training loss for batch 3377 : 0.28815943002700806\n",
      "Training loss for batch 3378 : 0.12422426044940948\n",
      "Training loss for batch 3379 : 0.06068218871951103\n",
      "Training loss for batch 3380 : 0.02488761581480503\n",
      "Training loss for batch 3381 : 0.02125329151749611\n",
      "Training loss for batch 3382 : 0.15064118802547455\n",
      "Training loss for batch 3383 : 0.024150095880031586\n",
      "Training loss for batch 3384 : 0.011189543642103672\n",
      "Training loss for batch 3385 : 0.15623250603675842\n",
      "Training loss for batch 3386 : 0.11590899527072906\n",
      "Training loss for batch 3387 : 0.016822833567857742\n",
      "Training loss for batch 3388 : 0.08060146868228912\n",
      "Training loss for batch 3389 : 8.789471905856772e-08\n",
      "Training loss for batch 3390 : 0.11364606767892838\n",
      "Training loss for batch 3391 : 0.07355047017335892\n",
      "Training loss for batch 3392 : 0.07297385483980179\n",
      "Training loss for batch 3393 : 0.0632663369178772\n",
      "Training loss for batch 3394 : 0.07784464210271835\n",
      "Training loss for batch 3395 : 0.14394274353981018\n",
      "Training loss for batch 3396 : 0.17115835845470428\n",
      "Training loss for batch 3397 : 0.07645010203123093\n",
      "Training loss for batch 3398 : 0.17503471672534943\n",
      "Training loss for batch 3399 : 0.03169846162199974\n",
      "Training loss for batch 3400 : 0.06701971590518951\n",
      "Training loss for batch 3401 : 0.01927856169641018\n",
      "Training loss for batch 3402 : 0.02114970237016678\n",
      "Training loss for batch 3403 : 0.03478355333209038\n",
      "Training loss for batch 3404 : 0.0336255244910717\n",
      "Training loss for batch 3405 : 0.186977818608284\n",
      "Training loss for batch 3406 : 0.16231584548950195\n",
      "Training loss for batch 3407 : 0.10875600576400757\n",
      "Training loss for batch 3408 : 0.1925809681415558\n",
      "Training loss for batch 3409 : 0.19657963514328003\n",
      "Training loss for batch 3410 : 0.15214189887046814\n",
      "Training loss for batch 3411 : 0.18909399211406708\n",
      "Training loss for batch 3412 : 0.043951574712991714\n",
      "Training loss for batch 3413 : 0.14159967005252838\n",
      "Training loss for batch 3414 : 0.08911969512701035\n",
      "Training loss for batch 3415 : 0.10632342845201492\n",
      "Training loss for batch 3416 : 0.11049848049879074\n",
      "Training loss for batch 3417 : 0.04990462213754654\n",
      "Training loss for batch 3418 : 0.12984874844551086\n",
      "Training loss for batch 3419 : 0.03583339974284172\n",
      "Training loss for batch 3420 : 0.01709919609129429\n",
      "Training loss for batch 3421 : 0.042419932782649994\n",
      "Training loss for batch 3422 : 0.07458139210939407\n",
      "Training loss for batch 3423 : 0.11648520082235336\n",
      "Training loss for batch 3424 : 0.08667308837175369\n",
      "Training loss for batch 3425 : 0.11347470432519913\n",
      "Training loss for batch 3426 : 0.004733414854854345\n",
      "Training loss for batch 3427 : 0.12017890810966492\n",
      "Training loss for batch 3428 : 0.07492358237504959\n",
      "Training loss for batch 3429 : 0.03624264895915985\n",
      "Training loss for batch 3430 : 0.11331930756568909\n",
      "Training loss for batch 3431 : 0.0901246890425682\n",
      "Training loss for batch 3432 : 0.09435298293828964\n",
      "Training loss for batch 3433 : 0.10128981620073318\n",
      "Training loss for batch 3434 : 0.043475814163684845\n",
      "Training loss for batch 3435 : 0.08221662789583206\n",
      "Training loss for batch 3436 : 0.07595932483673096\n",
      "Training loss for batch 3437 : 0.22940558195114136\n",
      "Training loss for batch 3438 : 0.010903522372245789\n",
      "Training loss for batch 3439 : 0.15359538793563843\n",
      "Training loss for batch 3440 : 0.04183293879032135\n",
      "Training loss for batch 3441 : 0.10055241733789444\n",
      "Training loss for batch 3442 : 0.16488520801067352\n",
      "Training loss for batch 3443 : 0.029271988198161125\n",
      "Training loss for batch 3444 : 0.1608678698539734\n",
      "Training loss for batch 3445 : 0.07471533119678497\n",
      "Training loss for batch 3446 : 0.05284479260444641\n",
      "Training loss for batch 3447 : 0.1696387082338333\n",
      "Training loss for batch 3448 : 0.001162368687801063\n",
      "Training loss for batch 3449 : 0.05798644572496414\n",
      "Training loss for batch 3450 : 0.09713926911354065\n",
      "Training loss for batch 3451 : 0.03292166814208031\n",
      "Training loss for batch 3452 : 0.06476017087697983\n",
      "Training loss for batch 3453 : 0.062215764075517654\n",
      "Training loss for batch 3454 : 0.27911871671676636\n",
      "Training loss for batch 3455 : 0.19891172647476196\n",
      "Training loss for batch 3456 : 0.04613606259226799\n",
      "Training loss for batch 3457 : 0.03265880048274994\n",
      "Training loss for batch 3458 : 0.08065111190080643\n",
      "Training loss for batch 3459 : 0.05892034247517586\n",
      "Training loss for batch 3460 : 0.14768639206886292\n",
      "Training loss for batch 3461 : 0.06374377757310867\n",
      "Training loss for batch 3462 : 0.0\n",
      "Training loss for batch 3463 : 0.09764406830072403\n",
      "Training loss for batch 3464 : 0.11754846572875977\n",
      "Training loss for batch 3465 : 0.07563616335391998\n",
      "Training loss for batch 3466 : 0.07502124458551407\n",
      "Training loss for batch 3467 : 0.07887950539588928\n",
      "Training loss for batch 3468 : 0.05217619240283966\n",
      "Training loss for batch 3469 : 0.16261835396289825\n",
      "Training loss for batch 3470 : 0.05874790623784065\n",
      "Training loss for batch 3471 : 0.18361489474773407\n",
      "Training loss for batch 3472 : 0.10059990733861923\n",
      "Training loss for batch 3473 : 0.15167689323425293\n",
      "Training loss for batch 3474 : 0.027130907401442528\n",
      "Training loss for batch 3475 : 0.002988201566040516\n",
      "Training loss for batch 3476 : 0.1435566395521164\n",
      "Training loss for batch 3477 : 0.05213664844632149\n",
      "Training loss for batch 3478 : 0.03725643828511238\n",
      "Training loss for batch 3479 : 0.21710975468158722\n",
      "Training loss for batch 3480 : 0.07312153279781342\n",
      "Training loss for batch 3481 : 0.08354224264621735\n",
      "Training loss for batch 3482 : 0.14626507461071014\n",
      "Training loss for batch 3483 : 0.049910999834537506\n",
      "Training loss for batch 3484 : 0.05650987848639488\n",
      "Training loss for batch 3485 : 0.06844083964824677\n",
      "Training loss for batch 3486 : 0.08705925196409225\n",
      "Training loss for batch 3487 : 0.06476382166147232\n",
      "Training loss for batch 3488 : 0.23442484438419342\n",
      "Training loss for batch 3489 : 0.16080889105796814\n",
      "Training loss for batch 3490 : 0.14826913177967072\n",
      "Training loss for batch 3491 : 0.10098148137331009\n",
      "Training loss for batch 3492 : 0.10656560957431793\n",
      "Training loss for batch 3493 : 0.06690353900194168\n",
      "Training loss for batch 3494 : 0.12397075444459915\n",
      "Training loss for batch 3495 : 0.1262560486793518\n",
      "Training loss for batch 3496 : 0.08088167756795883\n",
      "Training loss for batch 3497 : 0.17413058876991272\n",
      "Training loss for batch 3498 : 0.1592784970998764\n",
      "Training loss for batch 3499 : 0.009650367312133312\n",
      "Training loss for batch 3500 : 0.024733170866966248\n",
      "Training loss for batch 3501 : 0.03370080888271332\n",
      "Training loss for batch 3502 : 0.038824085146188736\n",
      "Training loss for batch 3503 : 0.08279363811016083\n",
      "Training loss for batch 3504 : 0.0110178142786026\n",
      "Training loss for batch 3505 : 0.29121270775794983\n",
      "Training loss for batch 3506 : 0.1006879210472107\n",
      "Training loss for batch 3507 : 0.17426162958145142\n",
      "Training loss for batch 3508 : 0.11429615318775177\n",
      "Training loss for batch 3509 : 0.06706873327493668\n",
      "Training loss for batch 3510 : 0.1433129757642746\n",
      "Training loss for batch 3511 : 0.1024816706776619\n",
      "Training loss for batch 3512 : 0.06906536966562271\n",
      "Training loss for batch 3513 : 0.11944868415594101\n",
      "Training loss for batch 3514 : 0.0940214991569519\n",
      "Training loss for batch 3515 : 0.1424400806427002\n",
      "Training loss for batch 3516 : 0.03566836193203926\n",
      "Training loss for batch 3517 : 0.1391853392124176\n",
      "Training loss for batch 3518 : 0.1809011697769165\n",
      "Training loss for batch 3519 : 0.09049975126981735\n",
      "Training loss for batch 3520 : 0.047604743391275406\n",
      "Training loss for batch 3521 : 0.04254097864031792\n",
      "Training loss for batch 3522 : 0.09448879957199097\n",
      "Training loss for batch 3523 : 0.1207858994603157\n",
      "Training loss for batch 3524 : 0.061823464930057526\n",
      "Training loss for batch 3525 : 0.20744089782238007\n",
      "Training loss for batch 3526 : 0.007421017624437809\n",
      "Training loss for batch 3527 : 0.1528802216053009\n",
      "Training loss for batch 3528 : 0.044346556067466736\n",
      "Training loss for batch 3529 : 0.031196117401123047\n",
      "Training loss for batch 3530 : 0.04895930364727974\n",
      "Training loss for batch 3531 : 4.559525734748604e-08\n",
      "Training loss for batch 3532 : 0.06362929940223694\n",
      "Training loss for batch 3533 : 0.051460087299346924\n",
      "Training loss for batch 3534 : 0.10482566803693771\n",
      "Training loss for batch 3535 : 0.1104985699057579\n",
      "Training loss for batch 3536 : 0.06827128678560257\n",
      "Training loss for batch 3537 : 0.0781879797577858\n",
      "Training loss for batch 3538 : 0.041423965245485306\n",
      "Training loss for batch 3539 : 0.04890046641230583\n",
      "Training loss for batch 3540 : 0.023838691413402557\n",
      "Training loss for batch 3541 : 0.1854533553123474\n",
      "Training loss for batch 3542 : 0.10264107584953308\n",
      "Training loss for batch 3543 : 0.13151930272579193\n",
      "Training loss for batch 3544 : 0.06015747785568237\n",
      "Training loss for batch 3545 : 0.04817582666873932\n",
      "Training loss for batch 3546 : 0.20094867050647736\n",
      "Training loss for batch 3547 : 0.13370928168296814\n",
      "Training loss for batch 3548 : 0.006231474224478006\n",
      "Training loss for batch 3549 : 0.22067055106163025\n",
      "Training loss for batch 3550 : 0.0015134414425119758\n",
      "Training loss for batch 3551 : 0.16228091716766357\n",
      "Training loss for batch 3552 : 0.01241796649992466\n",
      "Training loss for batch 3553 : 0.1407664716243744\n",
      "Training loss for batch 3554 : 0.1125132143497467\n",
      "Training loss for batch 3555 : 0.1610061079263687\n",
      "Training loss for batch 3556 : 0.16948315501213074\n",
      "Training loss for batch 3557 : 0.09617667645215988\n",
      "Training loss for batch 3558 : 0.06715307384729385\n",
      "Training loss for batch 3559 : 0.14217790961265564\n",
      "Training loss for batch 3560 : 0.12566669285297394\n",
      "Training loss for batch 3561 : 0.018087927252054214\n",
      "Training loss for batch 3562 : 0.2300940304994583\n",
      "Training loss for batch 3563 : 0.08201297372579575\n",
      "Training loss for batch 3564 : 0.04610929638147354\n",
      "Training loss for batch 3565 : 0.028286324813961983\n",
      "Training loss for batch 3566 : 0.2945469915866852\n",
      "Training loss for batch 3567 : 0.08224344998598099\n",
      "Training loss for batch 3568 : 0.17157168686389923\n",
      "Training loss for batch 3569 : 0.07629960030317307\n",
      "Training loss for batch 3570 : 0.08369863033294678\n",
      "Training loss for batch 3571 : 0.028109446167945862\n",
      "Training loss for batch 3572 : 0.044181328266859055\n",
      "Training loss for batch 3573 : 0.02726619318127632\n",
      "Training loss for batch 3574 : 0.10321874171495438\n",
      "Training loss for batch 3575 : 0.10094189643859863\n",
      "Training loss for batch 3576 : 0.05665323883295059\n",
      "Training loss for batch 3577 : 0.015525038354098797\n",
      "Training loss for batch 3578 : 0.03901756927371025\n",
      "Training loss for batch 3579 : 0.04153159260749817\n",
      "Training loss for batch 3580 : 0.021525919437408447\n",
      "Training loss for batch 3581 : 0.029864873737096786\n",
      "Training loss for batch 3582 : 0.07814449816942215\n",
      "Training loss for batch 3583 : 0.05676984414458275\n",
      "Training loss for batch 3584 : 0.005904447287321091\n",
      "Training loss for batch 3585 : 0.024099159985780716\n",
      "Training loss for batch 3586 : 0.1161404624581337\n",
      "Training loss for batch 3587 : 0.11993665248155594\n",
      "Training loss for batch 3588 : 0.0113248685374856\n",
      "Training loss for batch 3589 : 0.05400858074426651\n",
      "Training loss for batch 3590 : 0.025892050936818123\n",
      "Training loss for batch 3591 : 0.18093441426753998\n",
      "Training loss for batch 3592 : 0.17607170343399048\n",
      "Training loss for batch 3593 : 0.06242505833506584\n",
      "Training loss for batch 3594 : 0.057171210646629333\n",
      "Training loss for batch 3595 : 0.0\n",
      "Training loss for batch 3596 : 0.17221160233020782\n",
      "Training loss for batch 3597 : 0.08513323962688446\n",
      "Training loss for batch 3598 : 0.06185915693640709\n",
      "Training loss for batch 3599 : 0.015529878437519073\n",
      "Training loss for batch 3600 : 0.16214099526405334\n",
      "Training loss for batch 3601 : 0.02575317956507206\n",
      "Training loss for batch 3602 : 0.09108578413724899\n",
      "Training loss for batch 3603 : 0.003296464681625366\n",
      "Training loss for batch 3604 : 0.023169320076704025\n",
      "Training loss for batch 3605 : 0.041515424847602844\n",
      "Training loss for batch 3606 : 0.21486353874206543\n",
      "Training loss for batch 3607 : 0.13601112365722656\n",
      "Training loss for batch 3608 : 0.07854802906513214\n",
      "Training loss for batch 3609 : 0.04562867060303688\n",
      "Training loss for batch 3610 : 0.05077526345849037\n",
      "Training loss for batch 3611 : 0.035238783806562424\n",
      "Training loss for batch 3612 : 0.25344642996788025\n",
      "Training loss for batch 3613 : 0.1373358964920044\n",
      "Training loss for batch 3614 : 0.07328753173351288\n",
      "Training loss for batch 3615 : 0.04420096427202225\n",
      "Training loss for batch 3616 : 0.11661515384912491\n",
      "Training loss for batch 3617 : 0.1762537956237793\n",
      "Training loss for batch 3618 : 0.047140032052993774\n",
      "Training loss for batch 3619 : 0.06542757898569107\n",
      "Training loss for batch 3620 : 0.09035902470350266\n",
      "Training loss for batch 3621 : 0.0955522283911705\n",
      "Training loss for batch 3622 : 0.13565979897975922\n",
      "Training loss for batch 3623 : 0.045781344175338745\n",
      "Training loss for batch 3624 : 0.12075936049222946\n",
      "Training loss for batch 3625 : 0.054407939314842224\n",
      "Training loss for batch 3626 : 0.08687727898359299\n",
      "Training loss for batch 3627 : 0.024687057361006737\n",
      "Training loss for batch 3628 : 0.013230920769274235\n",
      "Training loss for batch 3629 : 0.0927971675992012\n",
      "Training loss for batch 3630 : 0.03513164445757866\n",
      "Training loss for batch 3631 : 0.15535412728786469\n",
      "Training loss for batch 3632 : 0.02867651730775833\n",
      "Training loss for batch 3633 : 0.1262785643339157\n",
      "Training loss for batch 3634 : 0.05909591168165207\n",
      "Training loss for batch 3635 : 0.05052346736192703\n",
      "Training loss for batch 3636 : 0.03261348605155945\n",
      "Training loss for batch 3637 : 0.04010193422436714\n",
      "Training loss for batch 3638 : 0.021509215235710144\n",
      "Training loss for batch 3639 : 0.08533371984958649\n",
      "Training loss for batch 3640 : 0.12679022550582886\n",
      "Training loss for batch 3641 : 0.045297108590602875\n",
      "Training loss for batch 3642 : 0.008931334130465984\n",
      "Training loss for batch 3643 : 0.07478098571300507\n",
      "Training loss for batch 3644 : 0.037571921944618225\n",
      "Training loss for batch 3645 : 0.11099300533533096\n",
      "Training loss for batch 3646 : 0.038569375872612\n",
      "Training loss for batch 3647 : 0.1336846500635147\n",
      "Training loss for batch 3648 : 0.17343467473983765\n",
      "Training loss for batch 3649 : 0.02223195880651474\n",
      "Training loss for batch 3650 : 5.613854980879296e-08\n",
      "Training loss for batch 3651 : 0.0\n",
      "Training loss for batch 3652 : 0.025404082611203194\n",
      "Training loss for batch 3653 : 0.1670595109462738\n",
      "Training loss for batch 3654 : 9.606616657720224e-08\n",
      "Training loss for batch 3655 : 0.053357258439064026\n",
      "Training loss for batch 3656 : 0.29381710290908813\n",
      "Training loss for batch 3657 : 0.011367402039468288\n",
      "Training loss for batch 3658 : 0.06660616397857666\n",
      "Training loss for batch 3659 : 0.03850814700126648\n",
      "Training loss for batch 3660 : 0.15112896263599396\n",
      "Training loss for batch 3661 : 0.13727784156799316\n",
      "Training loss for batch 3662 : 0.1895248144865036\n",
      "Training loss for batch 3663 : 0.15959323942661285\n",
      "Training loss for batch 3664 : 0.04119390249252319\n",
      "Training loss for batch 3665 : 0.060059815645217896\n",
      "Training loss for batch 3666 : 0.12720277905464172\n",
      "Training loss for batch 3667 : 0.23618103563785553\n",
      "Training loss for batch 3668 : 0.08686007559299469\n",
      "Training loss for batch 3669 : 0.04194340109825134\n",
      "Training loss for batch 3670 : 0.02860257960855961\n",
      "Training loss for batch 3671 : 0.06086548790335655\n",
      "Training loss for batch 3672 : 0.130575031042099\n",
      "Training loss for batch 3673 : 0.17215868830680847\n",
      "Training loss for batch 3674 : 0.027778958901762962\n",
      "Training loss for batch 3675 : 0.039529018104076385\n",
      "Training loss for batch 3676 : 0.23051957786083221\n",
      "Training loss for batch 3677 : 0.058365847915410995\n",
      "Training loss for batch 3678 : 0.22851333022117615\n",
      "Training loss for batch 3679 : 0.07917893677949905\n",
      "Training loss for batch 3680 : 0.02775740623474121\n",
      "Training loss for batch 3681 : 0.06760912388563156\n",
      "Training loss for batch 3682 : 0.02705259621143341\n",
      "Training loss for batch 3683 : 0.2148085981607437\n",
      "Training loss for batch 3684 : 0.07877963036298752\n",
      "Training loss for batch 3685 : 0.024015916511416435\n",
      "Training loss for batch 3686 : 0.11172480136156082\n",
      "Training loss for batch 3687 : 0.11843163520097733\n",
      "Training loss for batch 3688 : 0.16916827857494354\n",
      "Training loss for batch 3689 : 0.23110748827457428\n",
      "Training loss for batch 3690 : 0.14396443963050842\n",
      "Training loss for batch 3691 : 0.14745858311653137\n",
      "Training loss for batch 3692 : 0.07452230900526047\n",
      "Training loss for batch 3693 : 0.024330737069249153\n",
      "Training loss for batch 3694 : 0.07125470787286758\n",
      "Training loss for batch 3695 : 0.024814696982502937\n",
      "Training loss for batch 3696 : 0.12063568085432053\n",
      "Training loss for batch 3697 : 0.04441656172275543\n",
      "Training loss for batch 3698 : 0.0770711749792099\n",
      "Training loss for batch 3699 : 0.0639028251171112\n",
      "Training loss for batch 3700 : 0.08677428215742111\n",
      "Training loss for batch 3701 : 0.006598443258553743\n",
      "Training loss for batch 3702 : 0.03341449052095413\n",
      "Training loss for batch 3703 : 0.08219559490680695\n",
      "Training loss for batch 3704 : 0.17535655200481415\n",
      "Training loss for batch 3705 : 0.17803214490413666\n",
      "Training loss for batch 3706 : 0.07231405377388\n",
      "Training loss for batch 3707 : 0.009750674478709698\n",
      "Training loss for batch 3708 : 0.05064721778035164\n",
      "Training loss for batch 3709 : 0.11217337846755981\n",
      "Training loss for batch 3710 : 0.10344036668539047\n",
      "Training loss for batch 3711 : 0.09489493817090988\n",
      "Training loss for batch 3712 : 0.1701241135597229\n",
      "Training loss for batch 3713 : 0.2336045652627945\n",
      "Training loss for batch 3714 : 0.16806119680404663\n",
      "Training loss for batch 3715 : 0.08599705994129181\n",
      "Training loss for batch 3716 : 0.045146044343709946\n",
      "Training loss for batch 3717 : 0.045600034296512604\n",
      "Training loss for batch 3718 : 0.08117187768220901\n",
      "Training loss for batch 3719 : 0.08271607011556625\n",
      "Training loss for batch 3720 : 0.33525601029396057\n",
      "Training loss for batch 3721 : 0.058201923966407776\n",
      "Training loss for batch 3722 : 0.06227090209722519\n",
      "Training loss for batch 3723 : 0.08412351459264755\n",
      "Training loss for batch 3724 : 0.07377029955387115\n",
      "Training loss for batch 3725 : 0.13577476143836975\n",
      "Training loss for batch 3726 : 0.09348814934492111\n",
      "Training loss for batch 3727 : 0.05888327583670616\n",
      "Training loss for batch 3728 : 0.09028775244951248\n",
      "Training loss for batch 3729 : 0.04743687063455582\n",
      "Training loss for batch 3730 : 0.0479964055120945\n",
      "Training loss for batch 3731 : 0.03577873110771179\n",
      "Training loss for batch 3732 : 0.07349501550197601\n",
      "Training loss for batch 3733 : 0.0018245202954858541\n",
      "Training loss for batch 3734 : 0.03760399669408798\n",
      "Training loss for batch 3735 : 0.06252671033143997\n",
      "Training loss for batch 3736 : 0.040881577879190445\n",
      "Training loss for batch 3737 : 0.07606449723243713\n",
      "Training loss for batch 3738 : 0.004355314653366804\n",
      "Training loss for batch 3739 : 0.059778667986392975\n",
      "Training loss for batch 3740 : 0.003098582848906517\n",
      "Training loss for batch 3741 : 0.04910534247756004\n",
      "Training loss for batch 3742 : 0.07825956493616104\n",
      "Training loss for batch 3743 : 0.07055950909852982\n",
      "Training loss for batch 3744 : 0.03251146525144577\n",
      "Training loss for batch 3745 : 0.064414843916893\n",
      "Training loss for batch 3746 : 0.15461352467536926\n",
      "Training loss for batch 3747 : 0.011353407055139542\n",
      "Training loss for batch 3748 : 0.09960178285837173\n",
      "Training loss for batch 3749 : 0.09931232780218124\n",
      "Training loss for batch 3750 : 0.16871778666973114\n",
      "Training loss for batch 3751 : 0.2094244807958603\n",
      "Training loss for batch 3752 : 0.15643206238746643\n",
      "Training loss for batch 3753 : 0.11024419963359833\n",
      "Training loss for batch 3754 : 0.08063001930713654\n",
      "Training loss for batch 3755 : 0.029649382457137108\n",
      "Training loss for batch 3756 : 0.030516492202878\n",
      "Training loss for batch 3757 : 0.03080761805176735\n",
      "Training loss for batch 3758 : 0.08071307092905045\n",
      "Training loss for batch 3759 : 0.10502724349498749\n",
      "Training loss for batch 3760 : 0.18225370347499847\n",
      "Training loss for batch 3761 : 0.12912185490131378\n",
      "Training loss for batch 3762 : 0.09250744432210922\n",
      "Training loss for batch 3763 : 0.04866752028465271\n",
      "Training loss for batch 3764 : 0.05926577374339104\n",
      "Training loss for batch 3765 : 0.06666601449251175\n",
      "Training loss for batch 3766 : 0.014098375104367733\n",
      "Training loss for batch 3767 : 0.07550549507141113\n",
      "Training loss for batch 3768 : 0.04958117753267288\n",
      "Training loss for batch 3769 : 0.10176361352205276\n",
      "Training loss for batch 3770 : 0.04649028927087784\n",
      "Training loss for batch 3771 : 0.0937998965382576\n",
      "Training loss for batch 3772 : 0.0203793253749609\n",
      "Training loss for batch 3773 : 0.11696292459964752\n",
      "Training loss for batch 3774 : 0.21373657882213593\n",
      "Training loss for batch 3775 : 0.2543770372867584\n",
      "Training loss for batch 3776 : 0.0920669212937355\n",
      "Training loss for batch 3777 : 0.01979888044297695\n",
      "Training loss for batch 3778 : 0.007143120281398296\n",
      "Training loss for batch 3779 : 0.04316968470811844\n",
      "Training loss for batch 3780 : 0.12505993247032166\n",
      "Training loss for batch 3781 : 0.12483442574739456\n",
      "Training loss for batch 3782 : 0.03873039782047272\n",
      "Training loss for batch 3783 : 0.0913628414273262\n",
      "Training loss for batch 3784 : 0.10159625858068466\n",
      "Training loss for batch 3785 : 0.057474881410598755\n",
      "Training loss for batch 3786 : 0.09766462445259094\n",
      "Training loss for batch 3787 : 0.12701047956943512\n",
      "Training loss for batch 3788 : 0.041717223823070526\n",
      "Training loss for batch 3789 : 0.13695895671844482\n",
      "Training loss for batch 3790 : 0.09518815577030182\n",
      "Training loss for batch 3791 : 0.04351826757192612\n",
      "Training loss for batch 3792 : 0.019240092486143112\n",
      "Training loss for batch 3793 : 0.13896891474723816\n",
      "Training loss for batch 3794 : 0.05188937485218048\n",
      "Training loss for batch 3795 : 0.1574004739522934\n",
      "Training loss for batch 3796 : 0.14375294744968414\n",
      "Training loss for batch 3797 : 0.16786617040634155\n",
      "Training loss for batch 3798 : 0.2367231696844101\n",
      "Training loss for batch 3799 : 0.027509136125445366\n",
      "Training loss for batch 3800 : 0.013324571773409843\n",
      "Training loss for batch 3801 : 0.07974090427160263\n",
      "Training loss for batch 3802 : 0.055382099002599716\n",
      "Training loss for batch 3803 : 0.011495690792798996\n",
      "Training loss for batch 3804 : 0.06725557148456573\n",
      "Training loss for batch 3805 : 0.00699990289285779\n",
      "Training loss for batch 3806 : 0.0018173543503507972\n",
      "Training loss for batch 3807 : 0.07684945315122604\n",
      "Training loss for batch 3808 : 0.04826364293694496\n",
      "Training loss for batch 3809 : 0.009395888075232506\n",
      "Training loss for batch 3810 : 0.14660601317882538\n",
      "Training loss for batch 3811 : 0.04655367136001587\n",
      "Training loss for batch 3812 : 0.021801315248012543\n",
      "Training loss for batch 3813 : 0.14372101426124573\n",
      "Training loss for batch 3814 : 0.06794354319572449\n",
      "Training loss for batch 3815 : 0.12041597813367844\n",
      "Training loss for batch 3816 : 0.13967818021774292\n",
      "Training loss for batch 3817 : 0.040383584797382355\n",
      "Training loss for batch 3818 : 0.1385398954153061\n",
      "Training loss for batch 3819 : 0.08792273700237274\n",
      "Training loss for batch 3820 : 0.05382354184985161\n",
      "Training loss for batch 3821 : 0.10615475475788116\n",
      "Training loss for batch 3822 : 0.12692657113075256\n",
      "Training loss for batch 3823 : 0.14341521263122559\n",
      "Training loss for batch 3824 : 0.02686544880270958\n",
      "Training loss for batch 3825 : 0.20344069600105286\n",
      "Training loss for batch 3826 : 0.11286136507987976\n",
      "Training loss for batch 3827 : 0.061494674533605576\n",
      "Training loss for batch 3828 : 0.11460521072149277\n",
      "Training loss for batch 3829 : 1.7835930066212313e-07\n",
      "Training loss for batch 3830 : 0.14971192181110382\n",
      "Training loss for batch 3831 : 0.09188295900821686\n",
      "Training loss for batch 3832 : 0.16922615468502045\n",
      "Training loss for batch 3833 : 0.19223618507385254\n",
      "Training loss for batch 3834 : 0.06088456138968468\n",
      "Training loss for batch 3835 : 0.048061903566122055\n",
      "Training loss for batch 3836 : 0.19120663404464722\n",
      "Training loss for batch 3837 : 0.06644471734762192\n",
      "Training loss for batch 3838 : 0.13118807971477509\n",
      "Training loss for batch 3839 : 0.11195352673530579\n",
      "Training loss for batch 3840 : 0.2814643383026123\n",
      "Training loss for batch 3841 : 0.014547997154295444\n",
      "Training loss for batch 3842 : 0.08797728270292282\n",
      "Training loss for batch 3843 : 0.01253272034227848\n",
      "Training loss for batch 3844 : 0.04970317333936691\n",
      "Training loss for batch 3845 : 0.043307673186063766\n",
      "Training loss for batch 3846 : 0.08811678737401962\n",
      "Training loss for batch 3847 : 0.08583077788352966\n",
      "Training loss for batch 3848 : 0.06079010292887688\n",
      "Training loss for batch 3849 : 0.11242838948965073\n",
      "Training loss for batch 3850 : 0.09703104943037033\n",
      "Training loss for batch 3851 : 0.023928996175527573\n",
      "Training loss for batch 3852 : 0.12535999715328217\n",
      "Training loss for batch 3853 : 0.05604633688926697\n",
      "Training loss for batch 3854 : 0.06015265733003616\n",
      "Training loss for batch 3855 : 0.04377777874469757\n",
      "Training loss for batch 3856 : 0.11861136555671692\n",
      "Training loss for batch 3857 : 0.011073348112404346\n",
      "Training loss for batch 3858 : 0.18403245508670807\n",
      "Training loss for batch 3859 : 0.046548809856176376\n",
      "Training loss for batch 3860 : 0.1628401279449463\n",
      "Training loss for batch 3861 : 0.011604254133999348\n",
      "Training loss for batch 3862 : 0.08400414139032364\n",
      "Training loss for batch 3863 : 0.06224590539932251\n",
      "Training loss for batch 3864 : 0.008651399984955788\n",
      "Training loss for batch 3865 : 0.14544545114040375\n",
      "Training loss for batch 3866 : 0.007426284719258547\n",
      "Training loss for batch 3867 : 0.07294293493032455\n",
      "Training loss for batch 3868 : 0.07061350345611572\n",
      "Training loss for batch 3869 : 0.06180863082408905\n",
      "Training loss for batch 3870 : 0.04067816585302353\n",
      "Training loss for batch 3871 : 0.04721035808324814\n",
      "Training loss for batch 3872 : 0.11639557033777237\n",
      "Training loss for batch 3873 : 0.030737143009901047\n",
      "Training loss for batch 3874 : 0.09740079939365387\n",
      "Training loss for batch 3875 : 0.0811132937669754\n",
      "Training loss for batch 3876 : 0.04829249158501625\n",
      "Training loss for batch 3877 : 0.15638339519500732\n",
      "Training loss for batch 3878 : 0.14670589566230774\n",
      "Training loss for batch 3879 : 0.05506712570786476\n",
      "Training loss for batch 3880 : 0.12836547195911407\n",
      "Training loss for batch 3881 : 0.03481845185160637\n",
      "Training loss for batch 3882 : 0.06102742254734039\n",
      "Training loss for batch 3883 : 0.04369969293475151\n",
      "Training loss for batch 3884 : 0.09564456343650818\n",
      "Training loss for batch 3885 : 0.0705319195985794\n",
      "Training loss for batch 3886 : 0.11338186264038086\n",
      "Training loss for batch 3887 : 0.05919302999973297\n",
      "Training loss for batch 3888 : 0.10975641757249832\n",
      "Training loss for batch 3889 : 0.13185206055641174\n",
      "Training loss for batch 3890 : 0.02231835015118122\n",
      "Training loss for batch 3891 : 0.08325741440057755\n",
      "Training loss for batch 3892 : 0.04530836641788483\n",
      "Training loss for batch 3893 : 0.2171289175748825\n",
      "Training loss for batch 3894 : 0.029117176309227943\n",
      "Training loss for batch 3895 : 0.06875623017549515\n",
      "Training loss for batch 3896 : 0.06285452097654343\n",
      "Training loss for batch 3897 : 0.05468468740582466\n",
      "Training loss for batch 3898 : 0.10070827603340149\n",
      "Training loss for batch 3899 : 0.007147887721657753\n",
      "Training loss for batch 3900 : 0.07444673031568527\n",
      "Training loss for batch 3901 : 0.04653869941830635\n",
      "Training loss for batch 3902 : 0.0699838176369667\n",
      "Training loss for batch 3903 : 0.08932410925626755\n",
      "Training loss for batch 3904 : 0.01743462309241295\n",
      "Training loss for batch 3905 : 0.12810556590557098\n",
      "Training loss for batch 3906 : 0.10248133540153503\n",
      "Training loss for batch 3907 : 0.11800859868526459\n",
      "Training loss for batch 3908 : 0.10573191940784454\n",
      "Training loss for batch 3909 : 0.05828353762626648\n",
      "Training loss for batch 3910 : 0.08734207600355148\n",
      "Training loss for batch 3911 : 0.03710351511836052\n",
      "Training loss for batch 3912 : 0.12262798100709915\n",
      "Training loss for batch 3913 : 0.11501798778772354\n",
      "Training loss for batch 3914 : 0.15916648507118225\n",
      "Training loss for batch 3915 : 0.07388433068990707\n",
      "Training loss for batch 3916 : 0.008052538149058819\n",
      "Training loss for batch 3917 : 0.05806552246212959\n",
      "Training loss for batch 3918 : 0.10354463756084442\n",
      "Training loss for batch 3919 : 0.17502668499946594\n",
      "Training loss for batch 3920 : 0.06243199110031128\n",
      "Training loss for batch 3921 : 0.05629223212599754\n",
      "Training loss for batch 3922 : 0.048560746014118195\n",
      "Training loss for batch 3923 : 0.15556761622428894\n",
      "Training loss for batch 3924 : 0.0255255326628685\n",
      "Training loss for batch 3925 : 0.08785580843687057\n",
      "Training loss for batch 3926 : 0.02584032155573368\n",
      "Training loss for batch 3927 : 0.07291252166032791\n",
      "Training loss for batch 3928 : 0.098619744181633\n",
      "Training loss for batch 3929 : 0.02272777259349823\n",
      "Training loss for batch 3930 : 0.05743682011961937\n",
      "Training loss for batch 3931 : 0.12161143124103546\n",
      "Training loss for batch 3932 : 0.04750858619809151\n",
      "Training loss for batch 3933 : 0.011758803389966488\n",
      "Training loss for batch 3934 : 0.04089207574725151\n",
      "Training loss for batch 3935 : 0.08104480057954788\n",
      "Training loss for batch 3936 : 0.08877814561128616\n",
      "Training loss for batch 3937 : 0.0563262514770031\n",
      "Training loss for batch 3938 : 0.1274615228176117\n",
      "Training loss for batch 3939 : 0.1758223921060562\n",
      "Training loss for batch 3940 : 0.02323116362094879\n",
      "Training loss for batch 3941 : 0.21884413063526154\n",
      "Training loss for batch 3942 : 0.06605429947376251\n",
      "Training loss for batch 3943 : 0.23477739095687866\n",
      "Training loss for batch 3944 : 0.11712908744812012\n",
      "Training loss for batch 3945 : 0.02018415927886963\n",
      "Training loss for batch 3946 : 0.07982860505580902\n",
      "Training loss for batch 3947 : 0.030472224578261375\n",
      "Training loss for batch 3948 : 0.06170229613780975\n",
      "Training loss for batch 3949 : 0.11864767223596573\n",
      "Training loss for batch 3950 : 0.11575739085674286\n",
      "Training loss for batch 3951 : 0.03519401699304581\n",
      "Training loss for batch 3952 : 0.06139880791306496\n",
      "Training loss for batch 3953 : 0.12366810441017151\n",
      "Training loss for batch 3954 : 0.08639339357614517\n",
      "Training loss for batch 3955 : 0.027419060468673706\n",
      "Training loss for batch 3956 : 0.043220896273851395\n",
      "Training loss for batch 3957 : 0.05830775201320648\n",
      "Training loss for batch 3958 : 0.04433714598417282\n",
      "Training loss for batch 3959 : 0.08126577734947205\n",
      "Training loss for batch 3960 : 0.10129274427890778\n",
      "Training loss for batch 3961 : 0.05059933662414551\n",
      "Training loss for batch 3962 : 0.04194065183401108\n",
      "Training loss for batch 3963 : 0.0824507623910904\n",
      "Training loss for batch 3964 : 0.057357192039489746\n",
      "Training loss for batch 3965 : 0.15641848742961884\n",
      "Training loss for batch 3966 : 0.05689352750778198\n",
      "Training loss for batch 3967 : 0.037892453372478485\n",
      "Training loss for batch 3968 : 0.029272178187966347\n",
      "Training loss for batch 3969 : 0.02178281545639038\n",
      "Training loss for batch 3970 : 0.02641173079609871\n",
      "Training loss for batch 3971 : 0.05446171388030052\n",
      "Training loss for batch 3972 : 0.010251741856336594\n",
      "Training loss for batch 3973 : 0.06829892843961716\n",
      "Training loss for batch 3974 : 0.12675416469573975\n",
      "Training loss for batch 3975 : 0.35649219155311584\n",
      "Training loss for batch 3976 : 0.09119446575641632\n",
      "Training loss for batch 3977 : 0.0583503320813179\n",
      "Training loss for batch 3978 : 0.08481960743665695\n",
      "Training loss for batch 3979 : 0.08560549467802048\n",
      "Training loss for batch 3980 : 0.04786102473735809\n",
      "Training loss for batch 3981 : 0.04618934169411659\n",
      "Training loss for batch 3982 : 0.05712045729160309\n",
      "Training loss for batch 3983 : 0.04571019485592842\n",
      "Training loss for batch 3984 : 0.010598460212349892\n",
      "Training loss for batch 3985 : 0.05307936295866966\n",
      "Training loss for batch 3986 : 0.25051623582839966\n",
      "Training loss for batch 3987 : 0.1860884726047516\n",
      "Training loss for batch 3988 : 0.11108073592185974\n",
      "Training loss for batch 3989 : 0.08766244351863861\n",
      "Training loss for batch 3990 : 0.09872553497552872\n",
      "Training loss for batch 3991 : 0.12981858849525452\n",
      "Training loss for batch 3992 : 0.067270927131176\n",
      "Training loss for batch 3993 : 0.010121365077793598\n",
      "Training loss for batch 3994 : 0.053174518048763275\n",
      "Training loss for batch 3995 : 0.09208112955093384\n",
      "Training loss for batch 3996 : 0.043944310396909714\n",
      "Training loss for batch 3997 : 0.12741219997406006\n",
      "Training loss for batch 3998 : 0.047421347349882126\n",
      "Training loss for batch 3999 : 0.052539799362421036\n",
      "Training loss for batch 4000 : 0.1321326047182083\n",
      "Training loss for batch 4001 : 0.012141510844230652\n",
      "Training loss for batch 4002 : 0.09067760407924652\n",
      "Training loss for batch 4003 : 0.15517139434814453\n",
      "Training loss for batch 4004 : 0.01592009700834751\n",
      "Training loss for batch 4005 : 0.1147633045911789\n",
      "Training loss for batch 4006 : 0.0945749431848526\n",
      "Training loss for batch 4007 : 0.14365413784980774\n",
      "Training loss for batch 4008 : 0.05055014789104462\n",
      "Training loss for batch 4009 : 0.0975058302283287\n",
      "Training loss for batch 4010 : 0.020421884953975677\n",
      "Training loss for batch 4011 : 0.09627193957567215\n",
      "Training loss for batch 4012 : 0.07746440172195435\n",
      "Training loss for batch 4013 : 0.19214800000190735\n",
      "Training loss for batch 4014 : 0.08812215924263\n",
      "Training loss for batch 4015 : 0.035864681005477905\n",
      "Training loss for batch 4016 : 0.07861229032278061\n",
      "Training loss for batch 4017 : 0.01998727209866047\n",
      "Training loss for batch 4018 : 0.1551194041967392\n",
      "Training loss for batch 4019 : 0.11427187919616699\n",
      "Training loss for batch 4020 : 0.14075657725334167\n",
      "Training loss for batch 4021 : 0.020840704441070557\n",
      "Training loss for batch 4022 : 0.043035365641117096\n",
      "Training loss for batch 4023 : 0.0\n",
      "Training loss for batch 4024 : 0.11187829822301865\n",
      "Training loss for batch 4025 : 0.12983952462673187\n",
      "Training loss for batch 4026 : 0.02820676751434803\n",
      "Training loss for batch 4027 : 0.12542679905891418\n",
      "Training loss for batch 4028 : 0.12999327480793\n",
      "Training loss for batch 4029 : 0.13839130103588104\n",
      "Training loss for batch 4030 : 0.0114644980058074\n",
      "Training loss for batch 4031 : 0.031169258058071136\n",
      "Training loss for batch 4032 : 0.08923876285552979\n",
      "Training loss for batch 4033 : 0.09987373650074005\n",
      "Training loss for batch 4034 : 0.03187521547079086\n",
      "Training loss for batch 4035 : 0.09214238077402115\n",
      "Training loss for batch 4036 : 0.08883656561374664\n",
      "Training loss for batch 4037 : 0.0014005373232066631\n",
      "Training loss for batch 4038 : 0.043790172785520554\n",
      "Training loss for batch 4039 : 0.14994919300079346\n",
      "Training loss for batch 4040 : 0.14012649655342102\n",
      "Training loss for batch 4041 : 0.12571710348129272\n",
      "Training loss for batch 4042 : 0.02870633453130722\n",
      "Training loss for batch 4043 : 0.028102301061153412\n",
      "Training loss for batch 4044 : 0.028711531311273575\n",
      "Training loss for batch 4045 : 0.08233888447284698\n",
      "Training loss for batch 4046 : 0.11737650632858276\n",
      "Training loss for batch 4047 : 0.05658065155148506\n",
      "Training loss for batch 4048 : 0.06407049298286438\n",
      "Training loss for batch 4049 : 0.06514369696378708\n",
      "Training loss for batch 4050 : 0.0692206472158432\n",
      "Training loss for batch 4051 : 0.19069430232048035\n",
      "Training loss for batch 4052 : 0.09342534840106964\n",
      "Training loss for batch 4053 : 0.010968283750116825\n",
      "Training loss for batch 4054 : 0.032175131142139435\n",
      "Training loss for batch 4055 : 0.08751952648162842\n",
      "Training loss for batch 4056 : 0.034186434000730515\n",
      "Training loss for batch 4057 : 0.16922493278980255\n",
      "Training loss for batch 4058 : 0.05444273725152016\n",
      "Training loss for batch 4059 : 0.00778916897252202\n",
      "Training loss for batch 4060 : 0.06260615587234497\n",
      "Training loss for batch 4061 : 0.024231715127825737\n",
      "Training loss for batch 4062 : 0.08657658845186234\n",
      "Training loss for batch 4063 : 0.20973604917526245\n",
      "Training loss for batch 4064 : 0.1641881912946701\n",
      "Training loss for batch 4065 : 0.021531198173761368\n",
      "Training loss for batch 4066 : 0.14254432916641235\n",
      "Training loss for batch 4067 : 0.08689609169960022\n",
      "Training loss for batch 4068 : 0.16498400270938873\n",
      "Training loss for batch 4069 : 0.05160489305853844\n",
      "Training loss for batch 4070 : 0.03066905029118061\n",
      "Training loss for batch 4071 : 0.15827280282974243\n",
      "Training loss for batch 4072 : 0.13369551301002502\n",
      "Training loss for batch 4073 : 0.11221125721931458\n",
      "Training loss for batch 4074 : 0.0910380482673645\n",
      "Training loss for batch 4075 : 0.1628577560186386\n",
      "Training loss for batch 4076 : 0.02831459790468216\n",
      "Training loss for batch 4077 : 0.04340355843305588\n",
      "Training loss for batch 4078 : 0.0279191043227911\n",
      "Training loss for batch 4079 : 0.11958126723766327\n",
      "Training loss for batch 4080 : 0.10505878180265427\n",
      "Training loss for batch 4081 : 0.07142627984285355\n",
      "Training loss for batch 4082 : 0.028424875810742378\n",
      "Training loss for batch 4083 : 0.052145425230264664\n",
      "Training loss for batch 4084 : 0.050899457186460495\n",
      "Training loss for batch 4085 : 0.04175145551562309\n",
      "Training loss for batch 4086 : 0.13978812098503113\n",
      "Training loss for batch 4087 : 0.12549017369747162\n",
      "Training loss for batch 4088 : 0.08436714112758636\n",
      "Training loss for batch 4089 : 0.10881992429494858\n",
      "Training loss for batch 4090 : 0.10760067403316498\n",
      "Training loss for batch 4091 : 0.06849278509616852\n",
      "Training loss for batch 4092 : 0.01906009018421173\n",
      "Training loss for batch 4093 : 0.21230891346931458\n",
      "Training loss for batch 4094 : 0.2926445007324219\n",
      "Training loss for batch 4095 : 0.06478994339704514\n",
      "Training loss for batch 4096 : 0.09044938534498215\n",
      "Training loss for batch 4097 : 0.03635567054152489\n",
      "Training loss for batch 4098 : 0.024822071194648743\n",
      "Training loss for batch 4099 : 0.029889363795518875\n",
      "Training loss for batch 4100 : 0.09553001821041107\n",
      "Training loss for batch 4101 : 0.09548132866621017\n",
      "Training loss for batch 4102 : 0.04956731200218201\n",
      "Training loss for batch 4103 : 0.16026154160499573\n",
      "Training loss for batch 4104 : 0.21234138309955597\n",
      "Training loss for batch 4105 : 0.018294118344783783\n",
      "Training loss for batch 4106 : 0.019807010889053345\n",
      "Training loss for batch 4107 : 0.0774163082242012\n",
      "Training loss for batch 4108 : 0.05479452759027481\n",
      "Training loss for batch 4109 : 0.029746271669864655\n",
      "Training loss for batch 4110 : 0.06536433845758438\n",
      "Training loss for batch 4111 : 0.09856445342302322\n",
      "Training loss for batch 4112 : 0.09854202717542648\n",
      "Training loss for batch 4113 : 0.015221680514514446\n",
      "Training loss for batch 4114 : 0.016849391162395477\n",
      "Training loss for batch 4115 : 0.16844351589679718\n",
      "Training loss for batch 4116 : 0.11003532260656357\n",
      "Training loss for batch 4117 : 0.058398108929395676\n",
      "Training loss for batch 4118 : 0.053281236439943314\n",
      "Training loss for batch 4119 : 0.07023629546165466\n",
      "Training loss for batch 4120 : 0.12808597087860107\n",
      "Training loss for batch 4121 : 0.09007516503334045\n",
      "Training loss for batch 4122 : 0.07120013982057571\n",
      "Training loss for batch 4123 : 0.06062130630016327\n",
      "Training loss for batch 4124 : 0.062463197857141495\n",
      "Training loss for batch 4125 : 0.07812044769525528\n",
      "Training loss for batch 4126 : 0.20596136152744293\n",
      "Training loss for batch 4127 : 0.13148055970668793\n",
      "Training loss for batch 4128 : 0.09080144017934799\n",
      "Training loss for batch 4129 : 0.03917473554611206\n",
      "Training loss for batch 4130 : 0.17693327367305756\n",
      "Training loss for batch 4131 : 0.03996487334370613\n",
      "Training loss for batch 4132 : 0.17357787489891052\n",
      "Training loss for batch 4133 : 0.057440534234046936\n",
      "Training loss for batch 4134 : 0.1179036870598793\n",
      "Training loss for batch 4135 : 0.27413472533226013\n",
      "Training loss for batch 4136 : 0.08619775623083115\n",
      "Training loss for batch 4137 : 0.068318210542202\n",
      "Training loss for batch 4138 : 0.06971076130867004\n",
      "Training loss for batch 4139 : 0.09333931654691696\n",
      "Training loss for batch 4140 : 0.06697944551706314\n",
      "Training loss for batch 4141 : 0.01547100767493248\n",
      "Training loss for batch 4142 : 0.10201175510883331\n",
      "Training loss for batch 4143 : 0.03516344726085663\n",
      "Training loss for batch 4144 : 0.0290142260491848\n",
      "Training loss for batch 4145 : 0.05650372803211212\n",
      "Training loss for batch 4146 : 0.01274322159588337\n",
      "Training loss for batch 4147 : 0.02038625068962574\n",
      "Training loss for batch 4148 : 0.04372139647603035\n",
      "Training loss for batch 4149 : 0.03754789009690285\n",
      "Training loss for batch 4150 : 0.20012415945529938\n",
      "Training loss for batch 4151 : 0.050868190824985504\n",
      "Training loss for batch 4152 : 0.13810217380523682\n",
      "Training loss for batch 4153 : 0.0040657236240804195\n",
      "Parameter containing:\n",
      "tensor(-0.2614, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.0000100135803223\n",
      "Training loss for batch 1 : 1.0000100135803223\n",
      "Training loss for batch 2 : 1.0000100135803223\n",
      "Training loss for batch 3 : 1.0000100135803223\n",
      "Training loss for batch 4 : 1.0000098943710327\n",
      "Training loss for batch 5 : 1.0000100135803223\n",
      "Training loss for batch 6 : 1.0000098943710327\n",
      "Training loss for batch 7 : 1.0000100135803223\n",
      "Training loss for batch 8 : 1.0000100135803223\n",
      "Training loss for batch 9 : 1.0000100135803223\n",
      "Training loss for batch 10 : 1.0000100135803223\n",
      "Training loss for batch 11 : 1.0000100135803223\n",
      "Training loss for batch 12 : 0.06603266298770905\n",
      "Training loss for batch 13 : 0.014907541684806347\n",
      "Training loss for batch 14 : 0.1543578952550888\n",
      "Training loss for batch 15 : 0.10062749683856964\n",
      "Training loss for batch 16 : 0.051917318254709244\n",
      "Training loss for batch 17 : 0.036596816033124924\n",
      "Training loss for batch 18 : 0.028108593076467514\n",
      "Training loss for batch 19 : 0.022472470998764038\n",
      "Training loss for batch 20 : 0.27144208550453186\n",
      "Training loss for batch 21 : 0.028937995433807373\n",
      "Training loss for batch 22 : 0.03646521270275116\n",
      "Training loss for batch 23 : 0.06295102089643478\n",
      "Training loss for batch 24 : 0.07437718659639359\n",
      "Training loss for batch 25 : 0.0038428842090070248\n",
      "Training loss for batch 26 : 0.1758439540863037\n",
      "Training loss for batch 27 : 0.04979625344276428\n",
      "Training loss for batch 28 : 0.08574820309877396\n",
      "Training loss for batch 29 : 0.03856153413653374\n",
      "Training loss for batch 30 : 0.05704087018966675\n",
      "Training loss for batch 31 : 0.05699331313371658\n",
      "Training loss for batch 32 : 0.04560679942369461\n",
      "Training loss for batch 33 : 0.0588228702545166\n",
      "Training loss for batch 34 : 0.1479872614145279\n",
      "Training loss for batch 35 : 0.1387467235326767\n",
      "Training loss for batch 36 : 0.05975888669490814\n",
      "Training loss for batch 37 : 0.07011528313159943\n",
      "Training loss for batch 38 : 0.15408647060394287\n",
      "Training loss for batch 39 : 0.07110711187124252\n",
      "Training loss for batch 40 : 0.05625293776392937\n",
      "Training loss for batch 41 : 0.005652856081724167\n",
      "Training loss for batch 42 : 0.08129409700632095\n",
      "Training loss for batch 43 : 0.22457996010780334\n",
      "Training loss for batch 44 : 0.047437284141778946\n",
      "Training loss for batch 45 : 0.12340617924928665\n",
      "Training loss for batch 46 : 0.044452499598264694\n",
      "Training loss for batch 47 : 0.01426906418055296\n",
      "Training loss for batch 48 : 0.08959585428237915\n",
      "Training loss for batch 49 : 0.04232000559568405\n",
      "Training loss for batch 50 : 0.030803658068180084\n",
      "Training loss for batch 51 : 0.07746460288763046\n",
      "Training loss for batch 52 : 0.07675375789403915\n",
      "Training loss for batch 53 : 0.026926487684249878\n",
      "Training loss for batch 54 : 0.10721209645271301\n",
      "Training loss for batch 55 : 0.05335813760757446\n",
      "Training loss for batch 56 : 0.1528928130865097\n",
      "Training loss for batch 57 : 0.02601223811507225\n",
      "Training loss for batch 58 : 0.05324593931436539\n",
      "Training loss for batch 59 : 0.0014579633716493845\n",
      "Training loss for batch 60 : 0.08117882162332535\n",
      "Training loss for batch 61 : 0.060257818549871445\n",
      "Training loss for batch 62 : 0.02146254852414131\n",
      "Training loss for batch 63 : 0.00535804545506835\n",
      "Training loss for batch 64 : 0.06052522361278534\n",
      "Training loss for batch 65 : 0.0654425248503685\n",
      "Training loss for batch 66 : 0.12495449185371399\n",
      "Training loss for batch 67 : 0.05728853493928909\n",
      "Training loss for batch 68 : 0.14308536052703857\n",
      "Training loss for batch 69 : 0.19490842521190643\n",
      "Training loss for batch 70 : 0.06405499577522278\n",
      "Training loss for batch 71 : 0.035887815058231354\n",
      "Training loss for batch 72 : 0.13984568417072296\n",
      "Training loss for batch 73 : 0.027805916965007782\n",
      "Training loss for batch 74 : 0.015036683529615402\n",
      "Training loss for batch 75 : 0.0034076785668730736\n",
      "Training loss for batch 76 : 0.06191098690032959\n",
      "Training loss for batch 77 : 0.055632900446653366\n",
      "Training loss for batch 78 : 0.030071064829826355\n",
      "Training loss for batch 79 : 0.08449900895357132\n",
      "Training loss for batch 80 : 0.00541594298556447\n",
      "Training loss for batch 81 : 4.336814163252711e-05\n",
      "Training loss for batch 82 : 0.06633967906236649\n",
      "Training loss for batch 83 : 0.09091110527515411\n",
      "Training loss for batch 84 : 0.005086480174213648\n",
      "Training loss for batch 85 : 0.10685284435749054\n",
      "Training loss for batch 86 : 0.08789371699094772\n",
      "Training loss for batch 87 : 0.027337314561009407\n",
      "Training loss for batch 88 : 0.07683776319026947\n",
      "Training loss for batch 89 : 0.0027934941463172436\n",
      "Training loss for batch 90 : 0.06216510012745857\n",
      "Training loss for batch 91 : 0.0274422038346529\n",
      "Training loss for batch 92 : 0.02844553254544735\n",
      "Training loss for batch 93 : 0.021964596584439278\n",
      "Training loss for batch 94 : 0.11710219830274582\n",
      "Training loss for batch 95 : 0.06721600145101547\n",
      "Training loss for batch 96 : 0.048890121281147\n",
      "Training loss for batch 97 : 0.2840987741947174\n",
      "Training loss for batch 98 : 0.00125893612857908\n",
      "Training loss for batch 99 : 0.03668375685811043\n",
      "Training loss for batch 100 : 0.008989178575575352\n",
      "Training loss for batch 101 : 0.052604638040065765\n",
      "Training loss for batch 102 : 0.0\n",
      "Training loss for batch 103 : 0.03965461626648903\n",
      "Training loss for batch 104 : 0.007630886510014534\n",
      "Training loss for batch 105 : 0.0\n",
      "Training loss for batch 106 : 0.07412347942590714\n",
      "Training loss for batch 107 : 0.02672228403389454\n",
      "Training loss for batch 108 : 0.25357356667518616\n",
      "Training loss for batch 109 : 0.15668614208698273\n",
      "Training loss for batch 110 : 0.028239721432328224\n",
      "Training loss for batch 111 : 0.002208490390330553\n",
      "Training loss for batch 112 : 0.14602409303188324\n",
      "Training loss for batch 113 : 0.15225912630558014\n",
      "Training loss for batch 114 : 0.17498601973056793\n",
      "Training loss for batch 115 : 0.0798247754573822\n",
      "Training loss for batch 116 : 0.035011328756809235\n",
      "Training loss for batch 117 : 0.10616172850131989\n",
      "Training loss for batch 118 : 0.03363291174173355\n",
      "Training loss for batch 119 : 0.0\n",
      "Training loss for batch 120 : 0.2505997121334076\n",
      "Training loss for batch 121 : 0.058000970631837845\n",
      "Training loss for batch 122 : 0.0529516339302063\n",
      "Training loss for batch 123 : 0.007422490511089563\n",
      "Training loss for batch 124 : 5.5204933957497815e-09\n",
      "Training loss for batch 125 : 0.0735594630241394\n",
      "Training loss for batch 126 : 0.033662617206573486\n",
      "Training loss for batch 127 : 0.09085052460432053\n",
      "Training loss for batch 128 : 0.04240104556083679\n",
      "Training loss for batch 129 : 0.13205242156982422\n",
      "Training loss for batch 130 : 0.2213873714208603\n",
      "Training loss for batch 131 : 0.02511490136384964\n",
      "Training loss for batch 132 : 0.037112269550561905\n",
      "Training loss for batch 133 : 0.15876488387584686\n",
      "Training loss for batch 134 : 0.005597845651209354\n",
      "Training loss for batch 135 : 0.005730345379561186\n",
      "Training loss for batch 136 : 0.06505553424358368\n",
      "Training loss for batch 137 : 0.06943352520465851\n",
      "Training loss for batch 138 : 0.10531879216432571\n",
      "Training loss for batch 139 : 0.22684410214424133\n",
      "Training loss for batch 140 : 0.03342195972800255\n",
      "Training loss for batch 141 : 0.035687193274497986\n",
      "Training loss for batch 142 : 0.06391802430152893\n",
      "Training loss for batch 143 : 0.12425670027732849\n",
      "Training loss for batch 144 : 0.0063749048858881\n",
      "Training loss for batch 145 : 0.016024146229028702\n",
      "Training loss for batch 146 : 0.04813306778669357\n",
      "Training loss for batch 147 : 0.035961780697107315\n",
      "Training loss for batch 148 : 0.027515023946762085\n",
      "Training loss for batch 149 : 0.07429970800876617\n",
      "Training loss for batch 150 : 0.07712876051664352\n",
      "Training loss for batch 151 : 0.10787177085876465\n",
      "Training loss for batch 152 : 0.014971248805522919\n",
      "Training loss for batch 153 : 0.10345498472452164\n",
      "Training loss for batch 154 : 0.08264925330877304\n",
      "Training loss for batch 155 : 0.0708845853805542\n",
      "Training loss for batch 156 : 0.15314450860023499\n",
      "Training loss for batch 157 : 0.04887723550200462\n",
      "Training loss for batch 158 : 0.10187077522277832\n",
      "Training loss for batch 159 : 0.08448555320501328\n",
      "Training loss for batch 160 : 0.06711819767951965\n",
      "Training loss for batch 161 : 0.04442223161458969\n",
      "Training loss for batch 162 : 0.09679611027240753\n",
      "Training loss for batch 163 : 0.09536229074001312\n",
      "Training loss for batch 164 : 0.11258532851934433\n",
      "Training loss for batch 165 : 0.09314729273319244\n",
      "Training loss for batch 166 : 0.12992727756500244\n",
      "Training loss for batch 167 : 0.07146447896957397\n",
      "Training loss for batch 168 : 0.01255365926772356\n",
      "Training loss for batch 169 : 0.13649806380271912\n",
      "Training loss for batch 170 : 0.073527492582798\n",
      "Training loss for batch 171 : 0.058345772325992584\n",
      "Training loss for batch 172 : 0.12509173154830933\n",
      "Training loss for batch 173 : 0.06333327293395996\n",
      "Training loss for batch 174 : 0.07698964327573776\n",
      "Training loss for batch 175 : 0.06002240255475044\n",
      "Training loss for batch 176 : 0.0380139984190464\n",
      "Training loss for batch 177 : 0.11035321652889252\n",
      "Training loss for batch 178 : 0.15683163702487946\n",
      "Training loss for batch 179 : 0.03865640237927437\n",
      "Training loss for batch 180 : 0.08309295773506165\n",
      "Training loss for batch 181 : 0.06211473420262337\n",
      "Training loss for batch 182 : 0.10207194834947586\n",
      "Training loss for batch 183 : 0.061280954629182816\n",
      "Training loss for batch 184 : 0.11293287575244904\n",
      "Training loss for batch 185 : 0.035673175007104874\n",
      "Training loss for batch 186 : 0.03850318491458893\n",
      "Training loss for batch 187 : 0.1357044130563736\n",
      "Training loss for batch 188 : 0.10098985582590103\n",
      "Training loss for batch 189 : 0.1479911506175995\n",
      "Training loss for batch 190 : 0.021267453208565712\n",
      "Training loss for batch 191 : 0.07382013648748398\n",
      "Training loss for batch 192 : 0.13267087936401367\n",
      "Training loss for batch 193 : 0.17463411390781403\n",
      "Training loss for batch 194 : 0.016668248921632767\n",
      "Training loss for batch 195 : 0.015980280935764313\n",
      "Training loss for batch 196 : 0.05423528701066971\n",
      "Training loss for batch 197 : 0.12865349650382996\n",
      "Training loss for batch 198 : 0.0809682235121727\n",
      "Training loss for batch 199 : 0.021733883768320084\n",
      "Training loss for batch 200 : 0.008819887414574623\n",
      "Training loss for batch 201 : 0.042029090225696564\n",
      "Training loss for batch 202 : 0.1011456772685051\n",
      "Training loss for batch 203 : 0.19094426929950714\n",
      "Training loss for batch 204 : 0.005165997892618179\n",
      "Training loss for batch 205 : 0.06247727945446968\n",
      "Training loss for batch 206 : 0.10488545149564743\n",
      "Training loss for batch 207 : 0.06157691776752472\n",
      "Training loss for batch 208 : 0.09669661521911621\n",
      "Training loss for batch 209 : 0.03616781905293465\n",
      "Training loss for batch 210 : 0.06551709026098251\n",
      "Training loss for batch 211 : 0.1021985113620758\n",
      "Training loss for batch 212 : 0.05540423095226288\n",
      "Training loss for batch 213 : 0.033192697912454605\n",
      "Training loss for batch 214 : 0.026554329320788383\n",
      "Training loss for batch 215 : 0.05186626687645912\n",
      "Training loss for batch 216 : 0.12757930159568787\n",
      "Training loss for batch 217 : 0.04289017617702484\n",
      "Training loss for batch 218 : 0.009803598746657372\n",
      "Training loss for batch 219 : 0.01139050256460905\n",
      "Training loss for batch 220 : 0.1134745329618454\n",
      "Training loss for batch 221 : 0.03327077254652977\n",
      "Training loss for batch 222 : 0.013260704465210438\n",
      "Training loss for batch 223 : 0.15211614966392517\n",
      "Training loss for batch 224 : 0.11256631463766098\n",
      "Training loss for batch 225 : 0.008681665174663067\n",
      "Training loss for batch 226 : 0.07703447341918945\n",
      "Training loss for batch 227 : 0.11487407982349396\n",
      "Training loss for batch 228 : 0.10640744119882584\n",
      "Training loss for batch 229 : 0.026025308296084404\n",
      "Training loss for batch 230 : 0.11227293312549591\n",
      "Training loss for batch 231 : 0.02093440853059292\n",
      "Training loss for batch 232 : 0.011488277465105057\n",
      "Training loss for batch 233 : 0.09345048666000366\n",
      "Training loss for batch 234 : 0.12096384167671204\n",
      "Training loss for batch 235 : 0.044715866446495056\n",
      "Training loss for batch 236 : 0.009386321529746056\n",
      "Training loss for batch 237 : 0.06226562708616257\n",
      "Training loss for batch 238 : 0.17842751741409302\n",
      "Training loss for batch 239 : 0.027191495522856712\n",
      "Training loss for batch 240 : 0.1020299643278122\n",
      "Training loss for batch 241 : 0.04586074501276016\n",
      "Training loss for batch 242 : 0.10318423062562943\n",
      "Training loss for batch 243 : 0.10980936884880066\n",
      "Training loss for batch 244 : 0.014918245375156403\n",
      "Training loss for batch 245 : 0.005669775418937206\n",
      "Training loss for batch 246 : 0.029880506917834282\n",
      "Training loss for batch 247 : 0.10555591434240341\n",
      "Training loss for batch 248 : 0.017362520098686218\n",
      "Training loss for batch 249 : 0.03213554993271828\n",
      "Training loss for batch 250 : 0.021568413823843002\n",
      "Training loss for batch 251 : 0.023912876844406128\n",
      "Training loss for batch 252 : 0.006662130355834961\n",
      "Training loss for batch 253 : 0.0567234605550766\n",
      "Training loss for batch 254 : 0.019214842468500137\n",
      "Training loss for batch 255 : 0.053865376859903336\n",
      "Training loss for batch 256 : 0.13115279376506805\n",
      "Training loss for batch 257 : 0.08211826533079147\n",
      "Training loss for batch 258 : 0.044731974601745605\n",
      "Training loss for batch 259 : 0.05204236879944801\n",
      "Training loss for batch 260 : 0.07080135494470596\n",
      "Training loss for batch 261 : 0.10751443356275558\n",
      "Training loss for batch 262 : 0.0030510122887790203\n",
      "Training loss for batch 263 : 0.043479301035404205\n",
      "Training loss for batch 264 : 0.15630437433719635\n",
      "Training loss for batch 265 : 0.06741249561309814\n",
      "Training loss for batch 266 : 0.08555466681718826\n",
      "Training loss for batch 267 : 0.0377599336206913\n",
      "Training loss for batch 268 : 0.04088851436972618\n",
      "Training loss for batch 269 : 0.042937684804201126\n",
      "Training loss for batch 270 : 0.031932324171066284\n",
      "Training loss for batch 271 : 0.022802796214818954\n",
      "Training loss for batch 272 : 0.11765408515930176\n",
      "Training loss for batch 273 : 0.0021494075190275908\n",
      "Training loss for batch 274 : 0.006689173635095358\n",
      "Training loss for batch 275 : 0.040223341435194016\n",
      "Training loss for batch 276 : 0.018656689673662186\n",
      "Training loss for batch 277 : 0.0973837673664093\n",
      "Training loss for batch 278 : 0.048918552696704865\n",
      "Training loss for batch 279 : 0.06847503781318665\n",
      "Training loss for batch 280 : 0.03175473213195801\n",
      "Training loss for batch 281 : 0.10526853799819946\n",
      "Training loss for batch 282 : 0.1425664722919464\n",
      "Training loss for batch 283 : 0.07006161659955978\n",
      "Training loss for batch 284 : 0.10664798319339752\n",
      "Training loss for batch 285 : 0.15133410692214966\n",
      "Training loss for batch 286 : 0.1491396725177765\n",
      "Training loss for batch 287 : 0.20811384916305542\n",
      "Training loss for batch 288 : 0.02820785529911518\n",
      "Training loss for batch 289 : 0.028218859806656837\n",
      "Training loss for batch 290 : 0.03328118845820427\n",
      "Training loss for batch 291 : 0.014528651721775532\n",
      "Training loss for batch 292 : 0.12036138772964478\n",
      "Training loss for batch 293 : 0.022464852780103683\n",
      "Training loss for batch 294 : 0.12653709948062897\n",
      "Training loss for batch 295 : 0.13224779069423676\n",
      "Training loss for batch 296 : 0.1429145634174347\n",
      "Training loss for batch 297 : 0.03910721838474274\n",
      "Training loss for batch 298 : 0.22493581473827362\n",
      "Training loss for batch 299 : 0.02566632442176342\n",
      "Training loss for batch 300 : 0.04382297024130821\n",
      "Training loss for batch 301 : 0.11464965343475342\n",
      "Training loss for batch 302 : 0.03595864772796631\n",
      "Training loss for batch 303 : 0.032262541353702545\n",
      "Training loss for batch 304 : 0.0649503767490387\n",
      "Training loss for batch 305 : 0.044051311910152435\n",
      "Training loss for batch 306 : 0.06502959132194519\n",
      "Training loss for batch 307 : 0.1936766356229782\n",
      "Training loss for batch 308 : 0.08175692707300186\n",
      "Training loss for batch 309 : 0.06632028520107269\n",
      "Training loss for batch 310 : 0.04545937478542328\n",
      "Training loss for batch 311 : 0.10856079310178757\n",
      "Training loss for batch 312 : 0.13277538120746613\n",
      "Training loss for batch 313 : 0.029837632551789284\n",
      "Training loss for batch 314 : 0.11223320662975311\n",
      "Training loss for batch 315 : 0.04265914857387543\n",
      "Training loss for batch 316 : 0.030421920120716095\n",
      "Training loss for batch 317 : 0.06894225627183914\n",
      "Training loss for batch 318 : 0.02671300806105137\n",
      "Training loss for batch 319 : 0.03882075101137161\n",
      "Training loss for batch 320 : 0.0670468881726265\n",
      "Training loss for batch 321 : 0.055917445570230484\n",
      "Training loss for batch 322 : 0.10533098876476288\n",
      "Training loss for batch 323 : 0.012523139826953411\n",
      "Training loss for batch 324 : 0.10191475600004196\n",
      "Training loss for batch 325 : 0.037004485726356506\n",
      "Training loss for batch 326 : 1.947880612362951e-08\n",
      "Training loss for batch 327 : 0.0017649978399276733\n",
      "Training loss for batch 328 : 0.06934691220521927\n",
      "Training loss for batch 329 : 0.10932435840368271\n",
      "Training loss for batch 330 : 0.017480261623859406\n",
      "Training loss for batch 331 : 0.0680762454867363\n",
      "Training loss for batch 332 : 0.12825307250022888\n",
      "Training loss for batch 333 : 0.04109857976436615\n",
      "Training loss for batch 334 : 0.06353232264518738\n",
      "Training loss for batch 335 : 0.14884160459041595\n",
      "Training loss for batch 336 : 0.06155115365982056\n",
      "Training loss for batch 337 : 0.018553048372268677\n",
      "Training loss for batch 338 : 0.07576018571853638\n",
      "Training loss for batch 339 : 0.08669093996286392\n",
      "Training loss for batch 340 : 0.05986274406313896\n",
      "Training loss for batch 341 : 0.046876177191734314\n",
      "Training loss for batch 342 : 0.010914711281657219\n",
      "Training loss for batch 343 : 0.04395430535078049\n",
      "Training loss for batch 344 : 0.030324434861540794\n",
      "Training loss for batch 345 : 0.04330853000283241\n",
      "Training loss for batch 346 : 0.07795556634664536\n",
      "Training loss for batch 347 : 0.003377945628017187\n",
      "Training loss for batch 348 : 0.002262680558487773\n",
      "Training loss for batch 349 : 0.053918369114398956\n",
      "Training loss for batch 350 : 0.06125043332576752\n",
      "Training loss for batch 351 : 0.08527221530675888\n",
      "Training loss for batch 352 : 0.13565930724143982\n",
      "Training loss for batch 353 : 0.0020624524913728237\n",
      "Training loss for batch 354 : 0.030527887865900993\n",
      "Training loss for batch 355 : 0.09670694172382355\n",
      "Training loss for batch 356 : 0.013173207640647888\n",
      "Training loss for batch 357 : 0.11912722885608673\n",
      "Training loss for batch 358 : 0.0233662910759449\n",
      "Training loss for batch 359 : 0.08680980652570724\n",
      "Training loss for batch 360 : 0.09660334140062332\n",
      "Training loss for batch 361 : 0.05575317144393921\n",
      "Training loss for batch 362 : 0.09571517258882523\n",
      "Training loss for batch 363 : 0.018477290868759155\n",
      "Training loss for batch 364 : 0.06454964727163315\n",
      "Training loss for batch 365 : 0.07303538918495178\n",
      "Training loss for batch 366 : 0.10765203833580017\n",
      "Training loss for batch 367 : 0.09475420415401459\n",
      "Training loss for batch 368 : 0.033441852778196335\n",
      "Training loss for batch 369 : 0.024632882326841354\n",
      "Training loss for batch 370 : 0.1181209459900856\n",
      "Training loss for batch 371 : 0.015814919024705887\n",
      "Training loss for batch 372 : 0.03670036792755127\n",
      "Training loss for batch 373 : 0.031733427196741104\n",
      "Training loss for batch 374 : 0.02884603664278984\n",
      "Training loss for batch 375 : 0.0355355478823185\n",
      "Training loss for batch 376 : 0.06527262181043625\n",
      "Training loss for batch 377 : 0.11927255243062973\n",
      "Training loss for batch 378 : 0.009254089556634426\n",
      "Training loss for batch 379 : 0.001528630731627345\n",
      "Training loss for batch 380 : 0.06287506222724915\n",
      "Training loss for batch 381 : 0.08254048228263855\n",
      "Training loss for batch 382 : 0.0485055185854435\n",
      "Training loss for batch 383 : 0.04039107635617256\n",
      "Training loss for batch 384 : 0.10462670028209686\n",
      "Training loss for batch 385 : 0.009392639622092247\n",
      "Training loss for batch 386 : 8.246851734838856e-08\n",
      "Training loss for batch 387 : 0.03436189889907837\n",
      "Training loss for batch 388 : 0.13279975950717926\n",
      "Training loss for batch 389 : 0.039150070399045944\n",
      "Training loss for batch 390 : 0.07917524129152298\n",
      "Training loss for batch 391 : 0.04190705716609955\n",
      "Training loss for batch 392 : 0.0164035651832819\n",
      "Training loss for batch 393 : 0.013568505644798279\n",
      "Training loss for batch 394 : 0.07669096440076828\n",
      "Training loss for batch 395 : 0.20419740676879883\n",
      "Training loss for batch 396 : 0.07682507485151291\n",
      "Training loss for batch 397 : 0.10424463450908661\n",
      "Training loss for batch 398 : 0.06761802732944489\n",
      "Training loss for batch 399 : 0.07356880605220795\n",
      "Training loss for batch 400 : 0.10346929728984833\n",
      "Training loss for batch 401 : 0.06717677414417267\n",
      "Training loss for batch 402 : 0.027960410341620445\n",
      "Training loss for batch 403 : 0.1527809500694275\n",
      "Training loss for batch 404 : 0.06386399269104004\n",
      "Training loss for batch 405 : 0.034996021538972855\n",
      "Training loss for batch 406 : 0.04008810222148895\n",
      "Training loss for batch 407 : 0.04281170293688774\n",
      "Training loss for batch 408 : 0.0631832405924797\n",
      "Training loss for batch 409 : 0.03073410876095295\n",
      "Training loss for batch 410 : 0.1013951450586319\n",
      "Training loss for batch 411 : 0.013504029251635075\n",
      "Training loss for batch 412 : 0.045263055711984634\n",
      "Training loss for batch 413 : 0.010462353937327862\n",
      "Training loss for batch 414 : 0.023928191512823105\n",
      "Training loss for batch 415 : 0.017481403425335884\n",
      "Training loss for batch 416 : 0.039816562086343765\n",
      "Training loss for batch 417 : 0.109928198158741\n",
      "Training loss for batch 418 : 0.040888089686632156\n",
      "Training loss for batch 419 : 0.02057555504143238\n",
      "Training loss for batch 420 : 0.01225209329277277\n",
      "Training loss for batch 421 : 0.04645441845059395\n",
      "Training loss for batch 422 : 0.03766099736094475\n",
      "Training loss for batch 423 : 0.1447877436876297\n",
      "Training loss for batch 424 : 0.012245956808328629\n",
      "Training loss for batch 425 : 0.12954223155975342\n",
      "Training loss for batch 426 : 0.08042918145656586\n",
      "Training loss for batch 427 : 0.04306577146053314\n",
      "Training loss for batch 428 : 0.07744648307561874\n",
      "Training loss for batch 429 : 0.08771318197250366\n",
      "Training loss for batch 430 : 0.06621961295604706\n",
      "Training loss for batch 431 : 0.012890471145510674\n",
      "Training loss for batch 432 : 0.04544443264603615\n",
      "Training loss for batch 433 : 0.0352882444858551\n",
      "Training loss for batch 434 : 0.007078796625137329\n",
      "Training loss for batch 435 : 0.02304028533399105\n",
      "Training loss for batch 436 : 0.061570387333631516\n",
      "Training loss for batch 437 : 0.016570627689361572\n",
      "Training loss for batch 438 : 0.0753168910741806\n",
      "Training loss for batch 439 : 0.018873656168580055\n",
      "Training loss for batch 440 : 0.021367646753787994\n",
      "Training loss for batch 441 : 0.076378233730793\n",
      "Training loss for batch 442 : 0.030718643218278885\n",
      "Training loss for batch 443 : 0.023502137511968613\n",
      "Training loss for batch 444 : 0.13155975937843323\n",
      "Training loss for batch 445 : 2.2556568524123577e-07\n",
      "Training loss for batch 446 : 0.04831439629197121\n",
      "Training loss for batch 447 : 0.029284106567502022\n",
      "Training loss for batch 448 : 0.008771697990596294\n",
      "Training loss for batch 449 : 0.06799284368753433\n",
      "Training loss for batch 450 : 0.18476872146129608\n",
      "Training loss for batch 451 : 0.0054620918817818165\n",
      "Training loss for batch 452 : 0.05368075147271156\n",
      "Training loss for batch 453 : 0.06721384078264236\n",
      "Training loss for batch 454 : 0.03345378860831261\n",
      "Training loss for batch 455 : 0.03948289528489113\n",
      "Training loss for batch 456 : 0.0018050827784463763\n",
      "Training loss for batch 457 : 0.036618560552597046\n",
      "Training loss for batch 458 : 0.04049350693821907\n",
      "Training loss for batch 459 : 0.04721571132540703\n",
      "Training loss for batch 460 : 0.0377100370824337\n",
      "Training loss for batch 461 : 0.058152493089437485\n",
      "Training loss for batch 462 : 0.017072053626179695\n",
      "Training loss for batch 463 : 0.019304241985082626\n",
      "Training loss for batch 464 : 0.1553601175546646\n",
      "Training loss for batch 465 : 0.02791631780564785\n",
      "Training loss for batch 466 : 0.04454180970788002\n",
      "Training loss for batch 467 : 0.05461518093943596\n",
      "Training loss for batch 468 : 0.026944685727357864\n",
      "Training loss for batch 469 : 0.010694430209696293\n",
      "Training loss for batch 470 : 0.1474713236093521\n",
      "Training loss for batch 471 : 0.06114264205098152\n",
      "Training loss for batch 472 : 0.08035390079021454\n",
      "Training loss for batch 473 : 0.16287016868591309\n",
      "Training loss for batch 474 : 0.08835482597351074\n",
      "Training loss for batch 475 : 0.027057737112045288\n",
      "Training loss for batch 476 : 0.04780236631631851\n",
      "Training loss for batch 477 : 0.09668265283107758\n",
      "Training loss for batch 478 : 0.0037947704549878836\n",
      "Training loss for batch 479 : 0.11097784340381622\n",
      "Training loss for batch 480 : 0.05844252184033394\n",
      "Training loss for batch 481 : 0.026783162727952003\n",
      "Training loss for batch 482 : 0.10474865883588791\n",
      "Training loss for batch 483 : 0.07864335179328918\n",
      "Training loss for batch 484 : 0.0699375569820404\n",
      "Training loss for batch 485 : 0.05567258596420288\n",
      "Training loss for batch 486 : 0.011258823797106743\n",
      "Training loss for batch 487 : 0.06090128794312477\n",
      "Training loss for batch 488 : 0.07937241345643997\n",
      "Training loss for batch 489 : 0.02601286955177784\n",
      "Training loss for batch 490 : 0.029690368101000786\n",
      "Training loss for batch 491 : 0.10053020715713501\n",
      "Training loss for batch 492 : 0.007548662833869457\n",
      "Training loss for batch 493 : 0.01724541373550892\n",
      "Training loss for batch 494 : 0.04929879307746887\n",
      "Training loss for batch 495 : 0.015317833051085472\n",
      "Training loss for batch 496 : 0.08922772109508514\n",
      "Training loss for batch 497 : 0.09698399156332016\n",
      "Training loss for batch 498 : 0.012282240204513073\n",
      "Training loss for batch 499 : 0.014873666688799858\n",
      "Training loss for batch 500 : 0.07080628722906113\n",
      "Training loss for batch 501 : 0.014669319614768028\n",
      "Training loss for batch 502 : 0.1363867074251175\n",
      "Training loss for batch 503 : 0.10245081037282944\n",
      "Training loss for batch 504 : 0.10695198178291321\n",
      "Training loss for batch 505 : 0.033694278448820114\n",
      "Training loss for batch 506 : 0.042221494019031525\n",
      "Training loss for batch 507 : 0.09739507734775543\n",
      "Training loss for batch 508 : 0.013434184715151787\n",
      "Training loss for batch 509 : 0.16422422230243683\n",
      "Training loss for batch 510 : 0.026174072176218033\n",
      "Training loss for batch 511 : 0.0707542672753334\n",
      "Training loss for batch 512 : 0.057134244590997696\n",
      "Training loss for batch 513 : 0.04836321994662285\n",
      "Training loss for batch 514 : 0.007698997855186462\n",
      "Training loss for batch 515 : 0.01944754272699356\n",
      "Training loss for batch 516 : 0.02764541283249855\n",
      "Training loss for batch 517 : 0.05775482952594757\n",
      "Training loss for batch 518 : 0.01960296928882599\n",
      "Training loss for batch 519 : 0.16572687029838562\n",
      "Training loss for batch 520 : 0.048025526106357574\n",
      "Training loss for batch 521 : 0.099357970058918\n",
      "Training loss for batch 522 : 0.0\n",
      "Training loss for batch 523 : 0.019644683226943016\n",
      "Training loss for batch 524 : 0.09110956639051437\n",
      "Training loss for batch 525 : 0.08600972592830658\n",
      "Training loss for batch 526 : 0.010773488320410252\n",
      "Training loss for batch 527 : 0.029673513025045395\n",
      "Training loss for batch 528 : 0.032294172793626785\n",
      "Training loss for batch 529 : 0.1322302371263504\n",
      "Training loss for batch 530 : 0.2540436387062073\n",
      "Training loss for batch 531 : 0.14690560102462769\n",
      "Training loss for batch 532 : 0.10369584709405899\n",
      "Training loss for batch 533 : 0.06625214964151382\n",
      "Training loss for batch 534 : 0.00021953132818453014\n",
      "Training loss for batch 535 : 0.07731790095567703\n",
      "Training loss for batch 536 : 0.04225260019302368\n",
      "Training loss for batch 537 : 0.008014786057174206\n",
      "Training loss for batch 538 : 0.16156427562236786\n",
      "Training loss for batch 539 : 0.016851095482707024\n",
      "Training loss for batch 540 : 0.022230133414268494\n",
      "Training loss for batch 541 : 0.09334491938352585\n",
      "Training loss for batch 542 : 0.02968255616724491\n",
      "Training loss for batch 543 : 0.07533073425292969\n",
      "Training loss for batch 544 : 0.11824701726436615\n",
      "Training loss for batch 545 : 0.09698169678449631\n",
      "Training loss for batch 546 : 0.06603898108005524\n",
      "Training loss for batch 547 : 0.13284151256084442\n",
      "Training loss for batch 548 : 0.03612981364130974\n",
      "Training loss for batch 549 : 0.03289094194769859\n",
      "Training loss for batch 550 : 0.03079793229699135\n",
      "Training loss for batch 551 : 0.04622563347220421\n",
      "Training loss for batch 552 : 0.05516483634710312\n",
      "Training loss for batch 553 : 0.04029686748981476\n",
      "Training loss for batch 554 : 0.23497061431407928\n",
      "Training loss for batch 555 : 0.08152103424072266\n",
      "Training loss for batch 556 : 0.05097748711705208\n",
      "Training loss for batch 557 : 0.06623870879411697\n",
      "Training loss for batch 558 : 0.031630828976631165\n",
      "Training loss for batch 559 : 0.043784335255622864\n",
      "Training loss for batch 560 : 0.03617239370942116\n",
      "Training loss for batch 561 : 0.17786173522472382\n",
      "Training loss for batch 562 : 0.027510646730661392\n",
      "Training loss for batch 563 : 0.00039332263986580074\n",
      "Training loss for batch 564 : 0.030055180191993713\n",
      "Training loss for batch 565 : 0.061687860637903214\n",
      "Training loss for batch 566 : 0.019976533949375153\n",
      "Training loss for batch 567 : 0.03382938355207443\n",
      "Training loss for batch 568 : 0.03491443023085594\n",
      "Training loss for batch 569 : 0.027588333934545517\n",
      "Training loss for batch 570 : 0.16681696474552155\n",
      "Training loss for batch 571 : 0.06284325569868088\n",
      "Training loss for batch 572 : 0.030392875894904137\n",
      "Training loss for batch 573 : 0.012791348621249199\n",
      "Training loss for batch 574 : 0.10769251734018326\n",
      "Training loss for batch 575 : 0.04030897095799446\n",
      "Training loss for batch 576 : 0.030925573781132698\n",
      "Training loss for batch 577 : 0.009171436540782452\n",
      "Training loss for batch 578 : 0.015921184793114662\n",
      "Training loss for batch 579 : 0.018085354939103127\n",
      "Training loss for batch 580 : 0.020857006311416626\n",
      "Training loss for batch 581 : 0.1086898073554039\n",
      "Training loss for batch 582 : 0.15295250713825226\n",
      "Training loss for batch 583 : 0.044371891766786575\n",
      "Training loss for batch 584 : 0.020945169031620026\n",
      "Training loss for batch 585 : 0.03569673374295235\n",
      "Training loss for batch 586 : 0.004973385017365217\n",
      "Training loss for batch 587 : 0.0808248445391655\n",
      "Training loss for batch 588 : 0.047060322016477585\n",
      "Training loss for batch 589 : 0.011083022691309452\n",
      "Training loss for batch 590 : 0.02519986219704151\n",
      "Training loss for batch 591 : 0.05185540020465851\n",
      "Training loss for batch 592 : 0.03752967342734337\n",
      "Training loss for batch 593 : 0.051895543932914734\n",
      "Training loss for batch 594 : 0.025471296161413193\n",
      "Training loss for batch 595 : 0.030094251036643982\n",
      "Training loss for batch 596 : 0.018599091097712517\n",
      "Training loss for batch 597 : 0.049378540366888046\n",
      "Training loss for batch 598 : 0.1380125880241394\n",
      "Training loss for batch 599 : 0.05764784663915634\n",
      "Training loss for batch 600 : 0.06251257658004761\n",
      "Training loss for batch 601 : 0.05306176096200943\n",
      "Training loss for batch 602 : 0.06949122250080109\n",
      "Training loss for batch 603 : 0.11992958933115005\n",
      "Training loss for batch 604 : 0.10839629173278809\n",
      "Training loss for batch 605 : 0.022846955806016922\n",
      "Training loss for batch 606 : 0.03196258097887039\n",
      "Training loss for batch 607 : 0.10259474068880081\n",
      "Training loss for batch 608 : 0.04044651612639427\n",
      "Training loss for batch 609 : 0.06680037826299667\n",
      "Training loss for batch 610 : 0.06934794038534164\n",
      "Training loss for batch 611 : 0.02336837910115719\n",
      "Training loss for batch 612 : 0.03763319179415703\n",
      "Training loss for batch 613 : 0.01967892423272133\n",
      "Training loss for batch 614 : 0.004240313079208136\n",
      "Training loss for batch 615 : 0.22010332345962524\n",
      "Training loss for batch 616 : 0.04800766333937645\n",
      "Training loss for batch 617 : 0.06503964960575104\n",
      "Training loss for batch 618 : 0.03276786580681801\n",
      "Training loss for batch 619 : 0.08308127522468567\n",
      "Training loss for batch 620 : 0.01041992474347353\n",
      "Training loss for batch 621 : 0.0068641300313174725\n",
      "Training loss for batch 622 : 0.04867186024785042\n",
      "Training loss for batch 623 : 0.128843754529953\n",
      "Training loss for batch 624 : 0.022919321432709694\n",
      "Training loss for batch 625 : 0.10153383016586304\n",
      "Training loss for batch 626 : 0.09909113496541977\n",
      "Training loss for batch 627 : 0.0385352298617363\n",
      "Training loss for batch 628 : 0.06872127205133438\n",
      "Training loss for batch 629 : 0.023052513599395752\n",
      "Training loss for batch 630 : 0.01803901232779026\n",
      "Training loss for batch 631 : 0.06980600208044052\n",
      "Training loss for batch 632 : 0.13784275949001312\n",
      "Training loss for batch 633 : 0.09034650027751923\n",
      "Training loss for batch 634 : 0.1356724202632904\n",
      "Training loss for batch 635 : 0.022724851965904236\n",
      "Training loss for batch 636 : 0.05912763252854347\n",
      "Training loss for batch 637 : 0.06893058121204376\n",
      "Training loss for batch 638 : 0.12358187884092331\n",
      "Training loss for batch 639 : 0.035304535180330276\n",
      "Training loss for batch 640 : 0.0562652125954628\n",
      "Training loss for batch 641 : 0.07235964387655258\n",
      "Training loss for batch 642 : 0.030978625640273094\n",
      "Training loss for batch 643 : 0.015224861912429333\n",
      "Training loss for batch 644 : 0.018381983041763306\n",
      "Training loss for batch 645 : 0.1307390332221985\n",
      "Training loss for batch 646 : 0.018212946131825447\n",
      "Training loss for batch 647 : 0.026552870869636536\n",
      "Training loss for batch 648 : 0.08919479697942734\n",
      "Training loss for batch 649 : 0.054822176694869995\n",
      "Training loss for batch 650 : 0.062718965113163\n",
      "Training loss for batch 651 : 0.09325451403856277\n",
      "Training loss for batch 652 : 0.09589652717113495\n",
      "Training loss for batch 653 : 0.028506016358733177\n",
      "Training loss for batch 654 : 0.07279796153306961\n",
      "Training loss for batch 655 : 0.016839519143104553\n",
      "Training loss for batch 656 : 0.0207348745316267\n",
      "Training loss for batch 657 : 0.04140860214829445\n",
      "Training loss for batch 658 : 0.037123702466487885\n",
      "Training loss for batch 659 : 0.1964799016714096\n",
      "Training loss for batch 660 : 0.03500153496861458\n",
      "Training loss for batch 661 : 0.04659870266914368\n",
      "Training loss for batch 662 : 0.051690272986888885\n",
      "Training loss for batch 663 : 0.04455064237117767\n",
      "Training loss for batch 664 : 0.02499549463391304\n",
      "Training loss for batch 665 : 0.08183924108743668\n",
      "Training loss for batch 666 : 0.10985299944877625\n",
      "Training loss for batch 667 : 0.17406876385211945\n",
      "Training loss for batch 668 : 0.04014727845788002\n",
      "Training loss for batch 669 : 0.08980424702167511\n",
      "Training loss for batch 670 : 0.03342590853571892\n",
      "Training loss for batch 671 : 0.04814676195383072\n",
      "Training loss for batch 672 : 0.018289972096681595\n",
      "Training loss for batch 673 : 0.03797455504536629\n",
      "Training loss for batch 674 : 0.049134060740470886\n",
      "Training loss for batch 675 : 0.01306910440325737\n",
      "Training loss for batch 676 : 0.1539774090051651\n",
      "Training loss for batch 677 : 0.02577866241335869\n",
      "Training loss for batch 678 : 0.026477396488189697\n",
      "Training loss for batch 679 : 0.0015346998116001487\n",
      "Training loss for batch 680 : 0.09804217517375946\n",
      "Training loss for batch 681 : 0.029166601598262787\n",
      "Training loss for batch 682 : 0.02412298135459423\n",
      "Training loss for batch 683 : 0.07948511093854904\n",
      "Training loss for batch 684 : 0.030267566442489624\n",
      "Training loss for batch 685 : 0.039341039955616\n",
      "Training loss for batch 686 : 0.012995357625186443\n",
      "Training loss for batch 687 : 0.053249359130859375\n",
      "Training loss for batch 688 : 0.05031157657504082\n",
      "Training loss for batch 689 : 0.058643631637096405\n",
      "Training loss for batch 690 : 0.10384254902601242\n",
      "Training loss for batch 691 : 0.04153812676668167\n",
      "Training loss for batch 692 : 0.02008732594549656\n",
      "Training loss for batch 693 : 0.07071683555841446\n",
      "Training loss for batch 694 : 0.03666165843605995\n",
      "Training loss for batch 695 : 0.0809093713760376\n",
      "Training loss for batch 696 : 0.05294552445411682\n",
      "Training loss for batch 697 : 0.00041346100624650717\n",
      "Training loss for batch 698 : 0.012791819870471954\n",
      "Training loss for batch 699 : 0.04973107948899269\n",
      "Training loss for batch 700 : 0.026371611282229424\n",
      "Training loss for batch 701 : 0.06319084763526917\n",
      "Training loss for batch 702 : 0.06664776802062988\n",
      "Training loss for batch 703 : 0.06377594918012619\n",
      "Training loss for batch 704 : 0.07122969627380371\n",
      "Training loss for batch 705 : 0.0437236912548542\n",
      "Training loss for batch 706 : 0.054430969059467316\n",
      "Training loss for batch 707 : 0.05261771008372307\n",
      "Training loss for batch 708 : 0.023177148774266243\n",
      "Training loss for batch 709 : 0.0027619600296020508\n",
      "Training loss for batch 710 : 0.0280663650482893\n",
      "Training loss for batch 711 : 0.04733414947986603\n",
      "Training loss for batch 712 : 0.020358087494969368\n",
      "Training loss for batch 713 : 0.08945342898368835\n",
      "Training loss for batch 714 : 0.08057316392660141\n",
      "Training loss for batch 715 : 0.19259439408779144\n",
      "Training loss for batch 716 : 0.16971810162067413\n",
      "Training loss for batch 717 : 0.0691780149936676\n",
      "Training loss for batch 718 : 0.08920945972204208\n",
      "Training loss for batch 719 : 0.04971930757164955\n",
      "Training loss for batch 720 : 0.000297566904919222\n",
      "Training loss for batch 721 : 0.0343775637447834\n",
      "Training loss for batch 722 : 0.08826979994773865\n",
      "Training loss for batch 723 : 0.0267152301967144\n",
      "Training loss for batch 724 : 0.06732716411352158\n",
      "Training loss for batch 725 : 0.09454061836004257\n",
      "Training loss for batch 726 : 0.16640475392341614\n",
      "Training loss for batch 727 : 0.09830290824174881\n",
      "Training loss for batch 728 : 0.28965437412261963\n",
      "Training loss for batch 729 : 0.026869315654039383\n",
      "Training loss for batch 730 : 0.09360918402671814\n",
      "Training loss for batch 731 : 0.03021104820072651\n",
      "Training loss for batch 732 : 7.988743533360321e-08\n",
      "Training loss for batch 733 : 0.08933532983064651\n",
      "Training loss for batch 734 : 0.03904284909367561\n",
      "Training loss for batch 735 : 0.16524401307106018\n",
      "Training loss for batch 736 : 0.17242445051670074\n",
      "Training loss for batch 737 : 0.050403330475091934\n",
      "Training loss for batch 738 : 0.0610685832798481\n",
      "Training loss for batch 739 : 0.020199185237288475\n",
      "Training loss for batch 740 : 0.03691942244768143\n",
      "Training loss for batch 741 : 0.03230411931872368\n",
      "Training loss for batch 742 : 0.00865041371434927\n",
      "Training loss for batch 743 : 0.11341371387243271\n",
      "Training loss for batch 744 : 0.06590520590543747\n",
      "Training loss for batch 745 : 0.030392087996006012\n",
      "Training loss for batch 746 : 0.061162978410720825\n",
      "Training loss for batch 747 : 0.06444007903337479\n",
      "Training loss for batch 748 : 0.02166246622800827\n",
      "Training loss for batch 749 : 0.026073379442095757\n",
      "Training loss for batch 750 : 0.03076356276869774\n",
      "Training loss for batch 751 : 0.08284958451986313\n",
      "Training loss for batch 752 : 0.07801760733127594\n",
      "Training loss for batch 753 : 0.03135760501027107\n",
      "Training loss for batch 754 : 0.09000346809625626\n",
      "Training loss for batch 755 : 0.08573690056800842\n",
      "Training loss for batch 756 : 0.08276266604661942\n",
      "Training loss for batch 757 : 0.04803001135587692\n",
      "Training loss for batch 758 : 0.053677599877119064\n",
      "Training loss for batch 759 : 0.13862964510917664\n",
      "Training loss for batch 760 : 0.0481293760240078\n",
      "Training loss for batch 761 : 0.03456690534949303\n",
      "Training loss for batch 762 : 0.10094619542360306\n",
      "Training loss for batch 763 : 0.09685593843460083\n",
      "Training loss for batch 764 : 0.056178364902734756\n",
      "Training loss for batch 765 : 0.09838127344846725\n",
      "Training loss for batch 766 : 0.0829911082983017\n",
      "Training loss for batch 767 : 0.10188982635736465\n",
      "Training loss for batch 768 : 0.10522548109292984\n",
      "Training loss for batch 769 : 0.06976250559091568\n",
      "Training loss for batch 770 : 0.025880854576826096\n",
      "Training loss for batch 771 : 0.05166810750961304\n",
      "Training loss for batch 772 : 0.06075591221451759\n",
      "Training loss for batch 773 : 0.08291612565517426\n",
      "Training loss for batch 774 : 0.062841035425663\n",
      "Training loss for batch 775 : 0.1073533371090889\n",
      "Training loss for batch 776 : 0.04910909757018089\n",
      "Training loss for batch 777 : 0.03252327814698219\n",
      "Training loss for batch 778 : 0.058960773050785065\n",
      "Training loss for batch 779 : 0.0045413062907755375\n",
      "Training loss for batch 780 : 0.04642169177532196\n",
      "Training loss for batch 781 : 0.05149529501795769\n",
      "Training loss for batch 782 : 0.06642650067806244\n",
      "Training loss for batch 783 : 0.04996941611170769\n",
      "Training loss for batch 784 : 0.14735543727874756\n",
      "Training loss for batch 785 : 0.016320738941431046\n",
      "Training loss for batch 786 : 0.051472462713718414\n",
      "Training loss for batch 787 : 0.07740176469087601\n",
      "Training loss for batch 788 : 0.132191464304924\n",
      "Training loss for batch 789 : 0.10764168947935104\n",
      "Training loss for batch 790 : 0.14996786415576935\n",
      "Training loss for batch 791 : 0.026470676064491272\n",
      "Training loss for batch 792 : 0.20060406625270844\n",
      "Training loss for batch 793 : 1.432353116115337e-07\n",
      "Training loss for batch 794 : 0.055264800786972046\n",
      "Training loss for batch 795 : 0.002076675184071064\n",
      "Training loss for batch 796 : 0.007250514812767506\n",
      "Training loss for batch 797 : 0.05909593775868416\n",
      "Training loss for batch 798 : 0.1809769868850708\n",
      "Training loss for batch 799 : 0.024732977151870728\n",
      "Training loss for batch 800 : 0.11165908724069595\n",
      "Training loss for batch 801 : 0.02145940251648426\n",
      "Training loss for batch 802 : 0.08293180912733078\n",
      "Training loss for batch 803 : 0.05707014724612236\n",
      "Training loss for batch 804 : 0.01617295853793621\n",
      "Training loss for batch 805 : 0.0752999410033226\n",
      "Training loss for batch 806 : 0.08366795629262924\n",
      "Training loss for batch 807 : 0.0908738225698471\n",
      "Training loss for batch 808 : 0.14076466858386993\n",
      "Training loss for batch 809 : 0.14303673803806305\n",
      "Training loss for batch 810 : 0.14891313016414642\n",
      "Training loss for batch 811 : 0.28533944487571716\n",
      "Training loss for batch 812 : 0.1314079910516739\n",
      "Training loss for batch 813 : 0.05089571699500084\n",
      "Training loss for batch 814 : 0.07262264937162399\n",
      "Training loss for batch 815 : 0.03804689645767212\n",
      "Training loss for batch 816 : 0.07076437026262283\n",
      "Training loss for batch 817 : 0.06602869927883148\n",
      "Training loss for batch 818 : 0.08995655924081802\n",
      "Training loss for batch 819 : 0.10191106051206589\n",
      "Training loss for batch 820 : 0.1471894085407257\n",
      "Training loss for batch 821 : 0.05962101370096207\n",
      "Training loss for batch 822 : 0.09406045079231262\n",
      "Training loss for batch 823 : 0.048303596675395966\n",
      "Training loss for batch 824 : 0.027143577113747597\n",
      "Training loss for batch 825 : 0.028242729604244232\n",
      "Training loss for batch 826 : 0.05854114145040512\n",
      "Training loss for batch 827 : 0.04922329634428024\n",
      "Training loss for batch 828 : 0.019696038216352463\n",
      "Training loss for batch 829 : 0.031685132533311844\n",
      "Training loss for batch 830 : 0.1088029146194458\n",
      "Training loss for batch 831 : 0.08463972061872482\n",
      "Training loss for batch 832 : 0.00706868851557374\n",
      "Training loss for batch 833 : 0.007169564254581928\n",
      "Training loss for batch 834 : 0.018112841993570328\n",
      "Training loss for batch 835 : 0.16487406194210052\n",
      "Training loss for batch 836 : 0.09288597851991653\n",
      "Training loss for batch 837 : 0.10204539448022842\n",
      "Training loss for batch 838 : 0.010939419269561768\n",
      "Training loss for batch 839 : 0.051348552107810974\n",
      "Training loss for batch 840 : 0.1043369323015213\n",
      "Training loss for batch 841 : 0.09993183612823486\n",
      "Training loss for batch 842 : 0.1113070473074913\n",
      "Training loss for batch 843 : 0.05222207307815552\n",
      "Training loss for batch 844 : 0.060818128287792206\n",
      "Training loss for batch 845 : 0.10921598225831985\n",
      "Training loss for batch 846 : 0.1123528853058815\n",
      "Training loss for batch 847 : 0.07098811119794846\n",
      "Training loss for batch 848 : 0.02271639183163643\n",
      "Training loss for batch 849 : 0.057686857879161835\n",
      "Training loss for batch 850 : 0.09384848922491074\n",
      "Training loss for batch 851 : 0.12153631448745728\n",
      "Training loss for batch 852 : 0.010343877598643303\n",
      "Training loss for batch 853 : 0.07264812290668488\n",
      "Training loss for batch 854 : 0.09530583769083023\n",
      "Training loss for batch 855 : 0.11969637125730515\n",
      "Training loss for batch 856 : 0.03210686892271042\n",
      "Training loss for batch 857 : 0.04677721485495567\n",
      "Training loss for batch 858 : 0.04202580451965332\n",
      "Training loss for batch 859 : 0.019131634384393692\n",
      "Training loss for batch 860 : 1.998685306148218e-08\n",
      "Training loss for batch 861 : 0.059352610260248184\n",
      "Training loss for batch 862 : 0.08139582723379135\n",
      "Training loss for batch 863 : 0.13244089484214783\n",
      "Training loss for batch 864 : 0.17832355201244354\n",
      "Training loss for batch 865 : 0.05733102560043335\n",
      "Training loss for batch 866 : 0.03177830949425697\n",
      "Training loss for batch 867 : 0.10241228342056274\n",
      "Training loss for batch 868 : 0.1709246039390564\n",
      "Training loss for batch 869 : 0.07140302658081055\n",
      "Training loss for batch 870 : 0.011699497699737549\n",
      "Training loss for batch 871 : 0.023623816668987274\n",
      "Training loss for batch 872 : 0.026701543480157852\n",
      "Training loss for batch 873 : 0.037746530026197433\n",
      "Training loss for batch 874 : 0.1269078403711319\n",
      "Training loss for batch 875 : 0.08390698581933975\n",
      "Training loss for batch 876 : 0.0629628524184227\n",
      "Training loss for batch 877 : 0.09408572316169739\n",
      "Training loss for batch 878 : 0.10394994169473648\n",
      "Training loss for batch 879 : 0.020303113386034966\n",
      "Training loss for batch 880 : 0.03096936270594597\n",
      "Training loss for batch 881 : 0.03039788082242012\n",
      "Training loss for batch 882 : 0.09494443237781525\n",
      "Training loss for batch 883 : 0.027162892743945122\n",
      "Training loss for batch 884 : 0.05817677453160286\n",
      "Training loss for batch 885 : 0.04706032574176788\n",
      "Training loss for batch 886 : 0.019443606957793236\n",
      "Training loss for batch 887 : 0.009394935332238674\n",
      "Training loss for batch 888 : 0.007682875730097294\n",
      "Training loss for batch 889 : 0.004690935369580984\n",
      "Training loss for batch 890 : 0.07962797582149506\n",
      "Training loss for batch 891 : 0.15341785550117493\n",
      "Training loss for batch 892 : 0.029387110844254494\n",
      "Training loss for batch 893 : 0.07191288471221924\n",
      "Training loss for batch 894 : 0.07385207712650299\n",
      "Training loss for batch 895 : 0.03872889652848244\n",
      "Training loss for batch 896 : 0.19905297458171844\n",
      "Training loss for batch 897 : 0.011563429608941078\n",
      "Training loss for batch 898 : 0.025415411219000816\n",
      "Training loss for batch 899 : 0.020362263545393944\n",
      "Training loss for batch 900 : 0.0258049126714468\n",
      "Training loss for batch 901 : 0.06639119982719421\n",
      "Training loss for batch 902 : 0.01623234711587429\n",
      "Training loss for batch 903 : 0.04430149868130684\n",
      "Training loss for batch 904 : 0.008440240286290646\n",
      "Training loss for batch 905 : 0.058458857238292694\n",
      "Training loss for batch 906 : 0.0720253586769104\n",
      "Training loss for batch 907 : 0.04642728343605995\n",
      "Training loss for batch 908 : 0.06676150113344193\n",
      "Training loss for batch 909 : 0.07882841676473618\n",
      "Training loss for batch 910 : 0.09991875290870667\n",
      "Training loss for batch 911 : 0.05824417248368263\n",
      "Training loss for batch 912 : 0.07984254509210587\n",
      "Training loss for batch 913 : 0.0038652115035802126\n",
      "Training loss for batch 914 : 0.017080022022128105\n",
      "Training loss for batch 915 : 0.0075772786512970924\n",
      "Training loss for batch 916 : 0.058357249945402145\n",
      "Training loss for batch 917 : 0.042670272290706635\n",
      "Training loss for batch 918 : 0.16780734062194824\n",
      "Training loss for batch 919 : 0.07233170419931412\n",
      "Training loss for batch 920 : 0.0481528677046299\n",
      "Training loss for batch 921 : 0.019899625331163406\n",
      "Training loss for batch 922 : 0.034754831343889236\n",
      "Training loss for batch 923 : 0.015300452709197998\n",
      "Training loss for batch 924 : 0.16198565065860748\n",
      "Training loss for batch 925 : 0.09988797456026077\n",
      "Training loss for batch 926 : 0.12680286169052124\n",
      "Training loss for batch 927 : 0.0898979976773262\n",
      "Training loss for batch 928 : 0.16600313782691956\n",
      "Training loss for batch 929 : 0.08609285205602646\n",
      "Training loss for batch 930 : 0.0017592360964044929\n",
      "Training loss for batch 931 : 1.1101493058163214e-08\n",
      "Training loss for batch 932 : 0.06321985274553299\n",
      "Training loss for batch 933 : 0.08978120982646942\n",
      "Training loss for batch 934 : 0.07492605596780777\n",
      "Training loss for batch 935 : 0.024258291348814964\n",
      "Training loss for batch 936 : 0.012413368560373783\n",
      "Training loss for batch 937 : 0.08799812942743301\n",
      "Training loss for batch 938 : 0.07124212384223938\n",
      "Training loss for batch 939 : 0.07233019173145294\n",
      "Training loss for batch 940 : 0.009345299564301968\n",
      "Training loss for batch 941 : 5.653374657299537e-08\n",
      "Training loss for batch 942 : 0.009095418266952038\n",
      "Training loss for batch 943 : 0.1064036414027214\n",
      "Training loss for batch 944 : 0.036508165299892426\n",
      "Training loss for batch 945 : 0.0623997300863266\n",
      "Training loss for batch 946 : 0.02826588787138462\n",
      "Training loss for batch 947 : 0.04814353957772255\n",
      "Training loss for batch 948 : 0.12292560189962387\n",
      "Training loss for batch 949 : 0.12409958243370056\n",
      "Training loss for batch 950 : 0.14922192692756653\n",
      "Training loss for batch 951 : 0.10245351493358612\n",
      "Training loss for batch 952 : 0.05276424065232277\n",
      "Training loss for batch 953 : 0.07029805332422256\n",
      "Training loss for batch 954 : 0.015217287465929985\n",
      "Training loss for batch 955 : 0.0015794887440279126\n",
      "Training loss for batch 956 : 0.1487717628479004\n",
      "Training loss for batch 957 : 0.1992265284061432\n",
      "Training loss for batch 958 : 0.01267787255346775\n",
      "Training loss for batch 959 : 0.17277824878692627\n",
      "Training loss for batch 960 : 0.14609278738498688\n",
      "Training loss for batch 961 : 0.10371070355176926\n",
      "Training loss for batch 962 : 0.02129153534770012\n",
      "Training loss for batch 963 : 0.09218979626893997\n",
      "Training loss for batch 964 : 0.07942378520965576\n",
      "Training loss for batch 965 : 0.011214805766940117\n",
      "Training loss for batch 966 : 0.039722003042697906\n",
      "Training loss for batch 967 : 0.1477479338645935\n",
      "Training loss for batch 968 : 0.0023809028789401054\n",
      "Training loss for batch 969 : 0.09429142624139786\n",
      "Training loss for batch 970 : 0.06001142039895058\n",
      "Training loss for batch 971 : 0.08857028931379318\n",
      "Training loss for batch 972 : 0.05345121771097183\n",
      "Training loss for batch 973 : 0.03586491569876671\n",
      "Training loss for batch 974 : 0.0530841089785099\n",
      "Training loss for batch 975 : 0.030737953260540962\n",
      "Training loss for batch 976 : 0.08165217190980911\n",
      "Training loss for batch 977 : 0.03846132382750511\n",
      "Training loss for batch 978 : 0.04726662486791611\n",
      "Training loss for batch 979 : 0.03335657715797424\n",
      "Training loss for batch 980 : 0.00043955357978120446\n",
      "Training loss for batch 981 : 0.020648039877414703\n",
      "Training loss for batch 982 : 0.10985485464334488\n",
      "Training loss for batch 983 : 0.11831563711166382\n",
      "Training loss for batch 984 : 0.13745975494384766\n",
      "Training loss for batch 985 : 0.015561594627797604\n",
      "Training loss for batch 986 : 0.0928989127278328\n",
      "Training loss for batch 987 : 0.14866001904010773\n",
      "Training loss for batch 988 : 0.011773554608225822\n",
      "Training loss for batch 989 : 0.12129374593496323\n",
      "Training loss for batch 990 : 0.03547201305627823\n",
      "Training loss for batch 991 : 0.06992869824171066\n",
      "Training loss for batch 992 : 0.028786031529307365\n",
      "Training loss for batch 993 : 0.042489778250455856\n",
      "Training loss for batch 994 : 0.09486938267946243\n",
      "Training loss for batch 995 : 0.07866685092449188\n",
      "Training loss for batch 996 : 0.06146271899342537\n",
      "Training loss for batch 997 : 0.006188150029629469\n",
      "Training loss for batch 998 : 0.013356111943721771\n",
      "Training loss for batch 999 : 0.028816092759370804\n",
      "Training loss for batch 1000 : 0.18319180607795715\n",
      "Training loss for batch 1001 : 0.016630874946713448\n",
      "Training loss for batch 1002 : 0.06862889230251312\n",
      "Training loss for batch 1003 : 0.02088239975273609\n",
      "Training loss for batch 1004 : 0.05897602438926697\n",
      "Training loss for batch 1005 : 0.07781503349542618\n",
      "Training loss for batch 1006 : 0.0547601617872715\n",
      "Training loss for batch 1007 : 0.09721414744853973\n",
      "Training loss for batch 1008 : 0.0017731876578181982\n",
      "Training loss for batch 1009 : 0.030499916523694992\n",
      "Training loss for batch 1010 : 0.012445639818906784\n",
      "Training loss for batch 1011 : 0.12538659572601318\n",
      "Training loss for batch 1012 : 0.017603488638997078\n",
      "Training loss for batch 1013 : 0.010888472199440002\n",
      "Training loss for batch 1014 : 0.03219600021839142\n",
      "Training loss for batch 1015 : 0.2090648114681244\n",
      "Training loss for batch 1016 : 0.038094520568847656\n",
      "Training loss for batch 1017 : 0.07399704307317734\n",
      "Training loss for batch 1018 : 0.0850333422422409\n",
      "Training loss for batch 1019 : 0.04349154978990555\n",
      "Training loss for batch 1020 : 0.0989825651049614\n",
      "Training loss for batch 1021 : 0.09046987444162369\n",
      "Training loss for batch 1022 : 0.06675927340984344\n",
      "Training loss for batch 1023 : 0.018680501729249954\n",
      "Training loss for batch 1024 : 0.08782140165567398\n",
      "Training loss for batch 1025 : 0.18880271911621094\n",
      "Training loss for batch 1026 : 0.08642551302909851\n",
      "Training loss for batch 1027 : 0.0396793857216835\n",
      "Training loss for batch 1028 : 0.22451257705688477\n",
      "Training loss for batch 1029 : 0.07786627113819122\n",
      "Training loss for batch 1030 : 0.07842520624399185\n",
      "Training loss for batch 1031 : 0.17512936890125275\n",
      "Training loss for batch 1032 : 0.03567999601364136\n",
      "Training loss for batch 1033 : 0.023385893553495407\n",
      "Training loss for batch 1034 : 0.04195353761315346\n",
      "Training loss for batch 1035 : 0.07185015082359314\n",
      "Training loss for batch 1036 : 8.402762041725964e-09\n",
      "Training loss for batch 1037 : 0.057866327464580536\n",
      "Training loss for batch 1038 : 0.1563732773065567\n",
      "Training loss for batch 1039 : 0.18349620699882507\n",
      "Training loss for batch 1040 : 0.03521343320608139\n",
      "Training loss for batch 1041 : 0.06568846106529236\n",
      "Training loss for batch 1042 : 0.04128951579332352\n",
      "Training loss for batch 1043 : 0.010708215646445751\n",
      "Training loss for batch 1044 : 0.045769091695547104\n",
      "Training loss for batch 1045 : 0.05589713156223297\n",
      "Training loss for batch 1046 : 0.1187577098608017\n",
      "Training loss for batch 1047 : 0.14787553250789642\n",
      "Training loss for batch 1048 : 0.07318316400051117\n",
      "Training loss for batch 1049 : 0.12761400640010834\n",
      "Training loss for batch 1050 : 0.09176596254110336\n",
      "Training loss for batch 1051 : 0.06118867173790932\n",
      "Training loss for batch 1052 : 0.10551099479198456\n",
      "Training loss for batch 1053 : 0.05900632217526436\n",
      "Training loss for batch 1054 : 0.030971286818385124\n",
      "Training loss for batch 1055 : 0.12409883737564087\n",
      "Training loss for batch 1056 : 0.02994459681212902\n",
      "Training loss for batch 1057 : 0.13600857555866241\n",
      "Training loss for batch 1058 : 0.26442745327949524\n",
      "Training loss for batch 1059 : 0.026199329644441605\n",
      "Training loss for batch 1060 : 0.056102778762578964\n",
      "Training loss for batch 1061 : 0.05544033646583557\n",
      "Training loss for batch 1062 : 0.003939829301089048\n",
      "Training loss for batch 1063 : 0.001834458438679576\n",
      "Training loss for batch 1064 : 0.016435030847787857\n",
      "Training loss for batch 1065 : 0.044488806277513504\n",
      "Training loss for batch 1066 : 0.1023213341832161\n",
      "Training loss for batch 1067 : 0.08246339857578278\n",
      "Training loss for batch 1068 : 0.07182413339614868\n",
      "Training loss for batch 1069 : 0.07059626281261444\n",
      "Training loss for batch 1070 : 0.03270430117845535\n",
      "Training loss for batch 1071 : 0.03784992918372154\n",
      "Training loss for batch 1072 : 0.00849544070661068\n",
      "Training loss for batch 1073 : 0.08086075633764267\n",
      "Training loss for batch 1074 : 0.050272464752197266\n",
      "Training loss for batch 1075 : 0.09597048163414001\n",
      "Training loss for batch 1076 : 0.022259289398789406\n",
      "Training loss for batch 1077 : 0.1482425034046173\n",
      "Training loss for batch 1078 : 0.04073693975806236\n",
      "Training loss for batch 1079 : 0.07427289336919785\n",
      "Training loss for batch 1080 : 0.12177037447690964\n",
      "Training loss for batch 1081 : 0.09451453387737274\n",
      "Training loss for batch 1082 : 0.03928832709789276\n",
      "Training loss for batch 1083 : 0.17184005677700043\n",
      "Training loss for batch 1084 : 0.0025395257398486137\n",
      "Training loss for batch 1085 : 0.03687814250588417\n",
      "Training loss for batch 1086 : 0.09564680606126785\n",
      "Training loss for batch 1087 : 0.027418524026870728\n",
      "Training loss for batch 1088 : 0.07884494960308075\n",
      "Training loss for batch 1089 : 0.08207304775714874\n",
      "Training loss for batch 1090 : 0.008596125990152359\n",
      "Training loss for batch 1091 : 0.02735080197453499\n",
      "Training loss for batch 1092 : 0.09753701835870743\n",
      "Training loss for batch 1093 : 0.0344550684094429\n",
      "Training loss for batch 1094 : 0.0031182081438601017\n",
      "Training loss for batch 1095 : 0.01538306288421154\n",
      "Training loss for batch 1096 : 0.0005022386903874576\n",
      "Training loss for batch 1097 : 0.0655369982123375\n",
      "Training loss for batch 1098 : 0.0010795994894579053\n",
      "Training loss for batch 1099 : 0.006771584507077932\n",
      "Training loss for batch 1100 : 0.09715323150157928\n",
      "Training loss for batch 1101 : 0.056469496339559555\n",
      "Training loss for batch 1102 : 0.009350938722491264\n",
      "Training loss for batch 1103 : 0.13443835079669952\n",
      "Training loss for batch 1104 : 0.036459583789110184\n",
      "Training loss for batch 1105 : 0.0907604768872261\n",
      "Training loss for batch 1106 : 0.024342261254787445\n",
      "Training loss for batch 1107 : 0.10034636408090591\n",
      "Training loss for batch 1108 : 0.0733216181397438\n",
      "Training loss for batch 1109 : 0.07353012263774872\n",
      "Training loss for batch 1110 : 0.13415853679180145\n",
      "Training loss for batch 1111 : 0.11976613849401474\n",
      "Training loss for batch 1112 : 0.085960254073143\n",
      "Training loss for batch 1113 : 0.014665192924439907\n",
      "Training loss for batch 1114 : 0.11528883874416351\n",
      "Training loss for batch 1115 : 0.06920243799686432\n",
      "Training loss for batch 1116 : 0.06709437817335129\n",
      "Training loss for batch 1117 : 0.10728345066308975\n",
      "Training loss for batch 1118 : 0.020556988194584846\n",
      "Training loss for batch 1119 : 0.09688995033502579\n",
      "Training loss for batch 1120 : 0.07595166563987732\n",
      "Training loss for batch 1121 : 0.10372098535299301\n",
      "Training loss for batch 1122 : 0.053184784948825836\n",
      "Training loss for batch 1123 : 0.06273499876260757\n",
      "Training loss for batch 1124 : 0.03638933226466179\n",
      "Training loss for batch 1125 : 0.02084513008594513\n",
      "Training loss for batch 1126 : 0.019187703728675842\n",
      "Training loss for batch 1127 : 0.06966380774974823\n",
      "Training loss for batch 1128 : 0.017597606405615807\n",
      "Training loss for batch 1129 : 0.0779060572385788\n",
      "Training loss for batch 1130 : 0.0036862497217953205\n",
      "Training loss for batch 1131 : 0.19951845705509186\n",
      "Training loss for batch 1132 : 0.02195226028561592\n",
      "Training loss for batch 1133 : 0.08245949447154999\n",
      "Training loss for batch 1134 : 0.030284736305475235\n",
      "Training loss for batch 1135 : 0.022611519321799278\n",
      "Training loss for batch 1136 : 0.07399386912584305\n",
      "Training loss for batch 1137 : 0.016450950875878334\n",
      "Training loss for batch 1138 : 0.0562782846391201\n",
      "Training loss for batch 1139 : 0.1681707203388214\n",
      "Training loss for batch 1140 : 0.08461889624595642\n",
      "Training loss for batch 1141 : 0.053051192313432693\n",
      "Training loss for batch 1142 : 0.03037744015455246\n",
      "Training loss for batch 1143 : 0.0013176202774047852\n",
      "Training loss for batch 1144 : 0.09018964320421219\n",
      "Training loss for batch 1145 : 0.09038293361663818\n",
      "Training loss for batch 1146 : 0.01942966692149639\n",
      "Training loss for batch 1147 : 0.0684550330042839\n",
      "Training loss for batch 1148 : 0.08522722125053406\n",
      "Training loss for batch 1149 : 0.05313320830464363\n",
      "Training loss for batch 1150 : 0.03719046711921692\n",
      "Training loss for batch 1151 : 0.02591058984398842\n",
      "Training loss for batch 1152 : 0.0405539870262146\n",
      "Training loss for batch 1153 : 0.0033994291443377733\n",
      "Training loss for batch 1154 : 0.04785310477018356\n",
      "Training loss for batch 1155 : 0.031905174255371094\n",
      "Training loss for batch 1156 : 0.03787437453866005\n",
      "Training loss for batch 1157 : 0.03354499116539955\n",
      "Training loss for batch 1158 : 0.04588276147842407\n",
      "Training loss for batch 1159 : 0.10905290395021439\n",
      "Training loss for batch 1160 : 0.03155195713043213\n",
      "Training loss for batch 1161 : 0.04378772899508476\n",
      "Training loss for batch 1162 : 0.022526372224092484\n",
      "Training loss for batch 1163 : 0.02271275781095028\n",
      "Training loss for batch 1164 : 0.07845127582550049\n",
      "Training loss for batch 1165 : 0.019720474258065224\n",
      "Training loss for batch 1166 : 0.14273998141288757\n",
      "Training loss for batch 1167 : 0.10264285653829575\n",
      "Training loss for batch 1168 : 0.06819606572389603\n",
      "Training loss for batch 1169 : 0.030286844819784164\n",
      "Training loss for batch 1170 : 0.04588142782449722\n",
      "Training loss for batch 1171 : 0.0794840008020401\n",
      "Training loss for batch 1172 : 0.07266250252723694\n",
      "Training loss for batch 1173 : 0.1354173868894577\n",
      "Training loss for batch 1174 : 0.02911176159977913\n",
      "Training loss for batch 1175 : 0.09268271923065186\n",
      "Training loss for batch 1176 : 0.017081711441278458\n",
      "Training loss for batch 1177 : 0.0559038408100605\n",
      "Training loss for batch 1178 : 0.048102449625730515\n",
      "Training loss for batch 1179 : 0.0429660864174366\n",
      "Training loss for batch 1180 : 0.014927992597222328\n",
      "Training loss for batch 1181 : 0.2208423912525177\n",
      "Training loss for batch 1182 : 0.03082350827753544\n",
      "Training loss for batch 1183 : 0.03306649997830391\n",
      "Training loss for batch 1184 : 0.2461237609386444\n",
      "Training loss for batch 1185 : 0.19751182198524475\n",
      "Training loss for batch 1186 : 0.02707037143409252\n",
      "Training loss for batch 1187 : 0.09470012784004211\n",
      "Training loss for batch 1188 : 0.0778607577085495\n",
      "Training loss for batch 1189 : 0.1252678632736206\n",
      "Training loss for batch 1190 : 0.1316985785961151\n",
      "Training loss for batch 1191 : 0.1324167102575302\n",
      "Training loss for batch 1192 : 0.01042355690151453\n",
      "Training loss for batch 1193 : 0.1176471933722496\n",
      "Training loss for batch 1194 : 0.10606934130191803\n",
      "Training loss for batch 1195 : 0.010006819851696491\n",
      "Training loss for batch 1196 : 0.05072629824280739\n",
      "Training loss for batch 1197 : 0.12864550948143005\n",
      "Training loss for batch 1198 : 0.1222158670425415\n",
      "Training loss for batch 1199 : 0.007477283477783203\n",
      "Training loss for batch 1200 : 0.045006539672613144\n",
      "Training loss for batch 1201 : 0.1143491342663765\n",
      "Training loss for batch 1202 : 0.013254309073090553\n",
      "Training loss for batch 1203 : 0.03404666855931282\n",
      "Training loss for batch 1204 : 0.017665458843111992\n",
      "Training loss for batch 1205 : 0.09749627113342285\n",
      "Training loss for batch 1206 : 0.12225242704153061\n",
      "Training loss for batch 1207 : 0.05394626408815384\n",
      "Training loss for batch 1208 : 0.02422897145152092\n",
      "Training loss for batch 1209 : 0.02052517980337143\n",
      "Training loss for batch 1210 : 0.14245198667049408\n",
      "Training loss for batch 1211 : 0.1044604554772377\n",
      "Training loss for batch 1212 : 0.07344313710927963\n",
      "Training loss for batch 1213 : 0.07232199609279633\n",
      "Training loss for batch 1214 : 0.03578801080584526\n",
      "Training loss for batch 1215 : 0.05819564685225487\n",
      "Training loss for batch 1216 : 0.25134360790252686\n",
      "Training loss for batch 1217 : 0.07381241023540497\n",
      "Training loss for batch 1218 : 0.06467967480421066\n",
      "Training loss for batch 1219 : 0.05830654874444008\n",
      "Training loss for batch 1220 : 0.0835510790348053\n",
      "Training loss for batch 1221 : 0.08808692544698715\n",
      "Training loss for batch 1222 : 0.04888881742954254\n",
      "Training loss for batch 1223 : 0.2869802415370941\n",
      "Training loss for batch 1224 : 0.08415260165929794\n",
      "Training loss for batch 1225 : 0.10285858064889908\n",
      "Training loss for batch 1226 : 0.07421603053808212\n",
      "Training loss for batch 1227 : 0.030177056789398193\n",
      "Training loss for batch 1228 : 0.2151896357536316\n",
      "Training loss for batch 1229 : 0.0122228367254138\n",
      "Training loss for batch 1230 : 0.16170836985111237\n",
      "Training loss for batch 1231 : 0.041051995009183884\n",
      "Training loss for batch 1232 : 0.059778135269880295\n",
      "Training loss for batch 1233 : 0.06767989695072174\n",
      "Training loss for batch 1234 : 0.042899396270513535\n",
      "Training loss for batch 1235 : 0.09519093483686447\n",
      "Training loss for batch 1236 : 0.05913805589079857\n",
      "Training loss for batch 1237 : 0.054733067750930786\n",
      "Training loss for batch 1238 : 0.06328617036342621\n",
      "Training loss for batch 1239 : 0.05029173195362091\n",
      "Training loss for batch 1240 : 0.0432114414870739\n",
      "Training loss for batch 1241 : 0.08673181384801865\n",
      "Training loss for batch 1242 : 0.01954999379813671\n",
      "Training loss for batch 1243 : 0.1381647139787674\n",
      "Training loss for batch 1244 : 0.12894853949546814\n",
      "Training loss for batch 1245 : 0.07792957127094269\n",
      "Training loss for batch 1246 : 0.1045057401061058\n",
      "Training loss for batch 1247 : 0.019538087770342827\n",
      "Training loss for batch 1248 : 0.007782123517245054\n",
      "Training loss for batch 1249 : 0.09415797889232635\n",
      "Training loss for batch 1250 : 0.15728533267974854\n",
      "Training loss for batch 1251 : 0.013570166192948818\n",
      "Training loss for batch 1252 : 0.05296868085861206\n",
      "Training loss for batch 1253 : 0.05230956897139549\n",
      "Training loss for batch 1254 : 0.08620554953813553\n",
      "Training loss for batch 1255 : 0.03640446066856384\n",
      "Training loss for batch 1256 : 0.017187362536787987\n",
      "Training loss for batch 1257 : 0.0733683630824089\n",
      "Training loss for batch 1258 : 0.07776830345392227\n",
      "Training loss for batch 1259 : 0.0665181577205658\n",
      "Training loss for batch 1260 : 0.01721782423555851\n",
      "Training loss for batch 1261 : 0.06308948993682861\n",
      "Training loss for batch 1262 : 0.1962318867444992\n",
      "Training loss for batch 1263 : 0.10662773251533508\n",
      "Training loss for batch 1264 : 0.03154740855097771\n",
      "Training loss for batch 1265 : 0.023547308519482613\n",
      "Training loss for batch 1266 : 0.012052977457642555\n",
      "Training loss for batch 1267 : 0.06875557452440262\n",
      "Training loss for batch 1268 : 0.11223913729190826\n",
      "Training loss for batch 1269 : 0.10427114367485046\n",
      "Training loss for batch 1270 : 0.05847461149096489\n",
      "Training loss for batch 1271 : 0.04277405887842178\n",
      "Training loss for batch 1272 : 0.12324973195791245\n",
      "Training loss for batch 1273 : 0.09584518522024155\n",
      "Training loss for batch 1274 : 0.14792363345623016\n",
      "Training loss for batch 1275 : 0.08044403791427612\n",
      "Training loss for batch 1276 : 0.08475353568792343\n",
      "Training loss for batch 1277 : 0.11847390979528427\n",
      "Training loss for batch 1278 : 0.04068794474005699\n",
      "Training loss for batch 1279 : 0.04341418296098709\n",
      "Training loss for batch 1280 : 0.12441929429769516\n",
      "Training loss for batch 1281 : 0.09249567985534668\n",
      "Training loss for batch 1282 : 0.02153182402253151\n",
      "Training loss for batch 1283 : 0.011465686373412609\n",
      "Training loss for batch 1284 : 0.050882887095212936\n",
      "Training loss for batch 1285 : 0.10111108422279358\n",
      "Training loss for batch 1286 : 0.06818810105323792\n",
      "Training loss for batch 1287 : 0.0926140546798706\n",
      "Training loss for batch 1288 : 0.11316841840744019\n",
      "Training loss for batch 1289 : 0.040190503001213074\n",
      "Training loss for batch 1290 : 0.036314547061920166\n",
      "Training loss for batch 1291 : 0.06451452523469925\n",
      "Training loss for batch 1292 : 0.09715605527162552\n",
      "Training loss for batch 1293 : 0.053110238164663315\n",
      "Training loss for batch 1294 : 0.1329594999551773\n",
      "Training loss for batch 1295 : 0.034550201147794724\n",
      "Training loss for batch 1296 : 0.1654210090637207\n",
      "Training loss for batch 1297 : 0.09398073703050613\n",
      "Training loss for batch 1298 : 0.15241311490535736\n",
      "Training loss for batch 1299 : 0.004384447820484638\n",
      "Training loss for batch 1300 : 0.03797561302781105\n",
      "Training loss for batch 1301 : 0.02324037440121174\n",
      "Training loss for batch 1302 : 0.13207319378852844\n",
      "Training loss for batch 1303 : 0.132925346493721\n",
      "Training loss for batch 1304 : 0.028530120849609375\n",
      "Training loss for batch 1305 : 0.07624227553606033\n",
      "Training loss for batch 1306 : 0.03646641969680786\n",
      "Training loss for batch 1307 : 0.1785125732421875\n",
      "Training loss for batch 1308 : 0.0396505668759346\n",
      "Training loss for batch 1309 : 0.031777746975421906\n",
      "Training loss for batch 1310 : 0.13857300579547882\n",
      "Training loss for batch 1311 : 0.07591652870178223\n",
      "Training loss for batch 1312 : 0.006064946763217449\n",
      "Training loss for batch 1313 : 0.0010843542404472828\n",
      "Training loss for batch 1314 : 0.06683118641376495\n",
      "Training loss for batch 1315 : 0.02771748974919319\n",
      "Training loss for batch 1316 : 0.06896940618753433\n",
      "Training loss for batch 1317 : 0.10236690193414688\n",
      "Training loss for batch 1318 : 0.03442273288965225\n",
      "Training loss for batch 1319 : 0.07629623264074326\n",
      "Training loss for batch 1320 : 0.032535865902900696\n",
      "Training loss for batch 1321 : 0.11648354679346085\n",
      "Training loss for batch 1322 : 0.027950454503297806\n",
      "Training loss for batch 1323 : 0.024802131578326225\n",
      "Training loss for batch 1324 : 0.04164880886673927\n",
      "Training loss for batch 1325 : 0.01832396537065506\n",
      "Training loss for batch 1326 : 0.06195712462067604\n",
      "Training loss for batch 1327 : 0.060766369104385376\n",
      "Training loss for batch 1328 : 0.07699181884527206\n",
      "Training loss for batch 1329 : 0.10782033950090408\n",
      "Training loss for batch 1330 : 0.06118379905819893\n",
      "Training loss for batch 1331 : 0.07580670714378357\n",
      "Training loss for batch 1332 : 0.24028629064559937\n",
      "Training loss for batch 1333 : 0.03376901522278786\n",
      "Training loss for batch 1334 : 0.08064278215169907\n",
      "Training loss for batch 1335 : 0.09845131635665894\n",
      "Training loss for batch 1336 : 0.20280303061008453\n",
      "Training loss for batch 1337 : 0.08344239741563797\n",
      "Training loss for batch 1338 : 0.0231324452906847\n",
      "Training loss for batch 1339 : 0.06389806419610977\n",
      "Training loss for batch 1340 : 0.07494816929101944\n",
      "Training loss for batch 1341 : 0.057293832302093506\n",
      "Training loss for batch 1342 : 0.03068101592361927\n",
      "Training loss for batch 1343 : 0.0008107274770736694\n",
      "Training loss for batch 1344 : 0.054323215037584305\n",
      "Training loss for batch 1345 : 0.1642725169658661\n",
      "Training loss for batch 1346 : 0.051423218101263046\n",
      "Training loss for batch 1347 : 0.061701249331235886\n",
      "Training loss for batch 1348 : 0.1869080662727356\n",
      "Training loss for batch 1349 : 0.03923141956329346\n",
      "Training loss for batch 1350 : 0.05526699870824814\n",
      "Training loss for batch 1351 : 0.12403439730405807\n",
      "Training loss for batch 1352 : 0.09067468345165253\n",
      "Training loss for batch 1353 : 0.04774584621191025\n",
      "Training loss for batch 1354 : 0.0676153153181076\n",
      "Training loss for batch 1355 : 0.02016979642212391\n",
      "Training loss for batch 1356 : 0.14887920022010803\n",
      "Training loss for batch 1357 : 0.16270430386066437\n",
      "Training loss for batch 1358 : 0.0390230156481266\n",
      "Training loss for batch 1359 : 0.08760534971952438\n",
      "Training loss for batch 1360 : 0.01636437140405178\n",
      "Training loss for batch 1361 : 0.026178821921348572\n",
      "Training loss for batch 1362 : 0.022183388471603394\n",
      "Training loss for batch 1363 : 0.01787770539522171\n",
      "Training loss for batch 1364 : 0.08023958653211594\n",
      "Training loss for batch 1365 : 0.027499739080667496\n",
      "Training loss for batch 1366 : 0.06945041567087173\n",
      "Training loss for batch 1367 : 0.05739623308181763\n",
      "Training loss for batch 1368 : 0.03438617289066315\n",
      "Training loss for batch 1369 : 0.09494338929653168\n",
      "Training loss for batch 1370 : 0.1410844475030899\n",
      "Training loss for batch 1371 : 0.11024020612239838\n",
      "Training loss for batch 1372 : 0.08823675662279129\n",
      "Training loss for batch 1373 : 0.022574767470359802\n",
      "Training loss for batch 1374 : 0.12563025951385498\n",
      "Training loss for batch 1375 : 0.0064730048179626465\n",
      "Training loss for batch 1376 : 0.12815351784229279\n",
      "Training loss for batch 1377 : 0.16626974940299988\n",
      "Training loss for batch 1378 : 0.049592047929763794\n",
      "Training loss for batch 1379 : 0.001973944017663598\n",
      "Training loss for batch 1380 : 0.02422356605529785\n",
      "Training loss for batch 1381 : 0.04733691364526749\n",
      "Training loss for batch 1382 : 0.08006558567285538\n",
      "Training loss for batch 1383 : 0.18858927488327026\n",
      "Training loss for batch 1384 : 0.028712885454297066\n",
      "Training loss for batch 1385 : 0.05159280076622963\n",
      "Training loss for batch 1386 : 0.13159210979938507\n",
      "Training loss for batch 1387 : 0.010534851811826229\n",
      "Training loss for batch 1388 : 0.12625744938850403\n",
      "Training loss for batch 1389 : 0.09051212668418884\n",
      "Training loss for batch 1390 : 0.03745125234127045\n",
      "Training loss for batch 1391 : 0.07912460714578629\n",
      "Training loss for batch 1392 : 0.013530739583075047\n",
      "Training loss for batch 1393 : 0.061939552426338196\n",
      "Training loss for batch 1394 : 0.18332499265670776\n",
      "Training loss for batch 1395 : 0.15142373740673065\n",
      "Training loss for batch 1396 : 0.1265345811843872\n",
      "Training loss for batch 1397 : 0.02175096422433853\n",
      "Training loss for batch 1398 : 0.04845152050256729\n",
      "Training loss for batch 1399 : 0.05292743816971779\n",
      "Training loss for batch 1400 : 0.005661714822053909\n",
      "Training loss for batch 1401 : 0.06477735936641693\n",
      "Training loss for batch 1402 : 0.03830649331212044\n",
      "Training loss for batch 1403 : 0.0886748656630516\n",
      "Training loss for batch 1404 : 0.00577278807759285\n",
      "Training loss for batch 1405 : 0.07664524018764496\n",
      "Training loss for batch 1406 : 0.09032827615737915\n",
      "Training loss for batch 1407 : 0.24141302704811096\n",
      "Training loss for batch 1408 : 0.07021062076091766\n",
      "Training loss for batch 1409 : 0.1352841854095459\n",
      "Training loss for batch 1410 : 0.06233476474881172\n",
      "Training loss for batch 1411 : 0.07892579585313797\n",
      "Training loss for batch 1412 : 0.017632555216550827\n",
      "Training loss for batch 1413 : 0.07167833298444748\n",
      "Training loss for batch 1414 : 0.1296990066766739\n",
      "Training loss for batch 1415 : 0.035327330231666565\n",
      "Training loss for batch 1416 : 0.19793108105659485\n",
      "Training loss for batch 1417 : 0.08463285118341446\n",
      "Training loss for batch 1418 : 0.02289663627743721\n",
      "Training loss for batch 1419 : 0.13638144731521606\n",
      "Training loss for batch 1420 : 0.14003756642341614\n",
      "Training loss for batch 1421 : 0.03062089905142784\n",
      "Training loss for batch 1422 : 0.016046082600951195\n",
      "Training loss for batch 1423 : 0.07723458111286163\n",
      "Training loss for batch 1424 : 0.0018264122772961855\n",
      "Training loss for batch 1425 : 0.10133671760559082\n",
      "Training loss for batch 1426 : 0.0030588111840188503\n",
      "Training loss for batch 1427 : 0.05281185731291771\n",
      "Training loss for batch 1428 : 0.11084792762994766\n",
      "Training loss for batch 1429 : 0.10822794586420059\n",
      "Training loss for batch 1430 : 0.16924329102039337\n",
      "Training loss for batch 1431 : 0.06429934501647949\n",
      "Training loss for batch 1432 : 0.16276437044143677\n",
      "Training loss for batch 1433 : 0.03706882894039154\n",
      "Training loss for batch 1434 : 0.08976487815380096\n",
      "Training loss for batch 1435 : 0.13295255601406097\n",
      "Training loss for batch 1436 : 0.15150412917137146\n",
      "Training loss for batch 1437 : 0.05076248571276665\n",
      "Training loss for batch 1438 : 0.09942010790109634\n",
      "Training loss for batch 1439 : 0.034997571259737015\n",
      "Training loss for batch 1440 : 0.0686943531036377\n",
      "Training loss for batch 1441 : 0.013132438994944096\n",
      "Training loss for batch 1442 : 0.11389626562595367\n",
      "Training loss for batch 1443 : 0.03712135925889015\n",
      "Training loss for batch 1444 : 0.037615951150655746\n",
      "Training loss for batch 1445 : 0.013762246817350388\n",
      "Training loss for batch 1446 : 0.20644475519657135\n",
      "Training loss for batch 1447 : 0.04794697463512421\n",
      "Training loss for batch 1448 : 0.12011552602052689\n",
      "Training loss for batch 1449 : 0.16386114060878754\n",
      "Training loss for batch 1450 : 0.024034222587943077\n",
      "Training loss for batch 1451 : 0.023051366209983826\n",
      "Training loss for batch 1452 : 3.219337685322898e-08\n",
      "Training loss for batch 1453 : 0.03213074803352356\n",
      "Training loss for batch 1454 : 0.040999505668878555\n",
      "Training loss for batch 1455 : 0.11687426269054413\n",
      "Training loss for batch 1456 : 0.12172907590866089\n",
      "Training loss for batch 1457 : 0.03113497979938984\n",
      "Training loss for batch 1458 : 0.04077116400003433\n",
      "Training loss for batch 1459 : 0.26293009519577026\n",
      "Training loss for batch 1460 : 0.12371055036783218\n",
      "Training loss for batch 1461 : 0.06911056488752365\n",
      "Training loss for batch 1462 : 0.017212778329849243\n",
      "Training loss for batch 1463 : 0.17608976364135742\n",
      "Training loss for batch 1464 : 0.018022343516349792\n",
      "Training loss for batch 1465 : 0.06147563457489014\n",
      "Training loss for batch 1466 : 0.06660749018192291\n",
      "Training loss for batch 1467 : 0.06383367627859116\n",
      "Training loss for batch 1468 : 0.0952836349606514\n",
      "Training loss for batch 1469 : 0.10181494802236557\n",
      "Training loss for batch 1470 : 0.17000225186347961\n",
      "Training loss for batch 1471 : 0.0389506034553051\n",
      "Training loss for batch 1472 : 0.04177362844347954\n",
      "Training loss for batch 1473 : 0.07229596376419067\n",
      "Training loss for batch 1474 : 0.07717959582805634\n",
      "Training loss for batch 1475 : 0.024552568793296814\n",
      "Training loss for batch 1476 : 0.023200204595923424\n",
      "Training loss for batch 1477 : 0.08838878571987152\n",
      "Training loss for batch 1478 : 0.04058653116226196\n",
      "Training loss for batch 1479 : 0.1718548685312271\n",
      "Training loss for batch 1480 : 0.05118802934885025\n",
      "Training loss for batch 1481 : 0.034696292132139206\n",
      "Training loss for batch 1482 : 0.04601205140352249\n",
      "Training loss for batch 1483 : 0.029591094702482224\n",
      "Training loss for batch 1484 : 0.18751882016658783\n",
      "Training loss for batch 1485 : 0.015761148184537888\n",
      "Training loss for batch 1486 : 0.05738367512822151\n",
      "Training loss for batch 1487 : 0.04779619351029396\n",
      "Training loss for batch 1488 : 0.14424088597297668\n",
      "Training loss for batch 1489 : 0.12578891217708588\n",
      "Training loss for batch 1490 : 0.1132916808128357\n",
      "Training loss for batch 1491 : 0.07525729387998581\n",
      "Training loss for batch 1492 : 0.08458896726369858\n",
      "Training loss for batch 1493 : 0.0399310477077961\n",
      "Training loss for batch 1494 : 0.14029522240161896\n",
      "Training loss for batch 1495 : 0.18962982296943665\n",
      "Training loss for batch 1496 : 0.10535024851560593\n",
      "Training loss for batch 1497 : 0.040589749813079834\n",
      "Training loss for batch 1498 : 0.0038683817256242037\n",
      "Training loss for batch 1499 : 0.06386823952198029\n",
      "Training loss for batch 1500 : 0.004067540168762207\n",
      "Training loss for batch 1501 : 0.028897156938910484\n",
      "Training loss for batch 1502 : 0.19314704835414886\n",
      "Training loss for batch 1503 : 0.013589834794402122\n",
      "Training loss for batch 1504 : 0.09276881068944931\n",
      "Training loss for batch 1505 : 0.05512116849422455\n",
      "Training loss for batch 1506 : 1.7409812258506463e-08\n",
      "Training loss for batch 1507 : 0.13003814220428467\n",
      "Training loss for batch 1508 : 0.006396056152880192\n",
      "Training loss for batch 1509 : 0.10132613778114319\n",
      "Training loss for batch 1510 : 0.07159572094678879\n",
      "Training loss for batch 1511 : 0.05037488043308258\n",
      "Training loss for batch 1512 : 0.15498025715351105\n",
      "Training loss for batch 1513 : 0.0234640222042799\n",
      "Training loss for batch 1514 : 0.009924563579261303\n",
      "Training loss for batch 1515 : 0.018695298582315445\n",
      "Training loss for batch 1516 : 0.029894068837165833\n",
      "Training loss for batch 1517 : 0.013489818200469017\n",
      "Training loss for batch 1518 : 0.06847177445888519\n",
      "Training loss for batch 1519 : 0.09211252629756927\n",
      "Training loss for batch 1520 : 0.1478361338376999\n",
      "Training loss for batch 1521 : 0.08316083997488022\n",
      "Training loss for batch 1522 : 0.04401448741555214\n",
      "Training loss for batch 1523 : 0.012714117765426636\n",
      "Training loss for batch 1524 : 0.05788335204124451\n",
      "Training loss for batch 1525 : 0.04338797926902771\n",
      "Training loss for batch 1526 : 0.1373928189277649\n",
      "Training loss for batch 1527 : 0.016616949811577797\n",
      "Training loss for batch 1528 : 0.22763226926326752\n",
      "Training loss for batch 1529 : 0.04508006572723389\n",
      "Training loss for batch 1530 : 0.006194594781845808\n",
      "Training loss for batch 1531 : 0.03481326997280121\n",
      "Training loss for batch 1532 : 0.016297556459903717\n",
      "Training loss for batch 1533 : 0.03296765312552452\n",
      "Training loss for batch 1534 : 0.025533035397529602\n",
      "Training loss for batch 1535 : 0.11836867034435272\n",
      "Training loss for batch 1536 : 0.07991816103458405\n",
      "Training loss for batch 1537 : 0.06373316049575806\n",
      "Training loss for batch 1538 : 0.10123192518949509\n",
      "Training loss for batch 1539 : 0.1136941984295845\n",
      "Training loss for batch 1540 : 0.15170282125473022\n",
      "Training loss for batch 1541 : 0.07138869166374207\n",
      "Training loss for batch 1542 : 0.2018260806798935\n",
      "Training loss for batch 1543 : 0.03733222931623459\n",
      "Training loss for batch 1544 : 0.02159830369055271\n",
      "Training loss for batch 1545 : 0.06265521794557571\n",
      "Training loss for batch 1546 : 0.077396921813488\n",
      "Training loss for batch 1547 : 0.20813584327697754\n",
      "Training loss for batch 1548 : 0.021176960319280624\n",
      "Training loss for batch 1549 : 3.307142293351717e-08\n",
      "Training loss for batch 1550 : 0.15683555603027344\n",
      "Training loss for batch 1551 : 0.01993544027209282\n",
      "Training loss for batch 1552 : 0.06292814016342163\n",
      "Training loss for batch 1553 : 0.06387869268655777\n",
      "Training loss for batch 1554 : 0.09922918677330017\n",
      "Training loss for batch 1555 : 0.07354646921157837\n",
      "Training loss for batch 1556 : 0.14463216066360474\n",
      "Training loss for batch 1557 : 0.03378451243042946\n",
      "Training loss for batch 1558 : 0.0745423436164856\n",
      "Training loss for batch 1559 : 0.07866345345973969\n",
      "Training loss for batch 1560 : 0.025024786591529846\n",
      "Training loss for batch 1561 : 1.646437794988742e-07\n",
      "Training loss for batch 1562 : 0.08209928125143051\n",
      "Training loss for batch 1563 : 2.0767286557088482e-08\n",
      "Training loss for batch 1564 : 0.062459658831357956\n",
      "Training loss for batch 1565 : 0.13480506837368011\n",
      "Training loss for batch 1566 : 0.11944694072008133\n",
      "Training loss for batch 1567 : 0.10593177378177643\n",
      "Training loss for batch 1568 : 0.08019393682479858\n",
      "Training loss for batch 1569 : 0.09106141328811646\n",
      "Training loss for batch 1570 : 0.06619071215391159\n",
      "Training loss for batch 1571 : 0.051177978515625\n",
      "Training loss for batch 1572 : 0.021256908774375916\n",
      "Training loss for batch 1573 : 0.12680508196353912\n",
      "Training loss for batch 1574 : 0.016598448157310486\n",
      "Training loss for batch 1575 : 0.005175720434635878\n",
      "Training loss for batch 1576 : 0.03728131577372551\n",
      "Training loss for batch 1577 : 0.002962829079478979\n",
      "Training loss for batch 1578 : 0.039574652910232544\n",
      "Training loss for batch 1579 : 0.0014162346487864852\n",
      "Training loss for batch 1580 : 0.04243176430463791\n",
      "Training loss for batch 1581 : 0.03674332797527313\n",
      "Training loss for batch 1582 : 0.11607038229703903\n",
      "Training loss for batch 1583 : 0.10724844038486481\n",
      "Training loss for batch 1584 : 0.02592487633228302\n",
      "Training loss for batch 1585 : 0.001107969437725842\n",
      "Training loss for batch 1586 : 0.045600418001413345\n",
      "Training loss for batch 1587 : 0.07208148390054703\n",
      "Training loss for batch 1588 : 0.08298038691282272\n",
      "Training loss for batch 1589 : 0.03526194393634796\n",
      "Training loss for batch 1590 : 0.009536216035485268\n",
      "Training loss for batch 1591 : 0.1431482434272766\n",
      "Training loss for batch 1592 : 0.0745633915066719\n",
      "Training loss for batch 1593 : 0.056167684495449066\n",
      "Training loss for batch 1594 : 0.07966440916061401\n",
      "Training loss for batch 1595 : 0.06936485320329666\n",
      "Training loss for batch 1596 : 0.12395820021629333\n",
      "Training loss for batch 1597 : 0.054324276745319366\n",
      "Training loss for batch 1598 : 0.0942922830581665\n",
      "Training loss for batch 1599 : 0.03925585746765137\n",
      "Training loss for batch 1600 : 0.13514547049999237\n",
      "Training loss for batch 1601 : 0.02690674178302288\n",
      "Training loss for batch 1602 : 0.03243708610534668\n",
      "Training loss for batch 1603 : 0.04905648157000542\n",
      "Training loss for batch 1604 : 0.015624341554939747\n",
      "Training loss for batch 1605 : 0.12979641556739807\n",
      "Training loss for batch 1606 : 0.04557399824261665\n",
      "Training loss for batch 1607 : 0.00951121561229229\n",
      "Training loss for batch 1608 : 0.08887293934822083\n",
      "Training loss for batch 1609 : 0.05210214480757713\n",
      "Training loss for batch 1610 : 0.09794294834136963\n",
      "Training loss for batch 1611 : 0.012168167158961296\n",
      "Training loss for batch 1612 : 0.10012534260749817\n",
      "Training loss for batch 1613 : 0.06918144226074219\n",
      "Training loss for batch 1614 : 0.03907889127731323\n",
      "Training loss for batch 1615 : 0.055112842470407486\n",
      "Training loss for batch 1616 : 0.16134537756443024\n",
      "Training loss for batch 1617 : 0.042255084961652756\n",
      "Training loss for batch 1618 : 0.10812833905220032\n",
      "Training loss for batch 1619 : 0.1177477240562439\n",
      "Training loss for batch 1620 : 0.0980408564209938\n",
      "Training loss for batch 1621 : 0.023390505462884903\n",
      "Training loss for batch 1622 : 0.05186435580253601\n",
      "Training loss for batch 1623 : 0.017908282577991486\n",
      "Training loss for batch 1624 : 0.13414399325847626\n",
      "Training loss for batch 1625 : 0.03986617550253868\n",
      "Training loss for batch 1626 : 0.0426483079791069\n",
      "Training loss for batch 1627 : 0.13302838802337646\n",
      "Training loss for batch 1628 : 0.07494230568408966\n",
      "Training loss for batch 1629 : 0.019312549382448196\n",
      "Training loss for batch 1630 : 0.005436956416815519\n",
      "Training loss for batch 1631 : 0.10783179849386215\n",
      "Training loss for batch 1632 : 0.0610341876745224\n",
      "Training loss for batch 1633 : 0.08243480324745178\n",
      "Training loss for batch 1634 : 0.01131870225071907\n",
      "Training loss for batch 1635 : 0.0453205332159996\n",
      "Training loss for batch 1636 : 0.07269338518381119\n",
      "Training loss for batch 1637 : 0.04928113520145416\n",
      "Training loss for batch 1638 : 0.06655748933553696\n",
      "Training loss for batch 1639 : 0.07951486855745316\n",
      "Training loss for batch 1640 : 0.07894787937402725\n",
      "Training loss for batch 1641 : 0.01999724842607975\n",
      "Training loss for batch 1642 : 0.08728055655956268\n",
      "Training loss for batch 1643 : 0.003107726573944092\n",
      "Training loss for batch 1644 : 0.20292513072490692\n",
      "Training loss for batch 1645 : 0.012652924284338951\n",
      "Training loss for batch 1646 : 0.04690736159682274\n",
      "Training loss for batch 1647 : 0.0228603333234787\n",
      "Training loss for batch 1648 : 0.07895280420780182\n",
      "Training loss for batch 1649 : 0.07496438175439835\n",
      "Training loss for batch 1650 : 0.06626210361719131\n",
      "Training loss for batch 1651 : 0.02303711511194706\n",
      "Training loss for batch 1652 : 0.00245681032538414\n",
      "Training loss for batch 1653 : 0.08743657171726227\n",
      "Training loss for batch 1654 : 0.0980699360370636\n",
      "Training loss for batch 1655 : 0.04254394769668579\n",
      "Training loss for batch 1656 : 0.11039640754461288\n",
      "Training loss for batch 1657 : 0.05911093205213547\n",
      "Training loss for batch 1658 : 0.03427605703473091\n",
      "Training loss for batch 1659 : 0.11630205810070038\n",
      "Training loss for batch 1660 : 0.08912758529186249\n",
      "Training loss for batch 1661 : 0.002373520052060485\n",
      "Training loss for batch 1662 : 0.021284684538841248\n",
      "Training loss for batch 1663 : 0.08033464103937149\n",
      "Training loss for batch 1664 : 0.03192508965730667\n",
      "Training loss for batch 1665 : 0.03806387633085251\n",
      "Training loss for batch 1666 : 0.20944352447986603\n",
      "Training loss for batch 1667 : 0.011929676868021488\n",
      "Training loss for batch 1668 : 0.10073135048151016\n",
      "Training loss for batch 1669 : 0.11079569905996323\n",
      "Training loss for batch 1670 : 0.05832048878073692\n",
      "Training loss for batch 1671 : 0.0993097722530365\n",
      "Training loss for batch 1672 : 0.12512366473674774\n",
      "Training loss for batch 1673 : 0.12353630363941193\n",
      "Training loss for batch 1674 : 0.04694274812936783\n",
      "Training loss for batch 1675 : 0.052183423191308975\n",
      "Training loss for batch 1676 : 0.056064870208501816\n",
      "Training loss for batch 1677 : 0.019978275522589684\n",
      "Training loss for batch 1678 : 0.09842891991138458\n",
      "Training loss for batch 1679 : 0.07156620919704437\n",
      "Training loss for batch 1680 : 0.07277584820985794\n",
      "Training loss for batch 1681 : 0.051289986819028854\n",
      "Training loss for batch 1682 : 0.08910948038101196\n",
      "Training loss for batch 1683 : 0.11738132685422897\n",
      "Training loss for batch 1684 : 0.13674895465373993\n",
      "Training loss for batch 1685 : 0.09108129888772964\n",
      "Training loss for batch 1686 : 0.05337485298514366\n",
      "Training loss for batch 1687 : 0.018217431381344795\n",
      "Training loss for batch 1688 : 0.04785133898258209\n",
      "Training loss for batch 1689 : 0.11707434803247452\n",
      "Training loss for batch 1690 : 0.02858126536011696\n",
      "Training loss for batch 1691 : 0.011810416355729103\n",
      "Training loss for batch 1692 : 0.030685586854815483\n",
      "Training loss for batch 1693 : 0.12309098243713379\n",
      "Training loss for batch 1694 : 0.1538928747177124\n",
      "Training loss for batch 1695 : 0.0744507759809494\n",
      "Training loss for batch 1696 : 0.023063188418745995\n",
      "Training loss for batch 1697 : 0.05835758522152901\n",
      "Training loss for batch 1698 : 0.07524220645427704\n",
      "Training loss for batch 1699 : 0.03695933148264885\n",
      "Training loss for batch 1700 : 0.06112610176205635\n",
      "Training loss for batch 1701 : 0.02896622009575367\n",
      "Training loss for batch 1702 : 0.10401435196399689\n",
      "Training loss for batch 1703 : 0.020881347358226776\n",
      "Training loss for batch 1704 : 0.0014955654041841626\n",
      "Training loss for batch 1705 : 0.21855528652668\n",
      "Training loss for batch 1706 : 0.08906938880681992\n",
      "Training loss for batch 1707 : 0.028029097244143486\n",
      "Training loss for batch 1708 : 0.087946318089962\n",
      "Training loss for batch 1709 : 0.051154959946870804\n",
      "Training loss for batch 1710 : 0.02272428572177887\n",
      "Training loss for batch 1711 : 0.022733483463525772\n",
      "Training loss for batch 1712 : 0.0341164693236351\n",
      "Training loss for batch 1713 : 0.12882442772388458\n",
      "Training loss for batch 1714 : 0.12794756889343262\n",
      "Training loss for batch 1715 : 0.04555502161383629\n",
      "Training loss for batch 1716 : 0.002772009000182152\n",
      "Training loss for batch 1717 : 0.17286035418510437\n",
      "Training loss for batch 1718 : 0.10489247739315033\n",
      "Training loss for batch 1719 : 0.08163593709468842\n",
      "Training loss for batch 1720 : 0.043978068977594376\n",
      "Training loss for batch 1721 : 0.04910239577293396\n",
      "Training loss for batch 1722 : 0.0964290201663971\n",
      "Training loss for batch 1723 : 0.014407703652977943\n",
      "Training loss for batch 1724 : 0.04206345975399017\n",
      "Training loss for batch 1725 : 0.027052883058786392\n",
      "Training loss for batch 1726 : 0.006731261964887381\n",
      "Training loss for batch 1727 : 0.1400512158870697\n",
      "Training loss for batch 1728 : 0.004985791631042957\n",
      "Training loss for batch 1729 : 0.0861927792429924\n",
      "Training loss for batch 1730 : 0.07852455973625183\n",
      "Training loss for batch 1731 : 0.061065174639225006\n",
      "Training loss for batch 1732 : 0.28766071796417236\n",
      "Training loss for batch 1733 : 0.06065483018755913\n",
      "Training loss for batch 1734 : 0.013672192580997944\n",
      "Training loss for batch 1735 : 0.03071863204240799\n",
      "Training loss for batch 1736 : 0.07563237845897675\n",
      "Training loss for batch 1737 : 0.0835801362991333\n",
      "Training loss for batch 1738 : 0.06524060666561127\n",
      "Training loss for batch 1739 : 0.09918786585330963\n",
      "Training loss for batch 1740 : 0.10239711403846741\n",
      "Training loss for batch 1741 : 0.07911171019077301\n",
      "Training loss for batch 1742 : 0.10591932386159897\n",
      "Training loss for batch 1743 : 0.06187603995203972\n",
      "Training loss for batch 1744 : 0.0844075083732605\n",
      "Training loss for batch 1745 : 0.1430012732744217\n",
      "Training loss for batch 1746 : 0.11194989085197449\n",
      "Training loss for batch 1747 : 0.1083403155207634\n",
      "Training loss for batch 1748 : 0.052309513092041016\n",
      "Training loss for batch 1749 : 0.1458837240934372\n",
      "Training loss for batch 1750 : 0.07490357011556625\n",
      "Training loss for batch 1751 : 0.05714249610900879\n",
      "Training loss for batch 1752 : 0.12918540835380554\n",
      "Training loss for batch 1753 : 0.027393551543354988\n",
      "Training loss for batch 1754 : 0.05051988363265991\n",
      "Training loss for batch 1755 : 0.12692581117153168\n",
      "Training loss for batch 1756 : 0.021031472831964493\n",
      "Training loss for batch 1757 : 0.07294012606143951\n",
      "Training loss for batch 1758 : 0.0728437751531601\n",
      "Training loss for batch 1759 : 0.1135222539305687\n",
      "Training loss for batch 1760 : 0.08028919249773026\n",
      "Training loss for batch 1761 : 0.15381759405136108\n",
      "Training loss for batch 1762 : 0.1456899493932724\n",
      "Training loss for batch 1763 : 0.17627231776714325\n",
      "Training loss for batch 1764 : 0.008527164347469807\n",
      "Training loss for batch 1765 : 0.04470960795879364\n",
      "Training loss for batch 1766 : 0.04645388200879097\n",
      "Training loss for batch 1767 : 0.06390240788459778\n",
      "Training loss for batch 1768 : 0.12009669095277786\n",
      "Training loss for batch 1769 : 0.10497727245092392\n",
      "Training loss for batch 1770 : 0.07496132701635361\n",
      "Training loss for batch 1771 : 0.0182687658816576\n",
      "Training loss for batch 1772 : 0.011227943934500217\n",
      "Training loss for batch 1773 : 0.07135160267353058\n",
      "Training loss for batch 1774 : 0.14049337804317474\n",
      "Training loss for batch 1775 : 0.11116769909858704\n",
      "Training loss for batch 1776 : 0.0066665462218225\n",
      "Training loss for batch 1777 : 0.02279241755604744\n",
      "Training loss for batch 1778 : 0.11904911696910858\n",
      "Training loss for batch 1779 : 0.10371705889701843\n",
      "Training loss for batch 1780 : 0.036362286657094955\n",
      "Training loss for batch 1781 : 0.042675647884607315\n",
      "Training loss for batch 1782 : 0.22810162603855133\n",
      "Training loss for batch 1783 : 0.016911400482058525\n",
      "Training loss for batch 1784 : 0.039730288088321686\n",
      "Training loss for batch 1785 : 0.0013397751608863473\n",
      "Training loss for batch 1786 : 0.061008524149656296\n",
      "Training loss for batch 1787 : 0.03774459660053253\n",
      "Training loss for batch 1788 : 0.02312195859849453\n",
      "Training loss for batch 1789 : 0.014987900853157043\n",
      "Training loss for batch 1790 : 0.0997903123497963\n",
      "Training loss for batch 1791 : 0.03314102441072464\n",
      "Training loss for batch 1792 : 0.24326902627944946\n",
      "Training loss for batch 1793 : 0.11443199962377548\n",
      "Training loss for batch 1794 : 0.11455818265676498\n",
      "Training loss for batch 1795 : 0.08022518455982208\n",
      "Training loss for batch 1796 : 0.0049628643319010735\n",
      "Training loss for batch 1797 : 0.021511593833565712\n",
      "Training loss for batch 1798 : 0.04457346722483635\n",
      "Training loss for batch 1799 : 0.09281463921070099\n",
      "Training loss for batch 1800 : 0.03683461621403694\n",
      "Training loss for batch 1801 : 0.0437651164829731\n",
      "Training loss for batch 1802 : 0.1763216108083725\n",
      "Training loss for batch 1803 : 0.0842745453119278\n",
      "Training loss for batch 1804 : 0.2336026430130005\n",
      "Training loss for batch 1805 : 0.05412958562374115\n",
      "Training loss for batch 1806 : 0.08630900830030441\n",
      "Training loss for batch 1807 : 0.03978221118450165\n",
      "Training loss for batch 1808 : 0.008617160841822624\n",
      "Training loss for batch 1809 : 0.034630708396434784\n",
      "Training loss for batch 1810 : 0.08373778313398361\n",
      "Training loss for batch 1811 : 0.04911717772483826\n",
      "Training loss for batch 1812 : 0.0413193553686142\n",
      "Training loss for batch 1813 : 0.033051468431949615\n",
      "Training loss for batch 1814 : 0.0541631281375885\n",
      "Training loss for batch 1815 : 0.07548267394304276\n",
      "Training loss for batch 1816 : 0.04252331331372261\n",
      "Training loss for batch 1817 : 0.16180254518985748\n",
      "Training loss for batch 1818 : 0.1644948422908783\n",
      "Training loss for batch 1819 : 0.07716189324855804\n",
      "Training loss for batch 1820 : 0.08203751593828201\n",
      "Training loss for batch 1821 : 0.05233503505587578\n",
      "Training loss for batch 1822 : 0.041506338864564896\n",
      "Training loss for batch 1823 : 0.05816829577088356\n",
      "Training loss for batch 1824 : 0.1000756174325943\n",
      "Training loss for batch 1825 : 0.06910788267850876\n",
      "Training loss for batch 1826 : 0.07577969878911972\n",
      "Training loss for batch 1827 : 0.05739308148622513\n",
      "Training loss for batch 1828 : 0.15441343188285828\n",
      "Training loss for batch 1829 : 0.07248682528734207\n",
      "Training loss for batch 1830 : 0.050871264189481735\n",
      "Training loss for batch 1831 : 0.10740984976291656\n",
      "Training loss for batch 1832 : 0.04309212043881416\n",
      "Training loss for batch 1833 : 0.1598270833492279\n",
      "Training loss for batch 1834 : 0.03935898467898369\n",
      "Training loss for batch 1835 : 0.06342651695013046\n",
      "Training loss for batch 1836 : 0.07867395132780075\n",
      "Training loss for batch 1837 : 0.17574815452098846\n",
      "Training loss for batch 1838 : 5.456057294850325e-08\n",
      "Training loss for batch 1839 : 0.03678033873438835\n",
      "Training loss for batch 1840 : 0.1058216243982315\n",
      "Training loss for batch 1841 : 0.19645904004573822\n",
      "Training loss for batch 1842 : 0.057280685752630234\n",
      "Training loss for batch 1843 : 0.02717319317162037\n",
      "Training loss for batch 1844 : 0.03609362244606018\n",
      "Training loss for batch 1845 : 0.03716064989566803\n",
      "Training loss for batch 1846 : 0.05287203937768936\n",
      "Training loss for batch 1847 : 0.11705449968576431\n",
      "Training loss for batch 1848 : 0.12303739786148071\n",
      "Training loss for batch 1849 : 0.024590950459241867\n",
      "Training loss for batch 1850 : 0.037805408239364624\n",
      "Training loss for batch 1851 : 0.08924828469753265\n",
      "Training loss for batch 1852 : 0.10512285679578781\n",
      "Training loss for batch 1853 : 0.16338378190994263\n",
      "Training loss for batch 1854 : 0.036601994186639786\n",
      "Training loss for batch 1855 : 0.06765379011631012\n",
      "Training loss for batch 1856 : 0.027282647788524628\n",
      "Training loss for batch 1857 : 0.10289184749126434\n",
      "Training loss for batch 1858 : 0.2336745411157608\n",
      "Training loss for batch 1859 : 0.016328835859894753\n",
      "Training loss for batch 1860 : 0.04873763024806976\n",
      "Training loss for batch 1861 : 0.05746542662382126\n",
      "Training loss for batch 1862 : 0.043054062873125076\n",
      "Training loss for batch 1863 : 0.009004944004118443\n",
      "Training loss for batch 1864 : 0.10866951942443848\n",
      "Training loss for batch 1865 : 0.02645166590809822\n",
      "Training loss for batch 1866 : 0.0766126960515976\n",
      "Training loss for batch 1867 : 0.05271180346608162\n",
      "Training loss for batch 1868 : 0.036679815500974655\n",
      "Training loss for batch 1869 : 0.025591930374503136\n",
      "Training loss for batch 1870 : 0.02671353705227375\n",
      "Training loss for batch 1871 : 0.05671079829335213\n",
      "Training loss for batch 1872 : 0.07012666761875153\n",
      "Training loss for batch 1873 : 0.10619287192821503\n",
      "Training loss for batch 1874 : 0.08047937601804733\n",
      "Training loss for batch 1875 : 0.027999024838209152\n",
      "Training loss for batch 1876 : 0.0913984403014183\n",
      "Training loss for batch 1877 : 0.159434974193573\n",
      "Training loss for batch 1878 : 0.08770482242107391\n",
      "Training loss for batch 1879 : 0.07036120444536209\n",
      "Training loss for batch 1880 : 0.11631153523921967\n",
      "Training loss for batch 1881 : 0.03943917155265808\n",
      "Training loss for batch 1882 : 0.044056545943021774\n",
      "Training loss for batch 1883 : 0.034996867179870605\n",
      "Training loss for batch 1884 : 0.08820731192827225\n",
      "Training loss for batch 1885 : 0.04623817279934883\n",
      "Training loss for batch 1886 : 0.10953953117132187\n",
      "Training loss for batch 1887 : 0.05321166664361954\n",
      "Training loss for batch 1888 : 0.02625950612127781\n",
      "Training loss for batch 1889 : 0.08532537519931793\n",
      "Training loss for batch 1890 : 0.03283530846238136\n",
      "Training loss for batch 1891 : 0.12023469805717468\n",
      "Training loss for batch 1892 : 0.027316538617014885\n",
      "Training loss for batch 1893 : 0.05026410147547722\n",
      "Training loss for batch 1894 : 0.1011626273393631\n",
      "Training loss for batch 1895 : 0.013681560754776001\n",
      "Training loss for batch 1896 : 0.04429183155298233\n",
      "Training loss for batch 1897 : 0.06859440356492996\n",
      "Training loss for batch 1898 : 0.11042308062314987\n",
      "Training loss for batch 1899 : 0.0406942144036293\n",
      "Training loss for batch 1900 : 0.052047405391931534\n",
      "Training loss for batch 1901 : 0.045652925968170166\n",
      "Training loss for batch 1902 : 0.00922014843672514\n",
      "Training loss for batch 1903 : 0.026500418782234192\n",
      "Training loss for batch 1904 : 0.10649702697992325\n",
      "Training loss for batch 1905 : 0.10132937878370285\n",
      "Training loss for batch 1906 : 0.10158371925354004\n",
      "Training loss for batch 1907 : 0.1532329022884369\n",
      "Training loss for batch 1908 : 0.07217845320701599\n",
      "Training loss for batch 1909 : 0.0192330963909626\n",
      "Training loss for batch 1910 : 0.07261935621500015\n",
      "Training loss for batch 1911 : 0.2351117730140686\n",
      "Training loss for batch 1912 : 0.030551686882972717\n",
      "Training loss for batch 1913 : 0.05523298308253288\n",
      "Training loss for batch 1914 : 0.015400480479001999\n",
      "Training loss for batch 1915 : 0.03202130272984505\n",
      "Training loss for batch 1916 : 0.04689062014222145\n",
      "Training loss for batch 1917 : 0.04724685847759247\n",
      "Training loss for batch 1918 : 0.0073044500313699245\n",
      "Training loss for batch 1919 : 0.1343427300453186\n",
      "Training loss for batch 1920 : 0.09254665672779083\n",
      "Training loss for batch 1921 : 0.026876891031861305\n",
      "Training loss for batch 1922 : 0.06022754684090614\n",
      "Training loss for batch 1923 : 0.18237663805484772\n",
      "Training loss for batch 1924 : 0.08879460394382477\n",
      "Training loss for batch 1925 : 0.024609079584479332\n",
      "Training loss for batch 1926 : 0.0899624228477478\n",
      "Training loss for batch 1927 : 0.044015560299158096\n",
      "Training loss for batch 1928 : 0.06102553382515907\n",
      "Training loss for batch 1929 : 0.034349191933870316\n",
      "Training loss for batch 1930 : 0.049640703946352005\n",
      "Training loss for batch 1931 : 0.061180178076028824\n",
      "Training loss for batch 1932 : 0.12937860190868378\n",
      "Training loss for batch 1933 : 0.06372702121734619\n",
      "Training loss for batch 1934 : 0.09150565415620804\n",
      "Training loss for batch 1935 : 0.0812329351902008\n",
      "Training loss for batch 1936 : 0.037204042077064514\n",
      "Training loss for batch 1937 : 0.06618490815162659\n",
      "Training loss for batch 1938 : 0.02970423363149166\n",
      "Training loss for batch 1939 : 0.09818445146083832\n",
      "Training loss for batch 1940 : 0.14028620719909668\n",
      "Training loss for batch 1941 : 0.09717483818531036\n",
      "Training loss for batch 1942 : 0.059113964438438416\n",
      "Training loss for batch 1943 : 0.06718885153532028\n",
      "Training loss for batch 1944 : 0.053391654044389725\n",
      "Training loss for batch 1945 : 0.11880330741405487\n",
      "Training loss for batch 1946 : 0.09845675528049469\n",
      "Training loss for batch 1947 : 0.08163772523403168\n",
      "Training loss for batch 1948 : 0.1964740753173828\n",
      "Training loss for batch 1949 : 0.20662608742713928\n",
      "Training loss for batch 1950 : 0.019939471036195755\n",
      "Training loss for batch 1951 : 0.010658472776412964\n",
      "Training loss for batch 1952 : 0.006172549445182085\n",
      "Training loss for batch 1953 : 0.09972932934761047\n",
      "Training loss for batch 1954 : 0.06565835326910019\n",
      "Training loss for batch 1955 : 0.20264755189418793\n",
      "Training loss for batch 1956 : 0.04953526705503464\n",
      "Training loss for batch 1957 : 0.13728450238704681\n",
      "Training loss for batch 1958 : 0.04027184844017029\n",
      "Training loss for batch 1959 : 0.14789287745952606\n",
      "Training loss for batch 1960 : 0.010792690329253674\n",
      "Training loss for batch 1961 : 0.050912585109472275\n",
      "Training loss for batch 1962 : 0.1819346845149994\n",
      "Training loss for batch 1963 : 0.22569309175014496\n",
      "Training loss for batch 1964 : 0.05419198423624039\n",
      "Training loss for batch 1965 : 0.008563872426748276\n",
      "Training loss for batch 1966 : 0.06699732691049576\n",
      "Training loss for batch 1967 : 0.018756844103336334\n",
      "Training loss for batch 1968 : 0.05216626077890396\n",
      "Training loss for batch 1969 : 0.04746398702263832\n",
      "Training loss for batch 1970 : 0.07642677426338196\n",
      "Training loss for batch 1971 : 0.07467589527368546\n",
      "Training loss for batch 1972 : 0.027110887691378593\n",
      "Training loss for batch 1973 : 1.5374402551060484e-07\n",
      "Training loss for batch 1974 : 0.19624315202236176\n",
      "Training loss for batch 1975 : 0.07208527624607086\n",
      "Training loss for batch 1976 : 0.060283590108156204\n",
      "Training loss for batch 1977 : 0.019721737131476402\n",
      "Training loss for batch 1978 : 0.041533879935741425\n",
      "Training loss for batch 1979 : 0.030959727242588997\n",
      "Training loss for batch 1980 : 0.062129996716976166\n",
      "Training loss for batch 1981 : 0.12616905570030212\n",
      "Training loss for batch 1982 : 0.14246615767478943\n",
      "Training loss for batch 1983 : 0.06919337064027786\n",
      "Training loss for batch 1984 : 0.030460737645626068\n",
      "Training loss for batch 1985 : 0.045801855623722076\n",
      "Training loss for batch 1986 : 0.035035815089941025\n",
      "Training loss for batch 1987 : 0.09563599526882172\n",
      "Training loss for batch 1988 : 0.013311374932527542\n",
      "Training loss for batch 1989 : 0.13326981663703918\n",
      "Training loss for batch 1990 : 0.022117603570222855\n",
      "Training loss for batch 1991 : 0.1251303106546402\n",
      "Training loss for batch 1992 : 0.07793180644512177\n",
      "Training loss for batch 1993 : 0.04157819226384163\n",
      "Training loss for batch 1994 : 0.10014857351779938\n",
      "Training loss for batch 1995 : 0.08247978985309601\n",
      "Training loss for batch 1996 : 0.1991632729768753\n",
      "Training loss for batch 1997 : 0.08695179969072342\n",
      "Training loss for batch 1998 : 0.09723348915576935\n",
      "Training loss for batch 1999 : 0.026300981640815735\n",
      "Training loss for batch 2000 : 0.1097620502114296\n",
      "Training loss for batch 2001 : 0.2158202975988388\n",
      "Training loss for batch 2002 : 0.04760286584496498\n",
      "Training loss for batch 2003 : 0.02670034021139145\n",
      "Training loss for batch 2004 : 0.031620196998119354\n",
      "Training loss for batch 2005 : 0.07345989346504211\n",
      "Training loss for batch 2006 : 0.03758244588971138\n",
      "Training loss for batch 2007 : 0.21970140933990479\n",
      "Training loss for batch 2008 : 0.058832209557294846\n",
      "Training loss for batch 2009 : 0.07322941720485687\n",
      "Training loss for batch 2010 : 0.027109559625387192\n",
      "Training loss for batch 2011 : 0.08353198319673538\n",
      "Training loss for batch 2012 : 0.09053482860326767\n",
      "Training loss for batch 2013 : 0.10129522532224655\n",
      "Training loss for batch 2014 : 0.14571310579776764\n",
      "Training loss for batch 2015 : 0.023161856457591057\n",
      "Training loss for batch 2016 : 0.13911889493465424\n",
      "Training loss for batch 2017 : 0.14337264001369476\n",
      "Training loss for batch 2018 : 0.13628041744232178\n",
      "Training loss for batch 2019 : 0.04336296766996384\n",
      "Training loss for batch 2020 : 0.11424598097801208\n",
      "Training loss for batch 2021 : 0.12016229331493378\n",
      "Training loss for batch 2022 : 0.051300935447216034\n",
      "Training loss for batch 2023 : 0.07852645963430405\n",
      "Training loss for batch 2024 : 0.2342081516981125\n",
      "Training loss for batch 2025 : 0.032497432082891464\n",
      "Training loss for batch 2026 : 0.014449247159063816\n",
      "Training loss for batch 2027 : 0.027056002989411354\n",
      "Training loss for batch 2028 : 0.10640566051006317\n",
      "Training loss for batch 2029 : 0.055101074278354645\n",
      "Training loss for batch 2030 : 0.0449160672724247\n",
      "Training loss for batch 2031 : 0.012428182177245617\n",
      "Training loss for batch 2032 : 0.011603148654103279\n",
      "Training loss for batch 2033 : 0.01755216345191002\n",
      "Training loss for batch 2034 : 0.16584669053554535\n",
      "Training loss for batch 2035 : 0.018393173813819885\n",
      "Training loss for batch 2036 : 0.049922168254852295\n",
      "Training loss for batch 2037 : 0.13473384082317352\n",
      "Training loss for batch 2038 : 0.038394030183553696\n",
      "Training loss for batch 2039 : 0.06692445278167725\n",
      "Training loss for batch 2040 : 0.05838983878493309\n",
      "Training loss for batch 2041 : 0.1381349414587021\n",
      "Training loss for batch 2042 : 0.19235742092132568\n",
      "Training loss for batch 2043 : 0.11647914350032806\n",
      "Training loss for batch 2044 : 0.07438351958990097\n",
      "Training loss for batch 2045 : 0.07427927106618881\n",
      "Training loss for batch 2046 : 0.047361910343170166\n",
      "Training loss for batch 2047 : 0.015790946781635284\n",
      "Training loss for batch 2048 : 0.033280376344919205\n",
      "Training loss for batch 2049 : 0.020951222628355026\n",
      "Training loss for batch 2050 : 0.1270037144422531\n",
      "Training loss for batch 2051 : 0.06556853652000427\n",
      "Training loss for batch 2052 : 0.04978291690349579\n",
      "Training loss for batch 2053 : 0.07707521319389343\n",
      "Training loss for batch 2054 : 0.02984280325472355\n",
      "Training loss for batch 2055 : 0.061205558478832245\n",
      "Training loss for batch 2056 : 0.025394875556230545\n",
      "Training loss for batch 2057 : 0.029539605602622032\n",
      "Training loss for batch 2058 : 0.054751936346292496\n",
      "Training loss for batch 2059 : 0.027616778388619423\n",
      "Training loss for batch 2060 : 0.000798118591774255\n",
      "Training loss for batch 2061 : 0.11206375807523727\n",
      "Training loss for batch 2062 : 0.10375402122735977\n",
      "Training loss for batch 2063 : 0.060924235731363297\n",
      "Training loss for batch 2064 : 0.1110445111989975\n",
      "Training loss for batch 2065 : 0.1428518146276474\n",
      "Training loss for batch 2066 : 0.12383627146482468\n",
      "Training loss for batch 2067 : 0.039070043712854385\n",
      "Training loss for batch 2068 : 0.07905036211013794\n",
      "Training loss for batch 2069 : 0.24075065553188324\n",
      "Training loss for batch 2070 : 0.02741583064198494\n",
      "Training loss for batch 2071 : 0.0028121171053498983\n",
      "Training loss for batch 2072 : 0.11779709160327911\n",
      "Training loss for batch 2073 : 0.009840456768870354\n",
      "Training loss for batch 2074 : 0.007914518937468529\n",
      "Training loss for batch 2075 : 0.03128134459257126\n",
      "Training loss for batch 2076 : 0.10814708471298218\n",
      "Training loss for batch 2077 : 0.0010638110106810927\n",
      "Training loss for batch 2078 : 0.058283764868974686\n",
      "Training loss for batch 2079 : 0.12328914552927017\n",
      "Training loss for batch 2080 : 0.09118349105119705\n",
      "Training loss for batch 2081 : 0.04443434253334999\n",
      "Training loss for batch 2082 : 0.007086075376719236\n",
      "Training loss for batch 2083 : 0.07770977169275284\n",
      "Training loss for batch 2084 : 0.03893790394067764\n",
      "Training loss for batch 2085 : 0.02443564310669899\n",
      "Training loss for batch 2086 : 0.015885209664702415\n",
      "Training loss for batch 2087 : 4.671015219059882e-08\n",
      "Training loss for batch 2088 : 0.09446065127849579\n",
      "Training loss for batch 2089 : 0.017563102766871452\n",
      "Training loss for batch 2090 : 0.013747205957770348\n",
      "Training loss for batch 2091 : 0.10634808987379074\n",
      "Training loss for batch 2092 : 0.11852084845304489\n",
      "Training loss for batch 2093 : 0.04024619981646538\n",
      "Training loss for batch 2094 : 0.09207144379615784\n",
      "Training loss for batch 2095 : 0.08169268071651459\n",
      "Training loss for batch 2096 : 0.10137185454368591\n",
      "Training loss for batch 2097 : 0.10298387706279755\n",
      "Training loss for batch 2098 : 0.0686805248260498\n",
      "Training loss for batch 2099 : 0.08983338624238968\n",
      "Training loss for batch 2100 : 0.01347028836607933\n",
      "Training loss for batch 2101 : 0.12062154710292816\n",
      "Training loss for batch 2102 : 0.05067827180027962\n",
      "Training loss for batch 2103 : 0.03635595366358757\n",
      "Training loss for batch 2104 : 0.12643437087535858\n",
      "Training loss for batch 2105 : 0.013682498596608639\n",
      "Training loss for batch 2106 : 0.09829086810350418\n",
      "Training loss for batch 2107 : 0.06206679716706276\n",
      "Training loss for batch 2108 : 0.08217958360910416\n",
      "Training loss for batch 2109 : 0.03834659606218338\n",
      "Training loss for batch 2110 : 0.08209740370512009\n",
      "Training loss for batch 2111 : 0.09122154116630554\n",
      "Training loss for batch 2112 : 0.06344355642795563\n",
      "Training loss for batch 2113 : 0.13017843663692474\n",
      "Training loss for batch 2114 : 0.033631086349487305\n",
      "Training loss for batch 2115 : 0.021579880267381668\n",
      "Training loss for batch 2116 : 0.012858145870268345\n",
      "Training loss for batch 2117 : 0.08458114415407181\n",
      "Training loss for batch 2118 : 0.04326101019978523\n",
      "Training loss for batch 2119 : 0.08619090169668198\n",
      "Training loss for batch 2120 : 0.09388473629951477\n",
      "Training loss for batch 2121 : 0.061075981706380844\n",
      "Training loss for batch 2122 : 0.1409008800983429\n",
      "Training loss for batch 2123 : 0.09687653928995132\n",
      "Training loss for batch 2124 : 0.024906987324357033\n",
      "Training loss for batch 2125 : 0.028666796162724495\n",
      "Training loss for batch 2126 : 0.10311522334814072\n",
      "Training loss for batch 2127 : 0.03870697692036629\n",
      "Training loss for batch 2128 : 0.04934956133365631\n",
      "Training loss for batch 2129 : 0.06619860231876373\n",
      "Training loss for batch 2130 : 0.08915040642023087\n",
      "Training loss for batch 2131 : 0.07397384941577911\n",
      "Training loss for batch 2132 : 0.02685304917395115\n",
      "Training loss for batch 2133 : 0.06771820783615112\n",
      "Training loss for batch 2134 : 0.12382211536169052\n",
      "Training loss for batch 2135 : 0.06975658982992172\n",
      "Training loss for batch 2136 : 0.026076393201947212\n",
      "Training loss for batch 2137 : 0.0818871483206749\n",
      "Training loss for batch 2138 : 0.010852610692381859\n",
      "Training loss for batch 2139 : 0.09791356325149536\n",
      "Training loss for batch 2140 : 0.10250350087881088\n",
      "Training loss for batch 2141 : 0.06236913800239563\n",
      "Training loss for batch 2142 : 0.015341782942414284\n",
      "Training loss for batch 2143 : 0.15247990190982819\n",
      "Training loss for batch 2144 : 0.03214935585856438\n",
      "Training loss for batch 2145 : 0.1673898845911026\n",
      "Training loss for batch 2146 : 0.12494608014822006\n",
      "Training loss for batch 2147 : 0.08423825353384018\n",
      "Training loss for batch 2148 : 0.1801687479019165\n",
      "Training loss for batch 2149 : 0.030856100842356682\n",
      "Training loss for batch 2150 : 0.07506386935710907\n",
      "Training loss for batch 2151 : 0.0995805487036705\n",
      "Training loss for batch 2152 : 0.06359968334436417\n",
      "Training loss for batch 2153 : 0.08492738008499146\n",
      "Training loss for batch 2154 : 0.06592727452516556\n",
      "Training loss for batch 2155 : 0.03896257281303406\n",
      "Training loss for batch 2156 : 0.03003867156803608\n",
      "Training loss for batch 2157 : 0.04802140220999718\n",
      "Training loss for batch 2158 : 0.04941818490624428\n",
      "Training loss for batch 2159 : 0.05733572691679001\n",
      "Training loss for batch 2160 : 0.051747456192970276\n",
      "Training loss for batch 2161 : 0.12143583595752716\n",
      "Training loss for batch 2162 : 0.1565476655960083\n",
      "Training loss for batch 2163 : 0.07340960204601288\n",
      "Training loss for batch 2164 : 0.12639020383358002\n",
      "Training loss for batch 2165 : 0.015458657406270504\n",
      "Training loss for batch 2166 : 0.13749805092811584\n",
      "Training loss for batch 2167 : 0.06781512498855591\n",
      "Training loss for batch 2168 : 0.04759204387664795\n",
      "Training loss for batch 2169 : 0.09663784503936768\n",
      "Training loss for batch 2170 : 0.06715919822454453\n",
      "Training loss for batch 2171 : 0.02386491559445858\n",
      "Training loss for batch 2172 : 0.0945935994386673\n",
      "Training loss for batch 2173 : 0.03308246284723282\n",
      "Training loss for batch 2174 : 0.09851263463497162\n",
      "Training loss for batch 2175 : 0.030429130420088768\n",
      "Training loss for batch 2176 : 0.026097841560840607\n",
      "Training loss for batch 2177 : 0.09516850113868713\n",
      "Training loss for batch 2178 : 0.09381483495235443\n",
      "Training loss for batch 2179 : 0.04440690577030182\n",
      "Training loss for batch 2180 : 0.17239171266555786\n",
      "Training loss for batch 2181 : 0.0010888738324865699\n",
      "Training loss for batch 2182 : 0.027184858918190002\n",
      "Training loss for batch 2183 : 0.11613772809505463\n",
      "Training loss for batch 2184 : 0.04940779134631157\n",
      "Training loss for batch 2185 : 0.0178117286413908\n",
      "Training loss for batch 2186 : 0.05207657814025879\n",
      "Training loss for batch 2187 : 0.03918876498937607\n",
      "Training loss for batch 2188 : 0.028425097465515137\n",
      "Training loss for batch 2189 : 0.030758598819375038\n",
      "Training loss for batch 2190 : 0.0889989510178566\n",
      "Training loss for batch 2191 : 0.023020531982183456\n",
      "Training loss for batch 2192 : 0.023901982232928276\n",
      "Training loss for batch 2193 : 0.05140950158238411\n",
      "Training loss for batch 2194 : 0.16780029237270355\n",
      "Training loss for batch 2195 : 0.019936079159379005\n",
      "Training loss for batch 2196 : 0.025305703282356262\n",
      "Training loss for batch 2197 : 0.25423696637153625\n",
      "Training loss for batch 2198 : 0.13771510124206543\n",
      "Training loss for batch 2199 : 0.10930383950471878\n",
      "Training loss for batch 2200 : 0.032550472766160965\n",
      "Training loss for batch 2201 : 0.0058663333766162395\n",
      "Training loss for batch 2202 : 0.03344200178980827\n",
      "Training loss for batch 2203 : 0.0882394090294838\n",
      "Training loss for batch 2204 : 0.13603100180625916\n",
      "Training loss for batch 2205 : 0.03276199474930763\n",
      "Training loss for batch 2206 : 0.04750920832157135\n",
      "Training loss for batch 2207 : 0.13947083055973053\n",
      "Training loss for batch 2208 : 0.009859937243163586\n",
      "Training loss for batch 2209 : 0.1811671108007431\n",
      "Training loss for batch 2210 : 0.046374328434467316\n",
      "Training loss for batch 2211 : 0.08240348100662231\n",
      "Training loss for batch 2212 : 0.032399732619524\n",
      "Training loss for batch 2213 : 0.0038107880391180515\n",
      "Training loss for batch 2214 : 0.12708501517772675\n",
      "Training loss for batch 2215 : 0.06445033103227615\n",
      "Training loss for batch 2216 : 0.0012661527143791318\n",
      "Training loss for batch 2217 : 0.05676426738500595\n",
      "Training loss for batch 2218 : 0.12408499419689178\n",
      "Training loss for batch 2219 : 0.13937051594257355\n",
      "Training loss for batch 2220 : 0.07288938015699387\n",
      "Training loss for batch 2221 : 0.15886737406253815\n",
      "Training loss for batch 2222 : 0.04021112620830536\n",
      "Training loss for batch 2223 : 0.07038254290819168\n",
      "Training loss for batch 2224 : 0.12690864503383636\n",
      "Training loss for batch 2225 : 0.02869006246328354\n",
      "Training loss for batch 2226 : 0.07460196316242218\n",
      "Training loss for batch 2227 : 0.10888756066560745\n",
      "Training loss for batch 2228 : 0.11225487291812897\n",
      "Training loss for batch 2229 : 0.16606080532073975\n",
      "Training loss for batch 2230 : 0.10919062048196793\n",
      "Training loss for batch 2231 : 0.09478554874658585\n",
      "Training loss for batch 2232 : 0.020660636946558952\n",
      "Training loss for batch 2233 : 0.04703516885638237\n",
      "Training loss for batch 2234 : 0.06963710486888885\n",
      "Training loss for batch 2235 : 0.03324134275317192\n",
      "Training loss for batch 2236 : 0.1330493539571762\n",
      "Training loss for batch 2237 : 0.17793293297290802\n",
      "Training loss for batch 2238 : 0.11885450035333633\n",
      "Training loss for batch 2239 : 0.13540545105934143\n",
      "Training loss for batch 2240 : 0.1164693534374237\n",
      "Training loss for batch 2241 : 0.13288354873657227\n",
      "Training loss for batch 2242 : 0.07974247634410858\n",
      "Training loss for batch 2243 : 0.09755291044712067\n",
      "Training loss for batch 2244 : 0.13238342106342316\n",
      "Training loss for batch 2245 : 0.007849842309951782\n",
      "Training loss for batch 2246 : 0.09484883397817612\n",
      "Training loss for batch 2247 : 0.0427982360124588\n",
      "Training loss for batch 2248 : 0.019244318827986717\n",
      "Training loss for batch 2249 : 0.056837160140275955\n",
      "Training loss for batch 2250 : 0.09994051605463028\n",
      "Training loss for batch 2251 : 0.10456488281488419\n",
      "Training loss for batch 2252 : 0.030218010768294334\n",
      "Training loss for batch 2253 : 0.04864784702658653\n",
      "Training loss for batch 2254 : 0.09196621924638748\n",
      "Training loss for batch 2255 : 0.009515819139778614\n",
      "Training loss for batch 2256 : 0.07513978332281113\n",
      "Training loss for batch 2257 : 0.01888824999332428\n",
      "Training loss for batch 2258 : 0.014652998186647892\n",
      "Training loss for batch 2259 : 0.09996522963047028\n",
      "Training loss for batch 2260 : 0.017431480810046196\n",
      "Training loss for batch 2261 : 0.12466516345739365\n",
      "Training loss for batch 2262 : 0.08239776641130447\n",
      "Training loss for batch 2263 : 0.08202318102121353\n",
      "Training loss for batch 2264 : 0.064980648458004\n",
      "Training loss for batch 2265 : 0.10032669454813004\n",
      "Training loss for batch 2266 : 0.044183071702718735\n",
      "Training loss for batch 2267 : 0.032578010112047195\n",
      "Training loss for batch 2268 : 0.04018744081258774\n",
      "Training loss for batch 2269 : 0.09234819561243057\n",
      "Training loss for batch 2270 : 0.02166105806827545\n",
      "Training loss for batch 2271 : 0.13024403154850006\n",
      "Training loss for batch 2272 : 0.04526917636394501\n",
      "Training loss for batch 2273 : 0.09634125977754593\n",
      "Training loss for batch 2274 : 0.022687915712594986\n",
      "Training loss for batch 2275 : 0.06265505403280258\n",
      "Training loss for batch 2276 : 0.05263374000787735\n",
      "Training loss for batch 2277 : 0.09830117225646973\n",
      "Training loss for batch 2278 : 0.09666379541158676\n",
      "Training loss for batch 2279 : 0.00551254115998745\n",
      "Training loss for batch 2280 : 0.04308721795678139\n",
      "Training loss for batch 2281 : 0.08959022909402847\n",
      "Training loss for batch 2282 : 0.13315235078334808\n",
      "Training loss for batch 2283 : 0.12201829999685287\n",
      "Training loss for batch 2284 : 0.09582027792930603\n",
      "Training loss for batch 2285 : 0.05675002932548523\n",
      "Training loss for batch 2286 : 0.1580291986465454\n",
      "Training loss for batch 2287 : 0.035830553621053696\n",
      "Training loss for batch 2288 : 0.04260504990816116\n",
      "Training loss for batch 2289 : 0.1302555948495865\n",
      "Training loss for batch 2290 : 0.1171291172504425\n",
      "Training loss for batch 2291 : 0.034187525510787964\n",
      "Training loss for batch 2292 : 0.06036437302827835\n",
      "Training loss for batch 2293 : 0.04900597780942917\n",
      "Training loss for batch 2294 : 0.013485949486494064\n",
      "Training loss for batch 2295 : 0.07674330472946167\n",
      "Training loss for batch 2296 : 0.08186087012290955\n",
      "Training loss for batch 2297 : 0.13268321752548218\n",
      "Training loss for batch 2298 : 0.1824517846107483\n",
      "Training loss for batch 2299 : 0.02807486243546009\n",
      "Training loss for batch 2300 : 0.013008513487875462\n",
      "Training loss for batch 2301 : 0.08780607581138611\n",
      "Training loss for batch 2302 : 0.026566671207547188\n",
      "Training loss for batch 2303 : 0.1702403575181961\n",
      "Training loss for batch 2304 : 0.057856325060129166\n",
      "Training loss for batch 2305 : 0.0018926759948953986\n",
      "Training loss for batch 2306 : 0.11514145880937576\n",
      "Training loss for batch 2307 : 0.056714147329330444\n",
      "Training loss for batch 2308 : 0.024643728509545326\n",
      "Training loss for batch 2309 : 0.11312329769134521\n",
      "Training loss for batch 2310 : 0.020992709323763847\n",
      "Training loss for batch 2311 : 0.04038301855325699\n",
      "Training loss for batch 2312 : 0.10564935952425003\n",
      "Training loss for batch 2313 : 0.04724108427762985\n",
      "Training loss for batch 2314 : 0.07397229224443436\n",
      "Training loss for batch 2315 : 0.05290859192609787\n",
      "Training loss for batch 2316 : 0.027730973437428474\n",
      "Training loss for batch 2317 : 0.056445781141519547\n",
      "Training loss for batch 2318 : 0.0753374695777893\n",
      "Training loss for batch 2319 : 0.0004612952470779419\n",
      "Training loss for batch 2320 : 0.1439608633518219\n",
      "Training loss for batch 2321 : 0.026294488459825516\n",
      "Training loss for batch 2322 : 0.06874874979257584\n",
      "Training loss for batch 2323 : 4.5384652480606746e-08\n",
      "Training loss for batch 2324 : 0.06931717693805695\n",
      "Training loss for batch 2325 : 0.050950001925230026\n",
      "Training loss for batch 2326 : 0.031089434400200844\n",
      "Training loss for batch 2327 : 0.02054266445338726\n",
      "Training loss for batch 2328 : 0.10696547478437424\n",
      "Training loss for batch 2329 : 0.10888414829969406\n",
      "Training loss for batch 2330 : 0.020544877275824547\n",
      "Training loss for batch 2331 : 0.027157645672559738\n",
      "Training loss for batch 2332 : 0.22148360311985016\n",
      "Training loss for batch 2333 : 0.083265021443367\n",
      "Training loss for batch 2334 : 0.014066179282963276\n",
      "Training loss for batch 2335 : 0.021103717386722565\n",
      "Training loss for batch 2336 : 0.04756791144609451\n",
      "Training loss for batch 2337 : 0.20427371561527252\n",
      "Training loss for batch 2338 : 0.06082555651664734\n",
      "Training loss for batch 2339 : 0.013991539366543293\n",
      "Training loss for batch 2340 : 0.08244217187166214\n",
      "Training loss for batch 2341 : 0.10900776088237762\n",
      "Training loss for batch 2342 : 0.027918130159378052\n",
      "Training loss for batch 2343 : 0.04629037156701088\n",
      "Training loss for batch 2344 : 0.09756386280059814\n",
      "Training loss for batch 2345 : 0.09460195153951645\n",
      "Training loss for batch 2346 : 0.02230263315141201\n",
      "Training loss for batch 2347 : 0.0091664157807827\n",
      "Training loss for batch 2348 : 0.0717003270983696\n",
      "Training loss for batch 2349 : 0.05743471533060074\n",
      "Training loss for batch 2350 : 0.05774562805891037\n",
      "Training loss for batch 2351 : 0.05389389768242836\n",
      "Training loss for batch 2352 : 0.09467632323503494\n",
      "Training loss for batch 2353 : 0.03265564888715744\n",
      "Training loss for batch 2354 : 0.04131242260336876\n",
      "Training loss for batch 2355 : 0.030806416645646095\n",
      "Training loss for batch 2356 : 0.03765194118022919\n",
      "Training loss for batch 2357 : 0.2437683790922165\n",
      "Training loss for batch 2358 : 0.025672338902950287\n",
      "Training loss for batch 2359 : 0.06232626363635063\n",
      "Training loss for batch 2360 : 0.04705989733338356\n",
      "Training loss for batch 2361 : 0.05636339262127876\n",
      "Training loss for batch 2362 : 0.05523701757192612\n",
      "Training loss for batch 2363 : 0.02227676473557949\n",
      "Training loss for batch 2364 : 0.09042093902826309\n",
      "Training loss for batch 2365 : 0.05301642045378685\n",
      "Training loss for batch 2366 : 0.09539686143398285\n",
      "Training loss for batch 2367 : 0.09356478601694107\n",
      "Training loss for batch 2368 : 0.07799679040908813\n",
      "Training loss for batch 2369 : 0.013841373845934868\n",
      "Training loss for batch 2370 : 0.016535094007849693\n",
      "Training loss for batch 2371 : 0.12738075852394104\n",
      "Training loss for batch 2372 : 0.10579473525285721\n",
      "Training loss for batch 2373 : 0.07281593978404999\n",
      "Training loss for batch 2374 : 0.06855590641498566\n",
      "Training loss for batch 2375 : 0.07655489444732666\n",
      "Training loss for batch 2376 : 0.028777234256267548\n",
      "Training loss for batch 2377 : 0.03712866082787514\n",
      "Training loss for batch 2378 : 0.0\n",
      "Training loss for batch 2379 : 0.060884274542331696\n",
      "Training loss for batch 2380 : 0.012906447052955627\n",
      "Training loss for batch 2381 : 0.09181585162878036\n",
      "Training loss for batch 2382 : 0.04646685719490051\n",
      "Training loss for batch 2383 : 0.05323867127299309\n",
      "Training loss for batch 2384 : 0.13648854196071625\n",
      "Training loss for batch 2385 : 0.009450230747461319\n",
      "Training loss for batch 2386 : 0.044361699372529984\n",
      "Training loss for batch 2387 : 0.06086525693535805\n",
      "Training loss for batch 2388 : 0.03320316597819328\n",
      "Training loss for batch 2389 : 0.1359264850616455\n",
      "Training loss for batch 2390 : 0.07710214704275131\n",
      "Training loss for batch 2391 : 0.020765936002135277\n",
      "Training loss for batch 2392 : 0.14027920365333557\n",
      "Training loss for batch 2393 : 0.1654711663722992\n",
      "Training loss for batch 2394 : 0.15146976709365845\n",
      "Training loss for batch 2395 : 0.22223332524299622\n",
      "Training loss for batch 2396 : 0.09208650887012482\n",
      "Training loss for batch 2397 : 0.018290696665644646\n",
      "Training loss for batch 2398 : 0.034468892961740494\n",
      "Training loss for batch 2399 : 0.017495153471827507\n",
      "Training loss for batch 2400 : 0.015290524810552597\n",
      "Training loss for batch 2401 : 0.1534503847360611\n",
      "Training loss for batch 2402 : 0.029245086014270782\n",
      "Training loss for batch 2403 : 0.007608039304614067\n",
      "Training loss for batch 2404 : 0.13857944309711456\n",
      "Training loss for batch 2405 : 0.06927812099456787\n",
      "Training loss for batch 2406 : 0.010508248582482338\n",
      "Training loss for batch 2407 : 0.14542998373508453\n",
      "Training loss for batch 2408 : 0.07678595185279846\n",
      "Training loss for batch 2409 : 0.19620610773563385\n",
      "Training loss for batch 2410 : 0.059142448008060455\n",
      "Training loss for batch 2411 : 0.005370798520743847\n",
      "Training loss for batch 2412 : 0.08220560848712921\n",
      "Training loss for batch 2413 : 0.007448725868016481\n",
      "Training loss for batch 2414 : 0.12192821502685547\n",
      "Training loss for batch 2415 : 0.08032242953777313\n",
      "Training loss for batch 2416 : 0.2917361855506897\n",
      "Training loss for batch 2417 : 0.1691744476556778\n",
      "Training loss for batch 2418 : 0.017446368932724\n",
      "Training loss for batch 2419 : 0.009115612134337425\n",
      "Training loss for batch 2420 : 0.04850909486413002\n",
      "Training loss for batch 2421 : 0.030174072831869125\n",
      "Training loss for batch 2422 : 0.024070415645837784\n",
      "Training loss for batch 2423 : 0.1067313477396965\n",
      "Training loss for batch 2424 : 0.014567994512617588\n",
      "Training loss for batch 2425 : 0.07241516560316086\n",
      "Training loss for batch 2426 : 0.04269654303789139\n",
      "Training loss for batch 2427 : 0.015717191621661186\n",
      "Training loss for batch 2428 : 0.12146805226802826\n",
      "Training loss for batch 2429 : 0.08503247797489166\n",
      "Training loss for batch 2430 : 0.16517628729343414\n",
      "Training loss for batch 2431 : 0.12700465321540833\n",
      "Training loss for batch 2432 : 0.1109381765127182\n",
      "Training loss for batch 2433 : 0.018221860751509666\n",
      "Training loss for batch 2434 : 0.21553608775138855\n",
      "Training loss for batch 2435 : 0.07457384467124939\n",
      "Training loss for batch 2436 : 0.1153191328048706\n",
      "Training loss for batch 2437 : 0.06588231772184372\n",
      "Training loss for batch 2438 : 0.08541559427976608\n",
      "Training loss for batch 2439 : 0.12528096139431\n",
      "Training loss for batch 2440 : 0.04883449897170067\n",
      "Training loss for batch 2441 : 0.0821065753698349\n",
      "Training loss for batch 2442 : 0.10971509665250778\n",
      "Training loss for batch 2443 : 0.06489099562168121\n",
      "Training loss for batch 2444 : 0.058101899921894073\n",
      "Training loss for batch 2445 : 0.08833540230989456\n",
      "Training loss for batch 2446 : 0.17562539875507355\n",
      "Training loss for batch 2447 : 0.07929310202598572\n",
      "Training loss for batch 2448 : 0.08054060488939285\n",
      "Training loss for batch 2449 : 0.005111980717629194\n",
      "Training loss for batch 2450 : 0.10234609991312027\n",
      "Training loss for batch 2451 : 0.09730105102062225\n",
      "Training loss for batch 2452 : 0.051659468561410904\n",
      "Training loss for batch 2453 : 0.09158916771411896\n",
      "Training loss for batch 2454 : 0.09266750514507294\n",
      "Training loss for batch 2455 : 0.0700838714838028\n",
      "Training loss for batch 2456 : 0.09164166450500488\n",
      "Training loss for batch 2457 : 0.037986330687999725\n",
      "Training loss for batch 2458 : 0.09258490800857544\n",
      "Training loss for batch 2459 : 0.021266939118504524\n",
      "Training loss for batch 2460 : 0.11877072602510452\n",
      "Training loss for batch 2461 : 0.2579299509525299\n",
      "Training loss for batch 2462 : 0.12799431383609772\n",
      "Training loss for batch 2463 : 0.08509159088134766\n",
      "Training loss for batch 2464 : 0.01768660545349121\n",
      "Training loss for batch 2465 : 0.04820021614432335\n",
      "Training loss for batch 2466 : 0.17113856971263885\n",
      "Training loss for batch 2467 : 0.09530574828386307\n",
      "Training loss for batch 2468 : 0.13626882433891296\n",
      "Training loss for batch 2469 : 0.06174228712916374\n",
      "Training loss for batch 2470 : 0.05055918172001839\n",
      "Training loss for batch 2471 : 0.05236364156007767\n",
      "Training loss for batch 2472 : 0.05156010389328003\n",
      "Training loss for batch 2473 : 0.08317706733942032\n",
      "Training loss for batch 2474 : 0.03320722281932831\n",
      "Training loss for batch 2475 : 0.03732210397720337\n",
      "Training loss for batch 2476 : 0.06749292463064194\n",
      "Training loss for batch 2477 : 0.11436653882265091\n",
      "Training loss for batch 2478 : 0.1091032475233078\n",
      "Training loss for batch 2479 : 0.08979418873786926\n",
      "Training loss for batch 2480 : 0.04129887744784355\n",
      "Training loss for batch 2481 : 0.07070297747850418\n",
      "Training loss for batch 2482 : 0.06379418820142746\n",
      "Training loss for batch 2483 : 0.05361048877239227\n",
      "Training loss for batch 2484 : 0.08770694583654404\n",
      "Training loss for batch 2485 : 0.0017010520678013563\n",
      "Training loss for batch 2486 : 0.10797907412052155\n",
      "Training loss for batch 2487 : 0.08689822256565094\n",
      "Training loss for batch 2488 : 0.15062770247459412\n",
      "Training loss for batch 2489 : 0.05707421526312828\n",
      "Training loss for batch 2490 : 0.03883090242743492\n",
      "Training loss for batch 2491 : 0.03442426398396492\n",
      "Training loss for batch 2492 : 0.11732513457536697\n",
      "Training loss for batch 2493 : 0.011931311339139938\n",
      "Training loss for batch 2494 : 0.028466911986470222\n",
      "Training loss for batch 2495 : 0.059036463499069214\n",
      "Training loss for batch 2496 : 0.173800528049469\n",
      "Training loss for batch 2497 : 0.07955779880285263\n",
      "Training loss for batch 2498 : 0.007268879096955061\n",
      "Training loss for batch 2499 : 0.16847236454486847\n",
      "Training loss for batch 2500 : 0.027231810614466667\n",
      "Training loss for batch 2501 : 0.0837649255990982\n",
      "Training loss for batch 2502 : 0.11924866586923599\n",
      "Training loss for batch 2503 : 0.03757999464869499\n",
      "Training loss for batch 2504 : 0.052733104676008224\n",
      "Training loss for batch 2505 : 0.15163150429725647\n",
      "Training loss for batch 2506 : 0.1561824381351471\n",
      "Training loss for batch 2507 : 0.07764305174350739\n",
      "Training loss for batch 2508 : 0.05474015325307846\n",
      "Training loss for batch 2509 : 0.009516509249806404\n",
      "Training loss for batch 2510 : 0.13008493185043335\n",
      "Training loss for batch 2511 : 0.01773170195519924\n",
      "Training loss for batch 2512 : 0.08804456889629364\n",
      "Training loss for batch 2513 : 0.050586260855197906\n",
      "Training loss for batch 2514 : 0.02199111320078373\n",
      "Training loss for batch 2515 : 0.06638208031654358\n",
      "Training loss for batch 2516 : 0.015678131952881813\n",
      "Training loss for batch 2517 : 0.05498863384127617\n",
      "Training loss for batch 2518 : 0.05300046503543854\n",
      "Training loss for batch 2519 : 0.005721852649003267\n",
      "Training loss for batch 2520 : 0.06936200708150864\n",
      "Training loss for batch 2521 : 0.0419439859688282\n",
      "Training loss for batch 2522 : 0.09822366386651993\n",
      "Training loss for batch 2523 : 0.02699008211493492\n",
      "Training loss for batch 2524 : 0.030072523280978203\n",
      "Training loss for batch 2525 : 0.08009027689695358\n",
      "Training loss for batch 2526 : 0.08129237592220306\n",
      "Training loss for batch 2527 : 0.0\n",
      "Training loss for batch 2528 : 0.1194266676902771\n",
      "Training loss for batch 2529 : 0.10535363852977753\n",
      "Training loss for batch 2530 : 0.0314030684530735\n",
      "Training loss for batch 2531 : 0.18580475449562073\n",
      "Training loss for batch 2532 : 0.07558108866214752\n",
      "Training loss for batch 2533 : 0.023102423176169395\n",
      "Training loss for batch 2534 : 0.05803197622299194\n",
      "Training loss for batch 2535 : 0.004367918241769075\n",
      "Training loss for batch 2536 : 0.08546315878629684\n",
      "Training loss for batch 2537 : 0.1847858726978302\n",
      "Training loss for batch 2538 : 0.003921596799045801\n",
      "Training loss for batch 2539 : 0.07553680986166\n",
      "Training loss for batch 2540 : 0.1910925656557083\n",
      "Training loss for batch 2541 : 0.0354391410946846\n",
      "Training loss for batch 2542 : 0.07552795857191086\n",
      "Training loss for batch 2543 : 0.0007267586188390851\n",
      "Training loss for batch 2544 : 0.03307545930147171\n",
      "Training loss for batch 2545 : 0.008092252537608147\n",
      "Training loss for batch 2546 : 0.17364762723445892\n",
      "Training loss for batch 2547 : 0.03199417144060135\n",
      "Training loss for batch 2548 : 0.1295533925294876\n",
      "Training loss for batch 2549 : 0.08114813268184662\n",
      "Training loss for batch 2550 : 0.013687940314412117\n",
      "Training loss for batch 2551 : 0.010683510452508926\n",
      "Training loss for batch 2552 : 0.013577726669609547\n",
      "Training loss for batch 2553 : 0.09266936033964157\n",
      "Training loss for batch 2554 : 0.04080883786082268\n",
      "Training loss for batch 2555 : 0.12192825227975845\n",
      "Training loss for batch 2556 : 0.06116223707795143\n",
      "Training loss for batch 2557 : 0.013729230500757694\n",
      "Training loss for batch 2558 : 0.05432402715086937\n",
      "Training loss for batch 2559 : 0.08030262589454651\n",
      "Training loss for batch 2560 : 0.052597735077142715\n",
      "Training loss for batch 2561 : 0.030466897413134575\n",
      "Training loss for batch 2562 : 0.16771407425403595\n",
      "Training loss for batch 2563 : 0.074071504175663\n",
      "Training loss for batch 2564 : 0.0306063424795866\n",
      "Training loss for batch 2565 : 0.1257079392671585\n",
      "Training loss for batch 2566 : 0.07387564331293106\n",
      "Training loss for batch 2567 : 0.030048200860619545\n",
      "Training loss for batch 2568 : 0.00659535638988018\n",
      "Training loss for batch 2569 : 0.029840167611837387\n",
      "Training loss for batch 2570 : 0.0518815703690052\n",
      "Training loss for batch 2571 : 0.0792793482542038\n",
      "Training loss for batch 2572 : 0.06810930371284485\n",
      "Training loss for batch 2573 : 0.12971052527427673\n",
      "Training loss for batch 2574 : 0.08033162355422974\n",
      "Training loss for batch 2575 : 0.11787810176610947\n",
      "Training loss for batch 2576 : 0.07914351671934128\n",
      "Training loss for batch 2577 : 0.11616318672895432\n",
      "Training loss for batch 2578 : 0.010254104621708393\n",
      "Training loss for batch 2579 : 0.06190644949674606\n",
      "Training loss for batch 2580 : 0.008216816000640392\n",
      "Training loss for batch 2581 : 0.05242040753364563\n",
      "Training loss for batch 2582 : 0.02999185584485531\n",
      "Training loss for batch 2583 : 0.01847507804632187\n",
      "Training loss for batch 2584 : 0.0456986241042614\n",
      "Training loss for batch 2585 : 0.027968542650341988\n",
      "Training loss for batch 2586 : 0.059662893414497375\n",
      "Training loss for batch 2587 : 0.22355671226978302\n",
      "Training loss for batch 2588 : 0.20604047179222107\n",
      "Training loss for batch 2589 : 0.04765642061829567\n",
      "Training loss for batch 2590 : 0.03549282252788544\n",
      "Training loss for batch 2591 : 0.0633026510477066\n",
      "Training loss for batch 2592 : 0.1070173978805542\n",
      "Training loss for batch 2593 : 0.007940898649394512\n",
      "Training loss for batch 2594 : 0.09782551974058151\n",
      "Training loss for batch 2595 : 0.03293482959270477\n",
      "Training loss for batch 2596 : 0.04787317290902138\n",
      "Training loss for batch 2597 : 0.03927503526210785\n",
      "Training loss for batch 2598 : 0.04114382341504097\n",
      "Training loss for batch 2599 : 0.05069630965590477\n",
      "Training loss for batch 2600 : 0.1574924737215042\n",
      "Training loss for batch 2601 : 0.17201948165893555\n",
      "Training loss for batch 2602 : 0.003023462602868676\n",
      "Training loss for batch 2603 : 0.01464965008199215\n",
      "Training loss for batch 2604 : 0.03910335898399353\n",
      "Training loss for batch 2605 : 0.24229370057582855\n",
      "Training loss for batch 2606 : 0.03694221004843712\n",
      "Training loss for batch 2607 : 0.18280494213104248\n",
      "Training loss for batch 2608 : 0.01607975922524929\n",
      "Training loss for batch 2609 : 0.014286274090409279\n",
      "Training loss for batch 2610 : 0.023324890062212944\n",
      "Training loss for batch 2611 : 0.052342429757118225\n",
      "Training loss for batch 2612 : 0.01685846969485283\n",
      "Training loss for batch 2613 : 0.07271528244018555\n",
      "Training loss for batch 2614 : 0.06203680858016014\n",
      "Training loss for batch 2615 : 0.10839316993951797\n",
      "Training loss for batch 2616 : 0.0743328109383583\n",
      "Training loss for batch 2617 : 0.031751710921525955\n",
      "Training loss for batch 2618 : 0.08014950156211853\n",
      "Training loss for batch 2619 : 0.2453790158033371\n",
      "Training loss for batch 2620 : 0.12992173433303833\n",
      "Training loss for batch 2621 : 0.1840958148241043\n",
      "Training loss for batch 2622 : 0.11308801919221878\n",
      "Training loss for batch 2623 : 0.058105360716581345\n",
      "Training loss for batch 2624 : 0.096518374979496\n",
      "Training loss for batch 2625 : 0.09069234132766724\n",
      "Training loss for batch 2626 : 0.06540825217962265\n",
      "Training loss for batch 2627 : 0.08192423731088638\n",
      "Training loss for batch 2628 : 0.0461113378405571\n",
      "Training loss for batch 2629 : 0.11961757391691208\n",
      "Training loss for batch 2630 : 0.05567272752523422\n",
      "Training loss for batch 2631 : 0.027130858972668648\n",
      "Training loss for batch 2632 : 0.0629546195268631\n",
      "Training loss for batch 2633 : 0.022539539262652397\n",
      "Training loss for batch 2634 : 0.07112815231084824\n",
      "Training loss for batch 2635 : 0.005401622969657183\n",
      "Training loss for batch 2636 : 0.026548992842435837\n",
      "Training loss for batch 2637 : 0.024226674810051918\n",
      "Training loss for batch 2638 : 0.1467345505952835\n",
      "Training loss for batch 2639 : 0.09137299656867981\n",
      "Training loss for batch 2640 : 0.11971276253461838\n",
      "Training loss for batch 2641 : 0.10067184269428253\n",
      "Training loss for batch 2642 : 0.06271827965974808\n",
      "Training loss for batch 2643 : 0.13246746361255646\n",
      "Training loss for batch 2644 : 0.04958103597164154\n",
      "Training loss for batch 2645 : 0.007456491701304913\n",
      "Training loss for batch 2646 : 0.17318770289421082\n",
      "Training loss for batch 2647 : 0.09447000920772552\n",
      "Training loss for batch 2648 : 0.01698380522429943\n",
      "Training loss for batch 2649 : 0.03302760422229767\n",
      "Training loss for batch 2650 : 0.022804774343967438\n",
      "Training loss for batch 2651 : 0.1490120142698288\n",
      "Training loss for batch 2652 : 0.02271050214767456\n",
      "Training loss for batch 2653 : 0.04768144339323044\n",
      "Training loss for batch 2654 : 0.030163748189806938\n",
      "Training loss for batch 2655 : 0.20581328868865967\n",
      "Training loss for batch 2656 : 0.11022185534238815\n",
      "Training loss for batch 2657 : 0.0894830971956253\n",
      "Training loss for batch 2658 : 0.042182620614767075\n",
      "Training loss for batch 2659 : 0.02351164072751999\n",
      "Training loss for batch 2660 : 0.05036510154604912\n",
      "Training loss for batch 2661 : 0.10128207504749298\n",
      "Training loss for batch 2662 : 0.04206158593297005\n",
      "Training loss for batch 2663 : 0.11752763390541077\n",
      "Training loss for batch 2664 : 0.17390099167823792\n",
      "Training loss for batch 2665 : 0.10154569894075394\n",
      "Training loss for batch 2666 : 0.09375965595245361\n",
      "Training loss for batch 2667 : 0.07796455174684525\n",
      "Training loss for batch 2668 : 0.009485243819653988\n",
      "Training loss for batch 2669 : 0.05152738839387894\n",
      "Training loss for batch 2670 : 1.8181249572535307e-07\n",
      "Training loss for batch 2671 : 0.07327631860971451\n",
      "Training loss for batch 2672 : 0.029685594141483307\n",
      "Training loss for batch 2673 : 0.06162150949239731\n",
      "Training loss for batch 2674 : 0.059086959809064865\n",
      "Training loss for batch 2675 : 0.11431826651096344\n",
      "Training loss for batch 2676 : 0.05934542417526245\n",
      "Training loss for batch 2677 : 0.02340226247906685\n",
      "Training loss for batch 2678 : 0.04045563191175461\n",
      "Training loss for batch 2679 : 0.004910002462565899\n",
      "Training loss for batch 2680 : 0.08683396130800247\n",
      "Training loss for batch 2681 : 0.14296911656856537\n",
      "Training loss for batch 2682 : 0.008050724864006042\n",
      "Training loss for batch 2683 : 0.01875501684844494\n",
      "Training loss for batch 2684 : 0.0066238208673894405\n",
      "Training loss for batch 2685 : 0.020878693088889122\n",
      "Training loss for batch 2686 : 0.02037416398525238\n",
      "Training loss for batch 2687 : 0.18267659842967987\n",
      "Training loss for batch 2688 : 0.23554353415966034\n",
      "Training loss for batch 2689 : 0.16378352046012878\n",
      "Training loss for batch 2690 : 0.1345479041337967\n",
      "Training loss for batch 2691 : 0.1301087737083435\n",
      "Training loss for batch 2692 : 0.16092126071453094\n",
      "Training loss for batch 2693 : 0.2043173611164093\n",
      "Training loss for batch 2694 : 0.11646440625190735\n",
      "Training loss for batch 2695 : 0.020979121327400208\n",
      "Training loss for batch 2696 : 0.05221788212656975\n",
      "Training loss for batch 2697 : 0.08796011656522751\n",
      "Training loss for batch 2698 : 1.3539329302147962e-07\n",
      "Training loss for batch 2699 : 0.05151001736521721\n",
      "Training loss for batch 2700 : 0.007748294621706009\n",
      "Training loss for batch 2701 : 0.07441847771406174\n",
      "Training loss for batch 2702 : 0.014980924315750599\n",
      "Training loss for batch 2703 : 0.012268220074474812\n",
      "Training loss for batch 2704 : 0.10527162998914719\n",
      "Training loss for batch 2705 : 0.1744784563779831\n",
      "Training loss for batch 2706 : 0.12366915494203568\n",
      "Training loss for batch 2707 : 0.01996496506035328\n",
      "Training loss for batch 2708 : 0.07290347665548325\n",
      "Training loss for batch 2709 : 0.1426759958267212\n",
      "Training loss for batch 2710 : 0.037662748247385025\n",
      "Training loss for batch 2711 : 0.05756671354174614\n",
      "Training loss for batch 2712 : 0.052573610097169876\n",
      "Training loss for batch 2713 : 0.09580397605895996\n",
      "Training loss for batch 2714 : 0.0875602588057518\n",
      "Training loss for batch 2715 : 0.11558165401220322\n",
      "Training loss for batch 2716 : 0.05751476436853409\n",
      "Training loss for batch 2717 : 0.06626882404088974\n",
      "Training loss for batch 2718 : 0.12315120548009872\n",
      "Training loss for batch 2719 : 0.06661473214626312\n",
      "Training loss for batch 2720 : 0.14709173142910004\n",
      "Training loss for batch 2721 : 0.06243843585252762\n",
      "Training loss for batch 2722 : 0.17598497867584229\n",
      "Training loss for batch 2723 : 0.09863089770078659\n",
      "Training loss for batch 2724 : 0.1343987137079239\n",
      "Training loss for batch 2725 : 0.03772348165512085\n",
      "Training loss for batch 2726 : 0.0032057813368737698\n",
      "Training loss for batch 2727 : 0.1289243996143341\n",
      "Training loss for batch 2728 : 0.11467257142066956\n",
      "Training loss for batch 2729 : 0.10515834391117096\n",
      "Training loss for batch 2730 : 0.09471376240253448\n",
      "Training loss for batch 2731 : 0.03476373851299286\n",
      "Training loss for batch 2732 : 0.016085505485534668\n",
      "Training loss for batch 2733 : 0.06421483308076859\n",
      "Training loss for batch 2734 : 0.000741253315936774\n",
      "Training loss for batch 2735 : 0.04644336178898811\n",
      "Training loss for batch 2736 : 0.04490974545478821\n",
      "Training loss for batch 2737 : 0.15335069596767426\n",
      "Training loss for batch 2738 : 0.06677364557981491\n",
      "Training loss for batch 2739 : 0.018797114491462708\n",
      "Training loss for batch 2740 : 0.21567413210868835\n",
      "Training loss for batch 2741 : 0.025044739246368408\n",
      "Training loss for batch 2742 : 0.04511512070894241\n",
      "Training loss for batch 2743 : 0.18810176849365234\n",
      "Training loss for batch 2744 : 0.08602090924978256\n",
      "Training loss for batch 2745 : 0.052277542650699615\n",
      "Training loss for batch 2746 : 0.05455853417515755\n",
      "Training loss for batch 2747 : 0.09634402394294739\n",
      "Training loss for batch 2748 : 0.11373832821846008\n",
      "Training loss for batch 2749 : 0.14054268598556519\n",
      "Training loss for batch 2750 : 0.17981578409671783\n",
      "Training loss for batch 2751 : 0.06476202607154846\n",
      "Training loss for batch 2752 : 0.06232292205095291\n",
      "Training loss for batch 2753 : 0.03728339076042175\n",
      "Training loss for batch 2754 : 0.1811586320400238\n",
      "Training loss for batch 2755 : 0.08652446419000626\n",
      "Training loss for batch 2756 : 0.06326902657747269\n",
      "Training loss for batch 2757 : 0.10484857112169266\n",
      "Training loss for batch 2758 : 0.08083649724721909\n",
      "Training loss for batch 2759 : 0.07586729526519775\n",
      "Training loss for batch 2760 : 0.1006029024720192\n",
      "Training loss for batch 2761 : 0.058239832520484924\n",
      "Training loss for batch 2762 : 0.004617225378751755\n",
      "Training loss for batch 2763 : 0.1456601619720459\n",
      "Training loss for batch 2764 : 0.10184900462627411\n",
      "Training loss for batch 2765 : 0.05400180071592331\n",
      "Training loss for batch 2766 : 0.00965473335236311\n",
      "Training loss for batch 2767 : 0.023539789021015167\n",
      "Training loss for batch 2768 : 0.1449032723903656\n",
      "Training loss for batch 2769 : 0.05329902842640877\n",
      "Training loss for batch 2770 : 0.1226452887058258\n",
      "Training loss for batch 2771 : 0.10798922181129456\n",
      "Training loss for batch 2772 : 0.07376980036497116\n",
      "Training loss for batch 2773 : 0.10345782339572906\n",
      "Training loss for batch 2774 : 0.06342881917953491\n",
      "Training loss for batch 2775 : 0.028489012271165848\n",
      "Training loss for batch 2776 : 0.02581857144832611\n",
      "Training loss for batch 2777 : 0.027146607637405396\n",
      "Training loss for batch 2778 : 0.15279258787631989\n",
      "Training loss for batch 2779 : 0.1868559867143631\n",
      "Training loss for batch 2780 : 0.10573654621839523\n",
      "Training loss for batch 2781 : 0.01915867067873478\n",
      "Training loss for batch 2782 : 0.07493593543767929\n",
      "Training loss for batch 2783 : 0.2188776135444641\n",
      "Training loss for batch 2784 : 0.1455477476119995\n",
      "Training loss for batch 2785 : 0.0596928745508194\n",
      "Training loss for batch 2786 : 0.07173347473144531\n",
      "Training loss for batch 2787 : 0.23477469384670258\n",
      "Training loss for batch 2788 : 0.025956112891435623\n",
      "Training loss for batch 2789 : 0.02384008653461933\n",
      "Training loss for batch 2790 : 0.09520196914672852\n",
      "Training loss for batch 2791 : 0.09805232286453247\n",
      "Training loss for batch 2792 : 0.09371301531791687\n",
      "Training loss for batch 2793 : 0.04395322874188423\n",
      "Training loss for batch 2794 : 0.14138337969779968\n",
      "Training loss for batch 2795 : 0.10921370983123779\n",
      "Training loss for batch 2796 : 0.042511776089668274\n",
      "Training loss for batch 2797 : 0.02273317612707615\n",
      "Training loss for batch 2798 : 0.0853327214717865\n",
      "Training loss for batch 2799 : 0.05932828038930893\n",
      "Training loss for batch 2800 : 0.14153188467025757\n",
      "Training loss for batch 2801 : 0.07127444446086884\n",
      "Training loss for batch 2802 : 0.04661039263010025\n",
      "Training loss for batch 2803 : 0.0930778831243515\n",
      "Training loss for batch 2804 : 0.07031802088022232\n",
      "Training loss for batch 2805 : 0.009213329292833805\n",
      "Training loss for batch 2806 : 0.10620693117380142\n",
      "Training loss for batch 2807 : 0.05018076300621033\n",
      "Training loss for batch 2808 : 0.0626649409532547\n",
      "Training loss for batch 2809 : 0.06829815357923508\n",
      "Training loss for batch 2810 : 0.05473741516470909\n",
      "Training loss for batch 2811 : 0.10504021495580673\n",
      "Training loss for batch 2812 : 0.010190216824412346\n",
      "Training loss for batch 2813 : 0.020285049453377724\n",
      "Training loss for batch 2814 : 0.1076740249991417\n",
      "Training loss for batch 2815 : 0.11583700776100159\n",
      "Training loss for batch 2816 : 0.08523081988096237\n",
      "Training loss for batch 2817 : 0.1268574297428131\n",
      "Training loss for batch 2818 : 0.03268471360206604\n",
      "Training loss for batch 2819 : 0.01562720164656639\n",
      "Training loss for batch 2820 : 0.06751854717731476\n",
      "Training loss for batch 2821 : 0.12542134523391724\n",
      "Training loss for batch 2822 : 0.15807144343852997\n",
      "Training loss for batch 2823 : 0.10724323242902756\n",
      "Training loss for batch 2824 : 0.08222835510969162\n",
      "Training loss for batch 2825 : 0.14381244778633118\n",
      "Training loss for batch 2826 : 0.19858694076538086\n",
      "Training loss for batch 2827 : 0.1333344727754593\n",
      "Training loss for batch 2828 : 0.07462523877620697\n",
      "Training loss for batch 2829 : 0.14364801347255707\n",
      "Training loss for batch 2830 : 0.02569900080561638\n",
      "Training loss for batch 2831 : 0.11356603354215622\n",
      "Training loss for batch 2832 : 0.03848978504538536\n",
      "Training loss for batch 2833 : 0.09565509855747223\n",
      "Training loss for batch 2834 : 0.03990752622485161\n",
      "Training loss for batch 2835 : 0.05131543055176735\n",
      "Training loss for batch 2836 : 0.08650349825620651\n",
      "Training loss for batch 2837 : 0.10700386762619019\n",
      "Training loss for batch 2838 : 0.09327299892902374\n",
      "Training loss for batch 2839 : 0.027264108881354332\n",
      "Training loss for batch 2840 : 0.12695854902267456\n",
      "Training loss for batch 2841 : 0.1312207728624344\n",
      "Training loss for batch 2842 : 0.14053963124752045\n",
      "Training loss for batch 2843 : 0.04125354811549187\n",
      "Training loss for batch 2844 : 0.01243987213820219\n",
      "Training loss for batch 2845 : 0.09324871003627777\n",
      "Training loss for batch 2846 : 0.049333199858665466\n",
      "Training loss for batch 2847 : 0.009921824559569359\n",
      "Training loss for batch 2848 : 0.06007538363337517\n",
      "Training loss for batch 2849 : 0.16330425441265106\n",
      "Training loss for batch 2850 : 0.08523955196142197\n",
      "Training loss for batch 2851 : 0.036614518612623215\n",
      "Training loss for batch 2852 : 0.10698447376489639\n",
      "Training loss for batch 2853 : 0.06152857467532158\n",
      "Training loss for batch 2854 : 0.034586627036333084\n",
      "Training loss for batch 2855 : 0.09104087948799133\n",
      "Training loss for batch 2856 : 0.05682963877916336\n",
      "Training loss for batch 2857 : 0.11920931935310364\n",
      "Training loss for batch 2858 : 0.13776400685310364\n",
      "Training loss for batch 2859 : 0.067794069647789\n",
      "Training loss for batch 2860 : 0.05997175723314285\n",
      "Training loss for batch 2861 : 0.05796550214290619\n",
      "Training loss for batch 2862 : 0.14892716705799103\n",
      "Training loss for batch 2863 : 0.10596896708011627\n",
      "Training loss for batch 2864 : 0.18868963420391083\n",
      "Training loss for batch 2865 : 0.08659742772579193\n",
      "Training loss for batch 2866 : 0.09885556995868683\n",
      "Training loss for batch 2867 : 0.09289868921041489\n",
      "Training loss for batch 2868 : 0.08752337843179703\n",
      "Training loss for batch 2869 : 0.17080208659172058\n",
      "Training loss for batch 2870 : 0.07266218960285187\n",
      "Training loss for batch 2871 : 0.1052423045039177\n",
      "Training loss for batch 2872 : 0.1025237888097763\n",
      "Training loss for batch 2873 : 0.004787299782037735\n",
      "Training loss for batch 2874 : 0.024254562333226204\n",
      "Training loss for batch 2875 : 0.12201453745365143\n",
      "Training loss for batch 2876 : 0.003947488963603973\n",
      "Training loss for batch 2877 : 0.07679978013038635\n",
      "Training loss for batch 2878 : 0.033713072538375854\n",
      "Training loss for batch 2879 : 0.17448021471500397\n",
      "Training loss for batch 2880 : 0.13780736923217773\n",
      "Training loss for batch 2881 : 0.03953717276453972\n",
      "Training loss for batch 2882 : 0.10909885168075562\n",
      "Training loss for batch 2883 : 0.2043093740940094\n",
      "Training loss for batch 2884 : 0.000438802846474573\n",
      "Training loss for batch 2885 : 0.11981219053268433\n",
      "Training loss for batch 2886 : 0.19158510863780975\n",
      "Training loss for batch 2887 : 0.06550783663988113\n",
      "Training loss for batch 2888 : 0.1821993589401245\n",
      "Training loss for batch 2889 : 0.057606130838394165\n",
      "Training loss for batch 2890 : 0.0869702473282814\n",
      "Training loss for batch 2891 : 0.021674219518899918\n",
      "Training loss for batch 2892 : 0.08736097812652588\n",
      "Training loss for batch 2893 : 0.022894250229001045\n",
      "Training loss for batch 2894 : 0.2014726996421814\n",
      "Training loss for batch 2895 : 0.03937149420380592\n",
      "Training loss for batch 2896 : 0.021282171830534935\n",
      "Training loss for batch 2897 : 0.035867996513843536\n",
      "Training loss for batch 2898 : 0.047237690538167953\n",
      "Training loss for batch 2899 : 0.07586678862571716\n",
      "Training loss for batch 2900 : 0.0420689694583416\n",
      "Training loss for batch 2901 : 0.03508005291223526\n",
      "Training loss for batch 2902 : 0.05376721918582916\n",
      "Training loss for batch 2903 : 0.03003060817718506\n",
      "Training loss for batch 2904 : 0.17687660455703735\n",
      "Training loss for batch 2905 : 0.06450691819190979\n",
      "Training loss for batch 2906 : 0.06578393280506134\n",
      "Training loss for batch 2907 : 0.0374566912651062\n",
      "Training loss for batch 2908 : 0.08524203300476074\n",
      "Training loss for batch 2909 : 0.11726944148540497\n",
      "Training loss for batch 2910 : 0.09551504254341125\n",
      "Training loss for batch 2911 : 0.05283546820282936\n",
      "Training loss for batch 2912 : 0.0436176173388958\n",
      "Training loss for batch 2913 : 0.03667544946074486\n",
      "Training loss for batch 2914 : 0.11360927671194077\n",
      "Training loss for batch 2915 : 0.19226500391960144\n",
      "Training loss for batch 2916 : 0.13811856508255005\n",
      "Training loss for batch 2917 : 0.0059348405338823795\n",
      "Training loss for batch 2918 : 0.07451199740171432\n",
      "Training loss for batch 2919 : 0.16056273877620697\n",
      "Training loss for batch 2920 : 0.024438034743070602\n",
      "Training loss for batch 2921 : 0.15151679515838623\n",
      "Training loss for batch 2922 : 0.14074265956878662\n",
      "Training loss for batch 2923 : 0.07555600255727768\n",
      "Training loss for batch 2924 : 0.10239298641681671\n",
      "Training loss for batch 2925 : 0.08877426385879517\n",
      "Training loss for batch 2926 : 0.09120075404644012\n",
      "Training loss for batch 2927 : 0.15911628305912018\n",
      "Training loss for batch 2928 : 0.045864854007959366\n",
      "Training loss for batch 2929 : 0.022234564647078514\n",
      "Training loss for batch 2930 : 0.04263156279921532\n",
      "Training loss for batch 2931 : 0.08620191365480423\n",
      "Training loss for batch 2932 : 0.056162502616643906\n",
      "Training loss for batch 2933 : 0.058263156563043594\n",
      "Training loss for batch 2934 : 0.00649515213444829\n",
      "Training loss for batch 2935 : 0.10863696783781052\n",
      "Training loss for batch 2936 : 0.09587856382131577\n",
      "Training loss for batch 2937 : 0.13969099521636963\n",
      "Training loss for batch 2938 : 0.06251460313796997\n",
      "Training loss for batch 2939 : 0.006778432987630367\n",
      "Training loss for batch 2940 : 0.06330835819244385\n",
      "Training loss for batch 2941 : 0.07998882234096527\n",
      "Training loss for batch 2942 : 0.017035827040672302\n",
      "Training loss for batch 2943 : 0.08866941928863525\n",
      "Training loss for batch 2944 : 0.05909397080540657\n",
      "Training loss for batch 2945 : 0.07423828542232513\n",
      "Training loss for batch 2946 : 0.057685736566782\n",
      "Training loss for batch 2947 : 0.03959043323993683\n",
      "Training loss for batch 2948 : 0.1345188170671463\n",
      "Training loss for batch 2949 : 0.08176835626363754\n",
      "Training loss for batch 2950 : 0.12677232921123505\n",
      "Training loss for batch 2951 : 0.009995214641094208\n",
      "Training loss for batch 2952 : 0.04166962951421738\n",
      "Training loss for batch 2953 : 0.07803218811750412\n",
      "Training loss for batch 2954 : 0.0575687438249588\n",
      "Training loss for batch 2955 : 0.1645847111940384\n",
      "Training loss for batch 2956 : 0.017743106931447983\n",
      "Training loss for batch 2957 : 0.09978507459163666\n",
      "Training loss for batch 2958 : 0.12142519652843475\n",
      "Training loss for batch 2959 : 0.1582765430212021\n",
      "Training loss for batch 2960 : 0.06544478982686996\n",
      "Training loss for batch 2961 : 0.05250730365514755\n",
      "Training loss for batch 2962 : 0.07694811373949051\n",
      "Training loss for batch 2963 : 0.0022723281290382147\n",
      "Training loss for batch 2964 : 0.04526054114103317\n",
      "Training loss for batch 2965 : 0.10883882641792297\n",
      "Training loss for batch 2966 : 0.12927299737930298\n",
      "Training loss for batch 2967 : 0.09925179183483124\n",
      "Training loss for batch 2968 : 0.0849328562617302\n",
      "Training loss for batch 2969 : 0.11406932771205902\n",
      "Training loss for batch 2970 : 0.03710537776350975\n",
      "Training loss for batch 2971 : 0.1020529493689537\n",
      "Training loss for batch 2972 : 0.08032303303480148\n",
      "Training loss for batch 2973 : 0.1045946404337883\n",
      "Training loss for batch 2974 : 0.16221915185451508\n",
      "Training loss for batch 2975 : 0.2161756008863449\n",
      "Training loss for batch 2976 : 0.06829892843961716\n",
      "Training loss for batch 2977 : 0.11873403191566467\n",
      "Training loss for batch 2978 : 0.019839977845549583\n",
      "Training loss for batch 2979 : 0.07392667233943939\n",
      "Training loss for batch 2980 : 0.07265163958072662\n",
      "Training loss for batch 2981 : 0.01941659487783909\n",
      "Training loss for batch 2982 : 0.05995385721325874\n",
      "Training loss for batch 2983 : 0.07209822535514832\n",
      "Training loss for batch 2984 : 0.08478101342916489\n",
      "Training loss for batch 2985 : 0.023188261315226555\n",
      "Training loss for batch 2986 : 0.06495500355958939\n",
      "Training loss for batch 2987 : 0.008360966108739376\n",
      "Training loss for batch 2988 : 0.05473146587610245\n",
      "Training loss for batch 2989 : 0.012221308425068855\n",
      "Training loss for batch 2990 : 0.17169253528118134\n",
      "Training loss for batch 2991 : 0.16329246759414673\n",
      "Training loss for batch 2992 : 0.020347023382782936\n",
      "Training loss for batch 2993 : 0.039474036544561386\n",
      "Training loss for batch 2994 : 0.19072268903255463\n",
      "Training loss for batch 2995 : 0.06393913179636002\n",
      "Training loss for batch 2996 : 0.053350552916526794\n",
      "Training loss for batch 2997 : 0.04851666837930679\n",
      "Training loss for batch 2998 : 0.027902310714125633\n",
      "Training loss for batch 2999 : 0.12511584162712097\n",
      "Training loss for batch 3000 : 0.07421999424695969\n",
      "Training loss for batch 3001 : 0.0\n",
      "Training loss for batch 3002 : 0.06442668288946152\n",
      "Training loss for batch 3003 : 0.07863486558198929\n",
      "Training loss for batch 3004 : 0.07533577084541321\n",
      "Training loss for batch 3005 : 0.0547776073217392\n",
      "Training loss for batch 3006 : 0.04185343533754349\n",
      "Training loss for batch 3007 : 0.1474565714597702\n",
      "Training loss for batch 3008 : 0.10100607573986053\n",
      "Training loss for batch 3009 : 0.11331792175769806\n",
      "Training loss for batch 3010 : 0.02770749107003212\n",
      "Training loss for batch 3011 : 0.09610269218683243\n",
      "Training loss for batch 3012 : 0.09468282014131546\n",
      "Training loss for batch 3013 : 0.08664687722921371\n",
      "Training loss for batch 3014 : 0.010659239254891872\n",
      "Training loss for batch 3015 : 0.011395515874028206\n",
      "Training loss for batch 3016 : 0.06058303639292717\n",
      "Training loss for batch 3017 : 0.11651485413312912\n",
      "Training loss for batch 3018 : 0.04700302705168724\n",
      "Training loss for batch 3019 : 0.17477047443389893\n",
      "Training loss for batch 3020 : 0.06500467658042908\n",
      "Training loss for batch 3021 : 0.06341800093650818\n",
      "Training loss for batch 3022 : 0.04094696417450905\n",
      "Training loss for batch 3023 : 0.1600942611694336\n",
      "Training loss for batch 3024 : 0.1928565502166748\n",
      "Training loss for batch 3025 : 0.0545547716319561\n",
      "Training loss for batch 3026 : 0.02495591901242733\n",
      "Training loss for batch 3027 : 0.04131194204092026\n",
      "Training loss for batch 3028 : 0.10160514712333679\n",
      "Training loss for batch 3029 : 0.11941776424646378\n",
      "Training loss for batch 3030 : 0.11831970512866974\n",
      "Training loss for batch 3031 : 0.0379057377576828\n",
      "Training loss for batch 3032 : 0.12088590860366821\n",
      "Training loss for batch 3033 : 0.1848219782114029\n",
      "Training loss for batch 3034 : 0.10187957435846329\n",
      "Training loss for batch 3035 : 0.07104082405567169\n",
      "Training loss for batch 3036 : 0.2165514975786209\n",
      "Training loss for batch 3037 : 0.016675133258104324\n",
      "Training loss for batch 3038 : 0.22362913191318512\n",
      "Training loss for batch 3039 : 0.0017154370434582233\n",
      "Training loss for batch 3040 : 0.04535341262817383\n",
      "Training loss for batch 3041 : 0.3716872036457062\n",
      "Training loss for batch 3042 : 0.05287232995033264\n",
      "Training loss for batch 3043 : 0.03908633440732956\n",
      "Training loss for batch 3044 : 0.04796038195490837\n",
      "Training loss for batch 3045 : 0.06856104731559753\n",
      "Training loss for batch 3046 : 0.16681085526943207\n",
      "Training loss for batch 3047 : 0.18222904205322266\n",
      "Training loss for batch 3048 : 0.1575038731098175\n",
      "Training loss for batch 3049 : 0.037595611065626144\n",
      "Training loss for batch 3050 : 0.06209108978509903\n",
      "Training loss for batch 3051 : 0.07017356902360916\n",
      "Training loss for batch 3052 : 0.05239976570010185\n",
      "Training loss for batch 3053 : 0.05422893911600113\n",
      "Training loss for batch 3054 : 0.07430655509233475\n",
      "Training loss for batch 3055 : 0.06116776913404465\n",
      "Training loss for batch 3056 : 0.0267378780990839\n",
      "Training loss for batch 3057 : 0.05161339044570923\n",
      "Training loss for batch 3058 : 0.03625636175274849\n",
      "Training loss for batch 3059 : 0.04071653261780739\n",
      "Training loss for batch 3060 : 0.06137941777706146\n",
      "Training loss for batch 3061 : 0.08662654459476471\n",
      "Training loss for batch 3062 : 0.08942347019910812\n",
      "Training loss for batch 3063 : 0.05906493961811066\n",
      "Training loss for batch 3064 : 0.045020025223493576\n",
      "Training loss for batch 3065 : 0.05994386598467827\n",
      "Training loss for batch 3066 : 0.03795527294278145\n",
      "Training loss for batch 3067 : 0.03373768553137779\n",
      "Training loss for batch 3068 : 0.11886170506477356\n",
      "Training loss for batch 3069 : 0.004867380019277334\n",
      "Training loss for batch 3070 : 0.10502441227436066\n",
      "Training loss for batch 3071 : 0.03597821667790413\n",
      "Training loss for batch 3072 : 0.0\n",
      "Training loss for batch 3073 : 0.05555843561887741\n",
      "Training loss for batch 3074 : 0.12080398947000504\n",
      "Training loss for batch 3075 : 0.1302482634782791\n",
      "Training loss for batch 3076 : 0.07930608838796616\n",
      "Training loss for batch 3077 : 0.023581840097904205\n",
      "Training loss for batch 3078 : 0.09445010870695114\n",
      "Training loss for batch 3079 : 0.22845102846622467\n",
      "Training loss for batch 3080 : 0.15843148529529572\n",
      "Training loss for batch 3081 : 0.18159553408622742\n",
      "Training loss for batch 3082 : 0.029404984787106514\n",
      "Training loss for batch 3083 : 0.19192171096801758\n",
      "Training loss for batch 3084 : 0.014163769781589508\n",
      "Training loss for batch 3085 : 0.08509978652000427\n",
      "Training loss for batch 3086 : 0.06308647245168686\n",
      "Training loss for batch 3087 : 0.116596519947052\n",
      "Training loss for batch 3088 : 0.1661343276500702\n",
      "Training loss for batch 3089 : 0.06610928475856781\n",
      "Training loss for batch 3090 : 0.06383470445871353\n",
      "Training loss for batch 3091 : 0.05469343811273575\n",
      "Training loss for batch 3092 : 0.11850093305110931\n",
      "Training loss for batch 3093 : 0.006886232178658247\n",
      "Training loss for batch 3094 : 0.0027905753813683987\n",
      "Training loss for batch 3095 : 0.06221484765410423\n",
      "Training loss for batch 3096 : 0.08607570827007294\n",
      "Training loss for batch 3097 : 0.047048285603523254\n",
      "Training loss for batch 3098 : 0.04261646047234535\n",
      "Training loss for batch 3099 : 0.08014842867851257\n",
      "Training loss for batch 3100 : 0.1369045227766037\n",
      "Training loss for batch 3101 : 0.2834337055683136\n",
      "Training loss for batch 3102 : 0.22779099643230438\n",
      "Training loss for batch 3103 : 0.10269645601511002\n",
      "Training loss for batch 3104 : 0.1910158395767212\n",
      "Training loss for batch 3105 : 0.064906045794487\n",
      "Training loss for batch 3106 : 0.2188897579908371\n",
      "Training loss for batch 3107 : 0.07411905378103256\n",
      "Training loss for batch 3108 : 0.10684938728809357\n",
      "Training loss for batch 3109 : 0.08818517625331879\n",
      "Training loss for batch 3110 : 0.045817699283361435\n",
      "Training loss for batch 3111 : 0.2126840353012085\n",
      "Training loss for batch 3112 : 0.06098264083266258\n",
      "Training loss for batch 3113 : 0.08376874029636383\n",
      "Training loss for batch 3114 : 0.03891027718782425\n",
      "Training loss for batch 3115 : 0.03886415809392929\n",
      "Training loss for batch 3116 : 0.09810016304254532\n",
      "Training loss for batch 3117 : 0.18717962503433228\n",
      "Training loss for batch 3118 : 0.10156112909317017\n",
      "Training loss for batch 3119 : 0.07728730887174606\n",
      "Training loss for batch 3120 : 0.05786050856113434\n",
      "Training loss for batch 3121 : 0.1014174371957779\n",
      "Training loss for batch 3122 : 0.09380824863910675\n",
      "Training loss for batch 3123 : 0.02317913994193077\n",
      "Training loss for batch 3124 : 0.10887409001588821\n",
      "Training loss for batch 3125 : 0.052308738231658936\n",
      "Training loss for batch 3126 : 0.103338323533535\n",
      "Training loss for batch 3127 : 0.10106835514307022\n",
      "Training loss for batch 3128 : 0.10091438889503479\n",
      "Training loss for batch 3129 : 0.024404849857091904\n",
      "Training loss for batch 3130 : 0.20084792375564575\n",
      "Training loss for batch 3131 : 0.05514448508620262\n",
      "Training loss for batch 3132 : 0.1450076848268509\n",
      "Training loss for batch 3133 : 0.002794817090034485\n",
      "Training loss for batch 3134 : 0.08551173657178879\n",
      "Training loss for batch 3135 : 0.06986216455698013\n",
      "Training loss for batch 3136 : 0.0528801828622818\n",
      "Training loss for batch 3137 : 0.053806960582733154\n",
      "Training loss for batch 3138 : 0.15636207163333893\n",
      "Training loss for batch 3139 : 0.12381604313850403\n",
      "Training loss for batch 3140 : 0.04717862978577614\n",
      "Training loss for batch 3141 : 0.07314177602529526\n",
      "Training loss for batch 3142 : 0.09377504885196686\n",
      "Training loss for batch 3143 : 0.11167260259389877\n",
      "Training loss for batch 3144 : 0.08765093982219696\n",
      "Training loss for batch 3145 : 0.11918214708566666\n",
      "Training loss for batch 3146 : 0.14403071999549866\n",
      "Training loss for batch 3147 : 0.06809905171394348\n",
      "Training loss for batch 3148 : 0.06588651239871979\n",
      "Training loss for batch 3149 : 0.11532152444124222\n",
      "Training loss for batch 3150 : 0.023247141391038895\n",
      "Training loss for batch 3151 : 0.17019318044185638\n",
      "Training loss for batch 3152 : 0.008668681606650352\n",
      "Training loss for batch 3153 : 0.19704458117485046\n",
      "Training loss for batch 3154 : 0.0395159013569355\n",
      "Training loss for batch 3155 : 0.05178714543581009\n",
      "Training loss for batch 3156 : 0.045145146548748016\n",
      "Training loss for batch 3157 : 5.752903220468397e-08\n",
      "Training loss for batch 3158 : 0.08583743870258331\n",
      "Training loss for batch 3159 : 0.02496383525431156\n",
      "Training loss for batch 3160 : 0.11826188117265701\n",
      "Training loss for batch 3161 : 0.10893818736076355\n",
      "Training loss for batch 3162 : 0.014871882274746895\n",
      "Training loss for batch 3163 : 0.014304659329354763\n",
      "Training loss for batch 3164 : 0.18366673588752747\n",
      "Training loss for batch 3165 : 0.025779929012060165\n",
      "Training loss for batch 3166 : 0.08716929703950882\n",
      "Training loss for batch 3167 : 0.070449598133564\n",
      "Training loss for batch 3168 : 0.07616238296031952\n",
      "Training loss for batch 3169 : 0.12325039505958557\n",
      "Training loss for batch 3170 : 0.0004343816835898906\n",
      "Training loss for batch 3171 : 0.07441475987434387\n",
      "Training loss for batch 3172 : 0.11878658086061478\n",
      "Training loss for batch 3173 : 0.09996096044778824\n",
      "Training loss for batch 3174 : 0.18428409099578857\n",
      "Training loss for batch 3175 : 0.04281661659479141\n",
      "Training loss for batch 3176 : 0.08483617007732391\n",
      "Training loss for batch 3177 : 0.08314190059900284\n",
      "Training loss for batch 3178 : 0.07051537930965424\n",
      "Training loss for batch 3179 : 0.014949878677725792\n",
      "Training loss for batch 3180 : 0.11663356423377991\n",
      "Training loss for batch 3181 : 0.1478969305753708\n",
      "Training loss for batch 3182 : 0.06937552243471146\n",
      "Training loss for batch 3183 : 0.19128522276878357\n",
      "Training loss for batch 3184 : 0.08470802009105682\n",
      "Training loss for batch 3185 : 0.07338904589414597\n",
      "Training loss for batch 3186 : 0.08914056420326233\n",
      "Training loss for batch 3187 : 0.04529080539941788\n",
      "Training loss for batch 3188 : 0.06450117379426956\n",
      "Training loss for batch 3189 : 0.08006981015205383\n",
      "Training loss for batch 3190 : 0.1685761958360672\n",
      "Training loss for batch 3191 : 0.08122649043798447\n",
      "Training loss for batch 3192 : 0.028310224413871765\n",
      "Training loss for batch 3193 : 0.037625450640916824\n",
      "Training loss for batch 3194 : 0.06450176984071732\n",
      "Training loss for batch 3195 : 0.05140778422355652\n",
      "Training loss for batch 3196 : 0.10452558100223541\n",
      "Training loss for batch 3197 : 0.027178263291716576\n",
      "Training loss for batch 3198 : 0.06744299829006195\n",
      "Training loss for batch 3199 : 0.04981022700667381\n",
      "Training loss for batch 3200 : 0.182705819606781\n",
      "Training loss for batch 3201 : 0.052428487688302994\n",
      "Training loss for batch 3202 : 0.07864702492952347\n",
      "Training loss for batch 3203 : 0.1079268604516983\n",
      "Training loss for batch 3204 : 0.09197910130023956\n",
      "Training loss for batch 3205 : 0.06888455152511597\n",
      "Training loss for batch 3206 : 0.09049290418624878\n",
      "Training loss for batch 3207 : 0.19289906322956085\n",
      "Training loss for batch 3208 : 0.05083984509110451\n",
      "Training loss for batch 3209 : 0.08474324643611908\n",
      "Training loss for batch 3210 : 0.15763112902641296\n",
      "Training loss for batch 3211 : 0.15030644834041595\n",
      "Training loss for batch 3212 : 0.05448611080646515\n",
      "Training loss for batch 3213 : 0.07881268858909607\n",
      "Training loss for batch 3214 : 0.01413148082792759\n",
      "Training loss for batch 3215 : 0.14813120663166046\n",
      "Training loss for batch 3216 : 0.14249061048030853\n",
      "Training loss for batch 3217 : 0.04222455620765686\n",
      "Training loss for batch 3218 : 0.07813553512096405\n",
      "Training loss for batch 3219 : 0.12312325835227966\n",
      "Training loss for batch 3220 : 0.12835736572742462\n",
      "Training loss for batch 3221 : 0.10883484780788422\n",
      "Training loss for batch 3222 : 0.1374700367450714\n",
      "Training loss for batch 3223 : 0.10753940045833588\n",
      "Training loss for batch 3224 : 0.105754055082798\n",
      "Training loss for batch 3225 : 0.12228125333786011\n",
      "Training loss for batch 3226 : 0.07504827529191971\n",
      "Training loss for batch 3227 : 0.06591494381427765\n",
      "Training loss for batch 3228 : 0.17241036891937256\n",
      "Training loss for batch 3229 : 0.06687387824058533\n",
      "Training loss for batch 3230 : 0.040918104350566864\n",
      "Training loss for batch 3231 : 0.061329007148742676\n",
      "Training loss for batch 3232 : 0.12381977587938309\n",
      "Training loss for batch 3233 : 0.10325013101100922\n",
      "Training loss for batch 3234 : 0.11067841202020645\n",
      "Training loss for batch 3235 : 0.09591151773929596\n",
      "Training loss for batch 3236 : 0.058898549526929855\n",
      "Training loss for batch 3237 : 0.10568197071552277\n",
      "Training loss for batch 3238 : 0.04443662613630295\n",
      "Training loss for batch 3239 : 0.18340803682804108\n",
      "Training loss for batch 3240 : 0.11113438010215759\n",
      "Training loss for batch 3241 : 0.1181408241391182\n",
      "Training loss for batch 3242 : 0.1095471903681755\n",
      "Training loss for batch 3243 : 0.016365595161914825\n",
      "Training loss for batch 3244 : 0.038943685591220856\n",
      "Training loss for batch 3245 : 0.0897592157125473\n",
      "Training loss for batch 3246 : 0.026393242180347443\n",
      "Training loss for batch 3247 : 0.027661845088005066\n",
      "Training loss for batch 3248 : 0.050073135644197464\n",
      "Training loss for batch 3249 : 0.04378712922334671\n",
      "Training loss for batch 3250 : 0.12332524359226227\n",
      "Training loss for batch 3251 : 0.07668430358171463\n",
      "Training loss for batch 3252 : 0.05398866906762123\n",
      "Training loss for batch 3253 : 0.07364115864038467\n",
      "Training loss for batch 3254 : 0.016838442534208298\n",
      "Training loss for batch 3255 : 0.08176843076944351\n",
      "Training loss for batch 3256 : 0.038274697959423065\n",
      "Training loss for batch 3257 : 0.06245097517967224\n",
      "Training loss for batch 3258 : 0.09161009639501572\n",
      "Training loss for batch 3259 : 0.0\n",
      "Training loss for batch 3260 : 0.007013147231191397\n",
      "Training loss for batch 3261 : 0.09004678577184677\n",
      "Training loss for batch 3262 : 0.1320909708738327\n",
      "Training loss for batch 3263 : 0.04186300188302994\n",
      "Training loss for batch 3264 : 0.15298724174499512\n",
      "Training loss for batch 3265 : 0.07091128081083298\n",
      "Training loss for batch 3266 : 0.1177259013056755\n",
      "Training loss for batch 3267 : 0.07771989703178406\n",
      "Training loss for batch 3268 : 0.09441518038511276\n",
      "Training loss for batch 3269 : 0.004998637363314629\n",
      "Training loss for batch 3270 : 0.07014217227697372\n",
      "Training loss for batch 3271 : 0.15966619551181793\n",
      "Training loss for batch 3272 : 0.08326234668493271\n",
      "Training loss for batch 3273 : 0.06705187261104584\n",
      "Training loss for batch 3274 : 0.12057878822088242\n",
      "Training loss for batch 3275 : 0.07476913183927536\n",
      "Training loss for batch 3276 : 0.139275923371315\n",
      "Training loss for batch 3277 : 0.05503496155142784\n",
      "Training loss for batch 3278 : 0.09528428316116333\n",
      "Training loss for batch 3279 : 0.1068543866276741\n",
      "Training loss for batch 3280 : 0.02626907266676426\n",
      "Training loss for batch 3281 : 0.08200298249721527\n",
      "Training loss for batch 3282 : 0.015140412375330925\n",
      "Training loss for batch 3283 : 0.06267110258340836\n",
      "Training loss for batch 3284 : 0.10603952407836914\n",
      "Training loss for batch 3285 : 0.015336183831095695\n",
      "Training loss for batch 3286 : 0.030637336894869804\n",
      "Training loss for batch 3287 : 0.06887258589267731\n",
      "Training loss for batch 3288 : 0.038960471749305725\n",
      "Training loss for batch 3289 : 0.09359244257211685\n",
      "Training loss for batch 3290 : 0.007362497039139271\n",
      "Training loss for batch 3291 : 0.05963203310966492\n",
      "Training loss for batch 3292 : 0.039495136588811874\n",
      "Training loss for batch 3293 : 0.04322606697678566\n",
      "Training loss for batch 3294 : 0.08222027868032455\n",
      "Training loss for batch 3295 : 0.067332923412323\n",
      "Training loss for batch 3296 : 0.14715927839279175\n",
      "Training loss for batch 3297 : 0.08151501417160034\n",
      "Training loss for batch 3298 : 0.02799788862466812\n",
      "Training loss for batch 3299 : 0.1635775864124298\n",
      "Training loss for batch 3300 : 0.15715764462947845\n",
      "Training loss for batch 3301 : 0.02810029685497284\n",
      "Training loss for batch 3302 : 0.10519937425851822\n",
      "Training loss for batch 3303 : 0.04257506877183914\n",
      "Training loss for batch 3304 : 0.0627792626619339\n",
      "Training loss for batch 3305 : 0.17839734256267548\n",
      "Training loss for batch 3306 : 0.10897226631641388\n",
      "Training loss for batch 3307 : 0.062325503677129745\n",
      "Training loss for batch 3308 : 0.07403799891471863\n",
      "Training loss for batch 3309 : 0.06278271228075027\n",
      "Training loss for batch 3310 : 0.027168186381459236\n",
      "Training loss for batch 3311 : 0.0873447060585022\n",
      "Training loss for batch 3312 : 0.1427396535873413\n",
      "Training loss for batch 3313 : 0.036315206438302994\n",
      "Training loss for batch 3314 : 0.03751859441399574\n",
      "Training loss for batch 3315 : 0.12413474172353745\n",
      "Training loss for batch 3316 : 0.04646279662847519\n",
      "Training loss for batch 3317 : 0.063724085688591\n",
      "Training loss for batch 3318 : 0.11880931258201599\n",
      "Training loss for batch 3319 : 0.011084100231528282\n",
      "Training loss for batch 3320 : 0.06305686384439468\n",
      "Training loss for batch 3321 : 0.00832951907068491\n",
      "Training loss for batch 3322 : 0.06750746816396713\n",
      "Training loss for batch 3323 : 0.04716833680868149\n",
      "Training loss for batch 3324 : 0.040975987911224365\n",
      "Training loss for batch 3325 : 0.10804475843906403\n",
      "Training loss for batch 3326 : 0.0663396343588829\n",
      "Training loss for batch 3327 : 0.13415400683879852\n",
      "Training loss for batch 3328 : 0.13785232603549957\n",
      "Training loss for batch 3329 : 0.0316077396273613\n",
      "Training loss for batch 3330 : 0.16190971434116364\n",
      "Training loss for batch 3331 : 0.004039153456687927\n",
      "Training loss for batch 3332 : 0.08213770389556885\n",
      "Training loss for batch 3333 : 0.03580881282687187\n",
      "Training loss for batch 3334 : 0.0958467647433281\n",
      "Training loss for batch 3335 : 0.054698195308446884\n",
      "Training loss for batch 3336 : 0.13170461356639862\n",
      "Training loss for batch 3337 : 0.03188065066933632\n",
      "Training loss for batch 3338 : 0.04458850994706154\n",
      "Training loss for batch 3339 : 0.02862267754971981\n",
      "Training loss for batch 3340 : 0.07533330470323563\n",
      "Training loss for batch 3341 : 0.049710266292095184\n",
      "Training loss for batch 3342 : 0.09628228843212128\n",
      "Training loss for batch 3343 : 0.0909881591796875\n",
      "Training loss for batch 3344 : 0.077846460044384\n",
      "Training loss for batch 3345 : 0.04430102929472923\n",
      "Training loss for batch 3346 : 0.09841381758451462\n",
      "Training loss for batch 3347 : 0.049433380365371704\n",
      "Training loss for batch 3348 : 0.058094099164009094\n",
      "Training loss for batch 3349 : 0.04654056951403618\n",
      "Training loss for batch 3350 : 0.21780332922935486\n",
      "Training loss for batch 3351 : 0.0862581729888916\n",
      "Training loss for batch 3352 : 0.07255525887012482\n",
      "Training loss for batch 3353 : 0.02307290956377983\n",
      "Training loss for batch 3354 : 0.09338254481554031\n",
      "Training loss for batch 3355 : 0.18935631215572357\n",
      "Training loss for batch 3356 : 0.04451579228043556\n",
      "Training loss for batch 3357 : 0.017591197043657303\n",
      "Training loss for batch 3358 : 0.02646275982260704\n",
      "Training loss for batch 3359 : 0.08833055198192596\n",
      "Training loss for batch 3360 : 0.03405598923563957\n",
      "Training loss for batch 3361 : 0.004688069224357605\n",
      "Training loss for batch 3362 : 0.05538161098957062\n",
      "Training loss for batch 3363 : 0.027011381462216377\n",
      "Training loss for batch 3364 : 0.06363944709300995\n",
      "Training loss for batch 3365 : 0.07580757141113281\n",
      "Training loss for batch 3366 : 0.06433234363794327\n",
      "Training loss for batch 3367 : 0.08036302030086517\n",
      "Training loss for batch 3368 : 0.015857728198170662\n",
      "Training loss for batch 3369 : 0.04325472190976143\n",
      "Training loss for batch 3370 : 0.08765105158090591\n",
      "Training loss for batch 3371 : 0.24898578226566315\n",
      "Training loss for batch 3372 : 0.06329550594091415\n",
      "Training loss for batch 3373 : 0.07215085625648499\n",
      "Training loss for batch 3374 : 0.052754368633031845\n",
      "Training loss for batch 3375 : 0.12858201563358307\n",
      "Training loss for batch 3376 : 0.20394054055213928\n",
      "Training loss for batch 3377 : 0.02055952325463295\n",
      "Training loss for batch 3378 : 0.07357410341501236\n",
      "Training loss for batch 3379 : 0.17447718977928162\n",
      "Training loss for batch 3380 : 0.1252996027469635\n",
      "Training loss for batch 3381 : 0.0671139657497406\n",
      "Training loss for batch 3382 : 0.08060742914676666\n",
      "Training loss for batch 3383 : 0.018897635862231255\n",
      "Training loss for batch 3384 : 0.02011905610561371\n",
      "Training loss for batch 3385 : 0.05486568436026573\n",
      "Training loss for batch 3386 : 0.0713803693652153\n",
      "Training loss for batch 3387 : 0.07290636003017426\n",
      "Training loss for batch 3388 : 0.09093978255987167\n",
      "Training loss for batch 3389 : 0.08150976896286011\n",
      "Training loss for batch 3390 : 0.10501078516244888\n",
      "Training loss for batch 3391 : 0.05519269406795502\n",
      "Training loss for batch 3392 : 0.10940675437450409\n",
      "Training loss for batch 3393 : 0.08391522616147995\n",
      "Training loss for batch 3394 : 0.1083136722445488\n",
      "Training loss for batch 3395 : 0.05068296194076538\n",
      "Training loss for batch 3396 : 0.16858749091625214\n",
      "Training loss for batch 3397 : 0.04262042045593262\n",
      "Training loss for batch 3398 : 0.06648080050945282\n",
      "Training loss for batch 3399 : 0.019930751994252205\n",
      "Training loss for batch 3400 : 0.06112164631485939\n",
      "Training loss for batch 3401 : 0.024751003831624985\n",
      "Training loss for batch 3402 : 0.07719147950410843\n",
      "Training loss for batch 3403 : 0.03925324231386185\n",
      "Training loss for batch 3404 : 0.03811328858137131\n",
      "Training loss for batch 3405 : 0.03179764002561569\n",
      "Training loss for batch 3406 : 0.05996813252568245\n",
      "Training loss for batch 3407 : 0.1297670304775238\n",
      "Training loss for batch 3408 : 0.03785935789346695\n",
      "Training loss for batch 3409 : 0.201345294713974\n",
      "Training loss for batch 3410 : 0.15827524662017822\n",
      "Training loss for batch 3411 : 0.07054482400417328\n",
      "Training loss for batch 3412 : 0.08067293465137482\n",
      "Training loss for batch 3413 : 0.08515840768814087\n",
      "Training loss for batch 3414 : 0.025063034147024155\n",
      "Training loss for batch 3415 : 0.02682717889547348\n",
      "Training loss for batch 3416 : 0.02240796387195587\n",
      "Training loss for batch 3417 : 0.07878370583057404\n",
      "Training loss for batch 3418 : 0.07865975797176361\n",
      "Training loss for batch 3419 : 0.070688396692276\n",
      "Training loss for batch 3420 : 0.05369551479816437\n",
      "Training loss for batch 3421 : 0.18285009264945984\n",
      "Training loss for batch 3422 : 0.035953156650066376\n",
      "Training loss for batch 3423 : 0.21995702385902405\n",
      "Training loss for batch 3424 : 0.0370863601565361\n",
      "Training loss for batch 3425 : 0.06964119523763657\n",
      "Training loss for batch 3426 : 0.0756402462720871\n",
      "Training loss for batch 3427 : 0.15045571327209473\n",
      "Training loss for batch 3428 : 0.11541689187288284\n",
      "Training loss for batch 3429 : 0.04547536373138428\n",
      "Training loss for batch 3430 : 0.08298619836568832\n",
      "Training loss for batch 3431 : 0.019816221669316292\n",
      "Training loss for batch 3432 : 0.12225398421287537\n",
      "Training loss for batch 3433 : 0.055929798632860184\n",
      "Training loss for batch 3434 : 0.025927623733878136\n",
      "Training loss for batch 3435 : 0.24700449407100677\n",
      "Training loss for batch 3436 : 0.10313290357589722\n",
      "Training loss for batch 3437 : 0.06468211114406586\n",
      "Training loss for batch 3438 : 0.08317550271749496\n",
      "Training loss for batch 3439 : 0.048818327486515045\n",
      "Training loss for batch 3440 : 0.058238685131073\n",
      "Training loss for batch 3441 : 0.06020447611808777\n",
      "Training loss for batch 3442 : 0.04447873309254646\n",
      "Training loss for batch 3443 : 0.11403220891952515\n",
      "Training loss for batch 3444 : 0.0867810845375061\n",
      "Training loss for batch 3445 : 0.13126875460147858\n",
      "Training loss for batch 3446 : 0.06601928919553757\n",
      "Training loss for batch 3447 : 0.07335560023784637\n",
      "Training loss for batch 3448 : 0.11872881650924683\n",
      "Training loss for batch 3449 : 0.03391251340508461\n",
      "Training loss for batch 3450 : 0.022697577252984047\n",
      "Training loss for batch 3451 : 0.08361878246068954\n",
      "Training loss for batch 3452 : 0.03659054636955261\n",
      "Training loss for batch 3453 : 0.06162206083536148\n",
      "Training loss for batch 3454 : 0.050872571766376495\n",
      "Training loss for batch 3455 : 0.015254846774041653\n",
      "Training loss for batch 3456 : 0.3442225456237793\n",
      "Training loss for batch 3457 : 0.00032586607267148793\n",
      "Training loss for batch 3458 : 0.10364282876253128\n",
      "Training loss for batch 3459 : 0.1408294141292572\n",
      "Training loss for batch 3460 : 0.09551975131034851\n",
      "Training loss for batch 3461 : 0.11665716767311096\n",
      "Training loss for batch 3462 : 0.03695822134613991\n",
      "Training loss for batch 3463 : 0.022073233500123024\n",
      "Training loss for batch 3464 : 0.1779836118221283\n",
      "Training loss for batch 3465 : 0.07094784080982208\n",
      "Training loss for batch 3466 : 0.0991591140627861\n",
      "Training loss for batch 3467 : 0.012269693426787853\n",
      "Training loss for batch 3468 : 0.009175478480756283\n",
      "Training loss for batch 3469 : 0.09314614534378052\n",
      "Training loss for batch 3470 : 0.19615501165390015\n",
      "Training loss for batch 3471 : 0.08826155215501785\n",
      "Training loss for batch 3472 : 0.014959408901631832\n",
      "Training loss for batch 3473 : 0.06802161037921906\n",
      "Training loss for batch 3474 : 0.12640900909900665\n",
      "Training loss for batch 3475 : 0.07116123288869858\n",
      "Training loss for batch 3476 : 0.021593382582068443\n",
      "Training loss for batch 3477 : 0.11402151733636856\n",
      "Training loss for batch 3478 : 0.014087469317018986\n",
      "Training loss for batch 3479 : 0.08944682031869888\n",
      "Training loss for batch 3480 : 0.09895022958517075\n",
      "Training loss for batch 3481 : 0.06661469489336014\n",
      "Training loss for batch 3482 : 0.15407608449459076\n",
      "Training loss for batch 3483 : 0.06258237361907959\n",
      "Training loss for batch 3484 : 0.11901457607746124\n",
      "Training loss for batch 3485 : 0.12296686321496964\n",
      "Training loss for batch 3486 : 0.11520470678806305\n",
      "Training loss for batch 3487 : 0.05464503541588783\n",
      "Training loss for batch 3488 : 0.0984676331281662\n",
      "Training loss for batch 3489 : 0.10549308359622955\n",
      "Training loss for batch 3490 : 0.07235652953386307\n",
      "Training loss for batch 3491 : 0.03328651562333107\n",
      "Training loss for batch 3492 : 0.004419515375047922\n",
      "Training loss for batch 3493 : 0.03961692750453949\n",
      "Training loss for batch 3494 : 0.06712175160646439\n",
      "Training loss for batch 3495 : 0.14814938604831696\n",
      "Training loss for batch 3496 : 0.018966909497976303\n",
      "Training loss for batch 3497 : 0.009369044564664364\n",
      "Training loss for batch 3498 : 0.05235525593161583\n",
      "Training loss for batch 3499 : 0.18500444293022156\n",
      "Training loss for batch 3500 : 0.027827439829707146\n",
      "Training loss for batch 3501 : 0.17865580320358276\n",
      "Training loss for batch 3502 : 0.1977875977754593\n",
      "Training loss for batch 3503 : 0.15974490344524384\n",
      "Training loss for batch 3504 : 0.19505102932453156\n",
      "Training loss for batch 3505 : 0.15831780433654785\n",
      "Training loss for batch 3506 : 0.14890679717063904\n",
      "Training loss for batch 3507 : 0.07500237971544266\n",
      "Training loss for batch 3508 : 0.05740414187312126\n",
      "Training loss for batch 3509 : 0.04310532659292221\n",
      "Training loss for batch 3510 : 0.16032925248146057\n",
      "Training loss for batch 3511 : 0.06630555540323257\n",
      "Training loss for batch 3512 : 0.13732033967971802\n",
      "Training loss for batch 3513 : 0.09059300273656845\n",
      "Training loss for batch 3514 : 0.03255992382764816\n",
      "Training loss for batch 3515 : 0.030268197879195213\n",
      "Training loss for batch 3516 : 0.07658839225769043\n",
      "Training loss for batch 3517 : 0.01558363251388073\n",
      "Training loss for batch 3518 : 0.10057900100946426\n",
      "Training loss for batch 3519 : 0.12003906816244125\n",
      "Training loss for batch 3520 : 0.17449086904525757\n",
      "Training loss for batch 3521 : 0.08406832069158554\n",
      "Training loss for batch 3522 : 0.0495489165186882\n",
      "Training loss for batch 3523 : 0.09366588294506073\n",
      "Training loss for batch 3524 : 0.05578188970685005\n",
      "Training loss for batch 3525 : 0.034340810030698776\n",
      "Training loss for batch 3526 : 0.10368867218494415\n",
      "Training loss for batch 3527 : 0.2888055741786957\n",
      "Training loss for batch 3528 : 0.038150373846292496\n",
      "Training loss for batch 3529 : 0.10711365938186646\n",
      "Training loss for batch 3530 : 0.114645816385746\n",
      "Training loss for batch 3531 : 0.1074003353714943\n",
      "Training loss for batch 3532 : 0.06090271472930908\n",
      "Training loss for batch 3533 : 0.05332493782043457\n",
      "Training loss for batch 3534 : 0.034240491688251495\n",
      "Training loss for batch 3535 : 0.09738899767398834\n",
      "Training loss for batch 3536 : 0.05358164384961128\n",
      "Training loss for batch 3537 : 0.056754931807518005\n",
      "Training loss for batch 3538 : 0.09477546811103821\n",
      "Training loss for batch 3539 : 0.030178364366292953\n",
      "Training loss for batch 3540 : 0.16744834184646606\n",
      "Training loss for batch 3541 : 0.09708089381456375\n",
      "Training loss for batch 3542 : 0.07754763960838318\n",
      "Training loss for batch 3543 : 0.030783265829086304\n",
      "Training loss for batch 3544 : 0.0552801638841629\n",
      "Training loss for batch 3545 : 0.10068654268980026\n",
      "Training loss for batch 3546 : 0.04939715936779976\n",
      "Training loss for batch 3547 : 0.0430988185107708\n",
      "Training loss for batch 3548 : 0.04281145706772804\n",
      "Training loss for batch 3549 : 0.0345245860517025\n",
      "Training loss for batch 3550 : 0.05700318515300751\n",
      "Training loss for batch 3551 : 0.04000668600201607\n",
      "Training loss for batch 3552 : 0.035448841750621796\n",
      "Training loss for batch 3553 : 0.05796488747000694\n",
      "Training loss for batch 3554 : 0.10887036472558975\n",
      "Training loss for batch 3555 : 0.05370645597577095\n",
      "Training loss for batch 3556 : 0.06456609070301056\n",
      "Training loss for batch 3557 : 0.005968497134745121\n",
      "Training loss for batch 3558 : 0.03846683353185654\n",
      "Training loss for batch 3559 : 0.1146618127822876\n",
      "Training loss for batch 3560 : 0.0707109272480011\n",
      "Training loss for batch 3561 : 0.05099255591630936\n",
      "Training loss for batch 3562 : 0.06182751804590225\n",
      "Training loss for batch 3563 : 0.11367207765579224\n",
      "Training loss for batch 3564 : 0.21404045820236206\n",
      "Training loss for batch 3565 : 0.06139932945370674\n",
      "Training loss for batch 3566 : 0.029794538393616676\n",
      "Training loss for batch 3567 : 0.07797152549028397\n",
      "Training loss for batch 3568 : 0.22780194878578186\n",
      "Training loss for batch 3569 : 0.02055981196463108\n",
      "Training loss for batch 3570 : 0.035734836012125015\n",
      "Training loss for batch 3571 : 0.023768818005919456\n",
      "Training loss for batch 3572 : 0.058931004256010056\n",
      "Training loss for batch 3573 : 0.07347901910543442\n",
      "Training loss for batch 3574 : 0.11116677522659302\n",
      "Training loss for batch 3575 : 0.007137759122997522\n",
      "Training loss for batch 3576 : 0.08766547590494156\n",
      "Training loss for batch 3577 : 0.09244278073310852\n",
      "Training loss for batch 3578 : 0.037933044135570526\n",
      "Training loss for batch 3579 : 0.1495795100927353\n",
      "Training loss for batch 3580 : 0.04673241823911667\n",
      "Training loss for batch 3581 : 0.04735204577445984\n",
      "Training loss for batch 3582 : 0.0343405082821846\n",
      "Training loss for batch 3583 : 0.05007784068584442\n",
      "Training loss for batch 3584 : 0.07297437638044357\n",
      "Training loss for batch 3585 : 0.13080085813999176\n",
      "Training loss for batch 3586 : 0.041344985365867615\n",
      "Training loss for batch 3587 : 0.07303772866725922\n",
      "Training loss for batch 3588 : 0.09105991572141647\n",
      "Training loss for batch 3589 : 0.13394497334957123\n",
      "Training loss for batch 3590 : 0.05773056298494339\n",
      "Training loss for batch 3591 : 0.05691906437277794\n",
      "Training loss for batch 3592 : 0.07053091377019882\n",
      "Training loss for batch 3593 : 0.0008062577689997852\n",
      "Training loss for batch 3594 : 0.12589925527572632\n",
      "Training loss for batch 3595 : 0.04011539742350578\n",
      "Training loss for batch 3596 : 0.10334907472133636\n",
      "Training loss for batch 3597 : 0.014597958885133266\n",
      "Training loss for batch 3598 : 0.02249886281788349\n",
      "Training loss for batch 3599 : 0.07213088870048523\n",
      "Training loss for batch 3600 : 0.10649663209915161\n",
      "Training loss for batch 3601 : 0.09710577130317688\n",
      "Training loss for batch 3602 : 5.4938787741320994e-08\n",
      "Training loss for batch 3603 : 0.09665904939174652\n",
      "Training loss for batch 3604 : 0.1981431394815445\n",
      "Training loss for batch 3605 : 0.08541958779096603\n",
      "Training loss for batch 3606 : 0.05367868393659592\n",
      "Training loss for batch 3607 : 0.05002989247441292\n",
      "Training loss for batch 3608 : 0.12074315547943115\n",
      "Training loss for batch 3609 : 0.07384631037712097\n",
      "Training loss for batch 3610 : 0.010707523673772812\n",
      "Training loss for batch 3611 : 0.06077123433351517\n",
      "Training loss for batch 3612 : 0.0420948788523674\n",
      "Training loss for batch 3613 : 0.1315414309501648\n",
      "Training loss for batch 3614 : 0.04637226089835167\n",
      "Training loss for batch 3615 : 0.014973367564380169\n",
      "Training loss for batch 3616 : 0.03496357053518295\n",
      "Training loss for batch 3617 : 0.0238608680665493\n",
      "Training loss for batch 3618 : 0.056129418313503265\n",
      "Training loss for batch 3619 : 0.005651769693940878\n",
      "Training loss for batch 3620 : 0.051474373787641525\n",
      "Training loss for batch 3621 : 0.05038990080356598\n",
      "Training loss for batch 3622 : 0.07869329303503036\n",
      "Training loss for batch 3623 : 0.019552327692508698\n",
      "Training loss for batch 3624 : 0.053575702011585236\n",
      "Training loss for batch 3625 : 0.014559894800186157\n",
      "Training loss for batch 3626 : 0.024768823757767677\n",
      "Training loss for batch 3627 : 0.19655343890190125\n",
      "Training loss for batch 3628 : 0.03178989887237549\n",
      "Training loss for batch 3629 : 0.07369887828826904\n",
      "Training loss for batch 3630 : 0.09382162988185883\n",
      "Training loss for batch 3631 : 0.36096882820129395\n",
      "Training loss for batch 3632 : 0.09258575737476349\n",
      "Training loss for batch 3633 : 0.04722646251320839\n",
      "Training loss for batch 3634 : 0.19291216135025024\n",
      "Training loss for batch 3635 : 0.010353058576583862\n",
      "Training loss for batch 3636 : 0.08014363795518875\n",
      "Training loss for batch 3637 : 0.15143631398677826\n",
      "Training loss for batch 3638 : 0.03106699138879776\n",
      "Training loss for batch 3639 : 0.013652835041284561\n",
      "Training loss for batch 3640 : 0.038657933473587036\n",
      "Training loss for batch 3641 : 0.14105461537837982\n",
      "Training loss for batch 3642 : 0.033655062317848206\n",
      "Training loss for batch 3643 : 0.02916847914457321\n",
      "Training loss for batch 3644 : 0.039166007190942764\n",
      "Training loss for batch 3645 : 0.04333597049117088\n",
      "Training loss for batch 3646 : 0.01663488708436489\n",
      "Training loss for batch 3647 : 0.14015084505081177\n",
      "Training loss for batch 3648 : 0.055552300065755844\n",
      "Training loss for batch 3649 : 0.04983653873205185\n",
      "Training loss for batch 3650 : 0.11814485490322113\n",
      "Training loss for batch 3651 : 0.04942741245031357\n",
      "Training loss for batch 3652 : 0.03676528483629227\n",
      "Training loss for batch 3653 : 0.061919402331113815\n",
      "Training loss for batch 3654 : 0.09744153916835785\n",
      "Training loss for batch 3655 : 0.006851173937320709\n",
      "Training loss for batch 3656 : 0.07966724038124084\n",
      "Training loss for batch 3657 : 0.1300790160894394\n",
      "Training loss for batch 3658 : 0.023756863549351692\n",
      "Training loss for batch 3659 : 0.03621012344956398\n",
      "Training loss for batch 3660 : 0.10201959311962128\n",
      "Training loss for batch 3661 : 0.006625886540859938\n",
      "Training loss for batch 3662 : 0.03772951290011406\n",
      "Training loss for batch 3663 : 0.1893814653158188\n",
      "Training loss for batch 3664 : 0.14174523949623108\n",
      "Training loss for batch 3665 : 0.08682618290185928\n",
      "Training loss for batch 3666 : 0.09314451366662979\n",
      "Training loss for batch 3667 : 0.0048095956444740295\n",
      "Training loss for batch 3668 : 0.037282027304172516\n",
      "Training loss for batch 3669 : 0.030438775196671486\n",
      "Training loss for batch 3670 : 0.09629964083433151\n",
      "Training loss for batch 3671 : 0.030721189454197884\n",
      "Training loss for batch 3672 : 0.05658252537250519\n",
      "Training loss for batch 3673 : 0.12868596613407135\n",
      "Training loss for batch 3674 : 0.106179378926754\n",
      "Training loss for batch 3675 : 0.08888567239046097\n",
      "Training loss for batch 3676 : 0.03595099225640297\n",
      "Training loss for batch 3677 : 0.025049226358532906\n",
      "Training loss for batch 3678 : 0.044192321598529816\n",
      "Training loss for batch 3679 : 0.05309068039059639\n",
      "Training loss for batch 3680 : 0.14415454864501953\n",
      "Training loss for batch 3681 : 0.03904261440038681\n",
      "Training loss for batch 3682 : 0.024996252730488777\n",
      "Training loss for batch 3683 : 0.22099921107292175\n",
      "Training loss for batch 3684 : 0.09025311470031738\n",
      "Training loss for batch 3685 : 0.04632516950368881\n",
      "Training loss for batch 3686 : 0.29707521200180054\n",
      "Training loss for batch 3687 : 0.16524994373321533\n",
      "Training loss for batch 3688 : 0.12817785143852234\n",
      "Training loss for batch 3689 : 0.043728530406951904\n",
      "Training loss for batch 3690 : 0.033712007105350494\n",
      "Training loss for batch 3691 : 0.12768502533435822\n",
      "Training loss for batch 3692 : 0.036644380539655685\n",
      "Training loss for batch 3693 : 0.04021364450454712\n",
      "Training loss for batch 3694 : 0.006012543570250273\n",
      "Training loss for batch 3695 : 0.09206559509038925\n",
      "Training loss for batch 3696 : 0.13334955275058746\n",
      "Training loss for batch 3697 : 0.001425842521712184\n",
      "Training loss for batch 3698 : 0.07950625568628311\n",
      "Training loss for batch 3699 : 0.19129031896591187\n",
      "Training loss for batch 3700 : 0.02791011519730091\n",
      "Training loss for batch 3701 : 0.12360890954732895\n",
      "Training loss for batch 3702 : 0.12054052948951721\n",
      "Training loss for batch 3703 : 0.026182889938354492\n",
      "Training loss for batch 3704 : 0.1019526943564415\n",
      "Training loss for batch 3705 : 0.005785578396171331\n",
      "Training loss for batch 3706 : 0.1976989060640335\n",
      "Training loss for batch 3707 : 0.04510721564292908\n",
      "Training loss for batch 3708 : 0.1420266479253769\n",
      "Training loss for batch 3709 : 0.017664799466729164\n",
      "Training loss for batch 3710 : 0.07607842236757278\n",
      "Training loss for batch 3711 : 0.06471927464008331\n",
      "Training loss for batch 3712 : 0.11538546532392502\n",
      "Training loss for batch 3713 : 0.03147158771753311\n",
      "Training loss for batch 3714 : 0.0593930222094059\n",
      "Training loss for batch 3715 : 0.01928309164941311\n",
      "Training loss for batch 3716 : 0.12456312030553818\n",
      "Training loss for batch 3717 : 0.01269773580133915\n",
      "Training loss for batch 3718 : 0.030613478273153305\n",
      "Training loss for batch 3719 : 0.06219027191400528\n",
      "Training loss for batch 3720 : 0.16438741981983185\n",
      "Training loss for batch 3721 : 0.08108215779066086\n",
      "Training loss for batch 3722 : 0.06189357489347458\n",
      "Training loss for batch 3723 : 0.19116105139255524\n",
      "Training loss for batch 3724 : 0.04732341319322586\n",
      "Training loss for batch 3725 : 0.07438725233078003\n",
      "Training loss for batch 3726 : 0.036811769008636475\n",
      "Training loss for batch 3727 : 0.1068475991487503\n",
      "Training loss for batch 3728 : 0.03038361482322216\n",
      "Training loss for batch 3729 : 0.10886053740978241\n",
      "Training loss for batch 3730 : 0.12397366762161255\n",
      "Training loss for batch 3731 : 0.05110223963856697\n",
      "Training loss for batch 3732 : 0.10804013162851334\n",
      "Training loss for batch 3733 : 9.780563858807767e-11\n",
      "Training loss for batch 3734 : 0.04898446053266525\n",
      "Training loss for batch 3735 : 0.038292303681373596\n",
      "Training loss for batch 3736 : 0.06215100735425949\n",
      "Training loss for batch 3737 : 0.0943647101521492\n",
      "Training loss for batch 3738 : 0.05900014564394951\n",
      "Training loss for batch 3739 : 0.09708499163389206\n",
      "Training loss for batch 3740 : 0.0778256356716156\n",
      "Training loss for batch 3741 : 0.09493991732597351\n",
      "Training loss for batch 3742 : 0.0291256383061409\n",
      "Training loss for batch 3743 : 0.07230629771947861\n",
      "Training loss for batch 3744 : 0.1783028393983841\n",
      "Training loss for batch 3745 : 0.1319793313741684\n",
      "Training loss for batch 3746 : 0.17430394887924194\n",
      "Training loss for batch 3747 : 0.0354747399687767\n",
      "Training loss for batch 3748 : 0.06001046672463417\n",
      "Training loss for batch 3749 : 0.024973303079605103\n",
      "Training loss for batch 3750 : 0.08747803419828415\n",
      "Training loss for batch 3751 : 0.054748132824897766\n",
      "Training loss for batch 3752 : 0.13166308403015137\n",
      "Training loss for batch 3753 : 0.17739331722259521\n",
      "Training loss for batch 3754 : 0.05710304155945778\n",
      "Training loss for batch 3755 : 0.20843984186649323\n",
      "Training loss for batch 3756 : 0.06692704558372498\n",
      "Training loss for batch 3757 : 0.06881800293922424\n",
      "Training loss for batch 3758 : 0.018420470878481865\n",
      "Training loss for batch 3759 : 0.10402627289295197\n",
      "Training loss for batch 3760 : 0.04391108825802803\n",
      "Training loss for batch 3761 : 0.22408543527126312\n",
      "Training loss for batch 3762 : 0.15702156722545624\n",
      "Training loss for batch 3763 : 0.09226443618535995\n",
      "Training loss for batch 3764 : 0.03600243479013443\n",
      "Training loss for batch 3765 : 0.005591374821960926\n",
      "Training loss for batch 3766 : 0.11722027510404587\n",
      "Training loss for batch 3767 : 0.09765364974737167\n",
      "Training loss for batch 3768 : 0.11126779019832611\n",
      "Training loss for batch 3769 : 0.12923195958137512\n",
      "Training loss for batch 3770 : 0.13985811173915863\n",
      "Training loss for batch 3771 : 0.02803700603544712\n",
      "Training loss for batch 3772 : 0.08724474161863327\n",
      "Training loss for batch 3773 : 0.005320653319358826\n",
      "Training loss for batch 3774 : 2.6710702538679243e-09\n",
      "Training loss for batch 3775 : 0.18119744956493378\n",
      "Training loss for batch 3776 : 0.05359771102666855\n",
      "Training loss for batch 3777 : 0.042112644761800766\n",
      "Training loss for batch 3778 : 0.010475262068212032\n",
      "Training loss for batch 3779 : 0.05057685077190399\n",
      "Training loss for batch 3780 : 0.049186863005161285\n",
      "Training loss for batch 3781 : 0.03712264820933342\n",
      "Training loss for batch 3782 : 0.05426223948597908\n",
      "Training loss for batch 3783 : 0.1532868891954422\n",
      "Training loss for batch 3784 : 0.07027561217546463\n",
      "Training loss for batch 3785 : 0.08402270078659058\n",
      "Training loss for batch 3786 : 0.047154005616903305\n",
      "Training loss for batch 3787 : 0.03423410654067993\n",
      "Training loss for batch 3788 : 0.031402818858623505\n",
      "Training loss for batch 3789 : 0.034485988318920135\n",
      "Training loss for batch 3790 : 0.20441056787967682\n",
      "Training loss for batch 3791 : 0.13927970826625824\n",
      "Training loss for batch 3792 : 0.10386040061712265\n",
      "Training loss for batch 3793 : 0.11344866454601288\n",
      "Training loss for batch 3794 : 0.11344854533672333\n",
      "Training loss for batch 3795 : 0.09302535653114319\n",
      "Training loss for batch 3796 : 0.13590529561042786\n",
      "Training loss for batch 3797 : 0.08191964030265808\n",
      "Training loss for batch 3798 : 0.06187168136239052\n",
      "Training loss for batch 3799 : 0.08869245648384094\n",
      "Training loss for batch 3800 : 0.05056849494576454\n",
      "Training loss for batch 3801 : 0.015007620677351952\n",
      "Training loss for batch 3802 : 0.12060122936964035\n",
      "Training loss for batch 3803 : 0.22693748772144318\n",
      "Training loss for batch 3804 : 0.05923008173704147\n",
      "Training loss for batch 3805 : 0.06098325178027153\n",
      "Training loss for batch 3806 : 0.05272424966096878\n",
      "Training loss for batch 3807 : 0.06445679068565369\n",
      "Training loss for batch 3808 : 0.10421647876501083\n",
      "Training loss for batch 3809 : 0.17676471173763275\n",
      "Training loss for batch 3810 : 0.13142335414886475\n",
      "Training loss for batch 3811 : 0.05914045497775078\n",
      "Training loss for batch 3812 : 0.05308535322546959\n",
      "Training loss for batch 3813 : 0.07052097469568253\n",
      "Training loss for batch 3814 : 0.08624777942895889\n",
      "Training loss for batch 3815 : 0.0911480039358139\n",
      "Training loss for batch 3816 : 0.0220776554197073\n",
      "Training loss for batch 3817 : 0.017284709960222244\n",
      "Training loss for batch 3818 : 0.12404726445674896\n",
      "Training loss for batch 3819 : 0.10138805210590363\n",
      "Training loss for batch 3820 : 0.11889993399381638\n",
      "Training loss for batch 3821 : 0.14805559813976288\n",
      "Training loss for batch 3822 : 0.007332317065447569\n",
      "Training loss for batch 3823 : 0.029660051688551903\n",
      "Training loss for batch 3824 : 0.021087275817990303\n",
      "Training loss for batch 3825 : 0.08171997964382172\n",
      "Training loss for batch 3826 : 0.004738661926239729\n",
      "Training loss for batch 3827 : 0.044446952641010284\n",
      "Training loss for batch 3828 : 0.01561621855944395\n",
      "Training loss for batch 3829 : 0.08731834590435028\n",
      "Training loss for batch 3830 : 0.08325354009866714\n",
      "Training loss for batch 3831 : 0.2356744259595871\n",
      "Training loss for batch 3832 : 0.11275199055671692\n",
      "Training loss for batch 3833 : 0.1579696238040924\n",
      "Training loss for batch 3834 : 0.11132791638374329\n",
      "Training loss for batch 3835 : 0.13000673055648804\n",
      "Training loss for batch 3836 : 0.12068819254636765\n",
      "Training loss for batch 3837 : 0.11459090560674667\n",
      "Training loss for batch 3838 : 0.13213799893856049\n",
      "Training loss for batch 3839 : 0.08895529806613922\n",
      "Training loss for batch 3840 : 0.13232749700546265\n",
      "Training loss for batch 3841 : 0.039152998477220535\n",
      "Training loss for batch 3842 : 0.10580264031887054\n",
      "Training loss for batch 3843 : 0.07190988212823868\n",
      "Training loss for batch 3844 : 0.05564194545149803\n",
      "Training loss for batch 3845 : 0.05104701966047287\n",
      "Training loss for batch 3846 : 0.07283206284046173\n",
      "Training loss for batch 3847 : 0.1793455332517624\n",
      "Training loss for batch 3848 : 0.16559436917304993\n",
      "Training loss for batch 3849 : 0.14653564989566803\n",
      "Training loss for batch 3850 : 0.0044620749540627\n",
      "Training loss for batch 3851 : 0.12407294660806656\n",
      "Training loss for batch 3852 : 0.07253909856081009\n",
      "Training loss for batch 3853 : 0.10936176031827927\n",
      "Training loss for batch 3854 : 0.026375822722911835\n",
      "Training loss for batch 3855 : 0.08271296322345734\n",
      "Training loss for batch 3856 : 0.05692732706665993\n",
      "Training loss for batch 3857 : 0.1422145962715149\n",
      "Training loss for batch 3858 : 0.009308170527219772\n",
      "Training loss for batch 3859 : 0.10932586342096329\n",
      "Training loss for batch 3860 : 0.12453111261129379\n",
      "Training loss for batch 3861 : 0.08795369416475296\n",
      "Training loss for batch 3862 : 0.0273188017308712\n",
      "Training loss for batch 3863 : 6.49830980137267e-08\n",
      "Training loss for batch 3864 : 0.0801103487610817\n",
      "Training loss for batch 3865 : 0.04828716814517975\n",
      "Training loss for batch 3866 : 0.022654220461845398\n",
      "Training loss for batch 3867 : 0.032772164791822433\n",
      "Training loss for batch 3868 : 0.06478221714496613\n",
      "Training loss for batch 3869 : 0.13551583886146545\n",
      "Training loss for batch 3870 : 0.07523652166128159\n",
      "Training loss for batch 3871 : 0.10661773383617401\n",
      "Training loss for batch 3872 : 0.06840600073337555\n",
      "Training loss for batch 3873 : 0.0443883091211319\n",
      "Training loss for batch 3874 : 0.26065710186958313\n",
      "Training loss for batch 3875 : 0.11577188968658447\n",
      "Training loss for batch 3876 : 0.08785292506217957\n",
      "Training loss for batch 3877 : 0.1288362592458725\n",
      "Training loss for batch 3878 : 0.1224517822265625\n",
      "Training loss for batch 3879 : 0.10722221434116364\n",
      "Training loss for batch 3880 : 0.03630824387073517\n",
      "Training loss for batch 3881 : 0.06293083727359772\n",
      "Training loss for batch 3882 : 0.059950001537799835\n",
      "Training loss for batch 3883 : 0.09032934904098511\n",
      "Training loss for batch 3884 : 0.038856733590364456\n",
      "Training loss for batch 3885 : 0.0672401711344719\n",
      "Training loss for batch 3886 : 0.03627181053161621\n",
      "Training loss for batch 3887 : 0.11809215694665909\n",
      "Training loss for batch 3888 : 0.085197813808918\n",
      "Training loss for batch 3889 : 0.0904417559504509\n",
      "Training loss for batch 3890 : 0.08069488406181335\n",
      "Training loss for batch 3891 : 0.12042703479528427\n",
      "Training loss for batch 3892 : 0.03305782750248909\n",
      "Training loss for batch 3893 : 0.1090988889336586\n",
      "Training loss for batch 3894 : 0.13405978679656982\n",
      "Training loss for batch 3895 : 0.01569809764623642\n",
      "Training loss for batch 3896 : 0.07596386224031448\n",
      "Training loss for batch 3897 : 0.0570233017206192\n",
      "Training loss for batch 3898 : 0.15553869307041168\n",
      "Training loss for batch 3899 : 0.03861623629927635\n",
      "Training loss for batch 3900 : 0.041649069637060165\n",
      "Training loss for batch 3901 : 0.09752552956342697\n",
      "Training loss for batch 3902 : 0.11397629231214523\n",
      "Training loss for batch 3903 : 0.1347234845161438\n",
      "Training loss for batch 3904 : 0.05583402141928673\n",
      "Training loss for batch 3905 : 0.09604741632938385\n",
      "Training loss for batch 3906 : 0.15795186161994934\n",
      "Training loss for batch 3907 : 0.1892252117395401\n",
      "Training loss for batch 3908 : 0.09046375006437302\n",
      "Training loss for batch 3909 : 0.01744459941983223\n",
      "Training loss for batch 3910 : 0.09526229649782181\n",
      "Training loss for batch 3911 : 0.23816846311092377\n",
      "Training loss for batch 3912 : 0.16617535054683685\n",
      "Training loss for batch 3913 : 0.06536417454481125\n",
      "Training loss for batch 3914 : 0.055747754871845245\n",
      "Training loss for batch 3915 : 0.020838886499404907\n",
      "Training loss for batch 3916 : 0.015214617364108562\n",
      "Training loss for batch 3917 : 0.05618233606219292\n",
      "Training loss for batch 3918 : 0.11317635327577591\n",
      "Training loss for batch 3919 : 0.05439288169145584\n",
      "Training loss for batch 3920 : 0.11241649091243744\n",
      "Training loss for batch 3921 : 0.07482846826314926\n",
      "Training loss for batch 3922 : 0.1393713653087616\n",
      "Training loss for batch 3923 : 0.1780964732170105\n",
      "Training loss for batch 3924 : 0.009643699042499065\n",
      "Training loss for batch 3925 : 0.0\n",
      "Training loss for batch 3926 : 0.1654110699892044\n",
      "Training loss for batch 3927 : 0.03642374649643898\n",
      "Training loss for batch 3928 : 0.14941589534282684\n",
      "Training loss for batch 3929 : 0.18780410289764404\n",
      "Training loss for batch 3930 : 0.08439604192972183\n",
      "Training loss for batch 3931 : 0.1759878247976303\n",
      "Training loss for batch 3932 : 0.0658613070845604\n",
      "Training loss for batch 3933 : 0.08963655680418015\n",
      "Training loss for batch 3934 : 0.11116664856672287\n",
      "Training loss for batch 3935 : 0.15395691990852356\n",
      "Training loss for batch 3936 : 0.06030578911304474\n",
      "Training loss for batch 3937 : 0.05345061048865318\n",
      "Training loss for batch 3938 : 0.13143663108348846\n",
      "Training loss for batch 3939 : 0.10356596857309341\n",
      "Training loss for batch 3940 : 0.06258397549390793\n",
      "Training loss for batch 3941 : 0.06785581260919571\n",
      "Training loss for batch 3942 : 0.13834036886692047\n",
      "Training loss for batch 3943 : 0.03571963682770729\n",
      "Training loss for batch 3944 : 0.14149922132492065\n",
      "Training loss for batch 3945 : 0.05190582945942879\n",
      "Training loss for batch 3946 : 0.14104336500167847\n",
      "Training loss for batch 3947 : 0.09569711238145828\n",
      "Training loss for batch 3948 : 0.019785426557064056\n",
      "Training loss for batch 3949 : 0.047962743788957596\n",
      "Training loss for batch 3950 : 0.030854014679789543\n",
      "Training loss for batch 3951 : 0.07293012738227844\n",
      "Training loss for batch 3952 : 0.042018789798021317\n",
      "Training loss for batch 3953 : 0.09475302696228027\n",
      "Training loss for batch 3954 : 0.07508402317762375\n",
      "Training loss for batch 3955 : 0.03843029960989952\n",
      "Training loss for batch 3956 : 0.1546400487422943\n",
      "Training loss for batch 3957 : 0.02640477381646633\n",
      "Training loss for batch 3958 : 0.07518370449542999\n",
      "Training loss for batch 3959 : 0.07581314444541931\n",
      "Training loss for batch 3960 : 0.056798797100782394\n",
      "Training loss for batch 3961 : 0.17612811923027039\n",
      "Training loss for batch 3962 : 0.08154074847698212\n",
      "Training loss for batch 3963 : 0.06436797976493835\n",
      "Training loss for batch 3964 : 0.04440058395266533\n",
      "Training loss for batch 3965 : 0.07756276428699493\n",
      "Training loss for batch 3966 : 0.08807359635829926\n",
      "Training loss for batch 3967 : 0.1097695603966713\n",
      "Training loss for batch 3968 : 0.04292462393641472\n",
      "Training loss for batch 3969 : 0.16603559255599976\n",
      "Training loss for batch 3970 : 0.059044405817985535\n",
      "Training loss for batch 3971 : 0.18963691592216492\n",
      "Training loss for batch 3972 : 0.10167139768600464\n",
      "Training loss for batch 3973 : 0.0804375484585762\n",
      "Training loss for batch 3974 : 0.03030361421406269\n",
      "Training loss for batch 3975 : 0.08298098295927048\n",
      "Training loss for batch 3976 : 0.172649085521698\n",
      "Training loss for batch 3977 : 0.17258679866790771\n",
      "Training loss for batch 3978 : 0.015756558626890182\n",
      "Training loss for batch 3979 : 0.14488497376441956\n",
      "Training loss for batch 3980 : 0.04082668945193291\n",
      "Training loss for batch 3981 : 0.08885151147842407\n",
      "Training loss for batch 3982 : 0.01423052977770567\n",
      "Training loss for batch 3983 : 0.039417315274477005\n",
      "Training loss for batch 3984 : 0.008953111246228218\n",
      "Training loss for batch 3985 : 0.004103309009224176\n",
      "Training loss for batch 3986 : 0.07001183182001114\n",
      "Training loss for batch 3987 : 0.1063193678855896\n",
      "Training loss for batch 3988 : 0.047480516135692596\n",
      "Training loss for batch 3989 : 0.01929832249879837\n",
      "Training loss for batch 3990 : 0.15233337879180908\n",
      "Training loss for batch 3991 : 0.10321774333715439\n",
      "Training loss for batch 3992 : 0.07094238698482513\n",
      "Training loss for batch 3993 : 0.18750953674316406\n",
      "Training loss for batch 3994 : 0.029820609837770462\n",
      "Training loss for batch 3995 : 0.11079911887645721\n",
      "Training loss for batch 3996 : 0.11706461757421494\n",
      "Training loss for batch 3997 : 0.04407687857747078\n",
      "Training loss for batch 3998 : 0.11594882607460022\n",
      "Training loss for batch 3999 : 0.12598708271980286\n",
      "Training loss for batch 4000 : 0.020951364189386368\n",
      "Training loss for batch 4001 : 0.01636328548192978\n",
      "Training loss for batch 4002 : 0.055931419134140015\n",
      "Training loss for batch 4003 : 0.046146392822265625\n",
      "Training loss for batch 4004 : 0.08368954807519913\n",
      "Training loss for batch 4005 : 0.0829322561621666\n",
      "Training loss for batch 4006 : 0.05079729110002518\n",
      "Training loss for batch 4007 : 0.047543250024318695\n",
      "Training loss for batch 4008 : 0.11795778572559357\n",
      "Training loss for batch 4009 : 0.16236358880996704\n",
      "Training loss for batch 4010 : 0.01386594120413065\n",
      "Training loss for batch 4011 : 0.07093178480863571\n",
      "Training loss for batch 4012 : 0.2629762589931488\n",
      "Training loss for batch 4013 : 0.04471785947680473\n",
      "Training loss for batch 4014 : 0.18597061932086945\n",
      "Training loss for batch 4015 : 0.06011657789349556\n",
      "Training loss for batch 4016 : 0.05614331737160683\n",
      "Training loss for batch 4017 : 0.11286202073097229\n",
      "Training loss for batch 4018 : 0.17325256764888763\n",
      "Training loss for batch 4019 : 0.09812198579311371\n",
      "Training loss for batch 4020 : 0.019016098231077194\n",
      "Training loss for batch 4021 : 0.17006729543209076\n",
      "Training loss for batch 4022 : 0.12161245197057724\n",
      "Training loss for batch 4023 : 0.027886468917131424\n",
      "Training loss for batch 4024 : 0.020167047157883644\n",
      "Training loss for batch 4025 : 0.13706494867801666\n",
      "Training loss for batch 4026 : 0.21943151950836182\n",
      "Training loss for batch 4027 : 0.08990442752838135\n",
      "Training loss for batch 4028 : 0.015257835388183594\n",
      "Training loss for batch 4029 : 0.042698293924331665\n",
      "Training loss for batch 4030 : 0.03230394795536995\n",
      "Training loss for batch 4031 : 0.08816181868314743\n",
      "Training loss for batch 4032 : 0.09945560246706009\n",
      "Training loss for batch 4033 : 0.11629056930541992\n",
      "Training loss for batch 4034 : 0.06353269517421722\n",
      "Training loss for batch 4035 : 0.10479307919740677\n",
      "Training loss for batch 4036 : 0.17321130633354187\n",
      "Training loss for batch 4037 : 0.0641363337635994\n",
      "Training loss for batch 4038 : 0.10405972599983215\n",
      "Training loss for batch 4039 : 0.06163443252444267\n",
      "Training loss for batch 4040 : 0.007159932982176542\n",
      "Training loss for batch 4041 : 0.06879108399152756\n",
      "Training loss for batch 4042 : 0.07027320563793182\n",
      "Training loss for batch 4043 : 0.09830842912197113\n",
      "Training loss for batch 4044 : 0.11185913532972336\n",
      "Training loss for batch 4045 : 0.08336789906024933\n",
      "Training loss for batch 4046 : 0.1421767920255661\n",
      "Training loss for batch 4047 : 0.040951456874608994\n",
      "Training loss for batch 4048 : 0.09428392350673676\n",
      "Training loss for batch 4049 : 0.07770244777202606\n",
      "Training loss for batch 4050 : 0.05488909035921097\n",
      "Training loss for batch 4051 : 0.05786348879337311\n",
      "Training loss for batch 4052 : 0.06937235593795776\n",
      "Training loss for batch 4053 : 0.1598271280527115\n",
      "Training loss for batch 4054 : 0.08619879931211472\n",
      "Training loss for batch 4055 : 0.12063273787498474\n",
      "Training loss for batch 4056 : 0.2113422155380249\n",
      "Training loss for batch 4057 : 0.17002147436141968\n",
      "Training loss for batch 4058 : 0.1729651838541031\n",
      "Training loss for batch 4059 : 0.15798887610435486\n",
      "Training loss for batch 4060 : 0.06672900915145874\n",
      "Training loss for batch 4061 : 0.02966003306210041\n",
      "Training loss for batch 4062 : 0.12787717580795288\n",
      "Training loss for batch 4063 : 0.015901990234851837\n",
      "Training loss for batch 4064 : 0.08739735186100006\n",
      "Training loss for batch 4065 : 0.1478181779384613\n",
      "Training loss for batch 4066 : 0.12501561641693115\n",
      "Training loss for batch 4067 : 0.11171423643827438\n",
      "Training loss for batch 4068 : 0.13306325674057007\n",
      "Training loss for batch 4069 : 0.09556926786899567\n",
      "Training loss for batch 4070 : 0.11305487900972366\n",
      "Training loss for batch 4071 : 0.1620451807975769\n",
      "Training loss for batch 4072 : 0.10862216353416443\n",
      "Training loss for batch 4073 : 0.15747949481010437\n",
      "Training loss for batch 4074 : 0.10714226216077805\n",
      "Training loss for batch 4075 : 0.02243231236934662\n",
      "Training loss for batch 4076 : 0.10469672828912735\n",
      "Training loss for batch 4077 : 0.07162518799304962\n",
      "Training loss for batch 4078 : 0.18079087138175964\n",
      "Training loss for batch 4079 : 0.06165977567434311\n",
      "Training loss for batch 4080 : 0.08650310337543488\n",
      "Training loss for batch 4081 : 0.07293775677680969\n",
      "Training loss for batch 4082 : 0.05519914627075195\n",
      "Training loss for batch 4083 : 0.025197505950927734\n",
      "Training loss for batch 4084 : 0.11381004750728607\n",
      "Training loss for batch 4085 : 0.15085254609584808\n",
      "Training loss for batch 4086 : 0.06731244921684265\n",
      "Training loss for batch 4087 : 0.12277811020612717\n",
      "Training loss for batch 4088 : 0.002961409045383334\n",
      "Training loss for batch 4089 : 0.2265697717666626\n",
      "Training loss for batch 4090 : 0.10461864620447159\n",
      "Training loss for batch 4091 : 0.1375584900379181\n",
      "Training loss for batch 4092 : 0.21337851881980896\n",
      "Training loss for batch 4093 : 0.09624990075826645\n",
      "Training loss for batch 4094 : 0.03996070474386215\n",
      "Training loss for batch 4095 : 0.050602905452251434\n",
      "Training loss for batch 4096 : 0.04380694031715393\n",
      "Training loss for batch 4097 : 0.03549833223223686\n",
      "Training loss for batch 4098 : 0.03821244463324547\n",
      "Training loss for batch 4099 : 0.13557463884353638\n",
      "Training loss for batch 4100 : 0.03254580870270729\n",
      "Training loss for batch 4101 : 0.12848718464374542\n",
      "Training loss for batch 4102 : 0.12431636452674866\n",
      "Training loss for batch 4103 : 0.08832675218582153\n",
      "Training loss for batch 4104 : 0.03409969061613083\n",
      "Training loss for batch 4105 : 0.05128273367881775\n",
      "Training loss for batch 4106 : 0.1264861524105072\n",
      "Training loss for batch 4107 : 0.1418447494506836\n",
      "Training loss for batch 4108 : 0.17475485801696777\n",
      "Training loss for batch 4109 : 0.04103660583496094\n",
      "Training loss for batch 4110 : 0.06569699943065643\n",
      "Training loss for batch 4111 : 0.038657452911138535\n",
      "Training loss for batch 4112 : 0.039852824062108994\n",
      "Training loss for batch 4113 : 0.03524687886238098\n",
      "Training loss for batch 4114 : 0.20433297753334045\n",
      "Training loss for batch 4115 : 0.14961972832679749\n",
      "Training loss for batch 4116 : 0.09376292675733566\n",
      "Training loss for batch 4117 : 0.13908199965953827\n",
      "Training loss for batch 4118 : 0.10024398565292358\n",
      "Training loss for batch 4119 : 0.06736293435096741\n",
      "Training loss for batch 4120 : 0.08008044213056564\n",
      "Training loss for batch 4121 : 0.02704540640115738\n",
      "Training loss for batch 4122 : 0.06397172063589096\n",
      "Training loss for batch 4123 : 0.01607946865260601\n",
      "Training loss for batch 4124 : 0.23278400301933289\n",
      "Training loss for batch 4125 : 0.15693026781082153\n",
      "Training loss for batch 4126 : 0.03141109272837639\n",
      "Training loss for batch 4127 : 0.062016215175390244\n",
      "Training loss for batch 4128 : 0.14575326442718506\n",
      "Training loss for batch 4129 : 0.06865528970956802\n",
      "Training loss for batch 4130 : 0.09220613539218903\n",
      "Training loss for batch 4131 : 0.10115402191877365\n",
      "Training loss for batch 4132 : 0.026476506143808365\n",
      "Training loss for batch 4133 : 0.07095377892255783\n",
      "Training loss for batch 4134 : 0.061489686369895935\n",
      "Training loss for batch 4135 : 0.0709359422326088\n",
      "Training loss for batch 4136 : 0.06830089539289474\n",
      "Training loss for batch 4137 : 0.045935217291116714\n",
      "Training loss for batch 4138 : 0.01963932439684868\n",
      "Training loss for batch 4139 : 0.06728218495845795\n",
      "Training loss for batch 4140 : 0.15526486933231354\n",
      "Training loss for batch 4141 : 0.06605390459299088\n",
      "Training loss for batch 4142 : 0.04990093782544136\n",
      "Training loss for batch 4143 : 0.030755048617720604\n",
      "Training loss for batch 4144 : 0.11616641283035278\n",
      "Training loss for batch 4145 : 0.10143418610095978\n",
      "Training loss for batch 4146 : 0.06801058351993561\n",
      "Training loss for batch 4147 : 0.07817169278860092\n",
      "Training loss for batch 4148 : 0.14125406742095947\n",
      "Training loss for batch 4149 : 0.01829301379621029\n",
      "Training loss for batch 4150 : 0.08640560507774353\n",
      "Training loss for batch 4151 : 0.04548504576086998\n",
      "Training loss for batch 4152 : 0.01159958727657795\n",
      "Training loss for batch 4153 : 0.029043428599834442\n",
      "Parameter containing:\n",
      "tensor(-0.2614, device='cuda:0', requires_grad=True)\n",
      "Training loss for batch 0 : 1.0000100135803223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True, num_workers=4)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m training_dataloader:\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Batch shape: (N, Anchor-Positive-Negative, C, H, W)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\andrew\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[0;32m    106\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[0;32m    108\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    109\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\users\\andrew\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[1;32mc:\\users\\andrew\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_got_empty_message \u001b[39mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[39m.\u001b[39mPeekNamedPipe(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle)[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(wait([\u001b[39mself\u001b[39;49m], timeout))\n",
      "File \u001b[1;32mc:\\users\\andrew\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[39m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[39m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[39m.\u001b[39;49mkeys(), timeout)\n\u001b[0;32m    880\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[39m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[39mfor\u001b[39;00m ov \u001b[39min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\users\\andrew\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[39mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mWaitForMultipleObjects(L, \u001b[39mFalse\u001b[39;49;00m, timeout)\n\u001b[0;32m    812\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3407)\n",
    "record = []\n",
    "model.train()\n",
    "for epoch in range(EPOCHES):\n",
    "    for fold in range(dataset.kfolds):\n",
    "        # training_dataset, validation_dataset = dataset.kfold_cross_validation(fold)\n",
    "        training_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "        # validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "        index = 0\n",
    "        for batch in training_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # Batch shape: (N, Anchor-Positive-Negative, C, H, W)\n",
    "            predictions = model.forward_features(batch)\n",
    "            training_loss = loss_function(predictions)\n",
    "            training_loss.backward()    \n",
    "            print(f\"Training loss for batch {index} : {training_loss}\")\n",
    "            record.append(training_loss)\n",
    "            index += 1\n",
    "            optimizer.step()\n",
    "        print(loss_function.regularizing_strength)\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'loss_state_dict': loss_function.state_dict(),\n",
    "        'loss': training_loss,\n",
    "        }, f'./Checkpoint/model_{epoch}.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m record \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m record]\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mplot(record)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'record' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "record = [x.cpu().detach() for x in record]\n",
    "plt.plot(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/49828 [00:02<58:03, 14.30it/s] \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.99 GiB total capacity; 10.68 GiB already allocated; 0 bytes free; 11.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(testloader):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Batch shape: (N, Anchor-Positive-Negative, C, H, W)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward_features(batch)\n\u001b[0;32m     12\u001b[0m     test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_function(predictions)\n\u001b[0;32m     13\u001b[0m test_loss \u001b[39m=\u001b[39m test_loss \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(validation_dataloader) \n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Desktop\\Projects\\AgeRecognition\\models.py:36\u001b[0m, in \u001b[0;36mViTAgeRecognizer.forward_features\u001b[1;34m(self, pairings)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pairings\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[0;32m     35\u001b[0m     anchor_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(pairings[:, \u001b[39m0\u001b[39m, :, :, :])\n\u001b[1;32m---> 36\u001b[0m     positive_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(pairings[:, \u001b[39m1\u001b[39;49m, :, :, :])\n\u001b[0;32m     37\u001b[0m     negative_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(pairings[:, \u001b[39m2\u001b[39m, :, :, :])\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([anchor_feat, positive_feat, negative_feat], \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Desktop\\Projects\\AgeRecognition\\models.py:28\u001b[0m, in \u001b[0;36mViTAgeRecognizer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[0;32m     29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(x)\n\u001b[0;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torchvision\\models\\vision_transformer.py:298\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    295\u001b[0m batch_class_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_token\u001b[39m.\u001b[39mexpand(n, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    296\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([batch_class_token, x], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    300\u001b[0m \u001b[39m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[0;32m    301\u001b[0m x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torchvision\\models\\vision_transformer.py:157\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m torch\u001b[39m.\u001b[39m_assert(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding\n\u001b[1;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m)))\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torchvision\\models\\vision_transformer.py:118\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    117\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)\n\u001b[1;32m--> 118\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(y)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m y\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\activation.py:685\u001b[0m, in \u001b[0;36mGELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgelu(\u001b[39minput\u001b[39;49m, approximate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapproximate)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.99 GiB total capacity; 10.68 GiB already allocated; 0 bytes free; 11.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "TEST_PAIRINGS = './test_data.csv'\n",
    "state_dict = torch.load('./Checkpoint/model_1.pt')\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "testset = AgeRecognitionDataset(triplet_csv_path=TEST_PAIRINGS, image_dir=IMAGE_DIR, preprocessor=preprocessor, kfolds=5, device=DEVICE)\n",
    "testloader  = DataLoader(testset, batch_size=1)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for batch in tqdm(testloader):\n",
    "    # Batch shape: (N, Anchor-Positive-Negative, C, H, W)\n",
    "    predictions = model.forward_features(batch)\n",
    "    test_loss += loss_function(predictions)\n",
    "test_loss = test_loss * 8 / len(validation_dataloader) \n",
    "print(f\"Average test loss : {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agerec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
