{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from preprocessor import AgeRecognitionPreprocessor\n",
    "from dataset import AgeRecognitionDataset\n",
    "from models import vit_l_16_age_recognizer, vit_b_16_age_recognizer\n",
    "from loss import AgeRecognitionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "IMAGE_DIR = './Cleaned/'\n",
    "TRAINING_PAIRINGS = './training_data.csv'\n",
    "BATCH_SIZE = 12\n",
    "EPOCHES = 1000\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_b_16_age_recognizer().to(DEVICE)\n",
    "loss_function = AgeRecognitionLoss().to(DEVICE)\n",
    "preprocessor = AgeRecognitionPreprocessor()\n",
    "dataset = AgeRecognitionDataset(triplet_csv_path=TRAINING_PAIRINGS, image_dir=IMAGE_DIR, preprocessor=preprocessor, kfolds=5, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(list(model.parameters()) + list(loss_function.parameters()), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=EPOCHES, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6646/6646 [41:35<00:00,  2.66it/s]  \n",
      "100%|██████████| 6646/6646 [1:13:27<00:00,  1.51it/s]    \n",
      "100%|██████████| 6646/6646 [41:33<00:00,  2.67it/s]   \n",
      "100%|██████████| 6646/6646 [46:39<00:00,  2.37it/s]   \n",
      "100%|██████████| 6646/6646 [40:12<00:00,  2.75it/s]\n",
      "100%|██████████| 6646/6646 [59:43<00:00,  1.85it/s]    \n",
      "100%|██████████| 6646/6646 [1:15:26<00:00,  1.47it/s]    \n",
      "100%|██████████| 6646/6646 [40:43<00:00,  2.72it/s]\n",
      "100%|██████████| 6646/6646 [40:43<00:00,  2.72it/s]\n",
      "100%|██████████| 6646/6646 [40:09<00:00,  2.76it/s]  \n",
      "  0%|          | 20/6646 [00:19<1:49:04,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(training_dataloader):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Batch shape: (N, Anchor-Positive-Negative, C, H, W)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward_features(batch)\n\u001b[1;32m---> 11\u001b[0m     training_loss \u001b[39m=\u001b[39m loss_function(predictions)\n\u001b[0;32m     12\u001b[0m     \u001b[39m# print(f\"Training loss for batch {index} : {training_loss}\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[39m# index += 1\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Envs\\agerec\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Desktop\\Projects\\AgeRecognition\\loss.py:28\u001b[0m, in \u001b[0;36mAgeRecognitionLoss.forward\u001b[1;34m(self, predictions)\u001b[0m\n\u001b[0;32m     26\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     27\u001b[0m triplet_loss_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtriplet_loss(anch_feat, pos_feat, neg_feat)\n\u001b[1;32m---> 28\u001b[0m cosine_embedding_loss_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcosine_embedding_loss(pos_feat, neg_feat, \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39;49mones(batch_size)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n\u001b[0;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimportance\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m triplet_loss_value \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimportance\u001b[39m.\u001b[39mitem()) \u001b[39m*\u001b[39m cosine_embedding_loss_value\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHES):\n",
    "    for fold in range(dataset.kfolds):\n",
    "        training_dataset, validation_dataset = dataset.kfold_cross_validation(fold)\n",
    "        training_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "        validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "        model.train()\n",
    "        # index = 0\n",
    "        for batch in tqdm(training_dataloader):\n",
    "            # Batch shape: (N, Anchor-Positive-Negative, C, H, W)\n",
    "            predictions = model.forward_features(batch)\n",
    "            training_loss = loss_function(predictions)\n",
    "            # print(f\"Training loss for batch {index} : {training_loss}\")\n",
    "            # index += 1\n",
    "        optimizer.step()\n",
    "        # del(batch)\n",
    "\n",
    "        # model.eval()\n",
    "        # validation_loss = 0\n",
    "        # for batch in tqdm(validation_dataloader):\n",
    "        #     # Batch shape: (N, Anchor-Positive-Negative, C, H, W)\n",
    "        #     predictions = model.forward_features(batch)\n",
    "        #     validation_loss += loss_function(predictions)\n",
    "        # validation_loss = validation_loss * BATCH_SIZE / len(validation_dataloader) \n",
    "        # print(f\"Validation loss : {validation_loss}\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': training_loss,\n",
    "        }, f'./Checkpoint/model_{epoch}.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agerec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
